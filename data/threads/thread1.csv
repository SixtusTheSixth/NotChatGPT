"Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. However, it is well known that deep GCNs will suffer from over-smoothing problem, where node representations tend to be indistinguishable as we stack up more layers. Although extensive research has confirmed this prevailing understanding, few theoretical analyses have been conducted to study the expressivity and trainability of deep GCNs. In this work, we demonstrate these characterizations by studying the Gaussian Process Kernel (GPK) and Graph Neural Tangent Kernel (GNTK) of an infinitely-wide GCN, corresponding to the analysis on expressivity and trainability, respectively. We first prove the expressivity of infinitely-wide GCNs decaying at an exponential rate by applying the mean-field theory on GPK. Besides, we formulate the asymptotic behaviors of GNTK in the large depth, which enables us to reveal the dropping trainability of wide and deep GCNs at an exponential rate. Additionally, we extend our theoretical framework to analyze residual connection-resemble techniques. We found that these techniques can mildly mitigate exponential decay, but they failed to overcome it fundamentally. Finally, all theoretical results in this work are corroborated experimentally on a variety of graph-structured datasets.",0
"This research investigates the impact of graph neural networks on trainability loss through aggregation methods. By evaluating common aggregation techniques used in wide graph neural networks (WGNN), we found that there exists a significant trade-off between expressivity and trainability. Specifically, as the size of a GNN model grows larger, training becomes exponentially more difficult due to the increased complexity of the architecture. However, we discovered that applying appropriate aggregation techniques can mitigate this issue by allowing for faster convergence during training. Our results demonstrate that carefully chosen aggregation methods lead to significantly improved performance across multiple benchmark datasets while maintaining comparable accuracy to models without aggregation. Overall, our findings suggest that understanding the intricacies of how graphs are processed can greatly benefit practitioners looking to design effective GNN architectures for their particular domains.",1
"Most existing set encoding algorithms operate under the assumption that all the elements of the set are accessible during training and inference. Additionally, it is assumed that there are enough computational resources available for concurrently processing sets of large cardinality. However, both assumptions fail when the cardinality of the set is prohibitively large such that we cannot even load the set into memory. In more extreme cases, the set size could be potentially unlimited, and the elements of the set could be given in a streaming manner, where the model receives subsets of the full set data at irregular intervals. To tackle such practical challenges in large-scale set encoding, we go beyond the usual constraints of invariance and equivariance and introduce a new property termed Mini-Batch Consistency that is required for large scale mini-batch set encoding. We present a scalable and efficient set encoding mechanism that is amenable to mini-batch processing with respect to set elements and capable of updating set representations as more data arrives. The proposed method respects the required symmetries of invariance and equivariance as well as being Mini-Batch Consistent for random partitions of the input set. We perform extensive experiments and show that our method is computationally efficient and results in rich set encoding representations for set-structured data.",0
"Title: ""Mini-batch Consistent Slot Set Encoder for Scalable Set Encoding""  Abstract: In natural language processing tasks, such as machine reading comprehension (MRC) and question answering, one common challenge is encoding sets of objects into fixed-length vectors that can capture their semantic meaning while also remaining scalable. In practice, slot set encoders require large amounts of memory due to the use of full matrix operations, which makes them impractical for handling larger datasets. To address these limitations, we propose the mini-batch consistent slot set encoder (MCSE), which effectively utilizes mini-batch gradient descent optimization to encode each object separately while ensuring consistency across all possible combinations. Our method significantly reduces memory usage compared to existing methods without sacrificing accuracy. Through extensive experiments on challenging benchmarks, including WikiReading and TyDiQA, we demonstrate that MCSE achieves superior performance over state-of-the-art approaches while scaling better to larger dataset sizes. Overall, our work contributes towards enabling more efficient and effective natural language processing models on large scale data distributions.",1
"\textit{Attention} computes the dependency between representations, and it encourages the model to focus on the important selective features. Attention-based models, such as Transformer and graph attention network (GAT), are widely utilized for sequential data and graph-structured data. This paper suggests a new interpretation and generalized structure of the attention in Transformer and GAT. For the attention in Transformer and GAT, we derive that the attention is a product of two parts: 1) the RBF kernel to measure the similarity of two instances and 2) the exponential of $L^{2}$ norm to compute the importance of individual instances. From this decomposition, we generalize the attention in three ways. First, we propose implicit kernel attention with an implicit kernel function instead of manual kernel selection. Second, we generalize $L^{2}$ norm as the $L^{p}$ norm. Third, we extend our attention to structured multi-head attention. Our generalized attention shows better performance on classification, translation, and regression tasks.",0
"Sure, I can write an abstract for you! Here is my attempt at summarizing the content of your paper:  ""This paper presents a new method for implicit kernel attention that enables efficient processing and analysis of high-dimensional data. Our approach leverages deep neural networks to learn complex representations of input features, while simultaneously attending to relevant subspaces of varying sizes and shapes. By using implicit regularization techniques, we achieve state-of-the-art performance on challenging benchmark tasks without relying on explicit constraints or heuristics. In particular, our results demonstrate significant improvements over strong baseline models across a range of applications including image classification, speech recognition, and language modeling.""",1
"As a new approach to train generative models, \emph{generative adversarial networks} (GANs) have achieved considerable success in image generation. This framework has also recently been applied to data with graph structures. We propose labeled-graph generative adversarial networks (LGGAN) to train deep generative models for graph-structured data with node labels. We test the approach on various types of graph datasets, such as collections of citation networks and protein graphs. Experiment results show that our model can generate diverse labeled graphs that match the structural characteristics of the training data and outperforms all alternative approaches in quality and generality. To further evaluate the quality of the generated graphs, we use them on a downstream task of graph classification, and the results show that LGGAN can faithfully capture the important aspects of the graph structure.",0
"Recently, graph representation learning has gained significant attention due to its ability to effectively capture complex structured data. One popular approach is based on Graph Convolutional Neural Networks (GCNN), which can learn expressive features directly from raw graphs. However, these methods often struggle to generalize well beyond the training distribution, leading to poor performance in few shot settings. To overcome such limitations, we propose Labeled Graph Generative Adversarial Networks (LGANs) that combine generative modeling and adversarial training to synthesize novel, realistic labeled graphs with diverse structures and node attributes. Our framework involves two competing networks: a generator network G that produces labeled graphs and a discriminator network D that evaluates their quality by comparing them against real graphs. We train both networks jointly using a minimax game whereby our goal is to make the generated graphs indistinguishable from real ones. In addition, we introduce a novel architecture called Message Passing Block (MPB) within G for message propagation across multiple hops in the graph. MPB enables efficient computation while preserving permutation equivariance, crucial properties for many graph analytic tasks. Finally, we demonstrate state-of-the-art results achieved by our LGANs on several benchmark datasets including social circles detection, semantic scene completion, and image generation. For example, given only a small number of examples per class during testing, LGANs substantially outperforms existing approaches like GCNNs, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs) and other models designed for generating graphs or images. In summary, our work develops effective labe",1
"This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step -- an important aspect in spatiotemporal predictive learning. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e., mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of competitor neural network models.",0
"CloudLSTM is a novel recurrent neural model that addresses the challenge of spatiotemporal point-cloud stream forecasting by leveraging the power of Long Short Term Memory (LSTM) networks. By utilizing LSTMs, our approach can effectively capture both short and long term dependencies within high-dimensional spatial data streams. Our method outperforms state-of-the-art alternatives through the use of multi-scale feature fusion and residual connections which enable more efficient learning and better gradient propagation. Extensive experimental results on three benchmark datasets demonstrate the effectiveness of our proposed framework in both accuracy and efficiency. In conclusion, our work advances the field of predictive modeling and has important implications for applications such as autonomous drones and robotics where real-time decision making is crucial.",1
"Our interest is in scientific problems with the following characteristics: (1) Data are naturally represented as graphs; (2) The amount of data available is typically small; and (3) There is significant domain-knowledge, usually expressed in some symbolic form. These kinds of problems have been addressed effectively in the past by Inductive Logic Programming (ILP), by virtue of 2 important characteristics: (a) The use of a representation language that easily captures the relation encoded in graph-structured data, and (b) The inclusion of prior information encoded as domain-specific relations, that can alleviate problems of data scarcity, and construct new relations. Recent advances have seen the emergence of deep neural networks specifically developed for graph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have been shown to be able to handle graph-structured data, less has been done to investigate the inclusion of domain-knowledge. Here we investigate this aspect of GNNs empirically by employing an operation we term ""vertex-enrichment"" and denote the corresponding GNNs as ""VEGNNs"". Using over 70 real-world datasets and substantial amounts of symbolic domain-knowledge, we examine the result of vertex-enrichment across 5 different variants of GNNs. Our results provide support for the following: (a) Inclusion of domain-knowledge by vertex-enrichment can significantly improve the performance of a GNN. That is, the performance VEGNNs is significantly better than GNNs across all GNN variants; (b) The inclusion of domain-specific relations constructed using ILP improves the performance of VEGNNs, across all GNN variants. Taken together, the results provide evidence that it is possible to incorporate symbolic domain knowledge into a GNN, and that ILP can play an important role in providing high-level relationships that are not easily discovered by a GNN.",0
"Artificial intelligence research has made significant progress in recent years due to advancements in deep learning techniques such as convolutional neural networks (CNN), recurrent neural networks (RNN) and graph neural networks (GNN). GNNs have emerged as powerful models for handling structured data, including graphs, but one limitation of these models is that they require large amounts of labeled training data to perform well. This paper proposes incorporating symbolic domain knowledge into GNNs to improve their performance on small datasets. Our method first uses natural language processing techniques to extract features from textual descriptions of nodes in the graph and then combines these symbolic representations with node features learned by the GNN model. We evaluate our approach on several benchmark datasets and demonstrate that it significantly improves classification accuracy compared to baseline GNN models. These results highlight the potential benefits of integrating symbolic domain knowledge into GNN models, enabling them to learn more effectively even when limited amounts of labeled data are available. The proposed approach paves the way towards more generalizable and explainable artificial intelligence systems that can operate across different domains without relying exclusively on vast quantities of data.",1
"Graph-structured data arise in a variety of real-world context ranging from sensor and transportation to biological and social networks. As a ubiquitous tool to process graph-structured data, spectral graph filters have been used to solve common tasks such as denoising and anomaly detection, as well as design deep learning architectures such as graph neural networks. Despite being an important tool, there is a lack of theoretical understanding of the stability properties of spectral graph filters, which are important for designing robust machine learning models. In this paper, we study filter stability and provide a novel and interpretable upper bound on the change of filter output, where the bound is expressed in terms of the endpoint degrees of the deleted and newly added edges, as well as the spatial proximity of those edges. This upper bound allows us to reason, in terms of structural properties of the graph, when a spectral graph filter will be stable. We further perform extensive experiments to verify intuition that can be gained from the bound.",0
"Recently, there has been growing interest in understanding the stability properties of graph filters due to their wide range of applications from signal processing and computer vision to social network analysis and neuroscience. However, despite extensive research on spectral graph filtering, there still exists limited work addressing interpretability of the stability bounds obtained through these methods. This study addresses that gap by providing interpretable stability bounds for spectral graph filters via new uncertainty principles and concentration inequalities derived specifically for graphs. Our proposed framework enables users to quantify the influence of edge uncertainties, noise and sampling density on filter outputs while offering insights into underlying geometric and structural factors affecting the robustness of graph filtering. By extending existing theory to encompass the effects of stochasticity, we contribute towards reliable predictions in complex systems and enable more informed decision making across domains relying on graph filtering techniques. We demonstrate our findings using simulations as well as real data examples ranging from image and biological networks to large scale social graphs.",1
"Machine learning on graph-structured data has attracted high research interest due to the emergence of Graph Neural Networks (GNNs). Most of the proposed GNNs are based on the node homophily, i.e neighboring nodes share similar characteristics. However, in many complex networks, nodes that lie to distant parts of the graph share structurally equivalent characteristics and exhibit similar roles (e.g chemical properties of distant atoms in a molecule, type of social network users). A growing literature proposed representations that identify structurally equivalent nodes. However, most of the existing methods require high time and space complexity. In this paper, we propose VNEstruct, a simple approach, based on entropy measures of the neighborhood's topology, for generating low-dimensional structural representations, that is time-efficient and robust to graph perturbations. Empirically, we observe that VNEstruct exhibits robustness on structural role identification tasks. Moreover, VNEstruct can achieve state-of-the-art performance on graph classification, without incorporating the graph structure information in the optimization, in contrast to GNN competitors.",0
"This paper presents ego-based entropy measures for structural representations on graphs. In recent years, graph theory has become increasingly important as a tool for analyzing social networks, communication systems, and other complex relationships among objects or entities. One challenge faced by researchers in this field is how to effectively quantify the structure and dynamics of these graphs, particularly when they are large and contain a high degree of complexity. This work addresses this problem by proposing a set of novel entropy measures that take into account the perspective of individual nodes (ego) within a given graph. These measures provide valuable insights into the distribution of certain topological properties around each node, enabling new ways to study network structures from the point of view of specific actors or agents. By evaluating real-world datasets using our proposed methodology, we demonstrate the effectiveness of our approach in helping us better understand the nature of complex networks, including their formation, evolution, and function. We conclude by discussing future directions for research in this area and potential applications of our findings across different domains.",1
"Online real estate platforms have become significant marketplaces facilitating users' search for an apartment or a house. Yet it remains challenging to accurately appraise a property's value. Prior works have primarily studied real estate valuation based on hedonic price models that take structured data into account while accompanying unstructured data is typically ignored. In this study, we investigate to what extent an automated visual analysis of apartment floor plans on online real estate platforms can enhance hedonic rent price appraisal. We propose a tailored two-staged deep learning approach to learn price-relevant designs of floor plans from historical price data. Subsequently, we integrate the floor plan predictions into hedonic rent price models that account for both structural and locational characteristics of an apartment. Our empirical analysis based on a unique dataset of 9174 real estate listings suggests that current hedonic models underutilize the available data. We find that (1) the visual design of floor plans has significant explanatory power regarding rent prices - even after controlling for structural and locational apartment characteristics, and (2) harnessing floor plans results in an up to 10.56% lower out-of-sample prediction error. We further find that floor plans yield a particularly high gain in prediction performance for older and smaller apartments. Altogether, our empirical findings contribute to the existing research body by establishing the link between the visual design of floor plans and real estate prices. Moreover, our approach has important implications for online real estate platforms, which can use our findings to enhance user experience in their real estate listings.",0
"This study examines how incorporating floor plans into hedonic models can improve rent price appraisals. Traditional hedonic pricing models assume that all units within a building have similar characteristics. However, differences in floor plan layouts, such as square footage or number of bedrooms and bathrooms, can significantly affect rental prices. By including these variations in the modeling process, we can gain more accurate estimates of rental values. We use a combination of data from real estate listings and transaction records, along with advanced statistical techniques, to develop our models. Our results show that accounting for variation in floor plan features leads to improved accuracy in predicting rent prices. Additionally, we find evidence of nonlinear relationships between certain variables and rent price, suggesting further refinement of hedonic models may be necessary. Overall, our research demonstrates the importance of considering spatial dimensions in property valuation, particularly in today’s competitive housing market where unique floor plans play a crucial role in determining rental rates.",1
"Graph neural networks (GNNs) are popular to use for classifying structured data in the context of machine learning. But surprisingly, they are rarely applied to regression problems. In this work, we adopt GNN for a classic but challenging nonlinear regression problem, namely the network localization. Our main findings are in order. First, GNN is potentially the best solution to large-scale network localization in terms of accuracy, robustness and computational time. Second, proper thresholding of the communication range is essential to its superior performance. Simulation results corroborate that the proposed GNN based method outperforms all state-of-the-art benchmarks by far. Such inspiring results are theoretically justified in terms of data aggregation, non-line-of-sight (NLOS) noise removal and low-pass filtering effect, all affected by the threshold for neighbor selection. Code is available at https://github.com/Yanzongzi/GNN-For-localization.",0
"In recent years, graph neural networks have emerged as a powerful tool for modeling complex relationships among nodes in large, heterogeneous graphs. One area where these models have shown great promise is network localization, the task of estimating the location of nodes in a network based on their connectivity patterns. This paper presents a new approach to network localization using a novel variant of graph neural networks that can efficiently handle very large datasets. Our method leverages state-of-the-art machine learning techniques to capture both spatial and topological features of the network, and outperforms existing methods across several benchmark data sets. We demonstrate the effectiveness of our method through extensive experiments and provide detailed analysis of the results. Overall, this work represents a significant step forward in the field of network localization and highlights the potential impact of graph neural networks on many other challenges in computer science and beyond.",1
"The von Mises-Fisher (vMF) is a well-known density model for directional random variables. The recent surge of the deep embedding methodologies for high-dimensional structured data such as images or texts, aimed at extracting salient directional information, can make the vMF model even more popular. In this article, we will review the vMF model and its mixture, provide detailed recipes of how to train the models, focusing on the maximum likelihood estimators, in Python/PyTorch. In particular, implementation of vMF typically suffers from the notorious numerical issue of the Bessel function evaluation in the density normalizer, especially when the dimensionality is high, and we address the issue using the MPMath library that supports arbitrary precision. For the mixture learning, we provide both minibatch-based large-scale SGD learning, as well as the EM algorithm which is a full batch estimator. For each estimator/methodology, we test our implementation on some synthetic data, while we also demonstrate the use case in a more realistic scenario of image clustering. Our code is publicly available in https://github.com/minyoungkim21/vmf-lib.",0
"This paper presents a detailed description of our work on developing a Python implementation using PyTorch library that provides a flexible framework for estimating density functions based on von Mises Fisher distribution (MFD) and mixture models. Our approach extends traditional density estimation methods by offering more robust models capable of handling non-Gaussian distributions commonly encountered in modern data analysis. We discuss the mathematical foundations behind MFDs and their utility as a modeling tool for circular data, followed by a comprehensive overview of related literature. Next, we outline our methodology and results from experimental evaluations that demonstrate improved performance compared to other approaches such as kernel density estimation, Gaussian mixtures, and neural networks. Additionally, we provide details regarding parameter tuning strategies, software deployment, and future research directions. Overall, our implementation offers computational scientists powerful tools for analyzing complex datasets in various scientific domains while contributing new insights into theoretical developments underpinning density estimation theory.",1
"Graph neural networks have shown superior performance in a wide range of applications providing a powerful representation of graph-structured data. Recent works show that the representation can be further improved by auxiliary tasks. However, the auxiliary tasks for heterogeneous graphs, which contain rich semantic information with various types of nodes and edges, have less explored in the literature. In this paper, to learn graph neural networks on heterogeneous graphs we propose a novel self-supervised auxiliary learning method using meta-paths, which are composite relations of multiple edge types. Our proposed method is learning to learn a primary task by predicting meta-paths as auxiliary tasks. This can be viewed as a type of meta-learning. The proposed method can identify an effective combination of auxiliary tasks and automatically balance them to improve the primary task. Our methods can be applied to any graph neural networks in a plug-in manner without manual labeling or additional data. The experiments demonstrate that the proposed method consistently improves the performance of link prediction and node classification on heterogeneous graphs.",0
"This paper proposes a method called self-supervised auxiliary learning (S2AL) to effectively integrate multiple tasks into one model by using meta-paths as supervisory signals. We apply S2AL on heterogeneous graphs, where each task has different feature representations but can share some common substructures represented by meta-paths. Our approach learns a shared representation that captures both unique and shared features among all tasks via meta-path based adversarial training. In this way, we reduce interdependence among different tasks and improve individual models’ performance. Experiments conducted on several benchmark datasets demonstrate substantial improvement over baselines under various evaluation metrics. Our work provides new insight into how to better exploit structured data sources for improved machine learning results.",1
"Graph attention networks (GATs) have been recognized as powerful tools for learning in graph structured data. However, how to enable the attention mechanisms in GATs to smoothly consider both structural and feature information is still very challenging. In this paper, we propose Graph Joint Attention Networks (JATs) to address the aforementioned challenge. Different from previous attention-based graph neural networks (GNNs), JATs adopt novel joint attention mechanisms which can automatically determine the relative significance between node features and structural coefficients learned from graph topology, when computing the attention scores. Therefore, representations concerning more structural properties can be inferred by JATs. Besides, we theoretically analyze the expressive power of JATs and further propose an improved strategy for the joint attention mechanisms that enables JATs to reach the upper bound of expressive power which every message-passing GNN can ultimately achieve, i.e., 1-WL test. JATs can thereby be seen as most powerful message-passing GNNs. The proposed neural architecture has been extensively tested on widely used benchmarking datasets, and has been compared with state-of-the-art GNNs for various downstream predictive tasks. Experimental results show that JATs achieve state-of-the-art performance on all the testing datasets.",0
"Recently, there has been significant interest in joint attention tasks that involve two individuals interacting towards common goals such as communication or cooperation. In most real world scenarios, the interaction involves continuous exchanges between agents. To handle this type of interaction better, we developed a graph neural network (GNN) model based on message passing architecture called ""GraphJAT"". Our method enables us to capture relationships among multiple parties involved in an interactive task at once by leveraging attention mechanisms across them all simultaneously. By applying masked self attention operations in the edge convolution layers of GNNs, our GraphJAT model allows each agent to selectively attend to specific regions of other parties. We demonstrate the effectiveness of our approach via several benchmark datasets including social sciences, human behavior, computer vision and natural language understanding tasks, which shows competitive results compared with state-of-the art methods while maintaining interpretability. In summary, the proposed approach advances the current research frontier in multi-agent systems and opens up new possibilities for future work.",1
"Convolutional Neural Networks(CNNs) has achieved remarkable performance breakthrough in Euclidean structure data. Recently, aggregation-transformation based Graph Neural networks(GNNs) gradually produce a powerful performance on non-Euclidean data. In this paper, we propose a cross-correlation based graph convolution method allowing to naturally generalize CNNs to non-Euclidean domains and inherit the excellent natures of CNNs, such as local filters, parameter sharing, flexible receptive field, etc. Meanwhile, it leverages dynamically generated convolution kernel and cross-correlation operators to address the shortcomings of prior methods based on aggregation-transformation or their approximations. Our method has achieved or matched popular state-of-the-art results across three established graph benchmarks: the Cora, Citeseer, and Pubmed citation network datasets.",0
"This research presents a novel approach to designing convolutional neural networks (CNNs) for image processing tasks that involves using lookup tables to represent feature maps as graphs. In traditional CNN architectures, filters slide over input images in a sliding window manner to extract local features from neighborhood patches. These extracted features are then combined linearly in different layers of the network. However, such approaches can lead to diluted representations due to pooling operations which reduce spatial resolution by downsampling the feature map. Our proposed method, on the other hand, uses graph representation to capture nonlinear relationships among pixels and preserve spatial information. We use a look up table subnetwork to encode spatial layout into high dimensional embeddings, which captures higher level semantic representations compared to the original images. Then we employ GraphConv operation to perform filtering along edges of the graph, producing more expressive representations than regular convolution. Furthermore, our model benefits greatly from the power of modern deep learning frameworks where the computation graph and gradient flow naturally follow the data through these transformations. Experimental results show consistent performance improvement across multiple datasets and demonstrate the effectiveness of our proposed framework.",1
"The medical field is creating large amount of data that physicians are unable to decipher and use efficiently. Moreover, rule-based expert systems are inefficient in solving complicated medical tasks or for creating insights using big data. Deep learning has emerged as a more accurate and effective technology in a wide range of medical problems such as diagnosis, prediction and intervention. Deep learning is a representation learning method that consists of layers that transform the data non-linearly, thus, revealing hierarchical relationships and structures. In this review we survey deep learning application papers that use structured data, signal and imaging modalities from cardiology. We discuss the advantages and limitations of applying deep learning in cardiology that also apply in medicine in general, while proposing certain directions as the most viable for clinical use.",0
"Here is a possible abstract:  Deep learning has proven to be a powerful tool in many fields, including computer vision, natural language processing, and speech recognition. In recent years, researchers have started exploring the potential applications of deep learning in cardiology, which holds great promise due to the large amounts of data available from medical images and patient records. This review article provides an overview of current trends in applying deep learning techniques to problems related to heart health and disease management. We examine several approaches that have been proposed in the literature, such as using convolutional neural networks (CNNs) for automated analysis of echocardiograms and electrocardiograms (ECGs), identifying relevant features through transfer learning from pre-trained models, and incorporating domain knowledge into the model design process. We then discuss some of the challenges faced by these methods, such as dataset bias, interpretability, and integration with clinical workflows. Finally, we outline future directions for deep learning research in cardiology, emphasizing the need for collaboration across disciplines and continued evaluation of real-world impact. Overall, while there remain significant barriers to overcome, we believe that deep learning represents a promising path forward for advancing our understanding and treatment of heart diseases.",1
"Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.",0
"Summarizing text has been a longstanding challenge due to the difficulty of identifying important information from large amounts of data. Traditional methods such as manual summarization require extensive time and effort, while automated methods have struggled to provide accurate results. Recent advancements in natural language processing (NLP) have led to the development of structured neural models that can effectively extract key points from large documents. These models use deep learning techniques to analyze and identify critical sentences within texts, allowing them to generate high-quality summaries quickly and efficiently. This paper presents an overview of recent research on structured neural summarization, discussing their design, evaluation, and application. By providing a comprehensive analysis of the state-of-the-art technology, we aim to inspire further work towards developing innovative solutions for efficient summarization.",1
"Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",0
"Advances in machine learning have led to the development of increasingly complex neural networks that can perform a wide range of tasks such as image classification, natural language processing, and even playing games. However, these traditional feedforward neural networks have limitations in terms of scalability and interpretability. To address these issues, directed acyclic graph (DAG) neural networks have emerged as a promising alternative architecture. DAG neural networks allow for greater flexibility in model structure and enable parallel computation, making them well suited for handling large datasets. This paper presents a detailed study on how DAG neural networks can outperform traditional feedforward neural networks in several benchmarking tasks while offering improved explainability and transparency. Our findings showcase the potential of DAG neural networks for realizing more efficient and effective artificial intelligence systems across diverse applications.",1
"A big mystery in deep learning continues to be the ability of methods to generalize when the number of model parameters is larger than the number of training examples. In this work, we take a step towards a better understanding of the underlying phenomena of Deep Autoencoders (AEs), a mainstream deep learning solution for learning compressed, interpretable, and structured data representations. In particular, we interpret how AEs approximate the data manifold by exploiting their continuous piecewise affine structure. Our reformulation of AEs provides new insights into their mapping, reconstruction guarantees, as well as an interpretation of commonly used regularization techniques. We leverage these findings to derive two new regularizations that enable AEs to capture the inherent symmetry in the data. Our regularizations leverage recent advances in the group of transformation learning to enable AEs to better approximate the data manifold without explicitly defining the group underlying the manifold. Under the assumption that the symmetry of the data can be explained by a Lie group, we prove that the regularizations ensure the generalization of the corresponding AEs. A range of experimental evaluations demonstrate that our methods outperform other state-of-the-art regularization techniques.",0
"This paper presents an overview of deep autoencoders and their applications in understanding and generalizing data patterns. We begin by discussing how autoencoders can be used to learn representations that capture underlying structures in high dimensional data sets. Next, we explore recent advancements in training procedures that allow for efficient optimization and regularization of these models. Finally, we highlight promising future directions such as extending guarantees beyond traditional iid assumptions and leveraging new techniques from generative modeling research. Our focus throughout is on providing clear explanations and intuitions behind theoretical results while demonstrating concrete examples using real world datasets whenever possible. By emphasizing both theory and practice, our goal is to provide readers with the tools they need to design and evaluate autoencoder based methods on their own problems of interest.",1
"Graph Neural Networks (GNNs) have received considerable attention on graph-structured data learning for a wide variety of tasks. The well-designed propagation mechanism which has been demonstrated effective is the most fundamental part of GNNs. Although most of GNNs basically follow a message passing manner, litter effort has been made to discover and analyze their essential relations. In this paper, we establish a surprising connection between different propagation mechanisms with a unified optimization problem, showing that despite the proliferation of various GNNs, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term. Our proposed unified optimization framework, summarizing the commonalities between several of the most representative GNNs, not only provides a macroscopic view on surveying the relations between different GNNs, but also further opens up new opportunities for flexibly designing new GNNs. With the proposed framework, we discover that existing works usually utilize naive graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels showing low-pass or high-pass filtering capabilities respectively. Moreover, we provide the convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets clearly show that the proposed GNNs not only outperform the state-of-the-art methods but also have good ability to alleviate over-smoothing, and further verify the feasibility for designing GNNs with our unified optimization framework.",0
"This paper presents a novel optimization framework for graph neural networks (GNNs) that interprets and unifies their behavior under different conditions. We provide theoretical insights into how GNNs aggregate neighborhood information during the learning process by viewing them as special cases of generalized linear models. Our experiments show that our proposed method achieves superior performance on benchmark datasets compared to popular state-of-the-art baselines across various application domains such as social network analysis, molecular bioinformatics, chemoinformatics, and knowledge graph reasoning tasks. Overall, our work advances the understanding of deep learning models operating on graphs and has applications in diverse fields that rely heavily on node attributes associated within relational structures.",1
"Kernel methods have been among the most popular techniques in machine learning, where learning tasks are solved using the property of reproducing kernel Hilbert space (RKHS). In this paper, we propose a novel data analysis framework with reproducing kernel Hilbert $C^*$-module (RKHM) and kernel mean embedding (KME) in RKHM. Since RKHM contains richer information than RKHS or vector-valued RKHS (vv RKHS), analysis with RKHM enables us to capture and extract structural properties in multivariate data, functional data and other structured data. We show a branch of theories for RKHM to apply to data analysis, including the representer theorem, and the injectivity and universality of the proposed KME. We also show RKHM generalizes RKHS and vv RKHS. Then, we provide concrete procedures for employing RKHM and the proposed KME to data analysis.",0
"This paper examines reproducing kernel Hilbert C*-modules as a general framework for functional analysis. We study properties of reproducing kernels on arbitrary sets, including their positivity, injectivity, stability under linear transformations, and relationships with the positive cone of a von Neumann algebra. Next, we consider operator spaces associated with reproducing kernel Hilbert modules over a commutative unital C*-algebra, with special attention paid to the case of multiplier algebras of noncommutative domains. Lastly, we explore connections between reproducing kernel Hilbert C*-modules and classical function theory, discussing embedding results via kernel means and related holomorphic functional calculus. Our findings contribute new insight into the mathematical foundations underlying these areas of inquiry, with potential applications across fields ranging from pure mathematics to data science and machine learning. Keywords: reproducing kernel Hilbert module, operator space, embedded spectral triple, Schur multiplication, noncommutative domain. --arXiv:2008.11976",1
"Graph representation of structured data can facilitate the extraction of stereoscopic features, and it has demonstrated excellent ability when working with deep learning systems, the so-called Graph Neural Networks (GNNs). Choosing a promising architecture for constructing GNNs can be transferred to a hyperparameter optimisation problem, a very challenging task due to the size of the underlying search space and high computational cost for evaluating candidate GNNs. To address this issue, this research presents a novel genetic algorithm with a hierarchical evaluation strategy (HESGA), which combines the full evaluation of GNNs with a fast evaluation approach. By using full evaluation, a GNN is represented by a set of hyperparameter values and trained on a specified dataset, and root mean square error (RMSE) will be used to measure the quality of the GNN represented by the set of hyperparameter values (for regression problems). While in the proposed fast evaluation process, the training will be interrupted at an early stage, the difference of RMSE values between the starting and interrupted epochs will be used as a fast score, which implies the potential of the GNN being considered. To coordinate both types of evaluations, the proposed hierarchical strategy uses the fast evaluation in a lower level for recommending candidates to a higher level, where the full evaluation will act as a final assessor to maintain a group of elite individuals. To validate the effectiveness of HESGA, we apply it to optimise two types of deep graph neural networks. The experimental results on three benchmark datasets demonstrate its advantages compared to Bayesian hyperparameter optimization.",0
"In recent years, graph neural networks (GNNs) have emerged as a powerful tool for modelling complex relationships between data points in graph structures. However, finding optimal hyperparameters for GNN models remains challenging due to their high computational cost and sensitivity to parameter settings. This study presents a novel genetic algorithm (GA) with a hierarchical evaluation strategy to efficiently optimize hyperparameters for GNN models on benchmark datasets. Our approach improves upon existing methods by incorporating domain knowledge into the search process and using parallel computing techniques to speed up the optimization procedure. We show that our method outperforms state-of-the-art approaches across various benchmark datasets and architectures, achieving significant improvements in model accuracy while reducing computation time. This research contributes to the field of hyperparameter optimization and demonstrates the potential of GAs to effectively find optimal hyperparameter combinations for GNN models.",1
"Deep generative models, since their inception, have become increasingly more capable of generating novel and perceptually realistic signals (e.g., images and sound waves). With the emergence of deep models for graph structured data, natural interests seek extensions of these generative models for graphs. Successful extensions were seen recently in the case of learning from a collection of graphs (e.g., protein data banks), but the learning from a single graph has been largely under explored. The latter case, however, is important in practice. For example, graphs in financial and healthcare systems contain so much confidential information that their public accessibility is nearly impossible, but open science in these fields can only advance when similar data are available for benchmarking.   In this work, we propose an approach to generating a doppelganger graph that resembles a given one in many graph properties but nonetheless can hardly be used to reverse engineer the original one, in the sense of a near zero edge overlap. The approach is an orchestration of graph representation learning, generative adversarial networks, and graph realization algorithms. Through comparison with several graph generative models (either parameterized by neural networks or not), we demonstrate that our result barely reproduces the given graph but closely matches its properties. We further show that downstream tasks, such as node classification, on the generated graphs reach similar performance to the use of the original ones.",0
"This paper presents a method for creating a ""doppelganger graph"" using machine learning techniques. This method involves training a generative model on large amounts of data, such as images or text, and then using that model to create new samples that resemble existing ones while still being distinct from them. We show how this approach can be used to generate images that look like famous paintings, but are actually unique and different from the originals. Our experiments demonstrate the effectiveness of our method, achieving high accuracy on image generation tasks while preserving diversity and uniqueness. Overall, we believe that this work has important implications for computer graphics and art applications, and could potentially lead to more creative and innovative uses of artificial intelligence.",1
"A fundamental problem on graph-structured data is that of quantifying similarity between graphs. Graph kernels are an established technique for such tasks; in particular, those based on random walks and return probabilities have proven to be effective in wide-ranging applications, from bioinformatics to social networks to computer vision. However, random walk kernels generally suffer from slowness and tottering, an effect which causes walks to overemphasize local graph topology, undercutting the importance of global structure. To correct for these issues, we recast return probability graph kernels under the more general framework of density of states -- a framework which uses the lens of spectral analysis to uncover graph motifs and properties hidden within the interior of the spectrum -- and use our interpretation to construct scalable, composite density of states based graph kernels which balance local and global information, leading to higher classification accuracies on a host of benchmark datasets.",0
"Abstract: In this work, we introduce density of states graph kernels (DoSK), a novel approach for graph similarity comparison based on kernel representations of the density of states (DOS) from eigendecomposition techniques. These kernels capture key chemical fingerprints that allow us to effectively compare molecules across different chemotypes and enrich biological interpretations. We evaluate our approach on benchmark datasets and demonstrate its superior performance over state-of-the-art graph kernels, validating its potential applications in areas such as virtual screening, drug discovery, and protein structure modeling. By leveraging machine learning models trained with these high-quality features derived from DoSK, we anticipate improved accuracy in predictive tasks related to molecular interactions and their effects on cellular processes. This study broadens the scope of applicability of graph kernels beyond common application domains and paves the way for new perspectives in chemical informatics research.",1
"Lots of neural network architectures have been proposed to deal with learning tasks on graph-structured data. However, most of these models concentrate on only node features during the learning process. The edge features, which usually play a similarly important role as the nodes, are often ignored or simplified by these models. In this paper, we present edge-featured graph attention networks, namely EGATs, to extend the use of graph neural networks to those tasks learning on graphs with both node and edge features. These models can be regarded as extensions of graph attention networks (GATs). By reforming the model structure and the learning process, the new models can accept node and edge features as inputs, incorporate the edge information into feature representations, and iterate both node and edge features in a parallel but mutual way. The results demonstrate that our work is highly competitive against other node classification approaches, and can be well applied in edge-featured graph learning tasks.",0
"Graph attention networks (GATs) have become popular tools for modeling graph-structured data due to their ability to automatically weigh importance of neighboring nodes while aggregating node features. However, they face difficulty in capturing global dependencies that extend beyond local neighborhoods. In ""Edge-Featured Graph Attention Network"", the authors propose a novel approach called edge-featured GAT (efGAT), which incorporates edge features into the attention mechanism to better capture such distant relationships. This method introduces additional parameters to estimate edge weights based on edge features, improving the expressiveness of the attention operation and enabling the network to effectively learn both short-range and long-range dependencies. The proposed architecture outperforms standard GAT models across multiple benchmark datasets, demonstrating the effectiveness of incorporating edge features into graph neural networks.",1
"There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.",0
"Title: ""A Novel Approach to Graph Neural Network Models""  The graph neural network (GNN) model has emerged as one of the most promising techniques in machine learning due to its ability to effectively process complex data structures such as graphs and networks. Despite their success, existing GNN models still have limitations that restrict their applicability in real world scenarios. To address these issues, we propose a novel approach to designing GNN models by introducing a comprehensive taxonomy of key components and operations within GNN architectures. By providing a clear classification scheme and a thorough analysis of each component, our framework allows researchers to better understand how different designs can impact performance across diverse domains and datasets. Our new methodology paves the way towards developing more effective GNN models by establishing guidelines for selecting appropriate building blocks and identifying future directions for improvement. In summary, our work contributes valuable insights into the design of modern machine learning systems using graphs and presents exciting opportunities for advancing state-of-the-art methods in this rapidly evolving field.",1
"Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-of-the-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes.",0
"Recent advances in deep learning have led to a surge of interest in convolutional neural networks (CNNs), which are particularly effective at processing large volumes of image data. Despite their success, traditional CNN models suffer from several limitations that make them less suitable for certain types of analysis tasks, such as problems requiring non-rigid object pose estimation and deformable shape prediction. This paper presents an innovative new approach to address these issues by developing Deep Parameteric Convolutional Neural Networks (DPCN). DPCN is based on the idea of incorporating continuous latent variables into the standard CNN architecture. These continuous latent vectors allow the model to capture non-rigid deformations as well as high order variations, resulting in improved performance across a range of application domains. To validate our methodology, we evaluated our proposed DPCN system on multiple benchmark datasets, demonstrating significant improvements over state-of-the-art methods across all metrics. Overall, our findings suggest that the integration of continuous representations within conventional CNN architectures provides an effective means of boosting their capabilities, leading to more accurate and robust analysis in challenging scenarios. Our work paves the way for further research in designing deep learning systems that can effectively handle complex real-world applications involving geometric transformations and uncertainty.",1
"Graph convolutional networks have achieved great success on graph-structured data. Many graph convolutional networks can be regarded as low-pass filters for graph signals. In this paper, we propose a new model, BiGCN, which represents a graph neural network as a bi-directional low-pass filter. Specifically, we not only consider the original graph structure information but also the latent correlation between features, thus BiGCN can filter the signals along with both the original graph and a latent feature-connection graph. Our model outperforms previous graph neural networks in the tasks of node classification and link prediction on most of the benchmark datasets, especially when we add noise to the node features.",0
"In summary. Our new graph neural network model BiGCN uses bi-directional low-pass filtering (BPF) convolutions that effectively integrate structured features from different scales into feature learning processes for graphs. By using multiple BPF layers, we ensure that our model can capture global features by learning both short-range dependencies along local paths and long-range dependencies across distant nodes in large networks. Our design elegantly integrates dynamic node attention mechanisms, resulting in a more expressive architecture. Experimental results show improvements over state-of-the-art models on diverse benchmark datasets for tasks ranging from semi-supervised classification to recommendation systems. These significant advancements validate BiGCN as an effective tool for solving real-world problems involving complex relationships. Keywords: Graph Neural Networks (GNN), Relational Learning, Semi-Supervised Learning, Recommendation Systems",1
"Graph convolutional networks (GCN) have recently demonstrated their potential in analyzing non-grid structure data that can be represented as graphs. The core idea is to encode the local topology of a graph, via convolutions, into the feature of a center node. In this paper, we propose a novel GCN model, which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike conventional GCN models that carry out node-based attentions within each layer, the proposed SPAGAN conducts path-based attention that explicitly accounts for the influence of a sequence of nodes yielding the minimum cost, or shortest path, between the center node and its higher-order neighbors. SPAGAN therefore allows for a more informative and intact exploration of the graph structure and further {a} more effective aggregation of information from distant neighbors into the center node, as compared to node-based GCN methods. We test SPAGAN on the downstream classification task on several standard datasets, and achieve performances superior to the state of the art. Code is publicly available at https://github.com/ihollywhy/SPAGAN.",0
"""SPAGAN (Shortest Path Graph Attention Network) addresses one of the most important problems faced by many computer vision tasks today - finding global dependencies between objects without losing local details. This challenge can often lead to incomplete solutions that either overlook crucial connections between features or oversimplify complex relationships within images. To solve this problem, we propose a novel graph attention network architecture that leverages shortest path distance transforms to learn contextualized representations from image graphs. Our model constructs a fully connected graph where each node represents a feature map location and edge weights capture pairwise similarity between nodes. By incorporating shortest paths into our graph construction process, we preserve long range information while keeping computational complexity low enough for scalability on high resolution imagery. We then train an encoder-decoder architecture on these graphs to perform arbitrary visual reasoning tasks such as object detection, segmentation, or pose estimation. Through extensive experiments across multiple datasets and tasks, we demonstrate how SPAGAN consistently outperforms existing state-of-the art methods. Additionally, we ablate key components of our model and provide analysis showing improved performance in capturing spatial hierarchies in scene understanding.""",1
"A scalable semi-supervised node classification method on graph-structured data, called GraphHop, is proposed in this work. The graph contains attributes of all nodes but labels of a few nodes. The classical label propagation (LP) method and the emerging graph convolutional network (GCN) are two popular semi-supervised solutions to this problem. The LP method is not effective in modeling node attributes and labels jointly or facing a slow convergence rate on large-scale graphs. GraphHop is proposed to its shortcoming. With proper initial label vector embeddings, each iteration of GraphHop contains two steps: 1) label aggregation and 2) label update. In Step 1, each node aggregates its neighbors' label vectors obtained in the previous iteration. In Step 2, a new label vector is predicted for each node based on the label of the node itself and the aggregated label information obtained in Step 1. This iterative procedure exploits the neighborhood information and enables GraphHop to perform well in an extremely small label rate setting and scale well for very large graphs. Experimental results show that GraphHop outperforms state-of-the-art graph learning methods on a wide range of tasks (e.g., multi-label and multi-class classification on citation networks, social graphs, and commodity consumption graphs) in graphs of various sizes. Our codes are publicly available on GitHub (https://github.com/TianXieUSC/GraphHop).",0
"This paper presents a new approach for node classification in graph data, called GraphHop. We improve upon traditional label propagation algorithms by introducing a novel method that incorporates edge features into our model. Our algorithm shows promising results over benchmark datasets, demonstrating improved accuracy compared to state-of-the-art methods. In addition, we provide theoretical analysis supporting our enhanced model and empirical evaluation on synthetic and real world networks. Overall, GraphHop provides an efficient solution for predictive tasks on graphs, outperforming existing approaches while reducing computational cost.",1
"Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erd\H{o}s -- R\'{e}nyi graph. We show that when the Erd\H{o}s -- R\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ""information loss"" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.",0
"Graph neural networks (GNNs) have shown great promise for node classification on graphs. However, recent work has uncovered limitations in their expressive power for deep learning tasks. In particular, GNNs suffer from exponential decay in expressiveness as they pass messages along edges during propagation, causing them to become less powerful over time. This research investigates why such a limitation occurs by analyzing message passing through graphs using graph theory concepts. We present theoretical evidence that GNNs cannot recover certain global properties of graphs, leading to significant loss of expressive power. Our results show that even adding more layers can only partially mitigate this issue. Our findings highlight potential bottlenecks in current approaches towards designing GNN models and suggest directions for future research in developing increasingly effective methods. The insights presented in this study aim to steer the development of GNN architectures toward building deeper understanding of how these models operate and better leverage data structure and connectivity features in graph representation learning.",1
"In recent years, ride-hailing services have been increasingly prevalent as they provide huge convenience for passengers. As a fundamental problem, the timely prediction of passenger demands in different regions is vital for effective traffic flow control and route planning. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modeling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges (e.g., origin-destination relationship, geographical distance, etc.). Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed, and weighted (DDW) graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of DDW graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Moreover, the model employs a subtask to conduct pretraining so that it can obtain accurate results more quickly. We evaluate the proposed model on real-world datasets, and our experimental results demonstrate that Gallat outperforms the state-of-the-art approaches.",0
"This paper presents a method for predicting passenger mobility on transportation networks using representation learning techniques applied to dynamic directed graphs. By incorporating real-time traffic data and travel patterns into graph representations, we can accurately model complex human behavior and improve predictions for future commutes. Our proposed approach outperforms existing methods by leveraging both structural and temporal aspects of the network, allowing us to capture nuanced relationships between nodes and edges over time. With these insights, we can inform urban planning decisions and optimize public transport systems. Overall, our research demonstrates the power of representation learning in advancing transportation science and improving quality of life for citizens in congested metropolitan areas.",1
"The development of Graph Neural Networks (GNNs) has led to great progress in machine learning on graph-structured data. These networks operate via diffusing information across the graph nodes while capturing the structure of the graph. Recently there has also seen tremendous progress in quantum computing techniques. In this work, we explore applications of multi-particle quantum walks on diffusing information across graphs. Our model is based on learning the operators that govern the dynamics of quantum random walkers on graphs. We demonstrate the effectiveness of our method on classification and regression tasks.",0
"""This paper proposes bosonic random walk networks (BRWN) as a new method for graph learning."" Title: Bosonic Random Walk Networks for Graph Learning  Abstract: In recent years, there has been growing interest in developing efficient methods for graph learning, which involves identifying important features from large data sets. One approach that has gained popularity is using random walks on graphs, where each node is randomly selected and visited before moving onto one of its neighbors. However, traditional random walk models have limitations in capturing high dimensional relationships among nodes due to their linear nature. To address these issues, we propose bosonic random walk networks (BRWN), a novel nonlinear extension of classical random walk models.  In BRWN, nodes represent features in the data set while edges represent similarity relations between those features. During the network construction process, each feature visits its connected neighbors with probability proportional to its strength of interaction with other features, resulting in a weighted adjacency matrix. By modeling these weights through bosons, we capture complex interactions among features while retaining the computational simplicity of classic random walk models.  The proposed framework offers several advantages over existing techniques. Firstly, BRWN can effectively handle high-dimensional datasets by leveraging the power of boson sampling algorithms. Secondly, our methodology naturally encodes domain knowledge into the system, enabling incorporation of prior expertise during graph construction. Finally, experimental results demonstrate the superior performance of BRWN compared to state-of-the-art baseline methods across multiple application domains including social network analysis, image classification, and biological pathway discovery.  Overall, our work represents an innovative step forward in the development of random walk based graph learning approaches, paving the way fo",1
"Graph Neural Networks (GNNs) are the subject of intense focus by the machine learning community for problems involving relational reasoning. GNNs can be broadly divided into spatial and spectral approaches. Spatial approaches use a form of learned message-passing, in which interactions among vertices are computed locally, and information propagates over longer distances on the graph with greater numbers of message-passing steps. Spectral approaches use eigendecompositions of the graph Laplacian to produce a generalization of spatial convolutions to graph structured data which access information over short and long time scales simultaneously. Here we introduce the Spectral Graph Network, which applies message passing to both the spatial and spectral domains. Our model projects vertices of the spatial graph onto the Laplacian eigenvectors, which are each represented as vertices in a fully connected ""spectral graph"", and then applies learned message passing to them. We apply this model to various benchmark tasks including a graph-based variant of MNIST classification, molecular property prediction on MoleculeNet and QM9, and shortest path problems on random graphs. Our results show that the Spectral GN promotes efficient training, reaching high performance with fewer training iterations despite having more parameters. The model also provides robustness to edge dropout and outperforms baselines for the classification tasks. We also explore how these performance benefits depend on properties of the dataset.",0
"In recent years, graph networks have emerged as powerful tools for learning representations on graphs, including knowledge graphs and molecular structures. However, existing methods for training these models can suffer from high computational costs and limitations in scalability. To address these challenges, we propose a new method called Graph Networks with Spectral Message Passing (GNSMP).  Our approach builds upon recent advancements in spectral graph theory by leveraging the eigendecomposition of the graph Laplacian matrix. By doing so, our model achieves linear scaling complexity wrt the number of nodes and edges in the graph, making it applicable to large-scale datasets. We show that this efficient implementation provides both theoretical guarantees and empirical improvements over prior works. Furthermore, through extensive experiments across multiple benchmark datasets, we demonstrate that GNSMP outperforms state-of-the-art baselines in terms of accuracy and efficiency. Our findings suggest that spectral message passing may serve as a fundamental building block for future developments in graph representation learning.",1
"Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.",0
"In recent years, graph neural networks have emerged as powerful tools for learning on graphs and networks. However, one major challenge faced by these models is the large number of parameters they require, which can make training slow and computationally expensive. To address this issue, we propose a new method called Principal Neighbourhood Aggregation (PNA) that efficiently aggregates node features from their local neighborhoods into low-dimensional representations. Our approach uses a nonlinear operator based on singular value decomposition to capture salient structures in the network while reducing the dimensionality of the representation. We demonstrate through experiments on several benchmark datasets that our method achieves comparable performance to existing state-of-the-art methods while requiring fewer parameters and computational resources. Our work shows the potential of PNA to improve scalability and efficiency in graph neural networks without sacrificing accuracy.",1
"When compared to unimodal systems, multimodal biometric systems have several advantages, including lower error rate, higher accuracy, and larger population coverage. However, multimodal systems have an increased demand for integrity and privacy because they must store multiple biometric traits associated with each user. In this paper, we present a deep learning framework for feature-level fusion that generates a secure multimodal template from each user's face and iris biometrics. We integrate a deep hashing (binarization) technique into the fusion architecture to generate a robust binary multimodal shared latent representation. Further, we employ a hybrid secure architecture by combining cancelable biometrics with secure sketch techniques and integrate it with a deep hashing framework, which makes it computationally prohibitive to forge a combination of multiple biometrics that pass the authentication. The efficacy of the proposed approach is shown using a multimodal database of face and iris and it is observed that the matching performance is improved due to the fusion of multiple biometrics. Furthermore, the proposed approach also provides cancelability and unlinkability of the templates along with improved privacy of the biometric data. Additionally, we also test the proposed hashing function for an image retrieval application using a benchmark dataset. The main goal of this paper is to develop a method for integrating multimodal fusion, deep hashing, and biometric security, with an emphasis on structural data from modalities like face and iris. The proposed approach is in no way a general biometric security framework that can be applied to all biometric modalities, as further research is needed to extend the proposed framework to other unconstrained biometric modalities.",0
"Abstract: As biometric authentication becomes increasingly important for secure systems, multimodal biometric methods that rely on multiple types of biometric features (e.g., face recognition and fingerprint matching) have become more popular. One challenge facing these methods, however, is how to hash and compare biometric data while maintaining security against attacks such as replay attacks, where attackers attempt to use previously recorded legitimate data to gain unauthorized access. In this work, we propose deep hashing, a new method based on convolutional neural networks (CNNs), to effectively address this issue. We evaluate our approach using state-of-the-art datasets and demonstrate its effectiveness in protecting against both replay and other common attacks. Our results show significant improvements over traditional hashing approaches. By combining deep learning techniques with cryptographic principles, our method can provide improved performance without sacrificing security. This research offers promising solutions for securing real-world applications requiring robust and efficient multimodal biometric authentication.",1
"In this paper, we propose a novel hierarchical representation via message propagation (HRMP) method for robust model fitting, which simultaneously takes advantages of both the consensus analysis and the preference analysis to estimate the parameters of multiple model instances from data corrupted by outliers, for robust model fitting. Instead of analyzing the information of each data point or each model hypothesis independently, we formulate the consensus information and the preference information as a hierarchical representation to alleviate the sensitivity to gross outliers. Specifically, we firstly construct a hierarchical representation, which consists of a model hypothesis layer and a data point layer. The model hypothesis layer is used to remove insignificant model hypotheses and the data point layer is used to remove gross outliers. Then, based on the hierarchical representation, we propose an effective hierarchical message propagation (HMP) algorithm and an improved affinity propagation (IAP) algorithm to prune insignificant vertices and cluster the remaining data points, respectively. The proposed HRMP can not only accurately estimate the number and parameters of multiple model instances, but also handle multi-structural data contaminated with a large number of outliers. Experimental results on both synthetic data and real images show that the proposed HRMP significantly outperforms several state-of-the-art model fitting methods in terms of fitting accuracy and speed.",0
"Machine learning models are often trained on large datasets that contain noise or outliers which can affect their performance. In order to improve model fitting and reduce sensitivity to these data issues, hierarchical representation methods have been developed. These techniques involve representing high-dimensional data as a sequence of lower dimensional representations, where each level captures more semantic meaning than the previous one. However, existing hierarchical representation methods are limited by the need for expensive computational resources, manual feature engineering, and poor scalability across different domains. To address these limitations, we propose a new method called ""Message Passing Neural Networks"" (MPNN) that uses message propagation to learn a hierarchy of representations automatically from raw input data. Our approach outperforms state-of-the-art algorithms on several benchmark datasets while requiring significantly less computation time and no hand-engineered features. By leveraging message passing principles commonly used in graph neural networks, our method achieves robustness against noise and outliers without sacrificing model accuracy. Overall, MPNN presents a novel and effective solution for hierarchical representation and model fitting, opening up exciting opportunities for future research in machine learning and computer vision.",1
"Semi-supervised node classification on graph-structured data has many applications such as fraud detection, fake account and review detection, user's private attribute inference in social networks, and community detection. Various methods such as pairwise Markov Random Fields (pMRF) and graph neural networks were developed for semi-supervised node classification. pMRF is more efficient than graph neural networks. However, existing pMRF-based methods are less accurate than graph neural networks, due to a key limitation that they assume a heuristics-based constant edge potential for all edges. In this work, we aim to address the key limitation of existing pMRF-based methods. In particular, we propose to learn edge potentials for pMRF. Our evaluation results on various types of graph datasets show that our optimized pMRF-based method consistently outperforms existing graph neural networks in terms of both accuracy and efficiency. Our results highlight that previous work may have underestimated the power of pMRF for semi-supervised node classification.",0
"In recent years, semi-supervised node classification on graphs has gained increasing attention due to the rapid growth of data in complex systems such as social networks, biological networks, and knowledge bases. Two popular approaches for addressing this problem are Markov random fields (MRF) and graph neural networks (GNN). Although both MRFs and GNNs have been widely used in practice, their relative merits remain unclear, especially regarding efficiency and accuracy. This work compares MRFs and GNNs in terms of unlabelled data utilization, computational efficiency, scalability, robustness against noisy labels, flexibility and interpretability, and the ability to model high order interactions. Our results show that while both methods perform well overall, GNNs offer notable advantages over MRFs in most aspects, making them more suitable for many real-world applications. These findings contribute towards better understanding of how to design effective semi-supervised learning algorithms on graphs using different techniques.",1
"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",0
"This paper presents a novel method for learning local neighboring structure from point clouds that robustly captures shape representation under various transformations such as scaling, translation, rotation, reflection, and occlusion. Our approach uses graph convolutional neural networks (CNNs) on graphs constructed from unordered point sets to encode spatial features into low-dimensional representations. By leveraging explicit data augmentation during training time, we can capture the distribution of these transformations and learn a strong prior over shape spaces. Experiments show our model outperforms state-of-the-art methods across multiple benchmark datasets for tasks including registration, retrieval, and classification. Furthermore, thanks to the ability of encoding global shape structures, it demonstrates superior performance than traditional voxel-based techniques when handling objects with fine details like hairs and fibers. In summary, by incorporating local geometric relationships via graph CNNs and enforcing generalization through appropriate data augmentations, our framework learns a highly discriminative and robust model for accurate point cloud analysis.",1
"Spectral graph convolutional networks are generalizations of standard convolutional networks for graph-structured data using the Laplacian operator. A common misconception is the instability of spectral filters, i.e. the impossibility to transfer spectral filters between graphs of variable size and topology. This misbelief has limited the development of spectral networks for multi-graph tasks in favor of spatial graph networks. However, recent works have proved the stability of spectral filters under graph perturbation. Our work complements and emphasizes further the high quality of spectral transferability by benchmarking spectral graph networks on tasks involving graphs of different size and connectivity. Numerical experiments exhibit favorable performance on graph regression, graph classification, and node classification problems on two graph benchmarks. The implementation of our experiments is available on GitHub for reproducibility.",0
"In recent years, graph neural networks (GNNs) have been widely applied to many fields due to their effectiveness in processing irregular data structures such as graphs. One promising approach within GNNs is spectral methods, which utilize the eigendecomposition of the graph Laplacian to learn node representations that capture global properties of the graph structure. However, there has been limited research on evaluating the transferability of these learned representations across different datasets or tasks. This study aims to fill this gap by conducting an experimental analysis on the transferability of spectral graph network models. We train several popular spectral GNN architectures on multiple benchmark graph datasets and evaluate their performance on downstream node classification and link prediction tasks. Our results show that while spectral graph networks can achieve high accuracy on training tasks, they often struggle to generalize well to new tasks or domains. We provide insights into factors that may influence the transferability of these models, such as dataset size and model complexity. Overall, our findings contribute to a better understanding of the limitations and potential applications of spectral graph networks.",1
"Named Entity Recognition has been extensively investigated in many fields. However, the application of sensitive entity detection for production systems in financial institutions has not been well explored due to the lack of publicly available, labeled datasets. In this paper, we use internal and synthetic datasets to evaluate various methods of detecting NPI (Nonpublic Personally Identifiable) information commonly found within financial institutions, in both unstructured and structured data formats. Character-level neural network models including CNN, LSTM, BiLSTM-CRF, and CNN-CRF are investigated on two prediction tasks: (i) entity detection on multiple data formats, and (ii) column-wise entity prediction on tabular datasets. We compare these models with other standard approaches on both real and synthetic data, with respect to F1-score, precision, recall, and throughput. The real datasets include internal structured data and public email data with manually tagged labels. Our experimental results show that the CNN model is simple yet effective with respect to accuracy and throughput and thus, is the most suitable candidate model to be deployed in the production environment(s). Finally, we provide several lessons learned on data limitations, data labelling and the intrinsic overlap of data entities.",0
"In today's fast-paced world of finance, data security has never been more important. With cyber attacks becoming increasingly commonplace, financial institutions must take every precaution to protect their sensitive data from unauthorized access. One critical aspect of data security is identifying sensitive data in real time before it falls into the wrong hands. This paper presents high-throughput neural network models that can quickly detect and classify sensitive data within large datasets, allowing for proactive measures to prevent breaches. By training these models on vast amounts of labeled data, we achieve state-of-the art results in accuracy, speed, and scalability. Our approach employs an ensemble methodology, leveraging multiple models to increase robustness and reduce errors. We present detailed experimental evaluation of our system using benchmark datasets as well as case studies with several major banks. Overall, our work shows that deep learning techniques have significant potential in helping financial institutions improve their data protection strategies.",1
"Graph representation learning has many real-world applications, from super-resolution imaging, 3D computer vision to drug repurposing, protein classification, social networks analysis. An adequate representation of graph data is vital to the learning performance of a statistical or machine learning model for graph-structured data. In this paper, we propose a novel multiscale representation system for graph data, called decimated framelets, which form a localized tight frame on the graph. The decimated framelet system allows storage of the graph data representation on a coarse-grained chain and processes the graph data at multi scales where at each scale, the data is stored at a subgraph. Based on this, we then establish decimated G-framelet transforms for the decomposition and reconstruction of the graph data at multi resolutions via a constructive data-driven filter bank. The graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms. From this, we give a fast algorithm for the decimated G-framelet transforms, or FGT, that has linear computational complexity O(N) for a graph of size N. The theory of decimated framelets and FGT is verified with numerical examples for random graphs. The effectiveness is demonstrated by real-world applications, including multiresolution analysis for traffic network, and graph neural networks for graph classification tasks.",0
"This paper presents a novel decimation approach called Decimated Frames on Graphs (DG) that can quickly transform signals defined over irregular domains onto a lower dimensional representation for computational efficiency. By using graph theory concepts and framelets, DG provides a localized spectral analysis of irregular signal data such as images. With further mathematical tools, our method generalizes traditional sampling patterns like linear indices or radial frequencies on cartesian grids to irregular graphs and non-euclidean spaces. Experiments show that our fast algorithm significantly accelerates computation times while maintaining high accuracy compared to state-of-the-art methods.",1
"Most of the existing coastal flood Forecast and Early-Warning Systems do not model the flood, but instead, rely on the prediction of hydrodynamic conditions at the coast and on expert judgment. Recent scientific contributions are now capable to precisely model flood events, even in situations where wave overtopping plays a significant role. Such models are nevertheless costly-to-evaluate and surrogate ones need to be exploited for substantial computational savings. For the latter models, the hydro-meteorological forcing conditions (inputs) or flood events (outputs) are conveniently parametrised into scalar representations. However, they neglect the fact that inputs are actually functions (more precisely, time series), and that floods spatially propagate inland. Here, we introduce a multi-output Gaussian process model accounting for both criteria. On various examples, we test its versatility for both learning spatial maps and inferring unobserved ones. We demonstrate that efficient implementations are obtained by considering tensor-structured data and/or sparse-variational approximations. Finally, the proposed framework is applied on a coastal application aiming at predicting flood events. We conclude that accurate predictions are obtained in the order of minutes rather than the couples of days required by dedicated hydrodynamic simulators.",0
"Title: Multi-Output Gaussian Processes with Functional Data: A Study on Coastal Flood Hazard Assessment Abstract: Gaussian processes (GPs) have become increasingly popular as probabilistic models due to their flexibility and ability to capture complex relationships between inputs and outputs. In many real world applications, such as coastal flood hazard assessment, multiple outcomes need to be modeled simultaneously. This work proposes multi-output Gaussian processes (MOP GPs), which can jointly model correlated multiple outputs using functional data. MOP GPs allow us to exploit the similarities among different tasks by sharing some hyperparameters across tasks while still allowing each task to have unique characteristics. We demonstrate the applicability of our approach through a case study on coastal flood hazard assessment in Italy, where we consider three dependent variables related to coastal flooding. Our results show that MOP GPs perform well compared to single output GPs, providing improved predictions for the considered variables. Moreover, sensitivity analysis reveals the most influential factors affecting coastal flood risk along the Italian coastline. Overall, this research extends the scope of GP regression beyond traditional pointwise prediction problems and highlights the potential benefits of incorporating functional data into the framework.",1
"Class imbalance is a challenging issue in practical classification problems for deep learning models as well as for traditional models. Traditionally successful countermeasures such as synthetic over-sampling have had limited success with complex, structured data handled by deep learning models. In this work, we propose to use a Generative Adversarial Network (GAN) equipped with a generator network G, a discriminator network D and a classifier network C to remove the class-imbalance in visual data sets. The generator network is initialized with auto-encoder to make it stable. The discriminator D ensures that G adheres to class distribution of imbalanced class. In conventional methods, where Generator G competes with discriminator D in a min-max game, we propose to further add an additional classifier network to the original network. Now, the generator network tries to compete in a min-max game with Discriminator as well as the new classifier that we have introduced. An additional condition is enforced on generator network G to produce points in the convex hull of desired imbalanced class. Further the contention of adversarial game with classifier C, pushes conditional distribution learned by G towards the periphery of the respective class, compensating the problem of class imbalance. Experimental evidence shows that this initialization results in stable training of the network. We achieve state of the art performance on extreme visual classification task on the FashionMNIST, MNIST, SVHN, ExDark, MVTec Anomaly Detection dataset, Chest X-Ray dataset and others.",0
"Artificial intelligence (AI) algorithms have shown great potential in solving many real world problems by learning from large amounts of data. However, there exists a fundamental challenge in ensuring that these algorithms generalize well enough to perform on new unseen data sets as accurately as possible. One major source of error is due to the class imbalances present within datasets which can lead to degraded performance of machine learning models. This paper presents a novel framework capable of removing class imbalance based on uncertainty sampling. We apply our method, called polarity generative adversarial networks (Polarity-GAN), in combination with convolutional neural network architectures (CNNs). Our approach seeks to improve upon past state-of-the-art methods, like random oversampling/undersampling, by optimizing the selection of samples for training through uncertainty sampling. Experiments demonstrate Polarity-GAN produces superior results compared to traditional approaches across multiple domains including natural language processing tasks such as sentiment analysis, speech recognition and computer vision object detection. Ultimately, we believe this research has positive implications toward improving the robustness of artificial intelligence systems by addressing sources of error caused by class imbalanced data distribution.",1
"Deep Neural Network (DNN) models have vulnerabilities related to security concerns, with attackers usually employing complex hacking techniques to expose their structures. Data poisoning-enabled perturbation attacks are complex adversarial ones that inject false data into models. They negatively impact the learning process, with no benefit to deeper networks, as they degrade a model's accuracy and convergence rates. In this paper, we propose an attack-agnostic-based defense method for mitigating their influence. In it, a Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture which assists in neutralizing the effects of illegitimate perturbation samples in the feature space. To boost the robustness and trustworthiness of this method for correctly classifying attacked input samples, we regularize the hidden space of a trained model with a discriminative loss function called Polarized Contrastive Loss (PCL). It improves discrimination among samples in different classes and maintains the resemblance of those in the same class. Also, we integrate a DFL and PCL in a compact model for defending against data poisoning attacks. This method is trained and tested using the CIFAR-10 and MNIST datasets with data poisoning-enabled perturbation attacks, with the experimental results revealing its excellent performance compared with those of recent peer techniques.",0
"As deep neural networks (DNN) have become increasingly popular in various applications such as computer vision, natural language processing and speech recognition, they have been shown to be vulnerable to adversarial attacks. These attacks can cause DNNs to produce incorrect results by deliberately crafting inputs that result in misclassifications. While significant progress has been made on understanding the nature of these attacks and developing methods to mitigate them, there remains a gap in our ability to effectively combat adversarial examples for very deep networks. This paper focuses on addressing this challenge through the development of novel defense mechanisms tailored specifically for very deep networks. Our approach utilizes a combination of adversarial training techniques together with network architecture modifications to improve robustness against adversarial attacks. Our evaluation demonstrates the effectiveness of our proposed methodology in significantly reducing the impact of adversarial attacks on very deep networks while maintaining their overall performance on clean test data. By providing new insights into adversarial attack mitigation strategies for very deep networks, we aim to encourage further research in this area towards ensuring the reliability and security of deep learning systems in real-world applications.",1
"Self-supervised learning is currently gaining a lot of attention, as it allows neural networks to learn robust representations from large quantities of unlabeled data. Additionally, multi-task learning can further improve representation learning by training networks simultaneously on related tasks, leading to significant performance improvements. In this paper, we propose three novel self-supervised auxiliary tasks to train graph-based neural network models in a multi-task fashion. Since Graph Convolutional Networks are among the most promising approaches for capturing relationships among structured data points, we use them as a building block to achieve competitive results on standard semi-supervised graph classification tasks.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful models for processing complex data structures such as graphs and hypergraphs. These models leverage deep learning techniques like convolutional filters to capture local structure while maintaining global representations, making them ideal for tasks such as node classification on large scale real world datasets. However, training GNNs can still present challenges due to issues related to overfitting and poor generalization performance. To address these concerns, previous research has explored self supervision, where additional auxiliary tasks provide supervisory signals that enhance model robustness during training without requiring explicit annotations from human annotators. This work presents a methodology that incorporates multiple self-supervised auxiliary tasks into the training pipeline for graph-based neural network models. By using several small-scale self-supervised tasks, our approach reduces reliance on expensive task-specific annotated datasets while improving final model quality across several benchmark datasets commonly used to evaluate graph neural network models. We show through comprehensive experiments that our approach significantly outperforms prior art methods utilizing either single auxiliary or multi-task approaches across a wide range of performance metrics including accuracy, F1 score, precision, recall, ROC Area Under Curve (AUC), AUPR, and mean average precision (mAP). Additionally, we demonstrate how our framework generalizes well across domains by evaluating its efficacy across multiple datasets common in natural language processing (NLP) applications. Our contributions enable the wider adoption of GNNs for practitioners seeking high-quality results with limited labeled dat",1
"Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterised by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data, or assumes that elements of each data sample are drawn independently from some factorised probability distribution. These approaches are thus by construction blind to the correlation structure of real-world data sets and their impact on learning in neural networks. Here, we introduce a generative model for structured data sets that we call the hidden manifold model (HMM). The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a ""Gaussian Equivalence Property"" (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This permits us to analyse in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.",0
"Learning in deep artificial neural networks has been shown to depend critically on the network architecture, such as the number of layers and units per layer. However, recent studies have suggested that data manipulation techniques can also impact the performance of neural networks. In particular, techniques such as normalization and batch renormalization have been found to improve the stability and accuracy of training in certain settings. Motivated by these observations, we propose a new mathematical framework called the Hidden Manifold Model (HMM) which combines aspects of both data preprocessing and neural network architectures into a single unified formalism. This allows us to study the effect of different data structures on the learning process. Our analysis shows that properly designed manifolds can significantly improve generalization performance in feedforward and convolutional neural networks. We demonstrate our theory through extensive empirical evaluation on several benchmark datasets from computer vision and natural language processing domains. Overall, the proposed approach provides a new perspective on how to optimize the performance of modern machine learning systems.",1
"Many modern data analyses benefit from explicitly modeling dependence structure in data -- such as measurements across time or space, ordered words in a sentence, or genes in a genome. A gold standard evaluation technique is structured cross-validation (CV), which leaves out some data subset (such as data within a time interval or data in a geographic region) in each fold. But CV here can be prohibitively slow due to the need to re-run already-expensive learning algorithms many times. Previous work has shown approximate cross-validation (ACV) methods provide a fast and provably accurate alternative in the setting of empirical risk minimization. But this existing ACV work is restricted to simpler models by the assumptions that (i) data across CV folds are independent and (ii) an exact initial model fit is available. In structured data analyses, both these assumptions are often untrue. In the present work, we address (i) by extending ACV to CV schemes with dependence structure between the folds. To address (ii), we verify -- both theoretically and empirically -- that ACV quality deteriorates smoothly with noise in the initial fit. We demonstrate the accuracy and computational benefits of our proposed methods on a diverse set of real-world applications.",0
"This article presents a framework for performing cross-validation on structured models, such as those commonly used in machine learning applications. We describe how to approximate the true error rate by dividing the dataset into k folds, training the model k times, and using each set of predictions as validation data for the remaining sets. Our method ensures that all instances are eventually both trained and validated. By taking the average of these validation errors, we can estimate the expected test error of our model. Experiments show that our approximation tends to underestimate actual test error but still produces good results even at small sample sizes. Additionally, we demonstrate our approach works well across various types of structured prediction problems including linear regression and decision trees. Overall, our work provides a simple yet effective alternative to existing methods for approximating cross-validation on structured models.",1
"We consider the problem of learning a manifold from a teacher's demonstration. Extending existing approaches of learning from randomly sampled data points, we consider contexts where data may be chosen by a teacher. We analyze learning from teachers who can provide structured data such as individual examples (isolated data points) and demonstrations (sequences of points). Our analysis shows that for the purpose of teaching the topology of a manifold, demonstrations can yield remarkable decreases in the amount of data points required in comparison to teaching with randomly sampled points. We also discuss the implications of our analysis for learning in humans and machines.",0
"In our modern age, we often seek guidance on how to perform certain tasks from others who have expertise in those areas. In this paper, we consider situations where someone may learn from their teachers’ demonstration without explicit verbal instructions. This can happen through careful observation, trial-and-error, and feedback from the teacher. We propose that learning a task through imitation can involve building a “manifold,” which represents all possible variations of the task that could occur in different circumstances. Our model assumes access to a dataset containing multiple examples of the same underlying behavior executed by different experts (teachers) under diverse conditions. Using this data as input, our algorithm learns to predict likely next actions based on current context, generalizing across multiple demonstrations to capture the variability in human behavior. Through experiments on both real and synthetic datasets, we show that this method outperforms alternatives in terms of accuracy, robustness to missing demonstrations, interpretability, and the ability to generalize beyond seen scenarios. Finally, we discuss implications of these findings for understanding how humans might implicitly encode complex behaviors into mental representations and suggest future research directions motivated by connections to cognitive science literature.",1
"Graph-structured data exist in numerous applications in real life. As a state-of-the-art graph neural network, the graph convolutional network (GCN) plays an important role in processing graph-structured data. However, a recent study reported that GCNs are also vulnerable to adversarial attacks, which means that GCN models may suffer malicious attacks with unnoticeable modifications of the data. Among all the adversarial attacks on GCNs, there is a special kind of attack method called the universal adversarial attack, which generates a perturbation that can be applied to any sample and causes GCN models to output incorrect results. Although universal adversarial attacks in computer vision have been extensively researched, there are few research works on universal adversarial attacks on graph structured data. In this paper, we propose a targeted universal adversarial attack against GCNs. Our method employs a few nodes as the attack nodes. The attack capability of the attack nodes is enhanced through a small number of fake nodes connected to them. During an attack, any victim node will be misclassified by the GCN as the attack node class as long as it is linked to them. The experiments on three popular datasets show that the average attack success rate of the proposed attack on any victim node in the graph reaches 83% when using only 3 attack nodes and 6 fake nodes. We hope that our work will make the community aware of the threat of this type of attack and raise the attention given to its future defense.",0
"Graph Neural networks (GCNs) have recently emerged as powerful models for graph data, achieving state-of-the-art results in many application domains such as computer vision, natural language processing, and recommendation systems. However, their success has raised concerns regarding their robustness to adversarial attacks, which can pose significant threats to both individuals and society at large if exploited maliciously. In this paper, we present a targeted universal attack method that successfully deceives all 9 benchmark GCN architectures by generating near imperceptible perturbations that fool the classifiers into making incorrect predictions. Our approach leverages gradient estimation techniques from linear systems analysis to efficiently compute adversarial examples without relying on backpropagation. We empirically demonstrate our methods effectiveness across diverse datasets including Cora, Citeseer and Pubmed, showing high attack transferability among different architectures. The feasibility of launching targeted attacks against GCNs calls for further research on developing defense mechanisms capable of mitigating potential security risks posed by these vulnerabilities.",1
"Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e. by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.",0
"Title: ""Explaining Graph Neural Networks using Higher Order Connections"" Author: ****** Abstract: Graph neural networks (GNN) have gained popularity over recent years due their ability to process graph data such as social networks, molecular structures, and knowledge graphs. Despite their successes, understanding how GNNs make predictions remains challenging, particularly on large graphs where the number of neighbors per node may exceed thousands. In contrast, classical explainability techniques like visualizations based on local features often fail to capture global structure information that might be relevant for prediction tasks. We propose higher order explanations for GNN models by computing relevant walks -- paths through graphs that pass important edges connecting far away nodes which turn out to be crucial for explaining why certain model outputs occur. To evaluate our approach we apply it to several benchmark datasets commonly used to test GNNs, including citation graphs and protein interaction graphs. Our results demonstrate significant improvements over classical baseline approaches, especially for larger graphs where traditional methods struggle, offering new insights into the workings of state-of-the-art deep learning algorithms. Keywords: Explainable Artificial Intelligence; Deep Learning on Graph Data; Graph Convolutional Neural Networks; Walks Based Explanation Methods",1
"Trust and credibility in machine learning models is bolstered by the ability of a model to explain itsdecisions. While explainability of deep learning models is a well-known challenge, a further chal-lenge is clarity of the explanation itself, which must be interpreted by downstream users. Layer-wiseRelevance Propagation (LRP), an established explainability technique developed for deep models incomputer vision, provides intuitive human-readable heat maps of input images. We present the novelapplication of LRP for the first time with structured datasets using a deep neural network (1D-CNN),for Credit Card Fraud detection and Telecom Customer Churn prediction datasets. We show how LRPis more effective than traditional explainability concepts of Local Interpretable Model-agnostic Ex-planations (LIME) and Shapley Additive Explanations (SHAP) for explainability. This effectivenessis both local to a sample level and holistic over the whole testing set. We also discuss the significantcomputational time advantage of LRP (1-2s) over LIME (22s) and SHAP (108s), and thus its poten-tial for real time application scenarios. In addition, our validation of LRP has highlighted features forenhancing model performance, thus opening up a new area of research of using XAI as an approachfor feature subset selection",0
"This research paper presents a novel approach for explaining deep learning models that process structured data such as tables and graphs. Existing methods often rely on global explanations that fail to identify which parts of the input contribute most significantly to the model's prediction. In contrast, our method utilizes layer-wise relevance propagation (LRP) to assign attributions to individual features within each layer. We demonstrate the effectiveness of our approach by applying it to two important problems: table question answering and graph node classification. Our results show significant improvements over baseline models in terms of accuracy and interpretability. Furthermore, we provide insights into how our method can improve generalization capabilities and transfer learning. Overall, this work bridges the gap between accurate predictions and meaningful explanations for complex, high-dimensional data.",1
"Deep Learning architectures, albeit successful in most computer vision tasks, were designed for data with an underlying Euclidean structure, which is not usually fulfilled since pre-processed data may lie on a non-linear space. In this paper, we propose a geometry aware deep learning approach for skeleton-based action recognition. Skeleton sequences are first modeled as trajectories on Kendall's shape space and then mapped to the linear tangent space. The resulting structured data are then fed to a deep learning architecture, which includes a layer that optimizes over rigid and non rigid transformations of the 3D skeletons, followed by a CNN-LSTM network. The assessment on two large scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D 120, has proven that proposed approach outperforms existing geometric deep learning methods and is competitive with respect to recently published approaches.",0
"In this work, we present KShapeNet, a novel approach to skeletal action recognition using Riemannian networks on Kendall shape space. Our method leverages the power of machine learning techniques to accurately classify human actions from raw skeletal data. We propose a deep neural network architecture that operates directly on the Kendal shape space, allowing us to take full advantage of its rich mathematical structure. By treating shapes as high-dimensional objects, our model can capture subtle differences in human motion patterns that would otherwise go unnoticed by traditional approaches. Experimental results demonstrate the effectiveness of our method, achieving state-of-the-art performance on several benchmark datasets. Overall, KShapeNet represents a significant step forward in the field of skeletal action recognition and highlights the promise of Riemannian geometry in computer vision tasks.",1
"Time series are all around in real-world applications. However, unexpected accidents for example broken sensors or missing of the signals will cause missing values in time series, making the data hard to be utilized. It then does harm to the downstream applications such as traditional classification or regression, sequential data integration and forecasting tasks, thus raising the demand for data imputation. Currently, time series data imputation is a well-studied problem with different categories of methods. However, these works rarely take the temporal relations among the observations and treat the time series as normal structured data, losing the information from the time data. In recent, deep learning models have raised great attention. Time series methods based on deep learning have made progress with the usage of models like RNN, since it captures time information from data. In this paper, we mainly focus on time series imputation technique with deep learning methods, which recently made progress in this field. We will review and discuss their model architectures, their pros and cons as well as their effects to show the development of the time series imputation methods.",0
"Title: Time Series Data Imputation: A Survey on Deep Learning Approaches Abstract: Missing data is a common challenge faced by time series forecasting models. To address missing values in datasets, imputation techniques can be used to estimate missing values using available data. Recently, deep learning approaches have emerged as promising solutions for filling in missing data points. This survey reviews state-of-the-art methods that employ deep neural networks (DNNs) for time series data imputation and highlights their advantages over traditional statistical approaches. Firstly, we present several DNN architectures that have been proposed for time series data imputation such as recurrent neural networks (RNN), gated recurrent units (GRUs), Long Short-Term Memory Networks (LSTMs) and Echo State Networks (ESNs). We discuss how these architectures perform compared to traditional imputation methods such as mean, median, regression trees, random forest, autoregression, Kalman filter, ARIMA and SARIMA. Furthermore, we evaluate the performance of different loss functions employed during training such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) and Structured Output Difference Measure (SODM). Additionally, we examine various evaluation metrics utilized in benchmarking experiments including Mean Absolute Scaled Error (MASE), Relative Absolute Scaled Error (RASE), Index of Agreement (IOA) , Accuracy measures and Fill Percentage metrics. Finally, we analyze both qualitative and quantitative research findings from our review of literature studies performed on various real-world applications. We conclude that deep learning approaches hold great potential towards achieving improved accuracy in time series data imputation tasks, offering better estimation results than conventional statistics-based methodologies. Our survey provides insights into future direction",1
"Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the ""metagraph"" of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods",0
"Graph neural networks (GNNs) have been successful at handling graph data but they often suffer from high computational complexity due to their reliance on dense matrix operations. To overcome these limitations, we propose a scalable approach based on graph convolutional matrices that can handle heterogeneous graphs efficiently. Our method leverages low-rank approximations and tensorization techniques to reduce memory usage and computation time without sacrificing accuracy. We showcase the effectiveness of our model through experiments on several benchmark datasets including social network analysis and knowledge graph completion tasks. In summary, our work presents an efficient and scalable solution for processing large and complex heterogeneous graphs using GNNs.",1
"A model involving Gaussian processes (GPs) is introduced to simultaneously handle multi-task learning, clustering, and prediction for multiple functional data. This procedure acts as a model-based clustering method for functional data as well as a learning step for subsequent predictions for new tasks. The model is instantiated as a mixture of multi-task GPs with common mean processes. A variational EM algorithm is derived for dealing with the optimisation of the hyper-parameters along with the hyper-posteriors' estimation of latent variables and processes. We establish explicit formulas for integrating the mean processes and the latent clustering variables within a predictive distribution, accounting for uncertainty on both aspects. This distribution is defined as a mixture of cluster-specific GP predictions, which enhances the performances when dealing with group-structured data. The model handles irregular grid of observations and offers different hypotheses on the covariance structure for sharing additional information across tasks. The performances on both clustering and prediction tasks are assessed through various simulated scenarios and real datasets. The overall algorithm, called MagmaClust, is publicly available as an R package.",0
"This paper presents an approach to predicting outcomes for individual cases by modeling each case as a cluster and making predictions using a multi-task Gaussian process prior. We demonstrate that our method can produce more accurate predictions than standard Gaussian processes, especially when dealing with high-dimensional data sets or low sample sizes. Our approach uses an expectation propagation algorithm to approximate the posterior distribution over functions given noisy observations of those functions at training points. By integrating out unobserved functions in our approximation, we effectively pool knowledge across tasks, leading to better generalization performance on heldout test functions. Experimental results show that our method significantly improves predictive accuracy over existing alternatives. Additionally, our approach has several advantages over other kernel methods: it allows us to easily incorporate known structure into the prediction problem (e.g., physical laws); it naturally handles missing data; and it produces non-zero function estimates even when there is little data available. Cluster-specific predictions have emerged as an effective technique for enhancing the accuracy of outcome forecasting models, particularly when addressing complex datasets with limited samples. In this study, we propose a novel framework based on multi-task Gaussian processes, aimed at optimizing predictions by clustering similar instances together and building customized models tailored to each group. By leveraging expectation propagation algorithms to estimate the posterior distribution over functions, considering diverse scenarios simultaneously enables the sharing of insights among various task scenarios, thereby fostering improved overall accuracy compared to traditional approaches. Extensive experiments validate the superiority of our solution, emphasizing its potential benefits in real-world applications. Furthermore, the proposed method possesses distinct advantages over conventional kernel techniques, such as flexibility in accommodating structural assumptions, seamless handling of incomplete data, and facilitation of meaningful estimations even under scarce availability of evidence. These merits further enhance the versatility and effectiveness of our methodology in numerous scientific domains.",1
"Graph neural networks~(GNNs) apply deep learning techniques to graph-structured data and have achieved promising performance in graph representation learning. However, existing GNNs rely heavily on enough labels or well-designed negative samples. To address these issues, we propose a new self-supervised graph representation method: deep graph bootstrapping~(DGB). DGB consists of two neural networks: online and target networks, and the input of them are different augmented views of the initial graph. The online network is trained to predict the target network while the target network is updated with a slow-moving average of the online network, which means the online and target networks can learn from each other. As a result, the proposed DGB can learn graph representation without negative examples in an unsupervised manner. In addition, we summarize three kinds of augmentation methods for graph-structured data and apply them to the DGB. Experiments on the benchmark datasets show the DGB performs better than the current state-of-the-art methods and how the augmentation methods affect the performances.",0
"One promising direction of research in deep learning has been graph representation learning (GRL). GRL enables machine learning algorithms on graphs by first encoding nodes as embeddings that capture their relationships to other nodes in the graph. However, obtaining high quality node representations remains challenging due to limitations in supervision. In this work we propose self-supervised bootstrapping methods to learn accurate graph representations. Our approach takes advantage of weakly labeled data and carefully designed pretext tasks to guide the embedding process. We demonstrate the effectiveness of our method through extensive experimentation on real world datasets, showing consistent improvement over baselines without relying on large amounts of labeled training data. By integrating both semi-supervised learning and pretext task learning into one framework, we establish new state-of-the-art performance on benchmark datasets. This work shows great promise for future research in representation learning on graphs and provides valuable insights for practitioners working on natural language processing applications.",1
"Discrete structures play an important role in applications like program language modeling and software engineering. Current approaches to predicting complex structures typically consider autoregressive models for their tractability, with some sacrifice in flexibility. Energy-based models (EBMs) on the other hand offer a more flexible and thus more powerful approach to modeling such distributions, but require partition function estimation. In this paper we propose ALOE, a new algorithm for learning conditional and unconditional EBMs for discrete structured data, where parameter gradients are estimated using a learned sampler that mimics local search. We show that the energy function and sampler can be trained efficiently via a new variational form of power iteration, achieving a better trade-off between flexibility and tractability. Experimentally, we show that learning local search leads to significant improvements in challenging application domains. Most notably, we present an energy model guided fuzzer for software testing that achieves comparable performance to well engineered fuzzing engines like libfuzzer.",0
"Machine learning algorithms have made significant progress in recent years, but their application remains challenging due to their sensitivity to hyperparameters, difficulty in exploring high dimensions, and limitations in capturing complex relationships between variables. In order to address these issues, researchers have proposed new approaches such as energy-based models (EBMs), which have shown promise in modeling complex distributions and learning from data efficiently. However, current EBM methods suffer from drawbacks such as slow convergence and difficulties in estimating normalization constants. To overcome these limitations, we propose a novel algorithm called auxiliary-variable local exploration (ALE) that allows efficient optimization of discrete EBMs. Our method uses an adaptive procedure that automatically selects appropriate auxiliary variables based on local gradients, resulting in fast convergence rates while accurately approximating intractable normalization constants. We demonstrate the effectiveness of our approach through extensive experiments across multiple tasks, including image generation, density estimation, and machine translation. Overall, our work provides a promising direction for developing more powerful EBMs that can effectively handle high-dimensional datasets and complex relationships between variables.",1
"Graph-structured data are an integral part of many application domains, including chemoinformatics, computational biology, neuroimaging, and social network analysis. Over the last two decades, numerous graph kernels, i.e. kernel functions between graphs, have been proposed to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression settings. This manuscript provides a review of existing graph kernels, their applications, software plus data resources, and an empirical comparison of state-of-the-art graph kernels.",0
"This is a survey that summarizes and reviews graph kernels. Graph kernels have become popular methods in machine learning applications where structures as graphs can represent complex patterns better than flat vectorial data like matrices. Various types of graphs exist today (social networks, biological networks, web graphs), each representing some interesting but unique problems or datasets. Applying kernel functions on them enables working with high dimensionalities while taking advantage of their properties by designing efficient algorithms. In general kernels provide a mathematical toolbox that can efficiently compare objects using implicit features within spaces of very high dimensions which makes these objects “similar” if compared to others by any positive similarity metrics. On one hand, we know that there has been significant progress in developing new and different kinds of kernels for graphs since their introduction 2 decades ago. These efforts were motivated both by novel large scale real world applications from bioinformatics and chemo informatics, sensor networks etc., but also from more theoretical needs like improving scaling abilities of algorithms. We present many examples of successful applications and techniques along with references, and list several fundamental open challenges for future research directions. As always the challenge remains to balance accuracy of model predictions with tractability of training such models at larger scales involving ever growing sizes of input domains. Thus scalabilty issues remain important topics as well. Finally as deep leaning revolutionized neural network architectures recently it may inspire novel ideas that use similar principles to make kernel methods more powerful by capturing hierarchical relations. We aim towards comprehensive review, though there might still be some gaps we cannot bridge because of the vastness of relevant publications existing globally or the limited time horizon during which our work was conducted. Our paper provides extensive literature surveys of key technical points which should aid readers in further exploring all aspects of the topic. In summary this survey takes stock of the current state and discusses remaining grand challenegs related to applying kernels on graphs to make learning possible with increasing amounts of structured and unstructured data expected over next decade.",1
"In recent years, developing a speech understanding system that classifies a waveform to structured data, such as intents and slots, without first transcribing the speech to text has emerged as an interesting research problem. This work proposes such as system with an additional constraint of designing a system that has a small enough footprint to run on small micro-controllers and embedded systems with minimal latency. Given a streaming input speech signal, the proposed system can process it segment-by-segment without the need to have the entire stream at the moment of processing. The proposed system is evaluated on the publicly available Fluent Speech Commands dataset. Experiments show that the proposed system yields state-of-the-art performance with the advantage of low latency and a much smaller model when compared to other published works on the same task.",0
"This paper presents a novel approach to spoken language understanding that combines advances in natural language processing (NLP) with techniques from deep learning and computer vision. Our proposed system achieves state-of-the-art results without relying on automatic speech recognition (ASR), which can introduce significant latency and errors into the pipeline. Instead, we use pre-trained visual models to extract high-level representations directly from raw audio inputs. These features are then fed into a transformer network for contextualization and NLP tasks such as named entity recognition and sentiment analysis. We show that our method outperforms traditional ASR-based systems across multiple benchmark datasets while maintaining low latency. Overall, our work represents a major step forward towards realizing truly humanlike conversational agents that operate in real time.",1
"Optimal Transport is a theory that allows to define geometrical notions of distance between probability distributions and to find correspondences, relationships, between sets of points. Many machine learning applications are derived from this theory, at the frontier between mathematics and optimization. This thesis proposes to study the complex scenario in which the different data belong to incomparable spaces. In particular we address the following questions: how to define and apply Optimal Transport between graphs, between structured data? How can it be adapted when the data are varied and not embedded in the same metric space? This thesis proposes a set of Optimal Transport tools for these different cases. An important part is notably devoted to the study of the Gromov-Wasserstein distance whose properties allow to define interesting transport problems on incomparable spaces. More broadly, we analyze the mathematical properties of the various proposed tools, we establish algorithmic solutions to compute them and we study their applicability in numerous machine learning scenarii which cover, in particular, classification, simplification, partitioning of structured data, as well as heterogeneous domain adaptation.",0
"Abstract: In recent years, optimal transport theory has emerged as a powerful tool in mathematics and computer science, providing efficient algorithms for solving complex problems related to resource allocation, image processing, and machine learning. However, many real world applications involve incomparable spaces, where traditional methods fail due to the lack of a natural metric or measure. This paper contributes to the development of optimal transport on incomparable spaces by introducing new approaches based on partial orderings and set functions. We demonstrate how these methods can be used to solve difficult optimization problems in practice, such as planning routes through road networks that have no well-defined distance function. Our results generalize existing literature and offer promising directions for future research in this rapidly evolving field.",1
"Modern machine learning applications should be able to address the intrinsic challenges arising over inference on massive real-world datasets, including scalability and robustness to outliers. Despite the multiple benefits of Bayesian methods (such as uncertainty-aware predictions, incorporation of experts knowledge, and hierarchical modeling), the quality of classic Bayesian inference depends critically on whether observations conform with the assumed data generating model, which is impossible to guarantee in practice. In this work, we propose a variational inference method that, in a principled way, can simultaneously scale to large datasets, and robustify the inferred posterior with respect to the existence of outliers in the observed data. Reformulating Bayes theorem via the $\beta$-divergence, we posit a robustified pseudo-Bayesian posterior as the target of inference. Moreover, relying on the recent formulations of Riemannian coresets for scalable Bayesian inference, we propose a sparse variational approximation of the robustified posterior and an efficient stochastic black-box algorithm to construct it. Overall our method allows releasing cleansed data summaries that can be applied broadly in scenarios including structured data corruption. We illustrate the applicability of our approach in diverse simulated and real datasets, and various statistical models, including Gaussian mean inference, logistic and neural linear regression, demonstrating its superiority to existing Bayesian summarization methods in the presence of outliers.",0
"""The ability to accurately summarize large datasets has become increasingly important in today's data-driven world. Traditional methods often struggle when dealing with outliers that can distort the overall picture of the underlying distribution. To address this challenge, we propose a new method called ""$\beta$-cores"" which robustly handles outliers while providing reliable and efficient summary statistics. Our approach combines the advantages of both hierarchical clustering and density estimation techniques, allowing us to effectively identify clusters even under noisy conditions. We showcase the effectiveness of our framework through extensive experiments on synthetic and real datasets from various domains, demonstrating substantial improvement over state-of-the-art alternatives.""",1
"The ability to detect and count certain substructures in graphs is important for solving many tasks on graph-structured data, especially in the contexts of computational chemistry and biology as well as social network analysis. Inspired by this, we propose to study the expressive power of graph neural networks (GNNs) via their ability to count attributed graph substructures, extending recent works that examine their power in graph isomorphism testing and function approximation. We distinguish between two types of substructure counting: induced-subgraph-count and subgraph-count, and establish both positive and negative answers for popular GNN architectures. Specifically, we prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL) and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures consisting of 3 or more nodes, while they can perform subgraph-count of star-shaped substructures. As an intermediary step, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partly answering an open problem raised in Maron et al. (2019). We also prove positive results for k-WL and k-IGNs as well as negative results for k-WL with a finite number of iterations. We then conduct experiments that support the theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure counting and inspired by Murphy et al. (2019), we propose the Local Relational Pooling model and demonstrate that it is not only effective for substructure counting but also able to achieve competitive performance on molecular prediction tasks.",0
"This paper presents an analysis of the capability of graph neural networks (GNNs) to count substructures within large datasets. We investigate the challenges associated with this task and propose a novel approach that leverages GNNs to efficiently identify and quantify these structures. Our method outperforms traditional methods by utilizing the unique strengths of GNNs such as their ability to handle complex nonlinear relationships between entities within a dataset. In conclusion, our work demonstrates the effectiveness of GNNs in counting substructures and highlights the potential of this technology in enhancing computational efficiency across a variety of fields.",1
"We propose a flexible framework for clustering hypergraph-structured data based on recently proposed random walks utilizing edge-dependent vertex weights. When incorporating edge-dependent vertex weights (EDVW), a weight is associated with each vertex-hyperedge pair, yielding a weighted incidence matrix of the hypergraph. Such weightings have been utilized in term-document representations of text data sets. We explain how random walks with EDVW serve to construct different hypergraph Laplacian matrices, and then develop a suite of clustering methods that use these incidence matrices and Laplacians for hypergraph clustering. Using several data sets from real-life applications, we compare the performance of these clustering algorithms experimentally against a variety of existing hypergraph clustering methods. We show that the proposed methods produce higher-quality clusters and conclude by highlighting avenues for future work.",0
"Title: Hypergraph Random Walks, Laplacians, and Clustering  Abstract: This work presents a novel approach to clustering problems using hypergraphs, random walks, and laplacians. In traditional graph theory, graphs represent relationships between objects as nodes connected by edges. However, many real-world applications require modeling interactions among groups of entities rather than just pairwise connections, which can be better represented using hypergraphs. We develop two methods for generating uniform random walks on hypergraphs and define different types of laplacians based on these random walks. Our main contribution lies in exploiting the properties of these laplacians for efficient clustering. To evaluate our methodology, we perform extensive experiments across several domains such as image processing, social network analysis, bioinformatics, and text mining. Results demonstrate significant improvements over existing approaches in terms of accuracy, speed, and robustness. By providing new tools to analyze complex systems with non-binary relations, our study paves the way for advancements in areas that heavily rely on understanding multi-relational data structures like web search engines, recommender systems, and fraud detection.",1
"The recent introduction of Graph Neural Networks (GNNs) and their growing popularity in the past few years has enabled the application of deep learning algorithms to non-Euclidean, graph-structured data. GNNs have achieved state-of-the-art results across an impressive array of graph-based machine learning problems. Nevertheless, despite their rapid pace of development, much of the work on GNNs has focused on graph classification and embedding techniques, largely ignoring regression tasks over graph data. In this paper, we develop a Graph Mixture Density Network (GraphMDN), which combines graph neural networks with mixture density network (MDN) outputs. By combining these techniques, GraphMDNs have the advantage of naturally being able to incorporate graph structured information into a neural architecture, as well as the ability to model multi-modal regression targets. As such, GraphMDNs are designed to excel on regression tasks wherein the data are graph structured, and target statistics are better represented by mixtures of densities rather than singular values (so-called ``inverse problems""). To demonstrate this, we extend an existing GNN architecture known as Semantic GCN (SemGCN) to a GraphMDN structure, and show results from the Human3.6M pose estimation task. The extended model consistently outperforms both GCN and MDN architectures on their own, with a comparable number of parameters.",0
"GraphMDN: A novel approach that utilizes graph theory and deep neural networks to tackle challenging inverse problems GraphMDN combines principles from two powerful fields – graph theory and deep learning – to address difficult inverse problems encountered in computer vision, signal processing, and other domains where the output data is corrupted by noise or missing information. Inspired by traditional methods based on regularization theory, our method leverages graphs and graph Laplacians to encode structural priors into the optimization process. By embedding these prior knowledge into a flexible framework compatible with modern deep architectures, we can learn complex mappings without assuming any specific structure like sparsity, smoothness, or low rank constraints commonly used in existing techniques. Our approach outperforms state-of-the-art models across a variety of benchmark datasets, demonstrating remarkable robustness against varying degrees of noise contamination, degraded input quality, and limited training samples. These findings showcase the advantages of incorporating graph structured information into deep architectures, promising new perspectives for researchers working in data recovery, image denoising, and many others areas requiring effective inference algorithms. Keywords: inverse problem, graph, Laplacian matrix, regularization, deep neural network, signal processing, computer vision Inverse problems arise frequently in numerous applications such as computer vision, signal processing, and medical imaging [1]. They deal with reconstructing the original input data given partial observations or measurements impaired by noise or data loss. This challenge has been extensively studied since early work in the field [2], yielding effective solutions based on various mathematical frameworks [3]. Among them, compressed sensing (CS) and total variation (TV)-based approaches [4] have gained significant popularity due to their simplicity and good performance. To overcome drawbacks related to handcrafted features and limitations intrinsic to convex optimiza",1
"Graph Neural Networks achieve remarkable results on problems with structured data but come as black-box predictors. Transferring existing explanation techniques, such as occlusion, fails as even removing a single node or edge can lead to drastic changes in the graph. The resulting graphs can differ from all training examples, causing model confusion and wrong explanations. Thus, we argue that explicability must use graphs compliant with the distribution underlying the training data. We coin this property Distribution Compliant Explanation (DCE) and present a novel Contrastive GNN Explanation (CoGE) technique following this paradigm. An experimental study supports the efficacy of CoGE.",0
"This paper presents the first application of contrastive graph neural networks (CGNNs) on image generation tasks. We demonstrate that CGNNs can generate high quality images while providing interpretable explanations through attention maps. In addition, we show that by imposing constraints on the network architecture and training process, we can significantly improve both performance and interpretability of the model. Our results provide insight into how these models make predictions and suggest promising directions for future work in the field of generative modelling using deep learning techniques.  This study provides new insights into the role of contrastive graph neural networks (CGNNs) in improving image generation tasks. By leveraging this innovative approach, researchers were able to achieve better performance while simultaneously ensuring transparency and interpretability of their models. The implications of this development could pave the way for enhanced applications of artificial intelligence across multiple domains. Overall, the findings contribute valuable knowledge towards understanding generative modeling methods employing deep learning technologies.",1
"Graph, as an important data representation, is ubiquitous in many real world applications ranging from social network analysis to biology. How to correctly and effectively learn and extract information from graph is essential for a large number of machine learning tasks. Graph embedding is a way to transform and encode the data structure in high dimensional and non-Euclidean feature space to a low dimensional and structural space, which is easily exploited by other machine learning algorithms. We have witnessed a huge surge of such embedding methods, from statistical approaches to recent deep learning methods such as the graph convolutional networks (GCN). Deep learning approaches usually outperform the traditional methods in most graph learning benchmarks by building an end-to-end learning framework to optimize the loss function directly. However, most of the existing GCN methods can only perform convolution operations with node features, while ignoring the handy information in edge features, such as relations in knowledge graphs. To address this problem, we present CensNet, Convolution with Edge-Node Switching graph neural network, for learning tasks in graph-structured data with both node and edge features. CensNet is a general graph embedding framework, which embeds both nodes and edges to a latent feature space. By using line graph of the original undirected graph, the role of nodes and edges are switched, and two novel graph convolution operations are proposed for feature propagation. Experimental results on real-world academic citation networks and quantum chemistry graphs show that our approach achieves or matches the state-of-the-art performance in four graph learning tasks, including semi-supervised node classification, multi-task graph classification, graph regression, and link prediction.",0
"This paper presents a novel approach to graph neural networks (GNNs) that incorporates both nodes and edges as input features using co-embeddings. Traditional GNN methods primarily use graphs composed of node attributes and edge features, but these approaches often neglect the importance of capturing both types of relationships within a single model. In contrast, our method uses jointly embedded representations of nodes and edges, allowing the network to learn more expressive latent embeddings and improve performance on tasks such as node classification and link prediction. We evaluate our proposed method across several benchmark datasets and demonstrate improved results compared to state-of-the-art GNN models. Overall, our work highlights the potential benefits of integrating both node and edge information into GNN architectures.",1
"Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-the-art graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features.",0
"This paper presents the Graph Information Bottleneck (GIB), a novel methodology that addresses the challenges involved in extracting knowledge from large graph datasets. We demonstrate how GIB can effectively overcome these limitations by providing a more accurate representation of graph data while significantly reducing computational complexity. Our approach utilizes deep learning techniques to learn a compact and informative latent space for graphs, allowing us to capture essential characteristics without the need for explicit feature engineering. Results show that our proposed model outperforms traditional methods across several benchmarks in terms of both efficiency and effectiveness. In summary, the Graph Information Bottleneck offers a scalable solution for processing large graph datasets while maintaining high accuracy and interpretability.",1
"Graph neural networks (GNNs) have been widely used to analyze the graph-structured data in various application domains, e.g., social networks, molecular biology, and anomaly detection. With great power, the GNN models, usually as valuable Intellectual Properties of their owners, also become attractive targets of the attacker. Recent studies show that machine learning models are facing a severe threat called Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by the attacker pretending as a client. Unfortunately, existing works focus on the models trained on the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, we explore and develop model extraction attacks against GNN models. Given only black-box access to a target GNN model, the attacker aims to reconstruct a duplicated one via several nodes he obtained (called attacker nodes). We first systematically formalise the threat modeling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbor connectives of the attacker nodes. Then we present the detailed methods which utilize the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., more than 89% inputs in the target domain have the same output predictions as the victim model.",0
"Advances in deep learning have led to significant progress across many fields from computer vision to natural language processing. One crucial component of such advances has been graph neural networks (GNNs). GNNs provide powerful techniques that enable understanding of complex non-Euclidean domains where nodes carry out multiple types of data, edges connecting nodes represent different relationships among them, and each node may have an associated feature vector whose size depends on the complexity of the domain. Thereby, they capture essential properties intrinsic to these data structures while allowing the user to focus solely on model development without worrying about the underlying structure or type of input data. Unfortunately, their success motivates attackers to reverse engineer trained models so as to replicate their accuracy. Such efforts aim at stealing valuable intellectual property behind machine learning products either by copying or adapting their architecture to new tasks. Therefore there is a pressing need to examine the problem more closely and gain insights into ways for mitigating the threat of extraction attacks on GNN architectures. This work presents a taxonomy of existing works based on different dimensions that distinguish between approaches followed by adversaries. We demonstrate both white box and black box scenarios which lead us to discuss three broad categories of realisations. Finally we describe possible defensive strategies and outline future research directions towards mitigating the threat of extracting such models.",1
"Graph embeddings are a ubiquitous tool for machine learning tasks, such as node classification and link prediction, on graph-structured data. However, computing the embeddings for large-scale graphs is prohibitively inefficient even if we are interested only in a small subset of relevant vertices. To address this, we present an efficient graph coarsening approach, based on Schur complements, for computing the embedding of the relevant vertices. We prove that these embeddings are preserved exactly by the Schur complement graph that is obtained via Gaussian elimination on the non-relevant vertices. As computing Schur complements is expensive, we give a nearly-linear time algorithm that generates a coarsened graph on the relevant vertices that provably matches the Schur complement in expectation in each iteration. Our experiments involving prediction tasks on graphs demonstrate that computing embeddings on the coarsened graph, rather than the entire graph, leads to significant time savings without sacrificing accuracy.",0
"""Graph embedding techniques have become increasingly important in modern machine learning due to their ability to capture meaningful features from complex graphs. However, many graph embedding methods can suffer from high computational cost and poor scalability as the size of the input graphs grows. In order to address these limitations, we propose a novel approach called 'coarse-grained graph embedding', which leverages powerful geometric approximations and sparse representations to significantly reduce computation time while still preserving accuracy. We demonstrate that our method outperforms state-of-the-art alternatives on several benchmark datasets and real-world applications, such as protein structure prediction and drug discovery.""",1
"Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed multi-layer network architecture is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. The effectiveness of our model is demonstrated through extensive experiments on five benchmark datasets, achieving better or comparable anomaly detection results against strong baseline methods.",0
"In recent years, deep learning has emerged as a powerful tool for anomaly detection, particularly in computer vision tasks. One important component of these models is convolutional neural networks (CNNs), which have shown great success in processing images and extracting features that can identify unusual patterns. However, traditional CNN architectures suffer from some limitations such as irregular topology and high sensitivity to noise. To address these issues, we propose graph fairing convolutional network (GFCN) architectures that utilize graph operations to regularize and smooth the convolution process. GFCNs improve upon previous methods by preserving the advantages of regularization while reducing the loss of information through nonlinear projection. Our experiments show promising results on three benchmark datasets across multiple domains, demonstrating the effectiveness and versatility of our proposed approach.",1
"This paper presents and characterizes an Open Application Repository for Federated Learning (OARF), a benchmark suite for federated machine learning systems. Previously available benchmarks for federated learning have focused mainly on synthetic datasets and use a very limited number of applications. OARF includes different data partitioning methods (horizontal, vertical and hybrid) as well as emerging applications in image, text and structured data, which represent different scenarios in federated learning. Our characterization shows that the benchmark suite is diverse in data size, distribution, feature distribution and learning task complexity. We have developed reference implementations, and evaluated the important aspects of federated learning, including model accuracy, communication cost, differential privacy, secure multiparty computation and vertical federated learning.",0
"Federated learning is growing rapidly, but there remains little consensus on evaluation benchmarks that capture important aspects of this technology’s performance—especially given federated systems typically involve tradeoffs across multiple dimensions including accuracy, resource utilization, scalability, and privacy-related metrics (such as quantifiable levels of leakage). This paper addresses these challenging tradeoffs by introducing the Open Academic Research Federation (OARF) benchmark suite, which encompasses several datasets spanning different domains (finance, sentiment analysis, biological science), model architectures (including both deep neural networks and random forests), batch sizes, communication budgets, data heterogeneity settings, and other relevant factors such as serverless computing platforms. We conduct extensive experiments using real users providing their own local models to train, exploring how combinations of these dataset/architectural features impact global test accuracy, wall-clock training time, required server resources, and actual levels of user engagement and task completion rates. Using a set of interactive dashboards available online, we aim to encourage further research into characterizing the tradeoffs involved in deploying federated systems at scale while facilitating conversations between stakeholders seeking guidance on setting up their own OARF instances and contributing to our shared knowledge base on best practices in evaluating these complex distributed machine learning pipelines. Key findings include identifying key scalability thresholds beyond which system administrators may struggle to efficiently manage infrastructure demands; illuminating intricate dependencies among dataset characteristics and various forms of privacy leakage inherent within popular aggregation schemes; underscoring the importance o",1
"Graph neural network models have been extensively used to learn node representations for graph structured data in an end-to-end setting. These models often rely on localized first order approximations of spectral graph convolutions and hence are unable to capture higher-order relational information between nodes. Probabilistic Graphical Models form another class of models that provide rich flexibility in incorporating such relational information but are limited by inefficient approximate inference algorithms at higher order. In this paper, we propose to combine these approaches to learn better node and graph representations. First, we derive an efficient approximate sum-product loopy belief propagation inference algorithm for higher-order PGMs. We then embed the message passing updates into a neural network to provide the inductive bias of the inference algorithm in end-to-end learning. This gives us a model that is flexible enough to accommodate domain knowledge while maintaining the computational advantage. We further propose methods for constructing higher-order factors that are conditioned on node and edge features and share parameters wherever necessary. Our experimental evaluation shows that our model indeed captures higher-order information, substantially outperforming state-of-the-art $k$-order graph neural networks in molecular datasets.",0
"This paper presents a new method for efficiently propagating higher-order beliefs in neural networks. We show that by using a modified version of traditional belief propagation algorithms, we can achieve significantly faster convergence times while maintaining accurate results. Our approach leverages recent advances in deep learning to create a more powerful and flexible inference engine, allowing us to capture complex relationships between variables and distill large amounts of data into compact representations. In addition, we introduce several novel techniques to improve robustness and reduce the risk of overfitting. Through experimental evaluation on a range of challenging tasks, we demonstrate the effectiveness of our method compared to state-of-the-art alternatives. Overall, our work represents a significant step forward in enabling efficient probabilistic inference at scale.",1
"Fairness in machine learning is crucial when individuals are subject to automated decisions made by models in high-stake domains. Organizations that employ these models may also need to satisfy regulations that promote responsible and ethical A.I. While fairness metrics relying on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias, fairness in terms of the equalized ability to achieve recourse for different protected attribute groups has been relatively unexplored. We present a novel formulation for training neural networks that considers the distance of data points to the decision boundary such that the new objective: (1) reduces the average distance to the decision boundary between two groups for individuals subject to a negative outcome in each group, i.e. the network is more fair with respect to the ability to obtain recourse, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that training with this loss yields more fair and robust neural networks with similar accuracies to models trained without it. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse capabilities across groups are considered to train fairer neural networks, and a relation between error rates based fairness and recourse based fairness is investigated.",0
"Abstract: In recent years, deep learning has made significant progress towards solving complex real-world problems by utilizing large amounts of data and computational resources. One such domain that has benefited from these advancements is structured data prediction, where neural networks predict structured outputs based on input data. Despite their successes, deep neural networks have been criticized for overfitting and lacking robustness under distributional shift, which can result in degraded performance or even failures in applications that rely on them. In response to these concerns, we present a novel approach called FaiR-N, short for Fair and Robust Neural Networks for Structured Data. Our method is designed to address both overfitting and sensitivity to distribution shifts by utilizing fairness constraints derived from statistical metrics and adversarial training techniques to regularize model weights and increase generalization ability. Experimental results demonstrate consistent improvements across diverse benchmark datasets, further validating our contributions towards producing more reliable and trustworthy models. Overall, this work represents an important step towards developing high-quality, ethical AI systems capable of meeting user expectations while ensuring greater fairness and accountability.",1
"Exploiting the rapid advances in probabilistic inference, in particular variational Bayes and variational autoencoders (VAEs), for anomaly detection (AD) tasks remains an open research question. Previous works argued that training VAE models only with inliers is insufficient and the framework should be significantly modified in order to discriminate the anomalous instances. In this work, we exploit the deep conditional variational autoencoder (CVAE) and we define an original loss function together with a metric that targets hierarchically structured data AD. Our motivating application is a real world problem: monitoring the trigger system which is a basic component of many particle physics experiments at the CERN Large Hadron Collider (LHC). In the experiments we show the superior performance of this method for classical machine learning (ML) benchmarks and for our application.",0
"This article proposes a novel method called conditional variational autoencoder anomaly detection (CVAAD) that detects outliers using the reconstruction error of variational autoencoders (VAEs). Previous works often employ VAEs to learn representations as latent variables and then measure the anomaly score based on how far away these latent vectors are from the training data distribution. However, previous methods ignore the information on the input space which leads to suboptimal results. In contrast, CVAAD employs two encoders: one maps inputs into latent representations while another maps raw pixel values into compressed representation in latent space conditioned on original input features. By leveraging both high-level semantic features extracted by decoder and low-level details preserved by reconstructor, we can effectively capture complex structures and patterns of normal distributions. Our experiments show improved performance over state-of-the-art baselines under several evaluation metrics across multiple datasets such as MNIST, Fashion-MNIST, SVHN and CIFAR-10, demonstrating the effectiveness of our proposed approach.",1
"Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",0
"In recent years, hypergraphs have emerged as powerful tools for modeling complex relationships among entities in various domains such as computer vision, natural language processing, and knowledge graphs. Unlike traditional graph convolutional networks (GCN), which represent each node as a single entity and their connectivity as edges, hypergraphs can capture richer structures by representing nodes as sets and edges as relations between them. These structures make hypergraphs well suited for capturing dependencies and interactions among multiple entities within different modalities, enabling state-of-the-art performance on tasks that involve understanding these interdependencies. However, designing effective algorithms for learning representations from hypergraph data remains challenging due to the intrinsic complexity introduced by higher-order relationships. This paper presents two novel frameworks: hypergraph convolution and hypergraph attention mechanisms, capable of accurately extracting features from heterogeneous hypergraphs. Our experiments demonstrate significant improvements over GCN methods across diverse applications. Furthermore, we showcase how our models achieve competitive results with fewer parameters while generalizing better out-of-sample. Overall, the presented work provides foundational research towards developing more efficient deep learning architectures, particularly when dealing with intricate multimodal hypergraphs. While future investigations must examine other aspects of model development, validation, and deployment, our contributions constitute important steps forward in addressing real-world problems characterized by high-dimensionality, noise, uncertainty, imbalance, skewness, nonlinearity, and sparsity.",1
"Graphs are the most ubiquitous form of structured data representation used in machine learning. They model, however, only pairwise relations between nodes and are not designed for encoding the higher-order relations found in many real-world datasets. To model such complex relations, hypergraphs have proven to be a natural representation. Learning the node representations in a hypergraph is more complex than in a graph as it involves information propagation at two levels: within every hyperedge and across the hyperedges. Most current approaches first transform a hypergraph structure to a graph for use in existing geometric deep learning algorithms. This transformation leads to information loss, and sub-optimal exploitation of the hypergraph's expressive power. We present HyperSAGE, a novel hypergraph learning framework that uses a two-level neural message passing strategy to accurately and efficiently propagate information through hypergraphs. The flexible design of HyperSAGE facilitates different ways of aggregating neighborhood information. Unlike the majority of related work which is transductive, our approach, inspired by the popular GraphSAGE method, is inductive. Thus, it can also be used on previously unseen nodes, facilitating deployment in problems such as evolving or partially observed hypergraphs. Through extensive experimentation, we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets. We also demonstrate that the higher expressive power of HyperSAGE makes it more stable in learning node representations as compared to the alternatives.",0
"Artificial intelligence (AI) has recently benefited from advancements in deep learning methods applied to graph-structured data. However, real-world problems often involve complex relationships that cannot be captured by mere graphs. In our work we tackle this issue by extending recent state-of-the-art techniques for inductive representation learning on hypergraphs - structures consisting of entities and their interactions encoded as edges connected among k vertices each representing a different aspect of the interaction called dimensions. We introduce a novel framework named HyperSAGE for generalization and efficient computation of similarities between objects described via hyperedges over latent feature spaces. To showcase its effectiveness, we present experimental evidence for link prediction tasks demonstrating outperformance against several competing approaches across both synthetic benchmark datasets and two large real world knowledge bases YAGO2 and DBLP. Notably, we attain results on DBLP superior to previously published ones using other machine learning models. Additionally, we evaluate HyperSAGE in the context of semi-supervised learning achieving improvements comparable to those obtained by baseline supervised training alone. Our findings have implications on the scalability and expressivity of AI models designed for real world applications built upon structured representations. The code used to conduct our experiments can be accessed publicly at https://github.com/vladymir69/Hype rSAGE under permissible license terms. If you require additional details please don't hesitate to contact me directly. Keywords: induced substructure; hypergraph; relational machine learning; knowledge base completion.",1
"Graph neural networks have become an important tool for modeling structured data. In many real-world systems, intricate hidden information may exist, e.g., heterogeneity in nodes/edges, static node/edge attributes, and spatiotemporal node/edge features. However, most existing methods only take part of the information into consideration. In this paper, we present the Co-evolved Meta Graph Neural Network (CoMGNN), which applies meta graph attention to heterogeneous graphs with co-evolution of node and edge states. We further propose a spatiotemporal adaption of CoMGNN (ST-CoMGNN) for modeling spatiotemporal patterns on nodes and edges. We conduct experiments on two large-scale real-world datasets. Experimental results show that our models significantly outperform the state-of-the-art methods, demonstrating the effectiveness of encoding diverse information from different aspects.",0
"This paper presents a new method for processing heterogeneous graphs using meta graph attention (MGA). MGA allows nodes in the graph to receive different amounts of attention from one another based on their importance, which can improve the accuracy of downstream applications such as node classification and graph generation. The proposed method extends previous work by incorporating both node and edge evolution into the computation process. Edge evolution captures how the connections between nodes change over time, while node evolution accounts for changes in individual nodes within the same graph. By combining these two types of evolution, we can more accurately model dynamic relationships within the graph. In addition to enhanced evolution modeling, our approach uses MGA at each level of a hierarchical decomposition of the original graph. This hierarchy promotes efficient calculation and scalability as the number of edges grows very large. Experiments demonstrate that our method outperforms baseline methods across multiple domains, including social network analysis, molecular biology, and web page analysis. Our framework enables flexible integration of task-specific features and models, allowing practitioners to benefit from graph attentions without sacrificing customization needs. Overall, the contributions of this research lead to improved performance and increased usability for graph attention computations.",1
"Understanding customer behavior is fundamental for many use-cases in industry, especially in accelerated growth areas such as fin-tech and e-commerce. Structured data are often expensive, time-consuming and inadequate to analyze and study complex customer behaviors. In this paper, we propose a multi-graph embedding approach for creating a non-linear representation of customers in order to have a better knowledge of their characteristics without having any prior information about their financial status or their interests. By applying the current method we are able to predict users' future behavior with a reasonably high accuracy only by having the information of their friendship network. Potential applications include recommendation systems and credit risk forecasting.",0
"""Modeling customer behavior can provide valuable insights into consumer preferences and purchasing patterns, which can lead to better decision making for businesses. However, traditional methods have limitations in capturing complex relationships between customers and their interactions across different channels such as social media, transactions, online browsing, etc. In this study, we propose a multi-graph embedding approach that models these diverse interaction patterns using weighted edges, capturing both positive (friendly) and negative (unfriendly) behaviors. We validate our method on real-world data from a large e-commerce platform and show significant improvements over existing approaches in predicting key user actions and purchase outcomes. Our results demonstrate that friendship connections play a critical role in shaping customer behavior, highlighting the importance of social factors in understanding consumer decisions.""",1
"Graph Neural Networks (GNNs) have risen to prominence in learning representations for graph structured data. A single GNN layer typically consists of a feature transformation and a feature aggregation operation. The former normally uses feed-forward networks to transform features, while the latter aggregates the transformed features over the graph. Numerous recent works have proposed GNN models with different designs in the aggregation operation. In this work, we establish mathematically that the aggregation processes in a group of representative GNN models including GCN, GAT, PPNP, and APPNP can be regarded as (approximately) solving a graph denoising problem with a smoothness assumption. Such a unified view across GNNs not only provides a new perspective to understand a variety of aggregation operations but also enables us to develop a unified graph neural network framework UGNN. To demonstrate its promising potential, we instantiate a novel GNN model, ADA-UGNN, derived from UGNN, to handle graphs with adaptive smoothness across nodes. Comprehensive experiments show the effectiveness of ADA-UGNN.",0
"Graph neural networks have emerged as powerful tools for processing graph data. They can learn useful representations from graphs by propagating signals along their edges. In recent years, there has been growing interest in using these models for graph signal denoising tasks. This article presents a unified view on graph neural network approaches to graph signal denoising problems, highlighting common themes across different methods and providing insights into potential future research directions. We first discuss the core components of graph signal denoising techniques based on graph neural networks, including data preprocessing, noise modeling, optimization formulations, and architecture design. Then we survey several representative methods that fall under this umbrella term, exploring their strengths and weaknesses, open questions, and connections to related work in computer vision, natural language processing, and other domains. Finally, we suggest promising areas for further investigation, such as incorporating domain knowledge into graph neural networks, developing new architectures tailored for specific problem classes, and analyzing their theoretical properties in depth. By taking a broad perspective on graph neural network-based graph signal denoising, we hope to encourage collaboration among researchers working in different communities and foster deeper understanding of how these methods operate and can be improved.",1
"The Large Scale Visual Recognition Challenge based on the well-known Imagenet dataset catalyzed an intense flurry of progress in computer vision. Benchmark tasks have propelled other sub-fields of machine learning forward at an equally impressive pace, but in healthcare it has primarily been image processing tasks, such as in dermatology and radiology, that have experienced similar benchmark-driven progress. In the present study, we performed a comprehensive review of benchmarks in medical machine learning for structured data, identifying one based on the Medical Information Mart for Intensive Care (MIMIC-III) that allows the first direct comparison of predictive performance and thus the evaluation of progress on four clinical prediction tasks: mortality, length of stay, phenotyping, and patient decompensation. We find that little meaningful progress has been made over a 3 year period on these tasks, despite significant community engagement. Through our meta-analysis, we find that the performance of deep recurrent models is only superior to logistic regression on certain tasks. We conclude with a synthesis of these results, possible explanations, and a list of desirable qualities for future benchmarks in medical machine learning.",0
"This paper presents an evaluation of recent advances in machine learning applied to longitudinal electronic healthcare data (EHD). With the increasing availability of large amounts of EHD generated by modern medical technologies such as electronic health records (EHRs), there has been significant interest in using machine learning algorithms to extract valuable insights from these datasets. In particular, applying machine learning to longitudinal EHD allows researchers to model changes over time, which can provide new opportunities for understanding patient outcomes and improving care. We reviewed current literature on machine learning applications for longitudinal EHD and identified key challenges facing the field. Our findings suggest that while substantial progress has been made, there remain many open issues related to data quality, algorithm performance, and interpretability of results. To address these challenges, we propose several areas of future work including developing more advanced methods for handling missing data, exploring techniques for interpreting complex models, and identifying strategies for integrating multiple sources of evidence into decision making. Overall, our evaluation highlights both the promise and the complexity of machine learning for longitudinal EHD analysis, suggesting that further investigation is warranted.",1
"Graph Neural Networks (GNNs), a generalization of neural networks to graph-structured data, are often implemented using message passes between entities of a graph. While GNNs are effective for node classification, link prediction and graph classification, they are vulnerable to adversarial attacks, i.e., a small perturbation to the structure can lead to a non-trivial performance degradation. In this work, we propose Uncertainty Matching GNN (UM-GNN), that is aimed at improving the robustness of GNN models, particularly against poisoning attacks to the graph structure, by leveraging epistemic uncertainties from the message passing framework. More specifically, we propose to build a surrogate predictor that does not directly access the graph structure, but systematically extracts reliable knowledge from a standard GNN through a novel uncertainty-matching strategy. Interestingly, this uncoupling makes UM-GNN immune to evasion attacks by design, and achieves significantly improved robustness against poisoning attacks. Using empirical studies with standard benchmarks and a suite of global and target attacks, we demonstrate the effectiveness of UM-GNN, when compared to existing baselines including the state-of-the-art robust GCN.",0
"This paper proposes a novel approach to defend against poisoning attacks on graph neural networks (GNNs), which are commonly used in applications such as node classification and link prediction. Poisoning attacks are a serious threat to GNNs, where adversaries manipulate training data to reduce model accuracy or cause misclassification. Our method addresses this problem by introducing uncertainty-matching graph neural networks (UM-GNNs). UM-GNNs use Bayesian inference to estimate uncertainty in edge weights during both training and testing, allowing them to identify and discard potentially poisoned edges. We demonstrate through extensive experiments that our approach significantly outperforms state-of-the-art defense methods across several benchmark datasets. Additionally, we show that UM-GNNs can effectively detect poisoning attacks even when using only a small fraction of labeled data. Overall, our work shows that incorporating uncertainty into GNNs is a promising direction for improving their robustness to adversarial attacks.",1
"Graph structured data has wide applicability in various domains such as physics, chemistry, biology, computer vision, and social networks, to name a few. Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. GNN is a deep learning based method that learns a node representation by combining specific nodes and the structural/topological information of a graph. However, like other deep models, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. More specifically, to explain a node, we generate a nonlinear interpretable model from its $N$-hop neighborhood and then compute the K most representative features as the explanations of its prediction using HSIC Lasso. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.",0
"Abstract: Graph neural networks (GNN) have emerged as powerful models for representation learning on graph data, such as knowledge graphs and social networks. However, one challenge facing GNNs is their lack of interpretability due to the complexity of graph structures and nonlinear operations. To address this issue, we propose GraphLIME, a model interpretation framework that provides local interpretable model explanations specifically designed for GNNs. Inspired by SHAP values commonly used for deep learning interpretations, our method generates importance scores for each node or edge in a graph, indicating how they contribute to the prediction output. We showcase the effectiveness and versatility of GraphLIME through extensive experiments using real-world datasets across a wide range of applications, including link prediction and node classification tasks. Our results demonstrate GraphLIME can generate meaningful and accurate explanations, while offering valuable insights into the working principles behind GNNs. Given the significant implications for understanding complex graph phenomena and ensuring trustworthiness in decision making based on GNN predictions, GraphLIME has the potential for widespread impact in the broader machine learning community. Keywords: Graph neural network, explainable artificial intelligence, interpretability, local explanation, global sensitivity analysis",1
"Graph Neural Networks (GNNs) have attracted considerable attention and have emerged as a new promising paradigm to process graph-structured data. GNNs are usually stacked to multiple layers and the node representations in each layer are computed through propagating and aggregating the neighboring node features with respect to the graph. By stacking to multiple layers, GNNs are able to capture the long-range dependencies among the data on the graph and thus bring performance improvements. To train a GNN with multiple layers effectively, some normalization techniques (e.g., node-wise normalization, batch-wise normalization) are necessary. However, the normalization techniques for GNNs are highly task-relevant and different application tasks prefer to different normalization techniques, which is hard to know in advance. To tackle this deficiency, in this paper, we propose to learn graph normalization by optimizing a weighted combination of normalization techniques at four different levels, including node-wise normalization, adjacency-wise normalization, graph-wise normalization, and batch-wise normalization, in which the adjacency-wise normalization and the graph-wise normalization are newly proposed in this paper to take into account the local structure and the global structure on the graph, respectively. By learning the optimal weights, we are able to automatically select a single best or a best combination of multiple normalizations for a specific task. We conduct extensive experiments on benchmark datasets for different tasks, including node classification, link prediction, graph classification and graph regression, and confirm that the learned graph normalization leads to competitive results and that the learned weights suggest the appropriate normalization techniques for the specific task. Source code is released here https://github.com/cyh1112/GraphNormalization.",0
"Effectively normalizing large scale graphs has been a challenge due to the high computational requirements of traditional normalization methods such as batch renormalization and layerwise normalization. These methods can lead to significant memory usage and slow down training time on very deep neural networks. In this study we propose graph normalization as a solution to this problem, which utilizes Chebyshev approximations to speed up computations without sacrificing accuracy. This approach allows us to efficiently normalize extremely large datasets while reducing memory usage during training. Our experimental results show that our proposed method leads to improved performance compared to other state-of-the-art approaches.",1
"While existing predictive frameworks are able to handle Euclidean structured data (i.e, brain images), they might fail to generalize to geometric non-Euclidean data such as brain networks. Besides, these are rooted the sample selection step in using Euclidean or learned similarity measure between vectorized training and testing brain networks. Such sample connectomic representation might include irrelevant and redundant features that could mislead the training sample selection step. Undoubtedly, this fails to exploit and preserve the topology of the brain connectome. To overcome this major drawback, we propose Residual Embedding Similarity-Based Network selection (RESNets) for predicting brain network evolution trajectory from a single timepoint. RESNets first learns a compact geometric embedding of each training and testing sample using adversarial connectome embedding network. This nicely reduces the high-dimensionality of brain networks while preserving their topological properties via graph convolutional networks. Next, to compute the similarity between subjects, we introduce the concept of a connectional brain template (CBT), a fixed network reference, where we further represent each training and testing network as a deviation from the reference CBT in the embedding space. As such, we select the most similar training subjects to the testing subject at baseline by comparing their learned residual embeddings with respect to the pre-defined CBT. Once the best training samples are selected at baseline, we simply average their corresponding brain networks at follow-up timepoints to predict the evolution trajectory of the testing network. Our experiments on both healthy and disordered brain networks demonstrate the success of our proposed method in comparison to RESNets ablated versions and traditional approaches.",0
"This is a technical article that presents two new models developed by researchers at MIT: ResNet and DiffWave. These models were created using deep learning methods to solve problems related to image and video recognition, respectively. In particular, ResNet uses residual connections and a novel unit normalization approach to improve training stability and reduce overfitting, while DiffWave adopts wavelet convolutions and temporal differences to effectively capture temporal dependencies in videos. Both models outperform other state-of-the-art methods on benchmark datasets, demonstrating their effectiveness in computer vision tasks.",1
"Graph neural networks (GNNs) have achieved high performance in analyzing graph-structured data and have been widely deployed in safety-critical areas, such as finance and autonomous driving. However, only a few works have explored GNNs' robustness to adversarial attacks, and their designs are usually limited by the scale of input datasets (i.e., focusing on small graphs with only thousands of nodes). In this work, we propose, SAG, the first scalable adversarial attack method with Alternating Direction Method of Multipliers (ADMM). We first decouple the large-scale graph into several smaller graph partitions and cast the original problem into several subproblems. Then, we propose to solve these subproblems using projected gradient descent on both the graph topology and the node features that lead to considerably lower memory consumption compared to the conventional attack methods. Rigorous experiments further demonstrate that SAG can significantly reduce the computation and memory overhead compared with the state-of-the-art approach, making SAG applicable towards graphs with large size of nodes and edges.",0
"This work presents a new method for performing adversarial attacks on graph neural networks (GNNs) using alternating direction methods of multipliers (ADMM). GNNs have recently gained popularity due to their ability to handle graph data structures such as social networks, traffic flow maps, and biological networks. However, they remain vulnerable to adversarial attacks that manipulate inputs to cause incorrect predictions. Existing attack methods are limited by scalability issues and lack of versatility. To address these limitations, we propose ADMM-based adversarial attacks that can efficiently generate universal perturbations across multiple graphs without compromising effectiveness. Our evaluation shows significant improvement over existing methods in terms of success rate while maintaining computational efficiency. By shedding light on potential weaknesses of state-of-the-art GNN models, our study contributes to advancing the understanding and development of more robust machine learning systems.",1
"Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses generative models as priors over the components of a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation of separation results on CIFAR-10. We also provide qualitative results on LSUN.",0
"In recent years, deep generative models have proven successful at tasks such as image synthesis, speech generation, and style transfer. However, their use in audio source separation has remained limited due to high computational cost and poor interpretability. This work proposes using deep generative priors (DGPs) as regularizers for non-negative matrix factorization (NMF), a popular method for source separation. DGPs encourage NMF to generate physically plausible solutions that match prior distributions learned by generative models on large datasets. We evaluate the performance of our approach on real-world mixtures and show that it produces state-of-the-art results while remaining computationally efficient compared to other methods. Furthermore, we demonstrate how DGPs can improve the interpretability of the separated sources by analyzing their spectral characteristics and showing that they align well with human expectations. Overall, our findings suggest that incorporating DGPs into NMF significantly enhances both separation accuracy and interpretability, making them promising tools for future research in this field.",1
"Axis-aligned decision forests have long been the leading class of machine learning algorithms for modeling tabular data. In many applications of machine learning such as learning-to-rank, decision forests deliver remarkable performance. They also possess other coveted characteristics such as interpretability. Despite their widespread use and rich history, decision forests to date fail to consume raw structured data such as text, or learn effective representations for them, a factor behind the success of deep neural networks in recent years. While there exist methods that construct smoothed decision forests to achieve representation learning, the resulting models are decision forests in name only: They are no longer axis-aligned, use stochastic decisions, or are not interpretable. Furthermore, none of the existing methods are appropriate for problems that require a Transfer Learning treatment. In this work, we present a novel but intuitive proposal to achieve representation learning for decision forests without imposing new restrictions or necessitating structural changes. Our model is simply a decision forest, possibly trained using any forest learning algorithm, atop a deep neural network. By approximating the gradients of the decision forest through input perturbation, a purely analytical procedure, the decision forest directs the neural network to learn or fine-tune representations. Our framework has the advantage that it is applicable to any arbitrary decision forest and that it allows the use of arbitrary deep neural networks for representation learning. We demonstrate the feasibility and effectiveness of our proposal through experiments on synthetic and benchmark classification datasets.",0
"This paper presents a method for learning representations for axis-aligned decision trees that can improve their accuracy on a variety of tasks. Our approach involves adding random perturbations to the inputs before they are fed into the model, which helps the tree learn more robust features and generalize better across different domains. We demonstrate the effectiveness of our method using several experiments on benchmark datasets, showing significant improvements over previous methods. Additionally, we provide analysis on the learned representations and discuss possible applications beyond decision forests. Overall, this work contributes to advancing the state of art of machine learning by improving one specific type of classifier, and thus paving the way for new techniques based upon these fundamental ideas.",1
"Urban ride-hailing demand prediction is a crucial but challenging task for intelligent transportation system construction. Predictable ride-hailing demand can facilitate more reasonable vehicle scheduling and online car-hailing platform dispatch. Conventional deep learning methods with no external structured data can be accomplished via hybrid models of CNNs and RNNs by meshing plentiful pixel-level labeled data, but spatial data sparsity and limited learning capabilities on temporal long-term dependencies are still two striking bottlenecks. To address these limitations, we propose a new virtual graph modeling method to focus on significant demand regions and a novel Deep Multi-View Spatiotemporal Virtual Graph Neural Network (DMVST-VGNN) to strengthen learning capabilities of spatial dynamics and temporal long-term dependencies. Specifically, DMVST-VGNN integrates the structures of 1D Convolutional Neural Network, Multi Graph Attention Neural Network and Transformer layer, which correspond to short-term temporal dynamics view, spatial dynamics view and long-term temporal dynamics view respectively. In this paper, experiments are conducted on two large-scale New York City datasets in fine-grained prediction scenes. And the experimental results demonstrate effectiveness and superiority of DMVST-VGNN framework in significant citywide ride-hailing demand prediction.",0
"This paper presents a new approach to predict significant citywide ride-hailing demand. Our method leverages multiple views of data from diverse sources including GPS trajectories of vehicles and riders, road networks, weather conditions, events calendars, social media feeds, public transit schedules, and more, using graph neural networks (GNNs) that process both static graphs and temporal sequences. We train our model on these multi-view spatiotemporal virtual graphs which capture both the structure of urban areas and the dynamic behavior patterns of people and vehicles over time. Using large amounts of historical data, we demonstrate high accuracy in forecasting significant changes in ride-hailing demand over short periods ranging from several hours to one day ahead. By accurately predicting surges or dips in demand across entire cities, our proposed system can enable optimized allocation of driver fleets, fair pricing strategies, improved traffic management, and ultimately enhance customer satisfaction in modern mobility services.",1
"Representation learning over graph structure data has been widely studied due to its wide application prospects. However, previous methods mainly focus on static graphs while many real-world graphs evolve over time. Modeling such evolution is important for predicting properties of unseen networks. To resolve this challenge, we propose SGRNN, a novel neural architecture that applies stochastic latent variables to simultaneously capture the evolution in node attributes and topology. Specifically, deterministic states are separated from stochastic states in the iterative process to suppress mutual interference. With semi-implicit variational inference integrated to SGRNN, a non-Gaussian variational distribution is proposed to help further improve the performance. In addition, to alleviate KL-vanishing problem in SGRNN, a simple and interpretable structure is proposed based on the lower bound of KL-divergence. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed model. Code is available at https://github.com/StochasticGRNN/SGRNN.",0
"This study presents a novel approach based on stochastic graph recurrent neural networks (SGNNs) that allows us to model complex sequential data, such as speech signals. SGNNs use a probabilistic framework that incorporates uncertainty into the training process by estimating the posterior distributions over the network parameters given the observed sequences. Our experimental results demonstrate that our proposed method outperforms state-of-the-art techniques on several benchmark datasets. We show that by integrating temporal dependencies and uncertainty into graph neural networks, we can significantly improve their performance on challenging tasks such as speech recognition, while maintaining efficient computation. Overall, this work represents a significant contribution to the field of machine learning and shows great promise for further applications.",1
"Recently, the surge in popularity of Internet of Things (IoT), mobile devices, social media, etc. has opened up a large source for graph data. Graph embedding has been proved extremely useful to learn low-dimensional feature representations from graph structured data. These feature representations can be used for a variety of prediction tasks from node classification to link prediction. However, existing graph embedding methods do not consider users' privacy to prevent inference attacks. That is, adversaries can infer users' sensitive information by analyzing node representations learned from graph embedding algorithms. In this paper, we propose Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that integrates the disentangling and purging mechanisms to remove users' private information from learned node representations. The proposed method preserves the structural information and utility attributes of a graph while concealing users' private attributes from inference attacks. Extensive experiments on real-world graph datasets demonstrate the superior performance of APGE compared to the state-of-the-arts. Our source code can be found at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.",0
"This paper presents a novel graph embedding method that preserves privacy by addressing inference attacks. We propose adversarial perturbations that mitigate attribute leakage through iterative gradient sign descent and a clipping operation on each node’s representation. To evaluate our approach, we consider several benchmark datasets and demonstrate improvements over existing methods in terms of privacy measures. Our results indicate that the proposed model achieves strong protection against attribute inference attacks while retaining high accuracy for downstream tasks such as clustering and classification. Overall, our work contributes to advancing the field of privacy-preserving graph embeddings.",1
"Operator-Valued Kernels (OVKs) and associated vector-valued Reproducing Kernel Hilbert Spaces provide an elegant way to extend scalar kernel methods when the output space is a Hilbert space. Although primarily used in finite dimension for problems like multi-task regression, the ability of this framework to deal with infinite dimensional output spaces unlocks many more applications, such as functional regression, structured output prediction, and structured data representation. However, these sophisticated schemes crucially rely on the kernel trick in the output space, so that most of previous works have focused on the square norm loss function, completely neglecting robustness issues that may arise in such surrogate problems. To overcome this limitation, this paper develops a duality approach that allows to solve OVK machines for a wide range of loss functions. The infinite dimensional Lagrange multipliers are handled through a Double Representer Theorem, and algorithms for $\epsilon$-insensitive losses and the Huber loss are thoroughly detailed. Robustness benefits are emphasized by a theoretical stability analysis, as well as empirical improvements on structured data applications.",0
"This paper presents new results on duality theory for reproducing kernel Hilbert spaces (RKHS) with infinite dimensional outputs. We develop novel methods that allow us to characterize duality and derive robust loss functions that can be used in applications such as machine learning and signal processing. Our work extends previous research by addressing challenges associated with infinite dimensions and provides new insights into how these spaces can be utilized in practice. Through our theoretical development and numerical experiments, we demonstrate the effectiveness of our approach and highlight its potential impact on various fields.",1
"As machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses human knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better performance than other weak-labeling techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training.",0
"Automatic labeling of images can prove incredibly beneficial for many industrial applications such as quality control, defect detection, and decision making processes. However, manual image annotation remains costly and time consuming due to high levels of expertise required. In order to address these issues, we propose ""Inspector Gadget,"" which utilizes advanced techniques from deep learning, computer vision, and programming languages like Python, Rust, Java or C++. Inspector gadgets can identify objects or features in an image by extracting them through data mining and transforming into machine readable code that is integrated into user interfaces, thus speeding up process optimization. Our work demonstrates a novel approach using algorithms trained on datasets collected from domain experts, which enables accurate object recognition without requiring specialized knowledge. By combining state-of-the-art methodologies with human input, our system provides a powerful toolset for industries across numerous sectors including logistics, automotive manufacturing and pharmaceuticals. Additionally, Inspector gadget has the potential for deployment across emerging technologies such as drones and IoT devices. As such, our research represents a significant contribution towards efficient industrial practices, while reducing costs and increasing productivity.",1
"In this paper, we focus on learning low-dimensional embeddings for nodes in graph-structured data. To achieve this, we propose Caps2NE -- a new unsupervised embedding model leveraging a network of two capsule layers. Caps2NE induces a routing process to aggregate feature vectors of context neighbors of a given target node at the first capsule layer, then feed these features into the second capsule layer to infer a plausible embedding for the target node. Experimental results show that our proposed Caps2NE obtains state-of-the-art performances on benchmark datasets for the node classification task. Our code is available at: \url{https://github.com/daiquocnguyen/Caps2NE}.",0
"In the following document, we propose the use of capsule networks to learn node embeddings. We begin by providing background on graph neural networks (GNNs), which have recently gained popularity due to their ability to effectively capture structural features of graphs. However, GNNs suffer from limitations related to oversmoothing, overfitting, and lack of robustness to noise. To address these challenges, we present our proposed model, which leverages capsule networks’ unique capabilities such as dynamic routing and attention mechanisms. Specifically, we show how capsule networks can generate high-quality representations of nodes that accurately capture both local and global information of a graph. We evaluate our model through extensive experiments using several benchmark datasets, demonstrating superior performance compared to state-of-the art alternatives. This work offers promising results towards improving graph representation learning and has applications in areas ranging from social network analysis to bioinformatics. Our contributions include: proposal of capsule network architecture for GNNs; exploration of capsule network dynamics in the context of graph embedding; evaluation of the efficacy of capsule networks in learning graph representation; and demonstration of significant improvement in downstream tasks compared to current methods. By introducing capsule networks into the field of graph neural networking, we believe that we can expand research possibilities and provide opportunities to develop powerful models that better capture complex relationships within large scale data sets. Further research includes investigations into the potential of capsule networks for graph generation.",1
"Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. However, applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes as input raw point clouds. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on various datasets where our method achieves state-of-the-art performance.",0
"Artificial Intelligence (AI) has made significant strides in recent years in enabling machines to process complex data formats such as point clouds more efficiently than ever before. With advances in computer graphics technology, artists now have access to increasingly detailed geometry which in turn leads to higher levels of realism in generated images and videos. However, processing such dense datasets comes at a computational cost that can limit the complexity of scenes that can be rendered. In this work we present lattice segmentation, a new method designed specifically to address these challenges by providing both high quality results on a variety of datasets while running at interactive speeds. By adapting permutohedral lattices, our approach achieves state-of-the-art performance across all metrics for point cloud segmentation tasks.",1
"Deep generative models have made tremendous advances in image and signal representation learning and generation. These models employ the full Euclidean space or a bounded subset as the latent space, whose flat geometry, however, is often too simplistic to meaningfully reflect the manifold structure of the data. In this work, we advocate the use of a multi-chart latent space for better data representation. Inspired by differential geometry, we propose a \textbf{Chart Auto-Encoder (CAE)} and prove a universal approximation theorem on its representation capability. We show that the training data size and the network size scale exponentially in approximation error with an exponent depending on the intrinsic dimension of the data manifold. CAE admits desirable manifold properties that auto-encoders with a flat latent space fail to obey, predominantly proximity of data. We conduct extensive experimentation with synthetic and real-life examples to demonstrate that CAE provides reconstruction with high fidelity, preserves proximity in the latent space, and generates new data remaining near the manifold. These experiments show that CAE is advantageous over existing auto-encoders and variants by preserving the topology of the data manifold as well as its geometry.",0
"Title: ""Chart Auto-encoder for Dimensionality Reduction on Stuctured Data""",1
"The paper introduces two new aggregation functions to encode structural knowledge from tree-structured data. They leverage the Canonical and Tensor-Train decompositions to yield expressive context aggregation while limiting the number of model parameters. Finally, we define two novel neural recursive models for trees leveraging such aggregation functions, and we test them on two tree classification tasks, showing the advantage of proposed models when tree outdegree increases.",0
"This article presents the implementation of tensor decompositions into recursive neural networks (RNN) designed to process tree-structured data. We detail two novel approaches: Parallelized Canonical Polyadic (ParaCP) decomposition and Structured Random Projection. Both techniques improve on state-of-the art methods by allowing for increased scalability while minimizing computational complexity and memory utilization. In addition, we present experimental results showing that our implementations can effectively train models at larger scale than previous methods. Our work provides new opportunities for large-scale machine learning and artificial intelligence applications on trees, graphs, and other hierarchical structures such as XML documents and genealogies.",1
"Sparse incidence tensors can represent a variety of structured data. For example, we may represent attributed graphs using their node-node, node-edge, or edge-edge incidence matrices. In higher dimensions, incidence tensors can represent simplicial complexes and polytopes. In this paper, we formalize incidence tensors, analyze their structure, and present the family of equivariant networks that operate on them. We show that any incidence tensor decomposes into invariant subsets. This decomposition, in turn, leads to a decomposition of the corresponding equivariant linear maps, for which we prove an efficient pooling-and-broadcasting implementation.",0
"Deep learning has seen tremendous success in recent years across diverse fields such as computer vision, natural language processing, speech recognition, robotics, bioinformatics, among others. However, most existing deep learning models operate on regular grids (either pixel arrays or sets of discrete points) which may be far from optimal for representing complex geometric features commonly found in real world data. To address this limitation, we propose incidence networks: neural network architectures designed specifically for processing point cloud data represented using oriented incidences rather than traditional grids. Our key contributions include developing novel building blocks for constructing these architectures, introducing methods for training them efficiently without sacrificing expressivity, and demonstrating their effectiveness by achieving state-of-the-art results on several challenging benchmark datasets. Our approach opens up new possibilities for designing models that can effectively capture high-level abstractions from raw geometric data while maintaining efficiency, interpretability, and generality, paving the way towards more powerful systems in geometric deep learning.",1
"Variational autoencoder (VAE) is a widely used generative model for learning latent representations. Burda et al. in their seminal paper showed that learning capacity of VAE is limited by over-pruning. It is a phenomenon where a significant number of latent variables fail to capture any information about the input data and the corresponding hidden units become inactive. This adversely affects learning diverse and interpretable latent representations. As variational graph autoencoder (VGAE) extends VAE for graph-structured data, it inherits the over-pruning problem. In this paper, we adopt a model based approach and propose epitomic VGAE (EVGAE),a generative variational framework for graph datasets which successfully mitigates the over-pruning problem and also boosts the generative ability of VGAE. We consider EVGAE to consist of multiple sparse VGAE models, called epitomes, that are groups of latent variables sharing the latent space. This approach aids in increasing active units as epitomes compete to learn better representation of the graph data. We verify our claims via experiments on three benchmark datasets. Our experiments show that EVGAE has a better generative ability than VGAE. Moreover, EVGAE outperforms VGAE on link prediction task in citation networks.",0
"Title: Variational Graph Autoencoders for Generative Modeling of Complex Data  Abstract:  Generative models have been gaining increasing popularity due to their ability to generate synthetic data that closely resembles real observations. Recent advances in deep learning have led to significant improvements in generative modeling through techniques such as variational autoencoders (VAEs) and generative adversarial networks (GANs). However, these methods often struggle with modeling complex data structures like graphs and networks due to their inherent irregularities and connectivity patterns.  This paper introduces Epitomic Variational Graph Autoencoder (EVGA), a novel graph-based generative model that leverages ideas from VAEs and graph neural networks. EVGA learns an expressive latent space by encoding graph structural information into a continuous vector representation while preserving important topological features. This allows us to capture complex dependencies within the data and generate new samples that faithfully represent the underlying structure. Our approach can handle datasets with varying sizes and node properties without any manual engineering or preprocessing steps.  We demonstrate the effectiveness of our method on several benchmark datasets across different domains, including social network analysis, citation networks, and protein interactions. Results show that EVGA consistently outperforms state-of-the-art baselines in terms of both quality of generated data and quality measures such as edge recall, Jaccard similarity coefficient, and F1 score. Furthermore, we conduct ablation studies to analyze the impact of key design decisions and present visualizations to illustrate how our model captures meaningful relationships in the data.  Our work addresses current limitations in graph-based generative models and provides researchers and practitioners with a powerful tool for generating synthetic data that preserve essential characteristics of complex systems. With numerous applications in areas ranging from finance and economics to biology and public health, EVGA has t",1
"Active learning (AL) on attributed graphs has received increasing attention with the prevalence of graph-structured data. Although AL has been widely studied for alleviating label sparsity issues with the conventional non-related data, how to make it effective over attributed graphs remains an open research question. Existing AL algorithms on graphs attempt to reuse the classic AL query strategies designed for non-related data. However, they suffer from two major limitations. First, different AL query strategies calculated in distinct scoring spaces are often naively combined to determine which nodes to be labelled. Second, the AL query engine and the learning of the classifier are treated as two separating processes, resulting in unsatisfactory performance. In this paper, we propose a SEmi-supervised Adversarial active Learning (SEAL) framework on attributed graphs, which fully leverages the representation power of deep neural networks and devises a novel AL query strategy in an adversarial way. Our framework learns two adversarial components: a graph embedding network that encodes both the unlabelled and labelled nodes into a latent space, expecting to trick the discriminator to regard all nodes as already labelled, and a semi-supervised discriminator network that distinguishes the unlabelled from the existing labelled nodes in the latent space. The divergence score, generated by the discriminator in a unified latent space, serves as the informativeness measure to actively select the most informative node to be labelled by an oracle. The two adversarial components form a closed loop to mutually and simultaneously reinforce each other towards enhancing the active learning performance. Extensive experiments on four real-world networks validate the effectiveness of the SEAL framework with superior performance improvements to state-of-the-art baselines.",0
"In this paper, we introduce SEAL (Semi-Supervised Adversarial Active Learning on Attributed Graphs), a novel approach to learning from attributed graphs that have limited labeled data available. Our method leverages both semi-supervision and active learning techniques to effectively utilize unlabeled data while minimizing human annotation efforts. We employ adversarial training within our framework to improve model robustness and generalization ability. Furthermore, we provide extensive experimental evaluations across multiple datasets demonstrating the effectiveness of our proposed method compared to state-of-the-art alternatives. Overall, SEAL offers a promising solution for learning on large-scale attributed networks where annotations are scarce.",1
"Graph Convolutional Networks (GCNs) have already demonstrated their powerful ability to model the irregular data, e.g., skeletal data in human action recognition, providing an exciting new way to fuse rich structural information for nodes residing in different parts of a graph. In human action recognition, current works introduce a dynamic graph generation mechanism to better capture the underlying semantic skeleton connections and thus improves the performance. In this paper, we provide an orthogonal way to explore the underlying connections. Instead of introducing an expensive dynamic graph generation paradigm, we build a more efficient GCN on a Riemann manifold, which we think is a more suitable space to model the graph data, to make the extracted representations fit the embedding matrix. Specifically, we present a novel spatial-temporal GCN (ST-GCN) architecture which is defined via the Poincar\'e geometry such that it is able to better model the latent anatomy of the structure data. To further explore the optimal projection dimension in the Riemann space, we mix different dimensions on the manifold and provide an efficient way to explore the dimension for each ST-GCN layer. With the final resulted architecture, we evaluate our method on two current largest scale 3D datasets, i.e., NTU RGB+D and NTU RGB+D 120. The comparison results show that the model could achieve a superior performance under any given evaluation metrics with only 40\% model size when compared with the previous best GCN method, which proves the effectiveness of our model.",0
"In recent years, skeletal action recognition has become increasingly important due to advances in motion capture technology. However, traditional approaches based on handcrafted features have limited performance since they cannot handle complex human movements and variations. To overcome these limitations, researchers have explored deep learning methods that can learn feature representations automatically from raw sensor data. One popular approach is using graph convolutional networks (GCNs) which model the relationships among joints in a coordinate system. Another approach is using autoencoders which encode time sequences into fixed-size vectors while preserving spatial and temporal structures. Both GCNs and autoencoders have shown promising results but their performance could still benefit from the integration of other modalities such as depth maps or optical flows. In this work, we propose MixDim, a novel framework that integrates multiple modalities such as depth maps and optical flows along with joint coordinates in Poincare geometry for more accurate 3D skeleton-based action recognition. We first introduce a new representation called Joint Depth Maps (JDMs) which project 3D joint positions onto a 2D plane under camera viewpoint. Then, we apply graph convolutions to both original joint coordinates and the newly generated JDMs in each modality space separately. Next, we combine different modalities by linearly mixing their learned embeddings before feeding them into our final classification network. Our experiments show consistent improvements across several benchmark datasets compared to state-of-the-art baselines, demonstrating the effectiveness of the proposed framework.",1
"The effective representation, processing, analysis, and visualization of large-scale structured data, especially those related to complex domains such as networks and graphs, are one of the key questions in modern machine learning. Graph signal processing (GSP), a vibrant branch of signal processing models and algorithms that aims at handling data supported on graphs, opens new paths of research to address this challenge. In this article, we review a few important contributions made by GSP concepts and tools, such as graph filters and transforms, to the development of novel machine learning algorithms. In particular, our discussion focuses on the following three aspects: exploiting data structure and relational priors, improving data and computational efficiency, and enhancing model interpretability. Furthermore, we provide new perspectives on future development of GSP techniques that may serve as a bridge between applied mathematics and signal processing on one side, and machine learning and network science on the other. Cross-fertilization across these different disciplines may help unlock the numerous challenges of complex data analysis in the modern age.",0
"Graph Signal Processing (GSP) has emerged as an important field that addresses the processing and analysis of signals on graphs, which have become increasingly popular due to their ability to model complex systems and relationships. In recent years, GSP has been applied to several domains including social network analysis, image and video processing, sensor networks, and biological signal processing. This work presents a comprehensive survey of graph signal processing methods and techniques used in machine learning applications. The paper reviews state-of-the-art algorithms and tools developed for tasks such as classification, clustering, regression, dimensionality reduction, feature extraction, anomaly detection, and dynamic system identification. The authors also discuss open challenges and future research directions in this rapidly developing area of study. Overall, the paper provides valuable insights into the use of graph signal processing for machine learning, highlighting both its potential benefits and limitations. As such, it serves as a reference resource for researchers, practitioners, and students interested in exploring the intersection of these two fields.",1
"Understanding how certain brain regions relate to a specific neurological disorder has been an important area of neuroimaging research. A promising approach to identify the salient regions is using Graph Neural Networks (GNNs), which can be used to analyze graph structured data, e.g. brain networks constructed by functional magnetic resonance imaging (fMRI). We propose an interpretable GNN framework with a novel salient region selection mechanism to determine neurological brain biomarkers associated with disorders. Specifically, we design novel regularized pooling layers that highlight salient regions of interests (ROIs) so that we can infer which ROIs are important to identify a certain disease based on the node pooling scores calculated by the pooling layers. Our proposed framework, Pooling Regularized-GNN (PR-GNN), encourages reasonable ROI-selection and provides flexibility to preserve either individual- or group-level patterns. We apply the PR-GNN framework on a Biopoint Autism Spectral Disorder (ASD) fMRI dataset. We investigate different choices of the hyperparameters and show that PR-GNN outperforms baseline methods in terms of classification accuracy. The salient ROI detection results show high correspondence with the previous neuroimaging-derived biomarkers for ASD.",0
"Abstract: Deep learning methods have recently shown promising results in the analysis of functional magnetic resonance imaging (fMRI) data. However, most existing deep learning approaches model brain activity as independent voxels without considering spatial dependencies among neighboring regions. To address this issue, we propose Pooling Regularized Graph Neural Networks (PRGNN), which incorporates graph regularization into convolutional neural networks for efficient integration of spatial correlations. We evaluate PRGNN on four publicly available fMRI datasets for different tasks, including motor task, working memory, affective processing, and visual stimulation. Experimental results demonstrate that PRGNN achieves state-of-the-art performance across multiple benchmark metrics and provides more accurate biomarkers than other methods. Our findings suggest that integrating spatial dependencies can significantly improve the interpretability of fMRI predictions by identifying functionally meaningful biomarkers. Paper Title: ""Pooling Regularized Graph Neural Network for fMRI Biomarker Analysis""",1
"Tensor data with rich structural information becomes increasingly important in process modeling, monitoring, and diagnosis. Here structural information is referred to structural properties such as sparsity, smoothness, low-rank, and piecewise constancy. To reveal useful information from tensor data, we propose to decompose the tensor into the summation of multiple components based on different structural information of them. In this paper, we provide a new definition of structural information in tensor data. Based on it, we propose an additive tensor decomposition (ATD) framework to extract useful information from tensor data. This framework specifies a high dimensional optimization problem to obtain the components with distinct structural information. An alternating direction method of multipliers (ADMM) algorithm is proposed to solve it, which is highly parallelable and thus suitable for the proposed optimization problem. Two simulation examples and a real case study in medical image analysis illustrate the versatility and effectiveness of the ATD framework.",0
"Title: Additive Tensor Decomposition Considering Structural Data Information Author(s): Shi Wang, Xiaowen Dong, Wanqing Liu Abstract: This paper proposes an additive tensor decomposition method that considers structural data information, which can effectively reveal the underlying structure of high-order data tensors while exploiting the information hidden within them. By incorporating structured information into tensor factorization, we develop a new model named SADT (StructurAlly guided AdditivE Tensor DecompositiOn) that combines additive decompositions with regularizers inspired by graph signal processing theory. Experimental results demonstrate that our proposed approach outperforms state-of-the-art methods on several datasets and tasks such as image classification and collaborative filtering, showcasing its effectiveness in handling complex structured signals and data with multiple structures simultaneously.",1
"Graph neural networks emerge as a promising modeling method for applications dealing with datasets that are best represented in the graph domain. In specific, developing recommendation systems often require addressing sparse structured data which often lacks the feature richness in either the user and/or item side and requires processing within the correct context for optimal performance. These datasets intuitively can be mapped to and represented as networks or graphs. In this paper, we propose the Hierarchical BiGraph Neural Network (HBGNN), a hierarchical approach of using GNNs as recommendation systems and structuring the user-item features using a bigraph framework. Our experimental results show competitive performance with current recommendation system methods and transferability.",0
"This paper presents our proposed Hierarchical Bipartite Graph (BiG) neural network architecture designed to address some challenges faced by traditional recommendation systems. Our model leverages on insights from graph theory, machine learning, data mining, human intelligence gathering, social psychology, mathematical models that simulate user behavior to improve overall accuracy. We first define our novel objective function based on maximizing user satisfaction which we prove optimizes well under standard assumptions. Next, we describe our hierarchical bipartite graph design wherein entities such as users and items are embedded into two disjointed latent spaces connected via their interactions. Finally, we employ multi layers of GCNs with residual connections overcoming key limitations observed in prior work. Empirically evaluated across three diverse datasets ranging from movies to scientific articles shows better than average effectiveness relative to other state-of-the-art baselines with improvements up to ~27% recall@10 on average. Conclusions discuss implications for future research in building more accurate recommendation systems in general.",1
"Graph convolutional networks gain remarkable success in semi-supervised learning on graph structured data. The key to graph-based semisupervised learning is capturing the smoothness of labels or features over nodes exerted by graph structure. Previous methods, spectral methods and spatial methods, devote to defining graph convolution as a weighted average over neighboring nodes, and then learn graph convolution kernels to leverage the smoothness to improve the performance of graph-based semi-supervised learning. One open challenge is how to determine appropriate neighborhood that reflects relevant information of smoothness manifested in graph structure. In this paper, we propose GraphHeat, leveraging heat kernel to enhance low-frequency filters and enforce smoothness in the signal variation on the graph. GraphHeat leverages the local structure of target node under heat diffusion to determine its neighboring nodes flexibly, without the constraint of order suffered by previous methods. GraphHeat achieves state-of-the-art results in the task of graph-based semi-supervised classification across three benchmark datasets: Cora, Citeseer and Pubmed.",0
"This should provide a concise summary of your work without revealing any details that would compromise novelty. Additionally, please suggest two related papers from your field as references. Finally, feel free to add one additional comment at the end regarding the significance of your research question (e.g. why we care).",1
"The objective of active learning (AL) is to train classification models with less number of labeled instances by selecting only the most informative instances for labeling. The AL algorithms designed for other data types such as images and text do not perform well on graph-structured data. Although a few heuristics-based AL algorithms have been proposed for graphs, a principled approach is lacking. In this paper, we propose MetAL, an AL approach that selects unlabeled instances that directly improve the future performance of a classification model. For a semi-supervised learning problem, we formulate the AL task as a bilevel optimization problem. Based on recent work in meta-learning, we use the meta-gradients to approximate the impact of retraining the model with any unlabeled instance on the model performance. Using multiple graph datasets belonging to different domains, we demonstrate that MetAL efficiently outperforms existing state-of-the-art AL algorithms.",0
"This paper presents a novel active semi-supervised learning algorithm called ""MetAL"" (Meta Learning on Graphs) that leverages meta learning principles for enhanced generalization performance across multiple data sets in graph data analysis tasks. Traditional semi-supervised methods assume either strong prior knowledge over the target function or complex models built upon large amounts of labeled data, which may limit their applicability in real-world scenarios where only limited labeled samples and edge features are available. To address these challenges, MetAL integrates meta learning techniques into the existing SSL framework, resulting in a more flexible method capable of capturing both global and local patterns from graph structure as well as adapting itself to specific datasets. Experimental results demonstrate improved accuracy, robustness, and stability compared with state-of-the-art algorithms in four different application domains: image classification, sentiment analysis, citation network analysis, and bioinformatics. Overall, our study provides insights into the design of scalable, adaptive SSL strategies that can exploit weak supervision signals under a unified framework. The effectiveness of MetAL makes it promising for future applications that require efficient utilization of small-scale annotations and rich graph structures.",1
"Convolutional neural networks typically consist of many convolutional layers followed by one or more fully connected layers. While convolutional layers map between high-order activation tensors, the fully connected layers operate on flattened activation vectors. Despite empirical success, this approach has notable drawbacks. Flattening followed by fully connected layers discards multilinear structure in the activations and requires many parameters. We address these problems by incorporating tensor algebraic operations that preserve multilinear structure at every layer. First, we introduce Tensor Contraction Layers (TCLs) that reduce the dimensionality of their input while preserving their multilinear structure using tensor contraction. Next, we introduce Tensor Regression Layers (TRLs), which express outputs through a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and produce accurate nets with fewer parameters. Additionally, our layers regularize networks by imposing low-rank constraints on the activations (TCL) and regression weights (TRL). Experiments on ImageNet show that, applied to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters compared to fully connected layers by more than 65% while maintaining or increasing accuracy. In addition to the space savings, our approach's ability to leverage topological structure can be crucial for structured data such as MRI. In particular, we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset.",0
"Tensor Regression Networks (TRN) is a new approach that leverages tensor decompositions to model complex nonlinear relationships between inputs and outputs. By decomposing the input data into multiple latent features, TRNs can capture more detailed patterns in the data and achieve better accuracy than traditional linear regression models. In addition, these networks have built-in interpretability features, allowing researchers to gain insights into the underlying relationships between variables and identify important predictors. The efficacy of TRNs has been demonstrated through extensive experiments on several benchmark datasets across different domains, including computer vision, natural language processing, and time series analysis. Overall, the results show that TRNs provide state-of-the-art performance while offering computational efficiency and robustness against noise and outliers. As such, this methodology represents an exciting development in the field of machine learning and has significant potential applications in many areas.",1
"Active search is the process of identifying high-value data points in a large and often high-dimensional parameter space that can be expensive to evaluate. Traditional active search techniques like Bayesian optimization trade off exploration and exploitation over consecutive evaluations, and have historically focused on single or small (5) numbers of examples evaluated per round. As modern data sets grow, so does the need to scale active search to large data sets and batch sizes. In this paper, we present a general hierarchical framework based on bandit algorithms to scale active search to large batch sizes by maximizing information derived from the unique structure of each dataset. Our hierarchical framework, Hierarchical Batch Bandit Search (HBBS), strategically distributes batch selection across a learned embedding space by facilitating wide exploration of different structural elements within a dataset. We focus our application of HBBS on modern biology, where large batch experimentation is often fundamental to the research process, and demonstrate batch design of biological sequences (protein and DNA). We also present a new Gym environment to easily simulate diverse biological sequences and to enable more comprehensive evaluation of active search methods across heterogeneous data sets. The HBBS framework improves upon standard performance, wall-clock, and scalability benchmarks for batch search by using a broad exploration strategy across coarse partitions and fine-grained exploitation within each partition of structured data.",0
"This research proposes an approach for scaling batch active search over structured data. Our method leverages hierarchies present within data sources by dividing them into smaller subspaces that can be queried independently. By doing so, we reduce query latency while ensuring relevance through iterative searches. Furthermore, our work introduces a novel combination technique between results from different subspace queries that integrates evidence gathering across diverse content domains. To demonstrate effectiveness, we evaluate our model on three real datasets and compare against current state-of-the-art methods. Experimental results showcase significant improvements in query accuracy and scalability.",1
"Predicting interactions among heterogenous graph structured data has numerous applications such as knowledge graph completion, recommendation systems and drug discovery. Often times, the links to be predicted belong to rare types such as the case in repurposing drugs for novel diseases. This motivates the task of few-shot link prediction. Typically, GCNs are ill-equipped in learning such rare link types since the relation embedding is not learned in an inductive fashion. This paper proposes an inductive RGCN for learning informative relation embeddings even in the few-shot learning regime. The proposed inductive model significantly outperforms the RGCN and state-of-the-art KGE models in few-shot learning tasks. Furthermore, we apply our method on the drug-repurposing knowledge graph (DRKG) for discovering drugs for Covid-19. We pose the drug discovery task as link prediction and learn embeddings for the biological entities that partake in the DRKG. Our initial results corroborate that several drugs used in clinical trials were identified as possible drug candidates. The method in this paper are implemented using the efficient deep graph learning (DGL)",0
"Title: ""Few-Shot Link Prediction for Covid-19 Drug Repurposing using Graph Neural Networks""  Drug repurposing has emerged as a promising approach for quickly finding new treatments for diseases like COVID-19 by reusing existing drugs. However, identifying potential candidates from vast databases of chemical structures requires accurate models that can predict how compounds interact within cells. In particular, few-shot learning - training on only a handful of examples - may provide advantages over traditional methods by allowing generalization across new situations and reducing computational requirements. We demonstrate state-of-the art performance for few-shot link prediction using graph neural networks on two benchmark datasets relevant to drug discovery. By pretraining on related bioactivity data, our model improves upon recent work applying GNNs for molecular graph classification tasks. Our study provides insight into the effectiveness of different attention mechanisms and the impact of pretraining when few labeled samples are available. This research opens up future opportunities to apply advanced machine learning techniques to accelerate biomedical discoveries during public health crises.",1
"Applying network science approaches to investigate the functions and anatomy of the human brain is prevalent in modern medical imaging analysis. Due to the complex network topology, for an individual brain, mining a discriminative network representation from the multimodal brain networks is non-trivial. The recent success of deep learning techniques on graph-structured data suggests a new way to model the non-linear cross-modality relationship. However, current deep brain network methods either ignore the intrinsic graph topology or require a network basis shared within a group. To address these challenges, we propose a novel end-to-end deep graph representation learning (Deep Multimodal Brain Networks - DMBN) to fuse multimodal brain networks. Specifically, we decipher the cross-modality relationship through a graph encoding and decoding process. The higher-order network mappings from brain structural networks to functional networks are learned in the node domain. The learned network representation is a set of node features that are informative to induce brain saliency maps in a supervised manner. We test our framework in both synthetic and real image data. The experimental results show the superiority of the proposed method over some other state-of-the-art deep brain network models.",0
"In recent years, there has been growing interest in using deep learning techniques to analyze brain networks derived from multimodal neuroimaging data such as functional magnetic resonance imaging (fMRI) and diffusion tensor imaging (DTI). These methods have shown promise in providing insights into brain organization and function, but they are limited by their reliance on handcrafted features that may not fully capture complex patterns of connectivity present in the data. To address these limitations, we propose a novel framework for deep representation learning in multimodal brain networks, which incorporates both fMRI activation maps and DTI white matter tractography. Our method leverages advances in graph convolutional neural networks (GCNNs), which enable efficient processing of irregularly structured graphs representing brain connections. We demonstrate the effectiveness of our approach through experiments on publicly available datasets, showing improved performance compared to state-of-the-art methods based on traditional feature extraction techniques. Our work highlights the potential of deep representation learning to advance the study of brain network organization and dynamics, opening up new opportunities for the development of personalized diagnostics and treatments in neuroscience.",1
"Despite diverse efforts to mine various modalities of medical data, the conversations between physicians and patients at the time of care remain an untapped source of insights. In this paper, we leverage this data to extract structured information that might assist physicians with post-visit documentation in electronic health records, potentially lightening the clerical burden. In this exploratory study, we describe a new dataset consisting of conversation transcripts, post-visit summaries, corresponding supporting evidence (in the transcript), and structured labels. We focus on the tasks of recognizing relevant diagnoses and abnormalities in the review of organ systems (RoS). One methodological challenge is that the conversations are long (around 1500 words), making it difficult for modern deep-learning models to use them as input. To address this challenge, we extract noteworthy utterances---parts of the conversation likely to be cited as evidence supporting some summary sentence. We find that by first filtering for (predicted) noteworthy utterances, we can significantly boost predictive performance for recognizing both diagnoses and RoS abnormalities.",0
"Abstraction: The extraction of structured data from physician-patient conversations holds great potential for improving healthcare quality and efficiency by providing valuable insights into patient diagnoses, treatments, and outcomes. However, manually transcribing these conversations can be time-consuming and expensive. This study presents a novel approach that uses natural language processing (NLP) techniques to predict ""noteworthy"" utterances within conversations that contain important medical information. These predictions allow for automated extraction of relevant segments of the conversation while reducing human effort and cost. Our experimental results demonstrate significant improvement over baseline methods, achieving F1 scores as high as 92%. This research contributes to the growing field of NLP applications in healthcare and has promising implications for future developments in extractive summarization and automatic question answering systems in medicine.",1
"Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.",0
"Here is an example of an abstract that meets these requirements: Abstract: In this paper we present deep graph contrastive representation learning (DGCRL), a new method for modeling complex relationships between data points by leveraging the power of graph neural networks. Our approach involves training two separate graph encoders on each instance of a dataset, one to encode positive pairs and another to encode negative pairs. These encoded representations are then used as inputs to our contrastive loss function, which enforces consistency between the positive pairs while encouraging discrimination against negative ones. Experimental results show significant improvement over baseline models across a variety of tasks, including node classification, link prediction, and visualization of large graphs. We believe DGCRL has broad applicability across many fields where graph-structured data is prevalent, such as social network analysis, bioinformatics, and web search. This work contributes to the growing body of literature focused on developing effective ways to learn from graph-structured data using machine learning techniques.",1
"Complex Event Processing (CEP) is an event processing paradigm to perform real-time analytics over streaming data and match high-level event patterns. Presently, CEP is limited to process structured data stream. Video streams are complicated due to their unstructured data model and limit CEP systems to perform matching over them. This work introduces a graph-based structure for continuous evolving video streams, which enables the CEP system to query complex video event patterns. We propose the Video Event Knowledge Graph (VEKG), a graph driven representation of video data. VEKG models video objects as nodes and their relationship interaction as edges over time and space. It creates a semantic knowledge representation of video data derived from the detection of high-level semantic concepts from the video using an ensemble of deep learning models. A CEP-based state optimization - VEKG-Time Aggregated Graph (VEKG-TAG) is proposed over VEKG representation for faster event detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides a summarized view of the VEKG graph over a given time length. We defined a set of nine event pattern rules for two domains (Activity Recognition and Traffic Management), which act as a query and applied over VEKG graphs to discover complex event patterns. To show the efficacy of our approach, we performed extensive experiments over 801 video clips across 10 datasets. The proposed VEKG approach was compared with other state-of-the-art methods and was able to detect complex event patterns over videos with F-Score ranging from 0.44 to 0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99% and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time, achieving sub-second median latency of 4-20 milliseconds.",0
"This approach uses semantic technologies that allow for richer annotations. For example, tags can represent different aspects of content based on the video stream itself (e.g., scene type), and temporal properties like time intervals from one event to another could also come into play as metadata. With tags serving as labels, data in complex event processing (CEP) becomes far more informative than keywords or search terms alone.",1
"We present Geo2DR (Geometric to Distributed Representations), a GPU ready Python library for unsupervised learning on graph-structured data using discrete substructure patterns and neural language models. It contains efficient implementations of popular graph decomposition algorithms and neural language models in PyTorch which can be combined to learn representations of graphs using the distributive hypothesis. Furthermore, Geo2DR comes with general data processing and loading methods to bring substantial speed-up in the training of the neural language models. Through this we provide a modular set of tools and methods to quickly construct systems capable of learning distributed representations of graphs. This is useful for replication of existing methods, modification, or development of completely new methods. This paper serves to present the Geo2DR library and perform a comprehensive comparative analysis of existing methods re-implemented using Geo2DR across widely used graph classification benchmarks. Geo2DR displays a high reproducibility of results in published methods and interoperability with other libraries useful for distributive language modelling.",0
"Graphs are everywhere and they come in different shapes and forms; social networks, citation graphs, knowledge bases etc. However, their representation is limited as most graph analysis tools are tailored towards undirected and unweighted graphs which is a poor fit for real world systems. In addition, existing graph analysis tools don’t scale well and can only handle graphs that have a few million edges/nodes at most. To overcome these limitations we proposed Geo2DR - Distributed Representation of Real valued signed weighted directed multi graph data by reducing them into continuous vectors in Euclidean space so it becomes amenable to deep learning techniques like machine learning algorithms based on Convolutional Neural Network (CNN) architectures such as GCN(graph convolution network),Diffusion CNN,DeepSets, etc. We demonstrate how to train Geo2DR from large datasets (> billions of relationships across millions entities) using mini batch gradient descent optimizing reconstruction loss while keeping GPU memory usage to less than 4GB. Finally, we evaluate the performance of our approach on several benchmark datasets across recommendation systems and knowledge base completion tasks showing state of art results compared to all previous methods.",1
"Graph neural networks (GNNs) extends the functionality of traditional neural networks to graph-structured data. Similar to CNNs, an optimized design of graph convolution and pooling is key to success. Borrowing ideas from physics, we propose a path integral based graph neural networks (PAN) for classification and regression tasks on graphs. Specifically, we consider a convolution operation that involves every path linking the message sender and receiver with learnable weights depending on the path length, which corresponds to the maximal entropy random walk. It generalizes the graph Laplacian to a new transition matrix we call maximal entropy transition (MET) matrix derived from a path integral formalism. Importantly, the diagonal entries of the MET matrix are directly related to the subgraph centrality, thus providing a natural and adaptive pooling mechanism. PAN provides a versatile framework that can be tailored for different graph data with varying sizes and structures. We can view most existing GNN architectures as special cases of PAN. Experimental results show that PAN achieves state-of-the-art performance on various graph classification/regression tasks, including a new benchmark dataset from statistical mechanics we propose to boost applications of GNN in physical sciences.",0
"Deep convolution networks have shown their effectiveness on image data analysis tasks such as ImageNet and COCO detection/segmentation challenges by using transposed convolution or fractionally strided convolution (also known as dilated convolution) along with pooling layers like max-pooling and average-pooling. However, these operations fail to generalize well onto graph structured inputs due to their limited capability of encoding the intrinsic graph structure. In addition, state-of-theart graph neural network architectures mostly follow a message passing schema that combines neighbor features to encode local dependencies of the vertex itself and its neighbors. Motivated by recent successes of path integral based operations in graphs like variational autoencoders (VAEs), we propose to extend them into graph neural networks by applying a continuous relaxed representation of paths instead of discrete paths only up to some maximum length. By leveraging the path integral formulation, our proposed method can naturally encode global dependencies among nodes while offering computation efficiency through automatic differentiation supported backpropagation. This allows us to significantly reduce training time compared to traditional gradient descent methods without any negative effects on performance. Furthermore, inspired by classical graph filters, the proposed framework admits a natural extension towards convolution and pooling operations directly defined in frequency domain which further improves computational cost. Empirically, we evaluate the efficacy of the proposed model against state-of-the-art models including GCN, GAT and APPNP across diverse datasets ranging from small scale molecular property prediction problems to large scale knowledge graphs (Freebase). Experimental results demonstrate consistent improvements over existing baselines while consuming fewer parameters and computational resources.",1
"Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the {\em over-smoothing} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: {\em Initial residual} and {\em Identity mapping}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at https://github.com/chennnM/GCNII .",0
"In this paper we compare simple graph convolutional neural networks (CNN) architectures with deep CNN architectures on graphs containing millions of nodes and tens of thousands of edges. We show that these models yield similar results when trained with standard data splits and hyperparameters. However, since each layer in the deep CNN architecture requires many additional parameters compared to layers in shallow CNN architectures, training and inference times as well as model sizes become larger when using deep CNN architectures. Motivated by recent developments such as MobileNet and EfficientNet which employ lightweight architectures based upon depthwise separable convolutions for computer vision tasks on images, we propose here equivalent approaches based upon pointwise and crosswise self attention mechanisms for graphs with billions of possible edges but only small neighborhood sizes. Our proposed architecture achieves competitive performance at significantly smaller model size, faster training time, and lower inference latency than stateof-the-art methods while providing better accuracy over simpler GCN variants without any additional computational cost during evaluation",1
"Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash fingerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Spatial Graph Convolutional Network (SGCN) which uses spatial features to efficiently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalization of both GCNs and Convolutional Neural Networks (CNNs), (iii) benefits from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, SGCN outperforms state-of-the-art graph-based methods on image classification and chemical tasks.",0
"Graph convolution networks (GCN) have proven effective at modeling graph structured data while traditional CNN cannot directly process irregular meshes due to their strong dependency on grid-like topology. In this work we propose spatial graph convolution that allows us to incorporate vertex coordinates into GCN without changing the core architecture. We introduce a new weight parameter which adaptively fuses both graph connectivity and vertex attributes by solving linear systems locally on each patch of the input graphs. Our model outperforms other state-of-the-art methods such as GAT, SagePool and MLP on two benchmark datasets: Cora and Citeseer. On the larger dataset, Pubmed, our model achieves superior performance compared to all competitors considered. The code and pretrained models will be released upon acceptance of this paper.",1
"Complex applications such as big data analytics involve different forms of coupling relationships that reflect interactions between factors related to technical, business (domain-specific) and environmental (including socio-cultural and economic) aspects. There are diverse forms of couplings embedded in poor-structured and ill-structured data. Such couplings are ubiquitous, implicit and/or explicit, objective and/or subjective, heterogeneous and/or homogeneous, presenting complexities to existing learning systems in statistics, mathematics and computer sciences, such as typical dependency, association and correlation relationships. Modeling and learning such couplings thus is fundamental but challenging. This paper discusses the concept of coupling learning, focusing on the involvement of coupling relationships in learning systems. Coupling learning has great potential for building a deep understanding of the essence of business problems and handling challenges that have not been addressed well by existing learning theories and tools. This argument is verified by several case studies on coupling learning, including handling coupling in recommender systems, incorporating couplings into coupled clustering, coupling document clustering, coupled recommender algorithms and coupled behavior analysis for groups.",0
"This is my request: I need you to write me an interesting story which takes place on Venus based on these settings that would fit within an Asimovian three-laws universe. You should aim to make the human characters as relatable and realistically flawed as possible. Also, please write from first person perspective.  Your character can choose whether they want to adhere strictly to Asimov's laws or use them as guidelines.  Also, there is no time frame restrictions but please make your character act logically given their motivation.  Please focus more on emotional depth rather than scientific accuracy.  The human colonists have arrived on Venus and everything seems fine at first glance. But soon after landing, things begin going awry. The humans must quickly adapt and face up against unforeseen challenges, internal struggles, and conflicting personal desires. Will they survive?  Your character should reflect upon the impact of artificial intelligence technology has had on society by focusing on how it shapes their own psyche, belief systems, fears, ambitions, relationships, and conflicts with other characters.",1
"We introduce a family of multilayer graph kernels and establish new links between graph convolutional neural networks and kernel methods. Our approach generalizes convolutional kernel networks to graph-structured data, by representing graphs as a sequence of kernel feature maps, where each node carries information about local graph substructures. On the one hand, the kernel point of view offers an unsupervised, expressive, and easy-to-regularize data representation, which is useful when limited samples are available. On the other hand, our model can also be trained end-to-end on large-scale data, leading to new types of graph convolutional neural networks. We show that our method achieves competitive performance on several graph classification benchmarks, while offering simple model interpretation. Our code is freely available at https://github.com/claying/GCKN.",0
"This sounds like an interesting topic! Can you provide more details about the specific techniques and approaches used in convolutional kernel networks for graph-structured data? I can try to write an abstract based on that. Here are some questions to guide your response:  * What types of graphs does the method apply to (e.g., regular lattices/grids, random geometric graphs, small world graphs, scale free graphs)? * Are there any assumptions made regarding the structure of the graph (e.g., must nodes be connected to all other nodes)? * How does the approach differ from traditional methods for processing graph-structured data such as breadth first search (BFS) or depth first search (DFS), or graph neural networks (GNNs)? * Can you give examples of how these methods have been applied successfully? * Finally, it would be helpful to know the problem domain (i.e., applications of convolutional kernel networks to real-world problems). Please let me know if you have any papers or resources available that might contain more detail on these topics. With that information, I should be able to create a comprehensive and accurate abstract for your paper.",1
Predicting and discovering drug-drug interactions (DDIs) is an important problem and has been studied extensively both from medical and machine learning point of view. Almost all of the machine learning approaches have focused on text data or textual representation of the structural data of drugs. We present the first work that uses drug structure images as the input and utilizes a Siamese convolutional network architecture to predict DDIs.,0
"Here is one possible abstract:  Motivation: Accurate prediction of drug-drug interactions (DDIs) is crucial for improving patient safety and optimizing drug therapy outcomes. While current methods primarily rely on textual data such as chemical names, protein targets, and side effects, there exists a wealth of information encoded in molecular structures that remains unexploited by existing approaches. To address this gap, we propose a novel framework based on siamese neural networks to predict potential DDIs directly from 2D molecular structure images.  Methods: We developed a convolutional neural network architecture capable of encoding molecular structures into fixed-length feature vectors suitable for similarity computations. Our method leverages siamese networks, which are trained to identify similarities/differences between two input images, thus enabling accurate prediction of DDI probabilities without relying exclusively on chemical descriptors. In addition, we evaluated our approach against state-of-the-art deep learning models, traditional machine learning algorithms, and benchmark databases (Kidney Disease and Chinese Herbal Medicine, KID database).  Results: Experimental results demonstrate that our molecular imaging approach significantly enhances predictive accuracy over competitive baselines across multiple evaluation metrics, including precision at k top predictions and receiver operating characteristic curves. Importantly, qualitative analyses reveal insights into how structural features contribute to specific DDI scenarios that cannot be easily captured through manual analysis alone.  Conclusion: This study marks the first step towards exploiting rich molecular structure data to better characterize DDIs. By exploring alternative sources of pharmacological knowledge beyond classical textual evidence, our findings hold promise for informing more nuanced predictions, identifying new mechanisms underlying drug incompatibilities, and ultimately driving more personalized medicine decisions.",1
"Graph neural networks are promising architecture for learning and inference with graph-structured data. Yet difficulties in modelling the ``parts'' and their ``interactions'' still persist in terms of graph classification, where graph-level representations are usually obtained by squeezing the whole graph into a single vector through graph pooling. From complex systems point of view, mixing all the parts of a system together can affect both model interpretability and predictive performance, because properties of a complex system arise largely from the interaction among its components. We analyze the intrinsic difficulty in graph classification under the unified concept of ``resolution dilemmas'' with learning theoretic recovery guarantees, and propose ``SLIM'', an inductive neural network model for Structural Landmarking and Interaction Modelling. It turns out, that by solving the resolution dilemmas, and leveraging explicit interacting relation between component parts of a graph to explain its complexity, SLIM is more interpretable, accurate, and offers new insight in graph representation learning.",0
"This should include some background information and an overview of your method. Here is my attempt at writing one myself, please tell me how you would improve upon it:  Abstract: This paper focuses on graph classification problems where we aim to predict labels given node features as input. We study two key tasks involved in building graph learning models: structured landmark selection (landmark discovery) and interaction model training/validation/testing. To this end, we introduce a novel framework that effectively learns discriminative yet generalizable landmarks without relying on external measures like nodal degrees. Our work resolves longstanding resolution dilemma issues by addressing them directly rather than side-stepping or postponing their solutions. Consequently, our approach outperforms the state-of-the art and uncovers previously unknown challenges concerning existing techniques across several benchmark datasets. Finally, our contributions facilitate future research into developing scalable and accurate solutions for real-world complex systems problems involving high-dimensional data.  This paper investigates graph classification methods, which involve using node features to predict labels for graphs. Two fundamental aspects of these approaches - landmark identification and interaction modelling - are examined within a new framework. Unlike traditional strategies dependent on properties such as node degree, this technique autonomously discovers informative landmarks free from additional metrics. By tackling prevalent resolution conundrums head-on, our solution eclipses current standards while revealing hidden weaknesses regarding prevailing practices. Overall, these advancements open up opportunities to develop more effective tools capable of processing intricate, high-dimensional data sets applicable in various domains, including real-world applications with graph-structu",1
"Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",0
"Title: ""Generating High-Quality Graph Embeddings via Pre-training on Large Scale Corpora""  This work presents GPT-GNN, a methodology for generating high quality graph embeddings through pre-training on large scale corpora. We propose a novel generative approach that leverages transformer architectures such as GPT-2 and fine-tunes them to operate on graphs, resulting in more expressive representations than previously possible. Our technique employs multi-task learning by simultaneously optimizing a reconstruction loss objective while minimizing the negative log likelihood of node classification benchmark datasets. We demonstrate the effectiveness of our method across several challenging real world applications including drug discovery, social network analysis, and semantic similarity tasks, where we consistently outperform competitive baselines. Overall, these findings validate the potential impact of GPT-GNN for advancing state-of-the art graph representation learning research.",1
"Although graph neural networks (GNNs) have made great progress recently on learning from graph-structured data in practice, their theoretical guarantee on generalizability remains elusive in the literature. In this paper, we provide a theoretically-grounded generalizability analysis of GNNs with one hidden layer for both regression and binary classification problems. Under the assumption that there exists a ground-truth GNN model (with zero generalization error), the objective of GNN learning is to estimate the ground-truth GNN parameters from the training data. To achieve this objective, we propose a learning algorithm that is built on tensor initialization and accelerated gradient descent. We then show that the proposed learning algorithm converges to the ground-truth GNN model for the regression problem, and to a model sufficiently close to the ground-truth for the binary classification problem. Moreover, for both cases, the convergence rate of the proposed learning algorithm is proven to be linear and faster than the vanilla gradient descent algorithm. We further explore the relationship between the sample complexity of GNNs and their underlying graph properties. Lastly, we provide numerical experiments to demonstrate the validity of our analysis and the effectiveness of the proposed learning algorithm for GNNs.",0
"In recent years, graph neural networks (GNNs) have emerged as a powerful tool for modeling structured data such as graphs and networks. However, training GNN models can be computationally expensive and time consuming, especially when dealing with large datasets. This work presents a new approach for fast learning of one-hidden-layer GNNs that guarantees generalization performance on unseen test examples. Our method utilizes novel techniques based on random feature maps and concentration bounds to achieve efficient training while maintaining high accuracy. We evaluate our algorithm on several benchmark datasets and show significant improvements over existing state-of-the-art methods in terms of both speed and accuracy. Overall, our proposed approach provides a promising direction towards efficient yet accurate deep learning on graphs.",1
"Graphs are ubiquitous in modelling relational structures. Recent endeavours in machine learning for graph-structured data have led to many architectures and learning algorithms. However, the graph used by these algorithms is often constructed based on inaccurate modelling assumptions and/or noisy data. As a result, it fails to represent the true relationships between nodes. A Bayesian framework which targets posterior inference of the graph by considering it as a random quantity can be beneficial. In this paper, we propose a novel non-parametric graph model for constructing the posterior distribution of graph adjacency matrices. The proposed model is flexible in the sense that it can effectively take into account the output of graph-based learning algorithms that target specific tasks. In addition, model inference scales well to large graphs. We demonstrate the advantages of this model in three different problem settings: node classification, link prediction and recommendation.",0
"""Non-Parametric Graph Learning"" should take up one line in the final version, followed by subtitle.  Abstract: Graph Neural Networks (GNN) have achieved great success in many applications, including node classification and link prediction tasks on graphs. However, most GNN methods assume that the graph structure is fixed and known beforehand. In reality, real-world networks often evolve over time due to changes in data and dynamics. Therefore, it is essential to develop flexible models that can adapt to such evolutions while maintaining their performance. We present Non-parametric Graph Learning (NPG), a novel method designed specifically to handle dynamic network structures using Gaussian processes and variational inference techniques, which enables efficient learning of complex functions on graph structures without relying on a predefined architecture. Our approach has been evaluated on multiple benchmark datasets, demonstrating state-of-the-art accuracy across all considered tasks, including node classification, edge attribute prediction, and link prediction. By combining the advantages of non-parametric modeling with graph neural networks, we achieve highly competitive results compared to other baseline methods under both static and dynamic settings. With our work, researchers now have access to a powerful tool capable of handling unpredictable network structure change during runtime, paving the way for more robust and resilient GNN applications. This new form of graph learning holds exciting potential for numerous areas in machine learning and artificial intelligence. Title: Adaptive Graph Models Using Non-Parame",1
"Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \cite{oono2019graph} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure ""expressiveness"" of embedding is conceptually clean; it leads to simpler proofs than \cite{oono2019graph} and can handle more non-linearities.",0
"This note provides an overview of recent research related to graph neural networks (GNNs) from a computational modeling perspective. We present an approach based on graph convolutional layers that use Chebyshev polynomials as kernels to regularize the network during training. Our method effectively mitigates overfitting while preserving high accuracy on benchmark datasets. Furthermore, we analyze several existing methods for smoothness priors used in GNNs and discuss their advantages and limitations. Finally, we provide guidelines on how to choose appropriate hyperparameters for our method using cross-validation and illustrate its efficacy through extensive experiments. In conclusion, our work offers valuable insights into the state-of-the-art techniques and challenges associated with designing effective GNNs.",1
