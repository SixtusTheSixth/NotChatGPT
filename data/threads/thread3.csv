"We present Supervision by Registration and Triangulation (SRT), an unsupervised approach that utilizes unlabeled multi-view video to improve the accuracy and precision of landmark detectors. Being able to utilize unlabeled data enables our detectors to learn from massive amounts of unlabeled data freely available and not be limited by the quality and quantity of manual human annotations. To utilize unlabeled data, there are two key observations: (1) the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. (2) the detections of the same landmark in multiple synchronized and geometrically calibrated views should correspond to a single 3D point, i.e., multi-view consistency. Registration and multi-view consistency are sources of supervision that do not require manual labeling, thus it can be leveraged to augment existing training data during detector training. End-to-end training is made possible by differentiable registration and 3D triangulation modules. Experiments with 11 datasets and a newly proposed metric to measure precision demonstrate accuracy and precision improvements in landmark detection on both images and video. Code is available at https://github.com/D-X-Y/landmark-detection.",0
"We introduce a novel approach called Supervision by Registration and Triangulation (SRT), which employs unlabeled multi-view video to enhance the precision and accuracy of landmark detectors in an unsupervised manner. With the use of unlabeled data, our detectors can learn from vast amounts of freely available data without being restricted by the quality and quantity of human annotations. To leverage unlabeled data, we have made two key observations: (1) the detection of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow, and (2) the detection of the same landmark in multiple synchronized and geometrically calibrated views should correspond to a single 3D point, i.e., multi-view consistency. These sources of supervision do not require manual labeling, making it possible to supplement existing training data during detector training. End-to-end training is feasible due to differentiable registration and 3D triangulation modules. Our experiments on 11 datasets using a novel precision metric show improvements in accuracy and precision in landmark detection on both images and video. The code for this approach is available at https://github.com/D-X-Y/landmark-detection.",1
"Weighted Gaussian Curvature is an important measurement for images. However, its conventional computation scheme has low performance, low accuracy and requires that the input image must be second order differentiable. To tackle these three issues, we propose a novel discrete computation scheme for the weighted Gaussian curvature. Our scheme does not require the second order differentiability. Moreover, our scheme is more accurate, has smaller support region and computationally more efficient than the conventional schemes. Therefore, our scheme holds promise for a large range of applications where the weighted Gaussian curvature is needed, for example, image smoothing, cartoon texture decomposition, optical flow estimation, etc.",0
"The measurement of Weighted Gaussian Curvature is crucial in image analysis, but the conventional method for computing it is flawed in terms of low accuracy, poor performance, and the necessity of having a second order differentiable input image. To overcome these challenges, we have developed a new discrete computation approach for the weighted Gaussian curvature that does not require second order differentiability, is more precise, has a smaller support region, and is more computationally efficient than the conventional method. As a result, our approach is highly promising for a wide range of applications that require weighted Gaussian curvature, such as image smoothing, cartoon texture decomposition, and optical flow estimation.",1
"Self-driving cars and other autonomous vehicles need to detect and track objects in camera images. We present a simple online tracking algorithm that is based on a constant velocity motion model with a Kalman filter, and an assignment heuristic. The assignment heuristic relies on four metrics: An embedding vector that describes the appearance of objects and can be used to re-identify them, a displacement vector that describes the object movement between two consecutive video frames, the Mahalanobis distance between the Kalman filter states and the new detections, and a class distance. These metrics are combined with a linear SVM, and then the assignment problem is solved by the Hungarian algorithm. We also propose an efficient CNN architecture that estimates these metrics. Our multi-frame model accepts two consecutive video frames which are processed individually in the backbone, and then optical flow is estimated on the resulting feature maps. This allows the network heads to estimate the displacement vectors. We evaluate our approach on the challenging BDD100K tracking dataset. Our multi-frame model achieves a good MOTA value of 39.1% with low localization error of 0.206 in MOTP. Our fast single-frame model achieves an even lower localization error of 0.202 in MOTP, and a MOTA value of 36.8%.",0
"A tracking algorithm has been developed for self-driving cars and other autonomous vehicles to detect and follow objects in camera images. The algorithm is based on a constant velocity motion model with a Kalman filter, and an assignment heuristic that uses four metrics: an embedding vector, a displacement vector, the Mahalanobis distance, and a class distance. These metrics are combined with a linear SVM and solved with the Hungarian algorithm. An efficient CNN architecture is proposed to estimate these metrics, which accepts two consecutive video frames that are processed individually in the backbone, and then optical flow is estimated on the resulting feature maps. The approach was evaluated on the BDD100K tracking dataset, where the multi-frame model achieved a MOTA value of 39.1% with low localization error of 0.206 in MOTP, and the fast single-frame model achieved a lower localization error of 0.202 in MOTP and a MOTA value of 36.8%.",1
"Recently, flow-based methods have achieved promising success in video frame interpolation. However, electron microscopic (EM) images suffer from unstable image quality, low PSNR, and disorderly deformation. Existing flow-based interpolation methods cannot precisely compute optical flow for EM images since only predicting each position's unique offset. To overcome these problems, we propose a novel interpolation framework for EM images that progressively synthesizes interpolated features in a coarse-to-fine manner. First, we extract missing intermediate features by the proposed temporal spatial-adaptive (TSA) interpolation module. The TSA interpolation module aggregates temporal contexts and then adaptively samples the spatial-related features with the proposed residual spatial adaptive block. Second, we introduce a stacked deformable refinement block (SDRB) further enhance the reconstruction quality, which is aware of the matching positions and relevant features from input frames with the feedback mechanism. Experimental results demonstrate the superior performance of our approach compared to previous works, both quantitatively and qualitatively.",0
"Video frame interpolation has seen promising success with flow-based methods. However, electron microscopic (EM) images present challenges due to unstable image quality, low PSNR, and disorderly deformation. The current flow-based methods cannot accurately compute optical flow for EM images as they only predict unique offsets for each position. To address these issues, we propose a progressive interpolation framework for EM images that synthesizes features in a coarse-to-fine manner. Our approach uses a temporal spatial-adaptive (TSA) interpolation module to extract missing intermediate features by aggregating temporal contexts and adaptively sampling spatial-related features with a proposed residual spatial adaptive block. We also introduce a stacked deformable refinement block (SDRB) to enhance the reconstruction quality. This block is aware of the matching positions and relevant features from input frames with a feedback mechanism. Our experimental results demonstrate the superior performance of our approach compared to previous works, both quantitatively and qualitatively.",1
"Optical flow estimation with occlusion or large displacement is a problematic challenge due to the lost of corresponding pixels between consecutive frames. In this paper, we discover that the lost information is related to a large quantity of motion features (more than 40%) computed from the popular discriminative cost-volume feature would completely vanish due to invalid sampling, leading to the low efficiency of optical flow learning. We call this phenomenon the Vanishing Cost Volume Problem. Inspired by the fact that local motion tends to be highly consistent within a short temporal window, we propose a novel iterative Motion Feature Recovery (MFR) method to address the vanishing cost volume via modeling motion consistency across multiple frames. In each MFR iteration, invalid entries from original motion features are first determined based on the current flow. Then, an efficient network is designed to adaptively learn the motion correlation to recover invalid features for lost-information restoration. The final optical flow is then decoded from the recovered motion features. Experimental results on Sintel and KITTI show that our method achieves state-of-the-art performances. In fact, MFR currently ranks second on Sintel public website.",0
"The challenge of estimating optical flow in the presence of occlusion or significant displacement is problematic, as it results in the loss of corresponding pixels between consecutive frames. This paper explores the relationship between lost information and motion features, revealing that over 40% of features computed from the common discriminative cost-volume feature disappear due to invalid sampling, reducing optical flow efficiency. This is referred to as the Vanishing Cost Volume Problem. To address this issue, an iterative Motion Feature Recovery (MFR) method is proposed, leveraging the consistency of local motion within a short temporal window to recover lost information. Invalid motion features are first identified based on the current flow, and an efficient network is designed to recover these features via motion correlation. The final optical flow is then decoded from the recovered motion features. Experimental results on Sintel and KITTI datasets demonstrate that MFR achieves state-of-the-art performance, currently ranking second on the Sintel public website.",1
"Micro-Expression Recognition has become challenging, as it is extremely difficult to extract the subtle facial changes of micro-expressions. Recently, several approaches proposed several expression-shared features algorithms for micro-expression recognition. However, they do not reveal the specific discriminative characteristics, which lead to sub-optimal performance. This paper proposes a novel Feature Refinement ({FR}) with expression-specific feature learning and fusion for micro-expression recognition. It aims to obtain salient and discriminative features for specific expressions and also predict expression by fusing the expression-specific features. FR consists of an expression proposal module with attention mechanism and a classification branch. First, an inception module is designed based on optical flow to obtain expression-shared features. Second, in order to extract salient and discriminative features for specific expression, expression-shared features are fed into an expression proposal module with attention factors and proposal loss. Last, in the classification branch, labels of categories are predicted by a fusion of the expression-specific features. Experiments on three publicly available databases validate the effectiveness of FR under different protocol. Results on public benchmarks demonstrate that our FR provides salient and discriminative information for micro-expression recognition. The results also show our FR achieves better or competitive performance with the existing state-of-the-art methods on micro-expression recognition.",0
"Recognizing micro-expressions has become a challenge due to the difficulty in detecting subtle facial changes. Previous approaches have proposed expression-shared feature algorithms, but they fail to identify specific discriminative characteristics, leading to sub-optimal performance. To address this issue, this paper suggests a new approach called Feature Refinement ({FR}) that involves expression-specific feature learning and fusion. The aim is to obtain distinctive features for specific expressions and predict expressions by merging expression-specific features. FR consists of two modules: an inception module for obtaining expression-shared features and an expression proposal module with attention mechanism and proposal loss to extract salient and discriminative features for specific expressions. In the classification branch, a fusion of the expression-specific features predicts category labels. Experiments on three publicly available databases validate the effectiveness of FR under different protocols. The results demonstrate that FR provides valuable information for micro-expression recognition and performs better than or similarly to existing state-of-the-art methods.",1
"The objective of this paper is visual-only self-supervised video representation learning. We make the following contributions: (i) we investigate the benefit of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (InfoNCE) training, showing that this form of supervised contrastive learning leads to a clear improvement in performance; (ii) we propose a novel self-supervised co-training scheme to improve the popular infoNCE loss, exploiting the complementary information from different views, RGB streams and optical flow, of the same data source by using one view to obtain positive class samples for the other; (iii) we thoroughly evaluate the quality of the learnt representation on two different downstream tasks: action recognition and video retrieval. In both cases, the proposed approach demonstrates state-of-the-art or comparable performance with other self-supervised approaches, whilst being significantly more efficient to train, i.e. requiring far less training data to achieve similar performance.",0
"This paper focuses on the self-supervised learning of video representation using only visual cues. Our contributions are threefold: firstly, we examine the benefits of incorporating semantic-class positives into instance-based Info Noise Contrastive Estimation (InfoNCE) training, which demonstrates a significant improvement in performance. Secondly, we propose a new self-supervised co-training approach to enhance the popular infoNCE loss by leveraging information from different views, including RGB streams and optical flow, of the same data source. Thirdly, we conduct a comprehensive evaluation of the learned representation on two different downstream tasks, namely action recognition and video retrieval, where our approach outperforms or performs comparably with other self-supervised methods while being more efficient in training, requiring less training data to achieve similar results.",1
"Temporal information is essential to learning effective policies with Reinforcement Learning (RL). However, current state-of-the-art RL algorithms either assume that such information is given as part of the state space or, when learning from pixels, use the simple heuristic of frame-stacking to implicitly capture temporal information present in the image observations. This heuristic is in contrast to the current paradigm in video classification architectures, which utilize explicit encodings of temporal information through methods such as optical flow and two-stream architectures to achieve state-of-the-art performance. Inspired by leading video classification architectures, we introduce the Flow of Latents for Reinforcement Learning (Flare), a network architecture for RL that explicitly encodes temporal information through latent vector differences. We show that Flare (i) recovers optimal performance in state-based RL without explicit access to the state velocity, solely with positional state information, (ii) achieves state-of-the-art performance on pixel-based challenging continuous control tasks within the DeepMind control benchmark suite, namely quadruped walk, hopper hop, finger turn hard, pendulum swing, and walker run, and is the most sample efficient model-free pixel-based RL algorithm, outperforming the prior model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step benchmarks, respectively, and (iv), when augmented over rainbow DQN, outperforms this state-of-the-art level baseline on 5 of 8 challenging Atari games at 100M time step benchmark.",0
"To effectively learn policies with Reinforcement Learning (RL), it is necessary to have access to temporal information. However, current state-of-the-art RL algorithms either assume that this information is included in the state space or, when learning from pixels, use frame-stacking as a simple heuristic to capture the temporal information present in the image observations. This differs from the current trend in video classification architectures, which use explicit encodings of temporal information to achieve optimal performance. To address this, we introduce the Flow of Latents for Reinforcement Learning (Flare), an RL network architecture that encodes temporal information through latent vector differences. Our results show that Flare achieves optimal performance in state-based RL without explicit access to state velocity, and it outperforms prior model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step benchmarks, respectively, on challenging continuous control tasks within the DeepMind control benchmark suite. Additionally, when combined with rainbow DQN, Flare outperforms the state-of-the-art level baseline on 5 of 8 challenging Atari games at the 100M time step benchmark.",1
"In this paper, we propose a \textbf{Tr}ansformer-based RGB-D \textbf{e}gocentric \textbf{a}ction \textbf{r}ecognition framework, called Trear. It consists of two modules, inter-frame attention encoder and mutual-attentional fusion block. Instead of using optical flow or recurrent units, we adopt self-attention mechanism to model the temporal structure of the data from different modalities. Input frames are cropped randomly to mitigate the effect of the data redundancy. Features from each modality are interacted through the proposed fusion block and combined through a simple yet effective fusion operation to produce a joint RGB-D representation. Empirical experiments on two large egocentric RGB-D datasets, THU-READ and FPHA, and one small dataset, WCVS, have shown that the proposed method outperforms the state-of-the-art results by a large margin.",0
"The paper presents a new framework called Trear for RGB-D egocentric action recognition based on the Transformer model. The framework consists of two modules: an inter-frame attention encoder and a mutual-attentional fusion block. Instead of using optical flow or recurrent units, the framework adopts a self-attention mechanism to model the temporal structure of data from different modalities. To reduce data redundancy, input frames are randomly cropped. The proposed fusion block combines features from each modality through a simple and effective fusion operation to produce a joint RGB-D representation. Empirical experiments on three datasets, including two large egocentric RGB-D datasets (THU-READ and FPHA) and one small dataset (WCVS), demonstrate that the proposed method outperforms state-of-the-art results by a significant margin.",1
"Optical flow techniques are becoming increasingly performant and robust when estimating motion in a scene, but their performance has yet to be proven in the area of facial expression recognition. In this work, a variety of optical flow approaches are evaluated across multiple facial expression datasets, so as to provide a consistent performance evaluation. The aim of this work is not to propose a new expression recognition technique, but to understand better the adequacy of existing state-of-the art optical flow for encoding facial motion in the context of facial expression recognition. Our evaluations highlight the fact that motion approximation methods used to overcome motion discontinuities have a significant impact when optical flows are used to characterize facial expressions.",0
"Although optical flow techniques are improving and becoming more reliable in estimating motion in a scene, their effectiveness in recognizing facial expressions has not been proven. This study evaluates various optical flow methods on multiple facial expression datasets to provide a consistent evaluation. The goal is not to introduce a new expression recognition technique, but to examine the suitability of current state-of-the-art optical flow for encoding facial motion in the context of facial expression recognition. Our findings reveal that motion approximation methods used to address motion discontinuities have a considerable influence on the characterization of facial expressions with optical flows.",1
"Video facial expression recognition is useful for many applications and received much interest lately. Although some solutions give really good results in a controlled environment (no occlusion), recognition in the presence of partial facial occlusion remains a challenging task. To handle occlusions, solutions based on the reconstruction of the occluded part of the face have been proposed. These solutions are mainly based on the texture or the geometry of the face. However, the similarity of the face movement between different persons doing the same expression seems to be a real asset for the reconstruction. In this paper we exploit this asset and propose a new solution based on an auto-encoder with skip connections to reconstruct the occluded part of the face in the optical flow domain. To the best of our knowledge, this is the first proposition to directly reconstruct the movement for facial expression recognition. We validated our approach in the controlled dataset CK+ on which different occlusions were generated. Our experiments show that the proposed method reduce significantly the gap, in terms of recognition accuracy, between occluded and non-occluded situations. We also compare our approach with existing state-of-the-art solutions. In order to lay the basis of a reproducible and fair comparison in the future, we also propose a new experimental protocol that includes occlusion generation and reconstruction evaluation.",0
"Recently, video facial expression recognition has gained a lot of attention and has proven useful in various applications. Although some solutions have shown promising results in controlled environments without obstructions, recognizing facial expressions in the presence of partial obstruction remains a challenging task. To address this issue, solutions based on reconstructing the obscured parts of the face using texture or geometry have been proposed. However, the similarity in facial movements among different individuals performing the same expression can be advantageous for reconstruction. This paper proposes a novel solution based on an auto-encoder with skip connections to reconstruct the movement of the obscured parts of the face in the optical flow domain. This is the first proposal to directly reconstruct facial movement for expression recognition. The proposed approach is validated on the CK+ dataset, which includes various occlusions. Our experiments demonstrate that our method significantly reduces the gap in recognition accuracy between occluded and non-occluded situations. Additionally, we compare our approach with existing state-of-the-art solutions and propose a new experimental protocol that includes occlusion generation and reconstruction evaluation to facilitate future reproducible and fair comparison.",1
"We study the problem of animating images by transferring spatio-temporal visual effects (such as melting) from a collection of videos. We tackle two primary challenges in visual effect transfer: 1) how to capture the effect we wish to distill; and 2) how to ensure that only the effect, rather than content or artistic style, is transferred from the source videos to the input image. To address the first challenge, we evaluate five loss functions; the most promising one encourages the generated animations to have similar optical flow and texture motions as the source videos. To address the second challenge, we only allow our model to move existing image pixels from the previous frame, rather than predicting unconstrained pixel values. This forces any visual effects to occur using the input image's pixels, preventing unwanted artistic style or content from the source video from appearing in the output. We evaluate our method in objective and subjective settings, and show interesting qualitative results which demonstrate objects undergoing atypical transformations, such as making a face melt or a deer bloom.",0
"Our focus is on animating images by transferring spatio-temporal visual effects, like melting, from a set of videos. We encounter two main challenges in visual effect transfer, namely, how to capture the desired effect and how to ensure that only the effect, not content or artistic style, is transferred from source videos to the input image. To overcome the first challenge, we test five loss functions; the most successful one prompts the generated animations to have comparable optical flow and texture motions as the source videos. To address the second challenge, we only permit our model to shift existing image pixels from the previous frame, instead of predicting unconstrained pixel values. This ensures that visual effects are produced using the input image's pixels, preventing unwanted artistic style or content from the source video from appearing in the output. We assess our approach in objective and subjective settings, showing fascinating qualitative outcomes that demonstrate unusual transformations of objects, such as melting faces or blossoming deer.",1
"Speech-driven facial video generation has been a complex problem due to its multi-modal aspects namely audio and video domain. The audio comprises lots of underlying features such as expression, pitch, loudness, prosody(speaking style) and facial video has lots of variability in terms of head movement, eye blinks, lip synchronization and movements of various facial action units along with temporal smoothness. Synthesizing highly expressive facial videos from the audio input and static image is still a challenging task for generative adversarial networks. In this paper, we propose a multi-modal adaptive normalization(MAN) based architecture to synthesize a talking person video of arbitrary length using as input: an audio signal and a single image of a person. The architecture uses the multi-modal adaptive normalization, keypoint heatmap predictor, optical flow predictor and class activation map[58] based layers to learn movements of expressive facial components and hence generates a highly expressive talking-head video of the given person. The multi-modal adaptive normalization uses the various features of audio and video such as Mel spectrogram, pitch, energy from audio signals and predicted keypoint heatmap/optical flow and a single image to learn the respective affine parameters to generate highly expressive video. Experimental evaluation demonstrates superior performance of the proposed method as compared to Realistic Speech-Driven Facial Animation with GANs(RSDGAN) [53], Speech2Vid [10], and other approaches, on multiple quantitative metrics including: SSIM (structural similarity index), PSNR (peak signal to noise ratio), CPBD (image sharpness), WER(word error rate), blinks/sec and LMD(landmark distance). Further, qualitative evaluation and Online Turing tests demonstrate the efficacy of our approach.",0
"Generating facial videos based on speech has been a challenging task due to the multi-modal nature of the problem, which involves both audio and video domains. Audio has various underlying elements like pitch, expression, loudness, and prosody, while facial videos have head movements, lip synchronization, eye blinks, and movements of various facial action units with temporal smoothness, making it difficult for generative adversarial networks to synthesize highly expressive facial videos from the audio input and static image. To address this issue, we propose a multi-modal adaptive normalization (MAN) based architecture that uses various features of audio and video, such as Mel spectrogram, pitch, energy from audio signals, predicted keypoint heatmap/optical flow, and a single image, to learn the respective affine parameters and generate highly expressive talking-head videos of the given person. The proposed method outperforms other approaches, including RSDGAN and Speech2Vid, on multiple quantitative metrics and shows efficacy in qualitative evaluation and Online Turing tests.",1
"In this paper, we consider the compressed video background subtraction problem that separates the background and foreground of a video from its compressed measurements. The background of a video usually lies in a low dimensional space and the foreground is usually sparse. More importantly, each video frame is a natural image that has textural patterns. By exploiting these properties, we develop a message passing algorithm termed offline denoising-based turbo message passing (DTMP). We show that these structural properties can be efficiently handled by the existing denoising techniques under the turbo message passing framework. We further extend the DTMP algorithm to the online scenario where the video data is collected in an online manner. The extension is based on the similarity/continuity between adjacent video frames. We adopt the optical flow method to refine the estimation of the foreground. We also adopt the sliding window based background estimation to reduce complexity. By exploiting the Gaussianity of messages, we develop the state evolution to characterize the per-iteration performance of offline and online DTMP. Comparing to the existing algorithms, DTMP can work at much lower compression rates, and can subtract the background successfully with a lower mean squared error and better visual quality for both offline and online compressed video background subtraction.",0
"The focus of this paper is the issue of compressed video background subtraction, which aims to distinguish between the foreground and background of a video using its compressed measurements. Generally, the video's background is contained within a low-dimensional space, while the foreground is sparse. Moreover, each video frame is a natural image featuring textural patterns. By leveraging these features, we introduce an algorithm called offline denoising-based turbo message passing (DTMP), which is capable of effectively handling these structural properties using existing denoising techniques within the turbo message passing framework. We also extend DTMP to accommodate online video data collection, using the optical flow method to refine foreground estimation and a sliding window-based background estimation to minimize complexity. To evaluate the performance of offline and online DTMP, we utilize the state evolution method to analyze the per-iteration results. Compared to existing algorithms, DTMP can operate at lower compression rates and achieve a lower mean squared error and better visual quality for both offline and online compressed video background subtraction.",1
"Wearable cameras are becoming more and more popular in several applications, increasing the interest of the research community in developing approaches for recognizing actions from the first-person point of view. An open challenge in egocentric action recognition is that videos lack detailed information about the main actor's pose and thus tend to record only parts of the movement when focusing on manipulation tasks. Thus, the amount of information about the action itself is limited, making crucial the understanding of the manipulated objects and their context. Many previous works addressed this issue with two-stream architectures, where one stream is dedicated to modeling the appearance of objects involved in the action, and another to extracting motion features from optical flow. In this paper, we argue that learning features jointly from these two information channels is beneficial to capture the spatio-temporal correlations between the two better. To this end, we propose a single stream architecture able to do so, thanks to the addition of a self-supervised block that uses a pretext motion prediction task to intertwine motion and appearance knowledge. Experiments on several publicly available databases show the power of our approach.",0
"The popularity of wearable cameras has led to increased interest from the research community in developing methods for recognizing actions from a first-person perspective. However, recognizing actions in egocentric videos is challenging because they lack detailed information about the actor's pose, often capturing only parts of the movement when focusing on manipulation tasks. As a result, understanding the context of the manipulated objects is crucial. Previous works have used two-stream architectures to address this issue, but we propose a single stream architecture that learns features jointly from appearance and motion information. Our approach includes a self-supervised block that uses a pretext motion prediction task to intertwine motion and appearance knowledge, resulting in better spatio-temporal correlations. Experiments on publicly available databases demonstrate the effectiveness of our approach.",1
"We propose an unsupervised vision-based system to estimate the joint configurations of the robot arm from a sequence of RGB or RGB-D images without knowing the model a priori, and then adapt it to the task of category-independent articulated object pose estimation. We combine a classical geometric formulation with deep learning and extend the use of epipolar constraint to multi-rigid-body systems to solve this task. Given a video sequence, the optical flow is estimated to get the pixel-wise dense correspondences. After that, the 6D pose is computed by a modified PnP algorithm. The key idea is to leverage the geometric constraints and the constraint between multiple frames. Furthermore, we build a synthetic dataset with different kinds of robots and multi-joint articulated objects for the research of vision-based robot control and robotic vision. We demonstrate the effectiveness of our method on three benchmark datasets and show that our method achieves higher accuracy than the state-of-the-art supervised methods in estimating joint angles of robot arms and articulated objects.",0
"Our proposal is for an autonomous vision-based system that can determine the joint configurations of a robot arm using a sequence of RGB or RGB-D images, without prior knowledge of the model. We have extended the use of epipolar constraint to multi-rigid-body systems, using a combination of classical geometric formulation and deep learning. Optical flow is estimated to determine pixel-wise dense correspondences in a video sequence, and a modified PnP algorithm is used to calculate the 6D pose. We leverage the geometric constraints and the constraint between multiple frames to achieve this. Additionally, we have created a synthetic dataset comprising various robots and multi-joint articulated objects for research purposes. Our method has been shown to outperform state-of-the-art supervised methods in estimating joint angles of robot arms and articulated objects, as we demonstrate on three benchmark datasets.",1
"Analyzing motion between two consecutive images is one of the fundamental tasks in computer vision. In the lack of labeled data, the loss functions are split into consistency and smoothness, allowing for self-supervised training. This paper focuses on the cost function derivation and presents an unrolling iterative approach, transferring the hard L1 smoothness constraint into a softer multi-layer iterative scheme. More accurate gradients, especially near non-differential positions, improve the network's convergence, providing superior results on tested scenarios. We report state-of-the-art results on both MPI Sintel and KITTI 2015 unsupervised optical flow benchmarks. The provided approach can be used to enhance various architectures and not limited just to the presented pipeline.",0
"One of the fundamental tasks in computer vision is to analyze the motion between two consecutive images. When labeled data is not available, loss functions are divided into consistency and smoothness to enable self-supervised training. This paper focuses on deriving the cost function and introduces an iterative approach that converts the hard L1 smoothness constraint into a multi-layer iterative scheme that is more flexible. This results in more accurate gradients, particularly near non-differential positions, which improves the network's convergence and leads to superior results in tested scenarios. Our approach achieves state-of-the-art results on unsupervised optical flow benchmarks such as MPI Sintel and KITTI 2015. This method can be applied to enhance various architectures and is not limited to the presented pipeline.",1
"Low-quality modalities contain not only a lot of noisy information but also some discriminative features in RGBT tracking. However, the potentials of low-quality modalities are not well explored in existing RGBT tracking algorithms. In this work, we propose a novel duality-gated mutual condition network to fully exploit the discriminative information of all modalities while suppressing the effects of data noise. In specific, we design a mutual condition module, which takes the discriminative information of a modality as the condition to guide feature learning of target appearance in another modality. Such module can effectively enhance target representations of all modalities even in the presence of low-quality modalities. To improve the quality of conditions and further reduce data noise, we propose a duality-gated mechanism and integrate it into the mutual condition module. To deal with the tracking failure caused by sudden camera motion, which often occurs in RGBT tracking, we design a resampling strategy based on optical flow algorithms. It does not increase much computational cost since we perform optical flow calculation only when the model prediction is unreliable and then execute resampling when the sudden camera motion is detected. Extensive experiments on four RGBT tracking benchmark datasets show that our method performs favorably against the state-of-the-art tracking algorithms",0
"The current RGBT tracking algorithms do not fully utilize the potential of low-quality modalities, which contain both noisy information and discriminative features. To address this issue, we propose a novel duality-gated mutual condition network that can effectively exploit the discriminative information of all modalities while suppressing data noise. Our approach incorporates a mutual condition module, which uses the discriminative information from one modality to guide feature learning of target appearance in another modality. This module enhances target representations of all modalities, even when low-quality modalities are present. To further reduce data noise, we introduce a duality-gated mechanism into the mutual condition module. In addition, we address tracking failure caused by sudden camera motion by designing a resampling strategy based on optical flow algorithms. Our method achieves superior performance on four RGBT tracking benchmark datasets compared to state-of-the-art tracking algorithms, while incurring minimal computational cost.",1
"Unsupervised learning of optical flow, which leverages the supervision from view synthesis, has emerged as a promising alternative to supervised methods. However, the objective of unsupervised learning is likely to be unreliable in challenging scenes. In this work, we present a framework to use more reliable supervision from transformations. It simply twists the general unsupervised learning pipeline by running another forward pass with transformed data from augmentation, along with using transformed predictions of original data as the self-supervision signal. Besides, we further introduce a lightweight network with multiple frames by a highly-shared flow decoder. Our method consistently gets a leap of performance on several benchmarks with the best accuracy among deep unsupervised methods. Also, our method achieves competitive results to recent fully supervised methods while with much fewer parameters.",0
"A promising alternative to supervised methods is the unsupervised learning of optical flow, which relies on supervision from view synthesis. However, in challenging scenes, the objective of unsupervised learning may not be reliable. To address this issue, we propose a framework that utilizes more dependable supervision from transformations. This approach involves augmenting the unsupervised learning pipeline by conducting another forward pass with transformed data and using transformed predictions of the original data as the self-supervision signal. Additionally, we introduce a lightweight network with multiple frames through a highly-shared flow decoder. Our method consistently outperforms other deep unsupervised methods on several benchmarks, achieving the highest accuracy. Moreover, our method produces competitive results compared to recent fully supervised methods despite having significantly fewer parameters.",1
"Optical flow estimation is an important yet challenging problem in the field of video analytics. The features of different semantics levels/layers of a convolutional neural network can provide information of different granularity. To exploit such flexible and comprehensive information, we propose a semi-supervised Feature Pyramidal Correlation and Residual Reconstruction Network (FPCR-Net) for optical flow estimation from frame pairs. It consists of two main modules: pyramid correlation mapping and residual reconstruction. The pyramid correlation mapping module takes advantage of the multi-scale correlations of global/local patches by aggregating features of different scales to form a multi-level cost volume. The residual reconstruction module aims to reconstruct the sub-band high-frequency residuals of finer optical flow in each stage. Based on the pyramid correlation mapping, we further propose a correlation-warping-normalization (CWN) module to efficiently exploit the correlation dependency. Experiment results show that the proposed scheme achieves the state-of-the-art performance, with improvement by 0.80, 1.15 and 0.10 in terms of average end-point error (AEE) against competing baseline methods - FlowNet2, LiteFlowNet and PWC-Net on the Final pass of Sintel dataset, respectively.",0
"The estimation of optical flow is a difficult but crucial task in video analytics. Different levels of a convolutional neural network can offer varying degrees of information, and to take advantage of this, we have developed a semi-supervised network called the Feature Pyramidal Correlation and Residual Reconstruction Network (FPCR-Net) for estimating optical flow from pairs of frames. The FPCR-Net comprises two primary modules: pyramid correlation mapping and residual reconstruction. The former combines features of different scales to create a multi-level cost volume and uses multi-scale correlations of global/local patches. The latter aims to reconstruct the high-frequency residuals of finer optical flow in each stage. We have also proposed a correlation-warping-normalization (CWN) module based on the pyramid correlation mapping to make use of correlation dependency. Our proposed method has achieved state-of-the-art performance, with an improvement of 0.80, 1.15, and 0.10 in average end-point error (AEE) as compared to FlowNet2, LiteFlowNet, and PWC-Net respectively, on the Final pass of Sintel dataset.",1
"Independent Sign Language Recognition is a complex visual recognition problem that combines several challenging tasks of Computer Vision due to the necessity to exploit and fuse information from hand gestures, body features and facial expressions. While many state-of-the-art works have managed to deeply elaborate on these features independently, to the best of our knowledge, no work has adequately combined all three information channels to efficiently recognize Sign Language. In this work, we employ SMPL-X, a contemporary parametric model that enables joint extraction of 3D body shape, face and hands information from a single image. We use this holistic 3D reconstruction for SLR, demonstrating that it leads to higher accuracy than recognition from raw RGB images and their optical flow fed into the state-of-the-art I3D-type network for 3D action recognition and from 2D Openpose skeletons fed into a Recurrent Neural Network. Finally, a set of experiments on the body, face and hand features showed that neglecting any of these, significantly reduces the classification accuracy, proving the importance of jointly modeling body shape, facial expression and hand pose for Sign Language Recognition.",0
"Recognizing Sign Language independently poses a complex visual recognition challenge as it involves combining information from hand gestures, body features, and facial expressions. While previous works have independently explored these features, none have effectively fused all three channels to recognize Sign Language efficiently. To address this, we utilize SMPL-X, a modern parametric model that extracts 3D body shape, face, and hand information from a single image. Our holistic 3D reconstruction approach for Sign Language Recognition (SLR) surpasses recognition from raw RGB images and optical flow fed into the state-of-the-art I3D-type network for 3D action recognition and 2D Openpose skeletons fed into a Recurrent Neural Network (RNN). Our experiments confirm that neglecting any of the body, face, or hand features significantly reduces the classification accuracy, emphasizing the importance of jointly modeling all three features in SLR.",1
"The goal of this paper is to formulate a general framework for a constraint-based refinement of the optical flow using variational methods. We demonstrate that for a particular choice of the constraint, formulated as a minimization problem with the quadratic regularization, our results are close to the continuity equation based fluid flow. This closeness to the continuity model is theoretically justified through a modified augmented Lagrangian method and validated numerically. Further, along with the continuity constraint, our model can include geometric constraints as well. The correctness of our process is studied in the Hilbert space setting. Moreover, a special feature of our system is the possibility of a diagonalization by the Cauchy-Riemann operator and transforming it to a diffusion process on the curl and the divergence of the flow. Using the theory of semigroups on the decoupled system, we show that our process preserves the spatial characteristics of the divergence and the vorticities. We perform several numerical experiments and show the results on different datasets.",0
"The aim of this paper is to create a universal framework for refining the optical flow via constraint-based techniques using variational methods. We illustrate that by selecting a specific constraint, which takes the form of a minimization problem with quadratic regularization, our outcomes are in close proximity to the fluid flow based on the continuity equation. This proximity to the continuity model is theoretically justified using a modified augmented Lagrangian method, and verified numerically. Additionally, our model can incorporate geometric constraints in conjunction with the continuity constraint. We assess the accuracy of our process in the Hilbert space setting. Additionally, our system has a unique feature where the Cauchy-Riemann operator can diagonalize it, and it can be transformed into a diffusion process on the curl and divergence of the flow. Through the use of semigroup theory on the decoupled system, we establish that our process conserves the spatial characteristics of the vorticities and divergence. We conduct various numerical experiments and present the results on diverse datasets.",1
"The paper addresses the problem of recognition of actions in video with low inter-class variability such as Table Tennis strokes. Two stream, ""twin"" convolutional neural networks are used with 3D convolutions both on RGB data and optical flow. Actions are recognized by classification of temporal windows. We introduce 3D attention modules and examine their impact on classification efficiency. In the context of the study of sportsmen performances, a corpus of the particular actions of table tennis strokes is considered. The use of attention blocks in the network speeds up the training step and improves the classification scores up to 5% with our twin model. We visualize the impact on the obtained features and notice correlation between attention and player movements and position. Score comparison of state-of-the-art action classification method and proposed approach with attentional blocks is performed on the corpus. Proposed model with attention blocks outperforms previous model without them and our baseline.",0
"The primary focus of this paper is to address the challenge of identifying actions in videos with limited inter-class variability, such as Table Tennis strokes. To this end, we employ two identical convolutional neural networks that use 3D convolutions on both RGB data and optical flow. Our approach relies on temporal window classification to recognize actions. We also integrate 3D attention modules to improve classification efficiency. Specifically, we examine the influence of attention blocks on the efficacy of our model and observe a 5% improvement in classification scores compared to our twin model without attention blocks. We apply our method to a corpus of Table Tennis stroke actions and visualize the impact of attention on player movements and position. Finally, we compare our model's performance with state-of-the-art action classification methods and demonstrate that our proposed approach with attention blocks outperforms previous models and our baseline model.",1
"Learning the necessary high-level reasoning for video stabilization without the help of optical flow has proved to be one of the most challenging tasks in the field of computer vision. In this work, we present an iterative frame interpolation strategy to generate a novel dataset that is diverse enough to formulate video stabilization as a supervised learning problem unassisted by optical flow. A major benefit of treating video stabilization as a pure RGB based generative task over the conventional optical flow assisted approaches is the preservation of content and resolution, which is usually obstructed in the latter approaches. To do so, we provide a new video stabilization dataset and train an efficient network that can produce competitive stabilization results in a fraction of the time taken to do the same with the recent iterative frame interpolation schema. Our method provides qualitatively and quantitatively better results than those generated through state-of-the-art video stabilization methods. To the best of our knowledge, this is the only work that demonstrates the importance of perspective in formulating video stabilization as a deep learning problem instead of replacing it with an inter-frame motion measure",0
"The challenge of acquiring the necessary high-level reasoning for video stabilization without relying on optical flow has proven to be a difficult task in computer vision. This study introduces an iterative frame interpolation technique that generates a varied dataset to formulate video stabilization as a supervised learning problem without needing optical flow. The approach of treating video stabilization as a purely RGB-based generative task rather than relying on conventional optical flow methods has the advantage of preserving content and resolution, which can be obstructed in the latter approach. A new video stabilization dataset is provided, and an efficient network is trained to produce competitive stabilization results in a shorter amount of time than the recent iterative frame interpolation method. Our approach results in better qualitative and quantitative outcomes than state-of-the-art video stabilization methods. This study is the only one to demonstrate the significance of perspective in formulating video stabilization as a deep learning problem rather than replacing it with an inter-frame motion measure.",1
"In optical flow estimation task, coarse-to-fine (C2F) warping strategy is widely used to deal with the large displacement problem and provides efficiency and speed. However, limited by the small search range between the first images and warped second images, current coarse-to-fine optical flow networks fail to capture small and fast-moving objects which disappear at coarse resolution levels. To address this problem, we introduce a lightweight but effective Global Matching Component (GMC) to grab global matching features. We propose a new Hybrid Matching Optical Flow Network (HMFlow) by integrating GMC into existing coarse-to-fine networks seamlessly. Besides keeping in high accuracy and small model size, our proposed HMFlow can apply global matching features to guide the network to discover the small and fast-moving objects mismatched by local matching features. We also build a new dataset, named Small and Fast-Moving Chairs (SFChairs), for evaluation. The experimental results show that our proposed network achieves considerable performance, especially at regions with small and fast-moving objects.",0
"The coarse-to-fine (C2F) warping strategy is commonly used in optical flow estimation tasks to address the challenge of large displacement and improve efficiency. However, current C2F optical flow networks are limited by the small search range between images, resulting in the failure to capture small and fast-moving objects which disappear at coarse resolution levels. In response to this issue, we introduce a Global Matching Component (GMC) that is lightweight yet effective in capturing global matching features. We seamlessly integrate GMC into existing C2F networks to create a Hybrid Matching Optical Flow Network (HMFlow). Our proposed HMFlow maintains high accuracy and a small model size while also using global matching features to guide the network in detecting small and fast-moving objects that may be missed by local matching features. We also present a new dataset, SFChairs, for evaluation purposes. Results from our experiments demonstrate that our proposed network performs well, especially in regions with small and fast-moving objects.",1
"Abnormal event detection (AED) in urban surveillance videos has multiple challenges. Unlike other computer vision problems, the AED is not solely dependent on the content of frames. It also depends on the appearance of the objects and their movements in the scene. Various methods have been proposed to address the AED problem. Among those, deep learning based methods show the best results. This paper is based on deep learning methods and provides an effective way to detect and locate abnormal events in videos by handling spatio temporal data. This paper uses generative adversarial networks (GANs) and performs transfer learning algorithms on pre trained convolutional neural network (CNN) which result in an accurate and efficient model. The efficiency of the model is further improved by processing the optical flow information of the video. This paper runs experiments on two benchmark datasets for AED problem (UCSD Peds1 and UCSD Peds2) and compares the results with other previous methods. The comparisons are based on various criteria such as area under curve (AUC) and true positive rate (TPR). Experimental results show that the proposed method can effectively detect and locate abnormal events in crowd scenes.",0
"Urban surveillance videos pose multiple challenges when it comes to detecting abnormal events. Unlike other computer vision problems, the content of frames alone cannot determine if an event is abnormal. Appearance of objects and their movements in the scene also play a crucial role in AED. Various methods have been proposed to address this problem, with deep learning based techniques showing the most promising results. This paper proposes an effective way to detect and locate abnormal events in videos by leveraging spatio temporal data and using generative adversarial networks (GANs) and transfer learning algorithms on pre-trained convolutional neural networks (CNNs). The accuracy and efficiency of the proposed method is further improved by processing the optical flow information of the video. Experimental results on two benchmark datasets (UCSD Peds1 and UCSD Peds2) demonstrate that the proposed method outperforms previous methods based on criteria such as area under curve (AUC) and true positive rate (TPR) and can effectively detect and locate abnormal events in crowd scenes.",1
"Visual odometry networks commonly use pretrained optical flow networks in order to derive the ego-motion between consecutive frames. The features extracted by these networks represent the motion of all the pixels between frames. However, due to the existence of dynamic objects and texture-less surfaces in the scene, the motion information for every image region might not be reliable for inferring odometry due to the ineffectiveness of dynamic objects in derivation of the incremental changes in position. Recent works in this area lack attention mechanisms in their structures to facilitate dynamic reweighing of the feature maps for extracting more refined egomotion information. In this paper, we explore the effectiveness of self-attention in visual odometry. We report qualitative and quantitative results against the SOTA methods. Furthermore, saliency-based studies alongside specially designed experiments are utilized to investigate the effect of self-attention on VO. Our experiments show that using self-attention allows for the extraction of better features while achieving a better odometry performance compared to networks that lack such structures.",0
"Pretrained optical flow networks are frequently used in visual odometry networks to determine the ego-motion between consecutive frames. These networks extract features that reveal the motion of all pixels between frames. However, dynamic objects and texture-less surfaces in the scene may cause the motion information for every image region to be unreliable for inferring odometry. This is because dynamic objects are ineffective in deriving incremental changes in position. Recent research in this area lacks attention mechanisms to enable dynamic reweighing of feature maps for extracting more refined egomotion information. Our paper examines the effectiveness of self-attention in visual odometry and presents qualitative and quantitative results compared to state-of-the-art methods. We also conduct saliency-based studies and specially designed experiments to investigate the impact of self-attention on VO. Our experimental results demonstrate that self-attention enables the extraction of better features, resulting in superior odometry performance compared to networks that lack this structure.",1
"Skeleton-based action recognition has attracted research attentions in recent years. One common drawback in currently popular skeleton-based human action recognition methods is that the sparse skeleton information alone is not sufficient to fully characterize human motion. This limitation makes several existing methods incapable of correctly classifying action categories which exhibit only subtle motion differences. In this paper, we propose a novel framework for employing human pose skeleton and joint-centered light-weight information jointly in a two-stream graph convolutional network, namely, JOLO-GCN. Specifically, we use Joint-aligned optical Flow Patches (JFP) to capture the local subtle motion around each joint as the pivotal joint-centered visual information. Compared to the pure skeleton-based baseline, this hybrid scheme effectively boosts performance, while keeping the computational and memory overheads low. Experiments on the NTU RGB+D, NTU RGB+D 120, and the Kinetics-Skeleton dataset demonstrate clear accuracy improvements attained by the proposed method over the state-of-the-art skeleton-based methods.",0
"In recent years, there has been a surge of research interest in skeleton-based action recognition. However, current popular methods have a common shortcoming - the sparse skeleton information alone is insufficient for accurately characterizing human motion. This drawback renders several existing approaches incapable of correctly classifying action categories that exhibit subtle motion differences. In this paper, we introduce a novel framework called JOLO-GCN that combines human pose skeleton and joint-centered light-weight information in a two-stream graph convolutional network. Our approach uses Joint-aligned optical Flow Patches (JFP) to capture the local subtle motion around each joint and provide pivotal joint-centered visual information. This hybrid scheme effectively enhances performance while minimizing computational and memory overheads compared to the pure skeleton-based baseline. Our experiments on the NTU RGB+D, NTU RGB+D 120, and the Kinetics-Skeleton dataset show significant accuracy improvements over state-of-the-art skeleton-based methods.",1
"Complex blur such as the mixup of space-variant and space-invariant blur, which is hard to model mathematically, widely exists in real images. In this paper, we propose a novel image deblurring method that does not need to estimate blur kernels. We utilize a pair of images that can be easily acquired in low-light situations: (1) a blurred image taken with low shutter speed and low ISO noise; and (2) a noisy image captured with high shutter speed and high ISO noise. Slicing the blurred image into patches, we extend the Gaussian mixture model (GMM) to model the underlying intensity distribution of each patch using the corresponding patches in the noisy image. We compute patch correspondences by analyzing the optical flow between the two images. The Expectation Maximization (EM) algorithm is utilized to estimate the parameters of GMM. To preserve sharp features, we add an additional bilateral term to the objective function in the M-step. We eventually add a detail layer to the deblurred image for refinement. Extensive experiments on both synthetic and real-world data demonstrate that our method outperforms state-of-the-art techniques, in terms of robustness, visual quality, and quantitative metrics.",0
"Real images often contain complex blurs that are difficult to mathematically model, such as a combination of space-variant and space-invariant blur. This paper presents a new method for deblurring images that does not require the estimation of blur kernels. Instead, we use a pair of images captured in low-light conditions: a blurred image with a low shutter speed and low ISO noise, and a noisy image with a high shutter speed and high ISO noise. We divide the blurred image into patches and use the corresponding patches in the noisy image to extend the Gaussian mixture model (GMM) to model the underlying intensity distribution in each patch. We analyze the optical flow between the two images to compute patch correspondences, and use the Expectation Maximization (EM) algorithm to estimate the GMM parameters. To preserve sharp features, we add a bilateral term to the M-step objective function. Finally, we add a detail layer to refine the deblurred image. Our experiments on synthetic and real-world data show that our method is more robust and produces higher quality results than existing techniques, according to both visual and quantitative metrics.",1
"Facial features deformed according to the intended facial expression. Specific facial features are associated with specific facial expression, i.e. happy means the deformation of mouth. This paper presents the study of facial feature deformation for each facial expression by using an optical flow algorithm and segmented into three different regions of interest. The deformation of facial features shows the relation between facial the and facial expression. Based on the experiments, the deformations of eye and mouth are significant in all expressions except happy. For happy expression, cheeks and mouths are the significant regions. This work also suggests that different facial features' intensity varies in the way that they contribute to the recognition of the different facial expression intensity. The maximum magnitude across all expressions is shown by the mouth for surprise expression which is 9x10-4. While the minimum magnitude is shown by the mouth for angry expression which is 0.4x10-4.",0
"The intended facial expression causes deformities in facial features. Each facial expression corresponds to specific facial features, such as the mouth for a happy expression. This study uses an optical flow algorithm to analyze the deformation of facial features in three regions of interest for each expression. The results demonstrate the correlation between facial features and expressions. The experiments reveal that the eyes and mouth exhibit significant deformation in all expressions except for happy, where the cheeks and mouth are the predominant regions. Additionally, the intensity of different facial features varies and contributes differently to the recognition of facial expressions. The maximum magnitude of deformation is observed in the mouth for the surprise expression (9x10-4), while the minimum magnitude is observed in the mouth for the angry expression (0.4x10-4).",1
"The importance of inference in Machine Learning (ML) has led to an explosive number of different proposals in ML, and particularly in Deep Learning. In an attempt to reduce the complexity of Convolutional Neural Networks, we propose a Volterra filter-inspired Network architecture. This architecture introduces controlled non-linearities in the form of interactions between the delayed input samples of data. We propose a cascaded implementation of Volterra Filtering so as to significantly reduce the number of parameters required to carry out the same classification task as that of a conventional Neural Network. We demonstrate an efficient parallel implementation of this Volterra Neural Network (VNN), along with its remarkable performance while retaining a relatively simpler and potentially more tractable structure. Furthermore, we show a rather sophisticated adaptation of this network to nonlinearly fuse the RGB (spatial) information and the Optical Flow (temporal) information of a video sequence for action recognition. The proposed approach is evaluated on UCF-101 and HMDB-51 datasets for action recognition, and is shown to outperform state of the art CNN approaches.",0
"Machine Learning (ML) has seen a surge in proposals, especially in Deep Learning, due to the significance of inference. With the aim of reducing the complexity of Convolutional Neural Networks, our proposed architecture draws inspiration from Volterra filters. This architecture introduces controlled non-linearities by incorporating interactions between delayed input samples of data. We suggest a cascaded implementation of Volterra Filtering to reduce the parameter count while achieving the same classification task as a typical Neural Network. Our Volterra Neural Network (VNN) demonstrates impressive performance with a simpler and potentially more manageable structure. Additionally, we present a sophisticated adaptation of this network that combines RGB and Optical Flow information to recognize actions in video sequences. The proposed approach surpasses state-of-the-art CNN methods as evaluated on UCF-101 and HMDB-51 datasets.",1
"Contrary to the ongoing trend in automotive applications towards usage of more diverse and more sensors, this work tries to solve the complex scene flow problem under a monocular camera setup, i.e. using a single sensor. Towards this end, we exploit the latest achievements in single image depth estimation, optical flow, and sparse-to-dense interpolation and propose a monocular combination approach (MonoComb) to compute dense scene flow. MonoComb uses optical flow to relate reconstructed 3D positions over time and interpolates occluded areas. This way, existing monocular methods are outperformed in dynamic foreground regions which leads to the second best result among the competitors on the challenging KITTI 2015 scene flow benchmark.",0
"In contrast to the current trend in automotive technology that favors the use of a wider variety of sensors, this study addresses the intricate scene flow problem using only one sensor, namely a monocular camera setup. To tackle this challenge, the researchers utilized recent advancements in single image depth estimation, optical flow, and sparse-to-dense interpolation to propose a monocular combination approach called MonoComb. By using optical flow to relate reconstructed 3D positions over time and interpolating occluded areas, MonoComb outperforms existing monocular methods in dynamic foreground regions, achieving the second-best result among competitors in the challenging KITTI 2015 scene flow benchmark.",1
"Video semantic segmentation is active in recent years benefited from the great progress of image semantic segmentation. For such a task, the per-frame image segmentation is generally unacceptable in practice due to high computation cost. To tackle this issue, many works use the flow-based feature propagation to reuse the features of previous frames. However, the optical flow estimation inevitably suffers inaccuracy and then causes the propagated features distorted. In this paper, we propose distortion-aware feature correction to alleviate the issue, which improves video segmentation performance by correcting distorted propagated features. To be specific, we firstly propose to transfer distortion patterns from feature into image space and conduct effective distortion map prediction. Benefited from the guidance of distortion maps, we proposed Feature Correction Module (FCM) to rectify propagated features in the distorted areas. Our proposed method can significantly boost the accuracy of video semantic segmentation at a low price. The extensive experimental results on Cityscapes and CamVid show that our method outperforms the recent state-of-the-art methods.",0
"In recent years, video semantic segmentation has seen significant advancements due to the progress in image semantic segmentation. However, per-frame image segmentation is impractical because it requires high computation costs. To address this issue, many studies use flow-based feature propagation to reuse features from previous frames. Unfortunately, optical flow estimation often lacks accuracy and causes distorted propagated features. This paper introduces distortion-aware feature correction to improve video segmentation performance by correcting these distorted features. Specifically, the method transfers distortion patterns from features into image space and predicts effective distortion maps. The proposed Feature Correction Module (FCM) rectifies propagated features in the distorted areas with the guidance of these maps. This method is cost-effective and significantly improves accuracy compared to recent state-of-the-art methods. Extensive experiments on Cityscapes and CamVid demonstrate the effectiveness of our proposed method.",1
"Multimodal large-scale datasets for outdoor scenes are mostly designed for urban driving problems. The scenes are highly structured and semantically different from scenarios seen in nature-centered scenes such as gardens or parks. To promote machine learning methods for nature-oriented applications, such as agriculture and gardening, we propose the multimodal synthetic dataset for Enclosed garDEN scenes (EDEN). The dataset features more than 300K images captured from more than 100 garden models. Each image is annotated with various low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. Experimental results on the state-of-the-art methods for semantic segmentation and monocular depth prediction, two important tasks in computer vision, show positive impact of pre-training deep networks on our dataset for unstructured natural scenes. The dataset and related materials will be available at https://lhoangan.github.io/eden.",0
"Multimodal large-scale datasets that focus on outdoor scenes are typically tailored to address urban driving issues. These environments exhibit a high degree of organization and differ semantically from natural surroundings such as parks and gardens. To facilitate the development of machine learning algorithms for agricultural and gardening purposes, we propose the creation of a new multimodal synthetic dataset named Enclosed garDEN scenes (EDEN). The EDEN dataset comprises over 300,000 images derived from more than 100 garden models, each of which is annotated with a range of low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. Our experimental results on state-of-the-art methods for computer vision tasks, including semantic segmentation and monocular depth prediction, demonstrate a positive impact arising from the pre-training of deep networks on our dataset, which is specifically designed to address unstructured natural scenes. The EDEN dataset and associated materials will be made available at https://lhoangan.github.io/eden.",1
"Micro-expression (ME) recognition plays a crucial role in a wide range of applications, particularly in public security and psychotherapy. Recently, traditional methods rely excessively on machine learning design and the recognition rate is not high enough for its practical application because of its short duration and low intensity. On the other hand, some methods based on deep learning also cannot get high accuracy due to problems such as the imbalance of databases. To address these problems, we design a multi-stream convolutional neural network (MSCNN) for ME recognition in this paper. Specifically, we employ EVM and optical flow to magnify and visualize subtle movement changes in MEs and extract the masks from the optical flow images. And then, we add the masks, optical flow images, and grayscale images into the MSCNN. After that, in order to overcome the imbalance of databases, we added a random over-sampler after the Dense Layer of the neural network. Finally, extensive experiments are conducted on two public ME databases: CASME II and SAMM. Compared with many recent state-of-the-art approaches, our method achieves more promising recognition results.",0
"The recognition of micro-expressions (ME) is essential in various fields, including public safety and psychotherapy. However, current techniques rely heavily on machine learning and have low recognition rates due to the brevity and subtlety of MEs. Moreover, deep learning approaches face issues such as database imbalance, which hinders their accuracy. To address these challenges, we present a multi-stream convolutional neural network (MSCNN) for ME recognition. Our MSCNN utilizes EVM and optical flow to enhance and display tiny changes in MEs and extract masks from optical flow images. We then integrate these masks, optical flow images, and grayscale images into the MSCNN. To overcome the database imbalance, we introduce a random over-sampler after the Dense Layer of the neural network. We evaluate our approach on two public ME databases, CASME II and SAMM, and demonstrate better recognition results compared to existing state-of-the-art methods.",1
"Capsule networks (CapsNets) have recently shown promise to excel in most computer vision tasks, especially pertaining to scene understanding. In this paper, we explore CapsNet's capabilities in optical flow estimation, a task at which convolutional neural networks (CNNs) have already outperformed other approaches. We propose a CapsNet-based architecture, termed FlowCaps, which attempts to a) achieve better correspondence matching via finer-grained, motion-specific, and more-interpretable encoding crucial for optical flow estimation, b) perform better-generalizable optical flow estimation, c) utilize lesser ground truth data, and d) significantly reduce the computational complexity in achieving good performance, in comparison to its CNN-counterparts.",0
"Recently, Capsule networks (CapsNets) have demonstrated potential for excelling in computer vision tasks, particularly in relation to scene understanding. This study investigates the capabilities of CapsNets in optical flow estimation, a task that convolutional neural networks (CNNs) have already surpassed other methods. A CapsNet-based architecture named FlowCaps is proposed to achieve a) improved correspondence matching by utilizing more detailed, motion-specific, and understandable encoding essential for optical flow estimation, b) better generalization in optical flow estimation, c) use fewer ground truth data, and d) significantly reduce computational complexity when compared to CNNs while still achieving good performance.",1
"Accurate object segmentation is a crucial task in the context of robotic manipulation. However, creating sufficient annotated training data for neural networks is particularly time consuming and often requires manual labeling. To this end, we propose a simple, yet robust solution for learning to segment unknown objects grasped by a robot. Specifically, we exploit motion and temporal cues in RGB video sequences. Using optical flow estimation we first learn to predict segmentation masks of our given manipulator. Then, these annotations are used in combination with motion cues to automatically distinguish between background, manipulator and unknown, grasped object. In contrast to existing systems our approach is fully self-supervised and independent of precise camera calibration, 3D models or potentially imperfect depth data. We perform a thorough comparison with alternative baselines and approaches from literature. The object masks and views are shown to be suitable training data for segmentation networks that generalize to novel environments and also allow for watertight 3D reconstruction.",0
"Precise object segmentation carries significant importance in the realm of robotic manipulation. Yet, generating ample labeled data for neural networks can be an arduous and time-consuming process, often involving manual labeling. As a solution, we present a robust and straightforward method for learning to segment unknown objects held by a robot. Our approach employs motion and temporal cues in RGB video sequences, utilizing optical flow estimation to anticipate segmentation masks of the manipulator. These annotations, in combination with motion cues, automatically differentiate between the background, manipulator, and the object being held. In contrast to other systems, our approach is entirely self-supervised and not reliant on precise camera calibration, 3D models, or potentially imperfect depth data. We conducted a comprehensive comparison with other baselines and approaches from the literature and found that our object masks and views are ideal training data for segmentation networks that generalize to novel environments and permit watertight 3D reconstruction.",1
"The construction of models for video action classification progresses rapidly. However, the performance of those models can still be easily improved by ensembling with the same models trained on different modalities (e.g. Optical flow). Unfortunately, it is computationally expensive to use several modalities during inference. Recent works examine the ways to integrate advantages of multi-modality into a single RGB-model. Yet, there is still a room for improvement. In this paper, we explore the various methods to embed the ensemble power into a single model. We show that proper initialization, as well as mutual modality learning, enhances single-modality models. As a result, we achieve state-of-the-art results in the Something-Something-v2 benchmark.",0
"The progress in developing models for video action classification has been swift, but their performance can still be enhanced by combining them with models trained on different modalities like Optical flow. Unfortunately, this is computationally expensive during inference. Recent research has focused on integrating the benefits of multi-modality into a single RGB-model, but there is scope for improvement. This paper explores different techniques to incorporate ensemble power into a single model. Our findings demonstrate that proper initialization and mutual modality learning can improve single-modality models and lead to state-of-the-art results in the Something-Something-v2 benchmark.",1
"Interpolation of sparse pixel information towards a dense target resolution finds its application across multiple disciplines in computer vision. State-of-the-art interpolation of motion fields applies model-based interpolation that makes use of edge information extracted from the target image. For depth completion, data-driven learning approaches are widespread. Our work is inspired by latest trends in depth completion that tackle the problem of dense guidance for sparse information. We extend these ideas and create a generic cross-domain architecture that can be applied for a multitude of interpolation problems like optical flow, scene flow, or depth completion. In our experiments, we show that our proposed concept of Sparse Spatial Guided Propagation (SSGP) achieves improvements to robustness, accuracy, or speed compared to specialized algorithms.",0
"The use of interpolation to fill in gaps in pixel information is widely used in various fields of computer vision. The latest advancements in motion field interpolation rely on model-based techniques that utilize edge data from the intended image. Similarly, data-driven learning methods are commonly used in depth completion. Our research is influenced by recent developments in depth completion, which aim to address the challenge of creating a dense guide for sparse data. We have expanded on these ideas to create a versatile architecture that can be applied to a range of interpolation problems such as optical flow, scene flow, and depth completion. Our experiments demonstrate that our Sparse Spatial Guided Propagation (SSGP) approach is more robust, accurate, and faster than specialized algorithms.",1
"The interpretation of ego motion and scene change is a fundamental task for mobile robots. Optical flow information can be employed to estimate motion in the surroundings. Recently, unsupervised optical flow estimation has become a research hotspot. However, unsupervised approaches are often easy to be unreliable on partially occluded or texture-less regions. To deal with this problem, we propose CoT-AMFlow in this paper, an unsupervised optical flow estimation approach. In terms of the network architecture, we develop an adaptive modulation network that employs two novel module types, flow modulation modules (FMMs) and cost volume modulation modules (CMMs), to remove outliers in challenging regions. As for the training paradigm, we adopt a co-teaching strategy, where two networks simultaneously teach each other about challenging regions to further improve accuracy. Experimental results on the MPI Sintel, KITTI Flow and Middlebury Flow benchmarks demonstrate that our CoT-AMFlow outperforms all other state-of-the-art unsupervised approaches, while still running in real time. Our project page is available at https://sites.google.com/view/cot-amflow.",0
"Mobile robots require the ability to interpret ego motion and scene changes, making optical flow information a valuable tool for motion estimation in the surrounding environment. While unsupervised optical flow estimation has recently gained attention, these approaches are often unreliable in regions that are partially occluded or lack texture. To address this issue, our paper proposes CoT-AMFlow, an unsupervised optical flow estimation approach that utilizes an adaptive modulation network with flow modulation modules (FMMs) and cost volume modulation modules (CMMs) to remove outliers in challenging regions. The training paradigm involves a co-teaching strategy where two networks teach each other about challenging regions to improve accuracy. Experimental results demonstrate that CoT-AMFlow outperforms other unsupervised approaches while maintaining real-time performance. To learn more about our approach, please visit our project page at https://sites.google.com/view/cot-amflow.",1
"In this paper, we propose a spatio-temporal contextual network, STC-Flow, for optical flow estimation. Unlike previous optical flow estimation approaches with local pyramid feature extraction and multi-level correlation, we propose a contextual relation exploration architecture by capturing rich long-range dependencies in spatial and temporal dimensions. Specifically, STC-Flow contains three key context modules - pyramidal spatial context module, temporal context correlation module and recurrent residual contextual upsampling module, to build the relationship in each stage of feature extraction, correlation, and flow reconstruction, respectively. Experimental results indicate that the proposed scheme achieves the state-of-the-art performance of two-frame based methods on the Sintel dataset and the KITTI 2012/2015 datasets.",0
"This paper introduces a novel approach to optical flow estimation called STC-Flow, which utilizes a spatio-temporal contextual network. Unlike previous methods that rely on local pyramid feature extraction and multi-level correlation, we propose an architecture that explores contextual relations by capturing long-range dependencies in both spatial and temporal dimensions. STC-Flow comprises three context modules - pyramidal spatial context, temporal context correlation, and recurrent residual contextual upsampling - which are used to establish relationships in feature extraction, correlation, and flow reconstruction. We conducted experiments on the Sintel dataset and the KITTI 2012/2015 datasets, and our results indicate that our proposed approach outperforms current two-frame based methods and achieves state-of-the-art performance.",1
"Learning matching costs has been shown to be critical to the success of the state-of-the-art deep stereo matching methods, in which 3D convolutions are applied on a 4D feature volume to learn a 3D cost volume. However, this mechanism has never been employed for the optical flow task. This is mainly due to the significantly increased search dimension in the case of optical flow computation, ie, a straightforward extension would require dense 4D convolutions in order to process a 5D feature volume, which is computationally prohibitive. This paper proposes a novel solution that is able to bypass the requirement of building a 5D feature volume while still allowing the network to learn suitable matching costs from data. Our key innovation is to decouple the connection between 2D displacements and learn the matching costs at each 2D displacement hypothesis independently, ie, displacement-invariant cost learning. Specifically, we apply the same 2D convolution-based matching net independently on each 2D displacement hypothesis to learn a 4D cost volume. Moreover, we propose a displacement-aware projection layer to scale the learned cost volume, which reconsiders the correlation between different displacement candidates and mitigates the multi-modal problem in the learned cost volume. The cost volume is then projected to optical flow estimation through a 2D soft-argmin layer. Extensive experiments show that our approach achieves state-of-the-art accuracy on various datasets, and outperforms all published optical flow methods on the Sintel benchmark.",0
"The success of advanced deep stereo matching methods relies heavily on understanding matching costs. These methods apply 3D convolutions to a 4D feature volume to learn a 3D cost volume. However, this approach hasn't been applied to the optical flow task because of the increased search dimension, which would require computationally expensive dense 4D convolutions to process a 5D feature volume. In this paper, we introduce a new method that allows the network to learn matching costs without building a 5D feature volume. Our approach is based on displacement-invariant cost learning, which decouples the connection between 2D displacements and learns matching costs at each 2D displacement hypothesis independently. We apply a 2D convolution-based matching net to each hypothesis, which results in a 4D cost volume. Our method also includes a displacement-aware projection layer that mitigates the multi-modal problem by scaling the learned cost volume while considering the correlation between different displacement candidates. Finally, we project the cost volume to optical flow estimation through a 2D soft-argmin layer. Our experiments demonstrate that our approach is highly accurate and outperforms other published optical flow methods on the Sintel benchmark.",1
"Automatic fall detection is a vital technology for ensuring the health and safety of people. Home-based camera systems for fall detection often put people's privacy at risk. Thermal cameras can partially or fully obfuscate facial features, thus preserving the privacy of a person. Another challenge is the less occurrence of falls in comparison to the normal activities of daily living. As fall occurs rarely, it is non-trivial to learn algorithms due to class imbalance. To handle these problems, we formulate fall detection as an anomaly detection within an adversarial framework using thermal imaging. We present a novel adversarial network that comprises of two-channel 3D convolutional autoencoders which reconstructs the thermal data and the optical flow input sequences respectively. We introduce a technique to track the region of interest, a region-based difference constraint, and a joint discriminator to compute the reconstruction error. A larger reconstruction error indicates the occurrence of a fall. The experiments on a publicly available thermal fall dataset show the superior results obtained compared to the standard baseline.",0
"Ensuring the health and safety of individuals is highly dependent on automatic fall detection technology. However, the use of home-based camera systems for this purpose can compromise a person's privacy. To address this issue, thermal cameras have the ability to partially or fully obscure facial features, thereby safeguarding privacy. Moreover, the infrequency of falls compared to daily activities presents a challenge in developing algorithms for fall detection, due to class imbalance. To overcome these obstacles, an adversarial framework is utilized for anomaly detection, employing thermal imaging. This approach involves a novel adversarial network consisting of two-channel 3D convolutional autoencoders that reconstruct both thermal data and optical flow input sequences. Additionally, a region-based difference constraint and joint discriminator are utilized to track the region of interest and calculate the reconstruction error, respectively. A greater reconstruction error signifies the occurrence of a fall. Results of experiments conducted on a publicly available thermal fall dataset demonstrate the superiority of this method compared to the standard baseline.",1
"Applying image processing algorithms independently to each video frame often leads to temporal inconsistency in the resulting video. To address this issue, we present a novel and general approach for blind video temporal consistency. Our method is only trained on a pair of original and processed videos directly instead of a large dataset. Unlike most previous methods that enforce temporal consistency with optical flow, we show that temporal consistency can be achieved by training a convolutional network on a video with the Deep Video Prior. Moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. We demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. Extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. Our source codes are publicly available at github.com/ChenyangLEI/deep-video-prior.",0
"The application of image processing algorithms to individual video frames can result in inconsistencies over time. To tackle this issue, we have devised a new and universal method for achieving blind video temporal consistency. Our approach involves training on a pair of original and processed videos directly instead of a large dataset. Rather than relying on optical flow like most previous methods, our method employs a convolutional network trained on a video with the Deep Video Prior to achieve temporal consistency. We also propose an iteratively reweighted training strategy to deal with the challenging multimodal inconsistency problem. Our approach has been tested on 7 computer vision tasks on videos, and extensive quantitative and perceptual experiments have shown that it outperforms state-of-the-art methods in blind video temporal consistency. Our source codes can be accessed on github.com/ChenyangLEI/deep-video-prior.",1
"Video-based human pose estimation in crowded scenes is a challenging problem due to occlusion, motion blur, scale variation and viewpoint change, etc. Prior approaches always fail to deal with this problem because of (1) lacking of usage of temporal information; (2) lacking of training data in crowded scenes. In this paper, we focus on improving human pose estimation in videos of crowded scenes from the perspectives of exploiting temporal context and collecting new data. In particular, we first follow the top-down strategy to detect persons and perform single-person pose estimation for each frame. Then, we refine the frame-based pose estimation with temporal contexts deriving from the optical-flow. Specifically, for one frame, we forward the historical poses from the previous frames and backward the future poses from the subsequent frames to current frame, leading to stable and accurate human pose estimation in videos. In addition, we mine new data of similar scenes to HIE dataset from the Internet for improving the diversity of training set. In this way, our model achieves best performance on 7 out of 13 videos and 56.33 average w\_AP on test dataset of HIE challenge.",0
"The task of detecting human poses in videos of crowded scenes is quite difficult due to several factors such as occlusion, motion blur, scale variation, and changes in viewpoint. Previous approaches have failed to address this problem mainly because of the lack of temporal information usage and inadequate training data for crowded scenes. This research paper aims to enhance human pose estimation in videos of crowded scenes by utilizing temporal context and collecting more data. The proposed method involves detecting individuals and performing single-person pose estimation in each frame using the top-down approach. The frame-based pose estimation is then refined using optical flow-based temporal contexts, which involve propagating the historical poses from the previous frames and future poses from the subsequent frames into the current frame. This approach results in stable and accurate human pose estimation in videos. Additionally, new data collected from similar scenes to the HIE dataset from the internet is utilized to improve the diversity of the training set. The model proposed in this paper achieves the best performance on 7 out of 13 videos and an average w\_AP of 56.33 on the test dataset of the HIE challenge.",1
"This paper presents our solution to ACM MM challenge: Large-scale Human-centric Video Analysis in Complex Events\cite{lin2020human}; specifically, here we focus on Track3: Crowd Pose Tracking in Complex Events. Remarkable progress has been made in multi-pose training in recent years. However, how to track the human pose in crowded and complex environments has not been well addressed. We formulate the problem as several subproblems to be solved. First, we use a multi-object tracking method to assign human ID to each bounding box generated by the detection model. After that, a pose is generated to each bounding box with ID. At last, optical flow is used to take advantage of the temporal information in the videos and generate the final pose tracking result.",0
"Our solution to the ACM MM challenge of Large-scale Human-centric Video Analysis in Complex Events is presented in this paper, with a specific focus on Track3: Crowd Pose Tracking in Complex Events. Although there has been notable progress in multi-pose training, the issue of tracking human poses in crowded and complex environments remains unresolved. To address this, we have formulated the problem into several subproblems. Initially, we employ a multi-object tracking approach to allocate a human ID to each bounding box produced by the detection model. Then, a pose is assigned to each bounding box with an ID. Finally, we use optical flow to leverage the temporal data in the videos and produce the ultimate pose tracking outcome.",1
"In this paper, we introduce a new benchmark dataset named IPN Hand with sufficient size, variety, and real-world elements able to train and evaluate deep neural networks. This dataset contains more than 4,000 gesture samples and 800,000 RGB frames from 50 distinct subjects. We design 13 different static and dynamic gestures focused on interaction with touchless screens. We especially consider the scenario when continuous gestures are performed without transition states, and when subjects perform natural movements with their hands as non-gesture actions. Gestures were collected from about 30 diverse scenes, with real-world variation in background and illumination. With our dataset, the performance of three 3D-CNN models is evaluated on the tasks of isolated and continuous real-time HGR. Furthermore, we analyze the possibility of increasing the recognition accuracy by adding multiple modalities derived from RGB frames, i.e., optical flow and semantic segmentation, while keeping the real-time performance of the 3D-CNN model. Our empirical study also provides a comparison with the publicly available nvGesture (NVIDIA) dataset. The experimental results show that the state-of-the-art ResNext-101 model decreases about 30% accuracy when using our real-world dataset, demonstrating that the IPN Hand dataset can be used as a benchmark, and may help the community to step forward in the continuous HGR. Our dataset and pre-trained models used in the evaluation are publicly available at https://github.com/GibranBenitez/IPN-hand.",0
"A novel benchmark dataset called IPN Hand is presented in this paper, which is of adequate size, diversity, and incorporates real-world characteristics to facilitate the training and assessment of deep neural networks. The dataset comprises more than 4,000 gesture samples and 800,000 RGB frames from 50 distinctive subjects, including 13 static and dynamic gestures designed to interact with touchless screens. The focus is on continuous gestures without transitional states and natural hand movements as non-gesture actions, collected from approximately 30 diverse scenes with real-world variation in background and illumination. We assess the performance of three 3D-CNN models on isolated and continuous real-time hand gesture recognition tasks using our dataset and analyze the possibility of increasing recognition accuracy by incorporating multiple modalities derived from RGB frames such as optical flow and semantic segmentation. Our empirical analysis also includes a comparison with the publicly available nvGesture (NVIDIA) dataset. The experimental results show that the state-of-the-art ResNext-101 model experiences a 30% decrease in accuracy when using our real-world dataset, indicating that the IPN Hand dataset can serve as a benchmark for the continuous hand gesture recognition task, and may aid in the advancement of this field. The dataset and pre-trained models utilized in the evaluation are publicly available at https://github.com/GibranBenitez/IPN-hand.",1
"In recent years, surveillance cameras are widely deployed in public places, and the general crime rate has been reduced significantly due to these ubiquitous devices. Usually, these cameras provide cues and evidence after crimes are conducted, while they are rarely used to prevent or stop criminal activities in time. It is both time and labor consuming to manually monitor a large amount of video data from surveillance cameras. Therefore, automatically recognizing violent behaviors from video signals becomes essential. This paper summarizes several existing video datasets for violence detection and proposes the RWF-2000 database with 2,000 videos captured by surveillance cameras in real-world scenes. Also, we present a new method that utilizes both the merits of 3D-CNNs and optical flow, namely Flow Gated Network. The proposed approach obtains an accuracy of 87.25% on the test set of our proposed database. The database and source codes are currently open to access.",0
"Surveillance cameras have become prevalent in public areas in recent years, leading to a significant reduction in overall crime rates. However, these cameras are typically used for providing post-crime evidence rather than actively preventing criminal activities. The manual monitoring of vast amounts of video data from these cameras is a time-consuming task. As a result, it is crucial to develop automated methods to recognize violent behavior from video signals. This article provides an overview of various video datasets for violence detection and introduces the RWF-2000 database, which comprises 2,000 videos captured by surveillance cameras in real-world scenarios. Furthermore, we present a novel technique called the Flow Gated Network, which combines the strengths of 3D-CNNs and optical flow. The proposed approach achieves an accuracy of 87.25% on the test set of our database. Our database and source codes are accessible to the public.",1
"Visual voice activity detection (V-VAD) uses visual features to predict whether a person is speaking or not. V-VAD is useful whenever audio VAD (A-VAD) is inefficient either because the acoustic signal is difficult to analyze or because it is simply missing. We propose two deep architectures for V-VAD, one based on facial landmarks and one based on optical flow. Moreover, available datasets, used for learning and for testing V-VAD, lack content variability. We introduce a novel methodology to automatically create and annotate very large datasets in-the-wild -- WildVVAD -- based on combining A-VAD with face detection and tracking. A thorough empirical evaluation shows the advantage of training the proposed deep V-VAD models with this dataset.",0
"The method of using visual features to determine if a person is speaking, known as visual voice activity detection (V-VAD), can be helpful when the traditional method of audio VAD (A-VAD) is not effective due to difficult acoustic signals or absence of audio. Two deep architectures for V-VAD, which rely on facial landmarks and optical flow, have been suggested. However, the available datasets used for training and testing V-VAD lack content variability. To address this issue, a novel method called WildVVAD has been introduced which automatically creates and annotates large datasets in-the-wild by combining A-VAD with face detection and tracking. Empirical evaluations demonstrate the benefits of training the proposed deep V-VAD models with the WildVVAD dataset.",1
"Nowadays 360 video analysis has become a significant research topic in the field since the appearance of high-quality and low-cost 360 wearable devices. In this paper, we propose a novel LiteFlowNet360 architecture for 360 videos optical flow estimation. We design LiteFlowNet360 as a domain adaptation framework from perspective video domain to 360 video domain. We adapt it from simple kernel transformation techniques inspired by Kernel Transformer Network (KTN) to cope with inherent distortion in 360 videos caused by the sphere-to-plane projection. First, we apply an incremental transformation of convolution layers in feature pyramid network and show that further transformation in inference and regularization layers are not important, hence reducing the network growth in terms of size and computation cost. Second, we refine the network by training with augmented data in a supervised manner. We perform data augmentation by projecting the images in a sphere and re-projecting to a plane. Third, we train LiteFlowNet360 in a self-supervised manner using target domain 360 videos. Experimental results show the promising results of 360 video optical flow estimation using the proposed novel architecture.",0
"The emergence of high-quality and affordable 360 wearable devices has made 360 video analysis a significant research topic. This paper presents the LiteFlowNet360 architecture, which is designed as a domain adaptation framework for optical flow estimation in 360 videos. To address the distortion caused by the sphere-to-plane projection, we incorporate simple kernel transformation techniques inspired by the Kernel Transformer Network (KTN). We demonstrate that incremental transformation of convolution layers in the feature pyramid network is sufficient, and further transformation in inference and regularization layers is unnecessary, thus reducing network size and computation cost. Additionally, we refine the network via supervised training using augmented data obtained by projecting images onto a sphere and re-projecting them onto a plane. Finally, we train LiteFlowNet360 in a self-supervised manner using target domain 360 videos, and our experimental results show promising outcomes for 360 video optical flow estimation using this novel architecture.",1
"Moving objects in scenes are still a severe challenge for the SLAM system. Many efforts have tried to remove the motion regions in the images by detecting moving objects. In this way, the keypoints belonging to motion regions will be ignored in the later calculations. In this paper, we proposed a novel motion removal method, leveraging semantic information and optical flow to extract motion regions. Different from previous works, we don't predict moving objects or motion regions directly from image sequences. We computed rigid optical flow, synthesized by the depth and pose, and compared it against the estimated optical flow to obtain initial motion regions. Then, we utilized K-means to finetune the motion region masks with instance segmentation masks. The ORB-SLAM2 integrated with the proposed motion removal method achieved the best performance in both indoor and outdoor dynamic environments.",0
"The SLAM system still faces a significant challenge when dealing with moving objects in scenes. To address this, previous efforts have attempted to detect moving objects in images and ignore their corresponding keypoints in subsequent calculations. Our paper proposes a new motion removal approach that utilizes semantic information and optical flow to isolate motion regions. Unlike previous methods, we do not directly predict moving objects or motion regions from image sequences. Instead, we use rigid optical flow synthesized by depth and pose and compare it against estimated optical flow to identify initial motion regions. We then refine these regions using K-means and instance segmentation masks. By integrating our method with ORB-SLAM2, we achieved the best performance in dynamic indoor and outdoor environments.",1
"Drowsiness driving is a major cause of traffic accidents and thus numerous previous researches have focused on driver drowsiness detection. Many drive relevant factors have been taken into consideration for fatigue detection and can lead to high precision, but there are still several serious constraints, such as most existing models are environmentally susceptible. In this paper, fatigue detection is considered as temporal action detection problem instead of image classification. The proposed detection system can be divided into four parts: (1) Localize the key patches of the detected driver picture which are critical for fatigue detection and calculate the corresponding optical flow. (2) Contrast Limited Adaptive Histogram Equalization (CLAHE) is used in our system to reduce the impact of different light conditions. (3) Three individual two-stream networks combined with attention mechanism are designed for each feature to extract temporal information. (4) The outputs of the three sub-networks will be concatenated and sent to the fully-connected network, which judges the status of the driver. The drowsiness detection system is trained and evaluated on the famous Nation Tsing Hua University Driver Drowsiness Detection (NTHU-DDD) dataset and we obtain an accuracy of 94.46%, which outperforms most existing fatigue detection models.",0
"Numerous studies have focused on detecting driver drowsiness, as it is a leading cause of traffic accidents. While many factors have been considered for fatigue detection, such as drive-related variables, current models are still limited by environmental factors. This paper proposes a new approach to fatigue detection, treating it as a temporal action detection problem instead of image classification. The system consists of four parts: identifying critical patches in the driver picture, applying Contrast Limited Adaptive Histogram Equalization to reduce lighting effects, using three two-stream networks with attention mechanisms to extract temporal information, and sending the output to a fully-connected network to determine the driver's status. The system is trained and evaluated on the NTHU-DDD dataset, achieving an accuracy of 94.46%, surpassing most existing fatigue detection models.",1
"Scene flow represents the 3D motion of every point in the dynamic environments. Like the optical flow that represents the motion of pixels in 2D images, 3D motion representation of scene flow benefits many applications, such as autonomous driving and service robot. This paper studies the problem of scene flow estimation from two consecutive 3D point clouds. In this paper, a novel hierarchical neural network with double attention is proposed for learning the correlation of point features in adjacent frames and refining scene flow from coarse to fine layer by layer. The proposed network has a new more-for-less hierarchical architecture. The more-for-less means that the number of input points is greater than the number of output points for scene flow estimation, which brings more input information and balances the precision and resource consumption. In this hierarchical architecture, scene flow of different levels is generated and supervised respectively. A novel attentive embedding module is introduced to aggregate the features of adjacent points using a double attention method in a patch-to-patch manner. The proper layers for flow embedding and flow supervision are carefully considered in our network designment. Experiments show that the proposed network outperforms the state-of-the-art performance of 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets. We also apply the proposed network to realistic LiDAR odometry task, which is an key problem in autonomous driving. The experiment results demonstrate that our proposed network can outperform the ICP-based method and shows the good practical application ability.",0
"The motion of every point in dynamic environments can be represented using scene flow in 3D. This representation is useful for various applications, such as autonomous driving and service robots. This study focuses on estimating scene flow from two consecutive 3D point clouds, using a novel hierarchical neural network with double attention. The network has a more-for-less hierarchical architecture that allows for greater input information and balanced precision and resource consumption. Scene flow of different levels is generated and supervised using a novel attentive embedding module that aggregates features of adjacent points using a double attention method. The proposed network outperforms state-of-the-art methods for 3D scene flow estimation on FlyingThings3D and KITTI Scene Flow 2015 datasets. The network is also applied to realistic LiDAR odometry tasks, demonstrating superior performance compared to the ICP-based method and good practical application ability.",1
"Current state-of-the-art trackers often fail due to distractorsand large object appearance changes. In this work, we explore the use ofdense optical flow to improve tracking robustness. Our main insight is that, because flow estimation can also have errors, we need to incorporate an estimate of flow uncertainty for robust tracking. We present a novel tracking framework which combines appearance and flow uncertainty information to track objects in challenging scenarios. We experimentally verify that our framework improves tracking robustness, leading to new state-of-the-art results. Further, our experimental ablations shows the importance of flow uncertainty for robust tracking.",0
"The current advanced trackers tend to falter when faced with distractions and significant changes in the appearance of objects. This study delves into the prospect of enhancing tracking resilience through dense optical flow. The main revelation is that since flow estimation can also be erroneous, it is crucial to integrate an assessment of flow uncertainty to ensure robust tracking. The research introduces a new tracking framework that merges both appearance and flow uncertainty data to trace objects in difficult situations. Our experiments confirm that this framework enhances tracking resilience, yielding novel superior outcomes. Additionally, our experimental analyses underscore the significance of flow uncertainty for stable tracking.",1
"Person Re-Identification (ReID) is a challenging problem in many video analytics and surveillance applications, where a person's identity must be associated across a distributed non-overlapping network of cameras. Video-based person ReID has recently gained much interest because it allows capturing discriminant spatio-temporal information from video clips that is unavailable for image-based ReID. Despite recent advances, deep learning (DL) models for video ReID often fail to leverage this information to improve the robustness of feature representations. In this paper, the motion pattern of a person is explored as an additional cue for ReID. In particular, a flow-guided Mutual Attention network is proposed for fusion of image and optical flow sequences using any 2D-CNN backbone, allowing to encode temporal information along with spatial appearance information. Our Mutual Attention network relies on the joint spatial attention between image and optical flow features maps to activate a common set of salient features across them. In addition to flow-guided attention, we introduce a method to aggregate features from longer input streams for better video sequence-level representation. Our extensive experiments on three challenging video ReID datasets indicate that using the proposed Mutual Attention network allows to improve recognition accuracy considerably with respect to conventional gated-attention networks, and state-of-the-art methods for video-based person ReID.",0
"The identification of individuals across a network of cameras presents a complex challenge in various video analytics and surveillance scenarios. Recently, video-based person ReID has gained significant attention as it can capture spatial-temporal information that cannot be obtained through image-based ReID. However, despite recent advancements in deep learning models, they often fail to exploit this information to improve the representation of features. This study proposes a flow-guided Mutual Attention network that explores the motion pattern of individuals as an additional cue for ReID. The proposed network fuses image and optical flow sequences, allowing the encoding of temporal information with spatial appearance information. The Mutual Attention network uses joint spatial attention between image and optical flow feature maps to activate a common set of salient features. Additionally, the study introduces a method to aggregate features from longer input streams to improve video sequence-level representation. The experiments conducted on three challenging video ReID datasets demonstrate that using the proposed Mutual Attention network leads to a considerable improvement in recognition accuracy compared to conventional gated-attention networks and state-of-the-art methods for video-based person ReID.",1
"Drones shooting can be applied in dynamic traffic monitoring, object detecting and tracking, and other vision tasks. The variability of the shooting location adds some intractable challenges to these missions, such as varying scale, unstable exposure, and scene migration. In this paper, we strive to tackle the above challenges and automatically understand the crowd from the visual data collected from drones. First, to alleviate the background noise generated in cross-scene testing, a double-stream crowd counting model is proposed, which extracts optical flow and frame difference information as an additional branch. Besides, to improve the model's generalization ability at different scales and time, we randomly combine a variety of data transformation methods to simulate some unseen environments. To tackle the crowd density estimation problem under extreme dark environments, we introduce synthetic data generated by game Grand Theft Auto V(GTAV). Experiment results show the effectiveness of the virtual data. Our method wins the challenge with a mean absolute error (MAE) of 12.70. Moreover, a comprehensive ablation study is conducted to explore each component's contribution.",0
"The usage of drones for shooting can be employed for various vision tasks such as dynamic traffic monitoring, object detection, and tracking. However, the challenges posed by variable shooting locations, including varying scale, unstable exposure, and scene migration, make these missions difficult to accomplish. The purpose of this study is to overcome these challenges and automatically comprehend the crowd through the visual data obtained from drones. To achieve this, we propose a double-stream crowd counting model that extracts optical flow and frame difference information to alleviate background noise during cross-scene testing. Furthermore, to enhance the model's generalization ability for different scales and times, we randomly utilize a range of data transformation methods to simulate unseen environments. Additionally, we tackle the issue of crowd density estimation in extreme dark environments by introducing synthetic data generated through Grand Theft Auto V. Our approach proved successful, achieving a mean absolute error (MAE) of 12.70. We also conducted a comprehensive ablation study to determine each component's contribution.",1
"Optical flow, which expresses pixel displacement, is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network, recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent to the pixel displacement, a common approach is to:forward optical flow to a neural network and fine-tune this network on the task dataset. With this method,they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an \textbf{I}n-network \textbf{F}eature \textbf{F}low estimation module (IFF module) for video object detection. Without resorting pre-training on any additional dataset, our IFF module is able to directly produce \textbf{feature flow} which indicates the feature displacement. Our IFF module consists of a shallow module, which shares the features with the detection branches. This compact design enables our IFF-Net to accurately detect objects, while maintaining a fast inference speed. Furthermore, we propose a transformation residual loss (TRL) based on \textit{self-supervision}, which further improves the performance of our IFF-Net. Our IFF-Net outperforms existing methods and sets a state-of-the-art performance on ImageNet VID.",0
"Pixel displacement is commonly used in computer vision tasks through optical flow to provide motion information. However, recent state-of-the-art approaches utilize convolutional neural networks to solve problems directly on feature-level. Due to feature vector displacement not being consistent with pixel displacement, a common approach is to fine-tune a neural network on the task dataset to produce feature-level motion information. However, this approach has drawbacks in video object detection. To address these issues, we propose a novel network (IFF-Net) with an In-network Feature Flow estimation module (IFF module) that directly produces feature flow to indicate feature displacement. Our IFF module is self-supervised, and our IFF-Net outperforms existing methods on ImageNet VID without the need for pre-training on additional datasets.",1
"Face reenactment aims to animate a source face image to a different pose and expression provided by a driving image. Existing approaches are either designed for a specific identity, or suffer from the identity preservation problem in the one-shot or few-shot scenarios. In this paper, we introduce a method for one-shot face reenactment, which uses the reconstructed 3D meshes (i.e., the source mesh and driving mesh) as guidance to learn the optical flow needed for the reenacted face synthesis. Technically, we explicitly exclude the driving face's identity information in the reconstructed driving mesh. In this way, our network can focus on the motion estimation for the source face without the interference of driving face shape. We propose a motion net to learn the face motion, which is an asymmetric autoencoder. The encoder is a graph convolutional network (GCN) that learns a latent motion vector from the meshes, and the decoder serves to produce an optical flow image from the latent vector with CNNs. Compared to previous methods using sparse keypoints to guide the optical flow learning, our motion net learns the optical flow directly from 3D dense meshes, which provide the detailed shape and pose information for the optical flow, so it can achieve more accurate expression and pose on the reenacted face. Extensive experiments show that our method can generate high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.",0
"The objective of face reenactment is to animate a face image to a different pose and expression using a different image as a reference. However, current methods either focus on a specific individual or face identity preservation issues in one-shot or few-shot scenarios. This study proposes a one-shot face reenactment method that employs reconstructed 3D meshes to guide the learning of optical flow for reenacted face synthesis. The driving face's identity information is excluded from the reconstructed driving mesh, allowing the network to concentrate on motion estimation for the source face. An asymmetric autoencoder called a motion net is proposed to learn face motion, with the encoder a graph convolutional network and the decoder a CNN that produces an optical flow image from a latent vector. Our motion net learns the optical flow directly from 3D dense meshes, leading to more accurate expression and pose on the reenacted face compared to previous methods that use sparse keypoints for optical flow learning. Extensive experiments demonstrate that our approach generates high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.",1
"Video object detection is a tough task due to the deteriorated quality of video sequences captured under complex environments. Currently, this area is dominated by a series of feature enhancement based methods, which distill beneficial semantic information from multiple frames and generate enhanced features through fusing the distilled information. However, the distillation and fusion operations are usually performed at either frame level or instance level with external guidance using additional information, such as optical flow and feature memory. In this work, we propose a dual semantic fusion network (abbreviated as DSFNet) to fully exploit both frame-level and instance-level semantics in a unified fusion framework without external guidance. Moreover, we introduce a geometric similarity measure into the fusion process to alleviate the influence of information distortion caused by noise. As a result, the proposed DSFNet can generate more robust features through the multi-granularity fusion and avoid being affected by the instability of external guidance. To evaluate the proposed DSFNet, we conduct extensive experiments on the ImageNet VID dataset. Notably, the proposed dual semantic fusion network achieves, to the best of our knowledge, the best performance of 84.1\% mAP among the current state-of-the-art video object detectors with ResNet-101 and 85.4\% mAP with ResNeXt-101 without using any post-processing steps.",0
"The difficulty in video object detection is attributed to the poor quality of video sequences captured in complex environments. Feature enhancement methods currently dominate this area by extracting useful semantic information from multiple frames to create enhanced features. However, these extraction and fusion operations are often carried out at the frame or instance level with additional guidance such as optical flow and feature memory. In this study, we propose a dual semantic fusion network (DSFNet) that combines both frame and instance-level semantics in a unified fusion framework without external guidance. We also introduce a geometric similarity measure to reduce the influence of information distortion caused by noise. As a result, DSFNet generates more robust features and is not affected by the instability of external guidance. We evaluate the performance of DSFNet on the ImageNet VID dataset, and it achieves the best performance of 84.1\% mAP among current state-of-the-art video object detectors with ResNet-101 and 85.4\% mAP with ResNeXt-101 without any post-processing steps.",1
"Deformable convolution, originally proposed for the adaptation to geometric variations of objects, has recently shown compelling performance in aligning multiple frames and is increasingly adopted for video super-resolution. Despite its remarkable performance, its underlying mechanism for alignment remains unclear. In this study, we carefully investigate the relation between deformable alignment and the classic flow-based alignment. We show that deformable convolution can be decomposed into a combination of spatial warping and convolution. This decomposition reveals the commonality of deformable alignment and flow-based alignment in formulation, but with a key difference in their offset diversity. We further demonstrate through experiments that the increased diversity in deformable alignment yields better-aligned features, and hence significantly improves the quality of video super-resolution output. Based on our observations, we propose an offset-fidelity loss that guides the offset learning with optical flow. Experiments show that our loss successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Aside from the contributions to deformable alignment, our formulation inspires a more flexible approach to introduce offset diversity to flow-based alignment, improving its performance.",0
"Deformable convolution is a technique that was initially developed to adapt to the geometric variations of objects. Recently, it has been found to be highly effective in aligning multiple frames and is increasingly being used in video super-resolution. However, its mechanism for alignment remains unclear. This study aims to investigate the relationship between deformable alignment and classic flow-based alignment. It reveals that deformable convolution can be broken down into a combination of spatial warping and convolution. While these two alignments share a similar formulation, they differ in their offset diversity. The study shows that increased diversity in deformable alignment leads to better-aligned features, which significantly improves the quality of video super-resolution output. The findings have led to the proposal of an offset-fidelity loss that guides the offset learning with optical flow, which successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Furthermore, the study inspires a more flexible approach to introduce offset diversity to flow-based alignment, thereby improving its performance.",1
"Optical flow estimation is an important computer vision task, which aims at estimating the dense correspondences between two frames. RAFT (Recurrent All Pairs Field Transforms) currently represents the state-of-the-art in optical flow estimation. It has excellent generalization ability and has obtained outstanding results across several benchmarks. To further improve the robustness and achieve accurate optical flow estimation, we present PRAFlow (Pyramid Recurrent All-Pairs Flow), which builds upon the pyramid network structure. Due to computational limitation, our proposed network structure only uses two pyramid layers. At each layer, the RAFT unit is used to estimate the optical flow at the current resolution. Our model was trained on several simulate and real-image datasets, submitted to multiple leaderboards using the same model and parameters, and won the 2nd place in the optical flow task of ECCV 2020 workshop: Robust Vision Challenge.",0
"The task of optical flow estimation is significant in computer vision. It aims to establish dense correspondences between two frames, with RAFT currently being the most advanced method for this purpose. RAFT is highly adaptable and has achieved exceptional results across various benchmarks. To enhance the precision and robustness of optical flow estimation, we have developed PRAFlow (Pyramid Recurrent All-Pairs Flow) that builds on the pyramid network structure. Our network structure only employs two pyramid layers due to computational constraints. At each layer, the RAFT unit is utilized to estimate optical flow for the current resolution. Our model has been trained on numerous simulated and real-image datasets, and it was submitted to multiple leaderboards using the same model and parameters. We secured the 2nd position in the optical flow task of ECCV 2020 workshop: Robust Vision Challenge.",1
"In the learning based video compression approaches, it is an essential issue to compress pixel-level optical flow maps by developing new motion vector (MV) encoders. In this work, we propose a new framework called Resolution-adaptive Flow Coding (RaFC) to effectively compress the flow maps globally and locally, in which we use multi-resolution representations instead of single-resolution representations for both the input flow maps and the output motion features of the MV encoder. To handle complex or simple motion patterns globally, our frame-level scheme RaFC-frame automatically decides the optimal flow map resolution for each video frame. To cope different types of motion patterns locally, our block-level scheme called RaFC-block can also select the optimal resolution for each local block of motion features. In addition, the rate-distortion criterion is applied to both RaFC-frame and RaFC-block and select the optimal motion coding mode for effective flow coding. Comprehensive experiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly demonstrate the effectiveness of our overall RaFC framework after combing RaFC-frame and RaFC-block for video compression.",0
"In the realm of learning-based video compression, the compression of pixel-level optical flow maps is a crucial matter that requires the development of new motion vector encoders. This study introduces a fresh approach, named Resolution-adaptive Flow Coding (RaFC), that efficiently compresses flow maps globally and locally by using multi-resolution representations for the input flow maps and the output motion features of the motion vector encoder. To address complex or simple motion patterns globally, RaFC-frame, our frame-level scheme, decides the optimal flow map resolution for each video frame. Additionally, our block-level scheme, RaFC-block, selects the optimal resolution for each local block of motion features to handle different types of motion patterns locally. Furthermore, the rate-distortion criterion is employed to both RaFC-frame and RaFC-block to select the optimal motion coding mode for effective flow coding. Our comprehensive experimentation on four benchmark datasets HEVC, VTL, UVG, and MCL-JCV, clearly demonstrate the overall effectiveness of our RaFC framework after combining RaFC-frame and RaFC-block for video compression.",1
"We propose a lightweight real-time sign language detection model, as we identify the need for such a case in videoconferencing. We extract optical flow features based on human pose estimation and, using a linear classifier, show these features are meaningful with an accuracy of 80%, evaluated on the DGS Corpus. Using a recurrent model directly on the input, we see improvements of up to 91% accuracy, while still working under 4ms. We describe a demo application to sign language detection in the browser in order to demonstrate its usage possibility in videoconferencing applications.",0
"In response to the demand for sign language detection in videoconferencing, we propose a real-time model that is both efficient and accurate. Our approach involves extracting optical flow features through human pose estimation and utilizing a linear classifier, resulting in an 80% accuracy rate as evaluated on the DGS Corpus. By implementing a recurrent model directly on the input, we were able to achieve up to 91% accuracy while maintaining a processing time of under 4ms. To showcase the potential of our model in videoconferencing applications, we also present a demo application for sign language detection in web browsers.",1
