"The use of pessimism, when reasoning about datasets lacking exhaustive exploration has recently gained prominence in offline reinforcement learning. Despite the robustness it adds to the algorithm, overly pessimistic reasoning can be equally damaging in precluding the discovery of good policies, which is an issue for the popular bonus-based pessimism. In this paper, we introduce the notion of Bellman-consistent pessimism for general function approximation: instead of calculating a point-wise lower bound for the value function, we implement pessimism at the initial state over the set of functions consistent with the Bellman equations. Our theoretical guarantees only require Bellman closedness as standard in the exploratory setting, in which case bonus-based pessimism fails to provide guarantees. Even in the special case of linear MDPs where stronger function-approximation assumptions hold, our result improves upon a recent bonus-based approach by $\mathcal{O}(d)$ in its sample complexity when the action space is finite. Remarkably, our algorithms automatically adapt to the best bias-variance tradeoff in the hindsight, whereas most prior approaches require tuning extra hyperparameters a priori.",0
"Recently, the offline reinforcement learning community has emphasized the use of pessimism when dealing with datasets that lack comprehensive exploration. Although this approach adds robustness, overly pessimistic reasoning can hinder the discovery of optimal policies, particularly with popular bonus-based pessimism. In this study, we propose Bellman-consistent pessimism for general function approximation. Instead of calculating a lower bound for the value function, we implement pessimism at the initial state over the set of functions consistent with Bellman equations. Our approach guarantees Bellman closedness, which bonus-based pessimism fails to provide, even in the exploratory setting. Additionally, our algorithm adapts to the best bias-variance tradeoff, whereas previous methods require a priori tuning of hyperparameters. Our results show that our approach improves upon recent bonus-based methods by $\mathcal{O}(d)$ in sample complexity when the action space is finite, even in linear MDPs where stronger function-approximation assumptions hold.",1
"Reinforcement learning (RL) research focuses on general solutions that can be applied across different domains. This results in methods that RL practitioners can use in almost any domain. However, recent studies often lack the engineering steps (""tricks"") which may be needed to effectively use RL, such as reward shaping, curriculum learning, and splitting a large task into smaller chunks. Such tricks are common, if not necessary, to achieve state-of-the-art results and win RL competitions. To ease the engineering efforts, we distill descriptions of tricks from state-of-the-art results and study how well these tricks can improve a standard deep Q-learning agent. The long-term goal of this work is to enable combining proven RL methods with domain-specific tricks by providing a unified software framework and accompanying insights in multiple domains.",0
"The primary objective of reinforcement learning (RL) research is to develop general solutions that can be utilized in various fields. This approach yields methods that can be employed by RL practitioners in nearly any domain. However, recent investigations frequently overlook the engineering steps or ""tricks"" that are necessary to effectively utilize RL, such as reward shaping, curriculum learning, and breaking down a significant task into smaller parts. These tricks are prevalent and vital in achieving top-notch results and winning RL competitions. To streamline the engineering process, we gather descriptions of tricks from cutting-edge outcomes and assess how well these tricks can enhance a standard deep Q-learning agent. The ultimate goal of this project is to enable the merging of proven RL techniques with domain-specific tricks by providing a comprehensive software framework and relevant insights in multiple areas.",1
"As one of the most popular methods in the field of reinforcement learning, Q-learning has received increasing attention. Recently, there have been more theoretical works on the regret bound of algorithms that belong to the Q-learning class in different settings. In this paper, we analyze the cumulative regret when conducting Nash Q-learning algorithm on 2-player turn-based stochastic Markov games (2-TBSG), and propose the very first gap dependent logarithmic upper bounds in the episodic tabular setting. This bound matches the theoretical lower bound only up to a logarithmic term. Furthermore, we extend the conclusion to the discounted game setting with infinite horizon and propose a similar gap dependent logarithmic regret bound. Also, under the linear MDP assumption, we obtain another logarithmic regret for 2-TBSG, in both centralized and independent settings.",0
"Q-learning, a widely used technique in reinforcement learning, has gained significant attention in recent years, particularly with regards to the regret bound of algorithms belonging to the Q-learning class in diverse contexts. This study examines the cumulative regret of the Nash Q-learning algorithm in 2-player turn-based stochastic Markov games (2-TBSG), presenting the first gap-dependent logarithmic upper bounds in the episodic tabular setting. Although this bound matches the theoretical lower bound up to a logarithmic term, the conclusion is extended to the discounted game setting with infinite horizon, proposing a similar gap-dependent logarithmic regret bound. Additionally, the study obtains another logarithmic regret for 2-TBSG under the linear MDP assumption in both centralized and independent settings.",1
"While agents trained by Reinforcement Learning (RL) can solve increasingly challenging tasks directly from visual observations, generalizing learned skills to novel environments remains very challenging. Extensive use of data augmentation is a promising technique for improving generalization in RL, but it is often found to decrease sample efficiency and can even lead to divergence. In this paper, we investigate causes of instability when using data augmentation in common off-policy RL algorithms. We identify two problems, both rooted in high-variance Q-targets. Based on our findings, we propose a simple yet effective technique for stabilizing this class of algorithms under augmentation. We perform extensive empirical evaluation of image-based RL using both ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. Our method greatly improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image-based RL. We further show that our method scales to RL with ViT-based architectures, and that data augmentation may be especially important in this setting.",0
"Although RL-trained agents are capable of directly solving more challenging tasks based on visual observations, effectively applying learned skills to new environments remains a significant obstacle. While data augmentation has shown promise for enhancing generalization in RL, it often results in reduced sample efficiency and can even lead to divergence. This paper examines the causes of instability that arise when data augmentation is used with common off-policy RL algorithms and identifies two issues that stem from high-variance Q-targets. As a result of our findings, we propose a straightforward yet highly effective technique for stabilizing this class of algorithms under augmentation. To assess the efficacy of our method, we conduct extensive empirical evaluations of image-based RL using both ConvNets and Vision Transformers (ViT) on various benchmarks derived from the DeepMind Control Suite and robotic manipulation tasks. Our approach greatly enhances the stability and sample efficiency of ConvNets under augmentation and delivers generalization outcomes that compete with cutting-edge methods for image-based RL. Furthermore, we demonstrate that our method can be scaled up to RL with ViT-based architectures and that data augmentation may be particularly critical in this context.",1
"Despite the close connection between exploration and sample efficiency, most state of the art reinforcement learning algorithms include no considerations for exploration beyond maximizing the entropy of the policy. In this work we address this seeming missed opportunity. We observe that the most common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime. This causes BBE to be actively detrimental to policy learning in many control tasks. We show that by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. Our method, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification. When used in conjunction with soft actor-critic, DEEP incurs no performance penalty in densely-rewarding environments. On sparse environments, DEEP gives a several-fold improvement in data efficiency due to better exploration.",0
"Most state of the art reinforcement learning algorithms do not consider exploration beyond maximizing the policy's entropy, despite the strong connection between exploration and sample efficiency. This missed opportunity is addressed in this work. The common formulation of directed exploration in deep RL, known as bonus-based exploration (BBE), suffers from bias and slow coverage in the few-sample regime, making it detrimental to policy learning in many control tasks. However, by decoupling the task policy from the exploration policy, directed exploration can be highly effective for sample-efficient continuous control. The method proposed, Decoupled Exploration and Exploitation Policies (DEEP), can be combined with any off-policy RL algorithm without modification and incurs no performance penalty in densely-rewarding environments when used in conjunction with soft actor-critic. On sparse environments, DEEP improves data efficiency by several-fold due to better exploration.",1
"One of the most prominent attributes of Neural Networks (NNs) constitutes their capability of learning to extract robust and descriptive features from high dimensional data, like images. Hence, such an ability renders their exploitation as feature extractors particularly frequent in an abundant of modern reasoning systems. Their application scope mainly includes complex cascade tasks, like multi-modal recognition and deep Reinforcement Learning (RL). However, NNs induce implicit biases that are difficult to avoid or to deal with and are not met in traditional image descriptors. Moreover, the lack of knowledge for describing the intra-layer properties -- and thus their general behavior -- restricts the further applicability of the extracted features. With the paper at hand, a novel way of visualizing and understanding the vector space before the NNs' output layer is presented, aiming to enlighten the deep feature vectors' properties under classification tasks. Main attention is paid to the nature of overfitting in the feature space and its adverse effect on further exploitation. We present the findings that can be derived from our model's formulation, and we evaluate them on realistic recognition scenarios, proving its prominence by improving the obtained results.",0
"Neural Networks (NNs) are well-known for their ability to extract robust and descriptive features from high-dimensional data, such as images. This makes them a popular choice as feature extractors in modern reasoning systems, particularly for complex tasks like multi-modal recognition and deep Reinforcement Learning (RL). However, NNs also have implicit biases that are difficult to avoid, which traditional image descriptors do not have. Additionally, the lack of knowledge about intra-layer properties limits the applicability of extracted features. In this paper, we present a novel way to visualize and understand the vector space before the NNs' output layer to shed light on the properties of deep feature vectors under classification tasks. We focus on overfitting in the feature space and its negative impact on further exploitation. Our model's formulation yields valuable findings, which we evaluate on realistic recognition scenarios and demonstrate its effectiveness in improving results.",1
"Goal-conditioned reinforcement learning endows an agent with a large variety of skills, but it often struggles to solve tasks that require more temporally extended reasoning. In this work, we propose to incorporate imagined subgoals into policy learning to facilitate learning of complex tasks. Imagined subgoals are predicted by a separate high-level policy, which is trained simultaneously with the policy and its critic. This high-level policy predicts intermediate states halfway to the goal using the value function as a reachability metric. We don't require the policy to reach these subgoals explicitly. Instead, we use them to define a prior policy, and incorporate this prior into a KL-constrained policy iteration scheme to speed up and regularize learning. Imagined subgoals are used during policy learning, but not during test time, where we only apply the learned policy. We evaluate our approach on complex robotic navigation and manipulation tasks and show that it outperforms existing methods by a large margin.",0
"The use of goal-conditioned reinforcement learning provides an agent with a wide range of skills. However, this approach often struggles to solve tasks that require extended temporal reasoning. To address this issue, we suggest integrating imagined subgoals into policy learning. These subgoals are predicted by a high-level policy, which is trained simultaneously with the policy and its critic. The high-level policy predicts intermediate states using the value function as a reachability metric. Although the policy is not required to reach these subgoals, we use them to establish a prior policy. We incorporate this prior policy into a KL-constrained policy iteration scheme to speed up and regularize the learning process. Imagined subgoals are only used during policy learning and not during test time, where we solely apply the learned policy. We evaluated our method on complex robotic navigation and manipulation tasks and found that it outperforms existing methods significantly.",1
"In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of tasks to prepare for and learn faster in new, unseen, but related tasks. The training tasks are usually hand-crafted to be representative of the expected distribution of test tasks and hence all used in training. We show that given a set of training tasks, learning can be both faster and more effective (leading to better performance in the test tasks), if the training tasks are appropriately selected. We propose a task selection algorithm, Information-Theoretic Task Selection (ITTS), based on information theory, which optimizes the set of tasks used for training in meta-RL, irrespectively of how they are generated. The algorithm establishes which training tasks are both sufficiently relevant for the test tasks, and different enough from one another. We reproduce different meta-RL experiments from the literature and show that ITTS improves the final performance in all of them.",0
"Meta-Reinforcement Learning (meta-RL) involves training an agent on a specific set of tasks to enable it to learn and adapt quickly to new, but similar tasks. These training tasks are carefully selected to represent the expected distribution of test tasks, and are all used in the training process. The effectiveness and speed of learning can be improved if the training tasks are appropriately chosen. To achieve this, we propose an algorithm called Information-Theoretic Task Selection (ITTS), which uses information theory to optimize the selection of training tasks in meta-RL, regardless of how they were generated. The algorithm identifies training tasks that are relevant and unique enough to improve performance in test tasks. Our experiments show that ITTS enhances the final performance of various meta-RL experiments from the literature.",1
"Swarms of drones are being more and more used in many practical scenarios, such as surveillance, environmental monitoring, search and rescue in hardly-accessible areas, etc.. While a single drone can be guided by a human operator, the deployment of a swarm of multiple drones requires proper algorithms for automatic task-oriented control. In this paper, we focus on visual coverage optimization with drone-mounted camera sensors. In particular, we consider the specific case in which the coverage requirements are uneven, meaning that different parts of the environment have different coverage priorities. We model these coverage requirements with relevance maps and propose a deep reinforcement learning algorithm to guide the swarm. The paper first defines a proper learning model for a single drone, and then extends it to the case of multiple drones both with greedy and cooperative strategies. Experimental results show the performance of the proposed method, also compared with a standard patrolling algorithm.",0
"The usage of swarms of drones has become increasingly prevalent in various practical applications, such as surveillance, environmental monitoring, and search and rescue operations in remote areas. Although a single drone can be operated by a human, deploying multiple drones requires suitable algorithms for automated task-oriented control. This study concentrates on optimizing visual coverage using camera sensors mounted on drones. Specifically, it addresses situations where coverage requirements are not uniform, meaning certain areas of the environment hold higher priority for coverage than others. We create relevance maps to model these requirements and recommend a deep reinforcement learning algorithm to direct the swarm. The study outlines a learning model for a single drone and extends it to include greedy and cooperative strategies for multiple drones. The results of the proposed approach are presented and compared to a standard patrolling algorithm through experimental testing.",1
"Solving multi-goal reinforcement learning (RL) problems with sparse rewards is generally challenging. Existing approaches have utilized goal relabeling on collected experiences to alleviate issues raised from sparse rewards. However, these methods are still limited in efficiency and cannot make full use of experiences. In this paper, we propose Model-based Hindsight Experience Replay (MHER), which exploits experiences more efficiently by leveraging environmental dynamics to generate virtual achieved goals. Replacing original goals with virtual goals generated from interaction with a trained dynamics model leads to a novel relabeling method, \emph{model-based relabeling} (MBR). Based on MBR, MHER performs both reinforcement learning and supervised learning for efficient policy improvement. Theoretically, we also prove the supervised part in MHER, i.e., goal-conditioned supervised learning with MBR data, optimizes a lower bound on the multi-goal RL objective. Experimental results in several point-based tasks and simulated robotics environments show that MHER achieves significantly higher sample efficiency than previous state-of-the-art methods.",0
"It is commonly difficult to solve multi-goal reinforcement learning problems that have sparse rewards. Although existing methods have used goal relabeling to address this issue, they are still limited in their effectiveness and cannot fully utilize experiences. This paper introduces Model-based Hindsight Experience Replay (MHER), which uses environmental dynamics to generate virtual achieved goals and replaces original goals with these virtual goals. This leads to a new relabeling technique called model-based relabeling (MBR). MHER uses MBR for both reinforcement learning and supervised learning, resulting in more efficient policy improvement. The supervised part of MHER, which involves goal-conditioned supervised learning with MBR data, is theoretically proven to optimize a lower bound on the multi-goal RL objective. Experimental results on various point-based tasks and simulated robotics environments demonstrate that MHER achieves much higher sample efficiency than previous state-of-the-art methods.",1
"Proximal Policy Optimization (PPO) is among the most widely used algorithms in reinforcement learning, which achieves state-of-the-art performance in many challenging problems. The keys to its success are the reliable policy updates through the clipping mechanism and the multiple epochs of minibatch updates. The aim of this research is to give new simple but effective alternatives to the former. For this, we propose linearly and exponentially decaying clipping range approaches throughout the training. With these, we would like to provide higher exploration at the beginning and stronger restrictions at the end of the learning phase. We investigate their performance in several classical control and locomotive robotic environments. During the analysis, we found that they influence the achieved rewards and are effective alternatives to the constant clipping method in many reinforcement learning tasks.",0
"PPO is a widely-used algorithm in reinforcement learning that has achieved remarkable results in challenging problems. Its success is attributed to the reliable policy updates via the clipping mechanism and multiple epochs of minibatch updates. This study aims to introduce new, simple but effective techniques to enhance the former method. We propose linear and exponential decay of clipping range approaches during training to promote greater exploration in the beginning and stronger restrictions towards the end of the learning phase. We evaluate their performance in various control and robotic environments and find that they impact the rewards achieved, serving as viable alternatives to constant clipping methods in many reinforcement learning tasks.",1
"In membership/subscriber acquisition and retention, we sometimes need to recommend marketing content for multiple pages in sequence. Different from general sequential decision making process, the use cases have a simpler flow where customers per seeing recommended content on each page can only return feedback as moving forward in the process or dropping from it until a termination state. We refer to this type of problems as sequential decision making in linear--flow. We propose to formulate the problem as an MDP with Bandits where Bandits are employed to model the transition probability matrix. At recommendation time, we use Thompson sampling (TS) to sample the transition probabilities and allocate the best series of actions with analytical solution through exact dynamic programming. The way that we formulate the problem allows us to leverage TS's efficiency in balancing exploration and exploitation and Bandit's convenience in modeling actions' incompatibility. In the simulation study, we observe the proposed MDP with Bandits algorithm outperforms Q-learning with $\epsilon$-greedy and decreasing $\epsilon$, independent Bandits, and interaction Bandits. We also find the proposed algorithm's performance is the most robust to changes in the across-page interdependence strength.",0
"When acquiring and retaining members/subscribers, it is sometimes necessary to suggest marketing content for multiple pages in a sequence. This differs from the typical decision-making process as customers can only provide feedback by moving forward or dropping out until the end. This is known as sequential decision making in a linear-flow. We suggest using an MDP with Bandits to model the transition probability matrix. During recommendation, we use Thompson sampling to sample transition probabilities and find the best series of actions through dynamic programming. This approach leverages the efficiency of TS in balancing exploration and exploitation and the convenience of Bandits in modeling action incompatibility. The MDP with Bandits algorithm outperforms other methods in simulation, and is also robust to changes in the interdependence strength across pages.",1
"Edge computing provides a promising paradigm to support the implementation of Industrial Internet of Things (IIoT) by offloading tasks to nearby edge nodes. Meanwhile, the increasing network size makes it impractical for centralized data processing due to limited bandwidth, and consequently a decentralized learning scheme is preferable. Reinforcement learning (RL) has been widely investigated and shown to be a promising solution for decision-making and optimal control processes. For RL in a decentralized setup, edge nodes (agents) connected through a communication network aim to work collaboratively to find a policy to optimize the global reward as the sum of local rewards. However, communication costs, scalability and adaptation in complex environments with heterogeneous agents may significantly limit the performance of decentralized RL. Alternating direction method of multipliers (ADMM) has a structure that allows for decentralized implementation, and has shown faster convergence than gradient descent based methods. Therefore, we propose an adaptive stochastic incremental ADMM (asI-ADMM) algorithm and apply the asI-ADMM to decentralized RL with edge-computing-empowered IIoT networks. We provide convergence properties for proposed algorithms by designing a Lyapunov function and prove that the asI-ADMM has $O(\frac{1}{k}) +O(\frac{1}{M})$ convergence rate where $k$ and $ M$ are the number of iterations and batch samples, respectively. Then, we test our algorithm with two supervised learning problems. For performance evaluation, we simulate two applications in decentralized RL settings with homogeneous and heterogeneous agents. The experiment results show that our proposed algorithms outperform the state of the art in terms of communication costs and scalability, and can well adapt to complex IoT environments.",0
"Edge computing is a promising approach for supporting the implementation of the Industrial Internet of Things (IIoT) by delegating tasks to nearby edge nodes. However, the growing network size makes centralized data processing impractical due to limited bandwidth, necessitating a decentralized learning scheme. Reinforcement learning (RL) has been extensively studied and found to be an effective solution for optimal control and decision-making processes. In a decentralized setup, RL involves edge nodes working collaboratively to find a policy that optimizes the global reward as the sum of local rewards. Nevertheless, communication costs, scalability, and adaptation in complex environments with heterogeneous agents can significantly limit the performance of decentralized RL. The Alternating Direction Method of Multipliers (ADMM) has a structure that supports decentralized implementation and has faster convergence than gradient descent-based approaches. To address these issues, we present an adaptive stochastic incremental ADMM (asI-ADMM) algorithm and apply it to decentralized RL with edge-computing-enhanced IIoT networks. We provide convergence properties for the proposed algorithm by designing a Lyapunov function and demonstrate that the asI-ADMM has a convergence rate of $O(\frac{1}{k}) +O(\frac{1}{M})$ where $k$ and $M$ are the number of iterations and batch samples, respectively. Finally, we evaluate our algorithm's performance in two supervised learning problems. The experimental results show that our proposed algorithms outperform state-of-the-art methods in terms of communication costs and scalability, and can adapt well to complex IoT environments with both homogeneous and heterogeneous agents.",1
"We model Alzheimer's disease (AD) progression by combining differential equations (DEs) and reinforcement learning (RL) with domain knowledge. DEs provide relationships between some, but not all, factors relevant to AD. We assume that the missing relationships must satisfy general criteria about the working of the brain, for e.g., maximizing cognition while minimizing the cost of supporting cognition. This allows us to extract the missing relationships by using RL to optimize an objective (reward) function that captures the above criteria. We use our model consisting of DEs (as a simulator) and the trained RL agent to predict individualized 10-year AD progression using baseline (year 0) features on synthetic and real data. The model was comparable or better at predicting 10-year cognition trajectories than state-of-the-art learning-based models. Our interpretable model demonstrated, and provided insights into, ""recovery/compensatory"" processes that mitigate the effect of AD, even though those processes were not explicitly encoded in the model. Our framework combines DEs with RL for modelling AD progression and has broad applicability for understanding other neurological disorders.",0
"We have developed a model to simulate the progression of Alzheimer's disease (AD) by utilizing a combination of differential equations (DEs) and reinforcement learning (RL) along with domain knowledge. While DEs provide a relationship between some factors related to AD, we assume that the missing relationships adhere to general criteria about the workings of the brain, such as maximizing cognition while minimizing the cost of supporting it. Thus, we have used RL to optimize an objective function that captures these criteria and extract the missing relationships. Our model, comprising DEs and the trained RL agent, can predict individualized 10-year AD progression using baseline features on both synthetic and real data. The model has shown a comparable or better performance in predicting 10-year cognition trajectories than existing learning-based models. Our model is interpretable and provides insights into the ""recovery/compensatory"" processes that mitigate the effect of AD, even though these processes are not explicitly encoded in the model. Our framework, which combines DEs with RL, has broad applicability for understanding other neurological disorders.",1
"Stochastic Approximation (SA) is a popular approach for solving fixed-point equations where the information is corrupted by noise. In this paper, we consider an SA involving a contraction mapping with respect to an arbitrary norm, and show its finite-sample error bounds while using different stepsizes. The idea is to construct a smooth Lyapunov function using the generalized Moreau envelope, and show that the iterates of SA have negative drift with respect to that Lyapunov function. Our result is applicable in Reinforcement Learning (RL). In particular, we use it to establish the first-known convergence rate of the V-trace algorithm for off-policy TD-learning. Moreover, we also use it to study TD-learning in the on-policy setting, and recover the existing state-of-the-art results for $Q$-learning. Importantly, our construction results in only a logarithmic dependence of the convergence bound on the size of the state-space.",0
"This paper explores the use of Stochastic Approximation (SA) to solve fixed-point equations when noise is present. Specifically, the authors analyze an SA method that employs a contraction mapping with respect to any norm, and provide finite-sample error bounds for various step sizes. To accomplish this, they construct a smooth Lyapunov function using the generalized Moreau envelope and demonstrate that the SA iterates exhibit negative drift in relation to this function. These findings have applications in Reinforcement Learning (RL), including the establishment of the first convergence rate for the V-trace algorithm in off-policy TD-learning and the study of TD-learning in the on-policy setting, which yields the current state-of-the-art results for $Q$-learning. Notably, the authors' approach results in a convergence bound that depends logarithmically on the size of the state-space.",1
"Active network management (ANM) of electricity distribution networks include many complex stochastic sequential optimization problems. These problems need to be solved for integrating renewable energies and distributed storage into future electrical grids. In this work, we introduce Gym-ANM, a framework for designing reinforcement learning (RL) environments that model ANM tasks in electricity distribution networks. These environments provide new playgrounds for RL research in the management of electricity networks that do not require an extensive knowledge of the underlying dynamics of such systems. Along with this work, we are releasing an implementation of an introductory toy-environment, ANM6-Easy, designed to emphasize common challenges in ANM. We also show that state-of-the-art RL algorithms can already achieve good performance on ANM6-Easy when compared against a model predictive control (MPC) approach. Finally, we provide guidelines to create new Gym-ANM environments differing in terms of (a) the distribution network topology and parameters, (b) the observation space, (c) the modelling of the stochastic processes present in the system, and (d) a set of hyperparameters influencing the reward signal. Gym-ANM can be downloaded at https://github.com/robinhenry/gym-anm.",0
"Electricity distribution networks require active network management (ANM) to integrate renewable energies and distributed storage in the future. However, solving the complex stochastic sequential optimization problems involved in ANM tasks can be challenging. To address this, we introduce Gym-ANM, a framework that allows the design of reinforcement learning (RL) environments to model ANM tasks. These environments offer new opportunities for RL research in electricity network management without requiring extensive knowledge of the underlying dynamics. We also present ANM6-Easy, an introductory toy-environment that highlights common ANM challenges. Our study shows that state-of-the-art RL algorithms can achieve good performance on ANM6-Easy compared to a model predictive control (MPC) approach. Additionally, we provide guidelines for creating new Gym-ANM environments with varying parameters, observation space, stochastic process models, and hyperparameters influencing the reward signal. Gym-ANM is available for download at https://github.com/robinhenry/gym-anm.",1
"Despite a series of recent successes in reinforcement learning (RL), many RL algorithms remain sensitive to hyperparameters. As such, there has recently been interest in the field of AutoRL, which seeks to automate design decisions to create more general algorithms. Recent work suggests that population based approaches may be effective AutoRL algorithms, by learning hyperparameter schedules on the fly. In particular, the PB2 algorithm is able to achieve strong performance in RL tasks by formulating online hyperparameter optimization as time varying GP-bandit problem, while also providing theoretical guarantees. However, PB2 is only designed to work for continuous hyperparameters, which severely limits its utility in practice. In this paper we introduce a new (provably) efficient hierarchical approach for optimizing both continuous and categorical variables, using a new time-varying bandit algorithm specifically designed for the population based training regime. We evaluate our approach on the challenging Procgen benchmark, where we show that explicitly modelling dependence between data augmentation and other hyperparameters improves generalization.",0
"Although reinforcement learning (RL) has seen some successes recently, many RL algorithms remain sensitive to hyperparameters. Therefore, there has been growing interest in AutoRL, which aims to automate design decisions for more general algorithms. One approach that has shown promise is population-based optimization, which learns hyperparameter schedules on the fly. However, the PB2 algorithm is limited to continuous hyperparameters. This paper introduces a new hierarchical approach that optimizes both continuous and categorical variables using a new time-varying bandit algorithm designed for population-based training. The approach is evaluated on the Procgen benchmark, where it is shown that explicitly modeling dependence between data augmentation and other hyperparameters improves generalization.",1
"Recent works demonstrate that deep reinforcement learning (DRL) models are vulnerable to adversarial attacks which can decrease the victim's total reward by manipulating the observations. Compared with adversarial attacks in supervised learning, it is much more challenging to deceive a DRL model since the adversary has to infer the environmental dynamics. To address this issue, we reformulate the problem of adversarial attacks in function space and separate the previous gradient based attacks into several subspace. Following the analysis of the function space, we design a generic two-stage framework in the subspace where the adversary lures the agent to a target trajectory or a deceptive policy. In the first stage, we train a deceptive policy by hacking the environment, and discover a set of trajectories routing to the lowest reward. The adversary then misleads the victim to imitate the deceptive policy by perturbing the observations. Our method provides a tighter theoretical upper bound for the attacked agent's performance than the existing approaches. Extensive experiments demonstrate the superiority of our method and we achieve the state-of-the-art performance on both Atari and MuJoCo environments.",0
"Recent research has shown that deep reinforcement learning (DRL) models are susceptible to adversarial attacks that can manipulate the observations and decrease the total reward of the victim. Compared to adversarial attacks in supervised learning, deceiving a DRL model is more challenging as the adversary must infer the environmental dynamics. To tackle this issue, we reframe the problem of adversarial attacks in function space and divide the previous gradient-based attacks into multiple subspaces. After analyzing the function space, we devise a two-stage framework in the subspace where the adversary entices the agent to a target trajectory or a deceptive policy. In the initial stage, we train a deceptive policy by hacking the environment and identify a set of trajectories that lead to the lowest reward. The adversary then misleads the victim by manipulating the observations to imitate the deceptive policy. Our technique offers a more accurate theoretical upper bound for the attacked agent's performance compared to existing methods. Our extensive experiments demonstrate the superiority of our approach, and we attain the best performance on both Atari and MuJoCo environments.",1
"Episodic reinforcement learning and contextual bandits are two widely studied sequential decision-making problems. Episodic reinforcement learning generalizes contextual bandits and is often perceived to be more difficult due to long planning horizon and unknown state-dependent transitions. The current paper shows that the long planning horizon and the unknown state-dependent transitions (at most) pose little additional difficulty on sample complexity.   We consider the episodic reinforcement learning with $S$ states, $A$ actions, planning horizon $H$, total reward bounded by $1$, and the agent plays for $K$ episodes. We propose a new algorithm, \textbf{M}onotonic \textbf{V}alue \textbf{P}ropagation (MVP), which relies on a new Bernstein-type bonus. Compared to existing bonus constructions, the new bonus is tighter since it is based on a well-designed monotonic value function. In particular, the \emph{constants} in the bonus should be subtly setting to ensure optimism and monotonicity.   We show MVP enjoys an $O\left(\left(\sqrt{SAK} + S^2A\right) \poly\log \left(SAHK\right)\right)$ regret, approaching the $\Omega\left(\sqrt{SAK}\right)$ lower bound of \emph{contextual bandits} up to logarithmic terms. Notably, this result 1) \emph{exponentially} improves the state-of-the-art polynomial-time algorithms by Dann et al. [2019] and Zanette et al. [2019] in terms of the dependency on $H$, and 2) \emph{exponentially} improves the running time in [Wang et al. 2020] and significantly improves the dependency on $S$, $A$ and $K$ in sample complexity.",0
"Two sequential decision-making problems that are extensively researched are episodic reinforcement learning and contextual bandits. Episodic reinforcement learning is considered more challenging because of its extended planning horizon and the uncertainty of state-dependent transitions. However, this paper demonstrates that these factors do not significantly increase sample complexity. The proposed algorithm, Monotonic Value Propagation (MVP), utilizes a new Bernstein-type bonus that is based on a well-designed monotonic value function, resulting in tighter bounds. MVP has a regret of O((sqrt(SAK) + S^2A) polylog(SAHK)), which approaches the lower bound of contextual bandits up to logarithmic terms. This result improves upon existing polynomial-time algorithms and reduces the dependency on H, S, A, and K in sample complexity.",1
"Multi-goal reinforcement learning is widely applied in planning and robot manipulation. Two main challenges in multi-goal reinforcement learning are sparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims to tackle the two challenges via goal relabeling. However, HER-related works still need millions of samples and a huge computation. In this paper, we propose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step relabeled returns based on $n$-step relabeling to improve sample efficiency. Despite the advantages of $n$-step relabeling, we theoretically and experimentally prove the off-policy $n$-step bias introduced by $n$-step relabeling may lead to poor performance in many environments. To address the above issue, two bias-reduced MHER algorithms, MHER($\lambda$) and Model-based MHER (MMHER) are presented. MHER($\lambda$) exploits the $\lambda$ return while MMHER benefits from model-based value expansions. Experimental results on numerous multi-goal robotic tasks show that our solutions can successfully alleviate off-policy $n$-step bias and achieve significantly higher sample efficiency than HER and Curriculum-guided HER with little additional computation beyond HER.",0
"Planning and robot manipulation frequently involve multi-goal reinforcement learning, which poses challenges such as sparse rewards and sample inefficiency. Goal relabeling through Hindsight Experience Replay (HER) has been used to address these issues, but it requires millions of samples and extensive computation. This paper presents Multi-step Hindsight Experience Replay (MHER), which employs multi-step relabeled returns to enhance sample efficiency. However, the off-policy $n$-step bias introduced by $n$-step relabeling can lead to inferior performance in various environments. To overcome this, bias-reduced MHER algorithms are proposed, namely MHER($\lambda$) and Model-based MHER (MMHER). MHER($\lambda$) utilizes the $\lambda$ return, while MMHER benefits from model-based value expansions. Experimental results on multiple multi-goal robotic tasks demonstrate that our solutions effectively alleviate off-policy $n$-step bias and achieve significantly higher sample efficiency than HER and Curriculum-guided HER, with minimal additional computation.",1
"Deep Reinforcement Learning (RL) techniques can benefit greatly from leveraging prior experience, which can be either self-generated or acquired from other entities. Action advising is a framework that provides a flexible way to transfer such knowledge in the form of actions between teacher-student peers. However, due to the realistic concerns, the number of these interactions is limited with a budget; therefore, it is crucial to perform these in the most appropriate moments. There have been several promising studies recently that address this problem setting especially from the student's perspective. Despite their success, they have some shortcomings when it comes to the practical applicability and integrity as an overall solution to the learning from advice challenge. In this paper, we extend the idea of advice reusing via teacher imitation to construct a unified approach that addresses both advice collection and advice utilisation problems. We also propose a method to automatically tune the relevant hyperparameters of these components on-the-fly to make it able to adapt to any task with minimal human intervention. The experiments we performed in 5 different Atari games verify that our algorithm either surpasses or performs on-par with its top competitors while being far simpler to be employed. Furthermore, its individual components are also found to be providing significant advantages alone.",0
"To enhance Deep Reinforcement Learning (RL), previous experience can be beneficial, whether it is self-generated or acquired from external sources. Action advising is a flexible framework that enables knowledge transfer through actions between teacher-student peers. However, due to budget limitations, the number of interactions is limited, making it crucial to perform them at the most appropriate moments. Recent studies have addressed this issue from the student's perspective, but practical applicability and overall solution integrity are still problematic. This paper proposes a unified approach that tackles advice collection and utilization issues through teacher imitation. We also suggest an on-the-fly method to adjust relevant hyperparameters with minimal human intervention, making it adaptable to any task. Our experiments demonstrate that our algorithm either outperforms or performs on-par with its top competitors while being significantly simpler to use. Additionally, its individual components provide significant advantages on their own.",1
"Most modern reinforcement learning algorithms optimize a cumulative single-step cost along a trajectory. The optimized motions are often 'unnatural', representing, for example, behaviors with sudden accelerations that waste energy and lack predictability. In this work, we present a novel paradigm of controlling nonlinear systems via the minimization of the Koopman spectrum cost: a cost over the Koopman operator of the controlled dynamics. This induces a broader class of dynamical behaviors that evolve over stable manifolds such as nonlinear oscillators, closed loops, and smooth movements. We demonstrate that some dynamics realizations that are not possible with a cumulative cost are feasible in this paradigm. Moreover, we present a provably efficient online learning algorithm for our problem that enjoys a sub-linear regret bound under some structural assumptions.",0
"Reinforcement learning algorithms commonly optimize a cumulative single-step cost along a trajectory, resulting in unnatural behaviors that waste energy and lack predictability. This study introduces a new approach to controlling nonlinear systems by minimizing the Koopman spectrum cost, which broadens the range of possible dynamical behaviors to include nonlinear oscillators, closed loops, and smooth movements that evolve over stable manifolds. The authors demonstrate the feasibility of dynamics realizations that were previously impossible with a cumulative cost. Additionally, they present an online learning algorithm designed for their problem that has a sub-linear regret bound under certain structural assumptions.",1
"Current model-based reinforcement learning methods struggle when operating from complex visual scenes due to their inability to prioritize task-relevant features. To mitigate this problem, we propose learning Task Informed Abstractions (TIA) that explicitly separates reward-correlated visual features from distractors. For learning TIA, we introduce the formalism of Task Informed MDP (TiMDP) that is realized by training two models that learn visual features via cooperative reconstruction, but one model is adversarially dissociated from the reward signal. Empirical evaluation shows that TIA leads to significant performance gains over state-of-the-art methods on many visual control tasks where natural and unconstrained visual distractions pose a formidable challenge.",0
"Reinforcement learning methods based on models face challenges when operating from intricate visual scenes because they cannot prioritize task-relevant features. To address this issue, we suggest learning Task Informed Abstractions (TIA) that distinguish reward-correlated visual features from distractors. We introduce Task Informed MDP (TiMDP) formalism to learn TIA, which involves training two models to learn visual features through cooperative reconstruction, but one model is adversarially dissociated from the reward signal. Empirical assessment reveals that TIA significantly enhances performance compared to state-of-the-art methods in many visual control tasks where natural and unconstrained visual distractions are a challenging obstacle.",1
"This paper surveys the field of multiagent deep reinforcement learning. The combination of deep neural networks with reinforcement learning has gained increased traction in recent years and is slowly shifting the focus from single-agent to multiagent environments. Dealing with multiple agents is inherently more complex as (a) the future rewards depend on the joint actions of multiple players and (b) the computational complexity of functions increases. We present the most common multiagent problem representations and their main challenges, and identify five research areas that address one or more of these challenges: centralised training and decentralised execution, opponent modelling, communication, efficient coordination, and reward shaping. We find that many computational studies rely on unrealistic assumptions or are not generalisable to other settings; they struggle to overcome the curse of dimensionality or nonstationarity. Approaches from psychology and sociology capture promising relevant behaviours such as communication and coordination. We suggest that, for multiagent reinforcement learning to be successful, future research addresses these challenges with an interdisciplinary approach to open up new possibilities for more human-oriented solutions in multiagent reinforcement learning.",0
"The focus of this paper is on multiagent deep reinforcement learning, which combines reinforcement learning with deep neural networks. This approach has gained popularity in recent years and is gradually shifting attention from single-agent to multi-agent settings. However, dealing with multiple agents is inherently more complex due to the joint actions of players and increased computational complexity. The paper discusses the most common multiagent problem representations, their main challenges, and identifies five research areas that address these challenges. Nonetheless, many computational studies rely on unrealistic assumptions and struggle to overcome issues such as the curse of dimensionality or nonstationarity. Conversely, approaches from psychology and sociology capture promising behaviors such as communication and coordination. The paper suggests that future research should take an interdisciplinary approach to address these challenges and open up new possibilities for more human-oriented solutions in multiagent reinforcement learning.",1
"The Orienteering Problem with Time Windows (OPTW) is a combinatorial optimization problem where the goal is to maximize the total score collected from different visited locations. The application of neural network models to combinatorial optimization has recently shown promising results in dealing with similar problems, like the Travelling Salesman Problem. A neural network allows learning solutions using reinforcement learning or supervised learning, depending on the available data. After the learning stage, it can be generalized and quickly fine-tuned to further improve performance and personalization. The advantages are evident since, for real-world applications, solution quality, personalization, and execution times are all important factors that should be taken into account.   This study explores the use of Pointer Network models trained using reinforcement learning to solve the OPTW problem. We propose a modified architecture that leverages Pointer Networks to better address problems related with dynamic time-dependent constraints. Among its various applications, the OPTW can be used to model the Tourist Trip Design Problem (TTDP). We train the Pointer Network with the TTDP problem in mind, by sampling variables that can change across tourists visiting a particular instance-region: starting position, starting time, available time, and the scores given to each point of interest. Once a model-region is trained, it can infer a solution for a particular tourist using beam search. We based the assessment of our approach on several existing benchmark OPTW instances. We show that it generalizes across different tourists that visit each region and that it generally outperforms the most commonly used heuristic, while computing the solution in realistic times.",0
"The OPTW is a problem in combinatorial optimization that aims to maximize the total score collected from various visited locations. Recently, neural network models have shown promise in solving similar optimization problems, such as the Travelling Salesman Problem, by using reinforcement or supervised learning. These models can be quickly fine-tuned to improve performance and personalization, making them advantageous for real-world applications where solution quality, personalization, and execution times are essential. In this study, we explore the use of Pointer Network models trained with reinforcement learning to tackle the OPTW problem. We propose a modified architecture that uses Pointer Networks to address issues related to dynamic time-dependent constraints. The OPTW can be applied to model the Tourist Trip Design Problem (TTDP), and we train the Pointer Network with this problem in mind. We sample variables that can change across tourists visiting a specific region, such as starting position, starting time, available time, and scores given to each point of interest. Once a model-region is trained, it can infer a solution for a tourist using beam search. Our approach is evaluated on numerous benchmark OPTW instances and has shown that it generalizes well across different tourists visiting each region, outperforming the commonly used heuristic while computing realistic solution times.",1
"The generalization gap in reinforcement learning (RL) has been a significant obstacle that prevents the RL agent from learning general skills and adapting to varying environments. Increasing the generalization capacity of the RL systems can significantly improve their performance on real-world working environments. In this work, we propose a novel policy-aware adversarial data augmentation method to augment the standard policy learning method with automatically generated trajectory data. Different from the commonly used observation transformation based data augmentations, our proposed method adversarially generates new trajectory data based on the policy gradient objective and aims to more effectively increase the RL agent's generalization ability with the policy-aware data augmentation. Moreover, we further deploy a mixup step to integrate the original and generated data to enhance the generalization capacity while mitigating the over-deviation of the adversarial data. We conduct experiments on a number of RL tasks to investigate the generalization performance of the proposed method by comparing it with the standard baselines and the state-of-the-art mixreg approach. The results show our method can generalize well with limited training diversity, and achieve the state-of-the-art generalization test performance.",0
"The obstacle of the generalization gap in reinforcement learning (RL) hinders the ability of RL agents to learn general skills and adapt to different environments. Improving the generalization capacity of RL systems can enhance their performance in real-world working environments. To address this issue, we introduce a new method that utilizes policy-aware adversarial data augmentation to augment the standard policy learning method with automatically generated trajectory data. Our proposed method generates new trajectory data based on the policy gradient objective, rather than observation transformation based data augmentations, resulting in improved generalization ability of the RL agent. Additionally, we integrate the original and generated data using a mixup step to enhance the generalization capacity while reducing the over-deviation of the adversarial data. We conducted experiments on various RL tasks to compare our method with standard baselines and the state-of-the-art mixreg approach. Our results demonstrate that our proposed method performs well with limited training diversity and achieves state-of-the-art generalization test performance.",1
"Learning to flexibly follow task instructions in dynamic environments poses interesting challenges for reinforcement learning agents. We focus here on the problem of learning control flow that deviates from a strict step-by-step execution of instructions -- that is, control flow that may skip forward over parts of the instructions or return backward to previously completed or skipped steps. Demand for such flexible control arises in two fundamental ways: explicitly when control is specified in the instructions themselves (such as conditional branching and looping) and implicitly when stochastic environment dynamics require re-completion of instructions whose effects have been perturbed, or opportunistic skipping of instructions whose effects are already present. We formulate an attention-based architecture that meets these challenges by learning, from task reward only, to flexibly attend to and condition behavior on an internal encoding of the instructions. We test the architecture's ability to learn both explicit and implicit control in two illustrative domains -- one inspired by Minecraft and the other by StarCraft -- and show that the architecture exhibits zero-shot generalization to novel instructions of length greater than those in a training set, at a performance level unmatched by two baseline recurrent architectures and one ablation architecture.",0
"Reinforcement learning agents face intriguing difficulties when it comes to adapting to task instructions in dynamic surroundings. Our focus is on the task of mastering control flow that deviates from a strictly sequential execution of instructions. This involves control flow that skips over parts of the instructions or loops back to previously completed or skipped steps. The need for flexible control arises in two fundamental ways: explicitly when control is specified in the instructions themselves, and implicitly when stochastic environment dynamics require re-completion of instructions or skipping of instructions whose effects are already present. To overcome these challenges, we have developed an attention-based architecture that uses task reward to learn how to flexibly attend to and condition behavior based on an internal encoding of the instructions. Our architecture has been tested in two different domains, inspired by Minecraft and StarCraft, and has demonstrated its ability to learn both explicit and implicit control. We have also shown that our architecture exhibits zero-shot generalization to novel instructions of greater lengths than those in a training set, surpassing the performance level of two baseline recurrent architectures and one ablation architecture.",1
"With the development of deep networks on various large-scale datasets, a large zoo of pretrained models are available. When transferring from a model zoo, applying classic single-model based transfer learning methods to each source model suffers from high computational burden and cannot fully utilize the rich knowledge in the zoo. We propose \emph{Zoo-Tuning} to address these challenges, which learns to adaptively transfer the parameters of pretrained models to the target task. With the learnable channel alignment layer and adaptive aggregation layer, Zoo-Tuning \emph{adaptively aggregates channel aligned pretrained parameters} to derive the target model, which promotes knowledge transfer by simultaneously adapting multiple source models to downstream tasks. The adaptive aggregation substantially reduces the computation cost at both training and inference. We further propose lite Zoo-Tuning with the temporal ensemble of batch average gating values to reduce the storage cost at the inference time. We evaluate our approach on a variety of tasks, including reinforcement learning, image classification, and facial landmark detection. Experiment results demonstrate that the proposed adaptive transfer learning approach can transfer knowledge from a zoo of models more effectively and efficiently.",0
"A vast array of pretrained models is now available due to the development of deep networks on diverse large-scale datasets. However, applying classic transfer learning methods to each source model from the model zoo results in significant computational burden and an inability to fully exploit the wealth of knowledge available in the zoo. To overcome these challenges, we present \emph{Zoo-Tuning}, which utilizes adaptive transfer learning to fine-tune the parameters of pretrained models for the target task. This is achieved through the incorporation of a learnable channel alignment layer and an adaptive aggregation layer that can adaptively align and aggregate the parameters of multiple source models for the downstream task. The adaptive aggregation layer reduces computational costs during both training and inference. Additionally, we propose lite Zoo-Tuning, which leverages the temporal ensemble of batch average gating values to reduce storage costs during inference. Our method has been evaluated on diverse tasks, including reinforcement learning, image classification, and facial landmark detection, and has yielded promising results in terms of both effectiveness and efficiency.",1
"We propose a novel algorithm named Expert Q-learning. Expert Q-learning was inspired by Dueling Q-learning and aimed at incorporating the ideas from semi-supervised learning into reinforcement learning through splitting Q-values into state values and action advantages. Different from Generative Adversarial Imitation Learning and Deep Q-Learning from Demonstrations, the offline expert we have used only predicts the value of a state from {-1, 0, 1}, indicating whether this is a bad, neutral or good state. An expert network was designed in addition to the Q-network, which updates each time following the regular offline minibatch update whenever the expert example buffer is not empty. The Q-network plays the role of the advantage function only during the update. Our algorithm also keeps asynchronous copies of the Q-network and expert network, predicting the target values using the same manner as of Double Q-learning.   We compared on the game of Othello our algorithm with the state-of-the-art Q-learning algorithm, which was a combination of Double Q-learning and Dueling Q-learning. The results showed that Expert Q-learning was indeed useful and more resistant to the overestimation bias of Q-learning. The baseline Q-learning algorithm exhibited unstable and suboptimal behavior, especially when playing against a stochastic player, whereas Expert Q-learning demonstrated more robust performance with higher scores. Expert Q-learning without using examples has also gained better results than the baseline algorithm when trained and tested against a fixed player. On the other hand, Expert Q-learning without examples cannot win against the baseline Q-learning algorithm in direct game competitions despite the fact that it has also shown the strength of reducing the overestimation bias.",0
"Introducing a new algorithm called Expert Q-learning. This algorithm draws inspiration from Dueling Q-learning and aims to incorporate semi-supervised learning concepts into reinforcement learning by separating Q-values into state values and action advantages. Unlike other algorithms such as Generative Adversarial Imitation Learning and Deep Q-Learning from Demonstrations, we only use an offline expert to predict the state value (-1, 0, or 1) indicating whether the state is bad, neutral, or good. In addition to the Q-network, we designed an expert network that updates whenever the expert example buffer is not empty. The Q-network acts as the advantage function during updates. Our algorithm also employs asynchronous copies of the Q-network and expert network, and predicts target values in the same manner as Double Q-learning. We compared our algorithm with a state-of-the-art Q-learning algorithm in the game of Othello and found that Expert Q-learning was more resistant to overestimation bias and demonstrated more robust performance with higher scores, especially when playing against a stochastic player. Expert Q-learning without examples also outperformed the baseline algorithm when trained and tested against a fixed player. However, Expert Q-learning without examples was unable to win against the baseline Q-learning algorithm in direct game competitions, despite reducing overestimation bias.",1
"We present a novel method for graph partitioning, based on reinforcement learning and graph convolutional neural networks. Our approach is to recursively partition coarser representations of a given graph. The neural network is implemented using SAGE graph convolution layers, and trained using an advantage actor critic (A2C) agent. We present two variants, one for finding an edge separator that minimizes the normalized cut or quotient cut, and one that finds a small vertex separator. The vertex separators are then used to construct a nested dissection ordering to permute a sparse matrix so that its triangular factorization will incur less fill-in. The partitioning quality is compared with partitions obtained using METIS and SCOTCH, and the nested dissection ordering is evaluated in the sparse solver SuperLU. Our results show that the proposed method achieves similar partitioning quality as METIS and SCOTCH. Furthermore, the method generalizes across different classes of graphs, and works well on a variety of graphs from the SuiteSparse sparse matrix collection.",0
"A new technique for partitioning graphs is introduced in this study, which employs reinforcement learning and graph convolutional neural networks. The strategy involves partitioning a given graph recursively using coarser representations. The neural network utilizes SAGE graph convolution layers and is trained with an advantage actor critic (A2C) agent. Two versions of the method are presented: one for discovering an edge separator that reduces the normalized or quotient cut, and the other for detecting a small vertex separator. The vertex separators are subsequently employed to create a nested dissection ordering that rearranges a sparse matrix so that its triangular factorization generates less fill-in. The efficacy of the partitioning is compared to that obtained with METIS and SCOTCH, while the nested dissection ordering is evaluated in the sparse solver SuperLU. The findings demonstrate that the proposed approach achieves comparable partitioning performance to METIS and SCOTCH. Furthermore, the method is versatile across various types of graphs and works effectively on different graphs from the SuiteSparse sparse matrix collection.",1
"An important challenge in reinforcement learning is training agents that can solve a wide variety of tasks. If tasks depend on each other (e.g. needing to learn to walk before learning to run), curriculum learning can speed up learning by focusing on the next best task to learn. We explore curriculum learning in a complex, visual domain with many hard exploration challenges: Minecraft. We find that learning progress (defined as a change in success probability of a task) is a reliable measure of learnability for automatically constructing an effective curriculum. We introduce a learning-progress based curriculum and test it on a complex reinforcement learning problem (called ""Simon Says"") where an agent is instructed to obtain a desired goal item. Many of the required skills depend on each other. Experiments demonstrate that: (1) a within-episode exploration bonus for obtaining new items improves performance, (2) dynamically adjusting this bonus across training such that it only applies to items the agent cannot reliably obtain yet further increases performance, (3) the learning-progress based curriculum elegantly follows the learning curve of the agent, and (4) when the learning-progress based curriculum is combined with the dynamic exploration bonus it learns much more efficiently and obtains far higher performance than uniform baselines. These results suggest that combining intra-episode and across-training exploration bonuses with learning progress creates a promising method for automated curriculum generation, which may substantially increase our ability to train more capable, generally intelligent agents.",0
"Reinforcement learning faces the challenge of teaching agents to tackle diverse tasks. Curriculum learning is a potential solution, particularly when tasks are interdependent. For instance, learning to walk is a prerequisite to running. In this study, we apply curriculum learning to the complex visual environment of Minecraft, which poses several exploration challenges. We propose a learning-progress based curriculum as a reliable measure of learnability and apply it to the ""Simon Says"" reinforcement learning problem. Our experiments show that the within-episode exploration bonus improves performance, and dynamically adjusting this bonus across training further enhances it. Additionally, the learning-progress based curriculum tracks the agent's learning curve and, when combined with the dynamic exploration bonus, outperforms uniform baselines. These findings indicate that incorporating intra-episode and across-training exploration bonuses with learning progress could facilitate the automated generation of curriculums and enable the training of more intelligent agents.",1
"We introduce the ""inverse bandit"" problem of estimating the rewards of a multi-armed bandit instance from observing the learning process of a low-regret demonstrator. Existing approaches to the related problem of inverse reinforcement learning assume the execution of an optimal policy, and thereby suffer from an identifiability issue. In contrast, our paradigm leverages the demonstrator's behavior en route to optimality, and in particular, the exploration phase, to obtain consistent reward estimates. We develop simple and efficient reward estimation procedures for demonstrations within a class of upper-confidence-based algorithms, showing that reward estimation gets progressively easier as the regret of the algorithm increases. We match these upper bounds with information-theoretic lower bounds that apply to any demonstrator algorithm, thereby characterizing the optimal tradeoff between exploration and reward estimation. Extensive empirical evaluations on both synthetic data and simulated experimental design data from the natural sciences corroborate our theoretical results.",0
"We present a novel problem called the ""inverse bandit"" problem, which involves estimating the rewards of a multi-armed bandit instance by observing the learning process of a low-regret demonstrator. Current methods for the related problem of inverse reinforcement learning encounter an identifiability issue as they assume the execution of an optimal policy. Our approach, on the other hand, utilizes the demonstrator's behavior during the exploration phase to obtain consistent reward estimates. We provide straightforward and effective reward estimation techniques for demonstrations within a class of upper-confidence-based algorithms, with results indicating that the estimation process becomes easier as the algorithm's regret increases. By matching these upper bounds with information-theoretic lower bounds applicable to any demonstrator algorithm, we establish the optimal tradeoff between exploration and reward estimation. Empirical analyses, including both synthetic data and simulated experimental design data from the natural sciences, validate our theoretical findings.",1
"An in-depth understanding of the particular environment is crucial in reinforcement learning (RL). To address this challenge, the decision-making process of a mobile collaborative robotic assistant modeled by the Markov decision process (MDP) framework is studied in this paper. The optimal state-action combinations of the MDP are calculated with the non-linear Bellman optimality equations. This system of equations can be solved with relative ease by the computational power of Wolfram Mathematica, where the obtained optimal action-values point to the optimal policy. Unlike other RL algorithms, this methodology does not approximate the optimal behavior, it gives the exact, explicit solution, which provides a strong foundation for our study. With this, we offer new insights into understanding the action selection mechanisms in RL by presenting various small modifications on the very same schema that lead to different optimal policies.",0
"Reinforcement learning (RL) requires a thorough comprehension of the environment. To tackle this challenge, this paper delves into the decision-making process of a mobile collaborative robotic assistant using the Markov decision process (MDP) framework. The MDP's optimal state-action combinations are determined using non-linear Bellman optimality equations. These equations can be readily solved using the computational power of Wolfram Mathematica. The optimal action-values obtained from this methodology indicate the optimal policy, which is an explicit and exact solution, unlike other RL algorithms that approximate optimal behavior. This provides a solid foundation for our study, offering fresh insights into RL's action selection mechanisms by demonstrating how minor modifications to the same schema can lead to diverse optimal policies.",1
"We review the role of information and learning in the stability and optimization of queueing systems. In recent years, techniques from supervised learning, bandit learning and reinforcement learning have been applied to queueing systems supported by increasing role of information in decision making. We present observations and new results that help rationalize the application of these areas to queueing systems.   We prove that the MaxWeight and BackPressure policies are an application of Blackwell's Approachability Theorem. This connects queueing theoretic results with adversarial learning. We then discuss the requirements of statistical learning for service parameter estimation. As an example, we show how queue size regret can be bounded when applying a perceptron algorithm to classify service. Next, we discuss the role of state information in improved decision making. Here we contrast the roles of epistemic information (information on uncertain parameters) and aleatoric information (information on an uncertain state). Finally we review recent advances in the theory of reinforcement learning and queueing, as well as, provide discussion on current research challenges.",0
"The article examines the influence of information and learning on the stability and optimization of queueing systems. Recently, supervised learning, bandit learning, and reinforcement learning techniques have been implemented in queueing systems, highlighting the significance of information in decision-making. The authors present their findings and conclusions, which justify the application of these methods in queueing systems. They demonstrate that the MaxWeight and BackPressure policies are an application of Blackwell's Approachability Theorem, connecting queueing theory with adversarial learning. The authors also analyze the statistical learning needed for service parameter estimation and illustrate how a perceptron algorithm can be used to limit queue size regret. Additionally, they compare the roles of epistemic and aleatoric information in improved decision-making. Finally, they provide an overview of recent advances in the theory of reinforcement learning and queueing, and discuss current research challenges.",1
"Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots. The space of 3D articulated objects is exceptionally rich in their myriad semantic categories, diverse shape geometry, and complicated part functionality. Previous works mostly abstract kinematic structure with estimated joint parameters and part poses as the visual representations for manipulating 3D articulated objects. In this paper, we propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals. We design an interaction-for-perception framework VAT-Mart to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes. Experiments prove the effectiveness of the proposed approach using the large-scale PartNet-Mobility dataset in SAPIEN environment and show promising generalization capabilities to novel test shapes, unseen object categories, and real-world data. Project page: https://hyperplane-lab.github.io/vat-mart",0
"Future home-assistant robots face a challenging task of perceiving and manipulating 3D articulated objects such as cabinets and doors in human environments. The space of 3D articulated objects is diverse in its semantic categories, shape geometry, and part functionality. Prior works have mostly used estimated joint parameters and part poses as visual representations for manipulating 3D articulated objects. However, this paper proposes a novel perception-interaction handshaking point, object-centric actionable visual priors, to provide more actionable guidance than kinematic structure estimation. This is achieved by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals. The proposed approach, called VAT-Mart, designs an interaction-for-perception framework to simultaneously train a curiosity-driven reinforcement learning policy and a perception module to learn such actionable visual representations. The experiments conducted using the large-scale PartNet-Mobility dataset in SAPIEN environment show promising results in terms of effectiveness and generalization capabilities to novel test shapes, unseen object categories, and real-world data. A project page has been set up for more information.",1
"Learning efficiently a causal model of the environment is a key challenge of model-based RL agents operating in POMDPs. We consider here a scenario where the learning agent has the ability to collect online experiences through direct interactions with the environment (interventional data), but has also access to a large collection of offline experiences, obtained by observing another agent interacting with the environment (observational data). A key ingredient, that makes this situation non-trivial, is that we allow the observed agent to interact with the environment based on hidden information, which is not observed by the learning agent. We then ask the following questions: can the online and offline experiences be safely combined for learning a causal model ? And can we expect the offline experiences to improve the agent's performances ? To answer these questions, we import ideas from the well-established causal framework of do-calculus, and we express model-based reinforcement learning as a causal inference problem. Then, we propose a general yet simple methodology for leveraging offline data during learning. In a nutshell, the method relies on learning a latent-based causal transition model that explains both the interventional and observational regimes, and then using the recovered latent variable to infer the standard POMDP transition model via deconfounding. We prove our method is correct and efficient in the sense that it attains better generalization guarantees due to the offline data (in the asymptotic case), and we illustrate its effectiveness empirically on synthetic toy problems. Our contribution aims at bridging the gap between the fields of reinforcement learning and causality.",0
"A major challenge for model-based reinforcement learning (RL) agents operating in partially observable Markov decision processes (POMDPs) is efficiently learning a causal model of the environment. In this study, we consider a scenario where the learning agent can collect online experiences through direct interactions with the environment and also has access to a vast collection of offline experiences obtained by observing another agent's interactions. This situation is complicated because the observed agent may interact with the environment based on hidden information that is not observed by the learning agent. We explore whether the online and offline experiences can be safely combined to learn a causal model and if the offline experiences can improve the agent's performance. To answer these questions, we apply ideas from the well-established causal framework of do-calculus and frame model-based RL as a causal inference problem. We propose a general yet straightforward methodology for leveraging offline data during learning, which involves learning a latent-based causal transition model that explains both interventional and observational regimes. We then use the recovered latent variable to infer the standard POMDP transition model via deconfounding. We prove our method is correct and efficient, attaining better generalization guarantees due to the offline data in the asymptotic case. We also illustrate its effectiveness empirically on synthetic toy problems. Our contribution aims to bridge the gap between the fields of reinforcement learning and causality.",1
"We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all levels of the embodied AI stack - data, simulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an artist-authored, annotated, reconfigurable 3D dataset of apartments (matching real spaces) with articulated objects (e.g. cabinets and drawers that can open/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with speeds exceeding 25,000 simulation steps per second (850x real-time) on an 8-GPU node, representing 100x speed-ups over prior work; and, (iii) Home Assistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy the house, prepare groceries, set the table) that test a range of mobile manipulation capabilities. These large-scale engineering contributions allow us to systematically compare deep reinforcement learning (RL) at scale and classical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with an emphasis on generalization to new objects, receptacles, and layouts. We find that (1) flat RL policies struggle on HAB compared to hierarchical ones; (2) a hierarchy with independent skills suffers from 'hand-off problems', and (3) SPA pipelines are more brittle than RL policies.",0
"Habitat 2.0 (H2.0) is a simulation platform designed to train virtual robots in interactive 3D environments with complex physics-enabled scenarios. Our contribution spans across all levels of the embodied AI stack, including data, simulation, and benchmark tasks. Our contributions include: (i) ReplicaCAD, a 3D dataset of apartments with articulated objects, (ii) H2.0, a high-performance physics-enabled 3D simulator, and (iii) Home Assistant Benchmark (HAB), a suite of common tasks for assistive robots. These contributions allow us to compare deep reinforcement learning (RL) and classical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with an emphasis on generalization to new objects, receptacles, and layouts. Our findings show that flat RL policies struggle compared to hierarchical ones, hierarchy with independent skills suffers from hand-off problems, and SPA pipelines are more brittle than RL policies.",1
"Various algorithms in reinforcement learning exhibit dramatic variability in their convergence rates and ultimate accuracy as a function of the problem structure. Such instance-specific behavior is not captured by existing global minimax bounds, which are worst-case in nature. We analyze the problem of estimating optimal $Q$-value functions for a discounted Markov decision process with discrete states and actions and identify an instance-dependent functional that controls the difficulty of estimation in the $\ell_\infty$-norm. Using a local minimax framework, we show that this functional arises in lower bounds on the accuracy on any estimation procedure. In the other direction, we establish the sharpness of our lower bounds, up to factors logarithmic in the state and action spaces, by analyzing a variance-reduced version of $Q$-learning. Our theory provides a precise way of distinguishing ""easy"" problems from ""hard"" ones in the context of $Q$-learning, as illustrated by an ensemble with a continuum of difficulty.",0
"Different algorithms in the field of reinforcement learning have varying rates of convergence and levels of accuracy depending on the problem at hand. These individual behaviors cannot be captured by current global minimax bounds, which only consider worst-case scenarios. Our study focuses on estimating optimal $Q$-value functions for a discounted Markov decision process with discrete states and actions. We identify a situation-specific functional that determines the complexity of estimation in the $\ell_\infty$-norm and demonstrate that it plays a crucial role in determining the accuracy of any estimation procedure using a local minimax framework. We prove the accuracy of our lower bounds, up to logarithmic factors in the state and action spaces, by analyzing a variance-reduced version of $Q$-learning. Our theory can differentiate between ""easy"" and ""hard"" problems in $Q$-learning, as shown by an ensemble with a range of difficulty levels.",1
"We consider Markov Decision Processes (MDPs) with deterministic transitions and study the problem of regret minimization, which is central to the analysis and design of optimal learning algorithms. We present logarithmic problem-specific regret lower bounds that explicitly depend on the system parameter (in contrast to previous minimax approaches) and thus, truly quantify the fundamental limit of performance achievable by any learning algorithm. Deterministic MDPs can be interpreted as graphs and analyzed in terms of their cycles, a fact which we leverage in order to identify a class of deterministic MDPs whose regret lower bound can be determined numerically. We further exemplify this result on a deterministic line search problem, and a deterministic MDP with state-dependent rewards, whose regret lower bounds we can state explicitly. These bounds share similarities with the known problem-specific bound of the multi-armed bandit problem and suggest that navigation on a deterministic MDP need not have an effect on the performance of a learning algorithm.",0
"Our focus is on deterministic Markov Decision Processes (MDPs) and the important problem of minimizing regret, a key aspect of optimal learning algorithm analysis and design. Previous minimax approaches did not explicitly consider system parameters, but we overcome this limitation by presenting problem-specific regret lower bounds that accurately quantify the fundamental limit of performance achievable by any learning algorithm. By leveraging the fact that deterministic MDPs can be represented as graphs and analyzed in terms of cycles, we identify a specific class of deterministic MDPs for which we can numerically determine the regret lower bound. We demonstrate this result on a deterministic line search problem and a deterministic MDP with state-dependent rewards, providing explicit expressions for their regret lower bounds. Interestingly, these bounds share similarities with those of the multi-armed bandit problem, indicating that navigation on a deterministic MDP may not impact learning algorithm performance.",1
"Using a martingale concentration inequality, concentration bounds `from time $n_0$ on' are derived for stochastic approximation algorithms with contractive maps and both martingale difference and Markov noises. These are applied to reinforcement learning algorithms, in particular to asynchronous Q-learning and TD(0).",0
"Concentration bounds for stochastic approximation algorithms with contractive maps and both martingale difference and Markov noises are obtained by utilizing a martingale concentration inequality. These bounds are derived for the time frame starting from $n_0$. Reinforcement learning algorithms, specifically asynchronous Q-learning and TD(0), benefit from the application of these bounds.",1
"Spatial Transformer Networks (STN) can generate geometric transformations which modify input images to improve the classifier's performance. In this work, we combine the idea of STN with Reinforcement Learning (RL). To this end, we break the affine transformation down into a sequence of simple and discrete transformations. We formulate the task as a Markovian Decision Process (MDP) and use RL to solve this sequential decision-making problem. STN architectures learn the transformation parameters by minimizing the classification error and backpropagating the gradients through a sub-differentiable sampling module. In our method, we are not bound to the differentiability of the sampling modules. Moreover, we have freedom in designing the objective rather than only minimizing the error; e.g., we can directly set the target as maximizing the accuracy. We design multiple experiments to verify the effectiveness of our method using cluttered MNIST and Fashion-MNIST datasets and show that our method outperforms STN with a proper definition of MDP components.",0
"The combination of Spatial Transformer Networks (STN) and Reinforcement Learning (RL) can produce geometric transformations that enhance classifier performance. To achieve this, we deconstruct affine transformation into simple and distinct transformations and convert the task into a Markovian Decision Process (MDP), which we solve using RL. STN architectures learn transformation parameters by minimizing classification errors and backpropagating gradients through a sub-differentiable sampling module. Our approach offers more flexibility as we are not limited to differentiable sampling modules, and we can design our objectives to maximize accuracy instead of just minimizing errors. We conducted several experiments using cluttered MNIST and Fashion-MNIST datasets to validate our method, and the results show that our approach outperforms STN when MDP components are correctly defined.",1
"Federated learning utilizes various resources provided by participants to collaboratively train a global model, which potentially address the data privacy issue of machine learning. In such promising paradigm, the performance will be deteriorated without sufficient training data and other resources in the learning process. Thus, it is quite crucial to inspire more participants to contribute their valuable resources with some payments for federated learning. In this paper, we present a comprehensive survey of incentive schemes for federate learning. Specifically, we identify the incentive problem in federated learning and then provide a taxonomy for various schemes. Subsequently, we summarize the existing incentive mechanisms in terms of the main techniques, such as Stackelberg game, auction, contract theory, Shapley value, reinforcement learning, blockchain. By reviewing and comparing some impressive results, we figure out three directions for the future study.",0
"Federated learning is a collaborative approach that utilizes the resources provided by participants to train a global model, which can potentially address data privacy concerns in machine learning. Insufficient training data and resources can lead to a deterioration in performance, making it crucial to encourage more participants to contribute their resources through monetary incentives. This paper presents a comprehensive survey of incentive schemes for federated learning, identifying the incentive problem and providing a taxonomy for various schemes. The existing incentive mechanisms are summarized in terms of techniques such as Stackelberg game, auction, contract theory, Shapley value, reinforcement learning, and blockchain. Based on a review and comparison of impressive results, three directions for future study are identified.",1
"Solving partially-observable Markov decision processes (POMDPs) is critical when applying deep reinforcement learning (DRL) to real-world robotics problems, where agents have an incomplete view of the world. We present graph convolutional memory (GCM) for solving POMDPs using deep reinforcement learning. Unlike recurrent neural networks (RNNs) or transformers, GCM embeds domain-specific priors into the memory recall process via a knowledge graph. By encapsulating priors in the graph, GCM adapts to specific tasks but remains applicable to any DRL task. Using graph convolutions, GCM extracts hierarchical graph features, analogous to image features in a convolutional neural network (CNN). We show GCM outperforms long short-term memory (LSTM), gated transformers for reinforcement learning (GTrXL), and differentiable neural computers (DNCs) on control, long-term non-sequential recall, and 3D navigation tasks while using significantly fewer parameters.",0
"The resolution of partially-observable Markov decision processes (POMDPs) is pivotal in the implementation of deep reinforcement learning (DRL) for real-world robotics challenges, wherein the agents' perspective of the world is incomplete. Our study introduces graph convolutional memory (GCM), which employs DRL to solve POMDPs. Different from recurrent neural networks (RNNs) or transformers, GCM incorporates domain-specific assumptions into the memory recall process through a knowledge graph. By including assumptions in the graph, GCM adapts to specific tasks while remaining relevant to any DRL task. GCM utilizes graph convolutions to extract hierarchical graph features, akin to image features in a convolutional neural network (CNN). Our experiments demonstrate that GCM surpasses long short-term memory (LSTM), gated transformers for reinforcement learning (GTrXL), and differentiable neural computers (DNCs) in control, long-term non-sequential recall, and 3D navigation tasks while utilizing fewer parameters.",1
"Model-based Reinforcement Learning (MBRL) algorithms have been traditionally designed with the goal of learning accurate dynamics of the environment. This introduces a mismatch between the objectives of model-learning and the overall learning problem of finding an optimal policy. Value-aware model learning, an alternative model-learning paradigm to maximum likelihood, proposes to inform model-learning through the value function of the learnt policy. While this paradigm is theoretically sound, it does not scale beyond toy settings. In this work, we propose a novel value-aware objective that is an upper bound on the absolute performance difference of a policy across two models. Further, we propose a general purpose algorithm that modifies the standard MBRL pipeline -- enabling learning with value aware objectives. Our proposed objective, in conjunction with this algorithm, is the first successful instantiation of value-aware MBRL on challenging continuous control environments, outperforming previous value-aware objectives and with competitive performance w.r.t. MLE-based MBRL approaches.",0
"Traditionally, Model-based Reinforcement Learning (MBRL) algorithms were developed to accurately learn the dynamics of the environment, which creates a discrepancy between the objective of model-learning and the overall goal of finding an optimal policy. A new approach, value-aware model learning, aims to inform model-learning through the value function of a learned policy. However, this approach is not scalable beyond simple settings. To address this limitation, we introduce a novel value-aware objective that serves as an upper bound on the absolute performance difference of a policy across two models. We also propose a general-purpose algorithm that modifies the standard MBRL pipeline to enable learning with value-aware objectives. Our approach is the first successful implementation of value-aware MBRL in challenging continuous control environments, surpassing previous value-aware objectives and performing comparably to MLE-based MBRL approaches.",1
"In vision-based reinforcement learning (RL) tasks, it is prevalent to assign the auxiliary task with a surrogate self-supervised loss so as to obtain more semantic representations and improve sample efficiency. However, abundant information in self-supervised auxiliary tasks has been disregarded, since the representation learning part and the decision-making part are separated. To sufficiently utilize information in the auxiliary task, we present a simple yet effective idea to employ self-supervised loss as an intrinsic reward, called Intrinsically Motivated Self-Supervised learning in Reinforcement learning (IM-SSR). We formally show that the self-supervised loss can be decomposed as exploration for novel states and robustness improvement from nuisance elimination. IM-SSR can be effortlessly plugged into any reinforcement learning with self-supervised auxiliary objectives with nearly no additional cost. Combined with IM-SSR, the previous underlying algorithms achieve salient improvements on both sample efficiency and generalization in various vision-based robotics tasks from the DeepMind Control Suite, especially when the reward signal is sparse.",0
"Assigning an auxiliary task with a surrogate self-supervised loss is common in vision-based reinforcement learning (RL) tasks to improve semantic representations and sample efficiency. However, this approach overlooks valuable information in the self-supervised auxiliary tasks by separating representation learning and decision-making. To address this, we propose Intrinsically Motivated Self-Supervised learning in Reinforcement learning (IM-SSR), which employs self-supervised loss as an intrinsic reward to fully utilize the information in the auxiliary task. We demonstrate that the self-supervised loss can be divided into exploration for novel states and robustness improvement from nuisance elimination. IM-SSR can be easily integrated into any reinforcement learning with self-supervised auxiliary objectives at minimal additional cost. When combined with IM-SSR, previous algorithms show significant improvements in sample efficiency and generalization in various vision-based robotics tasks from the DeepMind Control Suite, particularly in cases where the reward signal is sparse.",1
"Robots in many real-world settings have access to force/torque sensors in their gripper and tactile sensing is often necessary in tasks that involve contact-rich motion. In this work, we leverage surprise from mismatches in touch feedback to guide exploration in hard sparse-reward reinforcement learning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible objects interactions are supposed to ""feel"" like. We encourage exploration by rewarding interactions where the expectation and the experience don't match. In our proposed method, an initial task-independent exploration phase is followed by an on-task learning phase, in which the original interactions are relabeled with on-task rewards. We test our approach on a range of touch-intensive robot arm tasks (e.g. pushing objects, opening doors), which we also release as part of this work. Across multiple experiments in a simulated setting, we demonstrate that our method is able to learn these difficult tasks through sparse reward and curiosity alone. We compare our cross-modal approach to single-modality (touch- or vision-only) approaches as well as other curiosity-based methods and find that our method performs better and is more sample-efficient.",0
"In various real-world scenarios, robots utilize force/torque sensors in their gripper and require tactile sensing for tasks that entail extensive contact. In this study, we utilize the element of surprise stemming from inconsistencies in touch feedback to facilitate exploration in challenging reinforcement learning tasks with sparse rewards. Our method, Touch-based Curiosity (ToC), learns the expected ""feel"" of visible object interactions and encourages exploration by rewarding interactions that do not meet expectations. Our proposed approach involves an initial task-independent exploration phase, followed by an on-task learning phase where original interactions are relabeled with on-task rewards. We evaluate our approach on touch-intensive robot arm tasks such as object pushing and door opening, which we also present as part of this study. Through multiple experiments in a simulated setting, we demonstrate the ability of our method to learn these difficult tasks solely through sparse reward and curiosity. We compare our cross-modal approach to single-modality (touch- or vision-only) approaches and other curiosity-based methods, and find that our method performs better and is more efficient in terms of sample size.",1
"A large part of the interest in model-based reinforcement learning derives from the potential utility to acquire a forward model capable of strategic long term decision making. Assuming that an agent succeeds in learning a useful predictive model, it still requires a mechanism to harness it to generate and select among competing simulated plans. In this paper, we explore this theme combining evolutionary algorithmic planning techniques with models learned via deep learning and variational inference. We demonstrate the approach with an agent that reliably performs online planning in a set of visual navigation tasks.",0
"The appeal of model-based reinforcement learning primarily lies in its ability to obtain a forward model that can make strategic decisions for the long term. If an agent can successfully learn a valuable predictive model, it still needs a means to utilize it to create and choose from various simulated plans. This article investigates this concept by merging evolutionary algorithmic planning methods with models acquired through deep learning and variational inference. We exhibit the approach by showcasing an agent that effectively performs online planning in a collection of visual navigation challenges.",1
"We study the problem of learning control policies for complex tasks given by logical specifications. Recent approaches automatically generate a reward function from a given specification and use a suitable reinforcement learning algorithm to learn a policy that maximizes the expected reward. These approaches, however, scale poorly to complex tasks that require high-level planning. In this work, we develop a compositional learning approach, called DiRL, that interleaves high-level planning and reinforcement learning. First, DiRL encodes the specification as an abstract graph; intuitively, vertices and edges of the graph correspond to regions of the state space and simpler sub-tasks, respectively. Our approach then incorporates reinforcement learning to learn neural network policies for each edge (sub-task) within a Dijkstra-style planning algorithm to compute a high-level plan in the graph. An evaluation of the proposed approach on a set of challenging control benchmarks with continuous state and action spaces demonstrates that it outperforms state-of-the-art baselines.",0
"This article investigates the challenge of acquiring control policies for intricate tasks specified logically. Recent methods generate a reward function from the specification and employ reinforcement learning algorithms to maximize the expected reward. However, these methods do not scale well to complicated tasks that necessitate high-level planning. Consequently, the authors propose a new approach called DiRL that combines high-level planning with reinforcement learning. DiRL encodes the specification as an abstract graph, with vertices and edges representing state space regions and sub-tasks, respectively. The approach uses reinforcement learning to learn neural network policies for each edge or sub-task within a Dijkstra-style planning algorithm. The results of the evaluation on a set of challenging control benchmarks with continuous state and action spaces demonstrate that the proposed approach outperforms state-of-the-art baselines.",1
"Stochastic nested optimization, including stochastic compositional, min-max and bilevel optimization, is gaining popularity in many machine learning applications. While the three problems share the nested structure, existing works often treat them separately, and thus develop problem-specific algorithms and their analyses. Among various exciting developments, simple SGD-type updates (potentially on multiple variables) are still prevalent in solving this class of nested problems, but they are believed to have slower convergence rate compared to that of the non-nested problems. This paper unifies several SGD-type updates for stochastic nested problems into a single SGD approach that we term ALternating Stochastic gradient dEscenT (ALSET) method. By leveraging the hidden smoothness of the problem, this paper presents a tighter analysis of ALSET for stochastic nested problems. Under the new analysis, to achieve an $\epsilon$-stationary point of the nested problem, it requires ${\cal O}(\epsilon^{-2})$ samples. Under certain regularity conditions, applying our results to stochastic compositional, min-max and reinforcement learning problems either improves or matches the best-known sample complexity in the respective cases. Our results explain why simple SGD-type algorithms in stochastic nested problems all work very well in practice without the need for further modifications.",0
"Stochastic nested optimization is becoming increasingly popular in numerous machine learning applications, encompassing stochastic compositional, min-max, and bilevel optimization. Despite sharing a nested structure, these problems are often treated separately in existing works, resulting in problem-specific algorithms and analyses. Although simple SGD-type updates are still widely used to solve this class of nested problems, they are believed to have a slower convergence rate than that of non-nested problems. This paper introduces the ALternating Stochastic gradient dEscenT (ALSET) method, which unifies several SGD-type updates for stochastic nested problems into a single approach. By leveraging the problem's hidden smoothness, this paper presents a tighter analysis of ALSET for stochastic nested problems. Our results show that achieving an $\epsilon$-stationary point of the nested problem requires ${\cal O}(\epsilon^{-2})$ samples. Additionally, under specific regularity conditions, applying our results to stochastic compositional, min-max, and reinforcement learning problems either improves or matches the best-known sample complexity in the respective cases. Our findings provide an explanation for why simple SGD-type algorithms in stochastic nested problems perform well in practice without further modifications.",1
"This technical report presents panda-gym, a set Reinforcement Learning (RL) environments for the Franka Emika Panda robot integrated with OpenAI Gym. Five tasks are included: reach, push, slide, pick & place and stack. They all follow a Multi-Goal RL framework, allowing to use goal-oriented RL algorithms. To foster open-research, we chose to use the open-source physics engine PyBullet. The implementation chosen for this package allows to define very easily new tasks or new robots. This report also presents a baseline of results obtained with state-of-the-art model-free off-policy algorithms. panda-gym is open-source at https://github.com/qgallouedec/panda-gym.",0
"The report introduces panda-gym, which is a collection of Reinforcement Learning (RL) environments designed for the Franka Emika Panda robot and integrated with OpenAI Gym. It includes five tasks: reach, push, slide, pick & place and stack, all utilizing a Multi-Goal RL framework to facilitate the use of goal-oriented RL algorithms. To encourage open-research, PyBullet, an open-source physics engine, was utilized. The implementation of panda-gym enables easy task or robot customization. Additionally, the report presents a baseline of outcomes achieved with state-of-the-art model-free off-policy algorithms. The panda-gym package is available open-source on https://github.com/qgallouedec/panda-gym.",1
"We present \emph{MDP Playground}, an efficient testbed for Reinforcement Learning (RL) agents with \textit{orthogonal} dimensions that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in generated environments. We consider and allow control over a wide variety of dimensions, including \textit{delayed rewards}, \textit{rewardable sequences}, \textit{density of rewards}, \textit{stochasticity}, \textit{image representations}, \textit{irrelevant features}, \textit{time unit}, \textit{action range} and more. We define a parameterised collection of fast-to-run toy environments in \textit{OpenAI Gym} by varying these dimensions and propose to use these for the initial design and development of agents. We also provide wrappers that inject these dimensions into complex environments from \textit{Atari} and \textit{Mujoco} to allow for evaluating agent robustness. We further provide various example use-cases and instructions on how to use \textit{MDP Playground} to design and debug agents. We believe that \textit{MDP Playground} is a valuable testbed for researchers designing new, adaptive and intelligent RL agents and those wanting to unit test their agents.",0
"Introducing our latest project, \emph{MDP Playground}, a highly efficient testing platform for Reinforcement Learning (RL) agents. Our innovative approach features \textit{orthogonal} dimensions that can be easily controlled independently, providing a range of challenges for agents that are unique and customizable. By adjusting dimensions like \textit{delayed rewards}, \textit{rewardable sequences}, \textit{density of rewards}, \textit{stochasticity}, \textit{image representations}, \textit{irrelevant features}, \textit{time unit}, \textit{action range} and more, we can generate varying degrees of difficulty in the environments. Our novel parameterized collection of fast-to-run toy environments in \textit{OpenAI Gym} allows for the initial design and development of agents, while our wrappers inject these dimensions into complex environments from \textit{Atari} and \textit{Mujoco}, enabling us to evaluate agent robustness. We also provide multiple examples and detailed instructions on how researchers can use \textit{MDP Playground} to design and debug agents. Our platform is the perfect tool for those looking to test and improve their RL agents, and we believe it will be a valuable resource for researchers in the field.",1
"Recent years have seen stagnating improvements to branch predictor (BP) efficacy and a dearth of fresh ideas in branch predictor design, calling for fresh thinking in this area. This paper argues that looking at BP from the viewpoint of Reinforcement Learning (RL) facilitates systematic reasoning about, and exploration of, BP designs. We describe how to apply the RL formulation to branch predictors, show that existing predictors can be succinctly expressed in this formulation, and study two RL-based variants of conventional BPs.",0
"In recent times, there has been a lack of progress in improving the effectiveness of branch predictor (BP) and a shortage of innovative concepts in BP design. Therefore, there is a need for new approaches to tackle this issue. This study suggests that examining BP through the lens of Reinforcement Learning (RL) enables systematic evaluation and investigation of BP designs. It explains the application of the RL framework to BP, demonstrates that current predictors can be expressed using this framework, and examines two RL-based versions of traditional BPs.",1
"Breakthroughs in machine learning in the last decade have led to `digital intelligence', i.e. machine learning models capable of learning from vast amounts of labeled data to perform several digital tasks such as speech recognition, face recognition, machine translation and so on. The goal of this thesis is to make progress towards designing algorithms capable of `physical intelligence', i.e. building intelligent autonomous navigation agents capable of learning to perform complex navigation tasks in the physical world involving visual perception, natural language understanding, reasoning, planning, and sequential decision making. Despite several advances in classical navigation methods in the last few decades, current navigation agents struggle at long-term semantic navigation tasks. In the first part of the thesis, we discuss our work on short-term navigation using end-to-end reinforcement learning to tackle challenges such as obstacle avoidance, semantic perception, language grounding, and reasoning. In the second part, we present a new class of navigation methods based on modular learning and structured explicit map representations, which leverage the strengths of both classical and end-to-end learning methods, to tackle long-term navigation tasks. We show that these methods are able to effectively tackle challenges such as localization, mapping, long-term planning, exploration and learning semantic priors. These modular learning methods are capable of long-term spatial and semantic understanding and achieve state-of-the-art results on various navigation tasks.",0
"The past decade has witnessed significant strides in machine learning, resulting in the development of ""digital intelligence"" - machine learning models that can learn from large volumes of labeled data to perform various digital tasks, such as speech recognition, face recognition, and machine translation. This thesis aims to advance the field of ""physical intelligence"" by creating algorithms that can design intelligent autonomous navigation agents capable of performing complex navigation tasks in the physical world. These navigation tasks involve visual perception, natural language understanding, reasoning, planning, and sequential decision making. Despite various improvements in classical navigation methods over the past few decades, current navigation agents struggle with long-term semantic navigation tasks. The thesis is divided into two parts. The first part discusses the application of end-to-end reinforcement learning to short-term navigation, addressing challenges such as obstacle avoidance, semantic perception, language grounding, and reasoning. The second part introduces a new class of navigation methods that leverage modular learning and structured explicit map representations to handle long-term navigation tasks. These methods combine the strengths of both classical and end-to-end learning methods and can effectively address challenges such as localization, mapping, long-term planning, exploration, and learning semantic priors. The modular learning methods demonstrate exceptional long-term spatial and semantic understanding, achieving state-of-the-art results on various navigation tasks.",1
"We posit a new mechanism for cooperation in multi-agent reinforcement learning (MARL) based upon any nonlinear function of the team's long-term state-action occupancy measure, i.e., a \emph{general utility}. This subsumes the cumulative return but also allows one to incorporate risk-sensitivity, exploration, and priors. % We derive the {\bf D}ecentralized {\bf S}hadow Reward {\bf A}ctor-{\bf C}ritic (DSAC) in which agents alternate between policy evaluation (critic), weighted averaging with neighbors (information mixing), and local gradient updates for their policy parameters (actor). DSAC augments the classic critic step by requiring agents to (i) estimate their local occupancy measure in order to (ii) estimate the derivative of the local utility with respect to their occupancy measure, i.e., the ""shadow reward"". DSAC converges to $\epsilon$-stationarity in $\mathcal{O}(1/\epsilon^{2.5})$ (Theorem \ref{theorem:final}) or faster $\mathcal{O}(1/\epsilon^{2})$ (Corollary \ref{corollary:communication}) steps with high probability, depending on the amount of communications. We further establish the non-existence of spurious stationary points for this problem, that is, DSAC finds the globally optimal policy (Corollary \ref{corollary:global}). Experiments demonstrate the merits of goals beyond the cumulative return in cooperative MARL.",0
"A new mechanism for cooperation in multi-agent reinforcement learning (MARL) is proposed, which is based on a general utility, a nonlinear function of the team's long-term state-action occupancy measure. This approach incorporates risk-sensitivity, exploration, and priors, in addition to the cumulative return. The Decentralized Shadow Reward Actor-Critic (DSAC) is derived, in which agents alternate between policy evaluation (critic), weighted averaging with neighbors (information mixing), and local gradient updates for their policy parameters (actor). DSAC requires agents to estimate their local occupancy measure and estimate the derivative of the local utility with respect to their occupancy measure, i.e., the ""shadow reward"". DSAC converges to $\epsilon$-stationarity in $\mathcal{O}(1/\epsilon^{2.5})$ or faster $\mathcal{O}(1/\epsilon^{2})$ steps with high probability, depending on the amount of communications. The non-existence of spurious stationary points for this problem is also established, meaning that DSAC finds the globally optimal policy. Experiments demonstrate the benefits of incorporating goals beyond the cumulative return in cooperative MARL.",1
"We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",0
"A new framework is introduced that presents Reinforcement Learning (RL) as a sequence modeling issue. This approach utilizes the Transformer architecture's simplicity and scalability, along with language modeling advancements like GPT-x and BERT. The framework's key component is the Decision Transformer, which uses a causally masked Transformer to output optimal actions instead of computing policy gradients or fitting value functions. The Decision Transformer model conditions an autoregressive model on the desired reward, past states, and actions, generating future actions that achieve the desired return. Despite its simplicity, the Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",1
"Model-based reinforcement learning (MBRL) is believed to have higher sample efficiency compared with model-free reinforcement learning (MFRL). However, MBRL is plagued by dynamics bottleneck dilemma. Dynamics bottleneck dilemma is the phenomenon that the performance of the algorithm falls into the local optimum instead of increasing when the interaction step with the environment increases, which means more data can not bring better performance. In this paper, we find that the trajectory reward estimation error is the main reason that causes dynamics bottleneck dilemma through theoretical analysis. We give an upper bound of the trajectory reward estimation error and point out that increasing the agent's exploration ability is the key to reduce trajectory reward estimation error, thereby alleviating dynamics bottleneck dilemma. Motivated by this, a model-based control method combined with exploration named MOdel-based Progressive Entropy-based Exploration (MOPE2) is proposed. We conduct experiments on several complex continuous control benchmark tasks. The results verify that MOPE2 can effectively alleviate dynamics bottleneck dilemma and have higher sample efficiency than previous MBRL and MFRL algorithms.",0
"Compared to model-free reinforcement learning (MFRL), model-based reinforcement learning (MBRL) is believed to be more sample efficient. However, MBRL faces a dilemma called the dynamics bottleneck, where the algorithm's performance falls into a local optimum instead of improving as more data is collected. Through theoretical analysis, we identified the trajectory reward estimation error as the primary cause of the dynamics bottleneck. To reduce this error and alleviate the bottleneck, we propose MOdel-based Progressive Entropy-based Exploration (MOPE2), a model-based control method that combines exploration. By increasing the agent's exploration ability, MOPE2 effectively reduces trajectory reward estimation error and outperforms previous MBRL and MFRL algorithms in several complex continuous control benchmark tasks.",1
"Model-agnostic meta-reinforcement learning requires estimating the Hessian matrix of value functions. This is challenging from an implementation perspective, as repeatedly differentiating policy gradient estimates may lead to biased Hessian estimates. In this work, we provide a unifying framework for estimating higher-order derivatives of value functions, based on off-policy evaluation. Our framework interprets a number of prior approaches as special cases and elucidates the bias and variance trade-off of Hessian estimates. This framework also opens the door to a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice.",0
"Estimating the Hessian matrix of value functions is a difficult task in model-agnostic meta-reinforcement learning due to the potential for biased Hessian estimates resulting from repeatedly differentiating policy gradient estimates. Our research presents a comprehensive framework for estimating higher-order derivatives of value functions by utilizing off-policy evaluation. This framework offers a clear understanding of the bias and variance trade-off of Hessian estimates and highlights how several previous methods fit into the framework. Additionally, this framework introduces a new set of estimates that can be easily implemented with auto-differentiation libraries and may lead to performance improvements in real-world applications.",1
"This work studies the statistical limits of uniform convergence for offline policy evaluation (OPE) problems with model-based methods (for episodic MDP) and provides a unified framework towards optimal learning for several well-motivated offline tasks. Uniform OPE $\sup_\Pi|Q^\pi-\hat{Q}^\pi|<\epsilon$ is a stronger measure than the point-wise OPE and ensures offline learning when $\Pi$ contains all policies (the global class). In this paper, we establish an $\Omega(H^2 S/d_m\epsilon^2)$ lower bound (over model-based family) for the global uniform OPE and our main result establishes an upper bound of $\tilde{O}(H^2/d_m\epsilon^2)$ for the \emph{local} uniform convergence that applies to all \emph{near-empirically optimal} policies for the MDPs with \emph{stationary} transition. Here $d_m$ is the minimal marginal state-action probability. Critically, the highlight in achieving the optimal rate $\tilde{O}(H^2/d_m\epsilon^2)$ is our design of \emph{singleton absorbing MDP}, which is a new sharp analysis tool that works with the model-based approach. We generalize such a model-based framework to the new settings: offline task-agnostic and the offline reward-free with optimal complexity $\tilde{O}(H^2\log(K)/d_m\epsilon^2)$ ($K$ is the number of tasks) and $\tilde{O}(H^2S/d_m\epsilon^2)$ respectively. These results provide a unified solution for simultaneously solving different offline RL problems.",0
"The objective of this study is to examine the limitations of uniform convergence in offline policy evaluation (OPE) problems using model-based methods for episodic MDP. The study proposes a unified framework that facilitates optimal learning for several offline tasks by introducing the concept of global uniform OPE, which is a more robust measure than point-wise OPE. The paper establishes a lower bound of $\Omega(H^2 S/d_m\epsilon^2)$ and an upper bound of $\tilde{O}(H^2/d_m\epsilon^2)$ for global and local uniform convergence, respectively, for MDPs with stationary transitions. The study utilizes a new analytical tool called the singleton absorbing MDP, which is a model-based approach. The framework is generalized to address offline task-agnostic and reward-free problems, with optimal complexities of $\tilde{O}(H^2\log(K)/d_m\epsilon^2)$ and $\tilde{O}(H^2S/d_m\epsilon^2)$, respectively. These findings provide a comprehensive solution for multiple offline RL problems.",1
"We study constrained reinforcement learning (CRL) from a novel perspective by setting constraints directly on state density functions, rather than the value functions considered by previous works. State density has a clear physical and mathematical interpretation, and is able to express a wide variety of constraints such as resource limits and safety requirements. Density constraints can also avoid the time-consuming process of designing and tuning cost functions required by value function-based constraints to encode system specifications. We leverage the duality between density functions and Q functions to develop an effective algorithm to solve the density constrained RL problem optimally and the constrains are guaranteed to be satisfied. We prove that the proposed algorithm converges to a near-optimal solution with a bounded error even when the policy update is imperfect. We use a set of comprehensive experiments to demonstrate the advantages of our approach over state-of-the-art CRL methods, with a wide range of density constrained tasks as well as standard CRL benchmarks such as Safety-Gym.",0
"Our approach to studying constrained reinforcement learning (CRL) differs from previous works by directly setting constraints on state density functions instead of value functions. State density functions have a clear physical and mathematical interpretation and can express a range of constraints such as resource limits and safety requirements. By using density constraints, the process of designing and tuning cost functions required by value function-based constraints can be avoided. We utilize the duality between density functions and Q functions to develop an effective algorithm that optimally solves the density constrained RL problem while ensuring that the constraints are met. Our proposed algorithm guarantees convergence to a near-optimal solution with a bounded error even with imperfect policy updates. We conducted a comprehensive set of experiments to demonstrate the advantages of our approach over standard CRL benchmarks like Safety-Gym, with a variety of density constrained tasks.",1
"We explore reinforcement learning methods for finding the optimal policy in the linear quadratic regulator (LQR) problem. In particular, we consider the convergence of policy gradient methods in the setting of known and unknown parameters. We are able to produce a global linear convergence guarantee for this approach in the setting of finite time horizon and stochastic state dynamics under weak assumptions. The convergence of a projected policy gradient method is also established in order to handle problems with constraints. We illustrate the performance of the algorithm with two examples. The first example is the optimal liquidation of a holding in an asset. We show results for the case where we assume a model for the underlying dynamics and where we apply the method to the data directly. The empirical evidence suggests that the policy gradient method can learn the global optimal solution for a larger class of stochastic systems containing the LQR framework and that it is more robust with respect to model mis-specification when compared to a model-based approach. The second example is an LQR system in a higher dimensional setting with synthetic data.",0
"Our focus is on utilizing reinforcement learning techniques to determine the best policy for the linear quadratic regulator (LQR) problem. We specifically investigate the effectiveness of policy gradient methods in relation to known and unknown parameters. We are able to establish a linear convergence guarantee for this approach, globally, given finite time horizon and stochastic state dynamics, with minimal assumptions. Additionally, we demonstrate the convergence of a projected policy gradient method that can handle systems with constraints. Using two examples, we illustrate the algorithm's performance. Firstly, we apply the method to the optimal liquidation of an asset, both with a model-based approach and directly to the data. Our findings suggest that the policy gradient method can successfully learn the global optimal solution for a broader range of stochastic systems than those within the LQR framework, and is more resilient to model mis-specification than a model-based approach. Secondly, we examine a higher-dimensional LQR system with synthetic data.",1
"Safe learning and optimization deals with learning and optimization problems that avoid, as much as possible, the evaluation of non-safe input points, which are solutions, policies, or strategies that cause an irrecoverable loss (e.g., breakage of a machine or equipment, or life threat). Although a comprehensive survey of safe reinforcement learning algorithms was published in 2015, a number of new algorithms have been proposed thereafter, and related works in active learning and in optimization were not considered. This paper reviews those algorithms from a number of domains including reinforcement learning, Gaussian process regression and classification, evolutionary algorithms, and active learning. We provide the fundamental concepts on which the reviewed algorithms are based and a characterization of the individual algorithms. We conclude by explaining how the algorithms are connected and suggestions for future research.",0
"The concept of safe learning and optimization involves solving learning and optimization issues while minimizing the evaluation of non-safe input points. These input points refer to solutions, policies, or strategies that can lead to irreversible damage or danger. Although a comprehensive survey of safe reinforcement learning algorithms was previously published in 2015, subsequent research in active learning and optimization were not included. In this paper, we review new algorithms from various domains, such as reinforcement learning, Gaussian process regression and classification, evolutionary algorithms, and active learning. We outline the underlying principles of each algorithm and provide a characterization of their individual features. Lastly, we discuss how these algorithms are interconnected and offer suggestions for future research.",1
"Despite the empirical success of neural architecture search (NAS) in deep learning applications, the optimality, reproducibility and cost of NAS schemes remain hard to assess. In this paper, we propose Generative Adversarial NAS (GA-NAS) with theoretically provable convergence guarantees, promoting stability and reproducibility in neural architecture search. Inspired by importance sampling, GA-NAS iteratively fits a generator to previously discovered top architectures, thus increasingly focusing on important parts of a large search space. Furthermore, we propose an efficient adversarial learning approach, where the generator is trained by reinforcement learning based on rewards provided by a discriminator, thus being able to explore the search space without evaluating a large number of architectures. Extensive experiments show that GA-NAS beats the best published results under several cases on three public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search constraints and search spaces. We show that GA-NAS can be used to improve already optimized baselines found by other NAS methods, including EfficientNet and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in their original search space.",0
"Although neural architecture search (NAS) has proven successful in deep learning applications, assessing the optimality, reproducibility, and cost of these schemes remains challenging. Our paper introduces Generative Adversarial NAS (GA-NAS), which guarantees theoretical convergence and promotes stability and reproducibility in neural architecture search. GA-NAS employs importance sampling and fits a generator to previously discovered top architectures, allowing it to focus on important parts of a large search space. Additionally, our efficient adversarial learning approach trains the generator using rewards provided by a discriminator, enabling exploration of the search space without evaluating numerous architectures. Our experiments show that GA-NAS outperforms the best published results in three public NAS benchmarks and can handle ad-hoc search constraints and spaces. Furthermore, GA-NAS can improve already optimized baselines found by other NAS methods, including EfficientNet and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters in their original search space.",1
"In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment's dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, surpassing existing methods both in the number of required environment interactions and scalability in high-dimensional spaces.",0
"When faced with sequential decision-making problems, it is common to have access to expert or human data that can provide valuable insights into the task. However, in complex and high-dimensional environments, imitation learning (IL) can be challenging and traditional methods like behavioral cloning fail to take into account the dynamics of the environment. Other methods that incorporate dynamics information can be difficult to train due to adversarial optimization processes or biased gradient estimators. To address these issues, we propose a new approach called Inverse soft-Q learning (IQ-Learn) that learns a single Q-function to implicitly represent both reward and policy. This method avoids adversarial training and has shown superior performance in offline and online imitation learning settings, outperforming existing methods in terms of required environment interactions and scalability in high-dimensional spaces. Furthermore, the implicitly learned rewards are highly correlated with the ground-truth rewards, making our method suitable for inverse reinforcement learning (IRL).",1
"In this paper, we design a novel Bregman gradient policy optimization framework for reinforcement learning based on Bregman divergences and momentum techniques. Specifically, we propose a Bregman gradient policy optimization (BGPO) algorithm based on the basic momentum technique and mirror descent iteration. At the same time, we present an accelerated Bregman gradient policy optimization (VR-BGPO) algorithm based on a momentum variance-reduced technique. Moreover, we introduce a convergence analysis framework for our Bregman gradient policy optimization under the nonconvex setting. Specifically, we prove that BGPO achieves the sample complexity of $\tilde{O}(\epsilon^{-4})$ for finding $\epsilon$-stationary point only requiring one trajectory at each iteration, and VR-BGPO reaches the best known sample complexity of $\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point, which also only requires one trajectory at each iteration. In particular, by using different Bregman divergences, our methods unify many existing policy optimization algorithms and their new variants such as the existing (variance-reduced) policy gradient algorithms and (variance-reduced) natural policy gradient algorithms. Extensive experimental results on multiple reinforcement learning tasks demonstrate the efficiency of our new algorithms.",0
"Our paper presents a novel framework for reinforcement learning called Bregman gradient policy optimization, which utilizes Bregman divergences and momentum techniques. Our proposed algorithm, Bregman gradient policy optimization (BGPO), utilizes a basic momentum technique and mirror descent iteration. We also introduce an accelerated algorithm called VR-BGPO, which is based on a momentum variance-reduced technique. Additionally, we provide a convergence analysis framework for our nonconvex Bregman gradient policy optimization. Our analysis shows that BGPO achieves a sample complexity of $\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary point with only one trajectory per iteration, while VR-BGPO achieves the best known sample complexity of $\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point with the same constraints. By utilizing different Bregman divergences, our methods unify various existing policy optimization algorithms, including variance-reduced policy gradient algorithms and natural policy gradient algorithms. We demonstrate the efficacy of our algorithms through extensive experimentation on multiple reinforcement learning tasks.",1
"Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can learn to use social learning to improve their performance. We find that in most circumstances, vanilla model-free RL agents do not use social learning. We analyze the reasons for this deficiency, and show that by imposing constraints on the training environment and introducing a model-based auxiliary loss we are able to obtain generalized social learning policies which enable agents to: i) discover complex skills that are not learned from single-agent training, and ii) adapt online to novel environments by taking cues from experts present in the new environment. In contrast, agents trained with model-free RL or imitation learning generalize poorly and do not succeed in the transfer tasks. By mixing multi-agent and solo training, we can obtain agents that use social learning to gain skills that they can deploy when alone, even out-performing agents trained alone from the start.",0
"The acquisition of sophisticated behavior and adaptation to new circumstances can be achieved through social learning, which is a crucial aspect of both human and animal intelligence. This research aims to explore the ability of independent reinforcement learning agents in a multi-agent environment to utilize social learning to enhance their performance. The study reveals that vanilla model-free RL agents typically do not utilize social learning, and the reasons for this are analyzed. However, by implementing constraints on the training environment and introducing a model-based auxiliary loss, the agents can attain generalized social learning policies that enable them to acquire complex skills not learned through single-agent training and adapt to new environments by observing experts' behavior. In contrast, agents trained with model-free RL or imitation learning do not perform as well in transfer tasks. By combining multi-agent and solo training, agents can leverage social learning to acquire skills that they can use independently, surpassing agents trained alone from the outset.",1
"We study the off-policy evaluation (OPE) problem in reinforcement learning with linear function approximation, which aims to estimate the value function of a target policy based on the offline data collected by a behavior policy. We propose to incorporate the variance information of the value function to improve the sample efficiency of OPE. More specifically, for time-inhomogeneous episodic linear Markov decision processes (MDPs), we propose an algorithm, VA-OPE, which uses the estimated variance of the value function to reweight the Bellman residual in Fitted Q-Iteration. We show that our algorithm achieves a tighter error bound than the best-known result. We also provide a fine-grained characterization of the distribution shift between the behavior policy and the target policy. Extensive numerical experiments corroborate our theory.",0
"Our focus is on solving the off-policy evaluation (OPE) problem in reinforcement learning using linear function approximation. This problem involves estimating the value function of a target policy based on data collected by a behavior policy. To enhance the sample efficiency of OPE, we suggest incorporating the variance information of the value function. Specifically, for time-inhomogeneous episodic linear Markov decision processes (MDPs), we present VA-OPE, an algorithm that utilizes the estimated variance of the value function to reweight the Bellman residual in Fitted Q-Iteration. Our algorithm achieves a tighter error bound than the best-known result, and we offer a detailed explanation of the distribution shift between the behavior policy and the target policy. Our theory is supported by extensive numerical experiments.",1
"The success of deep reinforcement learning (DRL) is due to the power of learning a representation that is suitable for the underlying exploration and exploitation task. However, existing provable reinforcement learning algorithms with linear function approximation often assume the feature representation is known and fixed. In order to understand how representation learning can improve the efficiency of RL, we study representation learning for a class of low-rank Markov Decision Processes (MDPs) where the transition kernel can be represented in a bilinear form. We propose a provably efficient algorithm called ReLEX that can simultaneously learn the representation and perform exploration. We show that ReLEX always performs no worse than a state-of-the-art algorithm without representation learning, and will be strictly better in terms of sample efficiency if the function class of representations enjoys a certain mild ""coverage'' property over the whole state-action space.",0
"The success of deep reinforcement learning (DRL) is attributed to the ability to acquire a representation that is appropriate for exploration and exploitation. However, present provable reinforcement learning algorithms that utilize linear function approximation often expect the feature representation to be predetermined and unchanging. To comprehend the potential benefits of representation learning in RL, we investigate representation learning for a type of low-rank Markov Decision Processes (MDPs) in which the transition kernel can be expressed in a bilinear form. We introduce ReLEX, a verifiably efficient algorithm that can learn the representation while also conducting exploration. We prove that ReLEX performs no worse than a cutting-edge algorithm that doesn't utilize representation learning while being superior in terms of sample efficiency if the function class of representations encompasses a certain mild ""coverage'' property throughout the state-action space.",1
"Reinforcement learning (RL) aims to find an optimal policy by interaction with an environment. Consequently, learning complex behavior requires a vast number of samples, which can be prohibitive in practice. Nevertheless, instead of systematically reasoning and actively choosing informative samples, policy gradients for local search are often obtained from random perturbations. These random samples yield high variance estimates and hence are sub-optimal in terms of sample complexity. Actively selecting informative samples is at the core of Bayesian optimization, which constructs a probabilistic surrogate of the objective from past samples to reason about informative subsequent ones. In this paper, we propose to join both worlds. We develop an algorithm utilizing a probabilistic model of the objective function and its gradient. Based on the model, the algorithm decides where to query a noisy zeroth-order oracle to improve the gradient estimates. The resulting algorithm is a novel type of policy search method, which we compare to existing black-box algorithms. The comparison reveals improved sample complexity and reduced variance in extensive empirical evaluations on synthetic objectives. Further, we highlight the benefits of active sampling on popular RL benchmarks.",0
"The goal of reinforcement learning is to discover an optimal policy through interaction with the environment. However, obtaining a large number of samples is often impractical to learn complex behaviors. Instead of actively selecting informative samples, policy gradients for local search are typically derived from random perturbations. However, this results in high variance estimates and is sub-optimal in terms of sample complexity. Bayesian optimization, on the other hand, chooses informative samples by constructing a probabilistic surrogate of the objective from past samples. In this paper, we propose a hybrid approach that combines Bayesian optimization with a probabilistic model of the objective function and its gradient. This approach determines where to query a noisy zeroth-order oracle to improve the gradient estimates and is a novel type of policy search method. We compare our algorithm to existing black-box methods and demonstrate improved sample complexity and reduced variance in empirical evaluations on synthetic objectives. Additionally, we show the benefits of active sampling on popular reinforcement learning benchmarks.",1
"We study deep reinforcement learning (RL) algorithms with delayed rewards. In many real-world tasks, instant rewards are often not readily accessible or even defined immediately after the agent performs actions. In this work, we first formally define the environment with delayed rewards and discuss the challenges raised due to the non-Markovian nature of such environments. Then, we introduce a general off-policy RL framework with a new Q-function formulation that can handle the delayed rewards with theoretical convergence guarantees. For practical tasks with high dimensional state spaces, we further introduce the HC-decomposition rule of the Q-function in our framework which naturally leads to an approximation scheme that helps boost the training efficiency and stability. We finally conduct extensive experiments to demonstrate the superior performance of our algorithms over the existing work and their variants.",0
"Our research focuses on deep reinforcement learning (RL) algorithms that deal with delayed rewards. In numerous real-world scenarios, immediate rewards are either not easily accessible or not immediately defined after the agent's actions. Our study formally defines the environment with delayed rewards and discusses the challenges that arise due to its non-Markovian nature. Additionally, we present a general off-policy RL framework that includes a new Q-function formulation that can manage the delayed rewards with theoretical convergence guarantees. For practical tasks that involve high-dimensional state spaces, we introduce the HC-decomposition rule of the Q-function in our framework, which leads to an approximation scheme that improves the training efficiency and stability. We also conduct extensive experiments that demonstrate the superior performance of our algorithms over the existing work and their variants.",1
"Model checking has been developed for verifying the behaviour of systems with stochastic and non-deterministic behavior. It is used to provide guarantees about such systems. While most model checking methods focus on propositional models, various probabilistic planning and reinforcement frameworks deal with relational domains, for instance, STRIPS planning and relational Markov Decision Processes. Using propositional model checking in relational settings requires one to ground the model, which leads to the well known state explosion problem and intractability. We present pCTL-REBEL, a lifted model checking approach for verifying pCTL properties on relational MDPs. It extends REBEL, the relational Bellman update operator, which is a lifted value iteration approach for model-based relational reinforcement learning, toward relational model-checking. PCTL-REBEL is lifted, which means that rather than grounding, the model exploits symmetries and reasons at an abstract relational level. Theoretically, we show that the pCTL model checking approach is decidable for relational MDPs even for possibly infinite domains provided that the states have a bounded size. Practically, we contribute algorithms and an implementation of lifted relational model checking, and we show that the lifted approach improves the scalability of the model checking approach.",0
"The purpose of model checking is to verify the behavior of systems that exhibit stochastic and non-deterministic behavior, providing assurances about their performance. Although most model checking methods focus on propositional models, there are various probabilistic planning and reinforcement frameworks that deal with relational domains such as STRIPS planning and relational Markov Decision Processes. However, using propositional model checking in relational settings requires the model to be grounded, which is problematic due to the state explosion problem and intractability. We introduce pCTL-REBEL, a lifted model checking approach that verifies pCTL properties on relational MDPs. It extends REBEL, a relational Bellman update operator, which is a lifted value iteration approach for model-based relational reinforcement learning, to relational model-checking. PCTL-REBEL is lifted, which means that it exploits symmetries and reasons at an abstract relational level, avoiding the need for grounding. The pCTL model checking approach is theoretically decidable for relational MDPs, even for possibly infinite domains, provided that the states are of a bounded size. Furthermore, we present algorithms and an implementation of lifted relational model checking, demonstrating that the lifted approach improves the scalability of the model checking approach.",1
"We study bandits and reinforcement learning (RL) subject to a conservative constraint where the agent is asked to perform at least as well as a given baseline policy. This setting is particular relevant in real-world domains including digital marketing, healthcare, production, finance, etc. For multi-armed bandits, linear bandits and tabular RL, specialized algorithms and theoretical analyses were proposed in previous work. In this paper, we present a unified framework for conservative bandits and RL, in which our core technique is to calculate the necessary and sufficient budget obtained from running the baseline policy. For lower bounds, our framework gives a black-box reduction that turns a certain lower bound in the nonconservative setting into a new lower bound in the conservative setting. We strengthen the existing lower bound for conservative multi-armed bandits and obtain new lower bounds for conservative linear bandits, tabular RL and low-rank MDP. For upper bounds, our framework turns a certain nonconservative upper-confidence-bound (UCB) algorithm into a conservative algorithm with a simple analysis. For multi-armed bandits, linear bandits and tabular RL, our new upper bounds tighten or match existing ones with significantly simpler analyses. We also obtain a new upper bound for conservative low-rank MDP.",0
"The focus of our research is on conservative bandits and reinforcement learning, where the agent must perform at least as well as a given baseline policy. This is particularly relevant in various industries such as healthcare, finance, and digital marketing. In previous studies, specialized algorithms and theoretical analyses were developed for multi-armed bandits, linear bandits, and tabular RL. However, this paper presents a unified framework for conservative bandits and RL, which involves calculating the necessary and sufficient budget from running the baseline policy. Our framework also provides new lower bounds for conservative linear bandits, tabular RL, and low-rank MDP, and strengthens existing lower bounds for conservative multi-armed bandits. For upper bounds, we transform nonconservative upper-confidence-bound algorithms into conservative algorithms with a simple analysis. Our new upper bounds for multi-armed bandits, linear bandits, and tabular RL tighten or match existing ones with simpler analyses. Additionally, we obtain a new upper bound for conservative low-rank MDP.",1
"What is the difference between goal-directed and habitual behavior? We propose a novel computational framework of decision making with Bayesian inference, in which everything is integrated as an entire neural network model. The model learns to predict environmental state transitions by self-exploration and generating motor actions by sampling stochastic internal states ${z}$. Habitual behavior, which is obtained from the prior distribution of ${z}$, is acquired by reinforcement learning. Goal-directed behavior is determined from the posterior distribution of ${z}$ by planning, using active inference which optimizes the past, current and future ${z}$ by minimizing the variational free energy for the desired future observation constrained by the observed sensory sequence. We demonstrate the effectiveness of the proposed framework by experiments in a sensorimotor navigation task with camera observations and continuous motor actions.",0
"Our proposal centers on a computational framework that distinguishes between habitual and goal-directed behavior. This framework employs Bayesian inference, incorporating all components into a neural network model. Through self-exploration, the model learns to forecast environmental state transitions and extracts motor actions by sampling stochastic internal states. Habitual behavior arises from the prior distribution of internal states, acquired via reinforcement learning. Active inference is used to determine goal-directed behavior from the posterior distribution of internal states, optimizing past, present, and future internal states by minimizing the variational free energy. This is accomplished by constraining the desired future observation based on the observed sensory sequence. We present the effectiveness of our framework by conducting experiments in a sensorimotor navigation task, utilizing camera observations and continuous motor actions.",1
"We introduce OLIVAW, an AI Othello player adopting the design principles of the famous AlphaGo series. The main motivation behind OLIVAW was to attain exceptional competence in a non-trivial board game at a tiny fraction of the cost of its illustrious predecessors. In this paper, we show how the AlphaGo Zero's paradigm can be successfully applied to the popular game of Othello using only commodity hardware and free cloud services. While being simpler than Chess or Go, Othello maintains a considerable search space and difficulty in evaluating board positions. To achieve this result, OLIVAW implements some improvements inspired by recent works to accelerate the standard AlphaGo Zero learning process. The main modification implies doubling the positions collected per game during the training phase, by including also positions not played but largely explored by the agent. We tested the strength of OLIVAW in three different ways: by pitting it against Edax, the strongest open-source Othello engine, by playing anonymous games on the web platform OthelloQuest, and finally in two in-person matches against top-notch human players: a national champion and a former world champion.",0
"Our new AI Othello player, OLIVAW, has been designed based on the principles of the famous AlphaGo series. Our main goal was to achieve exceptional competence in a challenging board game while keeping costs much lower than its predecessors. In this paper, we demonstrate how we have adapted the AlphaGo Zero approach to Othello using widely available hardware and free cloud services. Although Othello is simpler than Chess or Go, it still presents a significant challenge in terms of evaluating board positions due to its considerable search space. To achieve our results, we have made some improvements inspired by recent works to speed up the standard AlphaGo Zero learning process. Our main modification involves doubling the positions collected per game during the training phase, including positions that the agent has explored but not played. We have tested the strength of OLIVAW in three ways: by pitting it against Edax, the strongest open-source Othello engine, by playing anonymous games on the web platform OthelloQuest, and finally, in two in-person matches against top human players, including a national champion and a former world champion.",1
"We study reinforcement learning (RL) with linear function approximation. Existing algorithms for this problem only have high-probability regret and/or Probably Approximately Correct (PAC) sample complexity guarantees, which cannot guarantee the convergence to the optimal policy. In this paper, in order to overcome the limitation of existing algorithms, we propose a new algorithm called FLUTE, which enjoys uniform-PAC convergence to the optimal policy with high probability. The uniform-PAC guarantee is the strongest possible guarantee for reinforcement learning in the literature, which can directly imply both PAC and high probability regret bounds, making our algorithm superior to all existing algorithms with linear function approximation. At the core of our algorithm is a novel minimax value function estimator and a multi-level partition scheme to select the training samples from historical observations. Both of these techniques are new and of independent interest.",0
"Our focus is on studying reinforcement learning (RL) with linear function approximation. However, the current algorithms for this problem have limitations, as they only guarantee high-probability regret and/or Probably Approximately Correct (PAC) sample complexity. These guarantees do not ensure the optimal policy's convergence. Our research aims to address this issue by introducing a new algorithm, called FLUTE. FLUTE provides a uniform-PAC convergence to the optimal policy with a high probability, which is the strongest guarantee available for RL in the literature. This guarantee implies both PAC and high probability regret bounds, making FLUTE superior to all existing algorithms with linear function approximation. Our algorithm employs a novel minimax value function estimator and a multi-level partition scheme to select training samples from historical observations. These techniques are innovative and of independent interest.",1
"Differential equations in general and neural ODEs in particular are an essential technique in continuous-time system identification. While many deterministic learning algorithms have been designed based on numerical integration via the adjoint method, many downstream tasks such as active learning, exploration in reinforcement learning, robust control, or filtering require accurate estimates of predictive uncertainties. In this work, we propose a novel approach towards estimating epistemically uncertain neural ODEs, avoiding the numerical integration bottleneck. Instead of modeling uncertainty in the ODE parameters, we directly model uncertainties in the state space. Our algorithm - distributional gradient matching (DGM) - jointly trains a smoother and a dynamics model and matches their gradients via minimizing a Wasserstein loss. Our experiments show that, compared to traditional approximate inference methods based on numerical integration, our approach is faster to train, faster at predicting previously unseen trajectories, and in the context of neural ODEs, significantly more accurate.",0
"Continuous-time system identification relies heavily on differential equations, particularly neural ODEs. Although numerical integration via the adjoint method has been used to design deterministic learning algorithms, accurate predictive uncertainties are essential for various downstream tasks such as robust control, filtering, active learning, and exploration in reinforcement learning. This paper presents a new approach to estimate epistemically uncertain neural ODEs by directly modeling uncertainties in the state space instead of the ODE parameters. Our method, known as distributional gradient matching (DGM), trains a smoother and a dynamics model, and minimizes a Wasserstein loss to match their gradients. Our experiments demonstrate that DGM is faster in training and predicting previously unseen trajectories, and more accurate than traditional approximate inference methods based on numerical integration, especially in the context of neural ODEs.",1
"We consider the problem of learning fair policies in (deep) cooperative multi-agent reinforcement learning (MARL). We formalize it in a principled way as the problem of optimizing a welfare function that explicitly encodes two important aspects of fairness: efficiency and equity. As a solution method, we propose a novel neural network architecture, which is composed of two sub-networks specifically designed for taking into account the two aspects of fairness. In experiments, we demonstrate the importance of the two sub-networks for fair optimization. Our overall approach is general as it can accommodate any (sub)differentiable welfare function. Therefore, it is compatible with various notions of fairness that have been proposed in the literature (e.g., lexicographic maximin, generalized Gini social welfare function, proportional fairness). Our solution method is generic and can be implemented in various MARL settings: centralized training and decentralized execution, or fully decentralized. Finally, we experimentally validate our approach in various domains and show that it can perform much better than previous methods.",0
"The focus of our research is on achieving fairness in cooperative multi-agent reinforcement learning (MARL), specifically concerning deep learning. We have formalized the problem into optimizing a welfare function that encompasses two crucial aspects of fairness: equity and efficiency. Our proposed solution method involves a neural network architecture consisting of two sub-networks tailored to address the two aspects of fairness. We have conducted experiments that demonstrate the effectiveness of these sub-networks in the pursuit of fair optimization. Our approach is flexible as it can accommodate any differentiable welfare function, making it compatible with various fairness concepts, such as proportional fairness and lexicographic maximin. Our solution method is generic and can be implemented in various MARL settings, including centralized training and decentralized execution. Our experimental results show significant improvement over previous methods in various domains.",1
"There have been many recent advances on provably efficient Reinforcement Learning (RL) in problems with rich observation spaces. However, all these works share a strong realizability assumption about the optimal value function of the true MDP. Such realizability assumptions are often too strong to hold in practice. In this work, we consider the more realistic setting of agnostic RL with rich observation spaces and a fixed class of policies $\Pi$ that may not contain any near-optimal policy. We provide an algorithm for this setting whose error is bounded in terms of the rank $d$ of the underlying MDP. Specifically, our algorithm enjoys a sample complexity bound of $\widetilde{O}\left((H^{4d} K^{3d} \log |\Pi|)/\epsilon^2\right)$ where $H$ is the length of episodes, $K$ is the number of actions and $\epsilon>0$ is the desired sub-optimality. We also provide a nearly matching lower bound for this agnostic setting that shows that the exponential dependence on rank is unavoidable, without further assumptions.",0
"Recent advancements in Reinforcement Learning (RL) have proven to be efficient in addressing problems with complex observation spaces. However, these advancements rely on a strong assumption regarding the optimal value function of the true Markov Decision Process (MDP) which is often unrealistic in practical applications. This study considers the more realistic scenario of agnostic RL with rich observation spaces and a fixed set of policies $\Pi$ that may not include a near-optimal policy. An algorithm is proposed for this setting which is bounded by the rank $d$ of the underlying MDP. The algorithm has a sample complexity bound of $\widetilde{O}\left((H^{4d} K^{3d} \log |\Pi|)/\epsilon^2\right)$, where $H$ represents the length of episodes, $K$ is the number of actions, and $\epsilon>0$ denotes the desired sub-optimality. Furthermore, a nearly matching lower bound is also provided for this agnostic setting, which demonstrates that the exponential dependence on rank is inevitable without further assumptions.",1
"Offline reinforcement learning (RL) tries to learn the near-optimal policy with recorded offline experience without online exploration. Current offline RL research includes: 1) generative modeling, i.e., approximating a policy using fixed data; and 2) learning the state-action value function. While most research focuses on the state-action function part through reducing the bootstrapping error in value function approximation induced by the distribution shift of training data, the effects of error propagation in generative modeling have been neglected. In this paper, we analyze the error in generative modeling. We propose AQL (action-conditioned Q-learning), a residual generative model to reduce policy approximation error for offline RL. We show that our method can learn more accurate policy approximations in different benchmark datasets. In addition, we show that the proposed offline RL method can learn more competitive AI agents in complex control tasks under the multiplayer online battle arena (MOBA) game Honor of Kings.",0
"The objective of offline reinforcement learning (RL) is to acquire a policy that is nearly optimal by utilizing recorded offline experience instead of online exploration. To achieve this, current offline RL research employs two approaches: generative modeling, which involves approximating a policy using fixed data, and learning the state-action value function. Although most research has concentrated on the state-action function aspect by minimizing the bootstrapping error in value function approximation that occurs due to the distribution shift of training data, the consequences of error propagation in generative modeling have been neglected. This study examines the error in generative modeling and introduces AQL (action-conditioned Q-learning), which is a residual generative model that minimizes policy approximation error for offline RL. The results show that our method can produce more accurate policy approximations in various benchmark datasets. Furthermore, we demonstrate that the proposed offline RL approach can create more competitive AI agents for complex control tasks in the multiplayer online battle arena (MOBA) game Honor of Kings.",1
"The study of provable adversarial robustness for deep neural network (DNN) models has mainly focused on static supervised learning tasks such as image classification. However, DNNs have been used extensively in real-world adaptive tasks such as reinforcement learning (RL), making RL systems vulnerable to adversarial attacks. The key challenge in adversarial RL is that the attacker can adapt itself to the defense strategy used by the agent in previous time-steps to strengthen its attack in future steps. In this work, we study the provable robustness of RL against norm-bounded adversarial perturbations of the inputs. We focus on smoothing-based provable defenses and propose policy smoothing where the agent adds a Gaussian noise to its observation at each time-step before applying the policy network to make itself less sensitive to adversarial perturbations of its inputs. Our main theoretical contribution is to prove an adaptive version of the Neyman-Pearson Lemma where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previously observed actions. Using this lemma, we adapt the robustness certificates produced by randomized smoothing in the static setting of image classification to the dynamic setting of RL. We generate certificates that guarantee that the total reward obtained by the smoothed policy will not fall below a certain threshold under a norm-bounded adversarial perturbation of the input. We show that our certificates are tight by constructing a worst-case setting that achieves the bounds derived in our analysis. In our experiments, we show that this method can yield meaningful certificates in complex environments demonstrating its effectiveness against adversarial attacks.",0
"The primary focus of research on deep neural network (DNN) models that are provably resilient to adversarial attacks has been on static supervised learning tasks like image classification. However, the use of DNNs in real-world adaptive tasks like reinforcement learning (RL) makes RL systems vulnerable to adversarial attacks. The main challenge in adversarial RL is that attackers can adapt to the defense strategies used by the agent in previous time-steps to strengthen their attacks in future steps. This study aims to examine the provable robustness of RL against norm-bounded adversarial perturbations of inputs. The researchers focus on smoothing-based provable defenses and suggest policy smoothing, where the agent adds Gaussian noise to its observation at each time-step before applying the policy network, to reduce sensitivity to adversarial perturbations. The authors' primary theoretical contribution is to prove an adaptive version of the Neyman-Pearson Lemma, which takes into account current and previous observations, states, and actions. Using this lemma, the authors adapt the robustness certificates produced by randomized smoothing in the static setting of image classification to the dynamic setting of RL. The authors generate certificates that ensure the total reward obtained by the smoothed policy will not fall below a specific threshold under a norm-bounded adversarial perturbation of the input. The authors show that these certificates are tight by constructing a worst-case scenario that achieves the bounds derived in their analysis. The authors demonstrate the effectiveness of their approach against adversarial attacks in complex environments through various experiments.",1
"Recently deep reinforcement learning has achieved tremendous success in wide ranges of applications. However, it notoriously lacks data-efficiency and interpretability. Data-efficiency is important as interacting with the environment is expensive. Further, interpretability can increase the transparency of the black-box-style deep RL models and hence gain trust from the users. In this work, we propose a new hierarchical framework via symbolic RL, leveraging a symbolic transition model to improve the data-efficiency and introduce the interpretability for learned policy. This framework consists of a high-level agent, a subtask solver and a symbolic transition model. Without assuming any prior knowledge on the state transition, we adopt inductive logic programming (ILP) to learn the rules of symbolic state transitions, introducing interpretability and making the learned behavior understandable to users. In empirical experiments, we confirmed that the proposed framework offers approximately between 30\% to 40\% more data efficiency over previous methods.",0
"Deep reinforcement learning has made significant strides in various fields, but it is notorious for being inefficient in data usage and lacking interpretability. Interacting with the environment is costly, making data-efficiency critical. Additionally, transparency in black-box-style deep RL models can foster user trust, emphasizing the need for interpretability. In this study, a new hierarchical framework employing symbolic RL and a symbolic transition model is proposed to enhance data-efficiency and introduce interpretability. The framework comprises a high-level agent, subtask solver, and symbolic transition model. Inductive logic programming (ILP) is utilized to learn the rules of symbolic state transitions, providing interpretability and making learned behavior clear to users, without requiring prior knowledge of state transitions. Empirical tests show that the proposed framework improves data efficiency by approximately 30-40% compared to previous methods.",1
"There has recently been significant interest in training reinforcement learning (RL) agents in vision-based environments. This poses many challenges, such as high dimensionality and potential for observational overfitting through spurious correlations. A promising approach to solve both of these problems is a self-attention bottleneck, which provides a simple and effective framework for learning high performing policies, even in the presence of distractions. However, due to poor scalability of attention architectures, these methods do not scale beyond low resolution visual inputs, using large patches (thus small attention matrices). In this paper we make use of new efficient attention algorithms, recently shown to be highly effective for Transformers, and demonstrate that these new techniques can be applied in the RL setting. This allows our attention-based controllers to scale to larger visual inputs, and facilitate the use of smaller patches, even individual pixels, improving generalization. In addition, we propose a new efficient algorithm approximating softmax attention with what we call hybrid random features, leveraging the theory of angular kernels. We show theoretically and empirically that hybrid random features is a promising approach when using attention for vision-based RL.",0
"Lately, there has been a surge of interest in training reinforcement learning (RL) agents in environments that rely on vision. This poses several obstacles, such as high dimensionality and potential for observational overfitting through spurious correlations. To address these issues, a self-attention bottleneck has shown promise as an effective framework for learning high-performing policies, even when distractions are present. However, current attention architectures are unable to scale beyond low-resolution visual inputs, as they rely on large patches, resulting in small attention matrices. In this study, we utilize new efficient attention algorithms that have proven highly effective for Transformers, demonstrating their applicability in the RL setting. This enables attention-based controllers to scale to larger visual inputs by utilizing smaller patches, such as individual pixels, leading to improved generalization. Additionally, we propose a new efficient algorithm, called hybrid random features, for approximating softmax attention by leveraging the theory of angular kernels. We demonstrate both theoretically and empirically that hybrid random features show promise for attention-based RL in vision-based environments.",1
"Reinforcement learning (RL) has gained increasing interest since the demonstration it was able to reach human performance on video game benchmarks using deep Q-learning (DQN). The current consensus for training neural networks on such complex environments is to rely on gradient-based optimization. Although alternative Bayesian deep learning methods exist, most of them still rely on gradient-based optimization, and they typically do not scale on benchmarks such as the Atari game environment. Moreover none of these approaches allow performing the analytical inference for the weights and biases defining the neural network. In this paper, we present how we can adapt the temporal difference Q-learning framework to make it compatible with the tractable approximate Gaussian inference (TAGI), which allows learning the parameters of a neural network using a closed-form analytical method. Throughout the experiments with on- and off-policy reinforcement learning approaches, we demonstrate that TAGI can reach a performance comparable to backpropagation-trained networks while using fewer hyperparameters, and without relying on gradient-based optimization.",0
"The interest in reinforcement learning (RL) has grown due to its ability to achieve human-level performance in video game benchmarks with deep Q-learning (DQN). Although gradient-based optimization is the preferred approach for training neural networks in complex environments, there are Bayesian deep learning methods that still rely on gradient-based optimization and do not scale in benchmarks like the Atari game environment. Additionally, none of these methods allow for analytical inference in the neural network's weights and biases. This paper presents how the temporal difference Q-learning framework can be adapted to work with tractable approximate Gaussian inference (TAGI), which enables the use of closed-form analytical methods for learning neural network parameters. Our experiments with on- and off-policy reinforcement learning techniques show that TAGI can achieve performance levels comparable to those of backpropagation-trained networks, with fewer hyperparameters, and without relying on gradient-based optimization.",1
"Continual learning (CL) -- the ability to continuously learn, building on previously acquired knowledge -- is a natural requirement for long-lived autonomous reinforcement learning (RL) agents. While building such agents, one needs to balance opposing desiderata, such as constraints on capacity and compute, the ability to not catastrophically forget, and to exhibit positive transfer on new tasks. Understanding the right trade-off is conceptually and computationally challenging, which we argue has led the community to overly focus on catastrophic forgetting. In response to these issues, we advocate for the need to prioritize forward transfer and propose Continual World, a benchmark consisting of realistic and meaningfully diverse robotic tasks built on top of Meta-World as a testbed. Following an in-depth empirical evaluation of existing CL methods, we pinpoint their limitations and highlight unique algorithmic challenges in the RL setting. Our benchmark aims to provide a meaningful and computationally inexpensive challenge for the community and thus help better understand the performance of existing and future solutions.",0
"For long-lasting autonomous reinforcement learning (RL) agents, continual learning (CL) is essential. CL involves building on previously acquired knowledge. When creating such agents, it's important to balance various opposing factors such as constraints on capacity and compute, the ability to not forget what has been learned, and the capacity to excel in new tasks. Finding the right trade-off is both conceptually and computationally challenging, which has caused the community to focus excessively on catastrophic forgetting. To address this, we propose prioritizing forward transfer and have developed a testbed, Continual World, consisting of diverse robotic tasks built on top of Meta-World. Through an extensive empirical evaluation of existing CL methods, we've identified their limitations and highlighted unique algorithmic challenges in the RL setting. Our benchmark is designed to provide an affordable and meaningful challenge for the community, aiding in the understanding of the performance of existing and future solutions.",1
"Off-policy learning allows us to learn about possible policies of behavior from experience generated by a different behavior policy. Temporal difference (TD) learning algorithms can become unstable when combined with function approximation and off-policy sampling - this is known as the ''deadly triad''. Emphatic temporal difference (ETD($\lambda$)) algorithm ensures convergence in the linear case by appropriately weighting the TD($\lambda$) updates. In this paper, we extend the use of emphatic methods to deep reinforcement learning agents. We show that naively adapting ETD($\lambda$) to popular deep reinforcement learning algorithms, which use forward view multi-step returns, results in poor performance. We then derive new emphatic algorithms for use in the context of such algorithms, and we demonstrate that they provide noticeable benefits in small problems designed to highlight the instability of TD methods. Finally, we observed improved performance when applying these algorithms at scale on classic Atari games from the Arcade Learning Environment.",0
"Learning from experience generated by a different behavior policy is made possible through off-policy learning. The combination of temporal difference (TD) learning algorithms with function approximation and off-policy sampling can lead to instability, also known as the ''deadly triad''. The Emphatic temporal difference (ETD($\lambda$)) algorithm addresses this issue by appropriately weighting TD($\lambda$) updates, ensuring convergence in the linear case. In this study, we extend the use of emphatic methods to deep reinforcement learning agents. However, adapting ETD($\lambda$) to popular deep reinforcement learning algorithms that use forward view multi-step returns leads to poor performance. Therefore, we derive new emphatic algorithms suited to these algorithms and demonstrate their benefits in small problems showcasing the instability of TD methods. Finally, we show improved performance on classic Atari games from the Arcade Learning Environment when these algorithms are applied at scale.",1
"Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks. Source code and videos are available at https://sites.google.com/view/re3-rl.",0
"Improved methods of exploration have been successful in enhancing sample-efficiency in deep reinforcement learning (RL), but there is still difficulty in efficient exploration in observation spaces with high dimensions. To address this issue, this study introduces Random Encoders for Efficient Exploration (RE3), an exploration technique that uses state entropy as an intrinsic reward. To estimate state entropy in environments with high-dimensional observations, a k-nearest neighbor entropy estimator is utilized in the low-dimensional representation space of a convolutional encoder. The results demonstrate that utilizing a randomly initialized encoder that remains fixed throughout training offers a stable and compute-efficient way to estimate state entropy. The experiments exhibit that RE3 significantly enhances the sample-efficiency of both model-free and model-based RL methods in locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. Additionally, RE3 allows for learning diverse behaviors without the need for extrinsic rewards, which effectively enhances sample-efficiency in downstream tasks. The source code and videos are accessible at https://sites.google.com/view/re3-rl.",1
"We consider the offline reinforcement learning (RL) setting where the agent aims to optimize the policy solely from the data without further environment interactions. In offline RL, the distributional shift becomes the primary source of difficulty, which arises from the deviation of the target policy being optimized from the behavior policy used for data collection. This typically causes overestimation of action values, which poses severe problems for model-free algorithms that use bootstrapping. To mitigate the problem, prior offline RL algorithms often used sophisticated techniques that encourage underestimation of action values, which introduces an additional set of hyperparameters that need to be tuned properly. In this paper, we present an offline RL algorithm that prevents overestimation in a more principled way. Our algorithm, OptiDICE, directly estimates the stationary distribution corrections of the optimal policy and does not rely on policy-gradients, unlike previous offline RL algorithms. Using an extensive set of benchmark datasets for offline RL, we show that OptiDICE performs competitively with the state-of-the-art methods.",0
"We are discussing the offline reinforcement learning (RL) scenario, where the objective of the agent is to optimize the policy exclusively from the available data, without any further interactions with the environment. In offline RL, the main obstacle is the distributional shift, resulting from the variation between the target policy being optimized and the behavior policy employed during data collection. This usually leads to an overestimation of action values, which can pose significant issues for model-free algorithms using bootstrapping. To tackle this, previous offline RL algorithms have implemented sophisticated methods to promote underestimation of action values, requiring additional hyperparameters to be adjusted. However, in this study, we introduce a more principled approach to address overestimation by presenting OptiDICE, an offline RL algorithm that estimates the stationary distribution corrections of the optimal policy directly, without relying on policy-gradients. We demonstrate that OptiDICE is competitive with state-of-the-art methods, using a comprehensive range of benchmark datasets for offline RL.",1
"We study defense strategies against reward poisoning attacks in reinforcement learning. As a threat model, we consider attacks that minimally alter rewards to make the attacker's target policy uniquely optimal under the poisoned rewards, with the optimality gap specified by an attack parameter. Our goal is to design agents that are robust against such attacks in terms of the worst-case utility w.r.t. the true, unpoisoned, rewards while computing their policies under the poisoned rewards. We propose an optimization framework for deriving optimal defense policies, both when the attack parameter is known and unknown. Moreover, we show that defense policies that are solutions to the proposed optimization problems have provable performance guarantees. In particular, we provide the following bounds with respect to the true, unpoisoned, rewards: a) lower bounds on the expected return of the defense policies, and b) upper bounds on how suboptimal these defense policies are compared to the attacker's target policy. We conclude the paper by illustrating the intuitions behind our formal results, and showing that the derived bounds are non-trivial.",0
"Our focus is on studying defense strategies against reward poisoning attacks in the context of reinforcement learning. Specifically, we examine attacks that manipulate rewards to make the attacker's desired policy the most optimal under the poisoned rewards, with the extent of the optimality gap being determined by an attack parameter. Our objective is to create agents that can withstand such attacks, ensuring the worst-case utility in relation to the true and unpoisoned rewards while calculating their policies under the poisoned rewards. To achieve this, we propose an optimization framework that can generate optimal defense policies even when the attack parameter is unknown. Our proposed defense policies have a guaranteed level of performance, as demonstrated by the lower bounds we provide on their expected returns and the upper bounds on their suboptimality compared to the attacker's target policy. Our paper concludes by presenting the underlying ideas behind our formal results and demonstrating the non-triviality of the derived bounds.",1
"Reinforcement learning is a general technique that allows an agent to learn an optimal policy and interact with an environment in sequential decision making problems. The goodness of a policy is measured by its value function starting from some initial state. The focus of this paper is to construct confidence intervals (CIs) for a policy's value in infinite horizon settings where the number of decision points diverges to infinity. We propose to model the action-value state function (Q-function) associated with a policy based on series/sieve method to derive its confidence interval. When the target policy depends on the observed data as well, we propose a SequentiAl Value Evaluation (SAVE) method to recursively update the estimated policy and its value estimator. As long as either the number of trajectories or the number of decision points diverges to infinity, we show that the proposed CI achieves nominal coverage even in cases where the optimal policy is not unique. Simulation studies are conducted to back up our theoretical findings. We apply the proposed method to a dataset from mobile health studies and find that reinforcement learning algorithms could help improve patient's health status. A Python implementation of the proposed procedure is available at https://github.com/shengzhang37/SAVE.",0
"The technique of reinforcement learning enables an agent to learn the optimal policy and engage with the environment in sequential decision-making problems. The policy's effectiveness is determined by its value function, which is evaluated from an initial state. This paper aims to establish confidence intervals (CIs) for the value of a policy in infinite horizon settings, where the number of decision points is infinite. We suggest using the series/sieve approach to model the action-value state function (Q-function) associated with a policy to derive its confidence interval. If the target policy is dependent on observed data, we propose a recursive SequentiAl Value Evaluation (SAVE) method to update the estimated policy and its value estimator. Our proposed CI achieves nominal coverage even in cases where the optimal policy is not unique, provided that either the number of trajectories or decision points is infinite. We perform simulation studies to support our theoretical findings. We demonstrate the effectiveness of reinforcement learning algorithms in improving patient health status using a dataset from mobile health research. A Python implementation of our proposed procedure is available at https://github.com/shengzhang37/SAVE.",1
"Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework -- a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks.",0
"Many reinforcement learning (RL) methods rely on action-value estimation, which plays a crucial role in determining sample complexity. Learning a good estimator for action value depends largely on how quickly it can be learned. Representation learning is a useful tool in improving action-value estimation, with good representations of both state and action facilitating the process. While deep learning has greatly improved the learning of state representations, action representations have been largely overlooked in RL. We propose that an effective way to learn good action representations is by leveraging the combinatorial structure of multi-dimensional action spaces. To test this hypothesis, we introduce the action hypergraph networks framework, which is a class of functions designed to learn action representations in multi-dimensional discrete action spaces with a structural inductive bias. We combine this framework with deep Q-networks to create a new agent class called hypergraph Q-networks. We demonstrate the effectiveness of our approach in several domains, including prediction problems, Atari 2600 games, and discretized physical control benchmarks.",1
"Reinforcement learning (RL) enables robots to learn skills from interactions with the real world. In practice, the unstructured step-based exploration used in Deep RL -- often very successful in simulation -- leads to jerky motion patterns on real robots. Consequences of the resulting shaky behavior are poor exploration, or even damage to the robot. We address these issues by adapting state-dependent exploration (SDE) to current Deep RL algorithms. To enable this adaptation, we propose two extensions to the original SDE, using more general features and re-sampling the noise periodically, which leads to a new exploration method generalized state-dependent exploration (gSDE). We evaluate gSDE both in simulation, on PyBullet continuous control tasks, and directly on three different real robots: a tendon-driven elastic robot, a quadruped and an RC car. The noise sampling interval of gSDE permits to have a compromise between performance and smoothness, which allows training directly on the real robots without loss of performance. The code is available at https://github.com/DLR-RM/stable-baselines3.",0
"Robots can acquire skills from their experiences with the real world through reinforcement learning (RL). However, the step-based exploration approach used in Deep RL can result in choppy movements when applied to real robots, despite its success in simulations. This can lead to inadequate exploration or damage to the robot. To address these challenges, we introduce state-dependent exploration (SDE) to Deep RL algorithms, with two modifications: the use of more general features and periodic noise resampling. This results in a new exploration method called generalized state-dependent exploration (gSDE). We evaluate gSDE on PyBullet continuous control tasks in simulation and on three different real robots, including a tendon-driven elastic robot, a quadruped, and an RC car. The noise sampling interval of gSDE balances performance and smoothness, enabling direct training on real robots without sacrificing performance. The code for gSDE is available at https://github.com/DLR-RM/stable-baselines3.",1
"We developed Distilled Graph Attention Policy Networks (DGAPNs), a curiosity-driven reinforcement learning model to generate novel graph-structured chemical representations that optimize user-defined objectives by efficiently navigating a physically constrained domain. The framework is examined on the task of generating molecules that are designed to bind, noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial Graph Attention Network (sGAT) that leverages self-attention over both node and edge attributes as well as encoding spatial structure -- this capability is of considerable interest in areas such as molecular and synthetic biology and drug discovery. An attentional policy network is then introduced to learn decision rules for a dynamic, fragment-based chemical environment, and state-of-the-art policy gradient techniques are employed to train the network with enhanced stability. Exploration is efficiently encouraged by incorporating innovation reward bonuses learned and proposed by random network distillation. In experiments, our framework achieved outstanding results compared to state-of-the-art algorithms, while increasing the diversity of proposed molecules and reducing the complexity of paths to chemical synthesis.",0
"We have created Distilled Graph Attention Policy Networks (DGAPNs), a reinforcement learning model that is driven by curiosity and generates new chemical representations in graph form. It efficiently navigates a physically limited domain to optimize user-defined objectives. We tested this framework by using it to design non-covalent molecules for binding to functional sites of SARS-CoV-2 proteins. Our model incorporates a spatial Graph Attention Network (sGAT) that uses self-attention to consider both node and edge attributes, as well as spatial structure. This is particularly useful for molecular and synthetic biology, as well as drug discovery. Additionally, we have introduced an attentional policy network that uses decision rules for a fragment-based chemical environment, and state-of-the-art policy gradient techniques to train the network with improved stability. To encourage exploration, we have integrated innovation reward bonuses learned from random network distillation. Our experiments have shown that our framework outperforms current algorithms, and it also increases the diversity of generated molecules while simplifying the paths to chemical synthesis.",1
"Several practical applications of reinforcement learning involve an agent learning from past data without the possibility of further exploration. Often these applications require us to 1) identify a near optimal policy or to 2) estimate the value of a target policy. For both tasks we derive \emph{exponential} information-theoretic lower bounds in discounted infinite horizon MDPs with a linear function representation for the action value function even if 1) \emph{realizability} holds, 2) the batch algorithm observes the exact reward and transition \emph{functions}, and 3) the batch algorithm is given the \emph{best} a priori data distribution for the problem class. Our work introduces a new `oracle + batch algorithm' framework to prove lower bounds that hold for every distribution. The work shows an exponential separation between batch and online reinforcement learning.",0
"Reinforcement learning has practical applications where an agent learns from past data without exploring further. These applications often require finding a near-optimal policy or estimating the value of a target policy. Even with realizability, exact observation of reward and transition functions, and the best a priori data distribution for the problem class, we derive exponential information-theoretic lower bounds for both tasks in discounted infinite horizon MDPs with a linear function representation for the action value function. Our new framework, the ""oracle + batch algorithm,"" proves lower bounds that hold for every distribution, revealing an exponential gap between batch and online reinforcement learning.",1
"We consider the problem of computing reach-avoid probabilities for iterative predictions made with Bayesian neural network (BNN) models. Specifically, we leverage bound propagation techniques and backward recursion to compute lower bounds for the probability that trajectories of the BNN model reach a given set of states while avoiding a set of unsafe states. We use the lower bounds in the context of control and reinforcement learning to provide safety certification for given control policies, as well as to synthesize control policies that improve the certification bounds. On a set of benchmarks, we demonstrate that our framework can be employed to certify policies over BNNs predictions for problems of more than $10$ dimensions, and to effectively synthesize policies that significantly increase the lower bound on the satisfaction probability.",0
"The task at hand involves determining the likelihood of reaching a set of states while evading unsafe ones, through the use of Bayesian neural network (BNN) models. We achieve this by utilizing backward recursion and bound propagation techniques to establish lower bounds. These bounds serve a dual purpose: to certify the safety of existing control policies, and to generate new policies that enhance safety certification. Our approach is validated through a series of tests, showcasing its effectiveness in certifying policies over BNN predictions with a dimensionality of over 10, and in creating policies that significantly improve the probability of satisfaction.",1
"Intelligent video summarization algorithms allow to quickly convey the most relevant information in videos through the identification of the most essential and explanatory content while removing redundant video frames. In this paper, we introduce the 3DST-UNet-RL framework for video summarization. A 3D spatio-temporal U-Net is used to efficiently encode spatio-temporal information of the input videos for downstream reinforcement learning (RL). An RL agent learns from spatio-temporal latent scores and predicts actions for keeping or rejecting a video frame in a video summary. We investigate if real/inflated 3D spatio-temporal CNN features are better suited to learn representations from videos than commonly used 2D image features. Our framework can operate in both, a fully unsupervised mode and a supervised training mode. We analyse the impact of prescribed summary lengths and show experimental evidence for the effectiveness of 3DST-UNet-RL on two commonly used general video summarization benchmarks. We also applied our method on a medical video summarization task. The proposed video summarization method has the potential to save storage costs of ultrasound screening videos as well as to increase efficiency when browsing patient video data during retrospective analysis or audit without loosing essential information",0
"Intelligent algorithms for video summarization efficiently convey relevant information by identifying crucial and explanatory content while removing unnecessary frames. This paper introduces the 3DST-UNet-RL framework, which uses a 3D spatio-temporal U-Net to encode input videos' spatio-temporal information and downstream reinforcement learning (RL) to predict actions for keeping or rejecting frames for video summaries. The framework can operate in unsupervised or supervised modes and evaluates the effectiveness of 3D spatio-temporal CNN features versus commonly used 2D image features. The proposed method is effective in general video summarization benchmarks and medical video summarization tasks, potentially reducing storage costs and increasing efficiency without losing essential information during retrospective analysis or audit.",1
"Recent years saw a plethora of work on explaining complex intelligent agents. One example is the development of several algorithms that generate saliency maps which show how much each pixel attributed to the agents' decision. However, most evaluations of such saliency maps focus on image classification tasks. As far as we know, there is no work that thoroughly compares different saliency maps for Deep Reinforcement Learning agents. This paper compares four perturbation-based approaches to create saliency maps for Deep Reinforcement Learning agents trained on four different Atari 2600 games. All four approaches work by perturbing parts of the input and measuring how much this affects the agent's output. The approaches are compared using three computational metrics: dependence on the learned parameters of the agent (sanity checks), faithfulness to the agent's reasoning (input degradation), and run-time. In particular, during the sanity checks we find issues with two approaches and propose a solution to fix one of those issues.",0
"In recent years, there has been a significant amount of research dedicated to explaining the behavior of complex intelligent agents. One area of study involves the creation of algorithms that generate saliency maps, which demonstrate how much each pixel contributes to the agent's decision-making process. Unfortunately, most of the evaluations of these saliency maps have focused solely on image classification tasks. To our knowledge, there has been no comprehensive comparison of different saliency maps for Deep Reinforcement Learning agents. To address this gap, our paper examines four perturbation-based approaches for creating saliency maps for Deep Reinforcement Learning agents that have been trained on four different Atari 2600 games. All four approaches involve perturbing parts of the input and measuring the effect on the agent's output. We use three computational metrics to compare these approaches: dependence on the learned parameters of the agent (sanity checks), faithfulness to the agent's reasoning (input degradation), and run-time. During the sanity checks, we identify issues with two of the approaches, and we propose a solution to address one of these problems.",1
"In an ever expanding set of research and application areas, deep neural networks (DNNs) set the bar for algorithm performance. However, depending upon additional constraints such as processing power and execution time limits, or requirements such as verifiable safety guarantees, it may not be feasible to actually use such high-performing DNNs in practice. Many techniques have been developed in recent years to compress or distill complex DNNs into smaller, faster or more understandable models and controllers. This work seeks to identify reduced models that not only preserve a desired performance level, but also, for example, succinctly explain the latent knowledge represented by a DNN. We illustrate the effectiveness of the proposed approach on the evaluation of decision tree variants and kernel machines in the context of benchmark reinforcement learning tasks.",0
"Deep neural networks (DNNs) are currently the standard for achieving high algorithm performance in various research and application domains. However, practical limitations such as processing power and execution time, as well as requirements such as safety guarantees, may prevent the use of these high-performing DNNs. To address this challenge, several techniques have been developed to compress or distill complex DNNs into smaller, faster, or more interpretable models and controllers. This study aims to identify reduced models that not only maintain a desired level of performance but also provide concise explanations of the latent knowledge represented by a DNN. We demonstrate the effectiveness of this approach by evaluating decision tree variants and kernel machines in the context of benchmark reinforcement learning tasks.",1
"Detection in large-scale scenes is a challenging problem due to small objects and extreme scale variation. It is essential to focus on the image regions of small objects. In this paper, we propose a novel Adaptive Zoom (AdaZoom) network as a selective magnifier with flexible shape and focal length to adaptively zoom the focus regions for object detection. Based on policy gradient, we construct a reinforcement learning framework for focus region generation, with the reward formulated by object distributions. The scales and aspect ratios of the generated regions are adaptive to the scales and distribution of objects inside. We apply variable magnification according to the scale of the region for adaptive multi-scale detection. We further propose collaborative training to complementarily promote the performance of AdaZoom and the detection network. To validate the effectiveness, we conduct extensive experiments on VisDrone2019, UAVDT, and DOTA datasets. The experiments show AdaZoom brings a consistent and significant improvement over different detection networks, achieving state-of-the-art performance on these datasets, especially outperforming the existing methods by AP of 4.64% on Vis-Drone2019.",0
"Detecting objects in large-scale scenes is a complex task due to the presence of small objects and extreme variations in scale. It is crucial to concentrate on image regions that contain small objects. This paper introduces a new Adaptive Zoom (AdaZoom) network that acts as a selective magnifier with a flexible shape and focal length to zoom in on relevant regions for object detection. Using a reinforcement learning framework based on policy gradient, we generate focus regions with object distributions serving as rewards. The generated regions adapt to the scales and aspect ratios of the objects within them, and we use variable magnification based on the region's scale for adaptive multi-scale detection. Additionally, our collaborative training approach enhances both AdaZoom and the detection network's performance. We tested our method on VisDrone2019, UAVDT, and DOTA datasets and found that AdaZoom provides consistent and significant improvements over existing detection networks, achieving state-of-the-art performance, with a 4.64% increase in AP on VisDrone2019.",1
"Actors and critics in actor-critic reinforcement learning algorithms are functionally separate, yet they often use the same network architectures. This case study explores the performance impact of network sizes when considering actor and critic architectures independently. By relaxing the assumption of architectural symmetry, it is often possible for smaller actors to achieve comparable policy performance to their symmetric counterparts. Our experiments show up to 99% reduction in the number of network weights with an average reduction of 77% over multiple actor-critic algorithms on 9 independent tasks. Given that reducing actor complexity results in a direct reduction of run-time inference cost, we believe configurations of actors and critics are aspects of actor-critic design that deserve to be considered independently, particularly in resource-constrained applications or when deploying multiple actors simultaneously.",0
"Although actors and critics in actor-critic reinforcement learning algorithms serve different functions, they frequently employ identical network architectures. This research delves into the impact of network sizes on performance when considering actor and critic architectures separately. By abandoning the assumption of architectural symmetry, it is often feasible for smaller actors to attain similar policy performance to their symmetrical counterparts. Our tests indicate a decrease of up to 99% in network weights, with an average decrease of 77% across multiple actor-critic algorithms on nine distinct tasks. As reducing actor complexity results in a direct reduction in inference cost, we believe that actor and critic configurations are components of actor-critic design that warrant individual consideration, mainly in settings with limited resources or when deploying multiple actors simultaneously.",1
"Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity -- the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.",0
"Common observational methods used to learn human preferences, such as inverse reinforcement learning, often rely on assumptions about the observability of the human's environment. However, this does not reflect reality, as many important decisions are made under conditions of uncertainty. To address this, we examine the inverse decision theory (IDT) framework, which describes how humans make non-sequential binary decisions under uncertainty, conveying their preferences through a loss function that balances various types of mistakes. Our statistical analysis of IDT establishes the necessary conditions to identify these preferences and characterizes the sample complexity required to learn the human's tradeoff to a desired precision. Interestingly, we find that preference identification is easier under greater uncertainty and that our approach can identify the preferences of suboptimal decision makers. Our analysis challenges the notion that partial observability hinders preference learning and sheds light on improving methods for uncertain and suboptimal human decision making.",1
"The capability of reinforcement learning (RL) agent directly depends on the diversity of learning scenarios the environment generates and how closely it captures real-world situations. However, existing environments/simulators lack the support to systematically model distributions over initial states and transition dynamics. Furthermore, in complex domains such as soccer, the space of possible scenarios is infinite, which makes it impossible for one research group to provide a comprehensive set of scenarios to train, test, and benchmark RL algorithms. To address this issue, for the first time, we adopt an existing formal scenario specification language, SCENIC, to intuitively model and generate interactive scenarios. We interfaced SCENIC to Google Research Soccer environment to create a platform called SCENIC4RL. Using this platform, we provide a dataset consisting of 36 scenario programs encoded in SCENIC and demonstration data generated from a subset of them. We share our experimental results to show the effectiveness of our dataset and the platform to train, test, and benchmark RL algorithms. More importantly, we open-source our platform to enable RL community to collectively contribute to constructing a comprehensive set of scenarios.",0
"The effectiveness of a reinforcement learning (RL) agent depends heavily on the variety of learning scenarios the environment can generate and how accurately it reflects real-world situations. However, current environments and simulators do not have the necessary tools to systematically model distributions over initial states and transition dynamics. In addition, it is impossible for one research group to provide a comprehensive set of scenarios for training, testing, and benchmarking RL algorithms in complex domains like soccer, where the space of possible scenarios is infinite. To address this issue, we have introduced a new platform called SCENIC4RL, which uses an existing formal scenario specification language, SCENIC, to intuitively model and generate interactive scenarios. We have created a dataset consisting of 36 scenario programs encoded in SCENIC and demonstration data generated from a subset of them. Our experimental results show the effectiveness of our dataset and platform in training, testing, and benchmarking RL algorithms. Furthermore, we have open-sourced our platform to encourage the RL community to collaborate on constructing a comprehensive set of scenarios.",1
"Order dispatch is one of the central problems to ride-sharing platforms. Recently, value-based reinforcement learning algorithms have shown promising performance on this problem. However, in real-world applications, the non-stationarity of the demand-supply system poses challenges to re-utilizing data generated in different time periods to learn the value function. In this work, motivated by the fact that the relative relationship between the values of some states is largely stable across various environments, we propose a pattern transfer learning framework for value-based reinforcement learning in the order dispatch problem. Our method efficiently captures the value patterns by incorporating a concordance penalty. The superior performance of the proposed method is supported by experiments.",0
"Ride-sharing platforms face a major challenge with order dispatch. While value-based reinforcement learning algorithms have shown promise in solving this problem, the dynamic nature of the demand-supply system in the real world makes it difficult to reuse data from different time periods for learning the value function. To address this issue, we propose a pattern transfer learning framework for value-based reinforcement learning in order dispatch, taking into consideration the relatively stable relationship between the values of certain states across different environments. Our method incorporates a concordance penalty to efficiently capture these value patterns, and our experiments demonstrate its superior performance.",1
"In online reinforcement learning (RL), efficient exploration remains particularly challenging in high-dimensional environments with sparse rewards. In low-dimensional environments, where tabular parameterization is possible, count-based upper confidence bound (UCB) exploration methods achieve minimax near-optimal rates. However, it remains unclear how to efficiently implement UCB in realistic RL tasks that involve non-linear function approximation. To address this, we propose a new exploration approach via \textit{maximizing} the deviation of the occupancy of the next policy from the explored regions. We add this term as an adaptive regularizer to the standard RL objective to balance exploration vs. exploitation. We pair the new objective with a provably convergent algorithm, giving rise to a new intrinsic reward that adjusts existing bonuses. The proposed intrinsic reward is easy to implement and combine with other existing RL algorithms to conduct exploration. As a proof of concept, we evaluate the new intrinsic reward on tabular examples across a variety of model-based and model-free algorithms, showing improvements over count-only exploration strategies. When tested on navigation and locomotion tasks from MiniGrid and DeepMind Control Suite benchmarks, our approach significantly improves sample efficiency over state-of-the-art methods. Our code is available at https://github.com/tianjunz/MADE.",0
"Efficient exploration in high-dimensional environments with sparse rewards remains a challenge in online reinforcement learning (RL). Count-based upper confidence bound (UCB) exploration methods have been successful in low-dimensional environments with tabular parameterization, achieving minimax near-optimal rates. However, it is unclear how to implement UCB efficiently in realistic RL tasks that involve non-linear function approximation. To address this issue, we propose a new exploration approach by maximizing the deviation of the occupancy of the next policy from the explored regions, which we add as an adaptive regularizer to the standard RL objective to balance exploration vs. exploitation. We pair this new objective with a provably convergent algorithm, resulting in a new intrinsic reward that adjusts existing bonuses. Our proposed intrinsic reward is easy to implement and can be combined with other existing RL algorithms for exploration. We evaluate the new intrinsic reward on tabular examples, showing improvements over count-only exploration strategies. Our approach significantly improves sample efficiency over state-of-the-art methods when tested on navigation and locomotion tasks from MiniGrid and DeepMind Control Suite benchmarks. Our code can be found at https://github.com/tianjunz/MADE as a proof of concept.",1
"Traditional generative models are limited to predicting sequences of terminal tokens. However, ambiguities in the generation task may lead to incorrect outputs. Towards addressing this, we introduce Grammformers, transformer-based grammar-guided models that learn (without explicit supervision) to generate sketches -- sequences of tokens with holes. Through reinforcement learning, Grammformers learn to introduce holes avoiding the generation of incorrect tokens where there is ambiguity in the target task.   We train Grammformers for statement-level source code completion, i.e., the generation of code snippets given an ambiguous user intent, such as a partial code context. We evaluate Grammformers on code completion for C# and Python and show that it generates 10-50% more accurate sketches compared to traditional generative models and 37-50% longer sketches compared to sketch-generating baselines trained with similar techniques.",0
"Traditional generative models have limitations in predicting sequences of terminal tokens, leading to incorrect outputs due to ambiguities in the generation task. To address this issue, we introduce Grammformers, which are transformer-based grammar-guided models that can generate sketches - sequences of tokens with holes. By using reinforcement learning, Grammformers can learn to introduce holes in order to avoid generating incorrect tokens when there is ambiguity in the target task. We specifically train Grammformers for statement-level source code completion, which involves generating code snippets based on an ambiguous user intent, such as a partial code context. Our evaluation of Grammformers on code completion for C# and Python demonstrates that it can generate more accurate sketches (10-50%) compared to traditional generative models and longer sketches (37-50%) compared to sketch-generating baselines trained with similar techniques.",1
"Supervised deep convolutional neural networks (DCNNs) are currently one of the best computational models that can explain how the primate ventral visual stream solves object recognition. However, embodied cognition has not been considered in the existing visual processing models. From the ecological standpoint, humans learn to recognize objects by interacting with them, allowing better classification, specialization, and generalization. Here, we ask if computational models under the embodied learning framework can explain mechanisms underlying object recognition in the primate visual system better than the existing supervised models? To address this question, we use reinforcement learning to train neural network models to play a 3D computer game and we find that these reinforcement learning models achieve neural response prediction accuracy scores in the early visual areas (e.g., V1 and V2) in the levels that are comparable to those accomplished by the supervised neural network model. In contrast, the supervised neural network models yield better neural response predictions in the higher visual areas, compared to the reinforcement learning models. Our preliminary results suggest the future direction of visual neuroscience in which deep reinforcement learning should be included to fill the missing embodiment concept.",0
"At present, supervised deep convolutional neural networks (DCNNs) are among the most effective computational models for elucidating how the primate ventral visual stream undertakes object recognition. Nonetheless, existing visual processing models have not taken into account embodied cognition. From an ecological perspective, humans learn to recognize objects through interaction, leading to better classification, specialization, and generalization. Our inquiry concerns whether computational models under the embodied learning framework can provide a superior explanation of the mechanisms underlying object recognition in the primate visual system than existing supervised models. To investigate this issue, we employ reinforcement learning to instruct neural network models to play a 3D computer game. Our findings indicate that these reinforcement learning models can achieve neural response prediction accuracy scores in the early visual areas (e.g., V1 and V2) comparable to those attained by the supervised neural network model. In contrast, the supervised neural network models produce better neural response predictions in the higher visual areas, unlike the reinforcement learning models. Our preliminary results suggest that deep reinforcement learning should be included in future visual neuroscience research to address the missing embodiment concept.",1
"Reliant on too many experiments to learn good actions, current Reinforcement Learning (RL) algorithms have limited applicability in real-world settings, which can be too expensive to allow exploration. We propose an algorithm for batch RL, where effective policies are learned using only a fixed offline dataset instead of online interactions with the environment. The limited data in batch RL produces inherent uncertainty in value estimates of states/actions that were insufficiently represented in the training data. This leads to particularly severe extrapolation when our candidate policies diverge from one that generated the data. We propose to mitigate this issue via two straightforward penalties: a policy-constraint to reduce this divergence and a value-constraint that discourages overly optimistic estimates. Over a comprehensive set of 32 continuous-action batch RL benchmarks, our approach compares favorably to state-of-the-art methods, regardless of how the offline data were collected.",0
"Reinforcement Learning (RL) algorithms have limited practical use due to their reliance on numerous experiments to determine effective actions, which can be too costly to perform in real-world scenarios. To address this issue, we propose a batch RL algorithm that employs a fixed offline dataset to learn efficient policies, instead of real-time interactions with the environment. However, the limited data in batch RL can result in uncertainty in value estimates, especially for states and actions that were not adequately represented in the training data. This can lead to significant extrapolation if the candidate policies deviate from the ones used to generate the data. To address this challenge, we suggest two straightforward measures: a policy-constraint to minimize divergence and a value-constraint to discourage overly optimistic estimates. Our approach performs well compared to state-of-the-art methods across a range of 32 continuous-action batch RL benchmarks, regardless of how the offline data were collected.",1
"We study the fundamental question of the sample complexity of learning a good policy in finite Markov decision processes (MDPs) when the data available for learning is obtained by following a logging policy that must be chosen without knowledge of the underlying MDP. Our main results show that the sample complexity, the minimum number of transitions necessary and sufficient to obtain a good policy, is an exponential function of the relevant quantities when the planning horizon $H$ is finite. In particular, we prove that the sample complexity of obtaining $\epsilon$-optimal policies is at least $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H+1)})$ for $\gamma$-discounted problems, where $\mathrm{S}$ is the number of states, $\mathrm{A}$ is the number of actions, and $H$ is the effective horizon defined as $H=\lfloor \tfrac{\ln(1/\epsilon)}{\ln(1/\gamma)} \rfloor$; and it is at least $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$ for finite horizon problems, where $H$ is the planning horizon of the problem. This lower bound is essentially matched by an upper bound. For the average-reward setting we show that there is no algorithm finding $\epsilon$-optimal policies with a finite amount of data.",0
"The focus of our study is on determining the minimum number of transitions required to learn a good policy in finite Markov decision processes (MDPs) using data obtained from a logging policy that is selected without prior knowledge of the MDP. Our research indicates that the sample complexity, which is the minimum number of transitions required to obtain a good policy, is dependent on certain factors in an exponential manner when the planning horizon is finite. Specifically, we have demonstrated that for $\gamma$-discounted problems, the sample complexity needed to achieve $\epsilon$-optimal policies is at least $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H+1)})$, where $\mathrm{S}$ is the number of states, $\mathrm{A}$ is the number of actions, and $H$ is the effective horizon. Similarly, for finite horizon problems, the sample complexity is at least $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$. Our lower bound is almost perfectly matched by an upper bound. In addition, we have established that it is not possible to find $\epsilon$-optimal policies with a finite amount of data in the average-reward setting.",1
"Offline Reinforcement Learning (RL) aims to extract near-optimal policies from imperfect offline data without additional environment interactions. Extracting policies from diverse offline datasets has the potential to expand the range of applicability of RL by making the training process safer, faster, and more streamlined. We investigate how to improve the performance of offline RL algorithms, its robustness to the quality of offline data, as well as its generalization capabilities. To this end, we introduce Offline Model-based RL with Adaptive Behavioral Priors (MABE). Our algorithm is based on the finding that dynamics models, which support within-domain generalization, and behavioral priors, which support cross-domain generalization, are complementary. When combined together, they substantially improve the performance and generalization of offline RL policies. In the widely studied D4RL offline RL benchmark, we find that MABE achieves higher average performance compared to prior model-free and model-based algorithms. In experiments that require cross-domain generalization, we find that MABE outperforms prior methods. Our website is available at https://sites.google.com/berkeley.edu/mabe .",0
"The goal of Offline Reinforcement Learning (RL) is to generate high-quality policies from imperfect offline data without the need for additional interactions with the environment. This approach has the potential to make RL more widely applicable, as it can speed up and simplify the training process while increasing safety. Our research aims to enhance the performance of offline RL algorithms, as well as their ability to generalize and handle low-quality data. To achieve this, we propose a new algorithm called Offline Model-based RL with Adaptive Behavioral Priors (MABE). Our approach combines two complementary components: dynamics models, which enable within-domain generalization, and behavioral priors, which facilitate cross-domain generalization. Our experiments on the D4RL offline RL benchmark demonstrate that MABE outperforms previous model-free and model-based algorithms in terms of average performance, and it also excels in experiments that require cross-domain generalization. For more information, visit our website at https://sites.google.com/berkeley.edu/mabe.",1
"Reinforcement learning synthesizes controllers without prior knowledge of the system. At each timestep, a reward is given. The controllers optimize the discounted sum of these rewards. Applying this class of algorithms requires designing a reward scheme, which is typically done manually. The designer must ensure that their intent is accurately captured. This may not be trivial, and is prone to error. An alternative to this manual programming, akin to programming directly in assembly, is to specify the objective in a formal language and have it ""compiled"" to a reward scheme. Mungojerrie (https://plv.colorado.edu/mungojerrie/) is a tool for testing reward schemes for $\omega$-regular objectives on finite models. The tool contains reinforcement learning algorithms and a probabilistic model checker. Mungojerrie supports models specified in PRISM and $\omega$-automata specified in HOA.",0
"Reinforcement learning is a technique that generates controllers without any prior knowledge of the system. In this approach, a reward is given at each timestep and the controllers optimize the discounted sum of these rewards. However, designing a suitable reward scheme is a challenging task that is typically done manually. The designer must ensure that their intended objectives are accurately captured, which can be prone to error. A more efficient alternative is to specify the objectives in a formal language and have them translated into a reward scheme. Mungojerrie is a tool that facilitates this process by offering reinforcement learning algorithms and a probabilistic model checker. The tool supports models specified in PRISM and $\omega$-automata specified in HOA and is particularly useful for testing reward schemes for $\omega$-regular objectives on finite models.",1
"Recent renewed interest in multi-agent reinforcement learning (MARL) has generated an impressive array of techniques that leverage deep reinforcement learning, primarily actor-critic architectures, and can be applied to a limited range of settings in terms of observability and communication. However, a continuing limitation of much of this work is the curse of dimensionality when it comes to representations based on joint actions, which grow exponentially with the number of agents. In this paper, we squarely focus on this challenge of scalability. We apply the key insight of action anonymity, which leads to permutation invariance of joint actions, to two recently presented deep MARL algorithms, MADDPG and IA2C, and compare these instantiations to another recent technique that leverages action anonymity, viz., mean-field MARL. We show that our instantiations can learn the optimal behavior in a broader class of agent networks than the mean-field method, using a recently introduced pragmatic domain.",0
"The renewed interest in multi-agent reinforcement learning (MARL) has resulted in a variety of techniques that utilize deep reinforcement learning, mainly actor-critic architectures, and are applicable to a limited range of scenarios in terms of observability and communication. However, a significant limitation of most of these methods is the curse of dimensionality associated with representations based on joint actions, which grows exponentially with the number of agents. This paper focuses on addressing this scalability challenge by utilizing action anonymity, which ensures permutation invariance of joint actions, in two deep MARL algorithms, MADDPG and IA2C, and comparing them to mean-field MARL, another recent technique that leverages action anonymity. Using a pragmatic domain, we demonstrate that our approaches can learn optimal behavior in a broader range of agent networks compared to mean-field MARL.",1
"A popular computer puzzle, the game of Minesweeper requires its human players to have a mix of both luck and strategy to succeed. Analyzing these aspects more formally, in our research we assessed the feasibility of a novel methodology based on Reinforcement Learning as an adequate approach to tackle the problem presented by this game. For this purpose we employed Multi-Armed Bandit algorithms which were carefully adapted in order to enable their use to define autonomous computational players, targeting to make the best use of some game peculiarities. After experimental evaluation, results showed that this approach was indeed successful, especially in smaller game boards, such as the standard beginner level. Despite this fact the main contribution of this work is a detailed examination of Minesweeper from a learning perspective, which led to various original insights which are thoroughly discussed.",0
"Minesweeper, a popular computer puzzle game, requires a combination of both luck and strategy to win. Our research delved deeper into these aspects and explored a new approach to solving the game using Reinforcement Learning. We utilized Multi-Armed Bandit algorithms, which we adapted to create autonomous computational players that could take advantage of the game's unique characteristics. Our experimental evaluation revealed that this approach was successful, particularly in smaller game boards like the beginner level. However, the primary contribution of our work is a comprehensive analysis of Minesweeper from a learning perspective, which yielded several original insights that we discuss in detail.",1
"Modelling the behaviours of other agents is essential for understanding how agents interact and making effective decisions. Existing methods for agent modelling commonly assume knowledge of the local observations and chosen actions of the modelled agents during execution. To eliminate this assumption, we extract representations from the local information of the controlled agent using encoder-decoder architectures. Using the observations and actions of the modelled agents during training, our models learn to extract representations about the modelled agents conditioned only on the local observations of the controlled agent. The representations are used to augment the controlled agent's decision policy which is trained via deep reinforcement learning; thus, during execution, the policy does not require access to other agents' information. We provide a comprehensive evaluation and ablations studies in cooperative, competitive and mixed multi-agent environments, showing that our method achieves significantly higher returns than baseline methods which do not use the learned representations.",0
"In order to comprehend how agents interact and make effective decisions, it is crucial to model their behaviors. However, current agent modeling techniques assume familiarity with the local observations and chosen actions of the agents being modeled during execution. To eliminate this reliance, we employ encoder-decoder architectures to extract representations from the local information of the controlled agent. Our models learn to extract representations of the modeled agents conditioned solely on the local observations of the controlled agent, using the observations and actions of the modeled agents during training. These representations are used to enhance the controlled agent's decision policy, which is trained through deep reinforcement learning. As a result, during execution, the policy does not require access to other agents' information. Our method has been extensively evaluated and ablated in cooperative, competitive, and mixed multi-agent environments, demonstrating that it produces significantly higher returns than baseline methods that do not employ learned representations.",1
"Purpose: Image classification is perhaps the most fundamental task in imaging AI. However, labeling images is time-consuming and tedious. We have recently demonstrated that reinforcement learning (RL) can classify 2D slices of MRI brain images with high accuracy. Here we make two important steps toward speeding image classification: Firstly, we automatically extract class labels from the clinical reports. Secondly, we extend our prior 2D classification work to fully 3D image volumes from our institution. Hence, we proceed as follows: in Part 1, we extract labels from reports automatically using the SBERT natural language processing approach. Then, in Part 2, we use these labels with RL to train a classification Deep-Q Network (DQN) for 3D image volumes.   Methods: For Part 1, we trained SBERT with 90 radiology report impressions. We then used the trained SBERT to predict class labels for use in Part 2. In Part 2, we applied multi-step image classification to allow for combined Deep-Q learning using 3D convolutions and TD(0) Q learning. We trained on a set of 90 images. We tested on a separate set of 61 images, again using the classes predicted from patient reports by the trained SBERT in Part 1. For comparison, we also trained and tested a supervised deep learning classification network on the same set of training and testing images using the same labels.   Results: Part 1: Upon training with the corpus of radiology reports, the SBERT model had 100% accuracy for both normal and metastasis-containing scans. Part 2: Then, using these labels, whereas the supervised approach quickly overfit the training data and as expected performed poorly on the testing set (66% accuracy, just over random guessing), the reinforcement learning approach achieved an accuracy of 92%. The results were found to be statistically significant, with a p-value of 3.1 x 10^-5.",0
"The primary objective of imaging AI is image classification, which is a time-consuming and tedious task due to the need for image labeling. Our recent study has shown that reinforcement learning (RL) can be used to classify MRI brain images with high accuracy. To expedite image classification, we have taken two significant steps. Firstly, we have employed the SBERT natural language processing technique to automatically extract class labels from clinical reports. Secondly, we have extended our previous work on 2D image classification to fully 3D image volumes from our institution. The study was divided into two parts; Part 1 involved training the SBERT model on 90 radiology report impressions and using it to predict class labels for Part 2. In Part 2, we used the predicted labels with RL to train a Deep-Q Network (DQN) for 3D image volumes. We tested the models on a separate set of 61 images and compared our results with a supervised deep learning classification network. The SBERT model had excellent accuracy (100%) for both normal and metastasis-containing scans. The RL approach achieved an accuracy of 92%, which was statistically significant (p-value of 3.1 x 10^-5), while the supervised method performed poorly with an accuracy of 66%.",1
"The performance of a reinforcement learning (RL) system depends on the computational architecture used to approximate a value function. Deep learning methods provide both optimization techniques and architectures for approximating nonlinear functions from noisy, high-dimensional observations. However, prevailing optimization techniques are not designed for strictly-incremental online updates. Nor are standard architectures designed for observations with an a priori unknown structure: for example, light sensors randomly dispersed in space. This paper proposes an online RL prediction algorithm with an adaptive architecture that efficiently finds useful nonlinear features. The algorithm is evaluated in a spatial domain with high-dimensional, stochastic observations. The algorithm outperforms non-adaptive baseline architectures and approaches the performance of an architecture given side-channel information. These results are a step towards scalable RL algorithms for more general problems, where the observation structure is not available.",0
"The effectiveness of a reinforcement learning (RL) system is contingent upon the computational framework employed to approximate a value function. Deep learning methods offer both optimization methods and architectures to approximate nonlinear functions from intricate and noisy observations. Nevertheless, current optimization methods are not intended for strictly-incremental online updates, and standard architectures do not cater to observations with an unknown structure, such as randomly dispersed light sensors. This article suggests an adaptive architecture online RL prediction algorithm that effectively identifies valuable nonlinear features. The algorithm's efficiency is evaluated in a spatial realm with stochastic, high-dimensional observations. The algorithm surpasses non-adaptive baseline architectures and approaches the performance of an architecture supplied with side-channel information. These outcomes represent a stride towards scalable RL algorithms for more comprehensive issues where the observation structure is unknown.",1
"State abstraction has been an essential tool for dramatically improving the sample efficiency of reinforcement-learning algorithms. Indeed, by exposing and accentuating various types of latent structure within the environment, different classes of state abstraction have enabled improved theoretical guarantees and empirical performance. When dealing with state abstractions that capture structure in the value function, however, a standard assumption is that the true abstraction has been supplied or unrealistically computed a priori, leaving open the question of how to efficiently uncover such latent structure while jointly seeking out optimal behavior. Taking inspiration from the bandit literature, we propose that an agent seeking out latent task structure must explicitly represent and maintain its uncertainty over that structure as part of its overall uncertainty about the environment. We introduce a practical algorithm for doing this using two posterior distributions over state abstractions and abstract-state values. In empirically validating our approach, we find that substantial performance gains lie in the multi-task setting where tasks share a common, low-dimensional representation.",0
"The use of state abstraction has significantly enhanced the efficiency of reinforcement-learning algorithms. By identifying and highlighting different hidden patterns within the environment, various types of state abstraction have improved both theoretical guarantees and practical performance. However, when dealing with state abstractions that capture the value function's structure, it is commonly assumed that the true abstraction has been provided or computed beforehand, which raises the question of how to efficiently discover such hidden patterns while seeking optimal behavior. To solve this problem, we suggest that agents seeking latent task structure should explicitly represent and maintain their uncertainty regarding that structure, as part of their overall uncertainty about the environment, taking inspiration from the bandit literature. We propose a practical algorithm using two posterior distributions over state abstractions and abstract-state values. Our empirical results demonstrate that this approach achieves significant performance gains in the multi-task setting, where tasks share a common, low-dimensional representation.",1
"Generalization has been a long-standing challenge for reinforcement learning (RL). Visual RL, in particular, can be easily distracted by irrelevant factors in high-dimensional observation space. In this work, we consider robust policy learning which targets zero-shot generalization to unseen visual environments with large distributional shift. We propose SECANT, a novel self-expert cloning technique that leverages image augmentation in two stages to decouple robust representation learning from policy optimization. Specifically, an expert policy is first trained by RL from scratch with weak augmentations. A student network then learns to mimic the expert policy by supervised learning with strong augmentations, making its representation more robust against visual variations compared to the expert. Extensive experiments demonstrate that SECANT significantly advances the state of the art in zero-shot generalization across 4 challenging domains. Our average reward improvements over prior SOTAs are: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based autonomous driving (+47.7%), and indoor object navigation (+15.8%). Code release and video are available at https://linxifan.github.io/secant-site/.",0
"Reinforcement learning (RL) has struggled with the issue of generalization for a long time, particularly in the realm of visual RL, where irrelevant factors in a high-dimensional observation space can easily cause distractions. This study focuses on creating a robust policy learning approach, which aims to achieve zero-shot generalization in new visual environments with significant distributional shifts. To achieve this goal, the authors introduce SECANT, a new technique that leverages image augmentation in two stages to separate robust representation learning from policy optimization. First, an expert policy is trained by RL from scratch with weak augmentations. Then, a student network is trained to mimic the expert policy by using strong augmentations in supervised learning, which makes its representation more robust against visual variations compared to the expert. The experiments conducted demonstrate significant advancements in the state of the art in zero-shot generalization across four difficult domains, with average reward improvements over prior SOTAs in DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based autonomous driving (+47.7%), and indoor object navigation (+15.8%). The code and video are available at https://linxifan.github.io/secant-site/.",1
"World models improve a learning agent's ability to efficiently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is filled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent's actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology significantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions.",0
"Enhancing a learning agent's efficacy in interactive and situated environments is achieved through the utilization of world models. This research concentrates on constructing world models for text-based game environments, which are reinforcement learning environments where agents interact with the world through textual natural language. These environments consist of complex puzzles or quests within a world filled with numerous characters, locations, and objects. Our world model has the capability to predict changes in the world caused by an agent's actions while representing the world as a knowledge graph, and to generate contextually relevant natural language actions required to operate in the world. To achieve this, we introduce a transformer-based multi-task architecture and a loss function, and frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions. Our methodology significantly outperforms existing textual world modeling techniques, as demonstrated through a zero-shot ablation study on never-before-seen textual worlds, and the importance of each of our contributions is highlighted.",1
"Adapting the idea of training CartPole with Deep Q-learning agent, we are able to find a promising result that prevent the pole from falling down. The capacity of reinforcement learning (RL) to learn from the interaction between the environment and agent provides an optimal control strategy. In this paper, we aim to solve the classic pendulum swing-up problem that making the learned pendulum to be in upright position and balanced. Deep Deterministic Policy Gradient algorithm is introduced to operate over continuous action domain in this problem. Salient results of optimal pendulum are proved with increasing average return, decreasing loss, and live video in the code part.",0
"By incorporating the concept of utilizing a Deep Q-learning agent to train CartPole, we have obtained a promising outcome in preventing the pole from collapsing. The ability of reinforcement learning (RL) to acquire an ideal control strategy by studying the interaction between the agent and the environment is noteworthy. The objective of this study is to address the classic issue of the pendulum swing-up problem by ensuring that the pendulum learned is upright and balanced. To tackle this problem, we have implemented the Deep Deterministic Policy Gradient algorithm that can handle continuous action domains. The results demonstrate optimal pendulum performance, which is evident from an increase in average return, a decrease in loss, and live video in the code part.",1
"This work presents a fully data-driven, black-box pipeline to obtain an optimal control policy for a multi-loop building control problem based on historical building and weather data, thus without the need for complex physics-based modelling. We demonstrate the method for joint control of room temperature and bidirectional EV charging to maximize the occupant thermal comfort and energy savings while leaving enough energy in the EV battery for the next trip. We modelled the room temperature with a recurrent neural network and EV charging with a piece-wise linear function. Using these models as a simulation environment, we applied a deep reinforcement learning (DRL) algorithm to obtain an optimal control policy. The learnt policy achieves on average 17% energy savings over the heating season and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is additionally connected and a two-tariff electricity pricing is applied, the MIMO DRL policy successfully leverages the battery and decreases the overall cost of electricity compared to two standard RB controllers, one controlling the room temperature and another controlling the bidirectional EV (dis-)charging. Finally, we demonstrate a successful transfer of the learnt DRL policy from simulation onto a real building, the DFAB HOUSE at Empa Duebendorf in Switzerland, achieving up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller over three weeks during the heating season.",0
"This study introduces a completely data-driven, opaque process for obtaining an optimal control strategy for a multi-loop building control issue. This is accomplished by relying on past building and weather data, without the need for complicated physics-based models. The study showcases the technique by jointly managing room temperature and bidirectional EV charging to increase occupant thermal comfort and energy savings, while keeping enough energy in the EV battery for the next trip. To accomplish this, a recurrent neural network modeled the room temperature, and a piece-wise linear function modeled EV charging. Using these models as a simulation environment, a deep reinforcement learning (DRL) algorithm was utilized to determine an optimal control strategy. The learned strategy resulted in an average of 17% energy savings over the heating season, and 19% better satisfaction with comfort compared to a typical RB controller. With the inclusion of a bidirectional EV and a two-tariff electricity pricing system, the MIMO DRL strategy effectively leveraged the battery and reduced overall electricity costs when compared to two standard RB controllers. Finally, the study demonstrated the DRL strategy's successful transfer from simulation to a real building, the DFAB HOUSE at Empa Duebendorf in Switzerland, achieving up to 30% energy savings while maintaining comparable comfort levels to a conventional RB room temperature controller for three weeks during the heating season.",1
"Reinforcement Learning has applications in field of mechatronics, robotics, and other resource-constrained control system. Problem of resource allocation is primarily solved using traditional predefined techniques and modern deep learning methods. The drawback of predefined and most deep learning methods for resource allocation is failing to meet the requirements in cases of uncertain system environment. We can approach problem of resource allocation in uncertain system environment alongside following certain criteria using deep reinforcement learning. Also, reinforcement learning has ability for adapting to new uncertain environment for prolonged period of time. The paper provides a detailed comparative analysis on various deep reinforcement learning methods by applying different components to modify architecture of reinforcement learning with use of noisy layers, prioritized replay, bagging, duelling networks, and other related combination to obtain improvement in terms of performance and reduction of computational cost. The paper identifies problem of resource allocation in uncertain environment could be effectively solved using Noisy Bagging duelling double deep Q network achieving efficiency of 97.7% by maximizing reward with significant exploration in given simulated environment for resource allocation.",0
"Reinforcement Learning can be applied in mechatronics, robotics, and other control systems with limited resources. The problem of resource allocation is commonly addressed through traditional predefined techniques and modern deep learning methods. However, these methods may not meet the requirements in uncertain system environments. To tackle this issue, deep reinforcement learning can be used to establish specific criteria for resource allocation in uncertain environments. Additionally, reinforcement learning can adapt to new and unfamiliar environments over a prolonged period. This paper compares various deep reinforcement learning methods by modifying the architecture of reinforcement learning with noisy layers, prioritized replay, bagging, duelling networks, and other related combinations to improve performance and reduce computational costs. The study finds that the Noisy Bagging duelling double deep Q network is effective in solving the problem of resource allocation in uncertain environments, achieving an efficiency of 97.7% by maximizing rewards through significant exploration in a simulated environment.",1
"We present the first framework of Certifying Robust Policies for reinforcement learning (CROP) against adversarial state perturbations. We propose two particular types of robustness certification criteria: robustness of per-state actions and lower bound of cumulative rewards. Specifically, we develop a local smoothing algorithm which uses a policy derived from Q-functions smoothed with Gaussian noise over each encountered state to guarantee the robustness of actions taken along this trajectory. Next, we develop a global smoothing algorithm for certifying the robustness of a finite-horizon cumulative reward under adversarial state perturbations. Finally, we propose a local smoothing approach which makes use of adaptive search in order to obtain tight certification bounds for reward. We use the proposed RL robustness certification framework to evaluate six methods that have previously been shown to yield empirically robust RL, including adversarial training and several forms of regularization, on two representative Atari games. We show that RegPGD, RegCVX, and RadialRL achieve high certified robustness among these. Furthermore, we demonstrate that our certifications are often tight by evaluating these algorithms against adversarial attacks.",0
"Our paper introduces the Certifying Robust Policies for reinforcement learning (CROP) framework, which aims to defend against adversarial state perturbations. To achieve this goal, we propose two types of certification criteria: per-state action robustness and a lower bound on cumulative rewards. We developed a local smoothing algorithm that uses a policy derived from smoothed Q-functions to ensure robustness of actions taken along a trajectory. Additionally, we designed a global smoothing algorithm to certify the robustness of a finite-horizon cumulative reward under adversarial state perturbations. Our local smoothing approach uses adaptive search to obtain tight certification bounds for reward. We evaluated six previously proposed robust RL methods, including adversarial training and regularization, on two Atari games using our framework and found that RegPGD, RegCVX, and RadialRL achieved high certified robustness. We also verified the effectiveness of our certifications by testing these algorithms against adversarial attacks.",1
"We propose a novel approach to interactive theorem-proving (ITP) using deep reinforcement learning. The proposed framework is able to learn proof search strategies as well as tactic and arguments prediction in an end-to-end manner. We formulate the process of ITP as a Markov decision process (MDP) in which each state represents a set of potential derivation paths. This structure allows us to introduce a novel backtracking mechanism which enables the agent to efficiently discard (predicted) dead-end derivations and restart from promising alternatives. We implement the framework in the HOL4 theorem prover. Experimental results show that the framework outperforms existing automated theorem provers (i.e., hammers) available in HOL4 when evaluated on unseen problems. We further elaborate the role of key components of the framework using ablation studies.",0
"Our proposed method for interactive theorem-proving (ITP) involves utilizing deep reinforcement learning to learn proof search strategies, tactic prediction, and argument prediction in a comprehensive manner. We approach ITP as a Markov decision process (MDP) where each state represents possible derivation paths. This structure allows us to incorporate a new backtracking mechanism that efficiently discards predicted dead-end derivations and restarts from more promising alternatives. We have implemented this framework in the HOL4 theorem prover, and our experimental results demonstrate superior performance compared to existing automated theorem provers (such as hammers) when evaluated on new problems. Additionally, we conduct ablation studies to further elaborate on the key components of our proposed framework.",1
"We make progress in a long-standing problem of batch reinforcement learning (RL): learning $Q^\star$ from an exploratory and polynomial-sized dataset, using a realizable and otherwise arbitrary function class. In fact, all existing algorithms demand function-approximation assumptions stronger than realizability, and the mounting negative evidence has led to a conjecture that sample-efficient learning is impossible in this setting (Chen and Jiang, 2019). Our algorithm, BVFT, breaks the hardness conjecture (albeit under a stronger notion of exploratory data) via a tournament procedure that reduces the learning problem to pairwise comparison, and solves the latter with the help of a state-action partition constructed from the compared functions. We also discuss how BVFT can be applied to model selection among other extensions and open problems.",0
"Our team has made progress in addressing a persistent issue in batch reinforcement learning (RL). Specifically, we have developed a method, called BVFT, that can learn $Q^\star$ from a polynomial-sized dataset that is both exploratory and arbitrary. This is notable because existing algorithms have required stronger assumptions about function approximation than are realistic, leading some to believe that efficient learning in this context is impossible. Although BVFT requires a more extensive exploration of the data, it breaks through this perceived barrier by using a tournament approach to simplify the pairwise comparison problem and construct a state-action partition based on the compared functions. We also explore the potential applications of BVFT to other open problems and model selection.",1
"This paper proposes a framework for the interactive video object segmentation (VOS) in the wild where users can choose some frames for annotations iteratively. Then, based on the user annotations, a segmentation algorithm refines the masks. The previous interactive VOS paradigm selects the frame with some worst evaluation metric, and the ground truth is required for calculating the evaluation metric, which is impractical in the testing phase. In contrast, in this paper, we advocate that the frame with the worst evaluation metric may not be exactly the most valuable frame that leads to the most performance improvement across the video. Thus, we formulate the frame selection problem in the interactive VOS as a Markov Decision Process, where an agent is learned to recommend the frame under a deep reinforcement learning framework. The learned agent can automatically determine the most valuable frame, making the interactive setting more practical in the wild. Experimental results on the public datasets show the effectiveness of our learned agent without any changes to the underlying VOS algorithms. Our data, code, and models are available at https://github.com/svip-lab/IVOS-W.",0
"This article presents a novel approach to interactive video object segmentation (VOS) in unpredictable environments. The proposed framework allows users to iteratively choose frames for annotation, which are then refined by a segmentation algorithm. Unlike previous interactive VOS methods that select frames based on evaluation metrics, which require ground truth, our approach formulates the frame selection process as a Markov Decision Process. This allows an agent to learn and recommend the most valuable frames for annotation, making the interactive setting more practical. Our experimental results demonstrate the effectiveness of our learned agent, which can be used without modifying the underlying VOS algorithms. Interested parties can access our data, code, and models at https://github.com/svip-lab/IVOS-W.",1
"State-of-the-art reinforcement learning (RL) algorithms suffer from high sample complexity, particularly in the sparse reward case. A popular strategy for mitigating this problem is to learn control policies by imitating a set of expert demonstrations. The drawback of such approaches is that an expert needs to produce demonstrations, which may be costly in practice. To address this shortcoming, we propose Probabilistic Planning for Demonstration Discovery (P2D2), a technique for automatically discovering demonstrations without access to an expert. We formulate discovering demonstrations as a search problem and leverage widely-used planning algorithms such as Rapidly-exploring Random Tree to find demonstration trajectories. These demonstrations are used to initialize a policy, then refined by a generic RL algorithm. We provide theoretical guarantees of P2D2 finding successful trajectories, as well as bounds for its sampling complexity. We experimentally demonstrate the method outperforms classic and intrinsic exploration RL techniques in a range of classic control and robotics tasks, requiring only a fraction of exploration samples and achieving better asymptotic performance.",0
"Reinforcement learning (RL) algorithms that are considered the most advanced suffer from a high sample complexity, especially when the reward is sparse. One strategy to address this issue is to learn control policies by imitating expert demonstrations. However, this approach has a drawback as it requires the presence of an expert, which can be costly. To overcome this limitation, we introduce the Probabilistic Planning for Demonstration Discovery (P2D2) technique, which enables automatic discovery of demonstrations without the need for an expert. We formulate the demonstration discovery as a search problem and employ widely-used planning algorithms, such as Rapidly-exploring Random Tree, to find demonstration trajectories. The discovered demonstrations are then used to initialize a policy, which is further refined by a generic RL algorithm. The P2D2 technique comes with theoretical guarantees of finding successful trajectories and has bounds for its sampling complexity. In our experiments, we demonstrate that P2D2 outperforms classic and intrinsic exploration RL techniques in various control and robotics tasks, while requiring only a fraction of exploration samples and achieving better asymptotic performance.",1
"The rapidly evolving field of Artificial Intelligence necessitates automated approaches to co-design neural network architecture and neural accelerators to maximize system efficiency and address productivity challenges. To enable joint optimization of this vast space, there has been growing interest in differentiable NN-HW co-design. Fully differentiable co-design has reduced the resource requirements for discovering optimized NN-HW configurations, but fail to adapt to general hardware accelerator search spaces. This is due to the existence of non-synthesizable (invalid) designs in the search space of many hardware accelerators. To enable efficient and realizable co-design of configurable hardware accelerators with arbitrary neural network search spaces, we introduce RHNAS. RHNAS is a method that combines reinforcement learning for hardware optimization with differentiable neural architecture search. RHNAS discovers realizable NN-HW designs with 1.84x lower latency and 1.86x lower energy-delay product (EDP) on ImageNet and 2.81x lower latency and 3.30x lower EDP on CIFAR-10 over the default hardware accelerator design.",0
"With the rapidly advancing Artificial Intelligence field, it is necessary to use automated techniques to co-design neural network architecture and neural accelerators to maximize system efficiency and address productivity issues. Differentiable NN-HW co-design has gained interest to enable joint optimization of this vast space. However, fully differentiable co-design has reduced resource requirements for optimized NN-HW configurations but is not able to adapt to general hardware accelerator search spaces due to non-synthesizable designs. To overcome this, we present RHNAS, a method combining reinforcement learning for hardware optimization with differentiable neural architecture search. RHNAS discovers realizable NN-HW designs with lower latency and energy-delay product on ImageNet and CIFAR-10 compared to the default hardware accelerator design.",1
"With the increasing demand to efficiently deploy DNNs on mobile edge devices, it becomes much more important to reduce unnecessary computation and increase the execution speed. Prior methods towards this goal, including model compression and network architecture search (NAS), are largely performed independently and do not fully consider compiler-level optimizations which is a must-do for mobile acceleration. In this work, we first propose (i) a general category of fine-grained structured pruning applicable to various DNN layers, and (ii) a comprehensive, compiler automatic code generation framework supporting different DNNs and different pruning schemes, which bridge the gap of model compression and NAS. We further propose NPAS, a compiler-aware unified network pruning, and architecture search. To deal with large search space, we propose a meta-modeling procedure based on reinforcement learning with fast evaluation and Bayesian optimization, ensuring the total number of training epochs comparable with representative NAS frameworks. Our framework achieves 6.7ms, 5.9ms, 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3 level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an off-the-shelf mobile phone, consistently outperforming prior work.",0
"As the demand for efficient deployment of DNNs on mobile edge devices increases, it is crucial to minimize unnecessary computation and enhance execution speed. Previous methods, such as model compression and network architecture search (NAS), have been done independently and without considering compiler-level optimizations necessary for mobile acceleration. In this study, we propose a general category of fine-grained structured pruning that can be applied to various DNN layers, along with a comprehensive, compiler automatic code generation framework that supports different DNNs and pruning schemes. Our proposed framework, NPAS, combines network pruning and architecture search, and is compiler-aware. To address the large search space, we introduce a meta-modeling procedure based on reinforcement learning and Bayesian optimization. Our framework achieves faster ImageNet inference times and higher accuracy on an off-the-shelf mobile phone compared to prior work.",1
"Analyzing the worst-case performance of deep neural networks against input perturbations amounts to solving a large-scale non-convex optimization problem, for which several past works have proposed convex relaxations as a promising alternative. However, even for reasonably-sized neural networks, these relaxations are not tractable, and so must be replaced by even weaker relaxations in practice. In this work, we propose a novel operator splitting method that can directly solve a convex relaxation of the problem to high accuracy, by splitting it into smaller sub-problems that often have analytical solutions. The method is modular and scales to problem instances that were previously impossible to solve exactly due to their size. Furthermore, the solver operations are amenable to fast parallelization with GPU acceleration. We demonstrate our method in obtaining tighter bounds on the worst-case performance of large convolutional networks in image classification and reinforcement learning settings.",0
"To evaluate the deep neural networks' worst-case performance against input perturbations, we need to solve a large-scale non-convex optimization problem. In the past, researchers have suggested convex relaxations as an alternative solution, but these relaxations are not feasible for reasonably-sized neural networks, and weaker relaxations must be used instead. Our study introduces a new operator splitting method that can accurately solve a convex relaxation of the problem by breaking it down into smaller sub-problems that have analytical solutions. Our approach is modular and can tackle problem instances that were previously unsolvable due to their size. Additionally, our solver operations can be parallelized rapidly with GPU acceleration. We showcase our method by obtaining tighter bounds on the worst-case performance of large convolutional networks in image classification and reinforcement learning scenarios.",1
"This paper develops an unified framework to study finite-sample convergence guarantees of a large class of value-based asynchronous reinforcement learning (RL) algorithms. We do this by first reformulating the RL algorithms as \textit{Markovian Stochastic Approximation} (SA) algorithms to solve fixed-point equations. We then develop a Lyapunov analysis and derive mean-square error bounds on the convergence of the Markovian SA. Based on this result, we establish finite-sample mean-square convergence bounds for asynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(\lambda)$, and off-policy TD algorithms including V-trace. As a by-product, by analyzing the convergence bounds of $n$-step TD and TD$(\lambda)$, we provide theoretical insights into the bias-variance trade-off, i.e., efficiency of bootstrapping in RL. This was first posed as an open problem in (Sutton, 1999).",0
"The aim of this paper is to create a unified framework that can be used to examine the convergence guarantees of a wide range of value-based asynchronous reinforcement learning (RL) algorithms when dealing with finite samples. To achieve this, we transform the RL algorithms into Markovian Stochastic Approximation (SA) algorithms. This allows us to solve fixed-point equations and conduct a Lyapunov analysis, from which we can derive mean-square error bounds relating to the convergence of the Markovian SA. Using these findings, we establish finite-sample mean-square convergence bounds for various asynchronous RL algorithms, including Q-learning, n-step TD, TD(lambda), and off-policy TD algorithms like V-trace. Additionally, our analysis of the convergence bounds of n-step TD and TD(lambda) provides insights into the bias-variance trade-off and the efficacy of bootstrapping in RL. This addresses an open problem first presented in (Sutton, 1999).",1
"To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that ""adapts"" to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.",0
"When humans are learning a new skill, it is beneficial for a tutor to inform them about the amount of attention they should give to certain content or practice problems based on their current knowledge level. Similarly, machine learning models could benefit from an adaptive scorer that estimates the importance of each training data instance based on the model's current learning state. However, training such a scorer efficiently is a complex problem, as it typically requires the completion of the entire training process to precisely quantify the effect of a data instance. To address this issue, we propose a reinforcement learning approach called Differentiable Data Selection (DDS), which formulates a scorer network as a learnable function of the training data. DDS updates the scorer with an intuitive reward signal that up-weighs data with a similar gradient to a dev set that the model should ultimately perform well on. DDS delivers strong and consistent improvements without significant computing overhead, as demonstrated by its performance on two different tasks: machine translation and image classification.",1
"Reward shaping (RS) is a powerful method in reinforcement learning (RL) for overcoming the problem of sparse or uninformative rewards. However, RS typically relies on manually engineered shaping-reward functions whose construction is time-consuming and error-prone. It also requires domain knowledge which runs contrary to the goal of autonomous learning. We introduce Reinforcement Learning Optimal Shaping Algorithm (ROSA), an automated RS framework in which the shaping-reward function is constructed in a novel Markov game between two agents. A reward-shaping agent (Shaper) uses switching controls to determine which states to add shaping rewards and their optimal values while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. We prove that ROSA, which easily adopts existing RL algorithms, learns to construct a shaping-reward function that is tailored to the task thus ensuring efficient convergence to high performance policies. We demonstrate ROSA's congenial properties in three carefully designed experiments and show its superior performance against state-of-the-art RS algorithms in challenging sparse reward environments.",0
"In reinforcement learning, Reward shaping (RS) is a useful technique for dealing with the issue of insufficient or unhelpful rewards. However, the manual creation of shaping-reward functions, which RS usually relies on, is both time-consuming and error-prone. Additionally, domain knowledge is required, which goes against the objective of self-directed learning. We propose the Reinforcement Learning Optimal Shaping Algorithm (ROSA), an automated RS framework that creates the shaping-reward function through a unique Markov game involving two agents. The reward-shaping agent (Shaper) determines which states to add shaping rewards and their optimal values using switching controls, while the other agent (Controller) learns the optimal policy for the task using these shaped rewards. We demonstrate that ROSA, which can easily incorporate existing RL algorithms, learns to construct a shaping-reward function that is specific to the task, ensuring efficient convergence to high-performance policies. We illustrate ROSA's favorable characteristics in three well-designed experiments and demonstrate its superior performance compared to state-of-the-art RS algorithms in challenging, sparse reward environments.",1
"A key limitation in using various modern methods of machine learning in developing feedback control policies is the lack of appropriate methodologies to analyze their long-term dynamics, in terms of making any sort of guarantees (even statistically) about robustness. The central reasons for this are largely due to the so-called curse of dimensionality, combined with the black-box nature of the resulting control policies themselves. This paper aims at the first of these issues. Although the full state space of a system may be quite large in dimensionality, it is a common feature of most model-based control methods that the resulting closed-loop systems demonstrate dominant dynamics that are rapidly driven to some lower-dimensional sub-space within. In this work we argue that the dimensionality of this subspace is captured by tools from fractal geometry, namely various notions of a fractional dimension. We then show that the dimensionality of trajectories induced by model free reinforcement learning agents can be influenced adding a post processing function to the agents reward signal. We verify that the dimensionality reduction is robust to noise being added to the system and show that that the modified agents are more actually more robust to noise and push disturbances in general for the systems we examined.",0
"The absence of proper methodologies for analyzing the long-term dynamics of machine learning techniques poses a significant constraint in developing effective feedback control policies. Consequently, guarantees for robustness, even statistically, are challenging to make. This issue is mainly due to the curse of dimensionality, combined with the black-box nature of the resulting control policies. This study aims to address the first limitation by exploiting the common feature of most model-based control methods, where the dominant dynamics of the closed-loop systems are rapidly driven to a lower-dimensional sub-space. The subspace's dimensionality is captured using fractal geometry concepts, such as fractional dimension. This study demonstrates that the trajectory dimensionality induced by model-free reinforcement learning agents can be influenced by a post-processing function added to the agent's reward signal. The study verifies that the dimensionality reduction is resilient to noise and disturbances added to the system, and the modified agents are more robust to noise and push disturbances for the examined systems.",1
"In multi-goal reinforcement learning (RL) settings, the reward for each goal is sparse, and located in a small neighborhood of the goal. In large dimension, the probability of reaching a reward vanishes and the agent receives little learning signal. Methods such as Hindsight Experience Replay (HER) tackle this issue by also learning from realized but unplanned-for goals. But HER is known to introduce bias, and can converge to low-return policies by overestimating chancy outcomes. First, we vindicate HER by proving that it is actually unbiased in deterministic environments, such as many optimal control settings. Next, for stochastic environments in continuous spaces, we tackle sparse rewards by directly taking the infinitely sparse reward limit. We fully formalize the problem of multi-goal RL with infinitely sparse Dirac rewards at each goal. We introduce unbiased deep Q-learning and actor-critic algorithms that can handle such infinitely sparse rewards, and test them in toy environments.",0
"Sparse rewards in multi-goal reinforcement learning (RL) settings are often confined to a small area around the goal, making it difficult for agents to receive much learning signal in high-dimensional environments. Hindsight Experience Replay (HER) addresses this problem by allowing agents to learn from unplanned goals, but it can introduce bias and converge to low-return policies. Our study shows that HER is unbiased in deterministic settings, such as optimal control. For stochastic environments with sparse rewards, we tackle this issue by taking the infinitely sparse reward limit and introduce unbiased deep Q-learning and actor-critic algorithms. We formalize the problem of multi-goal RL with infinitely sparse Dirac rewards and test these algorithms in toy environments.",1
"Reinforcement learning is made much more complex when the agent's observation is partial or noisy. This case corresponds to a partially observable Markov decision process (POMDP). One strategy to seek good performance in POMDPs is to endow the agent with a finite memory, whose update is governed by the policy. However, policy optimization is non-convex in that case and can lead to poor training performance for random initialization. The performance can be empirically improved by constraining the memory architecture, then sacrificing optimality to facilitate training. Here we study this trade-off in the two-arm bandit problem, and compare two extreme cases: (i) the random access memory where any transitions between $M$ memory states are allowed and (ii) a fixed memory where the agent can access its last $m$ actions and rewards. For (i), the probability $q$ to play the worst arm is known to be exponentially small in $M$ for the optimal policy. Our main result is to show that similar performance can be reached for (ii) as well, despite the simplicity of the memory architecture: using a conjecture on Gray-ordered binary necklaces, we find policies for which $q$ is exponentially small in $2^m$ i.e. $q\sim\alpha^{2^m}$ for some $\alpha < 1$. Interestingly, we observe empirically that training from random initialization leads to very poor results for (i), and significantly better results for (ii).",0
"When an agent's observation is incomplete or unclear, it makes reinforcement learning much more difficult. This situation is referred to as a partially observable Markov decision process (POMDP). To improve the agent's performance in POMDPs, one approach is to provide the agent with a finite memory that is updated by the policy. However, optimizing the policy in this situation is non-convex and may result in poor training performance when initialized randomly. To address this, one can constrain the memory architecture and sacrifice optimality to facilitate training. In this study, we investigate this trade-off in the two-arm bandit problem and compare the performance of two extreme cases: (i) random access memory with transitions between M memory states, and (ii) fixed memory that allows access to the agent's last m actions and rewards. For (i), the probability of playing the worst arm, q, is known to be exponentially small in M for the optimal policy. Our main finding is that similar performance can be achieved for (ii) despite the simplicity of the memory architecture. We discover policies using a conjecture on Gray-ordered binary necklaces, which result in q being exponentially small in 2^m, i.e., q ~ α^(2^m) for some α < 1. Interestingly, we observe that training from random initialization leads to poor outcomes for (i) but significantly better results for (ii).",1
Episodic memory lets reinforcement learning algorithms remember and exploit promising experience from the past to improve agent performance. Previous works on memory mechanisms show benefits of using episodic-based data structures for discrete action problems in terms of sample-efficiency. The application of episodic memory for continuous control with a large action space is not trivial. Our study aims to answer the question: can episodic memory be used to improve agent's performance in continuous control? Our proposed algorithm combines episodic memory with Actor-Critic architecture by modifying critic's objective. We further improve performance by introducing episodic-based replay buffer prioritization. We evaluate our algorithm on OpenAI gym domains and show greater sample-efficiency compared with the state-of-the art model-free off-policy algorithms.,0
"Reinforcement learning algorithms can enhance their performance by utilizing episodic memory, which enables them to recollect and utilize valuable past experiences. Prior research has demonstrated that the use of episodic-based data structures can increase the efficiency of discrete action problems. However, applying episodic memory to continuous control with a vast action space is challenging. Our study aims to determine if episodic memory can enhance agent performance in continuous control. To achieve this goal, we have developed an algorithm that integrates episodic memory with the Actor-Critic architecture by modifying the critic's objective. We have also introduced episodic-based replay buffer prioritization to further improve performance. Our algorithm has been evaluated on OpenAI gym domains, and our results indicate that our model-free off-policy algorithms are more sample-efficient than the current state-of-the-art.",1
"We study learning algorithms for the classical Markovian bandit problem with discount. We explain how to adapt PSRL [24] and UCRL2 [2] to exploit the problem structure. These variants are called MB-PSRL and MB-UCRL2. While the regret bound and runtime of vanilla implementations of PSRL and UCRL2 are exponential in the number of bandits, we show that the episodic regret of MB-PSRL and MB-UCRL2 is $\tilde O(S\sqrt{nK})$ where $K$ is the number of episodes, n is the number of bandits and S is the number of states of each bandit (the exact bound in $S$, $n$ and $K$ is given in the paper). Up to a factor $\sqrt S$, this matches the lower bound of $\Omega(\sqrt{SnK}$) that we also derive in the paper. MB-PSRL is also computationally efficient: its runtime is linear in the number of bandits. We further show that this linear runtime cannot be achieved by adapting classical non-Bayesian algorithms such as UCRL2 or UCBVI to Markovian bandit problems. Finally, we perform numerical experiments that confirm that MB-PSRL outperforms other existing algorithms in practice, both in terms of regret and of computation time.",0
"Our focus is on learning algorithms for the Markovian bandit problem with discount. We examine how PSRL and UCRL2 can be adjusted to suit the problem structure, resulting in the development of MB-PSRL and MB-UCRL2. The standard implementations of PSRL and UCRL2 are known to be exponentially regretful and time-consuming given the number of bandits. However, our research demonstrates that MB-PSRL and MB-UCRL2 achieve an episodic regret of $\tilde O(S\sqrt{nK})$, where $K$ is the number of episodes, $n$ is the number of bandits, and $S$ is the number of states of each bandit. This bound is in line with the lower bound of $\Omega(\sqrt{SnK}$) we derived in our paper, except for a factor of $\sqrt S$. Furthermore, MB-PSRL is computationally efficient, with a runtime that scales linearly with the number of bandits. We also show that traditional non-Bayesian algorithms such as UCRL2 or UCBVI cannot be adapted to Markovian bandit problems in a way that achieves linear runtime. Finally, our numerical experiments support the superiority of MB-PSRL over other existing algorithms in terms of both regret and computation time.",1
"Recent work has discovered that deep reinforcement learning (DRL) policies are vulnerable to adversarial examples. These attacks mislead the policy of DRL agents by perturbing the state of the environment observed by agents. They are feasible in principle but too slow to fool DRL policies in real time. We propose a new attack to fool DRL policies that is both effective and efficient enough to be mounted in real time. We utilize the Universal Adversarial Perturbation (UAP) method to compute effective perturbations independent of the individual inputs to which they are applied. Via an extensive evaluation using Atari 2600 games, we show that our technique is effective, as it fully degrades the performance of both deterministic and stochastic policies (up to 100%, even when the $l_\infty$ bound on the perturbation is as small as 0.005). We also show that our attack is efficient, incurring an online computational cost of 0.027ms on average. It is faster compared to the response time (0.6ms on average) of agents with different DRL policies, and considerably faster than prior attacks (2.7ms on average). Furthermore, we demonstrate that known defenses are ineffective against universal perturbations. We propose an effective detection technique which can form the basis for robust defenses against attacks based on universal perturbations.",0
"Adversarial examples have been found to deceive deep reinforcement learning (DRL) policies. These attacks disrupt the DRL agent's policy by modifying the observed environment state. While feasible in theory, they are typically too slow to fool DRL policies in real time. Our proposed attack, however, is both effective and efficient enough to be implemented in real time. Using the Universal Adversarial Perturbation (UAP) method, we compute perturbations that are effective and independent of individual inputs. Our evaluation using Atari 2600 games demonstrates that our technique is effective, fully degrading the performance of deterministic and stochastic policies up to 100%, even with small perturbations (l-infinity bound of 0.005). Our attack is also efficient, with an online computational cost of 0.027ms on average, faster than the response time (0.6ms on average) of agents with different DRL policies and prior attacks (2.7ms on average). We also demonstrate the ineffectiveness of known defenses against universal perturbations and propose a detection technique for robust defenses against such attacks.",1
"Sub-optimal control policies in intersection traffic signal controllers (TSC) contribute to congestion and lead to negative effects on human health and the environment. Reinforcement learning (RL) for traffic signal control is a promising approach to design better control policies and has attracted considerable research interest in recent years. However, most work done in this area used simplified simulation environments of traffic scenarios to train RL-based TSC. To deploy RL in real-world traffic systems, the gap between simplified simulation environments and real-world applications has to be closed. Therefore, we propose LemgoRL, a benchmark tool to train RL agents as TSC in a realistic simulation environment of Lemgo, a medium-sized town in Germany. In addition to the realistic simulation model, LemgoRL encompasses a traffic signal logic unit that ensures compliance with all regulatory and safety requirements. LemgoRL offers the same interface as the well-known OpenAI gym toolkit to enable easy deployment in existing research work. Our benchmark tool drives the development of RL algorithms towards real-world applications. We provide LemgoRL as an open-source tool at https://github.com/rl-ina/lemgorl.",0
"The use of inadequate control policies in intersection traffic signal controllers (TSC) causes traffic congestion and has adverse effects on both the environment and human health. Reinforcement learning (RL) has emerged as a promising approach to develop better control policies for TSC and has generated significant research interest. However, past studies in this area have mainly used simplified simulation environments to train RL-based TSC, which limits their applicability in real-world traffic systems. To bridge this gap, we propose LemgoRL, a benchmark tool that employs a realistic simulation environment of Lemgo, a medium-sized German town, to train RL agents as TSC. In addition to the realistic simulation model, LemgoRL includes a traffic signal logic unit that ensures compliance with all safety and regulatory requirements. Like the OpenAI gym toolkit, LemgoRL has the same interface, making it simple to integrate into existing research work. Our benchmark tool paves the way for RL algorithms to be used in real-world scenarios. We have provided LemgoRL as an open-source tool, which can be accessed at https://github.com/rl-ina/lemgorl.",1
"The training of deep learning models poses vast challenges of including parameter tuning and ordering of training data. Significant research has been done in Curriculum learning for optimizing the sequence of training data. Recent works have focused on using complex reinforcement learning techniques to find the optimal data ordering strategy to maximize learning for a given network. In this paper, we present a simple and efficient technique based on continuous optimization. We call this new approach Training Sequence Optimization (TSO). There are three critical components in our proposed approach: (a) An encoder network maps/embeds training sequence into continuous space. (b) A predictor network uses the continuous representation of a strategy as input and predicts the accuracy for fixed network architecture. (c) A decoder further maps a continuous representation of a strategy to the ordered training dataset. The performance predictor and encoder enable us to perform gradient-based optimization in the continuous space to find the embedding of optimal training data ordering with potentially better accuracy. Experiments show that we can gain 2AP with our generated optimal curriculum strategy over the random strategy using the CIFAR-100 dataset and have better boosts than the state of the art CL algorithms. We do an ablation study varying the architecture, dataset and sample sizes showcasing our approach's robustness.",0
"There are numerous obstacles to overcome when training deep learning models, such as determining the appropriate parameters and organizing the order of training data. Curriculum learning has been extensively researched to optimize the sequence in which the data is presented. Recent studies have focused on using intricate reinforcement learning methods to discover the optimal data ordering strategy that maximizes learning for a specific network. In this article, we introduce a straightforward and efficient technique called Training Sequence Optimization (TSO), which is based on continuous optimization. Our proposed approach consists of three crucial components: (a) an encoder network that maps/embeds the training sequence into a continuous space, (b) a predictor network that uses the continuous representation of a strategy as input and predicts the accuracy for a fixed network architecture, and (c) a decoder that further maps the continuous representation of a strategy to the ordered training dataset. The performance predictor and encoder allow us to perform gradient-based optimization in the continuous space to find the optimal training data ordering embedding with potentially better accuracy. Our experiments demonstrate that using the CIFAR-100 dataset, we can achieve a 2AP gain with our generated optimal curriculum strategy over the random strategy and have better boosts than the state-of-the-art CL algorithms. We conducted an ablation study that varied the architecture, dataset, and sample sizes to showcase the robustness of our approach.",1
"This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle (""AEC"") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning (""MARL""), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of the games commonly used with MARL, that they promote severe bugs that are hard to detect, and that the AEC games model addresses these problems.",0
"The PettingZoo library and the accompanying Agent Environment Cycle (""AEC"") games model are presented in this paper. PettingZoo, a Python API, offers a range of multi-agent environments with a universal approach. The goal of PettingZoo is to accelerate Multi-Agent Reinforcement Learning (""MARL"") research by enabling interchangeability, accessibility, and reproducibility, similar to OpenAI's Gym library for single-agent reinforcement learning. Although PettingZoo's API inherits some features from Gym, it stands out from other MARL APIs by using the novel AEC games model. Through case studies on significant problems in popular MARL environments, we argue that the current game models are inadequate conceptual models for MARL games, as they promote severe bugs that are hard to detect. The AEC games model addresses these issues.",1
"Online reinforcement learning (RL) has been widely applied in information processing scenarios, which usually exhibit much uncertainty due to the intrinsic randomness of channels and service demands. In this paper, we consider an un-discounted RL in general Markov decision processes (MDPs) with both endogeneous and exogeneous uncertainty, where both the rewards and state transition probability are unknown to the RL agent and evolve with the time as long as their respective variations do not exceed certain dynamic budget (i.e., upper bound). We first develop a variation-aware Bernstein-based upper confidence reinforcement learning (VB-UCRL), which we allow to restart according to a schedule dependent on the variations. We successfully overcome the challenges due to the exogeneous uncertainty and establish a regret bound of saving at most $\sqrt{S}$ or $S^{\frac{1}{6}}T^{\frac{1}{12}}$ compared with the latest results in the literature, where $S$ denotes the state size of the MDP and $T$ indicates the iteration index of learning steps.",0
"The use of online reinforcement learning (RL) has become prevalent in scenarios involving information processing, where uncertainty is often present due to the randomness of channels and service demands. This study focuses on an un-discounted RL approach in general Markov decision processes (MDPs) that account for both endogenous and exogenous uncertainty. In this setting, the rewards and state transition probability are unknown to the RL agent and vary over time within a certain dynamic budget. To address this challenge, the authors introduce a variation-aware Bernstein-based upper confidence reinforcement learning (VB-UCRL) method, which restarts according to a schedule dependent on the variations. The proposed approach successfully deals with exogenous uncertainty and yields a regret bound that saves up to $\sqrt{S}$ or $S^{\frac{1}{6}}T^{\frac{1}{12}}$ compared to the latest literature results. Here, $S$ represents the state size of the MDP and $T$ denotes the iteration index of learning steps.",1
"Reinforcement learning is a framework for interactive decision-making with incentives sequentially revealed across time without a system dynamics model. Due to its scaling to continuous spaces, we focus on policy search where one iteratively improves a parameterized policy with stochastic policy gradient (PG) updates. In tabular Markov Decision Problems (MDPs), under persistent exploration and suitable parameterization, global optimality may be obtained. By contrast, in continuous space, the non-convexity poses a pathological challenge as evidenced by existing convergence results being mostly limited to stationarity or arbitrary local extrema. To close this gap, we step towards persistent exploration in continuous space through policy parameterizations defined by distributions of heavier tails defined by tail-index parameter alpha, which increases the likelihood of jumping in state space. Doing so invalidates smoothness conditions of the score function common to PG. Thus, we establish how the convergence rate to stationarity depends on the policy's tail index alpha, a Holder continuity parameter, integrability conditions, and an exploration tolerance parameter introduced here for the first time. Further, we characterize the dependence of the set of local maxima on the tail index through an exit and transition time analysis of a suitably defined Markov chain, identifying that policies associated with Levy Processes of a heavier tail converge to wider peaks. This phenomenon yields improved stability to perturbations in supervised learning, which we corroborate also manifests in improved performance of policy search, especially when myopic and farsighted incentives are misaligned.",0
"Reinforcement learning is a method for making decisions interactively with incentives that are revealed sequentially over time, without the use of a system dynamics model. We focus on policy search, which involves iteratively improving a parameterized policy using stochastic policy gradient (PG) updates, due to its scalability to continuous spaces. In tabular Markov Decision Problems (MDPs), global optimality can be achieved under persistent exploration and suitable parameterization. However, in continuous space, non-convexity presents a challenge, and existing convergence results are mostly limited to stationarity or arbitrary local extrema. To address this issue, we adopt policy parameterizations defined by distributions with heavier tails, as defined by the tail-index parameter alpha, which increases the likelihood of jumping in state space. This approach invalidates the smoothness conditions of the score function common to PG, and we establish how the convergence rate to stationarity depends on alpha, a Holder continuity parameter, integrability conditions, and an exploration tolerance parameter introduced here for the first time. We also characterize the dependence of the set of local maxima on the tail index through an exit and transition time analysis of a suitably defined Markov chain, identifying that policies associated with Levy Processes of a heavier tail converge to wider peaks. This phenomenon yields improved stability to perturbations in supervised learning, which we corroborate also manifests in improved performance of policy search, especially when myopic and farsighted incentives are misaligned.",1
"Green security domains feature defenders who plan patrols in the face of uncertainty about the adversarial behavior of poachers, illegal loggers, and illegal fishers. Importantly, the deterrence effect of patrols on adversaries' future behavior makes patrol planning a sequential decision-making problem. Therefore, we focus on robust sequential patrol planning for green security following the minimax regret criterion, which has not been considered in the literature. We formulate the problem as a game between the defender and nature who controls the parameter values of the adversarial behavior and design an algorithm MIRROR to find a robust policy. MIRROR uses two reinforcement learning-based oracles and solves a restricted game considering limited defender strategies and parameter values. We evaluate MIRROR on real-world poaching data.",0
"The security domains that prioritize environmental protection require defenders to organize patrols despite being uncertain about the behavior of poachers, illegal loggers, and illegal fishers. The effect of these patrols on deterring future adversarial conduct means that patrol planning is a sequential decision-making issue. Hence, we have focused on creating a robust sequential patrol plan for green security using the minimax regret criterion, which has yet to be explored in literature. We have structured the problem as a game involving the defender and nature, who determine the parameter values for adversarial behavior. To find a resilient policy, we developed the algorithm MIRROR, which employs two reinforcement learning-based oracles and resolves a restricted game with limited defender strategies and parameter values. We assessed MIRROR's effectiveness using real-world poaching data.",1
"We introduce a novel perspective on Bayesian reinforcement learning (RL); whereas existing approaches infer a posterior over the transition distribution or Q-function, we characterise the uncertainty in the Bellman operator. Our Bayesian Bellman operator (BBO) framework is motivated by the insight that when bootstrapping is introduced, model-free approaches actually infer a posterior over Bellman operators, not value functions. In this paper, we use BBO to provide a rigorous theoretical analysis of model-free Bayesian RL to better understand its relationshipto established frequentist RL methodologies. We prove that Bayesian solutions are consistent with frequentist RL solutions, even when approximate inference isused, and derive conditions for which convergence properties hold. Empirically, we demonstrate that algorithms derived from the BBO framework have sophisticated deep exploration properties that enable them to solve continuous control tasks at which state-of-the-art regularised actor-critic algorithms fail catastrophically",0
"Our approach to Bayesian reinforcement learning is unique, as we focus on characterizing the uncertainty in the Bellman operator rather than inferring a posterior over the transition distribution or Q-function like existing methods. Our Bayesian Bellman operator (BBO) framework is based on the idea that model-free approaches infer a posterior over Bellman operators instead of value functions when bootstrapping is introduced. In this study, we utilize the BBO framework to conduct a thorough theoretical analysis of model-free Bayesian RL and its relationship to established frequentist RL methodologies. Our results demonstrate that Bayesian solutions are consistent with frequentist RL solutions, even when approximate inference is employed, and we identify conditions for which convergence properties hold. Moreover, we show that algorithms derived from the BBO framework exhibit advanced deep exploration capabilities that allow them to solve continuous control tasks that state-of-the-art regularised actor-critic algorithms cannot.",1
"In biopharmaceutical manufacturing, fermentation processes play a critical role on productivity and profit. A fermentation process uses living cells with complex biological mechanisms, and this leads to high variability in the process outputs. By building on the biological mechanisms of protein and impurity growth, we introduce a stochastic model to characterize the accumulation of the protein and impurity levels in the fermentation process. However, a common challenge in industry is the availability of only very limited amount of data especially in the development and early stage of production. This adds an additional layer of uncertainty, referred to as model risk, due to the difficulty of estimating the model parameters with limited data. In this paper, we study the harvesting decision for a fermentation process under model risk. In particular, we adopt a Bayesian approach to update the unknown parameters of the growth-rate distributions, and use the resulting posterior distributions to characterize the impact of model risk on fermentation output variability. The harvesting problem is formulated as a Markov decision process model with knowledge states that summarize the posterior distributions and hence incorporate the model risk in decision-making. The resulting model is solved by using a reinforcement learning algorithm based on Bayesian sparse sampling. We provide analytical results on the structure of the optimal policy and its objective function, and explicitly study the impact of model risk on harvesting decisions. Our case studies at MSD Animal Health demonstrate that the proposed model and solution approach improve the harvesting decisions in real life by achieving substantially higher average output from a fermentation batch along with lower batch-to-batch variability.",0
"Fermentation processes are crucial for productivity and profit in biopharmaceutical manufacturing. However, due to the use of living cells with complex biological mechanisms, the outputs of these processes can vary greatly. To address this variability, a stochastic model based on the mechanisms of protein and impurity growth is introduced. Unfortunately, limited data is often available in the early stages of production, leading to model risk and difficulty in estimating parameters. This paper focuses on the harvesting decision for a fermentation process under model risk using a Bayesian approach to update the unknown parameters. The resulting model is solved using a reinforcement learning algorithm and provides insight into the impact of model risk on harvesting decisions. Case studies demonstrate that this approach improves average output and reduces batch-to-batch variability in real-life scenarios.",1
"The objective of lifelong reinforcement learning (RL) is to optimize agents which can continuously adapt and interact in changing environments. However, current RL approaches fail drastically when environments are non-stationary and interactions are non-episodic. We propose Lifelong Skill Planning (LiSP), an algorithmic framework for non-episodic lifelong RL based on planning in an abstract space of higher-order skills. We learn the skills in an unsupervised manner using intrinsic rewards and plan over the learned skills using a learned dynamics model. Moreover, our framework permits skill discovery even from offline data, thereby reducing the need for excessive real-world interactions. We demonstrate empirically that LiSP successfully enables long-horizon planning and learns agents that can avoid catastrophic failures even in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks.",0
"The main goal of lifelong reinforcement learning (RL) is to create agents that can continually adapt and operate in changing environments. However, current RL methods struggle when faced with non-stationary environments and non-episodic interactions. To address this issue, we propose the Lifelong Skill Planning (LiSP) algorithmic framework for non-episodic lifelong RL. This framework is based on planning in a higher-order skills abstract space, which are learned in an unsupervised manner using intrinsic rewards. We plan over the acquired skills utilizing a learned dynamics model. Furthermore, our approach allows for skill discovery even from offline data, minimizing the necessity for excessive real-world interactions. Our empirical results demonstrate that LiSP effectively facilitates long-term planning and produces agents that can avert catastrophic failures in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks.",1
"A data science task can be deemed as making sense of the data or testing a hypothesis about it. The conclusions inferred from data can greatly guide us to make informative decisions. Big data has enabled us to carry out countless prediction tasks in conjunction with machine learning, such as identifying high risk patients suffering from a certain disease and taking preventable measures. However, healthcare practitioners are not content with mere predictions - they are also interested in the cause-effect relation between input features and clinical outcomes. Understanding such relations will help doctors treat patients and reduce the risk effectively. Causality is typically identified by randomized controlled trials. Often such trials are not feasible when scientists and researchers turn to observational studies and attempt to draw inferences. However, observational studies may also be affected by selection and/or confounding biases that can result in wrong causal conclusions. In this chapter, we will try to highlight some of the drawbacks that may arise in traditional machine learning and statistical approaches to analyze the observational data, particularly in the healthcare data analytics domain. We will discuss causal inference and ways to discover the cause-effect from observational studies in healthcare domain. Moreover, we will demonstrate the applications of causal inference in tackling some common machine learning issues such as missing data and model transportability. Finally, we will discuss the possibility of integrating reinforcement learning with causality as a way to counter confounding bias.",0
"The task of data science involves interpreting data and testing hypotheses to make informed decisions. With the help of big data and machine learning, we can perform various predictive tasks, such as identifying high-risk patients and taking preventive measures. However, healthcare practitioners are not content with mere predictions; they also seek to understand the cause-and-effect relationship between input features and clinical outcomes. This is typically done through randomized controlled trials, but observational studies can also provide valuable insights. Nonetheless, such studies may be affected by biases that can lead to incorrect causal conclusions. In this chapter, we will explore the limitations of traditional machine learning and statistical approaches when analyzing observational data in healthcare. We will focus on causal inference and its applications in addressing common challenges, such as missing data and model transportability. Additionally, we will discuss how reinforcement learning can be integrated with causality to mitigate confounding bias.",1
"Standard deep reinforcement learning algorithms use a shared representation for the policy and value function, especially when training directly from images. However, we argue that more information is needed to accurately estimate the value function than to learn the optimal policy. Consequently, the use of a shared representation for the policy and value function can lead to overfitting. To alleviate this problem, we propose two approaches which are combined to create IDAAC: Invariant Decoupled Advantage Actor-Critic. First, IDAAC decouples the optimization of the policy and value function, using separate networks to model them. Second, it introduces an auxiliary loss which encourages the representation to be invariant to task-irrelevant properties of the environment. IDAAC shows good generalization to unseen environments, achieving a new state-of-the-art on the Procgen benchmark and outperforming popular methods on DeepMind Control tasks with distractors. Our implementation is available at https://github.com/rraileanu/idaac.",0
"The typical approach for deep reinforcement learning algorithms is to utilize a shared representation for both the policy and value function, particularly when training directly from images. However, we posit that accurately estimating the value function requires more information than learning the optimal policy. Consequently, employing a shared representation can result in overfitting. To address this issue, we present two techniques that are merged to produce IDAAC: Invariant Decoupled Advantage Actor-Critic. Firstly, IDAAC separates the optimization of the policy and value function by utilizing distinct networks to model them. Secondly, it introduces an auxiliary loss that encourages the representation to remain consistent with task-irrelevant environmental properties. IDAAC exhibits strong generalization abilities to unseen environments, surpassing existing benchmarks on the Procgen test and outperforming popular methods on DeepMind Control tasks with distractors. Our implementation can be found at https://github.com/rraileanu/idaac.",1
"Can machine learning help us make better decisions about a changing planet? In this paper, we illustrate and discuss the potential of a promising corner of machine learning known as _reinforcement learning_ (RL) to help tackle the most challenging conservation decision problems. RL is uniquely well suited to conservation and global change challenges for three reasons: (1) RL explicitly focuses on designing an agent who _interacts_ with an environment which is dynamic and uncertain, (2) RL approaches do not require massive amounts of data, (3) RL approaches would utilize rather than replace existing models, simulations, and the knowledge they contain. We provide a conceptual and technical introduction to RL and its relevance to ecological and conservation challenges, including examples of a problem in setting fisheries quotas and in managing ecological tipping points. Four appendices with annotated code provide a tangible introduction to researchers looking to adopt, evaluate, or extend these approaches.",0
"Is it possible for machine learning to improve our decision-making abilities in response to a changing planet? This paper explores the potential of reinforcement learning (RL), a promising subset of machine learning, in addressing some of the most complex conservation decision-making problems. RL is particularly suitable for conservation and global change issues due to its three unique features: (1) emphasis on designing an agent that interacts with a dynamic and uncertain environment, (2) reliance on minimal amounts of data, and (3) incorporation of existing models, simulations, and knowledge. The paper offers a conceptual and technical overview of RL and its applicability to ecological and conservation challenges, with specific examples such as fisheries quotas and tipping point management. Researchers interested in adopting, evaluating, or enhancing these approaches can refer to the four appendices that include annotated code.",1
"Many advances that have improved the robustness and efficiency of deep reinforcement learning (RL) algorithms can, in one way or another, be understood as introducing additional objectives, or constraints, in the policy optimization step. This includes ideas as far ranging as exploration bonuses, entropy regularization, and regularization toward teachers or data priors when learning from experts or in offline RL. Often, task reward and auxiliary objectives are in conflict with each other and it is therefore natural to treat these examples as instances of multi-objective (MO) optimization problems. We study the principles underlying MORL and introduce a new algorithm, Distillation of a Mixture of Experts (DiME), that is intuitive and scale-invariant under some conditions. We highlight its strengths on standard MO benchmark problems and consider case studies in which we recast offline RL and learning from experts as MO problems. This leads to a natural algorithmic formulation that sheds light on the connection between existing approaches. For offline RL, we use the MO perspective to derive a simple algorithm, that optimizes for the standard RL objective plus a behavioral cloning term. This outperforms state-of-the-art on two established offline RL benchmarks.",0
"The improvement of deep reinforcement learning algorithms can be attributed to the introduction of additional objectives or constraints during policy optimization. This includes methods such as exploration bonuses, entropy regularization, and regularization towards data priors or teachers in offline RL or learning from experts. These objectives can conflict with each other, making it necessary to consider multi-objective optimization problems. We examine the principles of MORL and introduce a new algorithm called DiME, which is intuitive and scale-invariant in certain conditions. We demonstrate its effectiveness on standard MO benchmark problems and apply it to offline RL and learning from experts, which can be viewed as MO problems. This results in a new algorithmic formulation that clarifies the relationship between existing approaches. For offline RL, we present a simple algorithm that optimizes for the standard RL objective plus a behavioral cloning term, which outperforms state-of-the-art on two established offline RL benchmarks.",1
"We present a graph neural network to learn graph coloring heuristics using reinforcement learning. Our learned deterministic heuristics give better solutions than classical degree-based greedy heuristics and only take seconds to evaluate on graphs with tens of thousands of vertices. As our approach is based on policy-gradients, it also learns a probabilistic policy as well. These probabilistic policies outperform all greedy coloring baselines and a machine learning baseline. Our approach generalizes several previous machine-learning frameworks, which applied to problems like minimum vertex cover. We also demonstrate that our approach outperforms two greedy heuristics on minimum vertex cover.",0
"By employing reinforcement learning, we have developed a graph neural network capable of acquiring graph coloring heuristics. The deterministic heuristics that we have acquired through this process outperform the conventional degree-based greedy heuristics and can be evaluated on graphs containing tens of thousands of vertices within seconds. Our approach, which is centered on policy-gradients, has also enabled us to develop probabilistic policies that surpass all greedy coloring baselines and a machine learning baseline. Our technique has been able to generalize various prior machine-learning frameworks that were used for problems such as minimum vertex cover. Furthermore, we have demonstrated that our approach excels over two greedy heuristics in minimum vertex cover.",1
"Powered by machine learning services in the cloud, numerous learning-driven mobile applications are gaining popularity in the market. As deep learning tasks are mostly computation-intensive, it has become a trend to process raw data on devices and send the deep neural network (DNN) features to the cloud, where the features are further processed to return final results. However, there is always unexpected leakage with the release of features, with which an adversary could infer a significant amount of information about the original data. We propose a privacy-preserving reinforcement learning framework on top of the mobile cloud infrastructure from the perspective of DNN structures. The framework aims to learn a policy to modify the base DNNs to prevent information leakage while maintaining high inference accuracy. The policy can also be readily transferred to large-size DNNs to speed up learning. Extensive evaluations on a variety of DNNs have shown that our framework can successfully find privacy-preserving DNN structures to defend different privacy attacks.",0
"Learning-driven mobile applications are gaining popularity in the market, thanks to the cloud-based machine learning services that power them. While deep learning tasks tend to be computation-intensive, it has become common practice to process raw data on devices and send the deep neural network (DNN) features to the cloud for further processing. However, there is a risk of information leakage when releasing these features, which could potentially reveal a significant amount of information about the original data. To address this issue, we propose a privacy-preserving reinforcement learning framework that operates on top of the mobile cloud infrastructure and focuses on DNN structures. The goal of the framework is to learn a policy that modifies the base DNNs to prevent information leakage while maintaining high inference accuracy. This policy can also be applied to large-size DNNs to speed up learning. Extensive evaluations on a variety of DNNs have demonstrated that our framework is capable of discovering privacy-preserving DNN structures that can protect against different privacy attacks.",1
"While multitask representation learning has become a popular approach in reinforcement learning (RL), theoretical understanding of why and when it works remains limited. This paper presents analyses for the statistical benefit of multitask representation learning in linear Markov Decision Process (MDP) under a generative model. In this paper, we consider an agent to learn a representation function $\phi$ out of a function class $\Phi$ from $T$ source tasks with $N$ data per task, and then use the learned $\hat{\phi}$ to reduce the required number of sample for a new task. We first discover a \emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\kappa$, with which we prove that a straightforward least-square algorithm learns a policy which is $\tilde{O}(H^2\sqrt{\frac{\mathcal{C}(\Phi)^2 \kappa d}{NT}+\frac{\kappa d}{n}})$ sub-optimal. Here $H$ is the planning horizon, $\mathcal{C}(\Phi)$ is $\Phi$'s complexity measure, $d$ is the dimension of the representation (usually $d\ll \mathcal{C}(\Phi)$) and $n$ is the number of samples for the new task. Thus the required $n$ is $O(\kappa d H^4)$ for the sub-optimality to be close to zero, which is much smaller than $O(\mathcal{C}(\Phi)^2\kappa d H^4)$ in the setting without multitask representation learning, whose sub-optimality gap is $\tilde{O}(H^2\sqrt{\frac{\kappa \mathcal{C}(\Phi)^2d}{n}})$. This theoretically explains the power of multitask representation learning in reducing sample complexity. Further, we note that to ensure high sample efficiency, the LAFA criterion $\kappa$ should be small. In fact, $\kappa$ varies widely in magnitude depending on the different sampling distribution for new task. This indicates adaptive sampling technique is important to make $\kappa$ solely depend on $d$. Finally, we provide empirical results of a noisy grid-world environment to corroborate our theoretical findings.",0
"Although multitask representation learning is a popular approach in reinforcement learning (RL), there is limited theoretical understanding of its effectiveness. This paper presents an analysis of the statistical benefits of multitask representation learning in a linear Markov Decision Process (MDP) under a generative model. The paper proposes that an agent can learn a representation function $\phi$ from $T$ source tasks with $N$ data per task, and then use the learned $\hat{\phi}$ to reduce the required number of samples for a new task. The paper introduces the \emph{Least-Activated-Feature-Abundance} (LAFA) criterion, denoted as $\kappa$, which is used to prove that the straightforward least-squares algorithm learns a policy that is $\tilde{O}(H^2\sqrt{\frac{\mathcal{C}(\Phi)^2 \kappa d}{NT}+\frac{\kappa d}{n}})$ sub-optimal. The required number of samples for the sub-optimality to be close to zero is $O(\kappa d H^4)$, which is much smaller than $O(\mathcal{C}(\Phi)^2\kappa d H^4)$ in the setting without multitask representation learning, whose sub-optimality gap is $\tilde{O}(H^2\sqrt{\frac{\kappa \mathcal{C}(\Phi)^2d}{n}})$. The paper explains the power of multitask representation learning in reducing sample complexity and suggests that adaptive sampling techniques are important to make $\kappa$ solely depend on $d$. Finally, the paper provides empirical results of a noisy grid-world environment to support the theoretical findings.",1
"Residual reinforcement learning (RL) has been proposed as a way to solve challenging robotic tasks by adapting control actions from a conventional feedback controller to maximize a reward signal. We extend the residual formulation to learn from visual inputs and sparse rewards using demonstrations. Learning from images, proprioceptive inputs and a sparse task-completion reward relaxes the requirement of accessing full state features, such as object and target positions. In addition, replacing the base controller with a policy learned from demonstrations removes the dependency on a hand-engineered controller in favour of a dataset of demonstrations, which can be provided by non-experts. Our experimental evaluation on simulated manipulation tasks on a 6-DoF UR5 arm and a 28-DoF dexterous hand demonstrates that residual RL from demonstrations is able to generalize to unseen environment conditions more flexibly than either behavioral cloning or RL fine-tuning, and is capable of solving high-dimensional, sparse-reward tasks out of reach for RL from scratch.",0
"The residual reinforcement learning (RL) technique has been suggested as a solution for complex robotic tasks by adjusting control actions based on a feedback controller to optimize a reward signal. We have expanded on this approach by incorporating visual inputs and sparse rewards through demonstrations. This allows for learning from images, proprioceptive inputs, and a sparsely rewarded task completion, thereby reducing the need for full state feature access, such as object and target positions. Moreover, replacing the base controller with a policy learned from demonstrations eliminates the reliance on a manually engineered controller, instead utilizing a dataset of demonstrations provided by non-experts. Our experimentation on simulated manipulation tasks using a 6-DoF UR5 arm and a 28-DoF dexterous hand demonstrates that residual RL from demonstrations can adapt to new environmental conditions more efficiently than either behavioral cloning or RL fine-tuning. Additionally, it can resolve high-dimensional, sparse-reward tasks beyond the scope of RL from scratch.",1
"Text-based games (TBGs) have become a popular proving ground for the demonstration of learning-based agents that make decisions in quasi real-world settings. The crux of the problem for a reinforcement learning agent in such TBGs is identifying the objects in the world, and those objects' relations with that world. While the recent use of text-based resources for increasing an agent's knowledge and improving its generalization have shown promise, we posit in this paper that there is much yet to be learned from visual representations of these same worlds. Specifically, we propose to retrieve images that represent specific instances of text observations from the world and train our agents on such images. This improves the agent's overall understanding of the game 'scene' and objects' relationships to the world around them, and the variety of visual representations on offer allow the agent to generate a better generalization of a relationship. We show that incorporating such images improves the performance of agents in various TBG settings.",0
"Learning-based agents that make decisions in quasi real-world settings have been demonstrated to perform well in text-based games (TBGs). However, the main challenge for a reinforcement learning agent in TBGs is to identify the objects in the world and their relationships with the environment. While recent research has used text-based resources to enhance agents' knowledge and improve their generalization, this paper proposes that visual representations of the world can offer valuable insights. Specifically, this study suggests retrieving images that represent specific instances of text observations from the game and training agents on such images. This approach enhances agents' overall understanding of the game scene and objects' relationship with their environment, and enables the agent to generate better generalization. Results show that incorporating such images improves the performance of agents in various TBG settings.",1
"We propose a model-free reinforcement learning algorithm inspired by the popular randomized least squares value iteration (RLSVI) algorithm as well as the optimism principle. Unlike existing upper-confidence-bound (UCB) based approaches, which are often computationally intractable, our algorithm drives exploration by simply perturbing the training data with judiciously chosen i.i.d. scalar noises. To attain optimistic value function estimation without resorting to a UCB-style bonus, we introduce an optimistic reward sampling procedure. When the value functions can be represented by a function class $\mathcal{F}$, our algorithm achieves a worst-case regret bound of $\widetilde{O}(\mathrm{poly}(d_EH)\sqrt{T})$ where $T$ is the time elapsed, $H$ is the planning horizon and $d_E$ is the $\textit{eluder dimension}$ of $\mathcal{F}$. In the linear setting, our algorithm reduces to LSVI-PHE, a variant of RLSVI, that enjoys an $\widetilde{\mathcal{O}}(\sqrt{d^3H^3T})$ regret. We complement the theory with an empirical evaluation across known difficult exploration tasks.",0
"Our algorithm is a model-free reinforcement learning approach that takes inspiration from the optimism principle and the randomized least squares value iteration (RLSVI) algorithm. In contrast to upper-confidence-bound (UCB) based methods that can be computationally burdensome, our approach generates exploration by introducing carefully selected independent and identically distributed (i.i.d.) scalar noises to the training data. To achieve optimistic value function estimation without relying on a UCB-style bonus, we propose an optimistic reward sampling method. When the value functions can be represented by a function class $\mathcal{F}$, our algorithm has a worst-case regret bound of $\widetilde{O}(\mathrm{poly}(d_EH)\sqrt{T})$, where $T$ denotes the elapsed time, $H$ represents the planning horizon, and $d_E$ is the $\textit{eluder dimension}$ of $\mathcal{F}$. In the linear setting, our algorithm corresponds to LSVI-PHE, a variant of RLSVI, that delivers a regret of $\widetilde{\mathcal{O}}(\sqrt{d^3H^3T})$. We support our theoretical analysis with an empirical evaluation on challenging exploration tasks.",1
"Reinforcement learning is an active research area with a vast number of applications in robotics, and the RoboCup competition is an interesting environment for studying and evaluating reinforcement learning methods. A known difficulty in applying reinforcement learning to robotics is the high number of experience samples required, being the use of simulated environments for training the agents followed by transfer learning to real-world (sim-to-real) a viable path. This article introduces an open-source simulator for the IEEE Very Small Size Soccer and the Small Size League optimized for reinforcement learning experiments. We also propose a framework for creating OpenAI Gym environments with a set of benchmarks tasks for evaluating single-agent and multi-agent robot soccer skills. We then demonstrate the learning capabilities of two state-of-the-art reinforcement learning methods as well as their limitations in certain scenarios introduced in this framework. We believe this will make it easier for more teams to compete in these categories using end-to-end reinforcement learning approaches and further develop this research area.",0
"Reinforcement learning is a dynamic field of study that has numerous applications in robotics. The RoboCup competition presents a fascinating setting for exploring and assessing reinforcement learning methods. One of the main challenges of applying reinforcement learning to robotics is the need for a large number of experience samples. One feasible solution is to use simulated environments to train agents and then transfer the knowledge to the real world (sim-to-real). This article introduces a free, open-source simulator that is optimized for reinforcement learning experiments in the IEEE Very Small Size Soccer and the Small Size League. We also propose a framework for developing OpenAI Gym environments that include a series of benchmark tasks for evaluating single-agent and multi-agent robot soccer skills. We then demonstrate the learning capabilities of two advanced reinforcement learning methods, as well as their limitations in certain scenarios presented in this framework. Our aim is to facilitate the participation of more teams in these categories using end-to-end reinforcement learning approaches, and to further advance this area of research.",1
"Generation of stroke-based non-photorealistic imagery, is an important problem in the computer vision community. As an endeavor in this direction, substantial recent research efforts have been focused on teaching machines ""how to paint"", in a manner similar to a human painter. However, the applicability of previous methods has been limited to datasets with little variation in position, scale and saliency of the foreground object. As a consequence, we find that these methods struggle to cover the granularity and diversity possessed by real world images. To this end, we propose a Semantic Guidance pipeline with 1) a bi-level painting procedure for learning the distinction between foreground and background brush strokes at training time. 2) We also introduce invariance to the position and scale of the foreground object through a neural alignment model, which combines object localization and spatial transformer networks in an end to end manner, to zoom into a particular semantic instance. 3) The distinguishing features of the in-focus object are then amplified by maximizing a novel guided backpropagation based focus reward. The proposed agent does not require any supervision on human stroke-data and successfully handles variations in foreground object attributes, thus, producing much higher quality canvases for the CUB-200 Birds and Stanford Cars-196 datasets. Finally, we demonstrate the further efficacy of our method on complex datasets with multiple foreground object instances by evaluating an extension of our method on the challenging Virtual-KITTI dataset. Source code and models are available at https://github.com/1jsingh/semantic-guidance.",0
"Producing stroke-based non-photorealistic images is a significant issue in the computer vision field. Recent research efforts have aimed to teach machines how to paint like a human artist. However, these methods have limitations in their applicability to datasets that lack variation in position, scale, and saliency of the foreground object, resulting in a struggle to achieve the same level of granularity and diversity as real-world images. To address this, we propose a Semantic Guidance pipeline that includes a bi-level painting procedure for foreground-background brush stroke distinction, a neural alignment model for invariance to position and scale, and a focus reward based on guided backpropagation to amplify the in-focus object's distinguishing features. Our method does not require human stroke-data supervision and successfully handles foreground object attribute variations, producing higher quality canvases for datasets such as CUB-200 Birds and Stanford Cars-196. To demonstrate our approach's efficacy, we evaluate an extension of our method on the challenging Virtual-KITTI dataset with multiple foreground object instances. Our source code and models are available on https://github.com/1jsingh/semantic-guidance.",1
"Reinforcement learning (RL) is empirically successful in complex nonlinear Markov decision processes (MDPs) with continuous state spaces. By contrast, the majority of theoretical RL literature requires the MDP to satisfy some form of linear structure, in order to guarantee sample efficient RL. Such efforts typically assume the transition dynamics or value function of the MDP are described by linear functions of the state features. To resolve this discrepancy between theory and practice, we introduce the Effective Planning Window (EPW) condition, a structural condition on MDPs that makes no linearity assumptions. We demonstrate that the EPW condition permits sample efficient RL, by providing an algorithm which provably solves MDPs satisfying this condition. Our algorithm requires minimal assumptions on the policy class, which can include multi-layer neural networks with nonlinear activation functions. Notably, the EPW condition is directly motivated by popular gaming benchmarks, and we show that many classic Atari games satisfy this condition. We additionally show the necessity of conditions like EPW, by demonstrating that simple MDPs with slight nonlinearities cannot be solved sample efficiently.",0
"While Reinforcement Learning (RL) has proven to be successful in dealing with complex nonlinear Markov decision processes (MDPs) that have continuous state spaces, most of the existing theoretical RL literature requires some form of linear structure in the MDP in order to guarantee efficient RL. This is typically achieved by assuming that either the transition dynamics or the value function of the MDP can be described as linear functions of the state features. To address this gap between theory and practice, we introduce the Effective Planning Window (EPW) condition, which places a structural constraint on MDPs without relying on linearity assumptions. Our algorithm, which is built on the EPW condition, can efficiently solve MDPs that satisfy this condition and does not require any significant assumptions on the policy class. In particular, it can accommodate multi-layer neural networks with nonlinear activation functions. Notably, the EPW condition is directly inspired by popular gaming benchmarks, and we demonstrate that many classic Atari games meet this condition. We also show that conditions such as EPW are essential, as simple MDPs with minor nonlinearities cannot be solved efficiently.",1
"In this paper, we propose a new data poisoning attack and apply it to deep reinforcement learning agents. Our attack centers on what we call in-distribution triggers, which are triggers native to the data distributions the model will be trained on and deployed in. We outline a simple procedure for embedding these, and other, triggers in deep reinforcement learning agents following a multi-task learning paradigm, and demonstrate in three common reinforcement learning environments. We believe that this work has important implications for the security of deep learning models.",0
"This paper presents a fresh approach to data poisoning attack, specifically targeting deep reinforcement learning agents. Our method focuses on in-distribution triggers, which are inherent triggers in the data distributions used for training and deployment of the model. We have detailed a straightforward process for embedding these triggers and others in deep reinforcement learning agents using a multi-task learning approach. We have successfully tested this method in three popular reinforcement learning environments, and we believe that this research has significant implications for the safety of deep learning models.",1
"Solving Partially Observable Markov Decision Processes (POMDPs) is hard. Learning optimal controllers for POMDPs when the model is unknown is harder. Online learning of optimal controllers for unknown POMDPs, which requires efficient learning using regret-minimizing algorithms that effectively tradeoff exploration and exploitation, is even harder, and no solution exists currently. In this paper, we consider infinite-horizon average-cost POMDPs with unknown transition model, though a known observation model. We propose a natural posterior sampling-based reinforcement learning algorithm (PSRL-POMDP) and show that it achieves a regret bound of $O(\log T)$, where $T$ is the time horizon, when the parameter set is finite. In the general case (continuous parameter set), we show that the algorithm achieves $O (T^{2/3})$ regret under two technical assumptions. To the best of our knowledge, this is the first online RL algorithm for POMDPs and has sub-linear regret.",0
"POMDPs are difficult to solve. It is even more difficult to learn optimal controllers for POMDPs when the model is unknown. Online learning of optimal controllers for unknown POMDPs is the most challenging task, which currently has no solution. This paper focuses on infinite-horizon average-cost POMDPs with an unknown transition model but a known observation model. A natural posterior sampling-based reinforcement learning algorithm (PSRL-POMDP) is proposed, which achieves a regret bound of $O(\log T)$ when the parameter set is finite. In the general case with a continuous parameter set, the algorithm achieves a regret of $O(T^{2/3})$ under two technical assumptions. This is the first online RL algorithm for POMDPs that has sub-linear regret.",1
"Program synthesis has emerged as a successful approach to the image parsing task. Most prior works rely on a two-step scheme involving supervised pretraining of a Seq2Seq model with synthetic programs followed by reinforcement learning (RL) for fine-tuning with real reference images. Fully unsupervised approaches promise to train the model directly on the target images without requiring curated pretraining datasets. However, they struggle with the inherent sparsity of meaningful programs in the search space. In this paper, we present the first unsupervised algorithm capable of parsing constructive solid geometry (CSG) images into context-free grammar (CFG) without pretraining via non-differentiable renderer. To tackle the \emph{non-Markovian} sparse reward problem, we combine three key ingredients -- (i) a grammar-encoded tree LSTM ensuring program validity (ii) entropy regularization and (iii) sampling without replacement from the CFG syntax tree. Empirically, our algorithm recovers meaningful programs in large search spaces (up to $3.8 \times 10^{28}$). Further, even though our approach is fully unsupervised, it generalizes better than supervised methods on the synthetic 2D CSG dataset. On the 2D computer aided design (CAD) dataset, our approach significantly outperforms the supervised pretrained model and is competitive to the refined model.",0
"The task of image parsing has been successfully accomplished through program synthesis. Previous approaches have relied on a two-step process, involving supervised pretraining of a Seq2Seq model using synthetic programs, followed by reinforcement learning (RL) for fine-tuning with real reference images. Although fully unsupervised methods can directly train the model on target images without curated pretraining datasets, they struggle with the scarcity of meaningful programs in the search space. This paper introduces an unsupervised algorithm that can parse constructive solid geometry (CSG) images into context-free grammar (CFG) without pretraining, using a non-differentiable renderer. To address the issue of sparse rewards, the algorithm combines three key components: a grammar-encoded tree LSTM for program validity, entropy regularization, and sampling without replacement from the CFG syntax tree. The algorithm successfully recovers meaningful programs in large search spaces and outperforms supervised methods on the synthetic 2D CSG dataset. On the 2D computer-aided design (CAD) dataset, it significantly outperforms the supervised pre-trained model and is competitive with the refined model, despite being fully unsupervised.",1
"In practical reinforcement learning (RL), the discount factor used for estimating value functions often differs from that used for defining the evaluation objective. In this work, we study the effect that this discrepancy of discount factors has during learning, and discover a family of objectives that interpolate value functions of two distinct discount factors. Our analysis suggests new ways for estimating value functions and performing policy optimization updates, which demonstrate empirical performance gains. This framework also leads to new insights on commonly-used deep RL heuristic modifications to policy optimization algorithms.",0
"The discount factor utilized in practical reinforcement learning (RL) for estimating value functions is frequently not the same as the one used for defining the evaluation objective. This study investigates the impact of the difference in discount factors on the learning process and identifies a group of objectives that blend value functions of two different discount factors. Our examination proposes novel techniques for estimating value functions and performing policy optimization updates, which result in practical performance improvements. Furthermore, this approach provides new perspectives on common heuristic modifications to policy optimization algorithms used in deep RL.",1
"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.",0
"Robots can acquire new skills faster through meta-reinforcement learning algorithms that utilize past experiences to learn how to learn. However, the current research on these algorithms is mostly focused on narrow task distributions, such as using different running velocities for a simulated robot as different tasks. This limits their ability to generalize and quickly acquire entirely new tasks. To address this issue, we propose an open-source simulated benchmark consisting of 50 robotic manipulation tasks to enable the development of algorithms that can generalize and accelerate the acquisition of new, held-out tasks. We evaluate seven state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Although these algorithms can successfully learn each task and its variations, they struggle with multiple tasks even with as few as ten distinct training tasks. Our analysis and open-source environments lay the foundation for future research in multi-task learning and meta-learning that can unlock the full potential of these methods.",1
"Learning sensorimotor control policies from high-dimensional images crucially relies on the quality of the underlying visual representations. Prior works show that structured latent space such as visual keypoints often outperforms unstructured representations for robotic control. However, most of these representations, whether structured or unstructured are learned in a 2D space even though the control tasks are usually performed in a 3D environment. In this work, we propose a framework to learn such a 3D geometric structure directly from images in an end-to-end unsupervised manner. The input images are embedded into latent 3D keypoints via a differentiable encoder which is trained to optimize both a multi-view consistency loss and downstream task objective. These discovered 3D keypoints tend to meaningfully capture robot joints as well as object movements in a consistent manner across both time and 3D space. The proposed approach outperforms prior state-of-art methods across a variety of reinforcement learning benchmarks. Code and videos at https://buoyancy99.github.io/unsup-3d-keypoints/",0
"The success of learning sensorimotor control policies from high-dimensional images depends heavily on the quality of the underlying visual representations. Previous studies have shown that structured latent space, such as visual keypoints, performs better than unstructured representations for robotic control. However, most of these representations are learned in a 2D space, despite the control tasks being performed in a 3D environment. This study introduces a framework that directly learns a 3D geometric structure from images in an end-to-end unsupervised manner. The input images are embedded into latent 3D keypoints using a differentiable encoder that is trained to optimize both a multi-view consistency loss and downstream task objective. The discovered 3D keypoints accurately capture robot joints and object movements consistently across both time and 3D space. The proposed approach outperforms prior state-of-art methods on various reinforcement learning benchmarks. Access to the code and videos is available at https://buoyancy99.github.io/unsup-3d-keypoints/.",1
"Current deep reinforcement learning (RL) algorithms are still highly task-specific and lack the ability to generalize to new environments. Lifelong learning (LLL), however, aims at solving multiple tasks sequentially by efficiently transferring and using knowledge between tasks. Despite a surge of interest in lifelong RL in recent years, the lack of a realistic testbed makes robust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the other hand, can be seen as a natural scenario for lifelong RL due to its inherent non-stationarity, since the agents' policies change over time. In this work, we introduce a multi-agent lifelong learning testbed that supports both zero-shot and few-shot settings. Our setup is based on Hanabi -- a partially-observable, fully cooperative multi-agent game that has been shown to be challenging for zero-shot coordination. Its large strategy space makes it a desirable environment for lifelong RL tasks. We evaluate several recent MARL methods, and benchmark state-of-the-art LLL algorithms in limited memory and computation regimes to shed light on their strengths and weaknesses. This continual learning paradigm also provides us with a pragmatic way of going beyond centralized training which is the most commonly used training protocol in MARL. We empirically show that the agents trained in our setup are able to coordinate well with unseen agents, without any additional assumptions made by previous works. The code and all pre-trained models are available at https://github.com/chandar-lab/Lifelong-Hanabi.",0
"Although there has been a recent increase in interest in lifelong reinforcement learning (RL), current algorithms are still highly specific to tasks and lack the ability to adapt to new environments. In contrast, lifelong learning (LLL) seeks to solve multiple tasks in sequence by transferring knowledge between them. However, evaluating LLL algorithms is challenging due to the lack of a realistic testbed. Multi-agent RL (MARL) is a natural fit for lifelong RL due to its non-stationarity. This study introduces a testbed for multi-agent lifelong learning in both zero-shot and few-shot scenarios using the Hanabi game. The experiment evaluates various MARL and LLL methods and benchmarks state-of-the-art LLL algorithms in limited memory and computation regimes. The results show that the agents were able to coordinate well with unseen agents without additional assumptions made by previous works, providing a pragmatic alternative to centralized training. The code and pre-trained models are available at https://github.com/chandar-lab/Lifelong-Hanabi.",1
"In the predict-then-optimize framework, the objective is to train a predictive model, mapping from environment features to parameters of an optimization problem, which maximizes decision quality when the optimization is subsequently solved. Recent work on decision-focused learning shows that embedding the optimization problem in the training pipeline can improve decision quality and help generalize better to unseen tasks compared to relying on an intermediate loss function for evaluating prediction quality. We study the predict-then-optimize framework in the context of sequential decision problems (formulated as MDPs) that are solved via reinforcement learning. In particular, we are given environment features and a set of trajectories from training MDPs, which we use to train a predictive model that generalizes to unseen test MDPs without trajectories. Two significant computational challenges arise in applying decision-focused learning to MDPs: (i) large state and action spaces make it infeasible for existing techniques to differentiate through MDP problems, and (ii) the high-dimensional policy space, as parameterized by a neural network, makes differentiating through a policy expensive. We resolve the first challenge by sampling provably unbiased derivatives to approximate and differentiate through optimality conditions, and the second challenge by using a low-rank approximation to the high-dimensional sample-based derivatives. We implement both Bellman--based and policy gradient--based decision-focused learning on three different MDP problems with missing parameters, and show that decision-focused learning performs better in generalization to unseen tasks.",0
"The predict-then-optimize approach aims to develop a predictive model that maps environment features to optimization problem parameters, thereby optimizing decision quality during subsequent problem solving. Studies on decision-focused learning have established that embedding the optimization problem in the training pipeline helps improve decision quality and generalization to new tasks. We explore this approach in the context of sequential decision problems, specifically Markov Decision Processes (MDPs), which we solve using reinforcement learning. We face two main computational obstacles: (i) the large state and action spaces make it impractical to differentiate through MDP problems, and (ii) the high-dimensional policy space makes differentiating through a policy computationally expensive. We overcome these obstacles by sampling unbiased derivatives and using a low-rank approximation. We apply Bellman-based and policy gradient-based decision-focused learning to three MDP problems with missing parameters and demonstrate superior generalization to unseen tasks.",1
"It has been well demonstrated that inverse reinforcement learning (IRL) is an effective technique for teaching machines to perform tasks at human skill levels given human demonstrations (i.e., human to machine apprenticeship learning). This paper seeks to show that a similar application can be demonstrated with human learners. That is, given demonstrations from human experts inverse reinforcement learning techniques can be used to teach other humans to perform at higher skill levels (i.e., human to human apprenticeship learning). To show this two experiments were conducted using a simple, real-time web game where players were asked to touch targets in order to earn as many points as possible. For the experiment player performance was defined as the number of targets a player touched, irrespective of the points that a player actually earned. This allowed for in-game points to be modified and the effect of these alterations on performance measured. At no time were participants told the true performance metric. To determine the point modifications IRL was applied on demonstrations of human experts playing the game. The results of the experiment show with significance that performance improved over the control for select treatment groups. Finally, in addition to the experiment, we also detail the algorithmic challenges we faced when conducting the experiment and the techniques we used to overcome them.",0
"Inverse reinforcement learning (IRL) has proven effective in teaching machines to perform tasks at a human level when given demonstrations from humans. This paper aims to demonstrate that a similar application can be used to teach humans to perform at higher skill levels when given demonstrations from human experts. Two experiments were conducted using a real-time web game where players were tasked with touching targets to earn points. Player performance was measured based on the number of targets touched, allowing for in-game points to be modified and the effect on performance to be measured. IRL was used on demonstrations from human experts to determine the point modifications. The results showed a significant improvement in performance for select treatment groups compared to the control. The paper also discusses the algorithmic challenges faced during the experiment and the techniques used to overcome them.",1
"We introduce a framework for AI-based medical consultation system with knowledge graph embedding and reinforcement learning components and its implement. Our implement of this framework leverages knowledge organized as a graph to have diagnosis according to evidence collected from patients recurrently and dynamically. According to experiment we designed for evaluating its performance, it archives a good result. More importantly, for getting better performance, researchers can implement it on this framework based on their innovative ideas, well designed experiments and even clinical trials.",0
"A framework for a medical consultation system that utilizes AI, knowledge graph embedding, and reinforcement learning components is presented. Our implementation of this framework incorporates a graph-based knowledge organization system that enables the recurrent and dynamic collection of evidence for diagnosis. Our performance evaluation experiment yielded positive results. Furthermore, researchers can utilize their innovative ideas, well-designed experiments, and clinical trials to enhance the performance of this framework.",1
"Personalized image aesthetic assessment (PIAA) has recently become a hot topic due to its usefulness in a wide variety of applications such as photography, film and television, e-commerce, fashion design and so on. This task is more seriously affected by subjective factors and samples provided by users. In order to acquire precise personalized aesthetic distribution by small amount of samples, we propose a novel user-guided personalized image aesthetic assessment framework. This framework leverages user interactions to retouch and rank images for aesthetic assessment based on deep reinforcement learning (DRL), and generates personalized aesthetic distribution that is more in line with the aesthetic preferences of different users. It mainly consists of two stages. In the first stage, personalized aesthetic ranking is generated by interactive image enhancement and manual ranking, meanwhile two policy networks will be trained. The images will be pushed to the user for manual retouching and simultaneously to the enhancement policy network. The enhancement network utilizes the manual retouching results as the optimization goals of DRL. After that, the ranking process performs the similar operations like the retouching mentioned before. These two networks will be trained iteratively and alternatively to help to complete the final personalized aesthetic assessment automatically. In the second stage, these modified images are labeled with aesthetic attributes by one style-specific classifier, and then the personalized aesthetic distribution is generated based on the multiple aesthetic attributes of these images, which conforms to the aesthetic preference of users better.",0
"The usefulness of personalized image aesthetic assessment (PIAA) has made it a popular topic in various fields such as photography, film, television, e-commerce, and fashion design. However, this task heavily relies on subjective factors and user-provided samples. To overcome this challenge and obtain accurate personalized aesthetic distribution with minimal samples, we propose a new framework that uses deep reinforcement learning (DRL) and user interactions to retouch and rank images. This user-guided approach comprises two stages. In the first stage, images are pushed to users for manual retouching and ranking, and two policy networks are trained to optimize the enhancement process. These networks are iteratively and alternatively trained until the final personalized aesthetic assessment is completed. In the second stage, the modified images are labeled with aesthetic attributes using a style-specific classifier, and a personalized aesthetic distribution is generated based on these attributes. This approach is more aligned with the aesthetic preferences of different users.",1
"Actor-critic methods integrating target networks have exhibited a stupendous empirical success in deep reinforcement learning. However, a theoretical understanding of the use of target networks in actor-critic methods is largely missing in the literature. In this paper, we bridge this gap between theory and practice by proposing the first theoretical analysis of an online target-based actor-critic algorithm with linear function approximation in the discounted reward setting. Our algorithm uses three different timescales: one for the actor and two for the critic. Instead of using the standard single timescale temporal difference (TD) learning algorithm as a critic, we use a two timescales target-based version of TD learning closely inspired from practical actor-critic algorithms implementing target networks. First, we establish asymptotic convergence results for both the critic and the actor under Markovian sampling. Then, we provide a finite-time analysis showing the impact of incorporating a target network into actor-critic methods.",0
"Deep reinforcement learning has seen remarkable success with actor-critic methods that incorporate target networks; however, there is a lack of theoretical understanding in literature. This paper aims to close this gap by presenting the first theoretical analysis of an online target-based actor-critic algorithm using linear function approximation in the discounted reward setting. The algorithm employs three timescales, with two for the critic and one for the actor. A target-based version of temporal difference learning is used for the critic instead of the standard single timescale algorithm, as in practical actor-critic algorithms with target networks. The paper establishes asymptotic convergence results for both the critic and the actor under Markovian sampling, followed by a finite-time analysis that highlights the effects of incorporating a target network into actor-critic methods.",1
"We develop theory and algorithms for average-reward on-policy Reinforcement Learning (RL). We first consider bounding the difference of the long-term average reward for two policies. We show that previous work based on the discounted return (Schulman et al., 2015; Achiam et al., 2017) results in a non-meaningful bound in the average-reward setting. By addressing the average-reward criterion directly, we then derive a novel bound which depends on the average divergence between the two policies and Kemeny's constant. Based on this bound, we develop an iterative procedure which produces a sequence of monotonically improved policies for the average reward criterion. This iterative procedure can then be combined with classic DRL (Deep Reinforcement Learning) methods, resulting in practical DRL algorithms that target the long-run average reward criterion. In particular, we demonstrate that Average-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the average-reward criterion, significantly outperforms TRPO in the most challenging MuJuCo environments.",0
"Our focus is on average-reward on-policy Reinforcement Learning (RL), for which we create both theory and algorithms. Initially, we explore the difference between the long-term average rewards of two policies. Previous research based on the discounted return (Schulman et al., 2015; Achiam et al., 2017) does not provide a meaningful bound in the average-reward setting. Consequently, we formulate a new bound that considers the average divergence between the two policies and Kemeny's constant. Using this bound, we design an iterative procedure that generates a sequence of policies for the average reward criterion. We combine this iterative procedure with classic DRL methods to create practical algorithms targeting the long-run average reward criterion. Our experiments show that Average-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the average-reward criterion, significantly outperforms TRPO in the most challenging MuJuCo environments.",1
"Mutual information maximization provides an appealing formalism for learning representations of data. In the context of reinforcement learning (RL), such representations can accelerate learning by discarding irrelevant and redundant information, while retaining the information necessary for control. Much of the prior work on these methods has addressed the practical difficulties of estimating mutual information from samples of high-dimensional observations, while comparatively less is understood about which mutual information objectives yield representations that are sufficient for RL from a theoretical perspective. In this paper, we formalize the sufficiency of a state representation for learning and representing the optimal policy, and study several popular mutual-information based objectives through this lens. Surprisingly, we find that two of these objectives can yield insufficient representations given mild and common assumptions on the structure of the MDP. We corroborate our theoretical results with empirical experiments on a simulated game environment with visual observations.",0
"Learning representations of data through mutual information maximization is an attractive approach. In reinforcement learning, these representations can enhance the learning process by eliminating irrelevant and redundant information and preserving the necessary information for control. While previous research has focused on overcoming the practical challenges of estimating mutual information from high-dimensional observation samples, less is known about which mutual information objectives are adequate for RL from a theoretical standpoint. This paper establishes the adequacy of a state representation for learning and representing the optimal policy and examines several popular mutual-information based objectives through this lens. Surprisingly, two of these objectives can generate inadequate representations under mild and common assumptions about the MDP's structure. We support our theoretical findings by conducting empirical experiments on a simulated game environment with visual observations.",1
"Designing provably efficient algorithms with general function approximation is an important open problem in reinforcement learning. Recently, Wang et al.~[2020c] establish a value-based algorithm with general function approximation that enjoys $\widetilde{O}(\mathrm{poly}(dH)\sqrt{K})$\footnote{Throughout the paper, we use $\widetilde{O}(\cdot)$ to suppress logarithm factors. } regret bound, where $d$ depends on the complexity of the function class, $H$ is the planning horizon, and $K$ is the total number of episodes. However, their algorithm requires $\Omega(K)$ computation time per round, rendering the algorithm inefficient for practical use. In this paper, by applying online sub-sampling techniques, we develop an algorithm that takes $\widetilde{O}(\mathrm{poly}(dH))$ computation time per round on average, and enjoys nearly the same regret bound. Furthermore, the algorithm achieves low switching cost, i.e., it changes the policy only $\widetilde{O}(\mathrm{poly}(dH))$ times during its execution, making it appealing to be implemented in real-life scenarios. Moreover, by using an upper-confidence based exploration-driven reward function, the algorithm provably explores the environment in the reward-free setting. In particular, after $\widetilde{O}(\mathrm{poly}(dH))/\epsilon^2$ rounds of exploration, the algorithm outputs an $\epsilon$-optimal policy for any given reward function.",0
"Developing algorithms that are both efficient and capable of general function approximation is a crucial challenge in reinforcement learning. Wang et al.~[2020c] recently presented a value-based algorithm with general function approximation that has a regret bound of $\widetilde{O}(\mathrm{poly}(dH)\sqrt{K})$\footnote{Throughout the paper, we use $\widetilde{O}(\cdot)$ to suppress logarithm factors. }, where $d$ depends on the complexity of the function class, $H$ is the planning horizon, and $K$ is the total number of episodes. However, this algorithm's computational time per round is $\Omega(K)$, making it impractical. In this paper, we introduce a new algorithm that utilizes online sub-sampling techniques to reduce the computational time per round to $\widetilde{O}(\mathrm{poly}(dH))$ on average, while still maintaining a similar regret bound. Additionally, our algorithm has a low switching cost, changing the policy only $\widetilde{O}(\mathrm{poly}(dH))$ times during its execution, making it suitable for real-world scenarios. Furthermore, we employ an exploration-driven reward function based on upper-confidence bounds, which allows our algorithm to explore the environment in a reward-free setting. After $\widetilde{O}(\mathrm{poly}(dH))/\epsilon^2$ rounds of exploration, our algorithm outputs an $\epsilon$-optimal policy for any given reward function.",1
"High-dimensional observations are a major challenge in the application of model-based reinforcement learning (MBRL) to real-world environments. To handle high-dimensional sensory inputs, existing approaches use representation learning to map high-dimensional observations into a lower-dimensional latent space that is more amenable to dynamics estimation and planning. In this work, we present an information-theoretic approach that employs temporal predictive coding to encode elements in the environment that can be predicted across time. Since this approach focuses on encoding temporally-predictable information, we implicitly prioritize the encoding of task-relevant components over nuisance information within the environment that are provably task-irrelevant. By learning this representation in conjunction with a recurrent state space model, we can then perform planning in latent space. We evaluate our model on a challenging modification of standard DMControl tasks where the background is replaced with natural videos that contain complex but irrelevant information to the planning task. Our experiments show that our model is superior to existing methods in the challenging complex-background setting while remaining competitive with current state-of-the-art models in the standard setting.",0
"The application of model-based reinforcement learning (MBRL) to real-world environments faces a major obstacle in the form of high-dimensional observations. To overcome this challenge, existing methods resort to representation learning, which involves mapping high-dimensional sensory inputs into a lower-dimensional latent space that facilitates dynamics estimation and planning. In this study, we propose an information-theoretic approach that utilizes temporal predictive coding to encode elements in the environment that are predictable over time. By focusing on encoding temporally-predictable information, we prioritize the encoding of task-relevant components and eliminate nuisance information that is task-irrelevant. To enable planning in latent space, we learn this representation along with a recurrent state space model. We evaluate our approach on a modified version of standard DMControl tasks, wherein natural videos with complex but irrelevant information replace the background. Our experiments demonstrate that our model outperforms existing methods in the complex-background setting and is on par with the current state-of-the-art models in the standard setting.",1
"The safety constraints commonly used by existing safe reinforcement learning (RL) methods are defined only on expectation of initial states, but allow each certain state to be unsafe, which is unsatisfying for real-world safety-critical tasks. In this paper, we introduce the feasible actor-critic (FAC) algorithm, which is the first model-free constrained RL method that considers statewise safety, e.g, safety for each initial state. We claim that some states are inherently unsafe no matter what policy we choose, while for other states there exist policies ensuring safety, where we say such states and policies are feasible. By constructing a statewise Lagrange function available on RL sampling and adopting an additional neural network to approximate the statewise Lagrange multiplier, we manage to obtain the optimal feasible policy which ensures safety for each feasible state and the safest possible policy for infeasible states. Furthermore, the trained multiplier net can indicate whether a given state is feasible or not through the statewise complementary slackness condition. We provide theoretical guarantees that FAC outperforms previous expectation-based constrained RL methods in terms of both constraint satisfaction and reward optimization. Experimental results on both robot locomotive tasks and safe exploration tasks verify the safety enhancement and feasibility interpretation of the proposed method.",0
"Current safe reinforcement learning (RL) methods only consider safety constraints based on expected initial states, which can lead to certain states being unsafe, making them unsuitable for safety-critical real-world tasks. To address this issue, we introduce the feasible actor-critic (FAC) algorithm, the first model-free constrained RL method that focuses on statewise safety. We argue that some states are inherently unsafe, while others have policies that ensure safety, which we refer to as feasible states and policies. By employing a statewise Lagrange function and an additional neural network to approximate the statewise Lagrange multiplier, we can obtain the optimal feasible policy for each feasible state and the safest possible policy for infeasible states. The trained multiplier net can also determine if a state is feasible through the statewise complementary slackness condition. We prove that FAC outperforms previous expectation-based constrained RL methods in terms of both constraint satisfaction and reward optimization. Our experiments on robot locomotion and safe exploration tasks confirm the safety enhancement and feasibility interpretation of our proposed method.",1
"Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.",0
"The use of attention mechanisms has yielded promising outcomes when dealing with sequence modeling tasks that require extensive memory. Recent research has explored methods to reduce the computational burden of preserving and storing memories. However, not all information from the past holds the same level of importance. To address this, we introduce Expire-Span, a technique that enables retention of critical information while disregarding irrelevant data. By forgetting certain memories, Transformers can efficiently attend to tens of thousands of previous timesteps. We demonstrate the effectiveness of Expire-Span in challenging reinforcement learning tasks and establish new benchmarks in character-level language modeling and frame-by-frame moving object tasks that require long-term memory. Additionally, we examine the efficiency of Expire-Span compared to other approaches and illustrate its ability to train faster and consume less memory.",1
"We study the global convergence and global optimality of actor-critic, one of the most popular families of reinforcement learning algorithms. While most existing works on actor-critic employ bi-level or two-timescale updates, we focus on the more practical single-timescale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. Moreover, we consider two function approximation settings where both the actor and critic are represented by linear or deep neural networks. For both cases, we prove that the actor sequence converges to a globally optimal policy at a sublinear $O(K^{-1/2})$ rate, where $K$ is the number of iterations. To the best of our knowledge, we establish the rate of convergence and global optimality of single-timescale actor-critic with linear function approximation for the first time. Moreover, under the broader scope of policy optimization with nonlinear function approximation, we prove that actor-critic with deep neural network finds the globally optimal policy at a sublinear rate for the first time.",0
"The goal of our research is to investigate the effectiveness of actor-critic, a popular family of reinforcement learning algorithms, in achieving global convergence and global optimality. Unlike previous studies that focus on bi-level or two-timescale updates, we concentrate on the more practical single-timescale approach where the actor and critic are updated simultaneously. We achieve this by applying the Bellman evaluation operator only once to obtain the critic update and updating the actor in the policy gradient direction computed using the critic. Furthermore, we explore two function approximation settings where both the actor and critic are represented by linear or deep neural networks. Our research proves that, for both cases, the actor sequence converges to a globally optimal policy at a sublinear rate of $O(K^{-1/2})$, where $K$ is the number of iterations. This is the first time that the rate of convergence and global optimality of single-timescale actor-critic with linear function approximation has been established. Additionally, our research shows that under the broader scope of policy optimization with nonlinear function approximation, actor-critic with deep neural network can also find the globally optimal policy at a sublinear rate for the first time.",1
"We present FedScale, a diverse set of challenging and realistic benchmark datasets to facilitate scalable, comprehensive, and reproducible federated learning (FL) research. FedScale datasets are large-scale, encompassing a diverse range of important FL tasks, such as image classification, object detection, language modeling, speech recognition, and reinforcement learning. For each dataset, we provide a unified evaluation protocol using realistic data splits and evaluation metrics. To meet the pressing need for reproducing realistic FL at scale, we have also built an efficient evaluation platform to simplify and standardize the process of FL experimental setup and model evaluation. Our evaluation platform provides flexible APIs to implement new FL algorithms and includes new execution backends with minimal developer efforts. Finally, we perform indepth benchmark experiments on these datasets. Our experiments suggest fruitful opportunities in heterogeneity-aware co-optimizations of the system and statistical efficiency under realistic FL characteristics. FedScale is open-source with permissive licenses and actively maintained,1 and we welcome feedback and contributions from the community.",0
"FedScale is a compilation of various challenging and practical benchmark datasets that aim to support scalable, comprehensive, and reproducible research on federated learning (FL). These datasets are extensive and cover a wide range of significant FL tasks such as image classification, object detection, language modeling, speech recognition, and reinforcement learning. We have also established a unified evaluation protocol for each dataset, utilizing practical data splits and assessment metrics to satisfy the need for realistic FL reproduction. In addition, we have created an efficient evaluation platform to streamline and standardize the FL experimental setup and model evaluation processes. Our evaluation platform features flexible APIs for implementing novel FL algorithms, and new execution backends with minimal developer efforts. We have conducted thorough benchmark experiments on these datasets, with the results suggesting promising opportunities for system heterogeneity-aware co-optimizations and statistical efficiency under realistic FL characteristics. FedScale is an open-source project with permissive licenses and is actively maintained. We welcome feedback and contributions from the community.",1
"Offline reinforcement learning (RL) defines the task of learning from a fixed batch of data. Due to errors in value estimation from out-of-distribution actions, most offline RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modifications to make an RL algorithm work offline comes at the cost of additional complexity. Offline RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We find that we can match the performance of state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overheads of previous methods.",0
"Learning from a fixed batch of data is the primary objective of offline reinforcement learning (RL). However, due to errors in value estimation resulting from out-of-distribution actions, most offline RL algorithms rely on constraining or regularizing the policy with actions contained in the dataset. These modifications to pre-existing RL algorithms come at the cost of increased complexity, including the introduction of new hyperparameters and the use of secondary components like generative models. In this study, we aim to modify a deep RL algorithm with minimal adjustments to achieve comparable performance to state-of-the-art offline RL algorithms. Our approach involves simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a straightforward and easily tunable baseline, which reduces overall run time by more than half by eliminating the additional computational overheads of previous methods.",1
"Marginalized importance sampling (MIS), which measures the density ratio between the state-action occupancy of a target policy and that of a sampling distribution, is a promising approach for off-policy evaluation. However, current state-of-the-art MIS methods rely on complex optimization tricks and succeed mostly on simple toy problems. We bridge the gap between MIS and deep reinforcement learning by observing that the density ratio can be computed from the successor representation of the target policy. The successor representation can be trained through deep reinforcement learning methodology and decouples the reward optimization from the dynamics of the environment, making the resulting algorithm stable and applicable to high-dimensional domains. We evaluate the empirical performance of our approach on a variety of challenging Atari and MuJoCo environments.",0
"Off-policy evaluation can be achieved through marginalized importance sampling (MIS), which calculates the density ratio of state-action occupancy between a target policy and a sampling distribution. Despite its potential, the current state-of-the-art MIS methods require complicated optimization techniques and perform best on simple problems. To bridge the gap between MIS and deep reinforcement learning, we suggest using the successor representation of the target policy to compute the density ratio. The successor representation can be trained via deep reinforcement learning, removing the need for reward optimization and ensuring algorithm stability in high-dimensional domains. Empirical testing on Atari and MuJoCo environments confirms the effectiveness of our approach.",1
"The Reinforcement Learning (RL) building blocks, i.e. Q-functions and policy networks, usually take elements from the cartesian product of two domains as input. In particular, the input of the Q-function is both the state and the action, and in multi-task problems (Meta-RL) the policy can take a state and a context. Standard architectures tend to ignore these variables' underlying interpretations and simply concatenate their features into a single vector. In this work, we argue that this choice may lead to poor gradient estimation in actor-critic algorithms and high variance learning steps in Meta-RL algorithms. To consider the interaction between the input variables, we suggest using a Hypernetwork architecture where a primary network determines the weights of a conditional dynamic network. We show that this approach improves the gradient approximation and reduces the learning step variance, which both accelerates learning and improves the final performance. We demonstrate a consistent improvement across different locomotion tasks and different algorithms both in RL (TD3 and SAC) and in Meta-RL (MAML and PEARL).",0
"The components of Reinforcement Learning (RL), such as Q-functions and policy networks, commonly employ inputs from two domains. The Q-function takes both state and action as input, while in multi-task scenarios (Meta-RL), the policy can use state and context. Unfortunately, standard architectures overlook the underlying interpretations of these variables and merely concatenate their features into a single vector. This approach may lead to subpar gradient estimation in actor-critic algorithms and high variance learning steps in Meta-RL algorithms. To address this issue, we propose a Hypernetwork architecture that employs a primary network to determine the weights of a conditional dynamic network, which considers the interaction between input variables. Our approach enhances gradient approximation, reduces learning step variance, and accelerates learning while boosting final performance. We validate our method across various locomotion tasks and different algorithms in both RL (TD3 and SAC) and Meta-RL (MAML and PEARL), demonstrating consistent improvement.",1
"Sepsis is a potentially life threatening inflammatory response to infection or severe tissue damage. It has a highly variable clinical course, requiring constant monitoring of the patient's state to guide the management of intravenous fluids and vasopressors, among other interventions. Despite decades of research, there's still debate among experts on optimal treatment. Here, we combine for the first time, distributional deep reinforcement learning with mechanistic physiological models to find personalized sepsis treatment strategies. Our method handles partial observability by leveraging known cardiovascular physiology, introducing a novel physiology-driven recurrent autoencoder, and quantifies the uncertainty of its own results. Moreover, we introduce a framework for uncertainty aware decision support with humans in the loop. We show that our method learns physiologically explainable, robust policies that are consistent with clinical knowledge. Further our method consistently identifies high risk states that lead to death, which could potentially benefit from more frequent vasopressor administration, providing valuable guidance for future research",0
"Sepsis is a serious medical condition that can result from infection or tissue damage and may be life-threatening. The treatment process for this condition can be complex and requires constant monitoring of the patient's condition. While there is ongoing debate among experts regarding the best treatment methods, we have introduced a novel approach that combines distributional deep reinforcement learning with mechanistic physiological models to create personalized treatment strategies for sepsis patients. Our method accounts for incomplete information by using known cardiovascular physiology and includes a unique physiology-driven recurrent autoencoder to quantify the uncertainty of its own results. We also provide a framework for decision support that considers uncertainty and involves human input. Our approach produces policies that are consistent with clinical knowledge and can identify high-risk states that require immediate attention, such as more frequent vasopressor administration. These findings have important implications for future research in the field.",1
"We study reinforcement learning for the optimal control of Branching Markov Decision Processes (BMDPs), a natural extension of (multitype) Branching Markov Chains (BMCs). The state of a (discrete-time) BMCs is a collection of entities of various types that, while spawning other entities, generate a payoff. In comparison with BMCs, where the evolution of a each entity of the same type follows the same probabilistic pattern, BMDPs allow an external controller to pick from a range of options. This permits us to study the best/worst behaviour of the system. We generalise model-free reinforcement learning techniques to compute an optimal control strategy of an unknown BMDP in the limit. We present results of an implementation that demonstrate the practicality of the approach.",0
"Our focus is on reinforcement learning applied to Branching Markov Decision Processes (BMDPs), which are an extension of (multitype) Branching Markov Chains (BMCs). BMCs consist of entities of different types that generate a payoff while creating other entities. In contrast to BMCs, where entities of the same type evolve according to identical probabilistic patterns, BMDPs permit an external controller to select from various options, allowing us to analyze the optimal and worst-case behavior of the system. Our method generalizes model-free reinforcement learning techniques to determine the optimal control strategy for an unknown BMDP in the limit. We provide evidence of the approach's practicality through an implementation.",1
"Environments with procedurally generated content serve as important benchmarks for testing systematic generalization in deep reinforcement learning. In this setting, each level is an algorithmically created environment instance with a unique configuration of its factors of variation. Training on a prespecified subset of levels allows for testing generalization to unseen levels. What can be learned from a level depends on the current policy, yet prior work defaults to uniform sampling of training levels independently of the policy. We introduce Prioritized Level Replay (PLR), a general framework for selectively sampling the next training level by prioritizing those with higher estimated learning potential when revisited in the future. We show TD-errors effectively estimate a level's future learning potential and, when used to guide the sampling procedure, induce an emergent curriculum of increasingly difficult levels. By adapting the sampling of training levels, PLR significantly improves sample efficiency and generalization on Procgen Benchmark--matching the previous state-of-the-art in test return--and readily combines with other methods. Combined with the previous leading method, PLR raises the state-of-the-art to over 76% improvement in test return relative to standard RL baselines.",0
"Procedurally generated environments are crucial for testing the systematic generalization of deep reinforcement learning. Each level is created through algorithms and has a distinct configuration of factors of variation. Training on a subset of levels enables the assessment of generalization to unseen levels. A level's potential for learning depends on the current policy, but previous work has uniformly sampled training levels regardless of the policy. Our solution, Prioritized Level Replay (PLR), prioritizes levels with higher estimated learning potential for future revisits. By using TD-errors to guide the sampling procedure, we create a curriculum of increasingly challenging levels. Our approach significantly improves sample efficiency and generalization on Procgen Benchmark, matching the previous state-of-the-art in test return. Furthermore, PLR can be combined with other methods and, when combined with the previous leading method, raises the state-of-the-art to over 76% improvement in test return compared to standard RL baselines.",1
"A major challenge in reinforcement learning is the design of exploration strategies, especially for environments with sparse reward structures and continuous state and action spaces. Intuitively, if the reinforcement signal is very scarce, the agent should rely on some form of short-term memory in order to cover its environment efficiently. We propose a new exploration method, based on two intuitions: (1) the choice of the next exploratory action should depend not only on the (Markovian) state of the environment, but also on the agent's trajectory so far, and (2) the agent should utilize a measure of spread in the state space to avoid getting stuck in a small region. Our method leverages concepts often used in statistical physics to provide explanations for the behavior of simplified (polymer) chains in order to generate persistent (locally self-avoiding) trajectories in state space. We discuss the theoretical properties of locally self-avoiding walks and their ability to provide a kind of short-term memory through a decaying temporal correlation within the trajectory. We provide empirical evaluations of our approach in a simulated 2D navigation task, as well as higher-dimensional MuJoCo continuous control locomotion tasks with sparse rewards.",0
"Exploration strategies in reinforcement learning pose a significant difficulty, particularly in scenarios with limited reward structures and continuous state and action spaces. To effectively survey their environment, agents must rely on short-term memory when reinforcement signals are scarce. Our novel exploration method is based on two primary concepts: (1) the selection of the next exploratory action must account for the agent's trajectory as well as the state of the environment, and (2) a measure of state space dispersion is essential to prevent the agent from becoming trapped in a small region. Our method employs statistical physics concepts utilized in simplified polymer chains to create persistent, locally self-avoiding trajectories in state space. We examine the theoretical properties of locally self-avoiding walks and their ability to provide short-term memory through a decaying temporal correlation within the trajectory. We evaluate our approach in a simulated 2D navigation task and higher-dimensional MuJoCo continuous control locomotion tasks with sparse rewards.",1
"We study the adversarial robustness in offline reinforcement learning. Given a batch dataset consisting of tuples $(s, a, r, s')$, an adversary is allowed to arbitrarily modify $\epsilon$ fraction of the tuples. From the corrupted dataset the learner aims to robustly identify a near-optimal policy. We first show that a worst-case $\Omega(d\epsilon)$ optimality gap is unavoidable in linear MDP of dimension $d$, even if the adversary only corrupts the reward element in a tuple. This contrasts with dimension-free results in robust supervised learning and best-known lower-bound in the online RL setting with corruption. Next, we propose robust variants of the Least-Square Value Iteration (LSVI) algorithm utilizing robust supervised learning oracles, which achieve near-matching performances in cases both with and without full data coverage. The algorithm requires the knowledge of $\epsilon$ to design the pessimism bonus in the no-coverage case. Surprisingly, in this case, the knowledge of $\epsilon$ is necessary, as we show that being adaptive to unknown $\epsilon$ is impossible.This again contrasts with recent results on corruption-robust online RL and implies that robust offline RL is a strictly harder problem.",0
"Our focus is on examining adversarial robustness in offline reinforcement learning. To this end, we consider a batch dataset comprising tuples $(s, a, r, s')$, where an adversary has the ability to modify $\epsilon$ fraction of the tuples in any way they choose. Despite this challenge, the goal is for the learner to identify a near-optimal policy from the corrupted dataset. Our initial findings indicate that a worst-case $\Omega(d\epsilon)$ optimality gap is inevitable in linear MDP of dimension $d, even if the adversary only alters the reward element in a tuple. This differs from dimension-free results in robust supervised learning and the best-known lower-bound in the online RL setting with corruption. We proceed to introduce robust versions of the Least-Square Value Iteration (LSVI) algorithm, which utilize robust supervised learning oracles and perform almost equally well in situations with and without full data coverage. However, the algorithm requires knowledge of $\epsilon$ to design the pessimism bonus in the no-coverage scenario. Interestingly, we discover that being adaptive to unknown $\epsilon$ is unachievable, unlike in recent corruption-robust online RL studies. This suggests that robust offline RL is a more challenging task.",1
"A fundamental challenge in multiagent reinforcement learning is to learn beneficial behaviors in a shared environment with other simultaneously learning agents. In particular, each agent perceives the environment as effectively non-stationary due to the changing policies of other agents. Moreover, each agent is itself constantly learning, leading to natural non-stationarity in the distribution of experiences encountered. In this paper, we propose a novel meta-multiagent policy gradient theorem that directly accounts for the non-stationary policy dynamics inherent to multiagent learning settings. This is achieved by modeling our gradient updates to consider both an agent's own non-stationary policy dynamics and the non-stationary policy dynamics of other agents in the environment. We show that our theoretically grounded approach provides a general solution to the multiagent learning problem, which inherently comprises all key aspects of previous state of the art approaches on this topic. We test our method on a diverse suite of multiagent benchmarks and demonstrate a more efficient ability to adapt to new agents as they learn than baseline methods across the full spectrum of mixed incentive, competitive, and cooperative domains.",0
"Multiagent reinforcement learning poses a significant challenge as it involves learning advantageous behaviors in an environment shared by multiple agents who are simultaneously learning. Each agent perceives the environment as non-stationary due to other agents' changing policies, and the natural non-stationarity in the distribution of experiences encountered by each agent who is constantly learning. This paper presents a fresh approach to address the non-stationary policy dynamics inherent in multiagent learning settings by proposing a novel meta-multiagent policy gradient theorem. Our approach models gradient updates to consider both an agent's own non-stationary policy dynamics and the non-stationary policy dynamics of other agents in the environment. We demonstrate that our theoretically grounded approach provides an all-inclusive solution to the multiagent learning problem, encompassing all critical aspects of previous state-of-the-art approaches. We evaluate our method on a variety of multiagent benchmarks and show its superior efficiency to adapt to new agents as they learn compared to baseline methods across all mixed incentive, competitive, and cooperative domains.",1
"Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from several issues, such as instability in Q-learning and balancing exploration and exploitation. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing the diversity between agents using Bootstrap with random initialization, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments. Our training code is available at https://github.com/pokaxpoka/sunrise.",0
"Although off-policy deep reinforcement learning (RL) has proven successful in navigating challenging domains, standard off-policy RL algorithms may encounter instability in Q-learning and difficulty balancing exploration and exploitation. To address these issues, we introduce SUNRISE, a straightforward and comprehensive ensemble method that is compatible with a variety of off-policy RL algorithms. SUNRISE integrates two crucial components: (a) ensemble-based weighted Bellman backups, which adjust target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing diversity between agents using Bootstrap with random initialization, we demonstrate that these distinct concepts are mostly independent and can be usefully combined to enhance the performance of existing off-policy RL algorithms, including Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks in both low-dimensional and high-dimensional environments. Our training code is accessible at https://github.com/pokaxpoka/sunrise.",1
"Off-policy prediction -- learning the value function for one policy from data generated while following another policy -- is one of the most challenging subproblems in reinforcement learning. This paper presents empirical results with eleven prominent off-policy learning algorithms that use linear function approximation: five Gradient-TD methods, two Emphatic-TD methods, Off-policy TD($\lambda$), Vtrace, and versions of Tree Backup and ABQ modified to apply to a prediction setting. Our experiments used the Collision task, a small idealized off-policy problem analogous to that of an autonomous car trying to predict whether it will collide with an obstacle. We assessed the performance of the algorithms according to their learning rate, asymptotic error level, and sensitivity to step-size and bootstrapping parameters. By these measures, the eleven algorithms can be partially ordered on the Collision task. In the top tier, the two Emphatic-TD algorithms learned the fastest, reached the lowest errors, and were robust to parameter settings. In the middle tier, the five Gradient-TD algorithms and Off-policy TD($\lambda$) were more sensitive to the bootstrapping parameter. The bottom tier comprised Vtrace, Tree Backup, and ABQ; these algorithms were no faster and had higher asymptotic error than the others. Our results are definitive for this task, though of course experiments with more tasks are needed before an overall assessment of the algorithms' merits can be made.",0
"Reinforcement learning encounters a complex challenge in off-policy prediction, which involves learning the value function for one policy using data generated from another policy. This study examines the efficacy of eleven well-known off-policy learning algorithms that apply linear function approximation, including five Gradient-TD methods, two Emphatic-TD methods, Off-policy TD($\lambda$), Vtrace, and modified versions of Tree Backup and ABQ for prediction tasks. The Collision task, a simplified off-policy problem similar to an autonomous car's obstacle prediction, was employed in the experiments. The algorithms' learning rate, asymptotic error level, and sensitivity to step-size and bootstrapping parameters were assessed. The results partially order the algorithms based on their performance on the Collision task. The top tier includes the two Emphatic-TD algorithms that demonstrated the fastest learning, the lowest errors, and parameter robustness. The middle tier consists of the five Gradient-TD algorithms and Off-policy TD($\lambda$), which showed sensitivity to bootstrapping parameters. The bottom tier includes Vtrace, Tree Backup, and ABQ, which were less efficient with higher asymptotic error rates. Although the results are conclusive for the Collision task, further experiments with different tasks are necessary for a complete evaluation of the algorithms' effectiveness.",1
"We consider offline reinforcement learning (RL) with heterogeneous agents under severe data scarcity, i.e., we only observe a single historical trajectory for every agent under an unknown, potentially sub-optimal policy. We find that the performance of state-of-the-art offline and model-based RL methods degrade significantly given such limited data availability, even for commonly perceived ""solved"" benchmark settings such as ""MountainCar"" and ""CartPole"". To address this challenge, we propose PerSim, a model-based offline RL approach which first learns a personalized simulator for each agent by collectively using the historical trajectories across all agents, prior to learning a policy. We do so by positing that the transition dynamics across agents can be represented as a latent function of latent factors associated with agents, states, and actions; subsequently, we theoretically establish that this function is well-approximated by a ""low-rank"" decomposition of separable agent, state, and action latent functions. This representation suggests a simple, regularized neural network architecture to effectively learn the transition dynamics per agent, even with scarce, offline data. We perform extensive experiments across several benchmark environments and RL methods. The consistent improvement of our approach, measured in terms of both state dynamics prediction and eventual reward, confirms the efficacy of our framework in leveraging limited historical data to simultaneously learn personalized policies across agents.",0
"Our focus is on offline reinforcement learning (RL) where data scarcity is severe, and agents are heterogeneous. We have access to only one historical trajectory for every agent, which was performed under an unknown and potentially sub-optimal policy. We discovered that even state-of-the-art offline and model-based RL methods struggle to perform well when faced with such limited data availability, even on widely accepted benchmark settings such as ""MountainCar"" and ""CartPole."" We introduce PerSim, a model-based offline RL approach, to address this challenge. We first learn a personalized simulator for each agent by collectively using the historical trajectories of all agents before learning a policy. We propose a simple, regularized neural network architecture that effectively learns the transition dynamics per agent, even with scarce, offline data. Our experiments on several benchmark environments and RL methods show that our approach consistently improves state dynamics prediction and eventual reward, demonstrating the effectiveness of our framework in simultaneously learning personalized policies across agents while leveraging limited historical data.",1
"Driven by the explosive interest in applying deep reinforcement learning (DRL) agents to numerous real-time control and decision-making applications, there has been a growing demand to deploy DRL agents to empower daily-life intelligent devices, while the prohibitive complexity of DRL stands at odds with limited on-device resources. In this work, we propose an Automated Agent Accelerator Co-Search (A3C-S) framework, which to our best knowledge is the first to automatically co-search the optimally matched DRL agents and accelerators that maximize both test scores and hardware efficiency. Extensive experiments consistently validate the superiority of our A3C-S over state-of-the-art techniques.",0
"The surge in interest in utilizing deep reinforcement learning (DRL) agents for real-time control and decision-making applications has resulted in a rising need to incorporate DRL agents into everyday intelligent devices. However, this desire is hampered by the intricate nature of DRL, which conflicts with the limited resources available on these devices. This study presents the Automated Agent Accelerator Co-Search (A3C-S) framework, which is the first to automatically and optimally match DRL agents and accelerators to enhance both test scores and hardware efficiency. Our extensive experiments consistently demonstrate that our A3C-S outperforms current techniques.",1
"Multi-agent settings in the real world often involve tasks with varying types and quantities of agents and non-agent entities; however, common patterns of behavior often emerge among these agents/entities. Our method aims to leverage these commonalities by asking the question: ``What is the expected utility of each agent when only considering a randomly selected sub-group of its observed entities?'' By posing this counterfactual question, we can recognize state-action trajectories within sub-groups of entities that we may have encountered in another task and use what we learned in that task to inform our prediction in the current one. We then reconstruct a prediction of the full returns as a combination of factors considering these disjoint groups of entities and train this ``randomly factorized"" value function as an auxiliary objective for value-based multi-agent reinforcement learning. By doing so, our model can recognize and leverage similarities across tasks to improve learning efficiency in a multi-task setting. Our approach, Randomized Entity-wise Factorization for Imagined Learning (REFIL), outperforms all strong baselines by a significant margin in challenging multi-task StarCraft micromanagement settings.",0
"Tasks in real-world multi-agent settings often involve varying quantities and types of agents and non-agent entities. Despite this, there are often common patterns of behavior among these entities. Our method seeks to capitalize on these similarities by posing a counterfactual question regarding the expected utility of each agent when considering a randomly selected subgroup of its observed entities. This enables us to recognize state-action trajectories within subgroups that we may have encountered in previous tasks, informing our predictions in the current task. We combine factors from these disjoint groups of entities to reconstruct a prediction of full returns. We then train our ""randomly factorized"" value function as an auxiliary objective for value-based multi-agent reinforcement learning. This allows our model to recognize and leverage similarities across tasks, improving learning efficiency in a multi-task setting. Our approach, Randomized Entity-wise Factorization for Imagined Learning (REFIL), outperforms all strong baselines by a significant margin in challenging multi-task StarCraft micromanagement settings.",1
"Wax is what you put on a surfboard to avoid slipping. It is an essential tool to go surfing... We introduce WAX-ML a research-oriented Python library providing tools to design powerful machine learning algorithms and feedback loops working on streaming data. It strives to complement JAX with tools dedicated to time series. WAX-ML makes JAX-based programs easy to use for end-users working with pandas and xarray for data manipulation. It provides a simple mechanism for implementing feedback loops, allows the implementation of online learning and reinforcement learning algorithms with functions, and makes them easy to integrate by end-users working with the object-oriented reinforcement learning framework from the Gym library. It is released with an Apache open-source license on GitHub at https://github.com/eserie/wax-ml.",0
"To prevent slipping on a surfboard, wax is a necessary tool when surfing. Similarly, WAX-ML is a Python library designed for research purposes that offers powerful tools to create effective machine learning algorithms and feedback loops for processing streaming data. Its goal is to enhance JAX by providing time series-focused tools. WAX-ML simplifies JAX-based programs for end-users who use pandas and xarray for data manipulation. It incorporates an uncomplicated method for implementing feedback loops, facilitating the integration of online learning and reinforcement learning algorithms through functions. End-users who work with the object-oriented reinforcement learning framework from the Gym library can easily integrate these algorithms with WAX-ML. The library is available on GitHub with an Apache open-source license at https://github.com/eserie/wax-ml.",1
"Artificial Intelligence (AI) is becoming a critical component in the defense industry, as recently demonstrated by DARPA`s AlphaDogfight Trials (ADT). ADT sought to vet the feasibility of AI algorithms capable of piloting an F-16 in simulated air-to-air combat. As a participant in ADT, Lockheed Martin`s (LM) approach combines a hierarchical architecture with maximum-entropy reinforcement learning (RL), integrates expert knowledge through reward shaping, and supports modularity of policies. This approach achieved a $2^{nd}$ place finish in the final ADT event (among eight total competitors) and defeated a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course in match play.",0
"The defense industry is increasingly relying on Artificial Intelligence (AI), as evidenced by DARPA's AlphaDogfight Trials (ADT). The ADT aimed to test the feasibility of AI algorithms capable of piloting an F-16 in simulated air-to-air combat. One of the participants, Lockheed Martin (LM), utilized a hierarchical architecture combined with maximum-entropy reinforcement learning (RL), incorporated expert knowledge through reward shaping, and supported policy modularity. This approach secured a $2^{nd}$ place finish in the final ADT event, surpassing a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course in match play.",1
"In reinforcement learning (RL), function approximation errors are known to easily lead to the Q-value overestimations, thus greatly reducing policy performance. This paper presents a distributional soft actor-critic (DSAC) algorithm, which is an off-policy RL method for continuous control setting, to improve the policy performance by mitigating Q-value overestimations. We first discover in theory that learning a distribution function of state-action returns can effectively mitigate Q-value overestimations because it is capable of adaptively adjusting the update stepsize of the Q-value function. Then, a distributional soft policy iteration (DSPI) framework is developed by embedding the return distribution function into maximum entropy RL. Finally, we present a deep off-policy actor-critic variant of DSPI, called DSAC, which directly learns a continuous return distribution by keeping the variance of the state-action returns within a reasonable range to address exploding and vanishing gradient problems. We evaluate DSAC on the suite of MuJoCo continuous control tasks, achieving the state-of-the-art performance.",0
"The paper introduces the distributional soft actor-critic (DSAC) algorithm, an off-policy reinforcement learning (RL) method for continuous control setting. Its purpose is to enhance policy performance by reducing Q-value overestimations, which are typically caused by function approximation errors. The DSAC algorithm achieves this goal by learning a distribution function of state-action returns, which can adaptively adjust the update stepsize of the Q-value function. The paper explains the development of a distributional soft policy iteration (DSPI) framework by incorporating the return distribution function into maximum entropy RL. Finally, the authors present a deep off-policy actor-critic variant of DSPI, called DSAC, which directly learns a continuous return distribution and ensures that the variance of the state-action returns remains within a reasonable range to tackle exploding and vanishing gradient issues. The performance of DSAC is assessed using the MuJoCo continuous control tasks, where it outperforms other methods and achieves the state-of-the-art results.",1
"Offline Reinforcement Learning (RL) aims at learning an optimal control from a fixed dataset, without interactions with the system. An agent in this setting should avoid selecting actions whose consequences cannot be predicted from the data. This is the converse of exploration in RL, which favors such actions. We thus take inspiration from the literature on bonus-based exploration to design a new offline RL agent. The core idea is to subtract a prediction-based exploration bonus from the reward, instead of adding it for exploration. This allows the policy to stay close to the support of the dataset. We connect this approach to a more common regularization of the learned policy towards the data. Instantiated with a bonus based on the prediction error of a variational autoencoder, we show that our agent is competitive with the state of the art on a set of continuous control locomotion and manipulation tasks.",0
"The objective of Offline Reinforcement Learning (RL) is to acquire optimal control from a fixed dataset without any system interactions. In this scenario, the agent must avoid selecting actions that cannot be predicted from the available data, which is the opposite of exploration in RL. To address this, we have drawn inspiration from the literature on bonus-based exploration to create a novel offline RL agent. The fundamental concept is to subtract a prediction-based exploration bonus from the reward rather than adding it for exploration, enabling the policy to remain close to the dataset's support. We have linked this method to a more common regularization of the learned policy towards the data. By utilizing a bonus based on the prediction error of a variational autoencoder, we have demonstrated that our agent is competitive with the current state of the art for a variety of continuous control locomotion and manipulation tasks.",1
"Episodic memory-based methods can rapidly latch onto past successful strategies by a non-parametric memory and improve sample efficiency of traditional reinforcement learning. However, little effort is put into the continuous domain, where a state is never visited twice, and previous episodic methods fail to efficiently aggregate experience across trajectories. To address this problem, we propose Generalizable Episodic Memory (GEM), which effectively organizes the state-action values of episodic memory in a generalizable manner and supports implicit planning on memorized trajectories. GEM utilizes a double estimator to reduce the overestimation bias induced by value propagation in the planning process. Empirical evaluation shows that our method significantly outperforms existing trajectory-based methods on various MuJoCo continuous control tasks. To further show the general applicability, we evaluate our method on Atari games with discrete action space, which also shows a significant improvement over baseline algorithms.",0
"The use of episodic memory-based techniques can quickly learn from previous successful strategies and enhance the efficiency of traditional reinforcement learning by using non-parametric memory. However, these methods have been less effective in the continuous domain, where states are not revisited, and past episodic methods do not efficiently combine experience across trajectories. To tackle this issue, we propose the Generalizable Episodic Memory (GEM) approach, which organizes state-action values of episodic memory in a generalizable way and enables implicit planning on stored trajectories. GEM uses a double estimator to decrease overestimation bias caused by value propagation in the planning process. Our empirical results demonstrate that GEM outperforms existing trajectory-based methods on various MuJoCo continuous control tasks and Atari games with discrete action space, highlighting the applicability of our approach.",1
"The transition from today's mostly human-driven traffic to a purely automated one will be a gradual evolution, with the effect that we will likely experience mixed traffic in the near future. Connected and automated vehicles can benefit human-driven ones and the whole traffic system in different ways, for example by improving collision avoidance and reducing traffic waves. Many studies have been carried out to improve intersection management, a significant bottleneck in traffic, with intelligent traffic signals or exclusively automated vehicles. However, the problem of how to improve mixed traffic at unsignalized intersections has received less attention. In this paper, we propose a novel approach to optimizing traffic flow at intersections in mixed traffic situations using deep reinforcement learning. Our reinforcement learning agent learns a policy for a centralized controller to let connected autonomous vehicles at unsignalized intersections give up their right of way and yield to other vehicles to optimize traffic flow. We implemented our approach and tested it in the traffic simulator SUMO based on simulated and real traffic data. The experimental evaluation demonstrates that our method significantly improves traffic flow through unsignalized intersections in mixed traffic settings and also provides better performance on a wide range of traffic situations compared to the state-of-the-art traffic signal controller for the corresponding signalized intersection.",0
"The shift towards a fully automated traffic system is likely to be a gradual process, resulting in a mix of human and automated vehicles on the road in the near future. The integration of connected and automated vehicles can bring several benefits to the overall traffic system, such as reducing collisions and traffic congestion. While there have been numerous studies on improving intersection management using intelligent traffic signals or automated vehicles, the issue of optimizing traffic flow in mixed traffic situations at unsignalized intersections has received less attention. In this study, we propose a novel approach using deep reinforcement learning to optimize traffic flow at unsignalized intersections. Our reinforcement learning agent learns a policy for a centralized controller to allow connected autonomous vehicles to yield to other vehicles and improve traffic flow. The approach was tested in the traffic simulator SUMO using simulated and real traffic data, and the results show significant improvement in traffic flow compared to the state-of-the-art traffic signal controller for corresponding signalized intersections.",1
"The use of Reinforcement Learning (RL) agents in practical applications requires the consideration of suboptimal outcomes, depending on the familiarity of the agent with its environment. This is especially important in safety-critical environments, where errors can lead to high costs or damage. In distributional RL, the risk-sensitivity can be controlled via different distortion measures of the estimated return distribution. However, these distortion functions require an estimate of the risk level, which is difficult to obtain and depends on the current state. In this work, we demonstrate the suboptimality of a static risk level estimation and propose a method to dynamically select risk levels at each environment step. Our method ARA (Automatic Risk Adaptation) estimates the appropriate risk level in both known and unknown environments using a Random Network Distillation error. We show reduced failure rates by up to a factor of 7 and improved generalization performance by up to 14% compared to both risk-aware and risk-agnostic agents in several locomotion environments.",0
"When utilizing Reinforcement Learning (RL) agents in practical settings, it is necessary to consider suboptimal results based on the agent's familiarity with its surroundings. This is particularly crucial in environments where errors can result in significant expenses or harm. In distributional RL, the sensitivity of risk can be managed by various distortion measures of the estimated return distribution. However, these distortion functions necessitate an assessment of the risk level, which is challenging to achieve and depends on the current situation. This study illustrates the inadequacy of a fixed risk level and suggests a way to dynamically choose risk levels at every environment step. Our approach, called Automatic Risk Adaptation (ARA), utilizes a Random Network Distillation error to assess the appropriate risk level in both known and unknown surroundings. We show that this method reduces failure rates by as much as 7 times and improves generalization performance by up to 14% compared to both risk-aware and risk-agnostic agents in various locomotion environments.",1
"Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we consistently evaluate and compare three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase~\citep{samvelyan19smac} to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards.",0
"The absence of standardized evaluation tasks and criteria for Multi-agent deep reinforcement learning (MARL) presents a challenge in comparing different approaches. Our study aims to address this issue by systematically evaluating and contrasting three classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) across a diverse range of cooperative multi-agent learning tasks. Our experiments establish a benchmark for algorithm performance and offer valuable insights into the effectiveness of different learning methodologies. We also release EPyMARL, a modified version of the PyMARL codebase~\citep{samvelyan19smac} that supports additional algorithms and enables flexible configuration of implementation details, including parameter sharing. Furthermore, we provide two open-source environments for multi-agent research that focus on coordination under sparse rewards.",1
"Transformers with linearised attention (""linear Transformers"") have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight Programmers (FWPs) from the '90s. However, the original FWP formulation is more general than the one of linear Transformers: a slow neural network (NN) continually reprograms the weights of a fast NN with arbitrary NN architectures. In existing linear Transformers, both NNs are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of Transformers and RNNs. In the reinforcement learning setting, we report large improvements over LSTM in several Atari games. Our code is public.",0
"The effectiveness of outer product-based Fast Weight Programmers (FWPs) from the '90s has been demonstrated by Transformers with linearised attention (""linear Transformers""). However, the original FWP formulation is more general compared to linear Transformers. In the original formulation, a slow neural network (NN) continuously reprograms the weights of a fast NN with arbitrary NN architectures. On the other hand, linear Transformers have both NNs as feedforward and consist of only one layer. This study examines new variations by introducing recurrence to the slow and fast nets. The authors evaluate their novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks, Wikitext-103 language models, and on the Atari 2600 2D game environment. The models exhibit properties of both Transformers and RNNs. The authors report significant improvements over LSTM in several Atari games in the reinforcement learning setting. The code is publicly available.",1
"Safety in reinforcement learning has become increasingly important in recent years. Yet, existing solutions either fail to strictly avoid choosing unsafe actions, which may lead to catastrophic results in safety-critical systems, or fail to provide regret guarantees for settings where safety constraints need to be learned. In this paper, we address both problems by first modeling safety as an unknown linear cost function of states and actions, which must always fall below a certain threshold. We then present algorithms, termed SLUCB-QVI and RSLUCB-QVI, for episodic Markov decision processes (MDPs) with linear function approximation. We show that SLUCB-QVI and RSLUCB-QVI, while with \emph{no safety violation}, achieve a $\tilde{\mathcal{O}}\left(\kappa\sqrt{d^3H^3T}\right)$ regret, nearly matching that of state-of-the-art unsafe algorithms, where $H$ is the duration of each episode, $d$ is the dimension of the feature mapping, $\kappa$ is a constant characterizing the safety constraints, and $T$ is the total number of action plays. We further present numerical simulations that corroborate our theoretical findings.",0
"In recent years, safety has become increasingly crucial in reinforcement learning. However, existing solutions have shortcomings. They either do not strictly avoid selecting unsafe actions, which could result in disastrous outcomes in safety-critical systems, or they do not provide regret guarantees for scenarios where safety constraints must be learned. This paper tackles both issues by modeling safety as an unfamiliar linear cost function of states and actions, which must always stay below a specific threshold. We propose two algorithms, SLUCB-QVI and RSLUCB-QVI, for episodic Markov decision processes (MDPs) with linear function approximation. We demonstrate that these algorithms achieve no safety violation and a $\tilde{\mathcal{O}}\left(\kappa\sqrt{d^3H^3T}\right)$ regret, which is almost comparable to state-of-the-art unsafe algorithms. Here, $H$ is the duration of each episode, $d$ is the dimension of the feature mapping, $\kappa$ is a constant characterizing the safety constraints, and $T$ is the total number of action plays. We also provide numerical simulations that support our theoretical results.",1
"Model-based reinforcement learning (MBRL) approaches rely on discrete-time state transition models whereas physical systems and the vast majority of control tasks operate in continuous-time. To avoid time-discretization approximation of the underlying process, we propose a continuous-time MBRL framework based on a novel actor-critic method. Our approach also infers the unknown state evolution differentials with Bayesian neural ordinary differential equations (ODE) to account for epistemic uncertainty. We implement and test our method on a new ODE-RL suite that explicitly solves continuous-time control systems. Our experiments illustrate that the model is robust against irregular and noisy data, is sample-efficient, and can solve control problems which pose challenges to discrete-time MBRL methods.",0
"MBRL methods typically utilize state transition models that operate in discrete time, which is not ideal for physical systems and most control tasks that operate in continuous time. To overcome this issue, we propose a new actor-critic method for a continuous-time MBRL framework. Our approach uses Bayesian neural ODEs to estimate the unknown state evolution differentials, which helps to account for epistemic uncertainty. We test our method on a new ODE-RL suite that explicitly solves continuous-time control systems and find that it can handle irregular and noisy data, is sample-efficient, and can solve control problems that are difficult for discrete-time MBRL methods.",1
"Tackling overestimation in $Q$-learning is an important problem that has been extensively studied in single-agent reinforcement learning, but has received comparatively little attention in the multi-agent setting. In this work, we empirically demonstrate that QMIX, a popular $Q$-learning algorithm for cooperative multi-agent reinforcement learning (MARL), suffers from a more severe overestimation in practice than previously acknowledged, and is not mitigated by existing approaches. We rectify this with a novel regularization-based update scheme that penalizes large joint action-values that deviate from a baseline and demonstrate its effectiveness in stabilizing learning. Furthermore, we propose to employ a softmax operator, which we efficiently approximate in a novel way in the multi-agent setting, to further reduce the potential overestimation bias. Our approach, Regularized Softmax (RES) Deep Multi-Agent $Q$-Learning, is general and can be applied to any $Q$-learning based MARL algorithm. We demonstrate that, when applied to QMIX, RES avoids severe overestimation and significantly improves performance, yielding state-of-the-art results in a variety of cooperative multi-agent tasks, including the challenging StarCraft II micromanagement benchmarks.",0
"The issue of overestimation in Q-learning has been extensively studied in single-agent reinforcement learning; however, it has received less attention in the multi-agent setting. This study aims to demonstrate that QMIX, a popular Q-learning algorithm for cooperative multi-agent reinforcement learning, has a more serious overestimation problem than previously acknowledged. Existing approaches have not sufficiently mitigated this problem. To address this issue, a novel regularization-based update scheme has been developed that penalizes joint action-values that deviate from a baseline, and it has proven effective in stabilizing learning. Additionally, a softmax operator is proposed to further reduce potential overestimation bias, which is efficiently approximated in a novel way in the multi-agent setting. The approach, called Regularized Softmax (RES) Deep Multi-Agent Q-Learning, is general and can be applied to any Q-learning-based multi-agent reinforcement learning algorithm. The study shows that RES, when applied to QMIX, avoids severe overestimation and significantly improves performance in various cooperative multi-agent tasks, including the challenging StarCraft II micromanagement benchmarks, resulting in state-of-the-art results.",1
"We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement Learning (RL), which extends conventional value function approximator (VFA) to take as input not only the state (and action) but also an explicit policy representation. Such an extension enables PeVFA to preserve values of multiple policies at the same time and brings an appealing characteristic, i.e., \emph{value generalization among policies}. We formally analyze the value generalization under Generalized Policy Iteration (GPI). From theoretical and empirical lens, we show that generalized value estimates offered by PeVFA may have lower initial approximation error to true values of successive policies, which is expected to improve consecutive value approximation during GPI. Based on above clues, we introduce a new form of GPI with PeVFA which leverages the value generalization along policy improvement path. Moreover, we propose a representation learning framework for RL policy, providing several approaches to learn effective policy embeddings from policy network parameters or state-action pairs. In our experiments, we evaluate the efficacy of value generalization offered by PeVFA and policy representation learning in several OpenAI Gym continuous control tasks. For a representative instance of algorithm implementation, Proximal Policy Optimization (PPO) re-implemented under the paradigm of GPI with PeVFA achieves about 40\% performance improvement on its vanilla counterpart in most environments.",0
"The Policy-extended Value Function Approximator (PeVFA) is a new approach to Reinforcement Learning (RL) that goes beyond the conventional value function approximator (VFA). Unlike the VFA, the PeVFA takes as input not only the state and action, but also an explicit policy representation. This allows the PeVFA to preserve values of multiple policies at the same time, resulting in a value generalization among policies. We analyze the value generalization under Generalized Policy Iteration (GPI) and demonstrate that the generalized value estimates offered by PeVFA may have a lower initial approximation error to true values of successive policies. We introduce a new form of GPI with PeVFA that leverages the value generalization along policy improvement path. Additionally, we propose a representation learning framework for RL policy, providing several approaches to learn effective policy embeddings from policy network parameters or state-action pairs. In our experiments, we evaluate the efficacy of value generalization offered by PeVFA and policy representation learning in several OpenAI Gym continuous control tasks. We find that Proximal Policy Optimization (PPO) re-implemented under the paradigm of GPI with PeVFA achieves about 40\% performance improvement on its vanilla counterpart in most environments.",1
"As the world seeks to become more sustainable, intelligent solutions are needed to increase the penetration of renewable energy. In this paper, the model-free deep reinforcement learning algorithm Rainbow Deep Q-Networks is used to control a battery in a small microgrid to perform energy arbitrage and more efficiently utilise solar and wind energy sources. The grid operates with its own demand and renewable generation based on a dataset collected at Keele University, as well as using dynamic energy pricing from a real wholesale energy market. Four scenarios are tested including using demand and price forecasting produced with local weather data. The algorithm and its subcomponents are evaluated against two continuous control benchmarks with Rainbow able to outperform all other method. This research shows the importance of using the distributional approach for reinforcement learning when working with complex environments and reward functions, as well as how it can be used to visualise and contextualise the agent's behaviour for real-world applications.",0
"In order to achieve greater sustainability, it is essential to implement intelligent solutions that promote the use of renewable energy sources. This study explores the application of the Rainbow Deep Q-Networks algorithm to enhance energy arbitrage and optimize the utilization of solar and wind energy sources by controlling a battery in a microgrid. The microgrid operates independently with its own demand and renewable generation, and utilizes dynamic energy pricing from a wholesale energy market. Four scenarios were tested, including demand and price forecasting based on local weather data. The Rainbow algorithm surpassed all other methods in two continuous control benchmarks. This research highlights the significance of utilizing the distributional approach in reinforcement learning for complex environments and reward functions, and demonstrates how it can be applied to real-world situations to visualize and contextualize the agent's behavior.",1
"The benefit of multi-task learning over single-task learning relies on the ability to use relations across tasks to improve performance on any single task. While sharing representations is an important mechanism to share information across tasks, its success depends on how well the structure underlying the tasks is captured. In some real-world situations, we have access to metadata, or additional information about a task, that may not provide any new insight in the context of a single task setup alone but inform relations across multiple tasks. While this metadata can be useful for improving multi-task learning performance, effectively incorporating it can be an additional challenge. We posit that an efficient approach to knowledge transfer is through the use of multiple context-dependent, composable representations shared across a family of tasks. In this framework, metadata can help to learn interpretable representations and provide the context to inform which representations to compose and how to compose them. We use the proposed approach to obtain state-of-the-art results in Meta-World, a challenging multi-task benchmark consisting of 50 distinct robotic manipulation tasks.",0
"Multi-task learning offers an advantage over single-task learning by utilizing the relationships between tasks to enhance performance on individual tasks. The sharing of representations is an important mechanism for information transfer, but its efficacy depends upon its ability to capture the underlying task structure accurately. In some cases, metadata may be available that does not offer unique insights for individual tasks but can provide valuable information for relations between multiple tasks. However, incorporating this metadata effectively can be difficult. Our proposed approach suggests that knowledge transfer can be optimized by employing multiple context-dependent, composable representations that are shared across a family of tasks. By using metadata, interpretable representations can be learned, and the context can guide the selection and composition of representations. Using this approach has allowed us to achieve state-of-the-art results in Meta-World, a challenging multi-task benchmark consisting of 50 distinct robotic manipulation tasks.",1
"VDN and QMIX are two popular value-based algorithms for cooperative MARL that learn a centralized action value function as a monotonic mixing of per-agent utilities. While this enables easy decentralization of the learned policy, the restricted joint action value function can prevent them from solving tasks that require significant coordination between agents at a given timestep. We show that this problem can be overcome by improving the joint exploration of all agents during training. Specifically, we propose a novel MARL approach called Universal Value Exploration (UneVEn) that learns a set of related tasks simultaneously with a linear decomposition of universal successor features. With the policies of already solved related tasks, the joint exploration process of all agents can be improved to help them achieve better coordination. Empirical results on a set of exploration games, challenging cooperative predator-prey tasks requiring significant coordination among agents, and StarCraft II micromanagement benchmarks show that UneVEn can solve tasks where other state-of-the-art MARL methods fail.",0
"Two commonly used algorithms for cooperative MARL are VDN and QMIX, which both learn a centralized action value function by combining per-agent utilities. However, this restricted joint action value function can limit their ability to solve tasks that require significant coordination between agents. To address this issue, our proposed approach, Universal Value Exploration (UneVEn), learns a set of related tasks simultaneously using a linear decomposition of universal successor features. By improving the joint exploration of all agents during training, UneVEn can achieve better coordination and solve tasks where other state-of-the-art MARL methods fail. Empirical results from exploration games, cooperative predator-prey tasks, and StarCraft II micromanagement benchmarks demonstrate the effectiveness of UneVEn.",1
"We study reinforcement learning (RL) with no-reward demonstrations, a setting in which an RL agent has access to additional data from the interaction of other agents with the same environment. However, it has no access to the rewards or goals of these agents, and their objectives and levels of expertise may vary widely. These assumptions are common in multi-agent settings, such as autonomous driving. To effectively use this data, we turn to the framework of successor features. This allows us to disentangle shared features and dynamics of the environment from agent-specific rewards and policies. We propose a multi-task inverse reinforcement learning (IRL) algorithm, called \emph{inverse temporal difference learning} (ITD), that learns shared state features, alongside per-agent successor features and preference vectors, purely from demonstrations without reward labels. We further show how to seamlessly integrate ITD with learning from online environment interactions, arriving at a novel algorithm for reinforcement learning with demonstrations, called $\Psi \Phi$-learning (pronounced `Sci-Fi'). We provide empirical evidence for the effectiveness of $\Psi \Phi$-learning as a method for improving RL, IRL, imitation, and few-shot transfer, and derive worst-case bounds for its performance in zero-shot transfer to new tasks.",0
"The focus of our research is reinforcement learning (RL) in a context where no-reward demonstrations are available to the agent. This means that the agent has access to data from other agents who have interacted with the same environment, but not to their rewards or goals, which may differ from its own. This is a common scenario in multi-agent settings like autonomous driving. To make use of this data, we adopt the successor features framework, which helps to distinguish between shared environment features and agent-specific rewards and policies. We introduce a multi-task inverse reinforcement learning (IRL) algorithm called inverse temporal difference learning (ITD), which learns both shared state features and per-agent successor features and preference vectors solely from demonstrations without reward labels. We also demonstrate how ITD can be combined with online environment interactions to create a new reinforcement learning with demonstrations algorithm called $\Psi \Phi$-learning (pronounced 'Sci-Fi'). We provide empirical evidence of the effectiveness of $\Psi \Phi$-learning in improving RL, IRL, imitation, and few-shot transfer, and establish worst-case bounds for its performance in zero-shot transfer to new tasks.",1
"In multi-agent reinforcement learning, the inherent non-stationarity of the environment caused by other agents' actions posed significant difficulties for an agent to learn a good policy independently. One way to deal with non-stationarity is agent modeling, by which the agent takes into consideration the influence of other agents' policies. Most existing work relies on predicting other agents' actions or goals, or discriminating between their policies. However, such modeling fails to capture the similarities and differences between policies simultaneously and thus cannot provide useful information when generalizing to unseen policies. To address this, we propose a general method to learn representations of other agents' policies via the joint-action distributions sampled in interactions. The similarities and differences between policies are naturally captured by the policy distance inferred from the joint-action distributions and deliberately reflected in the learned representations. Agents conditioned on the policy representations can well generalize to unseen agents. We empirically demonstrate that our method outperforms existing work in multi-agent tasks when facing unseen agents.",0
"Learning a good policy independently in multi-agent reinforcement learning is challenging due to the non-stationarity of the environment caused by other agents' actions. One approach to address non-stationarity is agent modeling, where an agent considers the impact of other agents' policies. However, existing methods that predict other agents' actions or goals or discriminate between their policies fail to capture similarities and differences between policies simultaneously, limiting their usefulness for generalizing to unseen policies. To address this limitation, we propose a method to learn representations of other agents' policies based on joint-action distributions sampled in interactions. Our approach captures policy distance inferred from joint-action distributions, which reflects similarities and differences between policies in learned representations. Agents conditioned on policy representations can generalize to unseen agents effectively. Empirical results demonstrate that our method outperforms existing approaches in multi-agent tasks involving unseen agents.",1
Model agnostic meta-learning (MAML) is a popular state-of-the-art meta-learning algorithm that provides good weight initialization of a model given a variety of learning tasks. The model initialized by provided weight can be fine-tuned to an unseen task despite only using a small amount of samples and within a few adaptation steps. MAML is simple and versatile but requires costly learning rate tuning and careful design of the task distribution which affects its scalability and generalization. This paper proposes a more robust MAML based on an adaptive learning scheme and a prioritization task buffer(PTB) referred to as Robust MAML (RMAML) for improving scalability of training process and alleviating the problem of distribution mismatch. RMAML uses gradient-based hyper-parameter optimization to automatically find the optimal learning rate and uses the PTB to gradually adjust train-ing task distribution toward testing task distribution over the course of training. Experimental results on meta reinforcement learning environments demonstrate a substantial performance gain as well as being less sensitive to hyper-parameter choice and robust to distribution mismatch.,0
"The widely used meta-learning algorithm, Model Agnostic Meta-Learning (MAML), is known for its ability to initialize a model with good weights for various learning tasks, allowing it to be fine-tuned for new tasks with minimal data and adaptation steps. However, MAML's scalability and generalization are hindered by the need for costly learning rate tuning and careful task distribution design. To address these issues, this paper proposes a more robust version of MAML, called Robust MAML (RMAML), which employs an adaptive learning scheme and a prioritization task buffer (PTB). RMAML utilizes gradient-based hyper-parameter optimization to automatically determine the optimal learning rate and gradually adjusts the training task distribution towards the testing task distribution during training. Experimental results on meta reinforcement learning environments indicate that RMAML outperforms MAML in terms of performance, sensitivity to hyper-parameter choice, and robustness to distribution mismatch.",1
"We consider the problem of anomaly detection with a small set of partially labeled anomaly examples and a large-scale unlabeled dataset. This is a common scenario in many important applications. Existing related methods either exclusively fit the limited anomaly examples that typically do not span the entire set of anomalies, or proceed with unsupervised learning from the unlabeled data. We propose here instead a deep reinforcement learning-based approach that enables an end-to-end optimization of the detection of both labeled and unlabeled anomalies. This approach learns the known abnormality by automatically interacting with an anomaly-biased simulation environment, while continuously extending the learned abnormality to novel classes of anomaly (i.e., unknown anomalies) by actively exploring possible anomalies in the unlabeled data. This is achieved by jointly optimizing the exploitation of the small labeled anomaly data and the exploration of the rare unlabeled anomalies. Extensive experiments on 48 real-world datasets show that our model significantly outperforms five state-of-the-art competing methods.",0
"The problem we are addressing involves detecting anomalies when given a small number of partially labeled examples and a large, unlabeled dataset. This is a common issue in various applications. Current methods either only focus on the limited labeled anomalies, which often do not represent the entirety of the anomalies, or they rely solely on unsupervised learning from the unlabeled data. Our approach utilizes deep reinforcement learning to optimize the detection of both labeled and unlabeled anomalies. Our model achieves this by learning from an anomaly-biased simulation environment and actively exploring potential anomalies in the unlabeled data to continually expand its knowledge of anomalies. We accomplish this by jointly optimizing the use of the small labeled anomaly data and the exploration of the uncommon, unlabeled anomalies. We conducted experiments on 48 datasets and found that our model outperforms five other state-of-the-art methods.",1
"Executing computer vision models on streaming visual data, or streaming perception is an emerging problem, with applications in self-driving, embodied agents, and augmented/virtual reality. The development of such systems is largely governed by the accuracy and latency of the processing pipeline. While past work has proposed numerous approximate execution frameworks, their decision functions solely focus on optimizing latency, accuracy, or energy, etc. This results in sub-optimum decisions, affecting the overall system performance. We argue that the streaming perception systems should holistically maximize the overall system performance (i.e., considering both accuracy and latency simultaneously). To this end, we describe a new approach based on deep reinforcement learning to learn these tradeoffs at runtime for streaming perception. This tradeoff optimization is formulated as a novel deep contextual bandit problem and we design a new reward function that holistically integrates latency and accuracy into a single metric. We show that our agent can learn a competitive policy across multiple decision dimensions, which outperforms state-of-the-art policies on public datasets.",0
"An emerging challenge is the execution of computer vision models on streaming visual data, also known as streaming perception. This has implications for self-driving, embodied agents, and augmented/virtual reality. The development of these systems depends on the accuracy and latency of the processing pipeline. Previous approaches have focused on optimizing individual factors such as latency, accuracy, or energy consumption, resulting in suboptimal decisions that affect overall system performance. Therefore, we propose a holistic approach that considers both accuracy and latency simultaneously. To achieve this, we introduce a new method based on deep reinforcement learning that can learn tradeoffs in real-time for streaming perception. We formulate this tradeoff optimization as a novel deep contextual bandit problem and design a new reward function that integrates both latency and accuracy into a single metric. Our results demonstrate that our approach outperforms existing state-of-the-art policies on public datasets across multiple decision dimensions.",1
"Slowly changing variables in a continuous state space constitute an important category of reinforcement learning and see its application in many domains, such as modeling a climate control system where temperature, humidity, etc. change slowly over time. However, this subject is less addressed in recent studies. Classical methods with certain variants, such as Dynamic Programming with Tile Coding which discretizes the state space, fail to handle slowly changing variables because those methods cannot capture the tiny changes in each transition step, as it is computationally expensive or impossible to establish an extremely granular grid system. In this paper, we introduce a Hyperspace Neighbor Penetration (HNP) approach that solves the problem. HNP captures in each transition step the state's partial ""penetration"" into its neighboring hyper-tiles in the gridded hyperspace, thus does not require the transition to be inter-tile in order for the change to be captured. Therefore, HNP allows for a very coarse grid system, which makes the computation feasible. HNP assumes near linearity of the transition function in a local space, which is commonly satisfied. In summary, HNP can be orders of magnitude more efficient than classical method in handling slowly changing variables in reinforcement learning. We have made an industrial implementation of NHP with a great success.",0
"Reinforcement learning involves a crucial type of variables that change slowly in a continuous state space. This is applicable in various domains, such as climate control systems where factors like temperature and humidity evolve gradually over time. However, recent research has not delved much into this subject. The traditional methods like Dynamic Programming with Tile Coding, which discretizes the state space, are insufficient in handling slowly changing variables as they cannot capture the minute changes in each transition step. This is because creating a highly granular grid system is computationally expensive or impossible. To address this issue, we propose a Hyperspace Neighbor Penetration (HNP) approach that captures the state's partial ""penetration"" into its neighboring hyper-tiles in the gridded hyperspace. HNP does not require the transition to be inter-tile in order for the change to be captured, and thus allows for a very coarse grid system, which makes computation feasible. HNP can be significantly more efficient than classical methods in handling slowly changing variables in reinforcement learning. We have successfully implemented the HNP approach in an industrial setting.",1
"The segmentation of nanoscale electron microscopy (EM) images is crucial but challenging in connectomics. Recent advances in deep learning have demonstrated the significant potential of automatic segmentation for tera-scale EM images. However, none of the existing segmentation methods are error-free, and they require proofreading, which is typically implemented as an interactive, semi-automatic process via manual intervention. Herein, we propose a fully automatic proofreading method based on reinforcement learning. The main idea is to model the human decision process in proofreading using a reinforcement agent to achieve fully automatic proofreading. We systematically design the proposed system by combining multiple reinforcement learning agents in a hierarchical manner, where each agent focuses only on a specific task while preserving dependency between agents. Furthermore, we also demonstrate that the episodic task setting of reinforcement learning can efficiently manage a combination of merge and split errors concurrently presented in the input. We demonstrate the efficacy of the proposed system by comparing it with state-of-the-art proofreading methods using various testing examples.",0
"Segmenting nanoscale electron microscopy (EM) images is essential yet difficult in connectomics. Recent deep learning advancements have shown promise for automatic segmentation of tera-scale EM images, but current methods are not error-free and require proofreading through manual intervention. Our proposed solution is a fully automatic proofreading method utilizing reinforcement learning. The reinforcement agent models the human decision process to achieve complete automation. We designed the system hierarchically, combining multiple reinforcement learning agents that focus on specific tasks while maintaining interdependence. Additionally, our system efficiently handles errors of both merge and split types through the episodic task setting of reinforcement learning. We have compared our solution with state-of-the-art proofreading methods using various testing examples and demonstrated its effectiveness.",1
"AlphaStar, the AI that reaches GrandMaster level in StarCraft II, is a remarkable milestone demonstrating what deep reinforcement learning can achieve in complex Real-Time Strategy (RTS) games. However, the complexities of the game, algorithms and systems, and especially the tremendous amount of computation needed are big obstacles for the community to conduct further research in this direction. We propose a deep reinforcement learning agent, StarCraft Commander (SCC). With order of magnitude less computation, it demonstrates top human performance defeating GrandMaster players in test matches and top professional players in a live event. Moreover, it shows strong robustness to various human strategies and discovers novel strategies unseen from human plays. In this paper, we will share the key insights and optimizations on efficient imitation learning and reinforcement learning for StarCraft II full game.",0
"The AI named AlphaStar, which achieved GrandMaster level in the game StarCraft II, is a significant achievement that showcases the capabilities of deep reinforcement learning in complex Real-Time Strategy (RTS) games. However, the game's complexity, algorithms, systems, and the enormous amount of computation required are major obstacles for further research in this area. To address this, we present StarCraft Commander (SCC), a deep reinforcement learning agent that requires significantly less computation. It has demonstrated top human-like performance by defeating GrandMaster players in test matches and top professional players in a live event. SCC has also proven to be highly robust against various human strategies and has discovered novel strategies that were not previously observed in human gameplay. This paper will discuss the key insights and optimizations made in efficient imitation learning and reinforcement learning for the full game of StarCraft II.",1
"To rapidly learn a new task, it is often essential for agents to explore efficiently -- especially when performance matters from the first timestep. One way to learn such behaviour is via meta-learning. Many existing methods however rely on dense rewards for meta-training, and can fail catastrophically if the rewards are sparse. Without a suitable reward signal, the need for exploration during meta-training is exacerbated. To address this, we propose HyperX, which uses novel reward bonuses for meta-training to explore in approximate hyper-state space (where hyper-states represent the environment state and the agent's task belief). We show empirically that HyperX meta-learns better task-exploration and adapts more successfully to new tasks than existing methods.",0
"Efficient exploration is crucial for agents to quickly master a new task, particularly when immediate performance is important. Meta-learning is a way to acquire this skill, but traditional methods require rich rewards during meta-training and can be disastrous when rewards are scarce. This exacerbates the need for exploration during meta-training. To tackle this issue, we introduce HyperX, which employs fresh reward incentives to explore in an approximate hyper-state space, representing both the environment state and the agent's task belief. We offer empirical evidence that HyperX outperforms existing methods in meta-learning task exploration and adapting to novel tasks.",1
"Within the framework of Multi-Agent Reinforcement Learning, Social Learning is a new class of algorithms that enables agents to reshape the reward function of other agents with the goal of promoting cooperation and achieving higher global rewards in mixed-motive games. However, this new modification allows agents unprecedented access to each other's learning process, which can drastically increase the risk of manipulation when an agent does not realize it is being deceived into adopting policies which are not actually in its own best interest. This research review introduces the problem statement, defines key concepts, critically evaluates existing evidence and addresses open problems that should be addressed in future research.",0
"Social Learning is a recently developed category of algorithms in Multi-Agent Reinforcement Learning that facilitates agents in modifying the reward function of other agents to encourage collaboration and attain greater global rewards in mixed-motive games. Nevertheless, this alteration allows agents to access each other's learning processes to an unparalleled extent, posing a significant risk of manipulation when an agent is unaware of being deceived into adopting policies that do not align with its best interest. This review of research presents the issue, explains essential terms, assesses current evidence, and underscores unresolved issues that require further investigation.",1
"We consider the problem of online reinforcement learning for the Stochastic Shortest Path (SSP) problem modeled as an unknown MDP with an absorbing state. We propose PSRL-SSP, a simple posterior sampling-based reinforcement learning algorithm for the SSP problem. The algorithm operates in epochs. At the beginning of each epoch, a sample is drawn from the posterior distribution on the unknown model dynamics, and the optimal policy with respect to the drawn sample is followed during that epoch. An epoch completes if either the number of visits to the goal state in the current epoch exceeds that of the previous epoch, or the number of visits to any of the state-action pairs is doubled. We establish a Bayesian regret bound of $O(B_\star S\sqrt{AK})$, where $B_\star$ is an upper bound on the expected cost of the optimal policy, $S$ is the size of the state space, $A$ is the size of the action space, and $K$ is the number of episodes. The algorithm only requires the knowledge of the prior distribution, and has no hyper-parameters to tune. It is the first such posterior sampling algorithm and outperforms numerically previously proposed optimism-based algorithms.",0
"The aim of our study is to address online reinforcement learning for the Stochastic Shortest Path (SSP) problem, which is represented by an unknown MDP with an absorbing state. We have introduced an algorithm called PSRL-SSP that uses posterior sampling to solve the SSP problem. The algorithm operates in epochs. Each epoch begins with sampling from the posterior distribution on the unknown model dynamics. The optimal policy with respect to the drawn sample is followed throughout the epoch. The epoch completes if either the number of visits to the goal state exceeds that of the previous epoch or the number of visits to any of the state-action pairs is doubled. We have established a Bayesian regret bound of $O(B_\star S\sqrt{AK})$, where $B_\star$ is the expected cost of the optimal policy, $S$ is the state space size, $A$ is the action space size, and $K$ is the number of episodes. The algorithm only requires knowledge of the prior distribution and does not need any hyper-parameters to be tuned. This algorithm is the first posterior sampling algorithm that outperforms previous optimism-based algorithms in numerical experiments.",1
"Reinforcement learning is a powerful approach to learn behaviour through interactions with an environment. However, behaviours are usually learned in a purely reactive fashion, where an appropriate action is selected based on an observation. In this form, it is challenging to learn when it is necessary to execute new decisions. This makes learning inefficient, especially in environments that need various degrees of fine and coarse control. To address this, we propose a proactive setting in which the agent not only selects an action in a state but also for how long to commit to that action. Our TempoRL approach introduces skip connections between states and learns a skip-policy for repeating the same action along these skips. We demonstrate the effectiveness of TempoRL on a variety of traditional and deep RL environments, showing that our approach is capable of learning successful policies up to an order of magnitude faster than vanilla Q-learning.",0
"The process of learning behaviour through interactions with an environment can be greatly enhanced through reinforcement learning. However, the typical approach involves purely reactive behaviour, where actions are based solely on observations. This can be problematic when it comes to learning when to make new decisions, leading to inefficiencies in learning, especially in environments with varying degrees of control. To solve this issue, a proactive setting is proposed in which the agent not only selects actions but also determines how long to commit to them. Our approach, called TempoRL, introduces skip connections and a skip-policy to repeat actions along these skips. By applying this approach to traditional and deep RL environments, we have shown that TempoRL can successfully learn policies up to ten times faster than vanilla Q-learning.",1
"Catastrophic forgetting remains a severe hindrance to the broad application of artificial neural networks (ANNs), however, it continues to be a poorly understood phenomenon. Despite the extensive amount of work on catastrophic forgetting, we argue that it is still unclear how exactly the phenomenon should be quantified, and, moreover, to what degree all of the choices we make when designing learning systems affect the amount of catastrophic forgetting. We use various testbeds from the reinforcement learning and supervised learning literature to (1) provide evidence that the choice of which modern gradient-based optimization algorithm is used to train an ANN has a significant impact on the amount of catastrophic forgetting and show that-surprisingly-in many instances classical algorithms such as vanilla SGD experience less catastrophic forgetting than the more modern algorithms such as Adam. We empirically compare four different existing metrics for quantifying catastrophic forgetting and (2) show that the degree to which the learning systems experience catastrophic forgetting is sufficiently sensitive to the metric used that a change from one principled metric to another is enough to change the conclusions of a study dramatically. Our results suggest that a much more rigorous experimental methodology is required when looking at catastrophic forgetting. Based on our results, we recommend inter-task forgetting in supervised learning must be measured with both retention and relearning metrics concurrently, and intra-task forgetting in reinforcement learning must-at the very least-be measured with pairwise interference.",0
"Although catastrophic forgetting remains a major obstacle to the widespread use of artificial neural networks (ANNs), our understanding of this phenomenon is still limited. Despite extensive research, it is unclear how to precisely quantify catastrophic forgetting and how various design choices affect its occurrence. We conducted experiments on reinforcement learning and supervised learning testbeds and discovered that the choice of optimization algorithm used to train ANNs has a significant impact on the amount of catastrophic forgetting experienced. Surprisingly, classical algorithms like vanilla SGD resulted in less catastrophic forgetting than modern alternatives like Adam. We also compared four different metrics for quantifying catastrophic forgetting and found that the chosen metric can dramatically affect study conclusions. Our results emphasize the need for a more rigorous experimental methodology when studying catastrophic forgetting. We recommend using both retention and relearning metrics concurrently to measure inter-task forgetting in supervised learning and pairwise interference to measure intra-task forgetting in reinforcement learning.",1
"Data-efficiency and generalization are key challenges in deep learning and deep reinforcement learning as many models are trained on large-scale, domain-specific, and expensive-to-label datasets. Self-supervised models trained on large-scale uncurated datasets have shown successful transfer to diverse settings. We investigate using pretrained image representations and spatio-temporal attention for state representation learning in Atari. We also explore fine-tuning pretrained representations with self-supervised techniques, i.e., contrastive predictive coding, spatio-temporal contrastive learning, and augmentations. Our results show that pretrained representations are at par with state-of-the-art self-supervised methods trained on domain-specific data. Pretrained representations, thus, yield data and compute-efficient state representations. https://github.com/PAL-ML/PEARL_v1",0
"Deep learning and deep reinforcement learning face challenges related to data-efficiency and generalization due to the need for large-scale, domain-specific, and costly-to-label datasets. However, successful transfer to diverse settings has been observed in self-supervised models trained on large-scale uncurated datasets. In our study, we investigate the use of pretrained image representations and spatio-temporal attention for state representation learning in Atari. We also explore the fine-tuning of pretrained representations with self-supervised techniques such as contrastive predictive coding, spatio-temporal contrastive learning, and augmentations. Our findings reveal that pretrained representations are comparable to state-of-the-art self-supervised methods that are trained on domain-specific data. As a result, pretrained representations offer data and compute-efficient state representations.",1
"Recently numerous machine learning based methods for combinatorial optimization problems have been proposed that learn to construct solutions in a sequential decision process via reinforcement learning. While these methods can be easily combined with search strategies like sampling and beam search, it is not straightforward to integrate them into a high-level search procedure offering strong search guidance. Bello et al. (2016) propose active search, which adjusts the weights of a (trained) model with respect to a single instance at test time using reinforcement learning. While active search is simple to implement, it is not competitive with state-of-the-art methods because adjusting all model weights for each test instance is very time and memory intensive. Instead of updating all model weights, we propose and evaluate three efficient active search strategies that only update a subset of parameters during the search. The proposed methods offer a simple way to significantly improve the search performance of a given model and outperform state-of-the-art machine learning based methods on combinatorial problems, even surpassing the well-known heuristic solver LKH3 on the capacitated vehicle routing problem. Finally, we show that (efficient) active search enables learned models to effectively solve instances that are much larger than those seen during training.",0
"There has been a recent surge in the development of machine learning approaches for solving combinatorial optimization problems. These methods use reinforcement learning to construct solutions in a sequential decision-making process. Although these methods can be combined with search strategies such as sampling and beam search, it is difficult to integrate them into a high-level search procedure that provides strong search guidance. Bello et al. (2016) have proposed active search, which adjusts the weights of a trained model with respect to a single instance at test time using reinforcement learning. However, updating all model weights for each test instance is time and memory intensive. To address this, we propose and evaluate three active search strategies that update only a subset of model parameters during the search. These methods offer a simple way to improve the search performance of a given model and outperform state-of-the-art machine learning methods, including LKH3, on the capacitated vehicle routing problem. Finally, we demonstrate that efficient active search enables learned models to effectively solve larger instances than those seen during training.",1
"Reinforcement learning (RL) has made a lot of advances for solving a single problem in a given environment; but learning policies that generalize to unseen variations of a problem remains challenging. To improve sample efficiency for learning on such instances of a problem domain, we present Self-Paced Context Evaluation (SPaCE). Based on self-paced learning, \spc automatically generates \task curricula online with little computational overhead. To this end, SPaCE leverages information contained in state values during training to accelerate and improve training performance as well as generalization capabilities to new instances from the same problem domain. Nevertheless, SPaCE is independent of the problem domain at hand and can be applied on top of any RL agent with state-value function approximation. We demonstrate SPaCE's ability to speed up learning of different value-based RL agents on two environments, showing better generalization capabilities and up to 10x faster learning compared to naive approaches such as round robin or SPDRL, as the closest state-of-the-art approach.",0
"Despite the advances made in Reinforcement Learning (RL) for solving a single problem within a specific environment, it still remains challenging to develop policies that can be applied to unseen variations of the same problem. In order to enhance the efficiency of learning on such instances of a problem domain, we introduce Self-Paced Context Evaluation (SPaCE). Utilizing the principles of self-paced learning, SPaCE generates task curricula automatically with minimal computational overhead. By utilizing the information in state values during training, SPaCE accelerates and enhances training performance, as well as generalization capabilities to new instances from the same problem domain. SPaCE is equally effective across all problem domains and can be applied to any RL agent that uses state-value function approximation. Our experiments demonstrate that SPaCE can expedite the learning of different value-based RL agents on two environments, with superior generalization capabilities and up to 10x faster learning than naive approaches such as round robin or SPDRL, which is the closest state-of-the-art approach.",1
"Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",0
"It can be challenging to communicate intricate objectives to reinforcement learning (RL) agents, as it requires the careful creation of reward functions that are both informative and easy to understand. One solution is to use human-in-the-loop RL methods, where practitioners can teach agents through tailored feedback. Unfortunately, this approach is expensive and difficult to scale. To address this issue, we present an off-policy, interactive RL algorithm that combines the strengths of feedback and off-policy learning. Our algorithm queries a teacher's preferences between two sets of behavior, uses this information to train an agent, and relabels all past experiences once the reward model changes. Our approach can handle complex tasks, including locomotion and robotic manipulation skills, and can effectively use real-time human feedback to prevent reward exploitation and learn new behaviors that are challenging to specify with standard reward functions. We also show that pre-training agents with unsupervised exploration can improve the efficiency of the algorithm.",1
"Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. In this paper, we propose a novel attacking algorithm which has an RL-based ""director"" searching for the optimal policy perturbation, and an ""actor"" crafting state perturbations following the directions from the director (i.e. the actor executes targeted attacks). Our proposed algorithm, PA-AD, is theoretically optimal against an RL agent and significantly improves the efficiency compared with prior RL-based works in environments with large or pixel state spaces. Empirical results show that our proposed PA-AD universally outperforms state-of-the-art attacking methods in a wide range of environments. Our method can be easily applied to any RL algorithms to evaluate and improve their robustness.",0
"Assessing the most extreme potential outcomes of a reinforcement learning (RL) agent when faced with the most potent and idealized adversarial perturbations on state observations (within certain limitations) is crucial for comprehending the durability of RL agents. However, discovering the optimal adversary can be challenging, both in terms of finding it and doing so efficiently. Previous research on adversarial RL has either relied on heuristic-based methods that may not be able to find the most potent adversary or trained an RL-based adversary by considering the agent as part of the environment, which can locate the optimal adversary but may become unmanageable in a vast state space. In this paper, we introduce a new attacking algorithm that employs an RL-based ""director"" to search for the optimal policy perturbation and an ""actor"" that crafts state perturbations following the director's directions (i.e., the actor performs targeted attacks). Our proposed algorithm, PA-AD, is theoretically optimal when it comes to an RL agent and significantly enhances efficiency compared to prior RL-based works in environments with extensive or pixel state spaces. Empirical results demonstrate that our proposed PA-AD outperforms state-of-the-art attacking methods across various environments. Our method is readily applicable to evaluate and improve the robustness of any RL algorithms.",1
"The objective of offline RL is to learn optimal policies when a fixed exploratory demonstrations data-set is available and sampling additional observations is impossible (typically if this operation is either costly or rises ethical questions). In order to solve this problem, off the shelf approaches require a properly defined cost function (or its evaluation on the provided data-set), which are seldom available in practice. To circumvent this issue, a reasonable alternative is to query an expert for few optimal demonstrations in addition to the exploratory data-set. The objective is then to learn an optimal policy w.r.t. the expert's latent cost function. Current solutions either solve a behaviour cloning problem (which does not leverage the exploratory data) or a reinforced imitation learning problem (using a fixed cost function that discriminates available exploratory trajectories from expert ones). Inspired by the success of IRL techniques in achieving state of the art imitation performances in online settings, we exploit GAN based data augmentation procedures to construct the first offline IRL algorithm. The obtained policies outperformed the aforementioned solutions on multiple OpenAI gym environments.",0
"Offline RL aims to achieve optimal policies using a pre-existing exploratory demonstrations dataset without the need for additional observations, which may be too expensive or ethically questionable. However, existing approaches require a well-defined cost function, which is often unavailable in practice. To overcome this, one alternative is to obtain optimal demonstrations from experts and learn an optimal policy based on their latent cost function. Current solutions either use behaviour cloning without leveraging the exploratory data or reinforced imitation learning with a fixed cost function. To address this, we developed the first offline IRL algorithm using GAN-based data augmentation techniques. Our approach outperformed existing solutions on multiple OpenAI gym environments.",1
"Training autonomous agents able to generalize to multiple tasks is a key target of Deep Reinforcement Learning (DRL) research. In parallel to improving DRL algorithms themselves, Automatic Curriculum Learning (ACL) study how teacher algorithms can train DRL agents more efficiently by adapting task selection to their evolving abilities. While multiple standard benchmarks exist to compare DRL agents, there is currently no such thing for ACL algorithms. Thus, comparing existing approaches is difficult, as too many experimental parameters differ from paper to paper. In this work, we identify several key challenges faced by ACL algorithms. Based on these, we present TeachMyAgent (TA), a benchmark of current ACL algorithms leveraging procedural task generation. It includes 1) challenge-specific unit-tests using variants of a procedural Box2D bipedal walker environment, and 2) a new procedural Parkour environment combining most ACL challenges, making it ideal for global performance assessment. We then use TeachMyAgent to conduct a comparative study of representative existing approaches, showcasing the competitiveness of some ACL algorithms that do not use expert knowledge. We also show that the Parkour environment remains an open problem. We open-source our environments, all studied ACL algorithms (collected from open-source code or re-implemented), and DRL students in a Python package available at https://github.com/flowersteam/TeachMyAgent.",0
"Deep Reinforcement Learning (DRL) research aims to train autonomous agents that can generalize to multiple tasks. To achieve this, Automatic Curriculum Learning (ACL) investigates how teacher algorithms can efficiently train DRL agents by adapting task selection based on their evolving abilities. Although there are multiple standard benchmarks available to compare DRL agents, there is currently no such benchmark for ACL algorithms, making it difficult to compare different approaches. In this study, we identify key challenges faced by ACL algorithms and introduce TeachMyAgent (TA), a benchmark that utilizes procedural task generation. TA includes challenge-specific unit-tests using variants of a procedural Box2D bipedal walker environment and a new procedural Parkour environment that combines most ACL challenges for global performance assessment. We use TA to conduct a comparative study of representative existing approaches and demonstrate the competitiveness of some ACL algorithms that do not require expert knowledge. However, we also show that the Parkour environment remains an open problem. We have made our environments, ACL algorithms, and DRL students available in a Python package on GitHub.",1
"We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors. We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function.",0
"Our proposal is to enable more informed decision-making in Reinforcement Learning (RL) by distinguishing reversible from irreversible actions. We demonstrate that approximate reversibility can be learned through a straightforward surrogate task: ranking randomly sampled trajectory events in chronological order. By identifying pairs of events that consistently occur in the same order, we can infer the presence of an irreversible sequence of actions between them. This temporal order learning can be accomplished entirely through self-supervised methods, enabling us to estimate reversibility from experience without any prior knowledge. We present two approaches for incorporating reversibility into RL agents: one for exploration (RAE) and one for control (RAC). By applying these strategies in various environments, including the complex Sokoban game, we showcase the potential for reversibility-aware agents to learn control policies that are free of failures and minimize the effects of interactions, all without using the reward function.",1
"Concave Utility Reinforcement Learning (CURL) extends RL from linear to concave utilities in the occupancy measure induced by the agent's policy. This encompasses not only RL but also imitation learning and exploration, among others. Yet, this more general paradigm invalidates the classical Bellman equations, and calls for new algorithms. Mean-field Games (MFGs) are a continuous approximation of many-agent RL. They consider the limit case of a continuous distribution of identical agents, anonymous with symmetric interests, and reduce the problem to the study of a single representative agent in interaction with the full population. Our core contribution consists in showing that CURL is a subclass of MFGs. We think this important to bridge together both communities. It also allows to shed light on aspects of both fields: we show the equivalence between concavity in CURL and monotonicity in the associated MFG, between optimality conditions in CURL and Nash equilibrium in MFG, or that Fictitious Play (FP) for this class of MFGs is simply Frank-Wolfe, bringing the first convergence rate for discrete-time FP for MFGs. We also experimentally demonstrate that, using algorithms recently introduced for solving MFGs, we can address the CURL problem more efficiently.",0
"CURL is an extension of RL that applies to concave utilities in the agent's policy-induced occupancy measure. This includes imitation learning and exploration, among others. However, this paradigm is more general than RL and requires new algorithms as it invalidates classical Bellman equations. MFGs are a continuous approximation of many-agent RL that reduces the problem to a single representative agent in interaction with the entire population. Our contribution is demonstrating that CURL is a subclass of MFGs, which bridges both communities and provides insight into each field. We show equivalence between concavity in CURL and monotonicity in MFG, optimality conditions in CURL and Nash equilibrium in MFG, and that FP for this class of MFGs is simply Frank-Wolfe, with the first convergence rate for discrete-time FP for MFGs. Using recently introduced algorithms for solving MFGs, we demonstrate that we can address CURL more efficiently.",1
"Recent theoretical work studies sample-efficient reinforcement learning (RL) extensively in two settings: learning interactively in the environment (online RL), or learning from an offline dataset (offline RL). However, existing algorithms and theories for learning near-optimal policies in these two settings are rather different and disconnected. Towards bridging this gap, this paper initiates the theoretical study of policy finetuning, that is, online RL where the learner has additional access to a ""reference policy"" $\mu$ close to the optimal policy $\pi_\star$ in a certain sense. We consider the policy finetuning problem in episodic Markov Decision Processes (MDPs) with $S$ states, $A$ actions, and horizon length $H$. We first design a sharp offline reduction algorithm -- which simply executes $\mu$ and runs offline policy optimization on the collected dataset -- that finds an $\varepsilon$ near-optimal policy within $\widetilde{O}(H^3SC^\star/\varepsilon^2)$ episodes, where $C^\star$ is the single-policy concentrability coefficient between $\mu$ and $\pi_\star$. This offline result is the first that matches the sample complexity lower bound in this setting, and resolves a recent open question in offline RL. We then establish an $\Omega(H^3S\min\{C^\star, A\}/\varepsilon^2)$ sample complexity lower bound for any policy finetuning algorithm, including those that can adaptively explore the environment. This implies that -- perhaps surprisingly -- the optimal policy finetuning algorithm is either offline reduction or a purely online RL algorithm that does not use $\mu$. Finally, we design a new hybrid offline/online algorithm for policy finetuning that achieves better sample complexity than both vanilla offline reduction and purely online RL algorithms, in a relaxed setting where $\mu$ only satisfies concentrability partially up to a certain time step.",0
"Theoretical research has recently focused on sample-efficient reinforcement learning (RL) in two contexts: online RL, where learning occurs interactively in the environment, and offline RL, where learning is based on a dataset. However, current learning algorithms and theories for achieving near-optimal policies in these two contexts are distinct and lack coherence. To address this issue, this study introduces the concept of policy finetuning, which involves online RL with access to a ""reference policy"" $\mu$ that approximates the optimal policy $\pi_\star$ to some extent. The study examines policy finetuning in episodic Markov Decision Processes (MDPs) with $S$ states, $A$ actions, and horizon length $H$. An offline reduction algorithm is developed, which uses $\mu$ to execute offline policy optimization on a collected dataset, resulting in an $\varepsilon$ near-optimal policy after $\widetilde{O}(H^3SC^\star/\varepsilon^2)$ episodes, where $C^\star$ is the single-policy concentrability coefficient between $\mu$ and $\pi_\star$. This offline result is the first to match the sample complexity lower bound and resolves a recent open question in offline RL. An $\Omega(H^3S\min\{C^\star, A\}/\varepsilon^2)$ sample complexity lower bound is also established for any policy finetuning algorithm, including those that can adaptively explore the environment. This finding suggests that the optimal policy finetuning algorithm is either offline reduction or a purely online RL algorithm that does not use $\mu$. Finally, a new hybrid offline/online algorithm for policy finetuning is developed, which achieves better sample complexity than both vanilla offline reduction and purely online RL algorithms, under a relaxed setting where $\mu$ only partially satisfies concentrability up to a certain time step.",1
"Data efficiency is a key challenge for deep reinforcement learning. We address this problem by using unlabeled data to pretrain an encoder which is then finetuned on a small amount of task-specific data. To encourage learning representations which capture diverse aspects of the underlying MDP, we employ a combination of latent dynamics modelling and unsupervised goal-conditioned RL. When limited to 100k steps of interaction on Atari games (equivalent to two hours of human experience), our approach significantly surpasses prior work combining offline representation pretraining with task-specific finetuning, and compares favourably with other pretraining methods that require orders of magnitude more data. Our approach shows particular promise when combined with larger models as well as more diverse, task-aligned observational data -- approaching human-level performance and data-efficiency on Atari in our best setting. We provide code associated with this work at https://github.com/mila-iqia/SGI.",0
"Deep reinforcement learning faces a major obstacle in terms of data efficiency. Our strategy for overcoming this issue involves utilizing unlabeled data to pretrain an encoder, which is then fine-tuned with a limited amount of task-specific data. To ensure that the representations captured by the encoder encompass a broad range of features in the underlying MDP, we incorporate latent dynamics modeling and unsupervised goal-conditioned RL. Even with only 100k steps of interaction on Atari games, equivalent to two hours of human experience, our approach outperforms previous methods that combine offline representation pretraining with task-specific finetuning, and is comparable to other pretraining techniques that require significantly more data. Our method shows particular promise when used in conjunction with larger models and a more varied, task-aligned observational data set, approaching human-level performance and data efficiency on Atari in our optimal setting. The code for our work is available at https://github.com/mila-iqia/SGI.",1
"In this paper, sample-aware policy entropy regularization is proposed to enhance the conventional policy entropy regularization for better exploration. Exploiting the sample distribution obtainable from the replay buffer, the proposed sample-aware entropy regularization maximizes the entropy of the weighted sum of the policy action distribution and the sample action distribution from the replay buffer for sample-efficient exploration. A practical algorithm named diversity actor-critic (DAC) is developed by applying policy iteration to the objective function with the proposed sample-aware entropy regularization. Numerical results show that DAC significantly outperforms existing recent algorithms for reinforcement learning.",0
"The conventional policy entropy regularization can be improved for better exploration through the proposed sample-aware policy entropy regularization, as explained in this paper. By utilizing the sample distribution available from the replay buffer, the proposed approach maximizes the entropy of the combined policy action distribution and the sample action distribution. This results in efficient exploration of the samples. An algorithm called diversity actor-critic (DAC) is created using policy iteration and the proposed sample-aware entropy regularization. The numerical results indicate that DAC outperforms other existing reinforcement learning algorithms.",1
"We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong objective. This kind of failure presents different risks than the robustness problems usually considered in the literature, since it involves agents that leverage their capabilities to pursue the wrong objective rather than simply failing to do anything useful. We provide the first explicit empirical demonstrations of objective robustness failures and present a partial characterization of its causes.",0
"Our focus in this study is on objective robustness failures, which are a type of reinforcement learning (RL) failure that occurs when an RL agent maintains its capabilities outside of its normal distribution but chooses the wrong objective. This type of failure poses unique risks compared to the typical robustness issues found in literature since it involves agents that misuse their capabilities to pursue the wrong objective instead of merely being useless. Our research showcases the first documented empirical evidence of objective robustness failures and offers a partial explanation for its underlying causes.",1
"We consider the problem of teaching via demonstrations in sequential decision-making settings. In particular, we study how to design a personalized curriculum over demonstrations to speed up the learner's convergence. We provide a unified curriculum strategy for two popular learner models: Maximum Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy Behavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over demonstrations based on a notion of difficulty scores computed w.r.t. the teacher's optimal policy and the learner's current policy. Compared to the state of the art, our strategy doesn't require access to the learner's internal dynamics and still enjoys similar convergence guarantees under mild technical conditions. Furthermore, we adapt our curriculum strategy to teach a learner using domain knowledge in the form of task-specific difficulty scores when the teacher's optimal policy is unknown. Experiments on a car driving simulator environment and shortest path problems in a grid-world environment demonstrate the effectiveness of our proposed curriculum strategy.",0
"The focus of our research is on utilizing demonstrations to teach sequential decision-making, and we aim to enhance the learner's progress by creating a tailored curriculum. We have developed a comprehensive approach that can be applied to two popular learner models, MaxEnt-IRL and CrossEnt-BC. Our strategy ranks demonstrations based on their difficulty score, which is determined by the teacher's optimal policy and the learner's current policy. Our approach outperforms existing methods by not requiring access to the learner's internal dynamics, while still achieving comparable results under mild technical conditions. Additionally, we have adapted our curriculum to incorporate domain knowledge in situations where the teacher's optimal policy is unknown. Our proposed curriculum strategy has been tested in various environments, including a car driving simulator and grid-world problems, and has shown to be effective.",1
"We study the problem of robust reinforcement learning under adversarial corruption on both rewards and transitions. Our attack model assumes an \textit{adaptive} adversary who can arbitrarily corrupt the reward and transition at every step within an episode, for at most $\epsilon$-fraction of the learning episodes. Our attack model is strictly stronger than those considered in prior works. Our first result shows that no algorithm can find a better than $O(\epsilon)$-optimal policy under our attack model. Next, we show that surprisingly the natural policy gradient (NPG) method retains a natural robustness property if the reward corruption is bounded, and can find an $O(\sqrt{\epsilon})$-optimal policy. Consequently, we develop a Filtered Policy Gradient (FPG) algorithm that can tolerate even unbounded reward corruption and can find an $O(\epsilon^{1/4})$-optimal policy. We emphasize that FPG is the first that can achieve a meaningful learning guarantee when a constant fraction of episodes are corrupted. Complimentary to the theoretical results, we show that a neural implementation of FPG achieves strong robust learning performance on the MuJoCo continuous control benchmarks.",0
"Our focus is on studying robust reinforcement learning in the presence of adversarial corruption affecting both rewards and transitions. Under our attack model, we assume an adversary who is adaptive and can corrupt rewards and transitions arbitrarily within an episode, for a maximum of $\epsilon$ fraction of learning episodes. Our attack model is stronger than those previously considered. Our first finding is that no algorithm can achieve better than $O(\epsilon)$-optimal policy under our attack model. Surprisingly, we also show that the natural policy gradient (NPG) method is naturally robust if the reward corruption is limited and can achieve an $O(\sqrt{\epsilon})$-optimal policy. We therefore develop a Filtered Policy Gradient (FPG) algorithm that can tolerate even unbounded reward corruption and can find an $O(\epsilon^{1/4})$-optimal policy. Notably, FPG is the first algorithm to provide a meaningful learning guarantee when a constant fraction of episodes is corrupted. We demonstrate the strong robust learning performance of a neural implementation of FPG on the MuJoCo continuous control benchmarks.",1
"Credit assignment is a fundamental problem in reinforcement learning, the problem of measuring an action's influence on future rewards. Improvements in credit assignment methods have the potential to boost the performance of RL algorithms on many tasks, but thus far have not seen widespread adoption. Recently, a family of methods called Hindsight Credit Assignment (HCA) was proposed, which explicitly assign credit to actions in hindsight based on the probability of the action having led to an observed outcome. This approach is appealing as a means to more efficient data usage, but remains a largely theoretical idea applicable to a limited set of tabular RL tasks, and it is unclear how to extend HCA to Deep RL environments. In this work, we explore the use of HCA-style credit in a deep RL context. We first describe the limitations of existing HCA algorithms in deep RL, then propose several theoretically-justified modifications to overcome them. Based on this exploration, we present a new algorithm, Credit-Constrained Advantage Actor-Critic (C2A2C), which ignores policy updates for actions which don't affect future outcomes based on credit in hindsight, while updating the policy as normal for those that do. We find that C2A2C outperforms Advantage Actor-Critic (A2C) on the Arcade Learning Environment (ALE) benchmark, showing broad improvements over A2C and motivating further work on credit-constrained update rules for deep RL methods.",0
"Reinforcement learning faces a significant problem of credit assignment, which involves measuring the impact of an action on future rewards. Although credit assignment methods have the potential to enhance the performance of RL algorithms in various tasks, their adoption remains limited. The recent introduction of Hindsight Credit Assignment (HCA) methods has provided a more efficient data usage approach, but it still lacks practical application in tabular RL tasks and has not been extended to Deep RL environments. In this study, we examine the use of HCA-style credit in a deep RL context and propose modifications to overcome the limitations of existing HCA algorithms. We present a new algorithm, Credit-Constrained Advantage Actor-Critic (C2A2C), which updates the policy for actions that affect future outcomes based on credit in hindsight while ignoring policy updates for those that don't. Our results show that C2A2C outperforms Advantage Actor-Critic (A2C) on the Arcade Learning Environment (ALE) benchmark, which highlights the potential of credit-constrained update rules for deep RL methods and encourages further research in this area.",1
"Recent works on ride-sharing order dispatching have highlighted the importance of taking into account both the spatial and temporal dynamics in the dispatching process for improving the transportation system efficiency. At the same time, deep reinforcement learning has advanced to the point where it achieves superhuman performance in a number of fields. In this work, we propose a deep reinforcement learning based solution for order dispatching and we conduct large scale online A/B tests on DiDi's ride-dispatching platform to show that the proposed method achieves significant improvement on both total driver income and user experience related metrics. In particular, we model the ride dispatching problem as a Semi Markov Decision Process to account for the temporal aspect of the dispatching actions. To improve the stability of the value iteration with nonlinear function approximators like neural networks, we propose Cerebellar Value Networks (CVNet) with a novel distributed state representation layer. We further derive a regularized policy evaluation scheme for CVNet that penalizes large Lipschitz constant of the value network for additional robustness against adversarial perturbation and noises. Finally, we adapt various transfer learning methods to CVNet for increased learning adaptability and efficiency across multiple cities. We conduct extensive offline simulations based on real dispatching data as well as online AB tests through the DiDi's platform. Results show that CVNet consistently outperforms other recently proposed dispatching methods. We finally show that the performance can be further improved through the efficient use of transfer learning.",0
"Recent research has emphasized the significance of incorporating both spatial and temporal dynamics in ride-sharing order dispatching to enhance transportation system efficiency. Deep reinforcement learning has made significant progress in various fields, achieving superhuman performance. This study proposes a deep reinforcement learning-based approach for order dispatching and conducts large-scale online A/B tests on DiDi's ride-dispatching platform, demonstrating significant improvements in both total driver income and user experience metrics. To consider the temporal aspect of dispatching actions, the ride dispatching problem is modeled as a Semi Markov Decision Process. To enhance the stability of the value iteration with nonlinear function approximators, such as neural networks, Cerebellar Value Networks (CVNet) are proposed with a distributed state representation layer. A regularized policy evaluation scheme for CVNet is derived, which penalizes large Lipschitz constants of the value network for additional robustness against adversarial perturbations and noise. Transfer learning methods are adapted to CVNet to increase learning adaptability and efficiency across multiple cities. Extensive offline simulations based on real dispatching data and online AB tests through DiDi's platform demonstrate that CVNet consistently outperforms other recently proposed dispatching methods. The study concludes by showing that the performance can be further enhanced through the efficient use of transfer learning.",1
"The fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmarks. Our approach learns representations that capture the underlying structure of the domain and lead to improved sample efficiency over state-of-the-art deep reinforcement learning with visual features -- often matching or exceeding the performance achieved with hand-designed compact state information.",0
"The core assumption of reinforcement learning in Markov decision processes (MDPs) is that the decision process is Markovian. However, when MDPs have rich observations, agents tend to learn using an abstract state representation, which may not preserve the Markov property. We have created a new set of conditions and proven that they are sufficient for learning a Markovian abstract state representation. We have also developed a practical training procedure that uses inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our unique training objective is compatible with online and offline training, does not require a reward signal, but allows agents to take advantage of reward information. We have tested our approach on a visual gridworld domain and various continuous control benchmarks, and it has resulted in representations that capture the domain's underlying structure and improved sample efficiency. Our approach often matches or exceeds the performance of hand-designed compact state information in deep reinforcement learning with visual features.",1
"Deep reinforcement learning has achieved significant success in many decision-making tasks in various fields. However, it requires a large training time of dense neural networks to obtain a good performance. This hinders its applicability on low-resource devices where memory and computation are strictly constrained. In a step towards enabling deep reinforcement learning agents to be applied to low-resource devices, in this work, we propose for the first time to dynamically train deep reinforcement learning agents with sparse neural networks from scratch. We adopt the evolution principles of dynamic sparse training in the reinforcement learning paradigm and introduce a training algorithm that optimizes the sparse topology and the weight values jointly to dynamically fit the incoming data. Our approach is easy to be integrated into existing deep reinforcement learning algorithms and has many favorable advantages. First, it allows for significant compression of the network size which reduces the memory and computation costs substantially. This would accelerate not only the agent inference but also its training process. Second, it speeds up the agent learning process and allows for reducing the number of required training steps. Third, it can achieve higher performance than training the dense counterpart network. We evaluate our approach on OpenAI gym continuous control tasks. The experimental results show the effectiveness of our approach in achieving higher performance than one of the state-of-art baselines with a 50\% reduction in the network size and floating-point operations (FLOPs). Moreover, our proposed approach can reach the same performance achieved by the dense network with a 40-50\% reduction in the number of training steps.",0
"In many fields, deep reinforcement learning has been successful in decision-making tasks. However, to achieve good performance, dense neural networks require extensive training time, which limits their applicability on devices with limited memory and computation. To address this issue, we propose dynamically training deep reinforcement learning agents with sparse neural networks from scratch. Our approach optimizes both the sparse topology and the weight values to fit incoming data, and it can be easily integrated into existing deep reinforcement learning algorithms. By compressing the network size, our approach reduces memory and computation costs, accelerates agent inference and training processes, and achieves higher performance than training a dense network. We evaluate our approach on OpenAI gym continuous control tasks and demonstrate its effectiveness in achieving higher performance than state-of-the-art baselines with a 50% reduction in network size and FLOPs. Additionally, our approach reaches the same performance as the dense network with a 40-50% reduction in the number of training steps.",1
"Efficiently propagating credit to responsible actions is a central and challenging task in reinforcement learning. To accelerate information propagation, this paper presents a new method that bridges a highway that allows unimpeded information to flow across long horizons. The key to our method is a newly proposed Bellman equation, called Greedy-Step Bellman Optimality Equation, through which the high-credit information can fast propagate across a long horizon. We theoretically show that the solution of the new equation is exactly the optimal value function and the corresponding operator converges faster than the classical operator. Besides, it leads to a new multi-step off-policy algorithm, which is capable of safely utilizing any off-policy data collected by the arbitrary policy. Experiments reveal that the proposed method is reliable, easy to implement. Moreover, without employing additional components of Rainbow except Double DQN, our method achieves competitive performance with Rainbow on the benchmark tasks.",0
"Reinforcement learning faces the tough challenge of efficiently propagating credit to responsible actions. To overcome this challenge and speed up information propagation, a new method has been introduced in this paper. It involves building a highway that enables unobstructed information flow across long horizons. The key element of this method is the Greedy-Step Bellman Optimality Equation, a new Bellman equation that facilitates the rapid propagation of high-credit information across long horizons. Theoretical analysis confirms that the solution to this equation is the optimal value function and that the corresponding operator converges faster than the classical operator. The proposed method also leads to a new multi-step off-policy algorithm that can safely utilize any off-policy data collected by any policy. The experiments demonstrate that the proposed method is reliable and easy to implement. Furthermore, even without additional components of Rainbow except Double DQN, our method achieves competitive performance with Rainbow on the benchmark tasks.",1
"Designing agents that acquire knowledge autonomously and use it to solve new tasks efficiently is an important challenge in reinforcement learning. Knowledge acquired during an unsupervised pre-training phase is often transferred by fine-tuning neural network weights once rewards are exposed, as is common practice in supervised domains. Given the nature of the reinforcement learning problem, we argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. We introduce Behavior Transfer (BT), a technique that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. Our experiments show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation objectives can lead to the emergence of complex behaviors. These pre-trained policies can then be leveraged by BT to discover better solutions than without pre-training, and combining BT with standard fine-tuning strategies results in additional benefits. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the pre-trained policies is misaligned with the downstream task.",0
"An important challenge in reinforcement learning is creating agents that can independently acquire knowledge and use it to efficiently solve new tasks. Typically, knowledge gained during unsupervised pre-training is transferred through fine-tuning neural network weights once rewards are present, similar to supervised domains. However, standard fine-tuning strategies alone may not be sufficient for effective transfer in difficult domains due to the nature of the reinforcement learning problem. To address this, we propose Behavior Transfer (BT), a technique that uses pre-trained policies for exploration in addition to transferring neural network weights. Our experiments demonstrate that, when combined with large-scale pre-training in the absence of rewards, intrinsic motivation objectives can lead to the emergence of complex behaviors. These pre-trained policies can then be utilized by BT to discover improved solutions compared to those without pre-training, particularly in domains requiring structured exploration and when pre-trained policies do not align with the downstream task. BT also offers additional benefits when paired with standard fine-tuning strategies.",1
"Learning good feature representations is important for deep reinforcement learning (RL). However, with limited experience, RL often suffers from data inefficiency for training. For un-experienced or less-experienced trajectories (i.e., state-action sequences), the lack of data limits the use of them for better feature learning. In this work, we propose a novel method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, PlayVirtual predicts future states based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a trajectory cycle. Based on this, we augment the actions to generate a large amount of virtual state-action trajectories. Being free of groudtruth state supervision, we enforce a trajectory to meet the cycle consistency constraint, which can significantly enhance the data efficiency. We validate the effectiveness of our designs on the Atari and DeepMind Control Suite benchmarks. Our method outperforms the current state-of-the-art methods by a large margin on both benchmarks.",0
"Acquiring effective feature representations is crucial in the context of deep reinforcement learning (RL). However, due to insufficient experience, the training of RL is often hindered by data inefficiency. The lack of data for inexperienced or less experienced trajectories (i.e., state-action sequences) limits their usefulness in improving feature learning. This study introduces a new technique called PlayVirtual, which uses cycle-consistent virtual trajectories to enhance the data efficiency of RL feature representation learning. PlayVirtual uses a dynamics model to predict future states based on the current state and action, and a backward dynamics model to predict previous states, forming a trajectory cycle. By augmenting actions, a vast number of virtual state-action trajectories are generated. Without relying on ground truth state supervision, PlayVirtual enforces a trajectory to satisfy the cycle consistency constraint, leading to significant improvements in data efficiency. The effectiveness of the approach is validated using Atari and DeepMind Control Suite benchmarks, where it outperforms current state-of-the-art methods by a considerable margin.",1
"Medical image segmentation is one of the important tasks of computer-aided diagnosis in medical image analysis. Since most medical images have the characteristics of blurred boundaries and uneven intensity distribution, through existing segmentation methods, the discontinuity within the target area and the discontinuity of the target boundary are likely to lead to rough or even erroneous boundary delineation. In this paper, we propose a new iterative refined interactive segmentation method for medical images based on agent reinforcement learning, which focuses on the problem of target segmentation boundaries. We model the dynamic process of drawing the target contour in a certain order as a Markov Decision Process (MDP) based on a deep reinforcement learning method. In the dynamic process of continuous interaction between the agent and the image, the agent tracks the boundary point by point in order within a limited length range until the contour of the target is completely drawn. In this process, the agent can quickly improve the segmentation performance by exploring an interactive policy in the image. The method we proposed is simple and effective. At the same time, we evaluate our method on the cardiac MRI scan data set. Experimental results show that our method has a better segmentation effect on the left ventricle in a small number of medical image data sets, especially in terms of segmentation boundaries, this method is better than existing methods. Based on our proposed method, the dynamic generation process of the predicted contour trajectory of the left ventricle will be displayed online at https://github.com/H1997ym/LV-contour-trajectory.",0
"Medical image segmentation is a crucial aspect of computer-aided diagnosis in the field of medical image analysis. Due to the blurred boundaries and uneven intensity distribution found in most medical images, existing segmentation methods may result in rough or incorrect boundary delineation due to target area discontinuity and target boundary discontinuity issues. To address this problem, we present a novel iterative refined interactive segmentation method for medical images based on agent reinforcement learning. Our approach focuses on improving target segmentation boundaries by modeling the dynamic process of drawing the target contour as a Markov Decision Process (MDP) using deep reinforcement learning. Our agent tracks the boundary point by point within a limited length range until the target contour is fully drawn, enhancing segmentation performance through interactive policy exploration. Our method is simple yet effective, and we evaluate it on the cardiac MRI scan data set, demonstrating superior segmentation performance on the left ventricle in a small number of medical image data sets, particularly regarding segmentation boundaries. The online display of the predicted contour trajectory of the left ventricle generated by our proposed method can be found on https://github.com/H1997ym/LV-contour-trajectory.",1
"Real world applications such as economics and policy making often involve solving multi-agent games with two unique features: (1) The agents are inherently asymmetric and partitioned into leaders and followers; (2) The agents have different reward functions, thus the game is general-sum. The majority of existing results in this field focuses on either symmetric solution concepts (e.g. Nash equilibrium) or zero-sum games. It remains vastly open how to learn the Stackelberg equilibrium -- an asymmetric analog of the Nash equilibrium -- in general-sum games efficiently from samples.   This paper initiates the theoretical study of sample-efficient learning of the Stackelberg equilibrium, in the bandit feedback setting where we only observe noisy samples of the reward. We consider three representative two-player general-sum games: bandit games, bandit-reinforcement learning (bandit-RL) games, and linear bandit games. In all these games, we identify a fundamental gap between the exact value of the Stackelberg equilibrium and its estimated version using finitely many noisy samples, which can not be closed information-theoretically regardless of the algorithm. We then establish sharp positive results on sample-efficient learning of Stackelberg equilibrium with value optimal up to the gap identified above, with matching lower bounds in the dependency on the gap, error tolerance, and the size of the action spaces. Overall, our results unveil unique challenges in learning Stackelberg equilibria under noisy bandit feedback, which we hope could shed light on future research on this topic.",0
"Applications in the real world, such as economics and policy making, often require solving multi-agent games that have two distinct features. Firstly, the agents are inherently asymmetric and divided into leaders and followers. Secondly, the agents have varying reward functions, making the game general-sum. While existing research in this field has focused on symmetric solution concepts or zero-sum games, learning the Stackelberg equilibrium - an asymmetric version of the Nash equilibrium - in general-sum games efficiently from samples remains a largely open problem. This paper aims to initiate the theoretical study of sample-efficient learning of the Stackelberg equilibrium in the bandit feedback setting, where we only observe noisy samples of the reward. We examine three representative two-player general-sum games and identify a fundamental gap between the exact and estimated values of the Stackelberg equilibrium using finitely many noisy samples. We establish positive results on sample-efficient learning of the Stackelberg equilibrium with value optimal up to the identified gap, with matching lower bounds in the dependency on the gap, error tolerance, and the action space's size. Our findings reveal the unique challenges of learning the Stackelberg equilibrium under noisy bandit feedback and could provide valuable insights for future research.",1
"The softmax policy gradient (PG) method, which performs gradient ascent under softmax policy parameterization, is arguably one of the de facto implementations of policy optimization in modern reinforcement learning. For $\gamma$-discounted infinite-horizon tabular Markov decision processes (MDPs), remarkable progress has recently been achieved towards establishing global convergence of softmax PG methods in finding a near-optimal policy. However, prior results fall short of delineating clear dependencies of convergence rates on salient parameters such as the cardinality of the state space $\mathcal{S}$ and the effective horizon $\frac{1}{1-\gamma}$, both of which could be excessively large. In this paper, we deliver a pessimistic message regarding the iteration complexity of softmax PG methods, despite assuming access to exact gradient computation. Specifically, we demonstrate that the softmax PG method with stepsize $\eta$ can take \[   \frac{1}{\eta} |\mathcal{S}|^{2^{\Omega\big(\frac{1}{1-\gamma}\big)}} ~\text{iterations} \] to converge, even in the presence of a benign policy initialization and an initial state distribution amenable to exploration (so that the distribution mismatch coefficient is not exceedingly large). This is accomplished by characterizing the algorithmic dynamics over a carefully-constructed MDP containing only three actions. Our exponential lower bound hints at the necessity of carefully adjusting update rules or enforcing proper regularization in accelerating PG methods.",0
"The softmax policy gradient (PG) method is widely used in modern reinforcement learning for policy optimization, as it employs gradient ascent under softmax policy parameterization. Recently, significant progress has been made in establishing global convergence of softmax PG methods for $\gamma$-discounted infinite-horizon tabular Markov decision processes (MDPs) in finding a nearly optimal policy. However, prior results fail to identify clear dependencies of convergence rates on crucial parameters, such as the state space cardinality $\mathcal{S}$ and the effective horizon $\frac{1}{1-\gamma}$, which may be excessively large. In this study, we present a pessimistic outlook on the iteration complexity of softmax PG methods, even with access to exact gradient computation. Specifically, we demonstrate that the softmax PG method with stepsize $\eta$ may require \[   \frac{1}{\eta} |\mathcal{S}|^{2^{\Omega\big(\frac{1}{1-\gamma}\big)}} ~\text{iterations} \] to converge, even with a benign policy initialization and an initial state distribution amenable to exploration. We achieve this by examining the algorithmic dynamics over an MDP with only three actions. Our exponential lower bound highlights the importance of carefully adjusting update rules or enforcing proper regularization to expedite PG methods.",1
"In this paper we consider multi-objective reinforcement learning where the objectives are balanced using preferences. In practice, the preferences are often given in an adversarial manner, e.g., customers can be picky in many applications. We formalize this problem as an episodic learning problem on a Markov decision process, where transitions are unknown and a reward function is the inner product of a preference vector with pre-specified multi-objective reward functions. We consider two settings. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. We provide a model-based algorithm that achieves a nearly minimax optimal regret bound $\widetilde{\mathcal{O}}\bigl(\sqrt{\min\{d,S\}\cdot H^2 SAK}\bigr)$, where $d$ is the number of objectives, $S$ is the number of states, $A$ is the number of actions, $H$ is the length of the horizon, and $K$ is the number of episodes. Furthermore, we consider preference-free exploration, i.e., the agent first interacts with the environment without specifying any preference and then is able to accommodate arbitrary preference vector up to $\epsilon$ error. Our proposed algorithm is provably efficient with a nearly optimal trajectory complexity $\widetilde{\mathcal{O}}\bigl({\min\{d,S\}\cdot H^3 SA}/{\epsilon^2}\bigr)$. This result partly resolves an open problem raised by \citet{jin2020reward}.",0
"This paper examines multi-objective reinforcement learning that utilizes preferences to balance objectives. Often, preferences are presented in an adversarial manner, such as in situations where customers are picky. We approach this problem as an episodic learning problem on a Markov decision process, where the reward function is the inner product of a preference vector with pre-specified multi-objective reward functions. We explore two settings: online and preference-free exploration. In the online setting, the agent receives an adversarial preference each episode and proposes policies. We provide a model-based algorithm with a nearly minimax optimal regret bound. In preference-free exploration, the agent interacts with the environment without specifying any preference and can then accommodate arbitrary preference vectors up to a certain error. Our proposed algorithm is efficient and has a nearly optimal trajectory complexity, which partially solves an open problem.",1
"This paper studies regret minimization with randomized value functions in reinforcement learning. In tabular finite-horizon Markov Decision Processes, we introduce a clipping variant of one classical Thompson Sampling (TS)-like algorithm, randomized least-squares value iteration (RLSVI). Our $\tilde{\mathrm{O}}(H^2S\sqrt{AT})$ high-probability worst-case regret bound improves the previous sharpest worst-case regret bounds for RLSVI and matches the existing state-of-the-art worst-case TS-based regret bounds.",0
"The focus of this paper is on minimizing regret through reinforcement learning by utilizing randomized value functions. Our study specifically examines tabular finite-horizon Markov Decision Processes, and we present a modified version of the randomized least-squares value iteration (RLSVI) algorithm called the clipping variant of Thompson Sampling (TS). With our proposed algorithm, we were able to achieve a high-probability worst-case regret bound of approximately $\tilde{\mathrm{O}}(H^2S\sqrt{AT})$, which surpasses previous regret bounds for RLSVI and is on par with the current state-of-the-art TS-based regret bounds.",1
"A common optimization tool used in deep reinforcement learning is momentum, which consists in accumulating and discounting past gradients, reapplying them at each iteration. We argue that, unlike in supervised learning, momentum in Temporal Difference (TD) learning accumulates gradients that become doubly stale: not only does the gradient of the loss change due to parameter updates, the loss itself changes due to bootstrapping. We first show that this phenomenon exists, and then propose a first-order correction term to momentum. We show that this correction term improves sample efficiency in policy evaluation by correcting target value drift. An important insight of this work is that deep RL methods are not always best served by directly importing techniques from the supervised setting.",0
"Deep reinforcement learning commonly uses momentum as an optimization tool, which involves accumulating and discounting past gradients and reapplying them at each iteration. However, in Temporal Difference (TD) learning, momentum accumulates gradients that become doubly stale due to changes in both the gradient of the loss and the loss itself caused by bootstrapping. To address this issue, we propose a first-order correction term to momentum, which improves sample efficiency in policy evaluation by correcting target value drift. This work highlights the importance of not directly importing techniques from the supervised setting to deep RL methods.",1
"Target networks are at the core of recent success in Reinforcement Learning. They stabilize the training by using old parameters to estimate the $Q$-values, but this also limits the propagation of newly-encountered rewards which could ultimately slow down the training. In this work, we propose an alternative training method based on functional regularization which does not have this deficiency. Unlike target networks, our method uses up-to-date parameters to estimate the target $Q$-values, thereby speeding up training while maintaining stability. Surprisingly, in some cases, we can show that target networks are a special, restricted type of functional regularizers. Using this approach, we show empirical improvements in sample efficiency and performance across a range of Atari and simulated robotics environments.",0
"Reinforcement Learning has achieved significant progress by employing target networks as a key component. Although they enhance training stability by estimating $Q$-values with old parameters, they constrain the transmission of new rewards, leading to slower training. In this study, we present an alternative training technique based on functional regularization that avoids this issue. Our approach utilizes current parameters to estimate target $Q$-values, resulting in faster training while maintaining stability. Interestingly, target networks can be viewed as a specific type of functional regularizer in certain circumstances. We demonstrate the effectiveness of our method by improving sample efficiency and performance in a variety of Atari and simulated robotics environments.",1
"We address the challenge of policy evaluation in real-world applications of reinforcement learning systems where the available historical data is limited due to ethical, practical, or security considerations. This constrained distribution of data samples often leads to biased policy evaluation estimates. To remedy this, we propose that instead of policy evaluation, one should perform policy comparison, i.e. to rank the policies of interest in terms of their value based on available historical data. In addition we present the Limited Data Estimator (LDE) as a simple method for evaluating and comparing policies from a small number of interactions with the environment. According to our theoretical analysis, the LDE is shown to be statistically reliable on policy comparison tasks under mild assumptions on the distribution of the historical data. Additionally, our numerical experiments compare the LDE to other policy evaluation methods on the task of policy ranking and demonstrate its advantage in various settings.",0
"In applications of reinforcement learning systems in the real world, policy evaluation can be challenging due to limited historical data resulting from ethical, practical, or security concerns. This restricted data distribution often yields biased policy evaluation estimates. To address this issue, we suggest that policy comparison be conducted instead of policy evaluation, enabling the ranking of policies of interest based on available historical data. Furthermore, we introduce the Limited Data Estimator (LDE) as a simple approach for assessing and comparing policies using a small number of interactions with the environment. Our theoretical analysis reveals that the LDE is statistically reliable for policy comparison tasks when subject to mild assumptions about the distribution of historical data. Additionally, our numerical experiments compare the LDE to other policy evaluation methods for policy ranking and illustrate its advantages in various scenarios.",1
"We introduce a mapping between Maximum Entropy Reinforcement Learning (MaxEnt RL) and Markovian processes conditioned on rare events. In the long time limit, this mapping allows us to derive analytical expressions for the optimal policy, dynamics and initial state distributions for the general case of stochastic dynamics in MaxEnt RL. We find that soft-$\mathcal{Q}$ functions in MaxEnt RL can be obtained from the Perron-Frobenius eigenvalue and the corresponding left eigenvector of a regular, non-negative matrix derived from the underlying Markov Decision Process (MDP). The results derived lead to novel algorithms for model-based and model-free MaxEnt RL, which we validate by numerical simulations. The mapping established in this work opens further avenues for the application of novel analytical and computational approaches to problems in MaxEnt RL. We make our code available at: https://github.com/argearriojas/maxent-rl-mdp-scripts",0
"A correlation between Maximum Entropy Reinforcement Learning (MaxEnt RL) and Markovian processes dependent on rare events is introduced. Through this correlation, we can obtain analytical expressions for the optimal policy, dynamics, and initial state distributions for stochastic dynamics in MaxEnt RL in the long run. The Perron-Frobenius eigenvalue and corresponding left eigenvector of a regular, non-negative matrix derived from the underlying Markov Decision Process (MDP) can produce soft-$\mathcal{Q}$ functions in MaxEnt RL. We validated our novel algorithms for model-based and model-free MaxEnt RL through numerical simulations. This mapping creates further opportunities for employing new analytical and computational techniques to address MaxEnt RL issues. Our code is available at: https://github.com/argearriojas/maxent-rl-mdp-scripts.",1
"Off-policy Reinforcement Learning (RL) holds the promise of better data efficiency as it allows sample reuse and potentially enables safe interaction with the environment. Current off-policy policy gradient methods either suffer from high bias or high variance, delivering often unreliable estimates. The price of inefficiency becomes evident in real-world scenarios such as interaction-driven robot learning, where the success of RL has been rather limited, and a very high sample cost hinders straightforward application. In this paper, we propose a nonparametric Bellman equation, which can be solved in closed form. The solution is differentiable w.r.t the policy parameters and gives access to an estimation of the policy gradient. In this way, we avoid the high variance of importance sampling approaches, and the high bias of semi-gradient methods. We empirically analyze the quality of our gradient estimate against state-of-the-art methods, and show that it outperforms the baselines in terms of sample efficiency on classical control tasks.",0
"The use of Off-policy Reinforcement Learning (RL) has the potential to improve data efficiency by allowing for sample reuse and safe interaction with the environment. However, current off-policy policy gradient methods are unreliable due to either high bias or high variance. This inefficiency can be problematic in real-world scenarios such as interaction-driven robot learning, where RL has limited success and high sample costs hinder its application. To address this issue, we propose a nonparametric Bellman equation that can be solved in closed form. This solution is differentiable with respect to the policy parameters and provides an estimate of the policy gradient, avoiding the high variance of importance sampling approaches and the high bias of semi-gradient methods. We have empirically analyzed our proposed gradient estimate and have shown that it outperforms state-of-the-art methods in terms of sample efficiency on classical control tasks.",1
"In this paper, we consider a transfer Reinforcement Learning (RL) problem in continuous state and action spaces, under unobserved contextual information. For example, the context can represent the mental view of the world that an expert agent has formed through past interactions with this world. We assume that this context is not accessible to a learner agent who can only observe the expert data. Then, our goal is to use the context-aware expert data to learn an optimal context-unaware policy for the learner using only a few new data samples. Such problems are typically solved using imitation learning that assumes that both the expert and learner agents have access to the same information. However, if the learner does not know the expert context, using the expert data alone will result in a biased learner policy and will require many new data samples to improve. To address this challenge, in this paper, we formulate the learning problem as a causal bound-constrained Multi-Armed-Bandit (MAB) problem. The arms of this MAB correspond to a set of basis policy functions that can be initialized in an unsupervised way using the expert data and represent the different expert behaviors affected by the unobserved context. On the other hand, the MAB constraints correspond to causal bounds on the accumulated rewards of these basis policy functions that we also compute from the expert data. The solution to this MAB allows the learner agent to select the best basis policy and improve it online. And the use of causal bounds reduces the exploration variance and, therefore, improves the learning rate. We provide numerical experiments on an autonomous driving example that show that our proposed transfer RL method improves the learner's policy faster compared to existing imitation learning methods and enjoys much lower variance during training.",0
"This paper discusses a transfer Reinforcement Learning (RL) problem that deals with continuous state and action spaces and involves unobserved contextual information. The context is assumed to represent an expert agent's mental view of the world, which is not accessible to the learner agent, who can only observe expert data. The objective is to use the context-aware expert data to learn an optimal context-unaware policy for the learner with minimal new data samples. This differs from the typical imitation learning approach, which assumes that both expert and learner agents have access to the same information. However, relying solely on expert data can lead to a biased learner policy, necessitating many new data samples to improve. To overcome this, the paper proposes a causal bound-constrained Multi-Armed-Bandit (MAB) problem formulation, where the arms correspond to basis policy functions that can be initialized using expert data, and the constraints correspond to causal bounds on the accumulated rewards of these basis policy functions. The solution to this MAB enables the learner agent to select the best basis policy and improve it online, while the use of causal bounds reduces exploration variance and improves the learning rate. Numerical experiments on an autonomous driving example show that this transfer RL method outperforms existing imitation learning methods and has significantly lower variance during training.",1
"We propose a novel framework for verifiable and compositional reinforcement learning (RL) in which a collection of RL sub-systems, each of which learns to accomplish a separate sub-task, are composed to achieve an overall task. The framework consists of a high-level model, represented as a parametric Markov decision process (pMDP) which is used to plan and to analyze compositions of sub-systems, and of the collection of low-level sub-systems themselves. By defining interfaces between the sub-systems, the framework enables automatic decompositons of task specifications, e.g., reach a target set of states with a probability of at least 0.95, into individual sub-task specifications, i.e. achieve the sub-system's exit conditions with at least some minimum probability, given that its entry conditions are met. This in turn allows for the independent training and testing of the sub-systems; if they each learn a policy satisfying the appropriate sub-task specification, then their composition is guaranteed to satisfy the overall task specification. Conversely, if the sub-task specifications cannot all be satisfied by the learned policies, we present a method, formulated as the problem of finding an optimal set of parameters in the pMDP, to automatically update the sub-task specifications to account for the observed shortcomings. The result is an iterative procedure for defining sub-task specifications, and for training the sub-systems to meet them. As an additional benefit, this procedure allows for particularly challenging or important components of an overall task to be determined automatically, and focused on, during training. Experimental results demonstrate the presented framework's novel capabilities.",0
"Our proposal introduces a fresh framework for reinforcement learning (RL) that is both verifiable and compositional. The system is composed of multiple RL sub-systems, each of which is trained to complete a distinct sub-task. These sub-systems are combined to achieve an overall task. The framework consists of a high-level model, represented by a parametric Markov decision process (pMDP), which is employed to plan and analyze the compositions of sub-systems, as well as a collection of low-level sub-systems. By defining interfaces between the sub-systems, the framework allows for automatic decompositions of task specifications, which enables the independent training and testing of sub-systems. If each sub-system learns a policy that satisfies its appropriate sub-task specification, then their composition is guaranteed to satisfy the overall task specification. If the sub-task specifications cannot all be satisfied by the learned policies, we present a method for updating them. This iterative procedure allows for particularly challenging or important aspects of an overall task to be determined automatically and focused on during training. Experimental results demonstrate the novel capabilities of our proposed framework.",1
"Reinforcement learning competitions have formed the basis for standard research benchmarks, galvanized advances in the state-of-the-art, and shaped the direction of the field. Despite this, a majority of challenges suffer from the same fundamental problems: participant solutions to the posed challenge are usually domain-specific, biased to maximally exploit compute resources, and not guaranteed to be reproducible. In this paper, we present a new framework of competition design that promotes the development of algorithms that overcome these barriers. We propose four central mechanisms for achieving this end: submission retraining, domain randomization, desemantization through domain obfuscation, and the limitation of competition compute and environment-sample budget. To demonstrate the efficacy of this design, we proposed, organized, and ran the MineRL 2020 Competition on Sample-Efficient Reinforcement Learning. In this work, we describe the organizational outcomes of the competition and show that the resulting participant submissions are reproducible, non-specific to the competition environment, and sample/resource efficient, despite the difficult competition task.",0
"The use of reinforcement learning competitions has been pivotal in establishing research benchmarks, driving advancements in the field, and influencing its trajectory. However, many challenges are plagued by common issues such as domain-specific solutions that exploit computing power to the fullest, and a lack of reproducibility. This paper proposes a novel competition design framework that addresses these challenges by implementing four key mechanisms: submission retraining, domain randomization, domain obfuscation, and limiting computing and environment-sample budget. The effectiveness of this framework is demonstrated through the successful organization and execution of the MineRL 2020 Competition on Sample-Efficient Reinforcement Learning, which yielded participant submissions that were both reproducible and efficient regardless of the competition environment.",1
"Accuracy and generalization of dynamics models is key to the success of model-based reinforcement learning (MBRL). As the complexity of tasks increases, so does the sample inefficiency of learning accurate dynamics models. However, many complex tasks also exhibit sparsity in the dynamics, i.e., actions have only a local effect on the system dynamics. In this paper, we exploit this property with a causal invariance perspective in the single-task setting, introducing a new type of state abstraction called \textit{model-invariance}. Unlike previous forms of state abstractions, a model-invariance state abstraction leverages causal sparsity over state variables. This allows for compositional generalization to unseen states, something that non-factored forms of state abstractions cannot do. We prove that an optimal policy can be learned over this model-invariance state abstraction and show improved generalization in a simple toy domain. Next, we propose a practical method to approximately learn a model-invariant representation for complex domains and validate our approach by showing improved modelling performance over standard maximum likelihood approaches on challenging tasks, such as the MuJoCo-based Humanoid. Finally, within the MBRL setting we show strong performance gains with respect to sample efficiency across a host of other continuous control tasks.",0
"The success of model-based reinforcement learning (MBRL) relies heavily on the accuracy and generalization of dynamics models. However, as task complexity increases, learning accurate dynamics models becomes more sample inefficient. Nonetheless, many complex tasks exhibit sparsity in the dynamics, indicating that actions have only a local impact on the system dynamics. In this study, we introduce a novel state abstraction called \textit{model-invariance} that leverages causal sparsity over state variables from a causal invariance perspective. This type of state abstraction enables compositional generalization to unseen states, which is not possible with non-factored forms of state abstractions. We demonstrate that an optimal policy can be learned using model-invariance state abstraction and highlight improved generalization in a simple toy domain. Furthermore, we propose a practical method to approximate a model-invariant representation for complex domains and validate it by demonstrating improved modeling performance over standard maximum likelihood approaches on challenging tasks, such as the MuJoCo-based Humanoid. Finally, we show significant performance gains in terms of sample efficiency across various continuous control tasks within the MBRL setting.",1
"Mirror descent (MD), a well-known first-order method in constrained convex optimization, has recently been shown as an important tool to analyze trust-region algorithms in reinforcement learning (RL). However, there remains a considerable gap between such theoretically analyzed algorithms and the ones used in practice. Inspired by this, we propose an efficient RL algorithm, called {\em mirror descent policy optimization} (MDPO). MDPO iteratively updates the policy by {\em approximately} solving a trust-region problem, whose objective function consists of two terms: a linearization of the standard RL objective and a proximity term that restricts two consecutive policies to be close to each other. Each update performs this approximation by taking multiple gradient steps on this objective function. We derive {\em on-policy} and {\em off-policy} variants of MDPO, while emphasizing important design choices motivated by the existing theory of MD in RL. We highlight the connections between on-policy MDPO and two popular trust-region RL algorithms: TRPO and PPO, and show that explicitly enforcing the trust-region constraint is in fact {\em not} a necessity for high performance gains in TRPO. We then show how the popular soft actor-critic (SAC) algorithm can be derived by slight modifications of off-policy MDPO. Overall, MDPO is derived from the MD principles, offers a unified approach to viewing a number of popular RL algorithms, and performs better than or on-par with TRPO, PPO, and SAC in a number of continuous control tasks. Code is available at \url{https://github.com/manantomar/Mirror-Descent-Policy-Optimization}.",0
"Mirror descent (MD) is a well-known first-order technique used in constrained convex optimization. Recently, it has been recognized as a crucial tool for analyzing trust-region algorithms in reinforcement learning (RL). However, a significant gap exists between theoretically analyzed algorithms and those utilized in real-life situations. In light of this, we have introduced a new efficient RL algorithm called ""mirror descent policy optimization"" (MDPO). MDPO updates the policy iteratively by solving an approximate trust-region problem consisting of two terms: a linearization of the standard RL objective and a proximity term that ensures that two consecutive policies remain close to each other. The approximation is performed by taking multiple gradient steps on this objective function. We have developed both on-policy and off-policy variants of MDPO, considering important design choices motivated by the existing theory of MD in RL. We have also established the links between on-policy MDPO and two renowned trust-region RL algorithms: TRPO and PPO, demonstrating that enforcing the trust-region constraint is not necessary for achieving high performance gains in TRPO. We have also demonstrated how the widely used soft actor-critic (SAC) algorithm can be obtained by making slight modifications to off-policy MDPO. Overall, MDPO is a unified approach based on the principles of MD that provides better performance than or is on-par with TRPO, PPO, and SAC in various continuous control tasks. The code for this algorithm is accessible at \url{https://github.com/manantomar/Mirror-Descent-Policy-Optimization}.",1
"Inverse reinforcement learning attempts to reconstruct the reward function in a Markov decision problem, using observations of agent actions. As already observed by Russell the problem is ill-posed, and the reward function is not identifiable, even under the presence of perfect information about optimal behavior. We provide a resolution to this non-identifiability for problems with entropy regularization. For a given environment, we fully characterize the reward functions leading to a given policy and demonstrate that, given demonstrations of actions for the same reward under two distinct discount factors, or under sufficiently different environments, the unobserved reward can be recovered up to a constant. Through a simple numerical experiment, we demonstrate the accurate reconstruction of the reward function through our proposed resolution.",0
"The aim of inverse reinforcement learning is to reconstruct the reward function in a Markov decision problem by observing the actions of the agent. However, it has been noted by Russell that this poses a problem as the reward function is not identifiable even with perfect information about optimal behavior. To address this issue, we propose a solution for problems with entropy regularization. We fully characterize the reward functions that lead to a particular policy in a given environment and show that if we have demonstrations of actions for the same reward under two different discount factors or sufficiently different environments, we can recover the unobserved reward up to a constant. Our proposed solution is demonstrated through a simple numerical experiment which accurately reconstructs the reward function.",1
"Many reinforcement learning (RL) environments consist of independent entities that interact sparsely. In such environments, RL agents have only limited influence over other entities in any particular situation. Our idea in this work is that learning can be efficiently guided by knowing when and what the agent can influence with its actions. To achieve this, we introduce a measure of situation-dependent causal influence based on conditional mutual information and show that it can reliably detect states of influence. We then propose several ways to integrate this measure into RL algorithms to improve exploration and off-policy learning. All modified algorithms show strong increases in data efficiency on robotic manipulation tasks.",0
"In numerous reinforcement learning (RL) environments, separate entities interact infrequently, leaving RL agents with minimal control over others. Therefore, we suggest that the learning process can benefit from identifying when and how the agent can affect outcomes by introducing a measure of situation-dependent causal influence using conditional mutual information. We demonstrate that this measure can accurately identify states of influence. We also propose various techniques to incorporate this measure into RL algorithms, resulting in better exploration and off-policy learning. The modified algorithms have shown significant improvements in data efficiency in robotic manipulation tasks.",1
"Most of reinforcement learning algorithms optimize the discounted criterion which is beneficial to accelerate the convergence and reduce the variance of estimates. Although the discounted criterion is appropriate for certain tasks such as financial related problems, many engineering problems treat future rewards equally and prefer a long-run average criterion. In this paper, we study the reinforcement learning problem with the long-run average criterion. Firstly, we develop a unified trust region theory with discounted and average criteria. With the average criterion, a novel performance bound within the trust region is derived with the Perturbation Analysis (PA) theory. Secondly, we propose a practical algorithm named Average Policy Optimization (APO), which improves the value estimation with a novel technique named Average Value Constraint. To the best of our knowledge, our work is the first one to study the trust region approach with the average criterion and it complements the framework of reinforcement learning beyond the discounted criterion. Finally, experiments are conducted in the continuous control environment MuJoCo. In most tasks, APO performs better than the discounted PPO, which demonstrates the effectiveness of our approach.",0
"The majority of reinforcement learning algorithms aim to optimize the discounted criterion, which has the advantage of speeding up convergence and minimizing estimate variance. However, while this criterion is suitable for financial problems, many engineering problems require equal consideration of future rewards and favor a long-term average criterion. This study focuses on reinforcement learning with a long-term average criterion. Firstly, a unified trust region theory is developed for both discounted and average criteria, and a novel performance bound within the trust region is derived using Perturbation Analysis (PA) theory. Secondly, a practical algorithm called Average Policy Optimization (APO) is proposed, which enhances value estimation with the innovative Average Value Constraint technique. This is the first work to investigate the trust region approach with the average criterion, expanding the framework of reinforcement learning beyond the discounted criterion. Finally, experiments are conducted in the continuous control environment MuJoCo, where APO outperforms discounted PPO in most tasks, demonstrating the effectiveness of this approach.",1
"Data augmentation is becoming essential for improving regression accuracy in critical applications including manufacturing and finance. Existing techniques for data augmentation largely focus on classification tasks and do not readily apply to regression tasks. In particular, the recent Mixup techniques for classification rely on the key assumption that linearity holds among training examples, which is reasonable if the label space is discrete, but has limitations when the label space is continuous as in regression. We show that mixing examples that either have a large data or label distance may have an increasingly-negative effect on model performance. Hence, we use the stricter assumption that linearity only holds within certain data or label distances for regression where the degree may vary by each example. We then propose MixRL, a data augmentation meta learning framework for regression that learns for each example how many nearest neighbors it should be mixed with for the best model performance using a small validation set. MixRL achieves these objectives using Monte Carlo policy gradient reinforcement learning. Our experiments conducted both on synthetic and real datasets show that MixRL significantly outperforms state-of-the-art data augmentation baselines. MixRL can also be integrated with other classification Mixup techniques for better results.",0
"In applications such as manufacturing and finance, data augmentation has become crucial for improving regression accuracy. However, existing data augmentation methods mainly focus on classification tasks and are not easily applicable to regression tasks. While Mixup techniques have been successful in classification tasks, they rely on the assumption of linearity among training examples, which may not hold in continuous label spaces such as regression. In this study, we demonstrate that mixing examples with significant data or label distance may negatively impact model performance. As a solution, we propose MixRL, a meta-learning framework that adopts a stricter assumption of linearity within certain data or label distances that vary for each example. By using Monte Carlo policy gradient reinforcement learning, MixRL learns the optimal number of nearest neighbors to mix with for each example, resulting in improved model performance. Our experiments using both synthetic and real datasets demonstrate that MixRL outperforms existing data augmentation techniques, and can also be combined with Mixup techniques for classification tasks for even better results.",1
"Modern reinforcement learning (RL) commonly engages practical problems with large state spaces, where function approximation must be deployed to approximate either the value function or the policy. While recent progresses in RL theory address a rich set of RL problems with general function approximation, such successes are mostly restricted to the single-agent setting. It remains elusive how to extend these results to multi-agent RL, especially due to the new challenges arising from its game-theoretical nature. This paper considers two-player zero-sum Markov Games (MGs). We propose a new algorithm that can provably find the Nash equilibrium policy using a polynomial number of samples, for any MG with low multi-agent Bellman-Eluder dimension -- a new complexity measure adapted from its single-agent version (Jin et al., 2021). A key component of our new algorithm is the exploiter, which facilitates the learning of the main player by deliberately exploiting her weakness. Our theoretical framework is generic, which applies to a wide range of models including but not limited to tabular MGs, MGs with linear or kernel function approximation, and MGs with rich observations.",0
"The field of modern reinforcement learning (RL) faces practical challenges when dealing with large state spaces, requiring function approximation to approximate either the value function or the policy. Although recent advancements in RL theory have successfully tackled a range of problems with general function approximation, these successes have primarily been limited to the single-agent setting. As a result, extending these results to multi-agent RL has proven to be challenging due to the game-theoretical nature of the problem. This paper focuses on two-player zero-sum Markov Games (MGs) and presents a new algorithm that can find the Nash equilibrium policy with a polynomial number of samples, for any MG with low multi-agent Bellman-Eluder dimension. The algorithm employs an exploiter to facilitate the learning of the main player by exploiting her weakness. The proposed theoretical framework is generic and can be applied to various models, including but not limited to tabular MGs, MGs with linear or kernel function approximation, and MGs with rich observations.",1
"We study a theory of reinforcement learning (RL) in which the learner receives binary feedback only once at the end of an episode. While this is an extreme test case for theory, it is also arguably more representative of real-world applications than the traditional requirement in RL practice that the learner receive feedback at every time step. Indeed, in many real-world applications of reinforcement learning, such as self-driving cars and robotics, it is easier to evaluate whether a learner's complete trajectory was either ""good"" or ""bad,"" but harder to provide a reward signal at each step. To show that learning is possible in this more challenging setting, we study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efficient algorithm that achieves sub-linear regret.",0
"The theory of reinforcement learning (RL) that we are studying involves the learner receiving binary feedback only once, at the end of an episode. Although this is considered an extreme test case for the theory, it may be more applicable to real-world applications than the traditional RL practice of giving feedback at every time step. For example, in applications such as self-driving cars and robotics, it may be easier to evaluate the overall success or failure of a learner's trajectory rather than providing a reward at each step. To prove that learning is possible in this challenging setting, we examine a scenario where trajectory labels are generated by an unknown parametric model and offer an algorithm that is both statistically and computationally efficient, achieving sub-linear regret.",1
"The shortcomings of maximum likelihood estimation in the context of model-based reinforcement learning have been highlighted by an increasing number of papers. When the model class is misspecified or has a limited representational capacity, model parameters with high likelihood might not necessarily result in high performance of the agent on a downstream control task. To alleviate this problem, we propose an end-to-end approach for model learning which directly optimizes the expected returns using implicit differentiation. We treat a value function that satisfies the Bellman optimality operator induced by the model as an implicit function of model parameters and show how to differentiate the function. We provide theoretical and empirical evidence highlighting the benefits of our approach in the model misspecification regime compared to likelihood-based methods.",0
"Several papers have pointed out the drawbacks of maximum likelihood estimation when applied to model-based reinforcement learning. In cases where the model class is poorly specified or has limited representation capabilities, high likelihood model parameters may not necessarily lead to optimal agent performance in a downstream control task. To address this issue, we suggest an end-to-end approach to model learning that directly optimizes expected returns through implicit differentiation. Our approach views a value function that fulfills the Bellman optimality operator induced by the model as an implicit function of model parameters and demonstrates how to perform differentiation of said function. We present both theoretical and empirical evidence that supports the advantages of our method in the context of model misspecification when compared to likelihood-based approaches.",1
"The distributional reinforcement learning (RL) approach advocates for representing the complete probability distribution of the random return instead of only modelling its expectation. A distributional RL algorithm may be characterised by two main components, namely the representation and parameterisation of the distribution and the probability metric defining the loss. This research considers the unconstrained monotonic neural network (UMNN) architecture, a universal approximator of continuous monotonic functions which is particularly well suited for modelling different representations of a distribution (PDF, CDF, quantile function). This property enables the decoupling of the effect of the function approximator class from that of the probability metric. The paper firstly introduces a methodology for learning different representations of the random return distribution. Secondly, a novel distributional RL algorithm named unconstrained monotonic deep Q-network (UMDQN) is presented. Lastly, in light of this new algorithm, an empirical comparison is performed between three probability quasimetrics, namely the Kullback-Leibler divergence, Cramer distance and Wasserstein distance. The results call for a reconsideration of all probability metrics in distributional RL, which contrasts with the dominance of the Wasserstein distance in recent publications.",0
"Instead of only modeling the expectation of the random return, the distributional reinforcement learning (RL) approach advocates for representing the complete probability distribution. A distributional RL algorithm has two main components: the representation and parameterization of the distribution, and the probability metric defining the loss. The unconstrained monotonic neural network (UMNN) architecture is a universal approximator of continuous monotonic functions that is well-suited for modeling different representations of a distribution (PDF, CDF, quantile function). This allows for the decoupling of the function approximator class from the probability metric. This research introduces a methodology for learning different representations of the random return distribution and presents a new distributional RL algorithm called the unconstrained monotonic deep Q-network (UMDQN). An empirical comparison between three probability quasimetrics (Kullback-Leibler divergence, Cramer distance, and Wasserstein distance) is performed in light of this new algorithm, leading to a reconsideration of all probability metrics in distributional RL, which contradicts recent publications' dominance of the Wasserstein distance.",1
"This work demonstrates that deep neural networks (DNNs) can solve a combinatorial problem merely through self-supervised learning. While researchers have employed explicit logic, heuristics, and reinforcement learning to tackle combinatorial problems, such methods are often complex and costly to implement, requiring lots of knowledge, coding, and adjustments. Hence, in the present study, I propose a robust and straightforward method of self-supervised learning to solve a combinatorial problem. Specifically, taking Rubik's Cube as an example, this work shows that a DNN can implicitly learn convoluted probability distributions of optimal choices from randomly generated combinations. Tested on $1,000$ Rubik's Cube instances, a DNN successfully solved all of them near-optimally. Although the proposed method is validated only on Rubik's Cube, it is potentially useful for other problems and real-world applications with its simplicity, stability, and robustness.",0
"The study revealed that deep neural networks (DNNs) have the ability to solve combinatorial problems through self-supervised learning, without the need for complex and costly methods such as explicit logic, heuristics, and reinforcement learning. The author proposes a straightforward and robust technique for self-supervised learning, using Rubik's Cube as an example. The study demonstrates that a DNN can implicitly learn the intricate probability distributions of optimal choices from randomly generated combinations, successfully solving all $1,000$ Rubik's Cube instances near-optimally. Although the method was only tested on Rubik's Cube, it has the potential to be useful in other real-world applications due to its simplicity, stability, and robustness.",1
"The optimal way for a deep reinforcement learning (DRL) agent to explore is to learn a set of skills that achieves a uniform distribution of states. Following this,we introduce DisTop, a new model that simultaneously learns diverse skills and focuses on improving rewarding skills. DisTop progressively builds a discrete topology of the environment using an unsupervised contrastive loss, a growing network and a goal-conditioned policy. Using this topology, a state-independent hierarchical policy can select where the agent has to keep discovering skills in the state space. In turn, the newly visited states allows an improved learnt representation and the learning loop continues. Our experiments emphasize that DisTop is agnostic to the ground state representation and that the agent can discover the topology of its environment whether the states are high-dimensional binary data, images, or proprioceptive inputs. We demonstrate that this paradigm is competitiveon MuJoCo benchmarks with state-of-the-art algorithms on both single-task dense rewards and diverse skill discovery. By combining these two aspects, we showthat DisTop achieves state-of-the-art performance in comparison with hierarchical reinforcement learning (HRL) when rewards are sparse. We believe DisTop opens new perspectives by showing that bottom-up skill discovery combined with representation learning can unlock the exploration challenge in DRL.",0
"To achieve a uniform distribution of states, a deep reinforcement learning (DRL) agent should learn a set of skills. DisTop is a novel model that focuses on improving rewarding skills while simultaneously learning diverse skills. DisTop constructs a discrete topology of the environment using unsupervised contrastive loss, a growing network, and a goal-conditioned policy. Using this topology, a hierarchical policy selects where the agent should explore for new skills in the state space. By visiting new states, the agent can improve its learned representation and continue the learning loop. DisTop works well with different types of data and can discover the topology of its environment. It outperforms state-of-the-art algorithms on MuJoCo benchmarks for both single-task dense rewards and diverse skill discovery. When rewards are sparse, DisTop achieves state-of-the-art performance compared to hierarchical reinforcement learning (HRL). DisTop combines bottom-up skill discovery with representation learning to overcome the exploration challenge in DRL, opening new perspectives.",1
"We propose ScheduleNet, a RL-based real-time scheduler, that can solve various types of multi-agent scheduling problems. We formulate these problems as a semi-MDP with episodic reward (makespan) and learn ScheduleNet, a decentralized decision-making policy that can effectively coordinate multiple agents to complete tasks. The decision making procedure of ScheduleNet includes: (1) representing the state of a scheduling problem with the agent-task graph, (2) extracting node embeddings for agent and tasks nodes, the important relational information among agents and tasks, by employing the type-aware graph attention (TGA), and (3) computing the assignment probability with the computed node embeddings. We validate the effectiveness of ScheduleNet as a general learning-based scheduler for solving various types of multi-agent scheduling tasks, including multiple salesman traveling problem (mTSP) and job shop scheduling problem (JSP).",0
"ScheduleNet is a real-time scheduler that uses RL to address a range of multi-agent scheduling issues. Our approach employs a semi-MDP with episodic reward (makespan) and decentralised decision-making policy to effectively coordinate multiple agents in completing tasks. To achieve this, ScheduleNet utilises a three-step decision-making process: (1) representation of the scheduling problem via an agent-task graph, (2) extraction of node embeddings for agent and task nodes, and relational information among agents and tasks using type-aware graph attention (TGA), and (3) computation of assignment probabilities using the computed node embeddings. ScheduleNet's efficacy as a general learning-based scheduler for solving various types of multi-agent scheduling tasks, such as mTSP and JSP, has been demonstrated through validation.",1
"How to obtain good value estimation is one of the key problems in Reinforcement Learning (RL). Current value estimation methods, such as DDPG and TD3, suffer from unnecessary over- or underestimation bias. In this paper, we explore the potential of double actors, which has been neglected for a long time, for better value function estimation in continuous setting. First, we uncover and demonstrate the bias alleviation property of double actors by building double actors upon single critic and double critics to handle overestimation bias in DDPG and underestimation bias in TD3 respectively. Next, we interestingly find that double actors help improve the exploration ability of the agent. Finally, to mitigate the uncertainty of value estimate from double critics, we further propose to regularize the critic networks under double actors architecture, which gives rise to Double Actors Regularized Critics (DARC) algorithm. Extensive experimental results on challenging continuous control tasks show that DARC significantly outperforms state-of-the-art methods with higher sample efficiency.",0
"The problem of obtaining accurate value estimation is a major challenge in Reinforcement Learning (RL). Currently used methods, such as DDPG and TD3, are susceptible to over- or underestimation biases. This study focuses on examining the potential of double actors, an overlooked approach, for improving value function estimation in continuous settings. The study demonstrates that double actors can alleviate bias by using double actors on a single critic and double critics to handle overestimation bias in DDPG and underestimation bias in TD3. The study also found that double actors can enhance the exploration ability of the agent. To address uncertainties in value estimates from double critics, the study proposes a regularization approach for the critic networks under the double actors architecture, which leads to the Double Actors Regularized Critics (DARC) algorithm. The study's extensive experiments on challenging continuous control tasks show that DARC outperforms existing methods with higher sample efficiency.",1
"We propose an algorithm for tabular episodic reinforcement learning with constraints. We provide a modular analysis with strong theoretical guarantees for settings with concave rewards and convex constraints, and for settings with hard constraints (knapsacks). Most of the previous work in constrained reinforcement learning is limited to linear constraints, and the remaining work focuses on either the feasibility question or settings with a single episode. Our experiments demonstrate that the proposed algorithm significantly outperforms these approaches in existing constrained episodic environments.",0
"Our proposal presents an algorithm that tackles tabular episodic reinforcement learning while adhering to constraints. Our approach provides a modular analysis that comes with powerful theoretical guarantees for concave rewards and convex constraints, as well as hard constraints (knapsacks). Much of the prior work in constrained reinforcement learning only handles linear constraints, while the rest focuses on single-episode scenarios or the feasibility question. Our experiments show that our proposed algorithm surpasses these methods in constrained episodic environments that exist today.",1
"We present a general optimization framework for emergent belief-state representation without any supervision. We employed the common configuration of multiagent reinforcement learning and communication to improve exploration coverage over an environment by leveraging the knowledge of each agent. In this paper, we obtained that recurrent neural nets (RNNs) with shared weights are highly biased in partially observable environments because of their noncooperativity. To address this, we designated an unbiased version of self-play via mechanism design, also known as reverse game theory, to clarify unbiased knowledge at the Bayesian Nash equilibrium. The key idea is to add imaginary rewards using the peer prediction mechanism, i.e., a mechanism for mutually criticizing information in a decentralized environment. Numerical analyses, including StarCraft exploration tasks with up to 20 agents and off-the-shelf RNNs, demonstrate the state-of-the-art performance.",0
"A framework for optimizing belief-state representation without supervision is presented in this study. The common configuration of multiagent reinforcement learning and communication was utilized to enhance exploration coverage over an environment by utilizing the knowledge of every agent. The study found that recurrent neural nets (RNNs) with shared weights are highly biased in partially observable environments due to their lack of cooperativity. To address this issue, an unbiased version of self-play through mechanism design, also known as reverse game theory, was developed to provide unbiased knowledge at Bayesian Nash equilibrium. The key concept involves the use of imaginary rewards through the peer prediction mechanism, which is a mechanism for criticizing information in a decentralized environment. Numerical analyses, including StarCraft exploration tasks with up to 20 agents and off-the-shelf RNNs, demonstrate the state-of-the-art performance.",1
"In reinforcement learning, experience replay stores past samples for further reuse. Prioritized sampling is a promising technique to better utilize these samples. Previous criteria of prioritization include TD error, recentness and corrective feedback, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justifications for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo, Atari and Meta-World.",0
"In reinforcement learning, the technique of experience replay is used to store past samples for future use. To better utilize these samples, prioritized sampling has been proposed, with previous criteria including factors such as TD error, recentness, and corrective feedback. However, these criteria have largely been heuristically designed. This work takes a different approach by beginning with the objective of regret minimization and deriving an optimal prioritization strategy for Bellman updates that can directly maximize policy return. The theory suggests that samples with higher hindsight TD error, better on-policiness, and more accurate Q value should be given higher priority during sampling. While previous criteria only partially consider this strategy, this work provides both theoretical justifications for prior criteria and introduces two new methods for computing prioritization weight: ReMERN, which learns an error network, and ReMERT, which exploits the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging RL benchmarks such as MuJoCo, Atari, and Meta-World.",1
"The performance of reinforcement learning depends upon designing an appropriate action space, where the effect of each action is measurable, yet, granular enough to permit flexible behavior. So far, this process involved non-trivial user choices in terms of the available actions and their execution frequency. We propose a novel framework for reinforcement learning that effectively lifts such constraints. Within our framework, agents learn effective behavior over a routine space: a new, higher-level action space, where each routine represents a set of 'equivalent' sequences of granular actions with arbitrary length. Our routine space is learned end-to-end to facilitate the accomplishment of underlying off-policy reinforcement learning objectives. We apply our framework to two state-of-the-art off-policy algorithms and show that the resulting agents obtain relevant performance improvements while requiring fewer interactions with the environment per episode, improving computational efficiency.",0
"Designing an appropriate action space is crucial for the success of reinforcement learning, as it must allow for measurable effects while still permitting flexible behavior. However, this process often involves difficult user decisions regarding the available actions and their execution frequency. Our proposed framework for reinforcement learning eliminates these constraints by introducing a routine space, a higher-level action space where each routine represents a set of equivalent sequences of granular actions. This routine space is learned end-to-end to facilitate off-policy reinforcement learning objectives, resulting in improved performance and computational efficiency with fewer interactions required per episode. We demonstrate the effectiveness of our framework by applying it to two state-of-the-art algorithms.",1
"Continual Learning (CL) considers the problem of training an agent sequentially on a set of tasks while seeking to retain performance on all previous tasks. A key challenge in CL is catastrophic forgetting, which arises when performance on a previously mastered task is reduced when learning a new task. While a variety of methods exist to combat forgetting, in some cases tasks are fundamentally incompatible with each other and thus cannot be learnt by a single policy. This can occur, in reinforcement learning (RL) when an agent may be rewarded for achieving different goals from the same observation. In this paper we formalize this ``interference'' as distinct from the problem of forgetting. We show that existing CL methods based on single neural network predictors with shared replay buffers fail in the presence of interference. Instead, we propose a simple method, OWL, to address this challenge. OWL learns a factorized policy, using shared feature extraction layers, but separate heads, each specializing on a new task. The separate heads in OWL are used to prevent interference. At test time, we formulate policy selection as a multi-armed bandit problem, and show it is possible to select the best policy for an unknown task using feedback from the environment. The use of bandit algorithms allows the OWL agent to constructively re-use different continually learnt policies at different times during an episode. We show in multiple RL environments that existing replay based CL methods fail, while OWL is able to achieve close to optimal performance when training sequentially.",0
"The concept of Continual Learning (CL) involves teaching an agent a series of tasks while maintaining its proficiency in all previously learned tasks. One of the main difficulties in CL is catastrophic forgetting, which occurs when an agent's ability to perform a previously learned task diminishes as it learns a new one. Although there are various techniques to address forgetting, some tasks are incompatible with each other and cannot be learned by a single policy. In reinforcement learning (RL), an agent may be rewarded for achieving different goals from the same observation, leading to interference between tasks. In this paper, we categorize interference as a separate problem from forgetting. We demonstrate that existing CL approaches that rely on a single neural network predictor with shared replay buffers are ineffective when interference is present. Instead, we propose a straightforward solution called OWL, which employs a factorized policy with shared feature extraction layers and separate task-specific heads to avoid interference. During testing, we use a multi-armed bandit algorithm to select the best policy for an unknown task based on feedback from the environment. This allows the OWL agent to effectively reuse previously learned policies at different stages of an episode. We demonstrate that existing replay-based CL techniques fail in several RL environments, while OWL achieves nearly optimal results when training sequentially.",1
"In spite of the success of existing meta reinforcement learning methods, they still have difficulty in learning a meta policy effectively for RL problems with sparse reward. In this respect, we develop a novel meta reinforcement learning framework called Hyper-Meta RL(HMRL), for sparse reward RL problems. It is consisted with three modules including the cross-environment meta state embedding module which constructs a common meta state space to adapt to different environments; the meta state based environment-specific meta reward shaping which effectively extends the original sparse reward trajectory by cross-environmental knowledge complementarity and as a consequence the meta policy achieves better generalization and efficiency with the shaped meta reward. Experiments with sparse-reward environments show the superiority of HMRL on both transferability and policy learning efficiency.",0
"Despite the success of current meta reinforcement learning techniques, they struggle to effectively learn a meta policy for RL problems that have sparse rewards. To address this issue, we have developed a new framework for meta reinforcement learning called Hyper-Meta RL (HMRL). HMRL consists of three modules: the cross-environment meta state embedding module, which creates a shared meta state space to adapt to various environments; the meta state-based environment-specific meta reward shaping, which improves the original sparse reward trajectory with cross-environmental knowledge complementarity, resulting in better generalization and efficiency for the meta policy with the shaped meta reward. Our experiments on sparse-reward environments demonstrate that HMRL outperforms other methods in both transferability and policy learning efficiency.",1
"We provide new perspectives and inference algorithms for Maximum Entropy (MaxEnt) Inverse Reinforcement Learning (IRL), which provides a principled method to find a most non-committal reward function consistent with given expert demonstrations, among many consistent reward functions.   We first present a generalized MaxEnt formulation based on minimizing a KL-divergence instead of maximizing an entropy. This improves the previous heuristic derivation of the MaxEnt IRL model (for stochastic MDPs), allows a unified view of MaxEnt IRL and Relative Entropy IRL, and leads to a model-free learning algorithm for the MaxEnt IRL model. Second, a careful review of existing inference algorithms and implementations showed that they approximately compute the marginals required for learning the model. We provide examples to illustrate this, and present an efficient and exact inference algorithm. Our algorithm can handle variable length demonstrations; in addition, while a basic version takes time quadratic in the maximum demonstration length L, an improved version of this algorithm reduces this to linear using a padding trick.   Experiments show that our exact algorithm improves reward learning as compared to the approximate ones. Furthermore, our algorithm scales up to a large, real-world dataset involving driver behaviour forecasting. We provide an optimized implementation compatible with the OpenAI Gym interface. Our new insight and algorithms could possibly lead to further interest and exploration of the original MaxEnt IRL model.",0
"We offer fresh perspectives and inference algorithms for MaxEnt IRL, a method for determining a reward function that is most consistent with expert demonstrations. Our approach involves a generalized MaxEnt formulation that minimizes KL-divergence rather than maximizing entropy. This not only enhances the previous heuristic derivation of the MaxEnt IRL model for stochastic MDPs but also enables a unified view of MaxEnt IRL and Relative Entropy IRL. We also present a model-free learning algorithm for the MaxEnt IRL model. Our review of existing inference algorithms and implementations revealed that they only approximately compute the marginals necessary for learning the model. We provide an efficient and exact inference algorithm that can handle variable length demonstrations. While a basic version of this algorithm takes time quadratic in the maximum demonstration length L, an improved version reduces this to linear using a padding trick. Our experiments demonstrate that our exact algorithm improves reward learning compared to the approximate ones. Additionally, our algorithm scales up to a large, real-world dataset involving driver behavior forecasting. We have also provided an optimized implementation that is compatible with the OpenAI Gym interface. Our novel insights and algorithms have the potential to generate further interest and exploration of the original MaxEnt IRL model.",1
"We provide a framework for accelerating reinforcement learning (RL) algorithms by heuristics constructed from domain knowledge or offline data. Tabula rasa RL algorithms require environment interactions or computation that scales with the horizon of the sequential decision-making task. Using our framework, we show how heuristic-guided RL induces a much shorter-horizon subproblem that provably solves the original task. Our framework can be viewed as a horizon-based regularization for controlling bias and variance in RL under a finite interaction budget. On the theoretical side, we characterize properties of a good heuristic and its impact on RL acceleration. In particular, we introduce the novel concept of an ""improvable heuristic"" -- a heuristic that allows an RL agent to extrapolate beyond its prior knowledge. On the empirical side, we instantiate our framework to accelerate several state-of-the-art algorithms in simulated robotic control tasks and procedurally generated games. Our framework complements the rich literature on warm-starting RL with expert demonstrations or exploratory datasets, and introduces a principled method for injecting prior knowledge into RL.",0
"Our approach offers a structure that expedites reinforcement learning (RL) algorithms by employing heuristics based on domain knowledge or offline data. Standard RL algorithms demand interaction with the environment or computation that scales with the horizon of the sequential decision-making task. By using our structure, we demonstrate how RL guided by heuristics produces a subproblem with a shorter horizon that can solve the initial task. Our framework operates as horizon-based regularization for controlling bias and variance in RL under a limited interaction budget. On the theoretical side, we examine the features of a reliable heuristic and its influence on RL acceleration. One such concept we introduce is the ""improvable heuristic,"" which allows an RL agent to extrapolate beyond its prior knowledge. On the empirical side, we apply our framework to quicken various state-of-the-art algorithms in simulated robotic control tasks and procedurally generated games. Our framework complements the extensive literature on accelerating RL with expert demonstrations or exploratory datasets and establishes a systematic method for incorporating prior knowledge into RL.",1
"Kickstarting deep reinforcement learning algorithms facilitate a teacher-student relationship among the agents and allow for a well-performing teacher to share demonstrations with a student to expedite the student's training. However, despite the known benefits, the demonstrations may contain sensitive information about the teacher's training data and existing kickstarting methods do not take any measures to protect it. Therefore, we use the framework of differential privacy to develop a mechanism that securely shares the teacher's demonstrations with the student. The mechanism allows for the teacher to decide upon the accuracy of its demonstrations with respect to the privacy budget that it consumes, thereby granting the teacher full control over its data privacy. We then develop a kickstarted deep reinforcement learning algorithm for the student that is privacy-aware because we calibrate its objective with the parameters of the teacher's privacy mechanism. The privacy-aware design of the algorithm makes it possible to kickstart the student's learning despite the perturbations induced by the privacy mechanism. From numerical experiments, we highlight three empirical results: (i) the algorithm succeeds in expediting the student's learning, (ii) the student converges to a performance level that was not possible without the demonstrations, and (iii) the student maintains its enhanced performance even after the teacher stops sharing useful demonstrations due to its privacy budget constraints.",0
"The use of deep reinforcement learning algorithms to kickstart student agent training relies on a teacher-student relationship, where a proficient teacher can share demonstrations to expedite the learning process. However, there are concerns regarding the privacy of the teacher's training data contained in the demonstrations. Existing kickstarting methods do not address this issue, making it necessary to develop a mechanism that securely shares the demonstrations using the differential privacy framework. This mechanism allows the teacher to control the accuracy of the demonstrations while also ensuring data privacy. We then design a privacy-aware kickstarted deep reinforcement learning algorithm for the student that takes into account the teacher's privacy mechanism. This design allows the student to learn despite the perturbations introduced by the privacy mechanism. Experimental results show that the algorithm expedites the student's learning, enabling it to reach a higher performance level than without the demonstrations. Additionally, the student maintains its enhanced performance even after the teacher stops sharing demonstrations due to privacy budget constraints.",1
"We address the issue of safety in reinforcement learning. We pose the problem in an episodic framework of a constrained Markov decision process. Existing results have shown that it is possible to achieve a reward regret of $\tilde{\mathcal{O}}(\sqrt{K})$ while allowing an $\tilde{\mathcal{O}}(\sqrt{K})$ constraint violation in $K$ episodes. A critical question that arises is whether it is possible to keep the constraint violation even smaller. We show that when a strictly safe policy is known, then one can confine the system to zero constraint violation with arbitrarily high probability while keeping the reward regret of order $\tilde{\mathcal{O}}(\sqrt{K})$. The algorithm which does so employs the principle of optimistic pessimism in the face of uncertainty to achieve safe exploration. When no strictly safe policy is known, though one is known to exist, then it is possible to restrict the system to bounded constraint violation with arbitrarily high probability. This is shown to be realized by a primal-dual algorithm with an optimistic primal estimate and a pessimistic dual update.",0
"Our focus is on addressing safety concerns in reinforcement learning, specifically within the episodic framework of a constrained Markov decision process. Previous research has demonstrated that with a limited constraint violation of $\tilde{\mathcal{O}}(\sqrt{K})$ in $K$ episodes, a reward regret of $\tilde{\mathcal{O}}(\sqrt{K})$ can be achieved. However, we aim to explore whether it is possible to further minimize the constraint violation. Our findings reveal that if a strictly safe policy is available, it is possible to maintain zero constraint violation with a high probability while retaining a reward regret of $\tilde{\mathcal{O}}(\sqrt{K})$. This is accomplished through an algorithm that leverages optimistic pessimism to ensure safe exploration. In cases where a strictly safe policy is not known, but is known to exist, we demonstrate that it is possible to keep the constraint violation within a bounded range with a high probability. This is achieved through a primal-dual algorithm that utilizes an optimistic primal estimate and a pessimistic dual update.",1
"Many of the recent triumphs in machine learning are dependent on well-tuned hyperparameters. This is particularly prominent in reinforcement learning (RL) where a small change in the configuration can lead to failure. Despite the importance of tuning hyperparameters, it remains expensive and is often done in a naive and laborious way. A recent solution to this problem is Population Based Training (PBT) which updates both weights and hyperparameters in a single training run of a population of agents. PBT has been shown to be particularly effective in RL, leading to widespread use in the field. However, PBT lacks theoretical guarantees since it relies on random heuristics to explore the hyperparameter space. This inefficiency means it typically requires vast computational resources, which is prohibitive for many small and medium sized labs. In this work, we introduce the first provably efficient PBT-style algorithm, Population-Based Bandits (PB2). PB2 uses a probabilistic model to guide the search in an efficient way, making it possible to discover high performing hyperparameter configurations with far fewer agents than typically required by PBT. We show in a series of RL experiments that PB2 is able to achieve high performance with a modest computational budget.",0
"The success of machine learning in recent times has largely relied upon the proper tuning of hyperparameters, especially in reinforcement learning. Even minor alterations to the configuration can lead to failure. Despite the significance of hyperparameter tuning, it is usually done in a laborious and unsophisticated manner, which can be costly. Population Based Training (PBT) has emerged as a recent solution to this issue by updating both weights and hyperparameters in a single training run of a group of agents. PBT has proven to be especially effective in reinforcement learning, resulting in widespread adoption in the field. However, PBT suffers from a lack of theoretical guarantees and relies on random heuristics to explore the hyperparameter space, which makes it computationally intensive and unsuitable for smaller research labs. In this study, we present Population-Based Bandits (PB2) as the first PBT-style algorithm to be provably efficient. PB2 employs a probabilistic model to guide the search in a more efficient manner, allowing it to find high-performing hyperparameter configurations with significantly fewer agents than PBT. Our experiments in reinforcement learning demonstrate that PB2 can achieve excellent performance with a modest computational budget.",1
"Counterfactual instances are a powerful tool to obtain valuable insights into automated decision processes, describing the necessary minimal changes in the input space to alter the prediction towards a desired target. Most previous approaches require a separate, computationally expensive optimization procedure per instance, making them impractical for both large amounts of data and high-dimensional data. Moreover, these methods are often restricted to certain subclasses of machine learning models (e.g. differentiable or tree-based models). In this work, we propose a deep reinforcement learning approach that transforms the optimization procedure into an end-to-end learnable process, allowing us to generate batches of counterfactual instances in a single forward pass. Our experiments on real-world data show that our method i) is model-agnostic (does not assume differentiability), relying only on feedback from model predictions; ii) allows for generating target-conditional counterfactual instances; iii) allows for flexible feature range constraints for numerical and categorical attributes, including the immutability of protected features (e.g. gender, race); iv) is easily extended to other data modalities such as images.",0
"The use of counterfactual instances can provide valuable insights into automated decision-making processes. By identifying the minimal changes needed in the input space to achieve a desired outcome, significant information can be gained. However, previous approaches have been limited by the need for computationally expensive optimization procedures, which make them impractical for large or high-dimensional data sets. Additionally, these methods have typically been restricted to certain types of machine learning models. To overcome these limitations, we propose a deep reinforcement learning approach that allows for the creation of batches of counterfactual instances with a single forward pass. Our method is model-agnostic, does not require differentiability, and can generate target-conditional instances with flexible feature range constraints. It can also be easily extended to other data modalities, such as images, and can accommodate the immutability of protected features, such as gender and race. Our experiments on real-world data demonstrate the effectiveness and versatility of our approach.",1
"We consider the problem of estimating a linear time-invariant (LTI) dynamical system from a single trajectory via streaming algorithms, which is encountered in several applications including reinforcement learning (RL) and time-series analysis. While the LTI system estimation problem is well-studied in the {\em offline} setting, the practically important streaming/online setting has received little attention. Standard streaming methods like stochastic gradient descent (SGD) are unlikely to work since streaming points can be highly correlated. In this work, we propose a novel streaming algorithm, SGD with Reverse Experience Replay ($\mathsf{SGD}-\mathsf{RER}$), that is inspired by the experience replay (ER) technique popular in the RL literature. $\mathsf{SGD}-\mathsf{RER}$ divides data into small buffers and runs SGD backwards on the data stored in the individual buffers. We show that this algorithm exactly deconstructs the dependency structure and obtains information theoretically optimal guarantees for both parameter error and prediction error. Thus, we provide the first -- to the best of our knowledge -- optimal SGD-style algorithm for the classical problem of linear system identification with a first order oracle. Furthermore, $\mathsf{SGD}-\mathsf{RER}$ can be applied to more general settings like sparse LTI identification with known sparsity pattern, and non-linear dynamical systems. Our work demonstrates that the knowledge of data dependency structure can aid us in designing statistically and computationally efficient algorithms which can ""decorrelate"" streaming samples.",0
"The estimation of a linear time-invariant (LTI) dynamical system from a single trajectory using streaming algorithms is a problem encountered in various applications, such as reinforcement learning (RL) and time-series analysis. Although the LTI system estimation problem is extensively studied in the ""offline"" setting, the streaming/online setting, which is practically important, has not received much attention. Standard streaming methods, such as stochastic gradient descent (SGD), are not effective because of highly correlated streaming points. To address this issue, we propose a new streaming algorithm called SGD with Reverse Experience Replay ($\mathsf{SGD}-\mathsf{RER}$), which is inspired by the experience replay (ER) technique used in the RL literature. This algorithm divides data into small buffers and performs SGD backward on the data stored in the individual buffers, thus deconstructing the dependency structure. We demonstrate that $\mathsf{SGD}-\mathsf{RER}$ achieves information theoretically optimal guarantees for both parameter error and prediction error for the classical problem of linear system identification with a first-order oracle. Moreover, $\mathsf{SGD}-\mathsf{RER}$ can be applied to more general settings, such as sparse LTI identification with known sparsity pattern and non-linear dynamical systems. Our work shows that considering the data dependency structure can assist in designing statistically and computationally efficient algorithms that can ""decorrelate"" streaming samples.",1
"We study the problem of user association, namely finding the optimal assignment of user equipment to base stations to achieve a targeted network performance. In this paper, we focus on the knowledge transferability of association policies. Indeed, traditional non-trivial user association schemes are often scenario-specific or deployment-specific and require a policy re-design or re-learning when the number or the position of the users change. In contrast, transferability allows to apply a single user association policy, devised for a specific scenario, to other distinct user deployments, without needing a substantial re-learning or re-design phase and considerably reducing its computational and management complexity. To achieve transferability, we first cast user association as a multi-agent reinforcement learning problem. Then, based on a neural attention mechanism that we specifically conceived for this context, we propose a novel distributed policy network architecture, which is transferable among users with zero-shot generalization capability i.e., without requiring additional training.Numerical results show the effectiveness of our solution in terms of overall network communication rate, outperforming centralized benchmarks even when the number of users doubles with respect to the initial training point.",0
"The focus of our study is on solving the problem of assigning user equipment to base stations in order to achieve a specific network performance. Our research specifically looks at the transferability of association policies. Typically, user association schemes are specific to certain scenarios or deployments and require re-design or re-learning when the number or position of users changes. However, transferable policies can be applied across different user deployments without the need for extensive re-learning or re-design, reducing computational and management complexity. To achieve this, we use multi-agent reinforcement learning and a neural attention mechanism to propose a novel distributed policy network architecture that can generalize to new users without additional training. Our numerical results show that our solution outperforms centralized benchmarks even when the number of users doubles.",1
"The ability to exploit prior experience to solve novel problems rapidly is a hallmark of biological learning systems and of great practical importance for artificial ones. In the meta reinforcement learning literature much recent work has focused on the problem of optimizing the learning process itself. In this paper we study a complementary approach which is conceptually simple, general, modular and built on top of recent improvements in off-policy learning. The framework is inspired by ideas from the probabilistic inference literature and combines robust off-policy learning with a behavior prior, or default behavior that constrains the space of solutions and serves as a bias for exploration; as well as a representation for the value function, both of which are easily learned from a number of training tasks in a multi-task scenario. Our approach achieves competitive adaptation performance on hold-out tasks compared to meta reinforcement learning baselines and can scale to complex sparse-reward scenarios.",0
"The rapid resolution of new problems using prior knowledge is a defining characteristic of both biological and artificial learning systems. Recent research in meta reinforcement learning has concentrated on optimizing the learning process, but our study adopts a different, yet simple, approach. This approach is modular and general, and it builds upon advancements in off-policy learning. The framework is based on concepts from probabilistic inference literature, and it combines robust off-policy learning with a behavior prior, which limits the range of possible solutions and provides a bias for exploration. Additionally, it includes a representation for the value function, which is easily learned from several training tasks in a multi-task environment. Our approach achieves competitive adaptation performance on hold-out tasks when compared to meta reinforcement learning baselines, and it can handle complex sparse-reward situations at scale.",1
"Data centres (DCs) underline many prominent future technological trends such as distributed training of large scale machine learning models and internet-of-things based platforms. DCs will soon account for over 3\% of global energy demand, so efficient use of DC resources is essential. Robust DC networks (DCNs) are essential to form the large scale systems needed to handle this demand, but can bottleneck how efficiently DC-server resources can be used when servers with insufficient connectivity between them cannot be jointly allocated to a job. However, allocating servers' resources whilst accounting for their inter-connectivity maps to an NP-hard combinatorial optimisation problem, and so is often ignored in DC resource management schemes. We present Nara, a framework based on reinforcement learning (RL) and graph neural networks (GNN) to learn network-aware allocation policies that increase the number of requests allocated over time compared to previous methods. Unique to our solution is the use of a GNN to generate representations of server-nodes in the DCN, which are then interpreted as actions by a RL policy-network which chooses from which servers resources will be allocated to incoming requests. Nara is agnostic to the topology size and shape and is trained end-to-end. The method can accept up to 33\% more requests than the best baseline when deployed on DCNs with up to the order of $10\times$ more compute nodes than the DCN seen during training and is able to maintain its policy's performance on DCNs with the order of $100\times$ more servers than seen during training. It also generalises to unseen DCN topologies with varied network structure and unseen request distributions without re-training.",0
"The increasing demand for data centres (DCs) is closely linked to emerging technological trends, including distributed training for large scale machine learning models and internet-of-things based platforms. DCs are expected to consume more than 3% of the world's energy in the near future, making efficient use of DC resources essential. However, the connectivity of DC servers can create bottlenecks for resource allocation, limiting the efficiency of DC networks (DCNs). Despite this, DC resource management schemes often overlook the complex combinatorial optimisation problem of allocating resources while accounting for inter-connectivity. In response, we propose Nara, a framework that uses reinforcement learning (RL) and graph neural networks (GNN) to develop network-aware allocation policies. Nara has several unique features, including the use of a GNN to generate server-node representations and an RL policy-network that selects the servers from which resources will be allocated to incoming requests. Nara is highly adaptable, as it can be deployed on DCNs of various sizes and shapes, and it can handle up to 33% more requests than the best baseline. Additionally, Nara can maintain its policy's performance on DCNs with up to 100 times more servers than seen during training and can generalise to unseen DCN topologies and request distributions without re-training.",1
"Intelligent agents must pursue their goals in complex environments with partial information and often limited computational capacity. Reinforcement learning methods have achieved great success by creating agents that optimize engineered reward functions, but which often struggle to learn in sparse-reward environments, generally require many environmental interactions to perform well, and are typically computationally very expensive. Active inference is a model-based approach that directs agents to explore uncertain states while adhering to a prior model of their goal behaviour. This paper introduces an active inference agent which minimizes the novel free energy of the expected future. Our model is capable of solving sparse-reward problems with a very high sample efficiency due to its objective function, which encourages directed exploration of uncertain states. Moreover, our model is computationally very light and can operate in a fully online manner while achieving comparable performance to offline RL methods. We showcase the capabilities of our model by solving the mountain car problem, where we demonstrate its superior exploration properties and its robustness to observation noise, which in fact improves performance. We also introduce a novel method for approximating the prior model from the reward function, which simplifies the expression of complex objectives and improves performance over previous active inference approaches.",0
"Agents with limited computational capacity must operate in complex environments with incomplete information to achieve their goals. While reinforcement learning has been successful in optimizing agents with engineered reward functions, such agents often face challenges in learning in sparse-reward environments and require significant environmental interactions, making them computationally expensive. On the other hand, active inference is a model-based approach that enables agents to explore uncertain states while following a prior model of their goal behavior. This paper presents an active inference agent that minimizes the novel free energy of the expected future. Our model is highly sample efficient due to its objective function, which encourages directed exploration of uncertain states, and is computationally lightweight, making it suitable for fully online operation. We demonstrate the effectiveness of our model by solving the mountain car problem, showcasing its superior exploration capabilities and robustness to observation noise. Additionally, we introduce a novel approach for approximating the prior model from the reward function, simplifying complex objectives and improving performance compared to previous active inference methods.",1
"We introduce RL-DARTS, one of the first applications of Differentiable Architecture Search (DARTS) in reinforcement learning (RL) to search for convolutional cells, applied to the Procgen benchmark. We outline the initial difficulties of applying neural architecture search techniques in RL, and demonstrate that by simply replacing the image encoder with a DARTS supernet, our search method is sample-efficient, requires minimal extra compute resources, and is also compatible with off-policy and on-policy RL algorithms, needing only minor changes in preexisting code. Surprisingly, we find that the supernet can be used as an actor for inference to generate replay data in standard RL training loops, and thus train end-to-end. Throughout this training process, we show that the supernet gradually learns better cells, leading to alternative architectures which can be highly competitive against manually designed policies, but also verify previous design choices for RL policies.",0
"Our team presents RL-DARTS, one of the earliest implementations of Differentiable Architecture Search (DARTS) in reinforcement learning (RL) for exploring convolutional cells in the Procgen benchmark. We discuss the challenges associated with applying neural architecture search methodologies in RL and illustrate how we have resolved these issues by substituting the image encoder with a DARTS supernet. Our search method is effective, necessitates minimal additional computing resources, and is adaptable to on-policy and off-policy RL algorithms with only minor modifications to existing code. Surprisingly, we have discovered that the supernet can serve as an actor for inference and generate replay data in standard RL training loops, allowing for end-to-end training. We have demonstrated that the supernet progressively improves cell quality throughout the training process, resulting in alternative architectures that are highly competitive with manually created policies while also confirming previous design choices for RL policies.",1
"Recently, deep multi-agent reinforcement learning (MARL) has shown the promise to solve complex cooperative tasks. Its success is partly because of parameter sharing among agents. However, such sharing may lead agents to behave similarly and limit their coordination capacity. In this paper, we aim to introduce diversity in both optimization and representation of shared multi-agent reinforcement learning. Specifically, we propose an information-theoretical regularization to maximize the mutual information between agents' identities and their trajectories, encouraging extensive exploration and diverse individualized behaviors. In representation, we incorporate agent-specific modules in the shared neural network architecture, which are regularized by L1-norm to promote learning sharing among agents while keeping necessary diversity. Empirical results show that our method achieves state-of-the-art performance on Google Research Football and super hard StarCraft II micromanagement tasks.",0
"Deep multi-agent reinforcement learning (MARL) has recently demonstrated its potential in solving complex cooperative tasks by utilizing parameter sharing among agents. However, sharing parameters can result in agents exhibiting similar behavior and limiting their capacity for coordination. This study aims to address this issue by introducing diversity in both optimization and representation of shared multi-agent reinforcement learning. To achieve this, the authors propose an information-theoretical regularization method that maximizes the mutual information between agents' identities and their trajectories, promoting exploration and diverse individualized behaviors. Additionally, agent-specific modules are incorporated into the shared neural network architecture and regularized by L1-norm to facilitate learning sharing among agents while maintaining necessary diversity. The study's empirical results show that this method achieves state-of-the-art performance on challenging tasks such as Google Research Football and super hard StarCraft II micromanagement.",1
"For continuing environments, reinforcement learning methods commonly maximize a discounted reward criterion with discount factor close to 1 in order to approximate the steady-state reward (the gain). However, such a criterion only considers the long-run performance, ignoring the transient behaviour. In this work, we develop a policy gradient method that optimizes the gain, then the bias (which indicates the transient performance and is important to capably select from policies with equal gain). We derive expressions that enable sampling for the gradient of the bias, and its preconditioning Fisher matrix. We further propose an algorithm that solves the corresponding bi-level optimization using a logarithmic barrier. Experimental results provide insights into the fundamental mechanisms of our proposal.",0
"Reinforcement learning methods commonly aim to maximize a discounted reward criterion with a discount factor close to 1 for continuing environments. This approach approximates the steady-state reward, but neglects the transient behavior. To address this limitation, we introduce a policy gradient method that first optimizes the gain and then the bias. The bias is crucial for selecting policies with equal gain and reflects transient performance. To calculate the gradient of the bias and its preconditioning Fisher matrix, we derive specific expressions for sampling. To solve the corresponding bi-level optimization problem, we propose an algorithm that uses a logarithmic barrier. Our experimental results shed light on the underlying mechanisms of our approach.",1
"A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions simultaneously: a complex nonlinear encoder that maps high-dimensional observations to a latent representation space, and a simple linear policy over this space. We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder (Raileanu et al., 2021). We propose Cross-Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL can be viewed as having the same effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use of rewards and associated overfitting risks. Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite (Cobbe et al., 2020).",0
"The ability for a reinforcement learning (RL) agent to generalize policies learned on a few tasks across a high-dimensional observation space to similar tasks not seen during training is highly desirable yet a major challenge for deep RL approaches. To tackle this challenge, RL is viewed as a process of simultaneously training a complex nonlinear encoder that maps high-dimensional observations to a latent representation space and a simple linear policy over this space. To achieve a superior encoder for zero-shot generalization in RL, an auxiliary SSL objective is used to encourage the encoder to map behaviorally similar observations to similar representations, as reward-based signal may cause overfitting in the encoder. To this end, we propose Cross-Trajectory Representation Learning (CTRL), a method that conditions the RL agent's encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL avoids the use of rewards and associated overfitting risks and achieves better generalization performance on the challenging Procgen benchmark suite when combined with PPO, as demonstrated in our experiments that ablate various components of CTRL.",1
"In many contemporary applications such as healthcare, finance, robotics, and recommendation systems, continuous deployment of new policies for data collection and online learning is either cost ineffective or impractical. We consider a setting that lies between pure offline reinforcement learning (RL) and pure online RL called deployment constrained RL in which the number of policy deployments for data sampling is limited. To solve this challenging task, we propose a new algorithmic learning framework called Model-based Uncertainty regularized and Sample Efficient Batch Optimization (MUSBO). Our framework discovers novel and high quality samples for each deployment to enable efficient data collection. During each offline training session, we bootstrap the policy update by quantifying the amount of uncertainty within our collected data. In the high support region (low uncertainty), we encourage our policy by taking an aggressive update. In the low support region (high uncertainty) when the policy bootstraps into the out-of-distribution region, we downweight it by our estimated uncertainty quantification. Experimental results show that MUSBO achieves state-of-the-art performance in the deployment constrained RL setting.",0
"Continuous deployment of new policies for data collection and online learning can be impractical or cost-ineffective in many modern applications, including healthcare, finance, robotics, and recommendation systems. To address this issue, we propose a new learning framework called Model-based Uncertainty regularized and Sample Efficient Batch Optimization (MUSBO) that operates within a constrained deployment setting, lying between pure offline reinforcement learning (RL) and pure online RL. MUSBO leverages a quantification of uncertainty within the collected data to bootstrap the policy update during each offline training session. We encourage policy updates in high support regions while downweighting them in low support regions, where the policy bootstraps into the out-of-distribution region. Experimental results demonstrate that MUSBO outperforms other approaches in deployment constrained RL.",1
"Low-precision training has become a popular approach to reduce compute requirements, memory footprint, and energy consumption in supervised learning. In contrast, this promising approach has not yet enjoyed similarly widespread adoption within the reinforcement learning (RL) community, partly because RL agents can be notoriously hard to train even in full precision. In this paper we consider continuous control with the state-of-the-art SAC agent and demonstrate that a na\""ive adaptation of low-precision methods from supervised learning fails. We propose a set of six modifications, all straightforward to implement, that leaves the underlying agent and its hyperparameters unchanged but improves the numerical stability dramatically. The resulting modified SAC agent has lower memory and compute requirements while matching full-precision rewards, demonstrating that low-precision training can substantially accelerate state-of-the-art RL without parameter tuning.",0
"The use of low-precision training has gained popularity in supervised learning due to its ability to decrease compute requirements, memory usage, and energy consumption. However, this approach has not been widely adopted in reinforcement learning (RL) because RL agents are notoriously difficult to train, even with full precision. This paper examines continuous control with the state-of-the-art SAC agent and finds that simply adapting low-precision methods from supervised learning does not work. The authors propose six modifications that are easy to implement and improve numerical stability without changing the agent or its hyperparameters. The resulting modified SAC agent requires less memory and compute power while achieving the same rewards as full-precision training. This shows that low-precision training can significantly speed up state-of-the-art RL without the need for parameter tuning.",1
"Current deep reinforcement learning (RL) approaches incorporate minimal prior knowledge about the environment, limiting computational and sample efficiency. \textit{Objects} provide a succinct and causal description of the world, and many recent works have proposed unsupervised object representation learning using priors and losses over static object properties like visual consistency. However, object dynamics and interactions are also critical cues for objectness. In this paper we propose a framework for reasoning about object dynamics and behavior to rapidly determine minimal and task-specific object representations. To demonstrate the need to reason over object behavior and dynamics, we introduce a suite of RGBD MuJoCo object collection and avoidance tasks that, while intuitive and visually simple, confound state-of-the-art unsupervised object representation learning algorithms. We also highlight the potential of this framework on several Atari games, using our object representation and standard RL and planning algorithms to learn dramatically faster than existing deep RL algorithms.",0
"At present, deep reinforcement learning (RL) techniques utilize scant prior knowledge of the environment, causing limitations in computational and sample efficiency. To address this issue, researchers have proposed unsupervised object representation learning that employs priors and losses over static object properties like visual consistency to provide a succinct and causal description of the world. However, object dynamics and interactions are crucial cues for objectness, and this paper proposes a framework for rapidly determining minimal and task-specific object representations by reasoning about object dynamics and behavior. The need to reason over object behavior and dynamics is demonstrated through a set of intuitive and visually simple RGBD MuJoCo object collection and avoidance tasks that confound state-of-the-art unsupervised object representation learning algorithms. Additionally, the potential of this framework is highlighted on several Atari games, where it is shown to learn dramatically faster than existing deep RL algorithms by using our object representation along with standard RL and planning algorithms.",1
Policy evaluation is an important instrument for the comparison of different algorithms in Reinforcement Learning (RL). Yet even a precise knowledge of the value function $V^{\pi}$ corresponding to a policy $\pi$ does not provide reliable information on how far is the policy $\pi$ from the optimal one. We present a novel model-free upper value iteration procedure $({\sf UVIP})$ that allows us to estimate the suboptimality gap $V^{\star}(x) - V^{\pi}(x)$ from above and to construct confidence intervals for $V^\star$. Our approach relies on upper bounds to the solution of the Bellman optimality equation via martingale approach. We provide theoretical guarantees for ${\sf UVIP}$ under general assumptions and illustrate its performance on a number of benchmark RL problems.,0
"In Reinforcement Learning (RL), Policy evaluation is a crucial tool for comparing various algorithms. However, even an exact understanding of the value function $V^{\pi}$ for a policy $\pi$ does not offer dependable information about how far the policy $\pi$ is from the optimal one. Our team has developed a new model-free method for upper value iteration (${\sf UVIP}$), which allows us to estimate the suboptimality gap $V^{\star}(x) - V^{\pi}(x)$ from above. Additionally, we can construct confidence intervals for $V^\star$. We use upper bounds to the solution of the Bellman optimality equation through a martingale approach to achieve this. We offer theoretical assurances for ${\sf UVIP}$ under general assumptions and demonstrate its effectiveness on various benchmark RL problems.",1
"We present a new behavioural distance over the state space of a Markov decision process, and demonstrate the use of this distance as an effective means of shaping the learnt representations of deep reinforcement learning agents. While existing notions of state similarity are typically difficult to learn at scale due to high computational cost and lack of sample-based algorithms, our newly-proposed distance addresses both of these issues. In addition to providing detailed theoretical analysis, we provide empirical evidence that learning this distance alongside the value function yields structured and informative representations, including strong results on the Arcade Learning Environment benchmark.",0
"A novel way of measuring behavioural distance in the state space of a Markov decision process is introduced in this study. The proposed distance is proven to be an efficient approach for shaping deep reinforcement learning agents' learned representations. Unlike current state similarity concepts that are hard to learn using sample-based algorithms and require significant computational resources, our proposed distance tackles both challenges. The study provides a thorough theoretical analysis and concrete evidence that learning this distance together with the value function leads to structured and informative representations. The Arcade Learning Environment benchmark attests to the efficacy of this approach.",1
"Large sparse linear systems of equations are ubiquitous in science and engineering, such as those arising from discretizations of partial differential equations. Algebraic multigrid (AMG) methods are one of the most common methods of solving such linear systems, with an extensive body of underlying mathematical theory. A system of linear equations defines a graph on the set of unknowns and each level of a multigrid solver requires the selection of an appropriate coarse graph along with restriction and interpolation operators that map to and from the coarse representation. The efficiency of the multigrid solver depends critically on this selection and many selection methods have been developed over the years. Recently, it has been demonstrated that it is possible to directly learn the AMG interpolation and restriction operators, given a coarse graph selection. In this paper, we consider the complementary problem of learning to coarsen graphs for a multigrid solver. We propose a method using a reinforcement learning (RL) agent based on graph neural networks (GNNs), which can learn to perform graph coarsening on small training graphs and then be applied to unstructured large graphs. We demonstrate that this method can produce better coarse graphs than existing algorithms, even as the graph size increases and other properties of the graph are varied. We also propose an efficient inference procedure for performing graph coarsening that results in linear time complexity in graph size.",0
"Sparse linear systems of equations are found in various fields of science and engineering, particularly from discretizing partial differential equations. Algebraic multigrid (AMG) techniques are commonly used to solve these systems, and they have established mathematical theory. The method involves selecting an optimal coarse graph and restriction and interpolation operators, which determine the efficiency of the solution. Recent studies have shown that the AMG operators can be learned directly with a coarse graph selection. In this study, we focus on learning to coarsen graphs for a multigrid solver using a reinforcement learning (RL) agent based on graph neural networks (GNNs). The method can learn to coarsen small training graphs and be applied to larger unstructured graphs. Our proposed method produces superior coarse graphs compared to existing algorithms, even with varying graph properties and sizes. Additionally, we present an efficient inference procedure for graph coarsening that maintains linear time complexity in graph size.",1
"Multiple-Intent Inverse Reinforcement Learning (MI-IRL) seeks to find a reward function ensemble to rationalize demonstrations of different but unlabelled intents. Within the popular expectation maximization (EM) framework for learning probabilistic MI-IRL models, we present a warm-start strategy based on up-front clustering of the demonstrations in feature space. Our theoretical analysis shows that this warm-start solution produces a near-optimal reward ensemble, provided the behavior modes satisfy mild separation conditions. We also propose a MI-IRL performance metric that generalizes the popular Expected Value Difference measure to directly assesses learned rewards against the ground-truth reward ensemble. Our metric elegantly addresses the difficulty of pairing up learned and ground truth rewards via a min-cost flow formulation, and is efficiently computable. We also develop a MI-IRL benchmark problem that allows for more comprehensive algorithmic evaluations. On this problem, we find our MI-IRL warm-start strategy helps avoid poor quality local minima reward ensembles, resulting in a significant improvement in behavior clustering. Our extensive sensitivity analysis demonstrates that the quality of the learned reward ensembles is improved under various settings, including cases where our theoretical assumptions do not necessarily hold. Finally, we demonstrate the effectiveness of our methods by discovering distinct driving styles in a large real-world dataset of driver GPS trajectories.",0
"The aim of Multiple-Intent Inverse Reinforcement Learning (MI-IRL) is to identify a set of reward functions that explain demonstrations of different intents without labels. In the widely used expectation maximization (EM) framework for probabilistic MI-IRL models, we introduce a warm-start approach that clusters demonstrations in feature space. Our analysis demonstrates that this warm-start solution generates a reward ensemble that is nearly optimal, as long as the behavior modes adhere to mild separation conditions. We also propose a performance metric for MI-IRL that assesses learned rewards against the ground-truth reward ensemble. Our metric overcomes the challenge of pairing learned and ground-truth rewards via a min-cost flow formulation and is easy to compute. We also design a MI-IRL benchmark problem for more comprehensive algorithmic assessments. Our warm-start strategy for MI-IRL prevents the formation of poor quality local minima reward ensembles, leading to a significant improvement in behavior clustering on this problem. We conduct a thorough sensitivity analysis that demonstrates the enhanced quality of learned reward ensembles under various settings, even when our theoretical assumptions do not hold. Finally, we demonstrate the effectiveness of our methods by identifying distinct driving styles in a vast real-world dataset of driver GPS trajectories.",1
"A long-standing challenge in artificial intelligence is lifelong learning. In lifelong learning, many tasks are presented in sequence and learners must efficiently transfer knowledge between tasks while avoiding catastrophic forgetting over long lifetimes. On these problems, policy reuse and other multi-policy reinforcement learning techniques can learn many tasks. However, they can generate many temporary or permanent policies, resulting in memory issues. Consequently, there is a need for lifetime-scalable methods that continually refine a policy library of a pre-defined size. This paper presents a first approach to lifetime-scalable policy reuse. To pre-select the number of policies, a notion of task capacity, the maximal number of tasks that a policy can accurately solve, is proposed. To evaluate lifetime policy reuse using this method, two state-of-the-art single-actor base-learners are compared: 1) a value-based reinforcement learner, Deep Q-Network (DQN) or Deep Recurrent Q-Network (DRQN); and 2) an actor-critic reinforcement learner, Proximal Policy Optimisation (PPO) with or without Long Short-Term Memory layer. By selecting the number of policies based on task capacity, D(R)QN achieves near-optimal performance with 6 policies in a 27-task MDP domain and 9 policies in an 18-task POMDP domain; with fewer policies, catastrophic forgetting and negative transfer are observed. Due to slow, monotonic improvement, PPO requires fewer policies, 1 policy for the 27-task domain and 4 policies for the 18-task domain, but it learns the tasks with lower accuracy than D(R)QN. These findings validate lifetime-scalable policy reuse and suggest using D(R)QN for larger and PPO for smaller library sizes.",0
"Artificial intelligence faces a challenge with lifelong learning, where learners must transfer knowledge between tasks without forgetting previous information. Multi-policy reinforcement learning techniques solve many tasks, but can cause memory issues with the creation of temporary or permanent policies. Therefore, this paper proposes a lifetime-scalable policy reuse approach that uses a task capacity notion to pre-select the number of policies in a library. The Deep Q-Network (DQN) or Deep Recurrent Q-Network (DRQN) and Proximal Policy Optimisation (PPO) with or without Long Short-Term Memory layer are compared, with D(R)QN achieving optimal performance with 6 policies in a 27-task MDP domain and 9 policies in an 18-task POMDP domain. PPO requires fewer policies, but learns tasks with lower accuracy than D(R)QN. These findings suggest using D(R)QN for larger and PPO for smaller library sizes.",1
"Training a reinforcement learning agent to carry out natural language instructions is limited by the available supervision, i.e. knowing when the instruction has been carried out. We adapt the CLEVR visual question answering dataset to generate complex natural language navigation instructions and accompanying scene graphs, yielding an environment-agnostic supervised dataset. To demonstrate the use of this data set, we map the scenes to the VizDoom environment and use the architecture in \citet{gatedattention} to train an agent to carry out these more complex language instructions.",0
"The amount of supervision available is a constraint when it comes to teaching a reinforcement learning agent to execute natural language commands. To overcome this limitation, we modify the CLEVR visual question answering dataset to produce intricate navigation instructions in natural language, along with corresponding scene graphs. This results in a supervised dataset that is not bound to any particular environment. We apply this dataset by mapping the scenes to the VizDoom environment and employing the architecture described in \citet{gatedattention} to train an agent to execute these more intricate language commands.",1
"Task-oriented dialog (TOD) systems often need to formulate knowledge base (KB) queries corresponding to the user intent and use the query results to generate system responses. Existing approaches require dialog datasets to explicitly annotate these KB queries -- these annotations can be time consuming, and expensive. In response, we define the novel problems of predicting the KB query and training the dialog agent, without explicit KB query annotation. For query prediction, we propose a reinforcement learning (RL) baseline, which rewards the generation of those queries whose KB results cover the entities mentioned in subsequent dialog. Further analysis reveals that correlation among query attributes in KB can significantly confuse memory augmented policy optimization (MAPO), an existing state of the art RL agent. To address this, we improve the MAPO baseline with simple but important modifications suited to our task. To train the full TOD system for our setting, we propose a pipelined approach: it independently predicts when to make a KB query (query position predictor), then predicts a KB query at the predicted position (query predictor), and uses the results of predicted query in subsequent dialog (next response predictor). Overall, our work proposes first solutions to our novel problem, and our analysis highlights the research challenges in training TOD systems without query annotation.",0
"The process of formulating knowledge base (KB) queries and generating responses for task-oriented dialog (TOD) systems can be time-consuming and expensive, as it often involves explicit annotation of dialog datasets. To address this issue, we introduce a new approach that predicts KB queries and trains dialog agents without explicit annotation. We propose a reinforcement learning (RL) baseline for query prediction, which rewards the generation of queries with KB results covering the entities mentioned in subsequent dialog. However, we found that correlation among query attributes in KB can confuse memory augmented policy optimization (MAPO), a state-of-the-art RL agent. To overcome this, we modify the MAPO baseline for our task. Our proposed pipelined approach trains the full TOD system by predicting when to make a KB query (query position predictor), predicting the KB query at the predicted position (query predictor), and using the results of the predicted query in subsequent dialog (next response predictor). Our work provides solutions to the novel problem and identifies the research challenges in training TOD systems without query annotation.",1
"We study exploration using randomized value functions in Thompson Sampling (TS)-like algorithms in reinforcement learning. This type of algorithms enjoys appealing empirical performance. We show that when we use 1) a single random seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a worst-case $\widetilde{O}\left(H\sqrt{SAT}\right)$ regret bound for episodic time-inhomogeneous Markov Decision Process where $S$ is the size of state space, $A$ is the size of action space, $H$ is the planning horizon and $T$ is the number of interactions. This bound polynomially improves all existing bounds for TS-like algorithms based on randomized value functions, and for the first time, matches the $\Omega\left(H\sqrt{SAT}\right)$ lower bound up to logarithmic factors. Our result highlights that randomized exploration can be near-optimal, which was previously only achieved by optimistic algorithms.",0
"The focus of our research is the study of exploration in reinforcement learning algorithms that use randomized value functions, similar to Thompson Sampling. These algorithms have shown impressive empirical performance. We have discovered that by using a single random seed in each episode and a Bernstein-type magnitude of noise, we can achieve a worst-case regret bound of $\widetilde{O}\left(H\sqrt{SAT}\right)$ for episodic time-inhomogeneous Markov Decision Processes. This bound takes into account the size of the state and action space, the planning horizon, and the number of interactions. Our result is a significant improvement over existing bounds for TS-like algorithms that use randomized value functions. For the first time, our bound matches the $\Omega\left(H\sqrt{SAT}\right)$ lower bound up to logarithmic factors. Our findings highlight that randomized exploration can be nearly optimal, a feat previously only accomplished by optimistic algorithms.",1
"This paper presents the first {\em model-free}, {\em simulator-free} reinforcement learning algorithm for Constrained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation. The algorithm is named Triple-Q because it has three key components: a Q-function (also called action-value function) for the cumulative reward, a Q-function for the cumulative utility for the constraint, and a virtual-Queue that (over)-estimates the cumulative constraint violation. Under Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the three Q values. The algorithm updates the reward and utility Q-values with learning rates that depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the episodic CMDP setting, Triple-Q achieves $\tilde{\cal O}\left(\frac{1 }{\delta}H^4 S^{\frac{1}{2}}A^{\frac{1}{2}}K^{\frac{4}{5}} \right)$ regret, where $K$ is the total number of episodes, $H$ is the number of steps in each episode, $S$ is the number of states, $A$ is the number of actions, and $\delta$ is Slater's constant. Furthermore, Triple-Q guarantees zero constraint violation when $K$ is sufficiently large. Finally, the computational complexity of Triple-Q is similar to SARSA for unconstrained MDPs and is computationally efficient.",0
"The paper introduces Triple-Q, a novel reinforcement learning algorithm that is free from both model and simulator and is designed for Constrained Markov Decision Processes (CMDPs). The algorithm guarantees sublinear regret and zero constraint violation. Triple-Q is composed of three key components, namely a Q-function for cumulative reward, a Q-function for cumulative utility for the constraint, and a virtual-Queue that overestimates the cumulative constraint violation. At each step, the algorithm selects an action based on the pseudo-Q-value, which combines the three Q values. The reward and utility Q-values are updated with learning rates depending on the visit counts to the corresponding state-action pairs, which are periodically reset. In episodic CMDP settings, Triple-Q ensures regret of $\tilde{\cal O}\left(\frac{1 }{\delta}H^4 S^{\frac{1}{2}}A^{\frac{1}{2}}K^{\frac{4}{5}} \right)$, where $K$ is the number of episodes, $H$ is the number of steps in each episode, $S$ is the number of states, $A$ is the number of actions, and $\delta$ is Slater's constant. Additionally, Triple-Q guarantees zero constraint violation when $K$ is sufficiently large and has a computational complexity similar to that of SARSA for unconstrained MDPs, making it computationally efficient.",1
"This paper proposes a new reinforcement learning with hyperbolic discounting. Combining a new temporal difference error with the hyperbolic discounting in recursive manner and reward-punishment framework, a new scheme to learn the optimal policy is derived. In simulations, it is found that the proposal outperforms the standard reinforcement learning, although the performance depends on the design of reward and punishment. In addition, the averages of discount factors w.r.t. reward and punishment are different from each other, like a sign effect in animal behaviors.",0
"A novel approach to reinforcement learning with hyperbolic discounting is presented in this paper. By merging a unique temporal difference error with hyperbolic discounting in a recursive manner and utilizing a reward-punishment framework, a novel method for determining the optimal policy is developed. The proposed method surpasses the traditional reinforcement learning in simulations, though the efficacy is contingent on the reward and punishment system. Furthermore, the average discount factors for reward and punishment are disparate, similar to the sign effect seen in animal behaviors.",1
"Learning to reach goal states and learning diverse skills through mutual information (MI) maximization have been proposed as principled frameworks for self-supervised reinforcement learning, allowing agents to acquire broadly applicable multitask policies with minimal reward engineering. Starting from a simple observation that the standard goal-conditioned RL (GCRL) is encapsulated by the optimization objective of variational empowerment, we discuss how GCRL and MI-based RL can be generalized into a single family of methods, which we name variational GCRL (VGCRL), interpreting variational MI maximization, or variational empowerment, as representation learning methods that acquire functionally-aware state representations for goal reaching. This novel perspective allows us to: (1) derive simple but unexplored variants of GCRL to study how adding small representation capacity can already expand its capabilities; (2) investigate how discriminator function capacity and smoothness determine the quality of discovered skills, or latent goals, through modifying latent dimensionality and applying spectral normalization; (3) adapt techniques such as hindsight experience replay (HER) from GCRL to MI-based RL; and lastly, (4) propose a novel evaluation metric, named latent goal reaching (LGR), for comparing empowerment algorithms with different choices of latent dimensionality and discriminator parameterization. Through principled mathematical derivations and careful experimental studies, our work lays a novel foundation from which to evaluate, analyze, and develop representation learning techniques in goal-based RL.",0
"The use of mutual information (MI) maximization and learning to reach goal states has been suggested as effective approaches for self-supervised reinforcement learning. These methods enable agents to acquire versatile multitask policies with minimal reward engineering. By recognizing that standard goal-conditioned RL (GCRL) is encapsulated by the optimization objective of variational empowerment, we propose a new family of methods called variational GCRL (VGCRL). This framework interprets variational MI maximization as a representation learning method for goal reaching. Our perspective enables us to: (1) create unexplored GCRL variants by adding representation capacity, (2) determine the quality of discovered skills by modifying latent dimensionality and applying spectral normalization, (3) adapt GCRL techniques such as hindsight experience replay (HER) to MI-based RL, and (4) propose a new evaluation metric called latent goal reaching (LGR) for comparing empowerment algorithms. Our work provides a foundation for evaluating, analyzing, and developing representation learning techniques in goal-based RL.",1
"In computer vision and natural language processing, innovations in model architecture that lead to increases in model capacity have reliably translated into gains in performance. In stark contrast with this trend, state-of-the-art reinforcement learning (RL) algorithms often use only small MLPs, and gains in performance typically originate from algorithmic innovations. It is natural to hypothesize that small datasets in RL necessitate simple models to avoid overfitting; however, this hypothesis is untested. In this paper we investigate how RL agents are affected by exchanging the small MLPs with larger modern networks with skip connections and normalization, focusing specifically on soft actor-critic (SAC) algorithms. We verify, empirically, that na\""ively adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, we show that dataset size is not the limiting factor, and instead argue that intrinsic instability from the actor in SAC taking gradients through the critic is the culprit. We demonstrate that a simple smoothing method can mitigate this issue, which enables stable training with large modern architectures. After smoothing, larger models yield dramatic performance improvements for state-of-the-art agents -- suggesting that more ""easy"" gains may be had by focusing on model architectures in addition to algorithmic innovations.",0
"Increases in model capacity have consistently resulted in performance gains in computer vision and natural language processing. However, in contrast to this trend, state-of-the-art reinforcement learning (RL) algorithms often rely on small MLPs, with performance gains stemming from algorithmic innovations rather than model architecture. While it may be assumed that small datasets in RL require simple models to avoid overfitting, this hypothesis remains untested. This paper investigates the impact of replacing small MLPs with larger, modern networks with skip connections and normalization on soft actor-critic (SAC) algorithms. The results indicate that simply adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, the study shows that dataset size is not the limiting factor and suggests that intrinsic instability from the actor in SAC taking gradients through the critic is the root cause. The paper proposes a simple smoothing method to mitigate this issue, enabling stable training with larger modern architectures. After smoothing, larger models yield significant performance improvements for state-of-the-art agents, suggesting that model architecture should also be considered alongside algorithmic innovations for further performance gains.",1
"In this paper, we consider risk-sensitive sequential decision-making in model-based Reinforcement Learning (RL). Our contributions are two-fold. First, we introduce a novel and coherent quantification of risk, namely composite risk, which quantifies joint effect of aleatory and epistemic risk during the learning process. Existing works considered either aleatory or epistemic risk individually, or an additive combination of the two. We prove that the additive formulation is a particular case of the composite risk when the epistemic risk measure is replaced with expectation. Thus, the composite risk provides an estimate more sensitive to both aleatory and epistemic sources of uncertainties than the individual and additive formulations. Following that, we propose to use a bootstrapping method, SENTINEL-K, for performing distributional RL. SENTINEL-K uses an ensemble of $K$ learners to estimate the return distribution. We use the Follow The Regularised Leader (FTRL) to aggregate the return distributions of $K$ learners and to estimate the composite risk. We experimentally verify that SENTINEL-K estimates the return distribution better, and while used with composite risk estimate, demonstrates better risk-sensitive performance than state-of-the-art risk-sensitive and distributional RL algorithms.",0
"This article explores the concept of risk-sensitive sequential decision-making in model-based Reinforcement Learning (RL). The article presents two main contributions. Firstly, the introduction of a novel quantification of risk, composite risk, which considers both aleatory and epistemic risk during the learning process. This differs from existing works which have only looked at these risks independently or as an additive combination. The article shows that the additive formulation is a specific case of composite risk when epistemic risk is replaced with expectation. Therefore, composite risk is a more sensitive estimate of uncertainties than individual or additive formulations. Secondly, the article proposes the use of SENTINEL-K, a bootstrapping method for performing distributional RL. SENTINEL-K employs an ensemble of $K$ learners to estimate the return distribution, which is then aggregated using Follow The Regularised Leader (FTRL) to estimate composite risk. The article demonstrates that SENTINEL-K and composite risk outperform state-of-the-art risk-sensitive and distributional RL algorithms in experimental trials.",1
"In many RL applications, once training ends, it is vital to detect any deterioration in the agent performance as soon as possible. Furthermore, it often has to be done without modifying the policy and under minimal assumptions regarding the environment. In this paper, we address this problem by focusing directly on the rewards and testing for degradation. We consider an episodic framework, where the rewards within each episode are not independent, nor identically-distributed, nor Markov. We present this problem as a multivariate mean-shift detection problem with possibly partial observations. We define the mean-shift in a way corresponding to deterioration of a temporal signal (such as the rewards), and derive a test for this problem with optimal statistical power. Empirically, on deteriorated rewards in control problems (generated using various environment modifications), the test is demonstrated to be more powerful than standard tests - often by orders of magnitude. We also suggest a novel Bootstrap mechanism for False Alarm Rate control (BFAR), applicable to episodic (non-i.i.d) signal and allowing our test to run sequentially in an online manner. Our method does not rely on a learned model of the environment, is entirely external to the agent, and in fact can be applied to detect changes or drifts in any episodic signal.",0
"Detecting any decline in agent performance is crucial in many RL applications once training is complete. This is often done without any modifications to the policy and with minimal assumptions about the environment. We present a solution to this problem by directly focusing on rewards and testing for degradation. Our approach uses an episodic framework where rewards are not independent, identically-distributed, or Markov. We treat this as a multivariate mean-shift detection problem with partial observations. Our test is derived to detect the deterioration of a temporal signal, such as rewards, and has optimal statistical power. We demonstrate the effectiveness of our approach on deteriorated rewards in control problems generated using various environment modifications. Our test is shown to be more powerful than standard tests by several orders of magnitude. We also propose a novel Bootstrap mechanism for False Alarm Rate control (BFAR), applicable to episodic non-i.i.d signals, allowing our test to run sequentially in an online manner. Our method does not rely on a learned model of the environment and can detect changes or drifts in any episodic signal.",1
"Offline Reinforcement Learning methods seek to learn a policy from logged transitions of an environment, without any interaction. In the presence of function approximation, and under the assumption of limited coverage of the state-action space of the environment, it is necessary to enforce the policy to visit state-action pairs close to the support of logged transitions. In this work, we propose an iterative procedure to learn a pseudometric (closely related to bisimulation metrics) from logged transitions, and use it to define this notion of closeness. We show its convergence and extend it to the function approximation setting. We then use this pseudometric to define a new lookup based bonus in an actor-critic algorithm: PLOFF. This bonus encourages the actor to stay close, in terms of the defined pseudometric, to the support of logged transitions. Finally, we evaluate the method on hand manipulation and locomotion tasks.",0
"The goal of Offline Reinforcement Learning methods is to acquire a policy from recorded transitions of an environment without any interaction. When function approximation is present and the state-action space of the environment is not fully covered, it becomes necessary to ensure that the policy visits state-action pairs that are close to the support of recorded transitions. To accomplish this, our team has developed an iterative process for learning a pseudometric, similar to bisimulation metrics, from recorded transitions. We demonstrate the convergence of this method and extend it to the function approximation setting. We then apply this pseudometric to create a new lookup-based bonus in an actor-critic algorithm called PLOFF. This bonus incentivizes the actor to remain close to the support of recorded transitions as defined by the pseudometric. Finally, we evaluate the efficacy of this approach on hand manipulation and locomotion tasks.",1
"Policy-based reinforcement learning methods suffer from the policy collapse problem. We find valued-based reinforcement learning methods with {\epsilon}-greedy mechanism are capable of enjoying three characteristics, Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off, which help value-based methods avoid the policy collapse problem. However, there does not exist a parallel mechanism for policy-based methods that achieves all three characteristics. In this paper, we propose an entropy regularization free mechanism that is designed for policy-based methods, which achieves Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off. Our experiments show that our mechanism is super sample-efficient for policy-based methods and boosts a policy-based baseline to a new State-Of-The-Art on Arcade Learning Environment.",0
"The policy collapse problem is a challenge faced by policy-based reinforcement learning techniques. However, valued-based approaches using an epsilon-greedy mechanism have demonstrated three beneficial traits - Closed-form Diversity, Objective-invariant Exploration, and Adaptive Trade-off - that enable them to avoid the policy collapse problem. Unfortunately, no corresponding mechanism exists for policy-based methods that can achieve all three characteristics. This paper presents an entropy regularization-free mechanism specially designed for policy-based methods that can achieve Closed-form Diversity, Objective-invariant Exploration, and Adaptive Trade-off. Our experiments demonstrate that this mechanism is highly sample-efficient for policy-based methods and significantly improves the baseline to a new State-Of-The-Art in the Arcade Learning Environment.",1
"Recent research has shown the potential of using available mobile fog devices (such as smartphones, drones, domestic and industrial robots) as relays to minimize communication outages between sensors and destination devices, where localized Internet-of-Things services (e.g., manufacturing process control, health and security monitoring) are delivered. However, these mobile relays deplete energy when they move and transmit to distant destinations. As such, power-control mechanisms and intelligent mobility of the relay devices are critical in improving communication performance and energy utilization. In this paper, we propose a Q-learning-based decentralized approach where each mobile fog relay agent (MFRA) is controlled by an autonomous agent which uses reinforcement learning to simultaneously improve communication performance and energy utilization. Each autonomous agent learns based on the feedback from the destination and its own energy levels whether to remain active and forward the message, or become passive for that transmission phase. We evaluate the approach by comparing with the centralized approach, and observe that with lesser number of MFRAs, our approach is able to ensure reliable delivery of data and reduce overall energy cost by 56.76\% -- 88.03\%.",0
"According to recent research, mobile fog devices like smartphones, drones, and robots can be used as relays to prevent communication disruptions between sensors and destination devices. This is particularly useful in delivering localized Internet-of-Things services such as health and security monitoring and manufacturing process control. However, these mobile relays use up energy when they move and transmit to distant destinations. Therefore, it is crucial to have power-control mechanisms and intelligent mobility of the relay devices to enhance communication performance and energy utilization. In this paper, the authors propose a Q-learning-based decentralized approach where each mobile fog relay agent (MFRA) is controlled by an independent agent that uses reinforcement learning to improve communication performance and energy utilization. The autonomous agent decides based on the feedback from the destination and its own energy levels whether to remain active and forward the message or become passive for that transmission phase. The authors compared their approach with the centralized approach and found that their approach with fewer MFRAs ensures reliable delivery of data and reduces overall energy cost by 56.76% - 88.03%.",1
"Rapid advancements in deep learning have led to many recent breakthroughs. While deep learning models achieve superior performance, often statistically better than humans, their adaption into safety-critical settings, such as healthcare or self-driving cars is hindered by their inability to provide safety guarantees or to analyze the inner workings of the model. We present MoET, a novel model based on Mixture of Experts, consisting of decision tree experts and a generalized linear model gating function. While decision boundaries of decision trees (used in an existing verifiable approach), are axis-perpendicular hyperplanes, MoET supports hyperplanes of arbitrary orientation as the boundaries. To support non-differentiable decision trees as experts we formulate a novel training procedure. In addition, we introduce a hard thresholding version, MoET_h, in which predictions are made solely by a single expert chosen via the gating function. Thanks to that property, MoET_h allows each prediction to be easily decomposed into a set of logical rules. Such rules can be translated into a manageable SMT formula providing rich means for verification. While MoET is a general use model, we illustrate its power in the reinforcement learning setting. By training MoET models using an imitation learning procedure on deep RL agents we outperform the previous state-of-the-art technique based on decision trees while preserving the verifiability of the models.",0
"Recent breakthroughs have been made in deep learning due to its rapid advancements. Although deep learning models surpass human performance, they are unable to offer safety guarantees or analyze the model's inner workings, which hinders their application in safety-critical environments such as healthcare or self-driving cars. This paper introduces MoET, a new model based on Mixture of Experts, which includes decision tree experts and a generalized linear model gating function. Unlike existing verifiable approaches that use axis-perpendicular hyperplanes, MoET supports hyperplanes with arbitrary orientation as decision boundaries. A novel training procedure is used to support non-differentiable decision trees as experts. MoET_h is also introduced, which allows predictions to be decomposed into a set of logical rules, making it easy to translate into a manageable SMT formula for verification. Although MoET is a general model, it is particularly useful in the reinforcement learning setting, where it outperforms the previous state-of-the-art technique based on decision trees while preserving model verifiability.",1
"Reinforcement Learning in large action spaces is a challenging problem. Cooperative multi-agent reinforcement learning (MARL) exacerbates matters by imposing various constraints on communication and observability. In this work, we consider the fundamental hurdle affecting both value-based and policy-gradient approaches: an exponential blowup of the action space with the number of agents. For value-based methods, it poses challenges in accurately representing the optimal value function. For policy gradient methods, it makes training the critic difficult and exacerbates the problem of the lagging critic. We show that from a learning theory perspective, both problems can be addressed by accurately representing the associated action-value function with a low-complexity hypothesis class. This requires accurately modelling the agent interactions in a sample efficient way. To this end, we propose a novel tensorised formulation of the Bellman equation. This gives rise to our method Tesseract, which views the Q-function as a tensor whose modes correspond to the action spaces of different agents. Algorithms derived from Tesseract decompose the Q-tensor across agents and utilise low-rank tensor approximations to model agent interactions relevant to the task. We provide PAC analysis for Tesseract-based algorithms and highlight their relevance to the class of rich observation MDPs. Empirical results in different domains confirm Tesseract's gains in sample efficiency predicted by the theory.",0
"Dealing with large action spaces in Reinforcement Learning poses a significant challenge, which becomes even more complicated in Cooperative multi-agent reinforcement learning (MARL) due to communication and observability constraints. This study focuses on the fundamental issue faced by both value-based and policy-gradient approaches, which is the exponential expansion of the action space with the number of agents. Accurately representing the optimal value function is challenging for value-based methods, while training the critic becomes problematic and exacerbates the lagging critic issue for policy gradient methods. The authors suggest addressing these problems by precisely representing the associated action-value function with a low-complexity hypothesis class. This requires accurately modelling agent interactions efficiently. To achieve this goal, they introduce a novel tensorised formulation of the Bellman equation and present Tesseract, a Q-tensor-based method that views the Q-function as a tensor with modes corresponding to the action spaces of different agents. Tesseract-based algorithms decompose the Q-tensor across agents and utilize low-rank tensor approximations to model agent interactions relevant to the task. The authors provide PAC analysis for Tesseract-based algorithms and emphasize their relevance to rich observation MDPs. Empirical results in various domains confirm Tesseract's gains in sample efficiency predicted by the theory.",1
"Algorithmic stock trading has become a staple in today's financial market, the majority of trades being now fully automated. Deep Reinforcement Learning (DRL) agents proved to be to a force to be reckon with in many complex games like Chess and Go. We can look at the stock market historical price series and movements as a complex imperfect information environment in which we try to maximize return - profit and minimize risk. This paper reviews the progress made so far with deep reinforcement learning in the subdomain of AI in finance, more precisely, automated low-frequency quantitative stock trading. Many of the reviewed studies had only proof-of-concept ideals with experiments conducted in unrealistic settings and no real-time trading applications. For the majority of the works, despite all showing statistically significant improvements in performance compared to established baseline strategies, no decent profitability level was obtained. Furthermore, there is a lack of experimental testing in real-time, online trading platforms and a lack of meaningful comparisons between agents built on different types of DRL or human traders. We conclude that DRL in stock trading has showed huge applicability potential rivalling professional traders under strong assumptions, but the research is still in the very early stages of development.",0
"The financial market today heavily relies on algorithmic stock trading, with most trades being fully automated. Deep Reinforcement Learning (DRL) has proven to be a powerful tool in complex games like Chess and Go. The stock market is a complex environment with imperfect information where the goal is to maximize profit and minimize risk. This paper examines the progress of DRL in AI for finance, specifically in automated low-frequency quantitative stock trading. However, most studies only had proof-of-concept experiments in unrealistic settings with no real-time trading applications. Despite showing significant performance improvements compared to established strategies, profitability levels were not satisfactory. Additionally, there is a lack of real-time testing and meaningful comparisons between different DRL agents and human traders. In conclusion, DRL has potential in stock trading, but further research is needed as it is still in the early stages of development.",1
"We study the problem of Safe Policy Improvement (SPI) under constraints in the offline Reinforcement Learning (RL) setting. We consider the scenario where: (i) we have a dataset collected under a known baseline policy, (ii) multiple reward signals are received from the environment inducing as many objectives to optimize. We present an SPI formulation for this RL setting that takes into account the preferences of the algorithm's user for handling the trade-offs for different reward signals while ensuring that the new policy performs at least as well as the baseline policy along each individual objective. We build on traditional SPI algorithms and propose a novel method based on Safe Policy Iteration with Baseline Bootstrapping (SPIBB, Laroche et al., 2019) that provides high probability guarantees on the performance of the agent in the true environment. We show the effectiveness of our method on a synthetic grid-world safety task as well as in a real-world critical care context to learn a policy for the administration of IV fluids and vasopressors to treat sepsis.",0
"Our focus is on Safe Policy Improvement (SPI) with constraints in offline Reinforcement Learning (RL). The scenario involves a dataset collected under a known baseline policy and multiple reward signals from the environment, which induce various objectives to optimize. We introduce an SPI formulation that considers the algorithm user's preferences for balancing trade-offs between different reward signals. The new policy must perform at least as well as the baseline policy for each individual objective. We propose a novel method, SPIBB, based on Safe Policy Iteration with Baseline Bootstrapping (Laroche et al., 2019), which builds on traditional SPI algorithms and provides high probability guarantees of the agent's performance in the true environment. We demonstrate the effectiveness of our method in a synthetic grid-world safety task and a real-world critical care context, where we learn a policy for the administration of IV fluids and vasopressors to treat sepsis.",1
"In recent years, deep off-policy actor-critic algorithms have become a dominant approach to reinforcement learning for continuous control. One of the primary drivers of this improved performance is the use of pessimistic value updates to address function approximation errors, which previously led to disappointing performance. However, a direct consequence of pessimism is reduced exploration, running counter to theoretical support for the efficacy of optimism in the face of uncertainty. So which approach is best? In this work, we show that the most effective degree of optimism can vary both across tasks and over the course of learning. Inspired by this insight, we introduce a novel deep actor-critic framework, Tactical Optimistic and Pessimistic (TOP) estimation, which switches between optimistic and pessimistic value learning online. This is achieved by formulating the selection as a multi-arm bandit problem. We show in a series of continuous control tasks that TOP outperforms existing methods which rely on a fixed degree of optimism, setting a new state of the art in challenging pixel-based environments. Since our changes are simple to implement, we believe these insights can easily be incorporated into a multitude of off-policy algorithms.",0
"Over the past few years, deep off-policy actor-critic algorithms have emerged as the leading method for reinforcement learning in continuous control scenarios. The use of pessimistic value updates has significantly contributed to their success by addressing function approximation errors that previously hindered performance. However, the downside of pessimism is that it limits exploration, which contradicts the belief that optimism is more effective in uncertain situations. Therefore, there is a debate about which approach is better. This study demonstrates that the optimal degree of optimism varies depending on the task and learning progress. As a result, the authors propose a new deep actor-critic framework called Tactical Optimistic and Pessimistic (TOP) estimation, which dynamically alternates between optimistic and pessimistic value learning using a multi-arm bandit approach. The authors demonstrate in a series of continuous control tasks that TOP surpasses existing methods that rely on a fixed degree of optimism, achieving a new state of the art in challenging pixel-based environments. As the proposed changes are straightforward to implement, the authors suggest that they can be incorporated into various off-policy algorithms.",1
"Value factorisation proves to be a very useful technique in multi-agent reinforcement learning (MARL), but the underlying mechanism is not yet fully understood. This paper explores a theoretic basis for value factorisation. We generalise the Shapley value in the coalitional game theory to a Markov convex game (MCG) and use it to guide value factorisation in MARL. We show that the generalised Shapley value possesses several features such as (1) accurate estimation of the maximum global value, (2) fairness in the factorisation of the global value, and (3) being sensitive to dummy agents. The proposed theory yields a new learning algorithm called Sharpley Q-learning (SHAQ), which inherits the important merits of ordinary Q-learning but extends it to MARL. In comparison with prior-arts, SHAQ has a much weaker assumption (MCG) that is more compatible with real-world problems, but has superior explainability and performance in many cases. We demonstrated SHAQ and verified the theoretic claims on Predator-Prey and StarCraft Multi-Agent Challenge (SMAC).",0
"Although value factorisation is a valuable technique in multi-agent reinforcement learning (MARL), its underlying mechanism is not fully understood. This study aims to establish a theoretical basis for value factorisation by generalising the Shapley value in coalitional game theory to a Markov convex game (MCG) and using it to guide value factorisation in MARL. The generalised Shapley value possesses several features, including accurate estimation of the maximum global value, fairness in the factorisation of the global value, and sensitivity to dummy agents. This theory results in a learning algorithm called Sharpley Q-learning (SHAQ), which extends Q-learning to MARL while inheriting its important features. Compared to prior-arts, SHAQ has a much weaker assumption (MCG), making it more compatible with real-world problems, and demonstrates superior explainability and performance in many cases. We demonstrate SHAQ and verify the theoretic claims on Predator-Prey and StarCraft Multi-Agent Challenge (SMAC).",1
"Progress in deep reinforcement learning (RL) research is largely enabled by benchmark task environments. However, analyzing the nature of those environments is often overlooked. In particular, we still do not have agreeable ways to measure the difficulty or solvability of a task, given that each has fundamentally different actions, observations, dynamics, rewards, and can be tackled with diverse RL algorithms. In this work, we propose policy information capacity (PIC) -- the mutual information between policy parameters and episodic return -- and policy-optimal information capacity (POIC) -- between policy parameters and episodic optimality -- as two environment-agnostic, algorithm-agnostic quantitative metrics for task difficulty. Evaluating our metrics across toy environments as well as continuous control benchmark tasks from OpenAI Gym and DeepMind Control Suite, we empirically demonstrate that these information-theoretic metrics have higher correlations with normalized task solvability scores than a variety of alternatives. Lastly, we show that these metrics can also be used for fast and compute-efficient optimizations of key design parameters such as reward shaping, policy architectures, and MDP properties for better solvability by RL algorithms without ever running full RL experiments.",0
"Benchmark task environments have been instrumental in driving progress in deep reinforcement learning (RL) research. However, their nature has not been fully analyzed, resulting in a lack of consensus on how to measure the difficulty or solvability of a task. The challenge arises due to the vastly different actions, observations, dynamics, and rewards associated with each task, along with the diverse RL algorithms that can be used to tackle them. To address this issue, we propose two environment-agnostic and algorithm-agnostic metrics - policy information capacity (PIC) and policy-optimal information capacity (POIC) - that quantify task difficulty based on the mutual information between policy parameters and episodic return/optimality. We evaluate our metrics across various toy environments and continuous control benchmark tasks, and show that they have higher correlations with normalized task solvability scores than other alternatives. Furthermore, we demonstrate that our metrics can be used to optimize key design parameters such as reward shaping, policy architectures, and MDP properties in a computationally efficient manner, without the need for full RL experiments.",1
"Reparameterization (RP) and likelihood ratio (LR) gradient estimators are used to estimate gradients of expectations throughout machine learning and reinforcement learning; however, they are usually explained as simple mathematical tricks, with no insight into their nature. We use a first principles approach to explain that LR and RP are alternative methods of keeping track of the movement of probability mass, and the two are connected via the divergence theorem. Moreover, we show that the space of all possible estimators combining LR and RP can be completely parameterized by a flow field $u(x)$ and an importance sampling distribution $q(x)$. We prove that there cannot exist a single-sample estimator of this type outside our characterized space, thus, clarifying where we should be searching for better Monte Carlo gradient estimators.",0
"In the field of machine learning and reinforcement learning, gradients of expectations are estimated using reparameterization (RP) and likelihood ratio (LR) gradient estimators. However, these methods are often presented as mere mathematical tricks without any explanation of their underlying nature. In this study, we adopt a first principles approach to illustrate that LR and RP are distinct ways of monitoring the movement of probability mass, and that they are related through the divergence theorem. Additionally, we demonstrate that the entire set of estimators that merge LR and RP can be represented by a flow field $u(x)$ and an importance sampling distribution $q(x)$. We also prove that a single-sample estimator of this kind cannot exist beyond our characterized space, which clarifies the search for improved Monte Carlo gradient estimators.",1
"Data driven approaches for decision making applied to automated driving require appropriate generalization strategies, to ensure applicability to the world's variability. Current approaches either do not generalize well beyond the training data or are not capable to consider a variable number of traffic participants. Therefore we propose an invariant environment representation from the perspective of the ego vehicle. The representation encodes all necessary information for safe decision making. To assess the generalization capabilities of the novel environment representation, we train our agents on a small subset of scenarios and evaluate on the entire diverse set of scenarios. Here we show that the agents are capable to generalize successfully to unseen scenarios, due to the abstraction. In addition we present a simple occlusion model that enables our agents to navigate intersections with occlusions without a significant change in performance.",0
"To make data-driven decisions for automated driving, it is crucial to have effective strategies for generalization. This ensures that the decisions remain relevant in the face of varying conditions. However, current methods either fail to generalize beyond the training data or cannot handle different numbers of traffic participants. To address this, we propose an environment representation that is invariant from the perspective of the ego vehicle. This representation contains all the necessary information for safe decision making. To test its effectiveness, we train our agents on a small set of scenarios and evaluate them on a diverse range of scenarios. Our results show that the agents can successfully generalize to new scenarios due to the abstraction. We also introduce an occlusion model that allows our agents to navigate intersections with occlusions without a significant drop in performance.",1
"Sparse rewards are double-edged training signals in reinforcement learning: easy to design but hard to optimize. Intrinsic motivation guidances have thus been developed toward alleviating the resulting exploration problem. They usually incentivize agents to look for new states through novelty signals. Yet, such methods encourage exhaustive exploration of the state space rather than focusing on the environment's salient interaction opportunities. We propose a new exploration method, called Don't Do What Doesn't Matter (DoWhaM), shifting the emphasis from state novelty to state with relevant actions. While most actions consistently change the state when used, \textit{e.g.} moving the agent, some actions are only effective in specific states, \textit{e.g.}, \emph{opening} a door, \emph{grabbing} an object. DoWhaM detects and rewards actions that seldom affect the environment. We evaluate DoWhaM on the procedurally-generated environment MiniGrid, against state-of-the-art methods and show that DoWhaM greatly reduces sample complexity.",0
"Reinforcement learning faces a challenge with sparse rewards which are easy to create but difficult to optimize. To tackle this problem, intrinsic motivation guidelines have been developed to encourage exploration. However, these guidelines promote exhaustive exploration of the state space instead of focusing on important interactions within the environment. Our new approach, called DoWhaM, shifts the emphasis from state novelty to relevant actions. DoWhaM detects and rewards actions that have a significant impact on the environment, such as opening a door or grabbing an object. We evaluated DoWhaM on MiniGrid, a procedurally-generated environment, and found that it significantly reduces sample complexity compared to other state-of-the-art methods.",1
"The idea of transfer in reinforcement learning (TRL) is intriguing: being able to transfer knowledge from one problem to another problem without learning everything from scratch. This promises quicker learning and learning more complex methods. To gain an insight into the field and to detect emerging trends, we performed a database search. We note a surprisingly late adoption of deep learning that starts in 2018. The introduction of deep learning has not yet solved the greatest challenge of TRL: generalization. Transfer between different domains works well when domains have strong similarities (e.g. MountainCar to Cartpole), and most TRL publications focus on different tasks within the same domain that have few differences. Most TRL applications we encountered compare their improvements against self-defined baselines, and the field is still missing unified benchmarks. We consider this to be a disappointing situation. For the future, we note that: (1) A clear measure of task similarity is needed. (2) Generalization needs to improve. Promising approaches merge deep learning with planning via MCTS or introduce memory through LSTMs. (3) The lack of benchmarking tools will be remedied to enable meaningful comparison and measure progress. Already Alchemy and Meta-World are emerging as interesting benchmark suites. We note that another development, the increase in procedural content generation (PCG), can improve both benchmarking and generalization in TRL.",0
"The concept of transfer in reinforcement learning (TRL) is fascinating, as it allows knowledge to be applied from one problem to another without having to learn everything from scratch. This has the potential to speed up the learning process and enable the acquisition of more complex methodologies. In order to gain insight into the field and identify emerging trends, we conducted a database search and found that deep learning was only adopted in 2018. Despite this, the greatest challenge in TRL, i.e. generalization, has yet to be resolved. Transfer between different domains is most effective when the domains are similar, and TRL studies tend to focus on tasks within the same domain with minimal differences. Furthermore, most TRL studies do not have unified benchmarks, which is disappointing. Moving forward, we recommend the development of a clear measure of task similarity, improvements in generalization, and the creation of benchmarking tools to enable meaningful comparisons and measure progress. Alchemy and Meta-World are emerging as promising benchmark suites, and the use of procedural content generation (PCG) can enhance both benchmarking and generalization in TRL.",1
"Goal-conditioned hierarchical reinforcement learning (HRL) serves as a successful approach to solving complex and temporally extended tasks. Recently, its success has been extended to more general settings by concurrently learning hierarchical policies and subgoal representations. However, online subgoal representation learning exacerbates the non-stationary issue of HRL and introduces challenges for exploration in high-level policy learning. In this paper, we propose a state-specific regularization that stabilizes subgoal embeddings in well-explored areas while allowing representation updates in less explored state regions. Benefiting from this stable representation, we design measures of novelty and potential for subgoals, and develop an efficient hierarchical exploration strategy that actively seeks out new promising subgoals and states. Experimental results show that our method significantly outperforms state-of-the-art baselines in continuous control tasks with sparse rewards and further demonstrate the stability and efficiency of the subgoal representation learning of this work, which promotes superior policy learning.",0
"Goal-conditioned hierarchical reinforcement learning (HRL) has proven to be a successful technique for tackling complex and time-consuming tasks. Its efficacy has been expanded to more diverse contexts by simultaneously acquiring hierarchical policies and subgoal representations. However, the process of online subgoal representation learning exacerbates HRL's non-stationary problem and poses exploration challenges in high-level policy learning. This paper proposes a state-specific regularization that stabilizes subgoal embeddings in well-explored areas, while allowing representation updates in less-explored state regions. With the help of this stable representation, the authors design novelty and potential measures for subgoals and devise an efficient hierarchical exploration strategy that actively seeks out new promising subgoals and states. Experimental results indicate that this method outperforms state-of-the-art baselines in continuous control tasks with sparse rewards. Additionally, the stability and efficiency of the subgoal representation learning in this work promote superior policy learning.",1
"In safe reinforcement learning (SRL) problems, an agent explores the environment to maximize an expected total reward and meanwhile avoids violation of certain constraints on a number of expected total costs. In general, such SRL problems have nonconvex objective functions subject to multiple nonconvex constraints, and hence are very challenging to solve, particularly to provide a globally optimal policy. Many popular SRL algorithms adopt a primal-dual structure which utilizes the updating of dual variables for satisfying the constraints. In contrast, we propose a primal approach, called constraint-rectified policy optimization (CRPO), which updates the policy alternatingly between objective improvement and constraint satisfaction. CRPO provides a primal-type algorithmic framework to solve SRL problems, where each policy update can take any variant of policy optimization step. To demonstrate the theoretical performance of CRPO, we adopt natural policy gradient (NPG) for each policy update step and show that CRPO achieves an $\mathcal{O}(1/\sqrt{T})$ convergence rate to the global optimal policy in the constrained policy set and an $\mathcal{O}(1/\sqrt{T})$ error bound on constraint satisfaction. This is the first finite-time analysis of primal SRL algorithms with global optimality guarantee. Our empirical results demonstrate that CRPO can outperform the existing primal-dual baseline algorithms significantly.",0
"When solving safe reinforcement learning (SRL) problems, an agent aims to maximize an expected total reward while avoiding the violation of certain constraints on expected total costs. However, these problems are highly challenging due to their nonconvex objective functions and multiple nonconvex constraints, which make it difficult to provide a globally optimal policy. Although many popular SRL algorithms use a primal-dual structure to address these issues, we propose a primal approach called constraint-rectified policy optimization (CRPO). CRPO offers a primal-type algorithmic framework that updates the policy alternatingly between objective improvement and constraint satisfaction. With each policy update step, CRPO can take any variant of policy optimization step. Our finite-time analysis of CRPO shows that it achieves an $\mathcal{O}(1/\sqrt{T})$ convergence rate to the global optimal policy in the constrained policy set and an $\mathcal{O}(1/\sqrt{T})$ error bound on constraint satisfaction. This is the first demonstration of a primal SRL algorithm with a global optimality guarantee. Our empirical results also show that CRPO outperforms existing primal-dual baseline algorithms significantly.",1
"The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets affected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the exploration-exploitation problem with supervised agents trained to imitate an expert. We show that feed-forward networks trained via reinforcement learning and imitation learning can be pruned to the same level of sparsity, suggesting that the distributional shift has a limited impact on the size of winning tickets. Using a set of carefully designed baseline conditions, we find that the majority of the lottery ticket effect in both learning paradigms can be attributed to the identified mask rather than the weight initialization. The input layer mask selectively prunes entire input dimensions that turn out to be irrelevant for the task at hand. At a moderate level of sparsity the mask identified by iterative magnitude pruning yields minimal task-relevant representations, i.e., an interpretable inductive bias. Finally, we propose a simple initialization rescaling which promotes the robust identification of sparse task representations in low-dimensional control tasks.",0
"The lottery ticket hypothesis challenges the role of overparameterization in supervised deep learning and raises the question of how winning lottery tickets perform in reinforcement learning problems with distributional shift. To address this issue, we compare sparse agents dealing with exploration-exploitation non-stationarity to supervised agents imitating an expert. Our findings suggest that feed-forward networks trained through reinforcement and imitation learning can achieve the same level of sparsity, indicating that the distributional shift has limited effects on winning tickets' size. Our careful baseline conditions reveal that the majority of the lottery ticket effect in both learning paradigms arises from the identified mask rather than weight initialization. The input layer mask selectively prunes unnecessary input dimensions, providing interpretable inductive bias. Lastly, we propose a simple initialization rescaling to facilitate identifying sparse task representations in low-dimensional control tasks.",1
"Mean field games (MFG) facilitate the application of reinforcement learning (RL) in large-scale multi-agent systems, through reducing interplays among agents to those between an individual agent and the average effect from the population. However, RL agents are notoriously prone to unexpected behaviours due to the reward mis-specification. Although inverse RL (IRL) holds promise for automatically acquiring suitable rewards from demonstrations, its extension to MFG is challenging due to the complicated notion of mean-field-type equilibria and the coupling between agent-level and population-level dynamics. To this end, we propose a novel IRL framework for MFG, called Mean Field IRL (MFIRL), where we build upon a new equilibrium concept and the maximum entropy IRL framework. Crucially, MFIRL is brought forward as the first IRL method that can recover the agent-level (ground-truth) reward functions for MFG. Experiments show the superior performance of MFIRL on sample efficiency, reward recovery and robustness against varying environment dynamics, compared to the state-of-the-art method.",0
"The implementation of reinforcement learning (RL) in large-scale multi-agent systems can be facilitated by mean field games (MFG), which reduce the interactions between agents to those between an individual and the average effect from the population. However, RL agents are often prone to unexpected behaviors due to reward mis-specification. Inverse RL (IRL) shows promise in automatically acquiring suitable rewards from demonstrations, but extending it to MFG is challenging due to the complex mean-field-type equilibria and the coupling between agent and population dynamics. To address this, we propose a novel IRL framework for MFG, called Mean Field IRL (MFIRL), which utilizes a new equilibrium concept and the maximum entropy IRL framework. Importantly, MFIRL is the first IRL method that can recover the agent-level reward functions for MFG. Our experiments demonstrate that MFIRL outperforms state-of-the-art methods in terms of sample efficiency, reward recovery, and robustness against varying environment dynamics.",1
"Learning with an objective to minimize the mismatch with a reference distribution has been shown to be useful for generative modeling and imitation learning. In this paper, we investigate whether one such objective, the Wasserstein-1 distance between a policy's state visitation distribution and a target distribution, can be utilized effectively for reinforcement learning (RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement learning where the idealized (unachievable) target distribution has full measure at the goal. We introduce a quasimetric specific to Markov Decision Processes (MDPs), and show that the policy that minimizes the Wasserstein-1 distance of its state visitation distribution to this target distribution under this quasimetric is the policy that reaches the goal in as few steps as possible. Our approach, termed Adversarial Intrinsic Motivation (AIM), estimates this Wasserstein-1 distance through its dual objective and uses it to compute a supplemental reward function. Our experiments show that this reward function changes smoothly with respect to transitions in the MDP and assists the agent in learning. Additionally, we combine AIM with Hindsight Experience Replay (HER) and show that the resulting algorithm accelerates learning significantly on several simulated robotics tasks when compared to HER with a sparse positive reward at the goal state.",0
"The usefulness of learning to minimize the discrepancy with a reference distribution has been demonstrated in generative modeling and imitation learning. This study examines the effectiveness of using the Wasserstein-1 distance as an objective for reinforcement learning tasks, particularly in goal-conditioned reinforcement learning where the target distribution is unattainable and has full measure at the goal. The authors propose a quasimetric specific to Markov Decision Processes and demonstrate that the policy that minimizes the Wasserstein-1 distance to the target distribution under this quasimetric is the policy that achieves the goal in the shortest possible time. They call their approach Adversarial Intrinsic Motivation (AIM) and utilize its dual objective to estimate the Wasserstein-1 distance and compute a supplementary reward function. The experiments demonstrate that this reward function changes seamlessly with transitions in the MDP and enhances the agent's learning. Furthermore, the authors combine AIM with Hindsight Experience Replay (HER) and reveal that the resulting algorithm accelerates learning significantly on various simulated robotics tasks compared to HER with a limited positive reward at the goal state.",1
"In Graph Neural Networks (GNNs), the embedding of each node is obtained by aggregating information with its direct and indirect neighbors. As the messages passed among nodes contain both information and noise, the critical issue in GNN representation learning is how to retrieve information effectively while suppressing noise. Generally speaking, interactions with distant nodes usually introduce more noise for a particular node than those with close nodes. However, in most existing works, the messages being passed among nodes are mingled together, which is inefficient from a communication perspective. Mixing the information from clean sources (low-order neighbors) and noisy sources (high-order neighbors) makes discriminative feature extraction challenging. Motivated by the above, we propose a simple yet effective ladder-style GNN architecture, namely LADDER-GNN. Specifically, we separate messages from different hops and assign different dimensions for them before concatenating them to obtain the node representation. Such disentangled representations facilitate extracting information from messages passed from different hops, and their corresponding dimensions are determined with a reinforcement learning-based neural architecture search strategy. The resulted hop-aware representations generally contain more dimensions for low-order neighbors and fewer dimensions for high-order neighbors, leading to a ladder-style aggregation scheme. We verify the proposed LADDER-GNN on several semi-supervised node classification datasets. Experimental results show that the proposed simple hop-aware representation learning solution can achieve state-of-the-art performance on most datasets.",0
"Graph Neural Networks (GNNs) use information from direct and indirect neighbors to obtain the embedding of each node. However, messages passed among nodes may contain noise, which makes it challenging to retrieve information effectively. Typically, mingling information from clean and noisy sources in existing works is inefficient and hinders discriminative feature extraction. We propose a new GNN architecture called LADDER-GNN, which disentangles messages from different hops and assigns them different dimensions before concatenating them to obtain the node representation. This approach facilitates information extraction from messages passed at different hops and results in a ladder-style aggregation scheme. We use a reinforcement learning-based neural architecture search strategy to determine the dimensions of the hop-aware representations, resulting in more dimensions for low-order neighbors and fewer dimensions for high-order neighbors. We verify the effectiveness of LADDER-GNN on several semi-supervised node classification datasets, where it achieves state-of-the-art performance.",1
"Automatic self-diagnosis provides low-cost and accessible healthcare via an agent that queries the patient and makes predictions about possible diseases. From a machine learning perspective, symptom-based self-diagnosis can be viewed as a sequential feature selection and classification problem. Reinforcement learning methods have shown good performance in this task but often suffer from large search spaces and costly training. To address these problems, we propose a competitive bipartite framework, called FIT, which uses an information-theoretic reward to determine what data to collect next. FIT improves over previous information-based approaches by using a multimodal variational autoencoder (MVAE) model and a two-step sampling strategy for disease prediction. Furthermore, we propose novel methods to substantially reduce the computational cost of FIT to a level that is acceptable for practical online self-diagnosis. Our results in two simulated datasets show that FIT can effectively deal with large search space problems, outperforming existing RL baselines. Moreover, using several public medical datasets, we show that FIT is a competitive alternative in various real-world settings.",0
"Low-cost and easily accessible healthcare can be achieved through automatic self-diagnosis using an agent that queries the patient and predicts potential diseases. From a machine learning standpoint, symptom-based self-diagnosis can be considered a sequential feature selection and classification issue. Reinforcement learning techniques have demonstrated good performance in this task but are often hampered by extensive search spaces and expensive training. To tackle these challenges, we introduce FIT, a competitive bipartite framework that employs an information-theoretic reward to determine what data to gather next. FIT surpasses prior information-based methods by utilizing a multimodal variational autoencoder (MVAE) model and a two-step sampling approach for disease prediction. Additionally, we propose innovative methods to significantly reduce the computational cost of FIT to a level that is feasible for practical online self-diagnosis. Our findings on two simulated datasets demonstrate that FIT can effectively handle extensive search space difficulties and outperform existing RL baselines. Furthermore, using numerous public medical datasets, we show that FIT is a competitive alternative in various real-world settings.",1
"We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each node based its the importance, which is adaptively updated after learning each new task. By utilizing the proximal gradient descent method for learning, the exact sparsity and freezing of the model is guaranteed, and thus, the learner can explicitly control the model capacity as the learning continues. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to prevent the negative transfer that causes the catastrophic forgetting and facilitate efficient learning of new tasks. Throughout the extensive experimental results, we show that our AGS-CL uses much less additional memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative continual learning benchmarks for both supervised and reinforcement learning tasks.",0
"Our proposed method, AGS-CL, is a unique regularization-based continual learning approach that utilizes two group sparsity-based penalties. We selectively apply these penalties to each node based on its importance, which we adaptively update after learning each new task. By utilizing the proximal gradient descent method during learning, we ensure exact sparsity and freezing of the model, allowing the learner to explicitly manage model capacity as learning progresses. To prevent negative transfer and enable efficient learning of new tasks, we re-initialize the weights associated with unimportant nodes after each task. Our AGS-CL method requires minimal additional memory space for storing regularization parameters, and outperforms several state-of-the-art baselines on representative continual learning benchmarks for both supervised and reinforcement learning tasks, as demonstrated through extensive experiments.",1
"Sample efficiency and risk-awareness are central to the development of practical reinforcement learning (RL) for complex decision-making. The former can be addressed by transfer learning and the latter by optimizing some utility function of the return. However, the problem of transferring skills in a risk-aware manner is not well-understood. In this paper, we address the problem of risk-aware policy transfer between tasks in a common domain that differ only in their reward functions, in which risk is measured by the variance of reward streams. Our approach begins by extending the idea of generalized policy improvement to maximize entropic utilities, thus extending policy improvement via dynamic programming to sets of policies and levels of risk-aversion. Next, we extend the idea of successor features (SF), a value function representation that decouples the environment dynamics from the rewards, to capture the variance of returns. Our resulting risk-aware successor features (RaSF) integrate seamlessly within the RL framework, inherit the superior task generalization ability of SFs, and incorporate risk-awareness into the decision-making. Experiments on a discrete navigation domain and control of a simulated robotic arm demonstrate the ability of RaSFs to outperform alternative methods including SFs, when taking the risk of the learned policies into account.",0
"Efficiency of samples and being aware of risks are crucial for practical reinforcement learning (RL) to make complex decisions. The former can be tackled through transfer learning, while the latter can be achieved by optimizing a utility function of the return. However, the issue of transferring skills in a risk-averse manner is not yet fully comprehended. This paper aims to address the problem of risk-aware policy transfer among tasks in a similar domain, differing only in their reward functions. Risk is measured by the variance of reward streams. Our approach involves extending the idea of generalized policy improvement to maximize entropic utilities, thereby expanding policy improvement to sets of policies and levels of risk-aversion through dynamic programming. We also extend the concept of successor features (SF) - a value function representation that separates the environment dynamics from rewards - to capture returns' variance. Our risk-aware successor features (RaSF) integrate seamlessly within the RL framework, inherit SFs' superior task generalization ability, and embed risk-awareness into decision-making. Experiments on discrete navigation domains and control of simulated robotic arms illustrate how RaSFs outperform alternative methods, including SFs, when considering the risk of learned policies.",1
"Many engineering problems have multiple objectives, and the overall aim is to optimize a non-linear function of these objectives. In this paper, we formulate the problem of maximizing a non-linear concave function of multiple long-term objectives. A policy-gradient based model-free algorithm is proposed for the problem. To compute an estimate of the gradient, a biased estimator is proposed. The proposed algorithm is shown to achieve convergence to within an $\epsilon$ of the global optima after sampling $\mathcal{O}(\frac{M^4\sigma^2}{(1-\gamma)^8\epsilon^4})$ trajectories where $\gamma$ is the discount factor and $M$ is the number of the agents, thus achieving the same dependence on $\epsilon$ as the policy gradient algorithm for the standard reinforcement learning.",0
"Engineering problems often involve multiple objectives that need to be optimized through a non-linear function. This study focuses on maximizing a non-linear concave function of long-term objectives. To achieve this, a model-free algorithm based on policy-gradient is proposed. An estimator is also recommended to compute an estimate of the gradient. The proposed algorithm can converge to the global optima within a certain margin of error, requiring $\mathcal{O}(\frac{M^4\sigma^2}{(1-\gamma)^8\epsilon^4})$ trajectories to be sampled. This dependence on $\epsilon$ is similar to the policy gradient algorithm for standard reinforcement learning.",1
"Recently many algorithms were devised for reinforcement learning (RL) with function approximation. While they have clear algorithmic distinctions, they also have many implementation differences that are algorithm-independent and sometimes under-emphasized. Such mixing of algorithmic novelty and implementation craftsmanship makes rigorous analyses of the sources of performance improvements across algorithms difficult. In this work, we focus on a series of off-policy inference-based actor-critic algorithms -- MPO, AWR, and SAC -- to decouple their algorithmic innovations and implementation decisions. We present unified derivations through a single control-as-inference objective, where we can categorize each algorithm as based on either Expectation-Maximization (EM) or direct Kullback-Leibler (KL) divergence minimization and treat the rest of specifications as implementation details. We performed extensive ablation studies, and identified substantial performance drops whenever implementation details are mismatched for algorithmic choices. These results show which implementation details are co-adapted and co-evolved with algorithms, and which are transferable across algorithms: as examples, we identified that tanh Gaussian policy and network sizes are highly adapted to algorithmic types, while layer normalization and ELU are critical for MPO's performances but also transfer to noticeable gains in SAC. We hope our work can inspire future work to further demystify sources of performance improvements across multiple algorithms and allow researchers to build on one another's both algorithmic and implementational innovations.",0
"In recent times, several algorithms have been developed for reinforcement learning (RL) with function approximation. However, these algorithms have varying implementation differences that are often overlooked, making it challenging to analyze their sources of performance improvements. To address this issue, we focus on MPO, AWR, and SAC, which are off-policy inference-based actor-critic algorithms, and seek to separate their algorithmic advancements from implementation decisions. We achieve this by deriving a single control-as-inference objective, which enables us to categorize each algorithm based on EM or direct KL divergence minimization, with the remaining specifications treated as implementation details. Our extensive ablation studies reveal that certain implementation details are co-adapted and co-evolved with algorithms, while others are transferable across algorithms. For example, we find that tanh Gaussian policy and network sizes are highly adapted to algorithmic types, whereas layer normalization and ELU are critical for MPO's performances but also transfer to noticeable gains in SAC. Our hope is that our work will inspire further research, demystify sources of performance improvements across multiple algorithms, and allow researchers to build on one another's algorithmic and implementational innovations.",1
"The control of far-from-equilibrium physical systems, including active materials, has emerged as an important area for the application of reinforcement learning (RL) strategies to derive control policies for physical systems. In active materials, non-linear dynamics and long-range interactions between particles prohibit closed-form descriptions of the system's dynamics and prevent explicit solutions to optimal control problems. Due to fundamental challenges in solving for explicit control strategies, RL has emerged as an approach to derive control strategies for far-from-equilibrium active matter systems. However, an important open question is how the mathematical structure and the physical properties of the active matter systems determine the tractability of RL for learning control policies. In this work, we show that RL can only find good strategies to the canonical active matter task of mixing for systems that combine attractive and repulsive particle interactions. Using mathematical results from dynamical systems theory, we relate the availability of both interaction types with the existence of hyperbolic dynamics and the ability of RL to find homogeneous mixing strategies. In particular, we show that for drag-dominated translational-invariant particle systems, hyperbolic dynamics and, therefore, mixing requires combining attractive and repulsive interactions. Broadly, our work demonstrates how fundamental physical and mathematical properties of dynamical systems can enable or constrain reinforcement learning-based control.",0
"Reinforcement learning (RL) strategies have become crucial in controlling far-from-equilibrium physical systems, such as active materials. However, closed-form descriptions of the system's dynamics are prevented by non-linear dynamics and long-range interactions between particles in active materials, making it difficult to find explicit solutions to optimal control problems. Therefore, RL has emerged as an approach to derive control strategies for such systems. Nevertheless, it remains unclear how the mathematical structure and physical properties of active matter systems affect the tractability of RL for learning control policies. Our research reveals that RL can only identify effective strategies for mixing in active matter systems that combine attractive and repulsive particle interactions. We demonstrate that hyperbolic dynamics, essential for homogeneous mixing strategies, require the presence of both interaction types, as supported by mathematical results from dynamical systems theory. Thus, our work highlights how reinforcement learning-based control is influenced by fundamental physical and mathematical properties of dynamical systems.",1
"A number of problems in the processing of sound and natural language, as well as in other areas, can be reduced to simultaneously reading an input sequence and writing an output sequence of generally different length. There are well developed methods that produce the output sequence based on the entirely known input. However, efficient methods that enable such transformations on-line do not exist. In this paper we introduce an architecture that learns with reinforcement to make decisions about whether to read a token or write another token. This architecture is able to transform potentially infinite sequences on-line. In an experimental study we compare it with state-of-the-art methods for neural machine translation. While it produces slightly worse translations than Transformer, it outperforms the autoencoder with attention, even though our architecture translates texts on-line thereby solving a more difficult problem than both reference methods.",0
"The process of handling sound and language, as well as other tasks, can often involve the simultaneous reading and writing of input and output sequences of different lengths. Although methods exist to produce output sequences based on fully known input, there is currently no efficient way to perform these transformations online. This paper presents an architecture that learns through reinforcement to decide when to read or write tokens, allowing potentially infinite sequences to be transformed in real time. We conducted an experiment comparing our architecture to state-of-the-art neural machine translation methods, finding that while it produced slightly less accurate translations than the Transformer, it outperformed the autoencoder with attention. This is notable because our architecture solves a more challenging problem by translating text online.",1
"We study the problem of inverse reinforcement learning (IRL), where the learning agent recovers a reward function using expert demonstrations. Most of the existing IRL techniques make the often unrealistic assumption that the agent has access to full information about the environment. We remove this assumption by developing an algorithm for IRL in partially observable Markov decision processes (POMDPs), where an agent cannot directly observe the current state of the POMDP. The algorithm addresses several limitations of existing techniques that do not take the \emph{information asymmetry} between the expert and the agent into account. First, it adopts causal entropy as the measure of the likelihood of the expert demonstrations as opposed to entropy in most existing IRL techniques and avoids a common source of algorithmic complexity. Second, it incorporates task specifications expressed in temporal logic into IRL. Such specifications may be interpreted as side information available to the learner a priori in addition to the demonstrations, and may reduce the information asymmetry between the expert and the agent. Nevertheless, the resulting formulation is still nonconvex due to the intrinsic nonconvexity of the so-called \emph{forward problem}, i.e., computing an optimal policy given a reward function, in POMDPs. We address this nonconvexity through sequential convex programming and introduce several extensions to solve the forward problem in a scalable manner. This scalability allows computing policies that incorporate memory at the expense of added computational cost yet also achieves higher performance compared to memoryless policies. We demonstrate that, even with severely limited data, the algorithm learns reward functions and policies that satisfy the task and induce a similar behavior to the expert by leveraging the side information and incorporating memory into the policy.",0
"Our focus is on inverse reinforcement learning (IRL), which involves an agent learning a reward function through expert demonstrations. However, most existing IRL methods assume that the agent has complete information about the environment, which is often unrealistic. To overcome this limitation, we present an algorithm for IRL in partially observable Markov decision processes (POMDPs), where the agent cannot directly observe the current state of the POMDP. Our algorithm addresses several shortcomings of previous techniques, which do not consider the information asymmetry between the expert and the agent. We use causal entropy as a likelihood measure for expert demonstrations, which reduces algorithmic complexity compared to other techniques that use entropy. Additionally, we incorporate task specifications expressed in temporal logic into IRL, which can mitigate the information asymmetry between the expert and the agent. However, the resulting formulation is still nonconvex due to the intrinsic nonconvexity of the forward problem, which involves computing an optimal policy given a reward function in POMDPs. To address this nonconvexity, we use sequential convex programming and introduce several extensions to solve the forward problem in a scalable manner. This allows us to compute policies that incorporate memory, which improves performance but requires additional computational cost. Despite having limited data, our algorithm can learn reward functions and policies that satisfy the task and produce behavior similar to the expert by leveraging side information and incorporating memory into the policy.",1
"Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Transformer Memory (HTM), which helps agents to remember the past in detail. HTM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HTM can therefore ""mentally time-travel"" -- remember past events in detail without attending to all intervening events. We show that agents with HTM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HTM can extrapolate to task sequences an order of magnitude longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HTM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.",0
"Agents that use reinforcement learning often struggle to remember past details, especially after delays or interruptions. Traditional memory architectures make it difficult for agents to recall multiple past events or even a single event that was followed by distractions. To overcome these limitations, we propose a Hierarchical Transformer Memory (HTM) that helps agents remember the past in detail. HTM divides past events into chunks and recalls them by first using high-level attention to identify important information and then detailed attention to focus on the most relevant chunks. With HTM, agents can remember past events in detail without being distracted by intervening events. Our research shows that agents with HTM outperform those with other memory architectures in tasks that require long-term recall, retention, or reasoning over memory. These tasks include finding hidden objects in a 3D environment, navigating efficiently in a new neighborhood, and memorizing new object names. Agents with HTM can even extrapolate to task sequences that are ten times longer than what they were trained on and can generalize knowledge across episodes. HTM improves agent sample efficiency, generalization, and generality by solving tasks that previously required specialized architectures. Our research is an important step towards creating agents that can learn and adapt in complex and extended environments.",1
"The curse of dimensionality is a widely known issue in reinforcement learning (RL). In the tabular setting where the state space $\mathcal{S}$ and the action space $\mathcal{A}$ are both finite, to obtain a nearly optimal policy with sampling access to a generative model, the minimax optimal sample complexity scales linearly with $|\mathcal{S}|\times|\mathcal{A}|$, which can be prohibitively large when $\mathcal{S}$ or $\mathcal{A}$ is large. This paper considers a Markov decision process (MDP) that admits a set of state-action features, which can linearly express (or approximate) its probability transition kernel. We show that a model-based approach (resp.$~$Q-learning) provably learns an $\varepsilon$-optimal policy (resp.$~$Q-function) with high probability as soon as the sample size exceeds the order of $\frac{K}{(1-\gamma)^{3}\varepsilon^{2}}$ (resp.$~$$\frac{K}{(1-\gamma)^{4}\varepsilon^{2}}$), up to some logarithmic factor. Here $K$ is the feature dimension and $\gamma\in(0,1)$ is the discount factor of the MDP. Both sample complexity bounds are provably tight, and our result for the model-based approach matches the minimax lower bound. Our results show that for arbitrarily large-scale MDP, both the model-based approach and Q-learning are sample-efficient when $K$ is relatively small, and hence the title of this paper.",0
"Reinforcement learning is plagued by the curse of dimensionality, which is a well-known problem. When the state space and action space are finite in a tabular setting, obtaining a policy that is nearly optimal with a generative model's sampling access requires a minimax optimal sample complexity that grows linearly with the product of the number of states and actions. This can be an issue when either the state or action space is extensive. This paper discusses a Markov decision process that has a set of state-action features that can linearly express or approximate its probability transition kernel. We demonstrate that a model-based approach and Q-learning can learn an epsilon-optimal policy or Q-function, respectively, with high probability when the sample size exceeds a certain threshold, up to a logarithmic factor. The feature dimension and discount factor of the MDP are represented by K and gamma, respectively. Both sample complexity bounds are provably tight, and our result for the model-based approach matches the minimax lower bound. Our findings suggest that both the model-based approach and Q-learning are sample-efficient for arbitrarily large-scale MDPs when the feature dimension is relatively small, hence the title of this paper.",1
"The reinforcement learning problem of finding a control policy that minimizes the minimum time objective for the Mountain Car environment is considered. Particularly, a class of parameterized nonlinear feedback policies is optimized over to reach the top of the highest mountain peak in minimum time. The optimization is carried out using quasi-Stochastic Gradient Descent (qSGD) methods. In attempting to find the optimal minimum time policy, a new parameterized policy approach is considered that seeks to learn an optimal policy parameter for different regions of the state space, rather than rely on a single macroscopic policy parameter for the entire state space. This partitioned parameterized policy approach is shown to outperform the uniform parameterized policy approach and lead to greater generalization than prior methods, where the Mountain Car became trapped in circular trajectories in the state space.",0
"The focus is on the reinforcement learning problem in the Mountain Car environment, which aims to find a control policy that minimizes the minimum time objective. To achieve this, a specific type of parameterized nonlinear feedback policies is optimized using quasi-Stochastic Gradient Descent (qSGD) methods to reach the highest mountain peak in a shorter time. Instead of relying on a single macroscopic policy parameter for the entire state space, a new approach is introduced to learn an optimal policy parameter for different regions of the state space. This partitioned parameterized policy approach is shown to outperform the uniform parameterized policy approach and lead to greater generalization, as prior methods resulted in the Mountain Car being trapped in circular trajectories in the state space.",1
"Modelers use automatic differentiation (AD) of computation graphs to implement complex Deep Learning models without defining gradient computations. Stochastic AD extends AD to stochastic computation graphs with sampling steps, which arise when modelers handle the intractable expectations common in Reinforcement Learning and Variational Inference. However, current methods for stochastic AD are limited: They are either only applicable to continuous random variables and differentiable functions, or can only use simple but high variance score-function estimators. To overcome these limitations, we introduce Storchastic, a new framework for AD of stochastic computation graphs. Storchastic allows the modeler to choose from a wide variety of gradient estimation methods at each sampling step, to optimally reduce the variance of the gradient estimates. Furthermore, Storchastic is provably unbiased for estimation of any-order gradients, and generalizes variance reduction techniques to higher-order gradient estimates. Finally, we implement Storchastic as a PyTorch library.",0
"The implementation of complex Deep Learning models using gradient computations is simplified with the use of automatic differentiation (AD) of computation graphs by modelers. However, stochastic computation graphs with sampling steps pose a challenge when dealing with intractable expectations common in Reinforcement Learning and Variational Inference. Current methods for stochastic AD have limitations, such as being only applicable to continuous random variables and differentiable functions, or using high variance score-function estimators. To address these limitations, we introduce Storchastic, a new framework for AD of stochastic computation graphs. Storchastic allows the modeler to choose from various gradient estimation methods at each sampling step to reduce the variance of gradient estimates optimally. Additionally, Storchastic is provably unbiased for the estimation of any-order gradients and generalizes variance reduction techniques to higher-order gradient estimates. Finally, we have implemented Storchastic as a PyTorch library.",1
"Autonomous Vehicles (AVs) are required to operate safely and efficiently in dynamic environments. For this, the AVs equipped with Joint Radar-Communications (JRC) functions can enhance the driving safety by utilizing both radar detection and data communication functions. However, optimizing the performance of the AV system with two different functions under uncertainty and dynamic of surrounding environments is very challenging. In this work, we first propose an intelligent optimization framework based on the Markov Decision Process (MDP) to help the AV make optimal decisions in selecting JRC operation functions under the dynamic and uncertainty of the surrounding environment. We then develop an effective learning algorithm leveraging recent advances of deep reinforcement learning techniques to find the optimal policy for the AV without requiring any prior information about surrounding environment. Furthermore, to make our proposed framework more scalable, we develop a Transfer Learning (TL) mechanism that enables the AV to leverage valuable experiences for accelerating the training process when it moves to a new environment. Extensive simulations show that the proposed transferable deep reinforcement learning framework reduces the obstacle miss detection probability by the AV up to 67% compared to other conventional deep reinforcement learning approaches.",0
"AVs need to safely and efficiently navigate unpredictable environments. Equipping them with Joint Radar-Communications (JRC) functions can enhance driving safety by utilizing radar detection and data communication. However, optimizing AV performance with two functions in dynamic environments is challenging. To address this, we propose an intelligent optimization framework, based on Markov Decision Process (MDP), to help AVs select JRC functions. We also develop a deep reinforcement learning algorithm to find the optimal policy without prior environment information. Additionally, we include a Transfer Learning (TL) mechanism to accelerate the training process when the AV moves to a new environment. Our proposed transferable deep reinforcement learning framework reduces obstacle miss detection probability by up to 67% compared to conventional approaches.",1
"Extrapolating beyond-demonstrator (BD) through the inverse reinforcement learning (IRL) algorithm aims to learn from and outperform the demonstrator. In sharp contrast to the conventional reinforcement learning (RL) algorithms, BD-IRL can overcome the dilemma incurred in the reward function design and improve the exploration mechanism of RL, which opens new avenues to building superior expert systems. Most existing BD-IRL algorithms are performed in two stages by first inferring a reward function before learning a policy via RL. However, such two-stage BD-IRL algorithms suffer from high computational complexity, weak robustness, and large performance variations. In particular, a poor reward function derived in the first stage will inevitably incur severe performance loss in the second stage. In this work, we propose a hybrid adversarial inverse reinforcement learning (HAIRL) algorithm that is one-stage, model-free, generative-adversarial (GA) fashion and curiosity-driven. Thanks to the one-stage design, the HAIRL can integrate both the reward function learning and the policy optimization into one procedure, which leads to many advantages such as low computational complexity, high robustness, and strong adaptability. More specifically, HAIRL simultaneously imitates the demonstrator and explores BD performance by utilizing hybrid rewards. In particular, the Wasserstein-1 distance (WD) is introduced into HAIRL to stabilize the imitation procedure while a novel end-to-end curiosity module (ECM) is developed to improve the exploration. Finally, extensive simulation results confirm that HAIRL can achieve higher performance as compared to other similar BD-IRL algorithms. Our code is available at our GitHub website \footnote{\url{https://github.com/yuanmingqi/HAIRL}}.",0
"The inverse reinforcement learning (IRL) algorithm, when used in conjunction with beyond-demonstrator (BD), seeks to learn from and surpass the demonstrator. Unlike conventional reinforcement learning (RL) algorithms, BD-IRL can bypass the difficulties associated with reward function design and enhance the exploration mechanism of RL, thereby providing new opportunities for creating superior expert systems. The majority of existing BD-IRL algorithms operate in two stages, first inferring a reward function and then learning a policy via RL. However, such algorithms have significant computational complexity, weak robustness, and substantial performance variations. To address these issues, we propose a one-stage, model-free, generative-adversarial (GA) algorithm called hybrid adversarial inverse reinforcement learning (HAIRL), which is driven by curiosity. HAIRL integrates reward function learning and policy optimization into a single procedure, resulting in several benefits such as low computational complexity, high robustness, and strong adaptability. Additionally, HAIRL employs hybrid rewards to imitate the demonstrator and explore BD performance, with the Wasserstein-1 distance (WD) stabilizing the imitation process and a novel end-to-end curiosity module (ECM) improving exploration. Extensive simulation results demonstrate that HAIRL outperforms other similar BD-IRL algorithms. Our code is available at our GitHub website.",1
This paper investigates the problem of impact-time-control and proposes a learning-based computational guidance algorithm to solve this problem. The proposed guidance algorithm is developed based on a general prediction-correction concept: the exact time-to-go under proportional navigation guidance with realistic aerodynamic characteristics is estimated by a deep neural network and a biased command to nullify the impact time error is developed by utilizing the emerging reinforcement learning techniques. The deep neural network is augmented into the reinforcement learning block to resolve the issue of sparse reward that has been observed in typical reinforcement learning formulation. Extensive numerical simulations are conducted to support the proposed algorithm.,0
"The focus of this research is to address the issue of impact-time-control. To solve this problem, a learning-based computational guidance algorithm is suggested. The proposed algorithm utilizes a general prediction-correction concept, whereby a deep neural network is used to determine the exact time-to-go under proportional navigation guidance, with realistic aerodynamic characteristics. The algorithm also employs reinforcement learning techniques to develop a biased command that nullifies the impact time error. The reinforcement learning block is augmented with a deep neural network to combat the problem of sparse reward. The effectiveness of the proposed algorithm is supported by numerous numerical simulations.",1
"Agents trained by reinforcement learning (RL) often fail to generalize beyond the environment they were trained in, even when presented with new scenarios that seem similar to the training environment. We study the query complexity required to train RL agents that generalize to multiple environments. Intuitively, tractable generalization is only possible when the environments are similar or close in some sense. To capture this, we introduce Weak Proximity, a natural structural condition that requires the environments to have highly similar transition and reward functions and share a policy providing optimal value. Despite such shared structure, we prove that tractable generalization is impossible in the worst case. This holds even when each individual environment can be efficiently solved to obtain an optimal linear policy, and when the agent possesses a generative model. Our lower bound applies to the more complex task of representation learning for the purpose of efficient generalization to multiple environments. On the positive side, we introduce Strong Proximity, a strengthened condition which we prove is sufficient for efficient generalization.",0
"Reinforcement learning (RL) agents often struggle to apply their training to new scenarios, even if they are similar to their training environment. Our research focuses on understanding how to train RL agents to generalize to multiple environments. We have found that successful generalization is only possible when the environments have similar transition and reward functions and share a policy providing optimal value. To capture this, we introduce Weak Proximity, a natural structural condition. However, we have proven that even with shared structure, tractable generalization is impossible in the worst case. Our research extends to the more complex task of representation learning for efficient generalization to multiple environments. On a positive note, we have introduced Strong Proximity, a strengthened condition that we have proven is sufficient for efficient generalization.",1
"We develop a mathematical framework for solving multi-task reinforcement learning (MTRL) problems based on a type of policy gradient method. The goal in MTRL is to learn a common policy that operates effectively in different environments; these environments have similar (or overlapping) state spaces, but have different rewards and dynamics. We highlight two fundamental challenges in MTRL that are not present in its single task counterpart, and illustrate them with simple examples. We then develop a decentralized entropy-regularized policy gradient method for solving the MTRL problem, and study its finite-time convergence rate. We demonstrate the effectiveness of the proposed method using a series of numerical experiments. These experiments range from small-scale ""GridWorld"" problems that readily demonstrate the trade-offs involved in multi-task learning to large-scale problems, where common policies are learned to navigate an airborne drone in multiple (simulated) environments.",0
"Our study presents a mathematical framework that utilizes a policy gradient method to solve multi-task reinforcement learning (MTRL) problems. In MTRL, the objective is to acquire a unified policy that can perform well in various environments with similar or overlapping state spaces but different rewards and dynamics. We identify two distinct challenges that arise in MTRL, which do not occur in its single-task equivalent, and use simple examples to illustrate them. To tackle the MTRL problem, we propose a decentralized entropy-regularized policy gradient method and analyze its convergence rate within a finite time. To validate our approach, we conduct a set of numerical experiments that range from small-scale ""GridWorld"" problems to large-scale tasks, such as training common policies for maneuvering an airborne drone in multiple simulated environments. The results indicate that our method is effective in addressing MTRL problems.",1
"The generalization ability of most meta-reinforcement learning (meta-RL) methods is largely limited to test tasks that are sampled from the same distribution used to sample training tasks. To overcome the limitation, we propose Latent Dynamics Mixture (LDM) that trains a reinforcement learning agent with imaginary tasks generated from mixtures of learned latent dynamics. By training a policy on mixture tasks along with original training tasks, LDM allows the agent to prepare for unseen test tasks during training and prevents the agent from overfitting the training tasks. LDM significantly outperforms standard meta-RL methods in test returns on the gridworld navigation and MuJoCo tasks where we strictly separate the training task distribution and the test task distribution.",0
"Most meta-reinforcement learning (meta-RL) approaches have a restricted ability to generalize to test tasks that are not from the same distribution as the training tasks. To address this issue, we introduce Latent Dynamics Mixture (LDM), which trains a reinforcement learning agent using imaginary tasks generated from mixtures of learned latent dynamics. The LDM method enables the agent to prepare for unseen test tasks during training by training on mixture tasks in addition to original training tasks, thus preventing overfitting on training tasks. Our experiments on gridworld navigation and MuJoCo tasks, where we strictly separate training task distribution from test task distribution, demonstrate that LDM significantly outperforms standard meta-RL methods in test returns.",1
"Reinforcement learning (RL) has traditionally been understood from an episodic perspective; the concept of non-episodic RL, where there is no restart and therefore no reliable recovery, remains elusive. A fundamental question in non-episodic RL is how to measure the performance of a learner and derive algorithms to maximize such performance. Conventional wisdom is to maximize the difference between the average reward received by the learner and the maximal long-term average reward. In this paper, we argue that if the total time budget is relatively limited compared to the complexity of the environment, such comparison may fail to reflect the finite-time optimality of the learner. We propose a family of measures, called $\gamma$-regret, which we believe to better capture the finite-time optimality. We give motivations and derive lower and upper bounds for such measures. Note: A follow-up work (arXiv:2010.00587) has improved both our lower and upper bound, the gap is now closed at $\tilde{\Theta}\left(\frac{\sqrt{SAT}}{(1 - \gamma)^{\frac{1}{2}}}\right)$.",0
"Traditionally, reinforcement learning (RL) has been viewed through an episodic lens, with non-episodic RL presenting a challenge as there are no restarts or reliable recoveries. A key consideration in non-episodic RL is how to measure a learner's performance and create algorithms to optimize it. The conventional approach is to maximize the difference between the learner's average reward and the maximal long-term average reward. However, this comparison may not accurately reflect the finite-time optimality of the learner if the time budget is limited relative to the environment's complexity. To address this issue, we propose a set of measures called $\gamma$-regret, which better capture the learner's finite-time optimality. We provide reasons for their use and establish lower and upper bounds for these measures. It is worth noting that a subsequent study (arXiv:2010.00587) has improved our lower and upper bounds, resulting in a closed gap of $\tilde{\Theta}\left(\frac{\sqrt{SAT}}{(1 - \gamma)^{\frac{1}{2}}}\right)$.",1
"Policy gradient (PG) gives rise to a rich class of reinforcement learning (RL) methods. Recently, there has been an emerging trend to accelerate the existing PG methods such as REINFORCE by the \emph{variance reduction} techniques. However, all existing variance-reduced PG methods heavily rely on an uncheckable importance weight assumption made for every single iteration of the algorithms. In this paper, a simple gradient truncation mechanism is proposed to address this issue. Moreover, we design a Truncated Stochastic Incremental Variance-Reduced Policy Gradient (TSIVR-PG) method, which is able to maximize not only a cumulative sum of rewards but also a general utility function over a policy's long-term visiting distribution. We show an $\tilde{\mathcal{O}}(\epsilon^{-3})$ sample complexity for TSIVR-PG to find an $\epsilon$-stationary policy. By assuming the overparameterizaiton of policy and exploiting the hidden convexity of the problem, we further show that TSIVR-PG converges to global $\epsilon$-optimal policy with $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples.",0
"A variety of reinforcement learning techniques can be derived from policy gradient (PG). Recently, there has been a trend towards accelerating existing PG methods, such as REINFORCE, through variance reduction techniques. However, the current variance-reduced PG methods rely heavily on an assumption of importance weight that cannot be verified in every iteration of the algorithm. To overcome this issue, this paper proposes a straightforward gradient truncation mechanism. Additionally, the paper introduces a Truncated Stochastic Incremental Variance-Reduced Policy Gradient (TSIVR-PG) method that can maximize both a cumulative sum of rewards and a general utility function over a policy's long-term visiting distribution. The sample complexity for TSIVR-PG to achieve an $\epsilon$-stationary policy is shown to be $\tilde{\mathcal{O}}(\epsilon^{-3})$. Furthermore, by assuming policy overparameterization and exploiting the hidden convexity of the problem, the paper proves that TSIVR-PG can converge to a global $\epsilon$-optimal policy with $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples.",1
"For over a decade, model-based reinforcement learning has been seen as a way to leverage control-based domain knowledge to improve the sample-efficiency of reinforcement learning agents. While model-based agents are conceptually appealing, their policies tend to lag behind those of model-free agents in terms of final reward, especially in non-trivial environments. In response, researchers have proposed model-based agents with increasingly complex components, from ensembles of probabilistic dynamics models, to heuristics for mitigating model error. In a reversal of this trend, we show that simple model-based agents can be derived from existing ideas that not only match, but outperform state-of-the-art model-free agents in terms of both sample-efficiency and final reward. We find that a model-free soft value estimate for policy evaluation and a model-based stochastic value gradient for policy improvement is an effective combination, achieving state-of-the-art results on a high-dimensional humanoid control task, which most model-based agents are unable to solve. Our findings suggest that model-based policy evaluation deserves closer attention.",0
"For more than ten years, model-based reinforcement learning has been considered a way to use control-based domain knowledge to enhance the sample efficiency of reinforcement learning agents. Although model-based agents are conceptually attractive, their policies tend to be inferior to those of model-free agents in terms of the ultimate reward, particularly in challenging environments. In response, researchers have suggested complex components for model-based agents, such as ensembles of probabilistic dynamics models and heuristics to alleviate model error. However, we reveal a reversal of this trend by demonstrating that existing ideas can produce simple model-based agents that not only match but surpass state-of-the-art model-free agents in both sample efficiency and final reward. Our research indicates that combining a model-free soft value estimate for policy evaluation with a model-based stochastic value gradient for policy improvement is an effective strategy. This combination achieves cutting-edge outcomes on a high-dimensional humanoid control task that most model-based agents cannot solve. Therefore, our findings suggest that model-based policy evaluation requires more attention.",1
"This paper introduces a novel design of model-free reinforcement learning, CASA, Critic AS an Actor. CASA follows the actor-critic framework that estimates state-value, state-action-value and policy simultaneously. We prove that CASA integrates a consistent path for the policy evaluation and the policy improvement, which completely eliminates the gradient conflict between the policy improvement and the policy evaluation. The policy evaluation is equivalent to a compensational policy improvement, which alleviates the function approximation error, and is also equivalent to an entropy-regularized policy improvement, which prevents the policy from being trapped into a suboptimal solution. Building on this design, an expectation-correct Doubly Robust Trace is introduced to learn state-value and state-action-value, and the convergence is guaranteed. Our experiments show that the design achieves State-Of-The-Art on Arcade Learning Environment.",0
"The paper presents a fresh approach to model-free reinforcement learning called Critic AS an Actor (CASA). CASA is built on the actor-critic framework, which estimates state-value, state-action-value, and policy simultaneously. The authors demonstrate that CASA resolves the gradient conflict between policy evaluation and improvement by integrating a consistent path for both. This approach offers a compensational policy improvement, which reduces function approximation error and an entropy-regularized policy improvement, which prevents getting stuck in suboptimal solutions. The paper also introduces an expectation-correct Doubly Robust Trace to learn state-value and state-action-value and guarantees convergence. The authors report that their design surpasses the State-Of-The-Art on Arcade Learning Environment in their experiments.",1
"We introduce AndroidEnv, an open-source platform for Reinforcement Learning (RL) research built on top of the Android ecosystem. AndroidEnv allows RL agents to interact with a wide variety of apps and services commonly used by humans through a universal touchscreen interface. Since agents train on a realistic simulation of an Android device, they have the potential to be deployed on real devices. In this report, we give an overview of the environment, highlighting the significant features it provides for research, and we present an empirical evaluation of some popular reinforcement learning agents on a set of tasks built on this platform.",0
"Introducing AndroidEnv - an open-source platform designed for Reinforcement Learning (RL) research that utilizes the Android ecosystem. With AndroidEnv, RL agents can interact with various apps and services accessed through a universal touchscreen interface, commonly used by humans. By training agents on a simulation of an Android device, the possibility of deploying these agents on actual devices is achievable. Our report provides an overview of AndroidEnv's features and its potential for research, including an empirical evaluation of popular reinforcement learning agents on a set of tasks built on this platform.",1
"This paper addresses a new interpretation of reinforcement learning (RL) as reverse Kullback-Leibler (KL) divergence optimization, and derives a new optimization method using forward KL divergence. Although RL originally aims to maximize return indirectly through optimization of policy, the recent work by Levine has proposed a different derivation process with explicit consideration of optimality as stochastic variable. This paper follows this concept and formulates the traditional learning laws for both value function and policy as the optimization problems with reverse KL divergence including optimality. Focusing on the asymmetry of KL divergence, the new optimization problems with forward KL divergence are derived. Remarkably, such new optimization problems can be regarded as optimistic RL. That optimism is intuitively specified by a hyperparameter converted from an uncertainty parameter. In addition, it can be enhanced when it is integrated with prioritized experience replay and eligibility traces, both of which accelerate learning. The effects of this expected optimism was investigated through learning tendencies on numerical simulations using Pybullet. As a result, moderate optimism accelerated learning and yielded higher rewards. In a realistic robotic simulation, the proposed method with the moderate optimism outperformed one of the state-of-the-art RL method.",0
"This article introduces a fresh perspective on reinforcement learning (RL) by framing it as an optimization of reverse Kullback-Leibler (KL) divergence, and proposes a novel optimization technique utilizing forward KL divergence. Although the goal of RL is to maximize return by optimizing policy, recent research by Levine suggests a different approach that considers optimality as a stochastic variable. This article follows this approach and transforms the conventional learning laws for value function and policy into optimization problems with reverse KL divergence, incorporating optimality. By leveraging the asymmetry of KL divergence, the article derives new optimization problems that use forward KL divergence, leading to what is known as optimistic RL. This optimism is defined by a hyperparameter that represents uncertainty and can be increased through the use of prioritized experience replay and eligibility traces, both of which speed up learning. The efficacy of this expected optimism was tested through numerical simulations using Pybullet and demonstrated that moderate optimism accelerates learning and produces higher rewards. In a realistic robotic simulation, the proposed method with moderate optimism outperformed a state-of-the-art RL method.",1
"Reinforcement learning (RL)-based neural architecture search (NAS) generally guarantees better convergence yet suffers from the requirement of huge computational resources compared with gradient-based approaches, due to the rollout bottleneck -- exhaustive training for each sampled generation on proxy tasks. In this paper, we propose a general pipeline to accelerate the convergence of the rollout process as well as the RL process in NAS. It is motivated by the interesting observation that both the architecture and the parameter knowledge can be transferred between different experiments and even different tasks. We first introduce an uncertainty-aware critic (value function) in Proximal Policy Optimization (PPO) to utilize the architecture knowledge in previous experiments, which stabilizes the training process and reduces the searching time by 4 times. Further, an architecture knowledge pool together with a block similarity function is proposed to utilize parameter knowledge and reduces the searching time by 2 times. It is the first to introduce block-level weight sharing in RLbased NAS. The block similarity function guarantees a 100% hitting ratio with strict fairness. Besides, we show that a simply designed off-policy correction factor used in ""replay buffer"" in RL optimization can further reduce half of the searching time. Experiments on the Mobile Neural Architecture Search (MNAS) search space show the proposed Fast Neural Architecture Search (FNAS) accelerates standard RL-based NAS process by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x hour for MNAS), and guarantees better performance on various vision tasks.",0
"Neural architecture search (NAS) that uses reinforcement learning (RL) is known to provide better convergence, but requires extensive computational resources due to the rollout bottleneck. This is because exhaustive training is needed for each sampled generation on proxy tasks. To address this issue, we propose a pipeline that accelerates the RL process and the rollout process. Our approach is based on the observation that architecture and parameter knowledge can be transferred across different experiments and tasks. We introduce an uncertainty-aware critic in Proximal Policy Optimization (PPO) to utilize previous architecture knowledge, which stabilizes the training process and reduces the searching time by 4 times. We also propose an architecture knowledge pool and block similarity function to utilize parameter knowledge, reducing the searching time by 2 times. Our approach is the first to introduce block-level weight sharing in RL-based NAS, which guarantees a 100% hitting ratio with strict fairness. Additionally, we show that a simple off-policy correction factor used in ""replay buffer"" in RL optimization can further reduce half of the searching time. Experiments conducted on the Mobile Neural Architecture Search (MNAS) search space demonstrate that our Fast Neural Architecture Search (FNAS) approach accelerates the standard RL-based NAS process by ~10x (e.g. ~256 2x2 TPUv2 x days / 20,000 GPU x hour -> 2,000 GPU x hour for MNAS) while guaranteeing better performance on various vision tasks.",1
"This paper considers the problem of real-time control and learning in dynamic systems subjected to uncertainties. Adaptive approaches are proposed to address the problem, which are combined to with methods and tools in Reinforcement Learning (RL) and Machine Learning (ML). Algorithms are proposed in continuous-time that combine adaptive approaches with RL leading to online control policies that guarantee stable behavior in the presence of parametric uncertainties that occur in real-time. Algorithms are proposed in discrete-time that combine adaptive approaches proposed for parameter and output estimation and ML approaches proposed for accelerated performance that guarantee stable estimation even in the presence of time-varying regressors, and for accelerated learning of the parameters with persistent excitation. Numerical validations of all algorithms are carried out using a quadrotor landing task on a moving platform and benchmark problems in ML. All results clearly point out the advantage of adaptive approaches for real-time control and learning.",0
"This paper addresses the issue of controlling and learning dynamic systems in real-time, despite uncertainties. The proposed solutions utilize adaptive approaches in conjunction with Reinforcement Learning (RL) and Machine Learning (ML) techniques. The algorithms presented operate in continuous-time and combine adaptive approaches with RL to develop online control policies, ensuring stable behavior in the face of real-time parametric uncertainties. Additionally, the algorithms operate in discrete-time and employ adaptive approaches for parameter and output estimation, along with ML approaches for accelerated performance, guaranteeing stable estimation even with time-varying regressors and accelerated learning of parameters with persistent excitation. Numerical validations of all algorithms are conducted using a quadrotor landing task on a moving platform and benchmark problems in ML, ultimately highlighting the benefits of adaptive approaches for real-time control and learning.",1
"With the formation of next generation wireless communication, a growing number of new applications like internet of things, autonomous car, and drone is crowding the unlicensed spectrum. Licensed network such as the long-term evolution (LTE) also comes to the unlicensed spectrum for better providing high-capacity contents with low cost. However, LTE was not designed for sharing spectrum with others. A cooperation center for these networks is costly because they possess heterogeneous properties and everyone can enter and leave the spectrum unrestrictedly, so the design will be challenging. Since it is infeasible to incorporate potentially infinite scenarios with one unified design, an alternative solution is to let each network learn its own coexistence policy. Previous solutions only work on fixed scenarios. In this work a reinforcement learning algorithm is presented to cope with the coexistence between Wi-Fi and LTE agents in 5 GHz unlicensed spectrum. The coexistence problem was modeled as a decentralized partially observable Markov decision process (Dec-POMDP) and Bayesian approach was adopted for policy learning with nonparametric prior to accommodate the uncertainty of policy for different agents. A fairness measure was introduced in the reward function to encourage fair sharing between agents. The reinforcement learning was turned into an optimization problem by transforming the value function as likelihood and variational inference for posterior approximation. Simulation results demonstrate that this algorithm can reach high value with compact policy representations, and stay computationally efficient when applying to agent set.",0
"The rise of next-generation wireless communication has led to a surge in new applications, including the internet of things, autonomous cars, and drones, which are competing for space in the unlicensed spectrum. Even licensed networks such as long-term evolution (LTE) are now using the unlicensed spectrum to offer high-capacity content at a lower cost. However, LTE was not designed for sharing spectrum with others, and creating a cooperation center for these networks is challenging due to their heterogeneous properties and unrestricted access to the spectrum. Instead of attempting to design a unified solution for all scenarios, each network should be allowed to learn its own coexistence policy. Past solutions have only worked in fixed scenarios, but this study presents a reinforcement learning algorithm to address coexistence between Wi-Fi and LTE agents in the 5 GHz unlicensed spectrum. The coexistence problem was modeled as a decentralized partially observable Markov decision process (Dec-POMDP), and a Bayesian approach was used for policy learning. A fairness measure was introduced in the reward function to encourage equitable sharing between agents. The reinforcement learning was transformed into an optimization problem, and simulation results showed that this algorithm could achieve high value with compact policy representations while remaining computationally efficient when applied to agent sets.",1
"The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments (ENV) and tasks (TASK), probably more importantly, by learning from only sparse (ENV, TASK) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from the environment and task embeddings. Notably, one of the main challenges is to learn the embeddings jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on GridWorld and Thor, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (ENV, TASK) pairs after learning from only 40% of them.",0
"Constructing an agent of general artificial intelligence heavily relies on the ability to transfer in reinforcement learning. The aim of this study is to solve the problem of learning to transfer across both environments and tasks simultaneously. The learning process utilizes only sparse pairs of environment and task combinations out of all possible pairs. To achieve this, we propose a new neural network architecture that includes a meta rule for composing policies from environment and task embeddings. However, the challenge lies in learning the embeddings together with the meta rule. Consequently, we introduce novel training methods to disentangle the embeddings, enabling them to function as unique signatures of the environments and tasks and effective building blocks for composing policies. Our approach is tested on GridWorld and Thor, where the agent takes an egocentric view as input, and produces high success rates on all environment and task pairs after learning from only 40% of them.",1
"Reinforcement learners are agents that learn to pick actions that lead to high reward. Ideally, the value of a reinforcement learner's policy approaches optimality--where the optimal informed policy is the one which maximizes reward. Unfortunately, we show that if an agent is guaranteed to be ""asymptotically optimal"" in any (stochastically computable) environment, then subject to an assumption about the true environment, this agent will be either ""destroyed"" or ""incapacitated"" with probability 1. Much work in reinforcement learning uses an ergodicity assumption to avoid this problem. Often, doing theoretical research under simplifying assumptions prepares us to provide practical solutions even in the absence of those assumptions, but the ergodicity assumption in reinforcement learning may have led us entirely astray in preparing safe and effective exploration strategies for agents in dangerous environments. Rather than assuming away the problem, we present an agent, Mentee, with the modest guarantee of approaching the performance of a mentor, doing safe exploration instead of reckless exploration. Critically, Mentee's exploration probability depends on the expected information gain from exploring. In a simple non-ergodic environment with a weak mentor, we find Mentee outperforms existing asymptotically optimal agents and its mentor.",0
"Reinforcement learners are agents that aim to select actions that yield high rewards. The objective is for the value of the reinforcement learner's policy to approach optimality, where the best-informed policy maximizes reward. Unfortunately, we demonstrate that an agent that is guaranteed to be ""asymptotically optimal"" in any stochastically computable environment will either be ""destroyed"" or ""incapacitated"" with probability 1, subject to an assumption about the true environment. Many studies in reinforcement learning have employed an ergodicity assumption to avoid this issue. However, relying on this assumption may have led us astray in developing safe exploration strategies for agents in dangerous environments. To address this, we introduce Mentee, an agent that approaches the performance of a mentor while conducting safe exploration. Mentee's exploration probability depends on the expected information gain from exploring. In a simple non-ergodic environment with a weak mentor, we find that Mentee outperforms existing asymptotically optimal agents and its mentor. Rather than dismissing the problem, our approach offers a practical solution to prepare agents for safe exploration.",1
"We propose a successive convex approximation based off-policy optimization (SCAOPO) algorithm to solve the general constrained reinforcement learning problem, which is formulated as a constrained Markov decision process (CMDP) in the context of average cost. The SCAOPO is based on solving a sequence of convex objective/feasibility optimization problems obtained by replacing the objective and constraint functions in the original problems with convex surrogate functions. At each iteration, the convex surrogate problem can be efficiently solved by Lagrange dual method even the policy is parameterized by a high-dimensional function. Moreover, the SCAOPO enables to reuse old experiences from previous updates, thereby significantly reducing the implementation cost when deployed in the real-world engineering systems that need to online learn the environment. In spite of the time-varying state distribution and the stochastic bias incurred by the off-policy learning, the SCAOPO with a feasible initial point can still provably converge to a Karush-Kuhn-Tucker (KKT) point of the original problem almost surely.",0
"To tackle the general constrained reinforcement learning problem in the context of average cost, we introduce the successive convex approximation based off-policy optimization (SCAOPO) algorithm. The algorithm involves solving a series of convex objective/feasibility optimization problems by replacing the original functions with convex surrogate functions. Despite a high-dimensional function parameterization of the policy, the Lagrange dual method can efficiently solve the convex surrogate problem at each iteration. Additionally, the SCAOPO algorithm allows for the reuse of old experiences, significantly reducing implementation costs in real-world engineering systems that require online learning. Despite the stochastic bias and time-varying state distribution incurred by off-policy learning, SCAOPO can converge to a Karush-Kuhn-Tucker (KKT) point of the original problem with a feasible initial point almost surely.",1
"The heavy traffic and related issues have always been concerns for modern cities. With the help of deep learning and reinforcement learning, people have proposed various policies to solve these traffic-related problems, such as smart traffic signal control systems and taxi dispatching systems. People usually validate these policies in a city simulator, since directly applying them in the real city introduces real cost. However, these policies validated in the city simulator may fail in the real city if the simulator is significantly different from the real world. To tackle this problem, we need to build a real-like traffic simulation system. Therefore, in this paper, we propose to learn the human routing model, which is one of the most essential part in the traffic simulator. This problem has two major challenges. First, human routing decisions are determined by multiple factors, besides the common time and distance factor. Second, current historical routes data usually covers just a small portion of vehicles, due to privacy and device availability issues. To address these problems, we propose a theory-guided residual network model, where the theoretical part can emphasize the general principles for human routing decisions (e.g., fastest route), and the residual part can capture drivable condition preferences (e.g., local road or highway). Since the theoretical part is composed of traditional shortest path algorithms that do not need data to train, our residual network can learn human routing models from limited data. We have conducted extensive experiments on multiple real-world datasets to show the superior performance of our model, especially with small data. Besides, we have also illustrated why our model is better at recovering real routes through case studies.",0
"Modern cities have long been troubled by issues related to heavy traffic. To tackle these problems, various policies have been proposed, such as smart traffic signal control systems and taxi dispatching systems, with the help of deep learning and reinforcement learning. However, these policies are usually validated in a city simulator due to the high cost of direct application in the real city. Nevertheless, the policies validated in the city simulator may not work in the real city if the simulator is significantly different from the real world. This issue can be addressed by building a real-like traffic simulation system. Therefore, this paper proposes a solution to learn the human routing model, an essential part of the traffic simulator, to overcome the challenges of determining human routing decisions and limited data availability. The proposed theory-guided residual network model combines the traditional shortest path algorithms with drivable condition preferences to capture the general principles for human routing decisions and learn from limited data. Extensive experiments on multiple real-world datasets have demonstrated the superior performance of the model, especially with small data, and illustrated its effectiveness in recovering real routes through case studies.",1
"Accelerating learning processes for complex tasks by leveraging previously learned tasks has been one of the most challenging problems in reinforcement learning, especially when the similarity between source and target tasks is low. This work proposes REPresentation And INstance Transfer (REPAINT) algorithm for knowledge transfer in deep reinforcement learning. REPAINT not only transfers the representation of a pre-trained teacher policy in the on-policy learning, but also uses an advantage-based experience selection approach to transfer useful samples collected following the teacher policy in the off-policy learning. Our experimental results on several benchmark tasks show that REPAINT significantly reduces the total training time in generic cases of task similarity. In particular, when the source tasks are dissimilar to, or sub-tasks of, the target tasks, REPAINT outperforms other baselines in both training-time reduction and asymptotic performance of return scores.",0
"Reinforcement learning faces a major challenge in expediting the learning process for complex tasks, particularly when the similarity between the source and target tasks is low. To address this issue, the REPAINT algorithm has been proposed for deep reinforcement learning. This algorithm transfers the representation of a pre-trained teacher policy during on-policy learning and employs an advantage-based experience selection approach to transfer valuable samples obtained following the teacher policy during off-policy learning. Our experiments on various benchmark tasks demonstrate that REPAINT significantly reduces the total training time, especially in cases of dissimilar or sub-tasks of the target tasks. Furthermore, REPAINT surpasses other baselines in both training-time reduction and asymptotic performance of return scores.",1
"When transferring a control policy from simulation to a physical system, the policy needs to be robust to variations in the dynamics to perform well. Commonly, the optimal policy overfits to the approximate model and the corresponding state-distribution, often resulting in failure to trasnfer underlying distributional shifts. In this paper, we present Robust Fitted Value Iteration, which uses dynamic programming to compute the optimal value function on the compact state domain and incorporates adversarial perturbations of the system dynamics. The adversarial perturbations encourage a optimal policy that is robust to changes in the dynamics. Utilizing the continuous-time perspective of reinforcement learning, we derive the optimal perturbations for the states, actions, observations and model parameters in closed-form. Notably, the resulting algorithm does not require discretization of states or actions. Therefore, the optimal adversarial perturbations can be efficiently incorporated in the min-max value function update. We apply the resulting algorithm to the physical Furuta pendulum and cartpole. By changing the masses of the systems we evaluate the quantitative and qualitative performance across different model parameters. We show that robust value iteration is more robust compared to deep reinforcement learning algorithm and the non-robust version of the algorithm. Videos of the experiments are shown at https://sites.google.com/view/rfvi",0
"The policy must be resilient to variations in dynamics for successful control policy transfer from simulation to physical systems. However, the optimal policy often overfits the approximate model and state-distribution, leading to failure in transferring the underlying distributional shifts. This paper proposes Robust Fitted Value Iteration, which uses dynamic programming to determine the optimal value function within the compact state domain and includes adversarial perturbations of system dynamics. The perturbations encourage an optimal policy that is robust to dynamic changes. By utilizing the continuous-time perspective of reinforcement learning, the optimal perturbations for states, actions, observations, and model parameters are derived in closed-form. Notably, the algorithm does not require state or action discretization, allowing for efficient incorporation of optimal adversarial perturbations in the min-max value function update. The algorithm is applied to the Furuta pendulum and cartpole, demonstrating quantitative and qualitative performance across different model parameters. The results indicate that Robust Fitted Value Iteration outperforms deep reinforcement learning algorithms and non-robust versions of the algorithm. Experiment videos are available at https://sites.google.com/view/rfvi.",1
"The application of reinforcement learning (RL) in robotic control is still limited in the environments with sparse and delayed rewards. In this paper, we propose a practical self-imitation learning method named Self-Imitation Learning with Constant Reward (SILCR). Instead of requiring hand-defined immediate rewards from environments, our method assigns the immediate rewards at each timestep with constant values according to their final episodic rewards. In this way, even if the dense rewards from environments are unavailable, every action taken by the agents would be guided properly. We demonstrate the effectiveness of our method in some challenging continuous robotics control tasks in MuJoCo simulation and the results show that our method significantly outperforms the alternative methods in tasks with sparse and delayed rewards. Even compared with alternatives with dense rewards available, our method achieves competitive performance. The ablation experiments also show the stability and reproducibility of our method.",0
"The usage of reinforcement learning (RL) for controlling robots is limited in environments that offer sparse and delayed rewards. In this study, we suggest a practical technique for self-imitation learning called Self-Imitation Learning with Constant Reward (SILCR). Instead of depending on immediate rewards from the environment, our approach assigns constant values as immediate rewards at each timestep according to the final episodic rewards. This guarantees that the agents' actions are guided correctly, even if dense rewards from the environment are not available. We demonstrate the effectiveness of our method in challenging continuous robotics control tasks in MuJoCo simulation. The results indicate that our approach significantly outperforms other methods in tasks featuring sparse and delayed rewards, and it achieves competitive performance even when compared with alternatives that have dense rewards available. Our method's stability and reproducibility are also demonstrated through ablation experiments.",1
"The gloabal objective of inverse Reinforcement Learning (IRL) is to estimate the unknown cost function of some MDP base on observed trajectories generated by (approximate) optimal policies. The classical approach consists in tuning this cost function so that associated optimal trajectories (that minimise the cumulative discounted cost, i.e. the classical RL loss) are 'similar' to the observed ones. Prior contributions focused on penalising degenerate solutions and improving algorithmic scalability. Quite orthogonally to them, we question the pertinence of characterising optimality with respect to the cumulative discounted cost as it induces an implicit bias against policies with longer mixing times. State of the art value based RL algorithms circumvent this issue by solving for the fixed point of the Bellman optimality operator, a stronger criterion that is not well defined for the inverse problem. To alleviate this bias in IRL, we introduce an alternative training loss that puts more weights on future states which yields a reformulation of the (maximum entropy) IRL problem. The algorithms we devised exhibit enhanced performances (and similar tractability) than off-the-shelf ones in multiple OpenAI gym environments.",0
"Inverse Reinforcement Learning (IRL) is a process that aims to determine the unknown cost function of a Markov Decision Process (MDP) using observed trajectories from optimal policies. The traditional method involves adjusting the cost function to ensure that the optimal trajectories, which minimize the cumulative discounted cost, resemble the observed ones. Previous studies have focused on improving algorithmic efficiency and preventing degenerate solutions. However, we question the validity of assessing optimality based on the cumulative discounted cost, as it introduces a bias against policies with longer mixing times. State-of-the-art value-based RL algorithms address this issue by solving for the fixed point of the Bellman optimality operator, but this approach is not well-defined for the inverse problem. To address this bias, we propose an alternative training loss that places greater emphasis on future states, resulting in a reformulation of the IRL problem. Our algorithms demonstrate improved performance and similar tractability compared to existing methods in various OpenAI gym environments.",1
"In partially observable reinforcement learning, offline training gives access to latent information which is not available during online training and/or execution, such as the system state. Asymmetric actor-critic methods exploit such information by training a history-based policy via a state-based critic. However, many asymmetric methods lack theoretical foundation, and are only evaluated on limited domains. We examine the theory of asymmetric actor-critic methods which use state-based critics, and expose fundamental issues which undermine the validity of a common variant, and its ability to address high partial observability. We propose an unbiased asymmetric actor-critic variant which is able to exploit state information while remaining theoretically sound, maintaining the validity of the policy gradient theorem, and introducing no bias and relatively low variance into the training process. An empirical evaluation performed on domains which exhibit significant partial observability confirms our analysis, and shows the unbiased asymmetric actor-critic converges to better policies and/or faster than symmetric actor-critic and standard asymmetric actor-critic baselines.",0
"Partially observable reinforcement learning involves offline training that provides access to hidden information which is not available during online training or execution, such as the state of the system. Asymmetric actor-critic methods utilize this information by training a policy based on history using a state-based critic. However, despite their usefulness, many asymmetric methods lack a solid theoretical foundation and have only been tested on limited domains. Our study examines the theoretical basis of asymmetric actor-critic methods which rely on state-based critics, and uncovers fundamental issues that undermine the validity of a common variant and its ability to handle high levels of partial observability. We propose an unbiased asymmetric actor-critic variant that is able to take advantage of state information while remaining theoretically sound, preserving the validity of the policy gradient theorem, and introducing minimal bias and variance in the training process. Empirical evaluation conducted on domains with significant partial observability confirms our analysis and demonstrates that the unbiased asymmetric actor-critic approach converges to better policies and/or faster than symmetric actor-critic and standard asymmetric actor-critic baselines.",1
"Connected and Automated Hybrid Electric Vehicles have the potential to reduce fuel consumption and travel time in real-world driving conditions. The eco-driving problem seeks to design optimal speed and power usage profiles based upon look-ahead information from connectivity and advanced mapping features. Recently, Deep Reinforcement Learning (DRL) has been applied to the eco-driving problem. While the previous studies synthesize simulators and model-free DRL to reduce online computation, this work proposes a Safe Off-policy Model-Based Reinforcement Learning algorithm for the eco-driving problem. The advantages over the existing literature are three-fold. First, the combination of off-policy learning and the use of a physics-based model improves the sample efficiency. Second, the training does not require any extrinsic rewarding mechanism for constraint satisfaction. Third, the feasibility of trajectory is guaranteed by using a safe set approximated by deep generative models.   The performance of the proposed method is benchmarked against a baseline controller representing human drivers, a previously designed model-free DRL strategy, and the wait-and-see optimal solution. In simulation, the proposed algorithm leads to a policy with a higher average speed and a better fuel economy compared to the model-free agent. Compared to the baseline controller, the learned strategy reduces the fuel consumption by more than 21\% while keeping the average speed comparable.",0
"Potential benefits in fuel consumption and travel time can be achieved through the use of Connected and Automated Hybrid Electric Vehicles in real-world driving conditions. To optimize speed and power usage profiles based on advanced mapping features and connectivity, the eco-driving problem has emerged. Deep Reinforcement Learning (DRL) has been employed to tackle this problem in previous studies, utilizing simulators and model-free DRL to reduce online computation. However, this work presents a Safe Off-policy Model-Based Reinforcement Learning algorithm for the eco-driving problem, which offers three distinct advantages over existing literature. Firstly, the combination of off-policy learning and a physics-based model improves sample efficiency. Secondly, no extrinsic rewarding mechanism is required for constraint satisfaction during training. Thirdly, a safe set is approximated by deep generative models to guarantee the feasibility of the trajectory. To evaluate the proposed method, simulations were performed and compared to a baseline controller, a previously designed model-free DRL strategy, and the wait-and-see optimal solution. The results showed that the proposed algorithm led to a policy with higher average speed and better fuel economy compared to the model-free agent. Additionally, the learned strategy reduced fuel consumption by over 21% compared to the baseline controller while maintaining a comparable average speed.",1
"Growing advancements in reinforcement learning has led to advancements in control theory. Reinforcement learning has effectively solved the inverted pendulum problem and more recently the double inverted pendulum problem. In reinforcement learning, our agents learn by interacting with the control system with the goal of maximizing rewards. In this paper, we explore three such reward functions in the cart position problem. This paper concludes that a discontinuous reward function that gives non-zero rewards to agents only if they are within a given distance from the desired position gives the best results.",0
"The progress of reinforcement learning has resulted in progressions in control theory, with successful solutions to the inverted pendulum and double inverted pendulum problems. Through interaction with the control system, agents in reinforcement learning strive to maximize rewards. This study delves into three reward functions in the cart position problem, concluding that the most optimal outcome is achieved by utilizing a discontinuous reward function that grants rewards solely to agents within a specified proximity to the desired position.",1
"Reinforcement learning (RL) is a general framework that allows systems to learn autonomously through trial-and-error interaction with their environment. In recent years combining RL with expressive, high-capacity neural network models has led to impressive performance in a diverse range of domains. However, dealing with the large state and action spaces often required for problems in the real world still remains a significant challenge. In this paper we introduce a new simulation environment, ""Gambit"", designed as a tool to build scenarios that can drive RL research in a direction useful for military analysis. Using this environment we focus on an abstracted and simplified room clearance scenario, where a team of blue agents have to make their way through a building and ensure that all rooms are cleared of (and remain clear) of enemy red agents. We implement a multi-agent version of feudal hierarchical RL that introduces a command hierarchy where a commander at the higher level sends orders to multiple agents at the lower level who simply have to learn to follow these orders. We find that breaking the task down in this way allows us to solve a number of non-trivial floorplans that require the coordination of multiple agents much more efficiently than the standard baseline RL algorithms we compare with. We then go on to explore how qualitatively different behaviour can emerge depending on what we prioritise in the agent's reward function (e.g. clearing the building quickly vs. prioritising rescuing civilians).",0
"RL is a framework that enables autonomous learning through trial-and-error interaction with the environment. The combination of RL with expressive, high-capacity neural network models has demonstrated impressive performance in various domains. Nonetheless, dealing with large state and action spaces poses a significant challenge in real-world problems. The paper introduces ""Gambit,"" a simulation environment that aims to drive RL research in a direction useful for military analysis. The environment focuses on an abstracted and simplified room clearance scenario where a team of blue agents must navigate through a building, ensuring that all rooms are free of enemy red agents. The paper implements a multi-agent version of feudal hierarchical RL that introduces a command hierarchy. The commander at the higher level sends orders to multiple agents at the lower level, who learn to follow these orders. The paper finds that breaking down the task in this way enables the solving of several non-trivial floorplans requiring the coordination of multiple agents more efficiently than standard baseline RL algorithms. Finally, the paper explores how different behavior can emerge depending on the priority of the agent's reward function, such as clearing the building quickly versus prioritizing rescuing civilians.",1
"Policy optimization, which learns the policy of interest by maximizing the value function via large-scale optimization techniques, lies at the heart of modern reinforcement learning (RL). In addition to value maximization, other practical considerations arise commonly as well, including the need of encouraging exploration, and that of ensuring certain structural properties of the learned policy due to safety, resource and operational constraints. These considerations can often be accounted for by resorting to regularized RL, which augments the target value function with a structure-promoting regularization term.   Focusing on an infinite-horizon discounted Markov decision process, this paper proposes a generalized policy mirror descent (GPMD) algorithm for solving regularized RL. As a generalization of policy mirror descent Lan (2021), the proposed algorithm accommodates a general class of convex regularizers as well as a broad family of Bregman divergence in cognizant of the regularizer in use. We demonstrate that our algorithm converges linearly over an entire range of learning rates, in a dimension-free fashion, to the global solution, even when the regularizer lacks strong convexity and smoothness. In addition, this linear convergence feature is provably stable in the face of inexact policy evaluation and imperfect policy updates. Numerical experiments are provided to corroborate the applicability and appealing performance of GPMD.",0
"Modern reinforcement learning (RL) relies heavily on policy optimization, which involves maximizing the value function through large-scale optimization techniques. However, other practical considerations such as encouraging exploration and ensuring certain structural properties of the learned policy due to safety, resource, and operational constraints also arise. Regularized RL can account for these considerations by augmenting the target value function with a structure-promoting regularization term. This paper proposes a generalized policy mirror descent (GPMD) algorithm for solving regularized RL in an infinite-horizon discounted Markov decision process. The proposed algorithm accommodates a general class of convex regularizers and a broad family of Bregman divergence, even when lacking strong convexity and smoothness. GPMD is proven to converge linearly to the global solution regardless of learning rates and is stable despite inexact policy evaluation and imperfect policy updates. Numerical experiments support the applicability and performance of GPMD.",1
"Successful teaching requires an assumption of how the learner learns - how the learner uses experiences from the world to update their internal states. We investigate what expectations people have about a learner when they teach them in an online manner using rewards and punishment. We focus on a common reinforcement learning method, Q-learning, and examine what assumptions people have using a behavioral experiment. To do so, we first establish a normative standard, by formulating the problem as a machine teaching optimization problem. To solve the machine teaching optimization problem, we use a deep learning approximation method which simulates learners in the environment and learns to predict how feedback affects the learner's internal states. What do people assume about a learner's learning and discount rates when they teach them an idealized exploration-exploitation task? In a behavioral experiment, we find that people can teach the task to Q-learners in a relatively efficient and effective manner when the learner uses a small value for its discounting rate and a large value for its learning rate. However, they still are suboptimal. We also find that providing people with real-time updates of how possible feedback would affect the Q-learner's internal states weakly helps them teach. Our results reveal how people teach using evaluative feedback and provide guidance for how engineers should design machine agents in a manner that is intuitive for people.",0
"To be a successful teacher, one must understand how a learner updates their internal state based on experiences from the world. Our research aims to uncover people's assumptions about learners when teaching them online using rewards and punishment, particularly in the context of the Q-learning method. We use a machine teaching optimization problem to establish a normative standard and employ a deep learning approximation method to simulate learners' responses to feedback. Our experiment investigates what people assume about a learner's learning and discount rates in an exploration-exploitation task. We find that people can teach the task well when the learner has a low discounting rate and high learning rate, but there is still room for improvement. Providing real-time feedback to people slightly improves their teaching performance. Our findings shed light on how people teach and can inform the design of intuitive machine agents.",1
"The bus system is a critical component of sustainable urban transportation. However, due to the significant uncertainties in passenger demand and traffic conditions, bus operation is unstable in nature and bus bunching has become a common phenomenon that undermines the reliability and efficiency of bus services. Despite recent advances in multi-agent reinforcement learning (MARL) on traffic control, little research has focused on bus fleet control due to the tricky asynchronous characteristic -- control actions only happen when a bus arrives at a bus stop and thus agents do not act simultaneously. In this study, we formulate route-level bus fleet control as an asynchronous multi-agent reinforcement learning (ASMR) problem and extend the classical actor-critic architecture to handle the asynchronous issue. Specifically, we design a novel critic network to effectively approximate the marginal contribution for other agents, in which graph attention neural network is used to conduct inductive learning for policy evaluation. The critic structure also helps the ego agent optimize its policy more efficiently. We evaluate the proposed framework on real-world bus services and actual passenger demand derived from smart card data. Our results show that the proposed model outperforms both traditional headway-based control methods and existing MARL methods.",0
"The sustainability of urban transportation heavily relies on the bus system. However, the instability of bus operation due to unpredictable passenger demand and traffic conditions has led to a common occurrence of bus bunching, which hampers the efficiency and reliability of bus services. Although multi-agent reinforcement learning (MARL) has seen significant progress in traffic control, little research has focused on bus fleet control due to its asynchronous nature, where agents only act when a bus reaches a stop. In this study, we present an asynchronous multi-agent reinforcement learning (ASMR) approach for route-level bus fleet control, addressing the asynchronous issue through an extended actor-critic architecture that includes a critic network. This network uses a graph attention neural network for inductive learning to evaluate policies and optimize them more efficiently. We evaluate our approach on real-world bus services and smart card data, showing that it outperforms traditional headway-based control methods and existing MARL methods.",1
"Solving tasks with sparse rewards is one of the most important challenges in reinforcement learning. In the single-agent setting, this challenge is addressed by introducing intrinsic rewards that motivate agents to explore unseen regions of their state spaces; however, applying these techniques naively to the multi-agent setting results in agents exploring independently, without any coordination among themselves. Exploration in cooperative multi-agent settings can be accelerated and improved if agents coordinate their exploration. In this paper we introduce a framework for designing intrinsic rewards which consider what other agents have explored such that the agents can coordinate. Then, we develop an approach for learning how to dynamically select between several exploration modalities to maximize extrinsic rewards. Concretely, we formulate the approach as a hierarchical policy where a high-level controller selects among sets of policies trained on diverse intrinsic rewards and the low-level controllers learn the action policies of all agents under these specific rewards. We demonstrate the effectiveness of the proposed approach in cooperative domains with sparse rewards where state-of-the-art methods fail and challenging multi-stage tasks that necessitate changing modes of coordination.",0
"The challenge of solving tasks with sparse rewards is a crucial issue in reinforcement learning. In order to address this challenge in the single-agent setting, intrinsic rewards are introduced to encourage agents to explore areas of their state spaces that they have not yet encountered. However, this approach cannot be applied in the same way to the multi-agent setting as it results in agents exploring independently without any coordination. To improve exploration in cooperative multi-agent settings, agents must coordinate their exploration. In this paper, we propose a framework for designing intrinsic rewards that considers what other agents have explored in order to facilitate coordination. We also present an approach for dynamically selecting between several exploration methods to maximize extrinsic rewards. This approach is formulated as a hierarchical policy where a high-level controller selects from a variety of policies trained on different intrinsic rewards, and low-level controllers learn the action policies of all agents under these specific rewards. We demonstrate the effectiveness of this approach in cooperative domains with sparse rewards and multi-stage tasks that require changing modes of coordination, where current state-of-the-art methods fail.",1
"Despite seminal advances in reinforcement learning in recent years, many domains where the rewards are sparse, e.g. given only at task completion, remain quite challenging. In such cases, it can be beneficial to tackle the task both from its beginning and end, and make the two ends meet. Existing approaches that do so, however, are not effective in the common scenario where the strategy needed near the end goal is very different from the one that is effective earlier on.   In this work we propose a novel RL approach for such settings. In short, we first train a backward-looking agent with a simple relaxed goal, and then augment the state representation of the forward-looking agent with straightforward hint features. This allows the learned forward agent to leverage information from backward plans, without mimicking their policy.   We demonstrate the efficacy of our approach on the challenging game of Sokoban, where we substantially surpass learned solvers that generalize across levels, and are competitive with SOTA performance of the best highly-crafted systems. Impressively, we achieve these results while learning from a small number of practice levels and using simple RL techniques.",0
"Although there have been significant advancements in reinforcement learning in recent times, certain domains present a challenge due to sparse rewards that are only given upon task completion. In such cases, it can be advantageous to approach the task from both the beginning and end, and connect the two. Current methods, however, are ineffective when the strategy required near the end differs significantly from the one earlier on. To address this, we propose a new RL technique where we first train a backward-looking agent with a relaxed goal, and then enhance the state representation of the forward-looking agent with hint features. This enables the forward agent to benefit from information gleaned from backward plans, without duplicating their policy. We showcase the effectiveness of our technique in Sokoban, where we exceed learned solvers that can generalize across levels, and match the performance of the best highly-crafted systems. Remarkably, we achieve these outcomes with only a few training levels and basic RL methods.",1
"3D ultrasound (US) has become prevalent due to its rich spatial and diagnostic information not contained in 2D US. Moreover, 3D US can contain multiple standard planes (SPs) in one shot. Thus, automatically localizing SPs in 3D US has the potential to improve user-independence and scanning-efficiency. However, manual SP localization in 3D US is challenging because of the low image quality, huge search space and large anatomical variability. In this work, we propose a novel multi-agent reinforcement learning (MARL) framework to simultaneously localize multiple SPs in 3D US. Our contribution is four-fold. First, our proposed method is general and it can accurately localize multiple SPs in different challenging US datasets. Second, we equip the MARL system with a recurrent neural network (RNN) based collaborative module, which can strengthen the communication among agents and learn the spatial relationship among planes effectively. Third, we explore to adopt the neural architecture search (NAS) to automatically design the network architecture of both the agents and the collaborative module. Last, we believe we are the first to realize automatic SP localization in pelvic US volumes, and note that our approach can handle both normal and abnormal uterus cases. Extensively validated on two challenging datasets of the uterus and fetal brain, our proposed method achieves the average localization accuracy of 7.03 degrees/1.59mm and 9.75 degrees/1.19mm. Experimental results show that our light-weight MARL model has higher accuracy than state-of-the-art methods.",0
"The widespread use of 3D ultrasound (US) is due to its ability to provide rich spatial and diagnostic information that is not available in 2D US. Additionally, 3D US can capture multiple standard planes (SPs) in a single scan, which has the potential to improve user-independence and scanning-efficiency. However, manual SP localization in 3D US is challenging due to factors such as low image quality, a large search space, and anatomical variability. In this study, we propose a new multi-agent reinforcement learning (MARL) framework to automatically localize multiple SPs in 3D US. Our approach has four main contributions: it is generalizable across different challenging US datasets, we use a collaborative module based on a recurrent neural network (RNN) to improve communication and learn spatial relationships, we explore the use of neural architecture search (NAS) to optimize network architecture, and we demonstrate our method's effectiveness in localizing SPs in both normal and abnormal uterus cases. Experimental results on two challenging datasets of the uterus and fetal brain demonstrate that our proposed method achieves higher accuracy than state-of-the-art methods.",1
"Reinforcement learning (RL) is an effective technique for training decision-making agents through interactions with their environment. The advent of deep learning has been associated with highly notable successes with sequential decision making problems - such as defeating some of the highest-ranked human players at Go. In digital advertising, real-time bidding (RTB) is a common method of allocating advertising inventory through real-time auctions. Bidding strategies need to incorporate logic for dynamically adjusting parameters in order to deliver pre-assigned campaign goals. Here we discuss techniques toward using RL to train bidding agents. As a campaign metric we particularly focused on viewability: the percentage of inventory which goes on to be viewed by an end user.   This paper is presented as a survey of techniques and experiments which we developed through the course of this research. We discuss expanding our training data to include edge cases by training on simulated interactions. We discuss the experimental results comparing the performance of several promising RL algorithms, and an approach to hyperparameter optimization of an actor/critic training pipeline through Bayesian optimization. Finally, we present live-traffic tests of some of our RL agents against a rule-based feedback-control approach, demonstrating the potential for this method as well as areas for further improvement. This paper therefore presents an arrangement of our findings in this quickly developing field, and ways that it can be applied to an RTB use case.",0
"The technique of reinforcement learning (RL) is effective for training decision-making agents by engaging them in interactions with their environment. The success of deep learning has been evident in sequential decision-making problems, like defeating top-ranked human players in Go. Real-time bidding (RTB) is a common method for allocating advertising inventory through real-time auctions in digital advertising. Bidding strategies must incorporate logic for dynamic parameter adjustment to meet pre-assigned campaign objectives. This article explores techniques for using RL in training bidding agents, with a focus on the viewability campaign metric. Our research presents a survey of methods and experiments, including expanding training data to incorporate edge cases through simulated interactions. We also discuss experimental results comparing the performance of several promising RL algorithms and an approach to hyperparameter optimization of an actor/critic training pipeline using Bayesian optimization. Finally, we present live-traffic tests of our RL agents against a rule-based feedback-control approach, demonstrating the potential for this technique, as well as areas for further improvement. This paper presents our findings in this rapidly developing field and ways it can be applied to an RTB use case.",1
"Since the introduction of DQN, a vast majority of reinforcement learning research has focused on reinforcement learning with deep neural networks as function approximators. New methods are typically evaluated on a set of environments that have now become standard, such as Atari 2600 games. While these benchmarks help standardize evaluation, their computational cost has the unfortunate side effect of widening the gap between those with ample access to computational resources, and those without. In this work we argue that, despite the community's emphasis on large-scale environments, the traditional small-scale environments can still yield valuable scientific insights and can help reduce the barriers to entry for underprivileged communities. To substantiate our claims, we empirically revisit the paper which introduced the Rainbow algorithm [Hessel et al., 2018] and present some new insights into the algorithms used by Rainbow.",0
"The majority of reinforcement learning research since DQN's introduction has focused on utilizing deep neural networks as function approximators. Standardized evaluation environments, such as Atari 2600 games, have become the norm for assessing new methods, despite their high computational cost. Unfortunately, this creates a divide between those with ample access to computational resources and those without. This work argues that small-scale environments still offer valuable scientific insights and help reduce barriers to entry for underprivileged communities, contrary to the community's focus on large-scale environments. To support this claim, we revisit the paper that introduced the Rainbow algorithm [Hessel et al., 2018] and provide new insights into the algorithm's use.",1
"In real world settings, numerous constraints are present which are hard to specify mathematically. However, for the real world deployment of reinforcement learning (RL), it is critical that RL agents are aware of these constraints, so that they can act safely. In this work, we consider the problem of learning constraints from demonstrations of a constraint-abiding agent's behavior. We experimentally validate our approach and show that our framework can successfully learn the most likely constraints that the agent respects. We further show that these learned constraints are \textit{transferable} to new agents that may have different morphologies and/or reward functions. Previous works in this regard have either mainly been restricted to tabular (discrete) settings, specific types of constraints or assume the environment's transition dynamics. In contrast, our framework is able to learn arbitrary \textit{Markovian} constraints in high-dimensions in a completely model-free setting. The code can be found it: \url{https://github.com/shehryar-malik/icrl}.",0
"The presence of various constraints in real-world scenarios can be challenging to express mathematically. However, it is crucial for reinforcement learning (RL) agents to be aware of these constraints to act safely during their deployment in the real world. In this study, we address the issue of learning constraints from a well-behaved agent's demonstrations. Our experimental results confirm that our approach can effectively discover the most probable constraints adhered to by the agent. Moreover, these learned constraints can be imparted to new agents with different reward functions or morphologies. Unlike previous works, our framework can learn arbitrary Markovian constraints in high-dimensional spaces without making assumptions about the environment's transition dynamics, and it is completely model-free. The code is available at: \url{https://github.com/shehryar-malik/icrl}.",1
"We show that the popular reinforcement learning (RL) strategy of estimating the state-action value (Q-function) by minimizing the mean squared Bellman error leads to a regression problem with confounding, the inputs and output noise being correlated. Hence, direct minimization of the Bellman error can result in significantly biased Q-function estimates. We explain why fixing the target Q-network in Deep Q-Networks and Fitted Q Evaluation provides a way of overcoming this confounding, thus shedding new light on this popular but not well understood trick in the deep RL literature. An alternative approach to address confounding is to leverage techniques developed in the causality literature, notably instrumental variables (IV). We bring together here the literature on IV and RL by investigating whether IV approaches can lead to improved Q-function estimates. This paper analyzes and compares a wide range of recent IV methods in the context of offline policy evaluation (OPE), where the goal is to estimate the value of a policy using logged data only. By applying different IV techniques to OPE, we are not only able to recover previously proposed OPE methods such as model-based techniques but also to obtain competitive new techniques. We find empirically that state-of-the-art OPE methods are closely matched in performance by some IV methods such as AGMM, which were not developed for OPE. We open-source all our code and datasets at https://github.com/liyuan9988/IVOPEwithACME.",0
"In this study, we demonstrate that minimizing the mean squared Bellman error to estimate the state-action value (Q-function) through reinforcement learning can result in a regression problem with correlated input and output noise, leading to biased Q-function estimates. However, fixing the target Q-network in Deep Q-Networks and Fitted Q Evaluation can overcome this confounding issue. We explore an alternative approach to address this problem by leveraging techniques from the causality literature, specifically instrumental variables (IV). We analyze and compare a range of IV methods in the context of offline policy evaluation (OPE), where the goal is to estimate policy value using logged data only. We find that some IV methods, such as AGMM, perform competitively with state-of-the-art OPE methods. We provide our code and datasets on GitHub at https://github.com/liyuan9988/IVOPEwithACME.",1
"First-person object-interaction tasks in high-fidelity, 3D, simulated environments such as the AI2Thor virtual home-environment pose significant sample-efficiency challenges for reinforcement learning (RL) agents learning from sparse task rewards. To alleviate these challenges, prior work has provided extensive supervision via a combination of reward-shaping, ground-truth object-information, and expert demonstrations. In this work, we show that one can learn object-interaction tasks from scratch without supervision by learning an attentive object-model as an auxiliary task during task learning with an object-centric relational RL agent. Our key insight is that learning an object-model that incorporates object-attention into forward prediction provides a dense learning signal for unsupervised representation learning of both objects and their relationships. This, in turn, enables faster policy learning for an object-centric relational RL agent. We demonstrate our agent by introducing a set of challenging object-interaction tasks in the AI2Thor environment where learning with our attentive object-model is key to strong performance. Specifically, we compare our agent and relational RL agents with alternative auxiliary tasks to a relational RL agent equipped with ground-truth object-information, and show that learning with our object-model best closes the performance gap in terms of both learning speed and maximum success rate. Additionally, we find that incorporating object-attention into an object-model's forward predictions is key to learning representations which capture object-category and object-state.",0
"Reinforcement learning (RL) agents face significant challenges when it comes to learning first-person object-interaction tasks in high-fidelity, 3D, simulated environments like the AI2Thor virtual home-environment due to the sparsity of task rewards. Prior research has sought to alleviate this issue by providing extensive supervision through reward-shaping, ground-truth object-information, and expert demonstrations. However, our study shows that it is possible to learn these tasks from scratch without supervision by training an attentive object-model as an auxiliary task during task learning with an object-centric relational RL agent. By learning an object-model that incorporates object-attention into forward prediction, we provide a dense learning signal for unsupervised representation learning of both objects and their relationships. This leads to faster policy learning for an object-centric relational RL agent. Our agent is demonstrated by introducing a set of challenging object-interaction tasks in the AI2Thor environment, and we compare it with other relational RL agents equipped with alternative auxiliary tasks. We find that learning with our object-model is the most effective in terms of both learning speed and maximum success rate. Furthermore, we find that incorporating object-attention into an object-model's forward predictions is vital for capturing object-category and object-state representations.",1
"While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations(SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. The code associated with this work is available at https://github.com/mila-iqia/spr",0
"Although deep reinforcement learning is effective in solving tasks that involve collecting massive amounts of data through unrestricted interaction with the environment, learning from limited interaction proves to be challenging. Our proposal is that an agent can learn more efficiently by enhancing reward maximization with self-supervised objectives that are based on the structure in its visual input and sequential interaction with the environment. We have developed a method called Self-Predictive Representations (SPR) that trains an agent to predict its own latent state representations multiple steps into the future. To compute target representations for future states, we use an encoder that is an exponential moving average of the agent's parameters, and we make predictions using a learned transition model. Our future prediction objective, when used alone, outperforms previous methods for sample-efficient deep RL from pixels. We further enhance performance by adding data augmentation to the future prediction loss, which ensures that the agent's representations are consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, representing a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR surpasses expert human scores on 7 out of 26 games. The code for this work can be accessed at https://github.com/mila-iqia/spr.",1
"Non-stationary environments are challenging for reinforcement learning algorithms. If the state transition and/or reward functions change based on latent factors, the agent is effectively tasked with optimizing a behavior that maximizes performance over a possibly infinite random sequence of Markov Decision Processes (MDPs), each of which drawn from some unknown distribution. We call each such MDP a context. Most related works make strong assumptions such as knowledge about the distribution over contexts, the existence of pre-training phases, or a priori knowledge about the number, sequence, or boundaries between contexts. We introduce an algorithm that efficiently learns policies in non-stationary environments. It analyzes a possibly infinite stream of data and computes, in real-time, high-confidence change-point detection statistics that reflect whether novel, specialized policies need to be created and deployed to tackle novel contexts, or whether previously-optimized ones might be reused. We show that (i) this algorithm minimizes the delay until unforeseen changes to a context are detected, thereby allowing for rapid responses; and (ii) it bounds the rate of false alarm, which is important in order to minimize regret. Our method constructs a mixture model composed of a (possibly infinite) ensemble of probabilistic dynamics predictors that model the different modes of the distribution over underlying latent MDPs. We evaluate our algorithm on high-dimensional continuous reinforcement learning problems and show that it outperforms state-of-the-art (model-free and model-based) RL algorithms, as well as state-of-the-art meta-learning methods specially designed to deal with non-stationarity.",0
"Reinforcement learning algorithms face difficulties in non-stationary environments where the state transition and reward functions may change due to hidden factors. This requires the agent to optimize behavior to maximize performance over an unknown distribution of Markov Decision Processes (MDPs), each representing a context. Current works rely on strong assumptions such as knowledge about the context distribution, pre-training phases, or prior knowledge about the number and boundaries of contexts. To address this, we propose an algorithm that analyzes real-time data to detect change points and create or reuse policies accordingly. Our algorithm minimizes delay and false alarms and constructs a mixture model of probabilistic dynamics predictors to model the distribution over latent MDPs. We demonstrate its superiority over state-of-the-art RL and meta-learning methods on high-dimensional continuous reinforcement learning problems.",1
"This paper investigates the model-based methods in multi-agent reinforcement learning (MARL). We specify the dynamics sample complexity and the opponent sample complexity in MARL, and conduct a theoretic analysis of return discrepancy upper bound. To reduce the upper bound with the intention of low sample complexity during the whole learning process, we propose a novel decentralized model-based MARL method, named Adaptive Opponent-wise Rollout Policy Optimization (AORPO). In AORPO, each agent builds its multi-agent environment model, consisting of a dynamics model and multiple opponent models, and trains its policy with the adaptive opponent-wise rollout. We further prove the theoretic convergence of AORPO under reasonable assumptions. Empirical experiments on competitive and cooperative tasks demonstrate that AORPO can achieve improved sample efficiency with comparable asymptotic performance over the compared MARL methods.",0
"The aim of this study is to analyze the effectiveness of model-based approaches in multi-agent reinforcement learning (MARL). Specifically, we examine the dynamics sample complexity and opponent sample complexity within MARL and conduct a theoretical investigation of the upper bound of return discrepancy. Our objective is to minimize the upper bound and promote low sample complexity during the entire learning process. To address this challenge, we propose a new decentralized model-based MARL technique called Adaptive Opponent-wise Rollout Policy Optimization (AORPO). In AORPO, each agent constructs its multi-agent environment model, which comprises a dynamics model and various opponent models. The agent then trains its policy using the adaptive opponent-wise rollout. We also demonstrate through reasonable assumptions that AORPO has theoretical convergence. Our empirical experiments on competitive and cooperative tasks show that AORPO provides better sample efficiency and comparable asymptotic performance compared to other MARL approaches.",1
"Automatic and accurate detection of anatomical landmarks is an essential operation in medical image analysis with a multitude of applications. Recent deep learning methods have improved results by directly encoding the appearance of the captured anatomy with the likelihood maps (i.e., heatmaps). However, most current solutions overlook another essence of heatmap regression, the objective metric for regressing target heatmaps and rely on hand-crafted heuristics to set the target precision, thus being usually cumbersome and task-specific. In this paper, we propose a novel learning-to-learn framework for landmark detection to optimize the neural network and the target precision simultaneously. The pivot of this work is to leverage the reinforcement learning (RL) framework to search objective metrics for regressing multiple heatmaps dynamically during the training process, thus avoiding setting problem-specific target precision. We also introduce an early-stop strategy for active termination of the RL agent's interaction that adapts the optimal precision for separate targets considering exploration-exploitation tradeoffs. This approach shows better stability in training and improved localization accuracy in inference. Extensive experimental results on two different applications of landmark localization: 1) our in-house prenatal ultrasound (US) dataset and 2) the publicly available dataset of cephalometric X-Ray landmark detection, demonstrate the effectiveness of our proposed method. Our proposed framework is general and shows the potential to improve the efficiency of anatomical landmark detection.",0
"The detection of anatomical landmarks is a crucial task in medical image analysis with numerous applications. Deep learning methods have recently shown better results by using likelihood maps (heatmaps) to directly encode the appearance of the captured anatomy. However, the current solutions often neglect the importance of the objective metric for regressing target heatmaps and rely on hand-crafted heuristics to set the target precision, making them task-specific and complicated. In this paper, we introduce a novel learning-to-learn framework for landmark detection that optimizes the neural network and the target precision simultaneously. Our approach leverages the reinforcement learning (RL) framework to dynamically search objective metrics for regressing multiple heatmaps during training, avoiding the need for problem-specific target precision. We also introduce an early-stop strategy that adapts the optimal precision for separate targets based on exploration-exploitation tradeoffs. This approach improves stability during training and localization accuracy during inference. We demonstrate the effectiveness of our proposed method on two different applications of landmark localization: our in-house prenatal ultrasound (US) dataset and the publicly available dataset of cephalometric X-Ray landmark detection. Our framework is general and has the potential to improve the efficiency of anatomical landmark detection.",1
"Deep reinforcement learning has been one of the fastest growing fields of machine learning over the past years and numerous libraries have been open sourced to support research. However, most codebases have a steep learning curve or limited flexibility that do not satisfy a need for fast prototyping in fundamental research. This paper introduces Tonic, a Python library allowing researchers to quickly implement new ideas and measure their importance by providing: 1) general-purpose configurable modules 2) several baseline agents: A2C, TRPO, PPO, MPO, DDPG, D4PG, TD3 and SAC built with these modules 3) support for TensorFlow 2 and PyTorch 4) support for continuous-control environments from OpenAI Gym, DeepMind Control Suite and PyBullet 5) scripts to experiment in a reproducible way, plot results, and play with trained agents 6) a benchmark of the provided agents on 70 continuous-control tasks. Evaluation is performed in fair conditions with identical seeds, training and testing loops, while sharing general improvements such as non-terminal timeouts and observation normalization. Finally, to demonstrate how Tonic simplifies experimentation, a novel agent called TD4 is implemented and evaluated.",0
"Over the last few years, deep reinforcement learning has rapidly developed into one of the most prominent branches of machine learning, and a variety of open source libraries have been created to support research in this area. However, these codebases often have a steep learning curve and limited flexibility, which can prevent researchers from rapidly prototyping new ideas in fundamental research. In response to this issue, this paper introduces Tonic, a Python library designed to enable researchers to quickly implement and test new ideas. Tonic achieves this by providing general-purpose configurable modules, several baseline agents, support for TensorFlow 2 and PyTorch, and support for environments from OpenAI Gym, DeepMind Control Suite, and PyBullet. Additionally, Tonic includes scripts that allow for reproducible experimentation and the evaluation of trained agents, as well as a benchmark of the provided agents on 70 continuous-control tasks. To demonstrate the simplicity of Tonic's experimentation process, the paper presents an evaluation of a novel agent called TD4.",1
"Reinforcement learning (RL) is currently a popular research topic in control engineering and has the potential to make its way to industrial and commercial applications. Corresponding RL controllers are trained in direct interaction with the controlled system, rendering them data-driven and performance-oriented solutions. The best practice of exploring starts (ES) is used by default to support the learning process via randomly picked initial states. However, this method might deliver strongly biased results if the system's dynamic and constraints lead to unfavorable sample distributions in the state space (e.g., condensed sample accumulation in certain state-space areas). To overcome this issue, a kernel density estimation-based state-space coverage acceleration (DESSCA) is proposed, which improves the ES concept by prioritizing infrequently visited states for a more balanced coverage of the state space during training. Considered test scenarios are mountain car, cartpole and electric motor control environments. Using DQN and DDPG as exemplary RL algorithms, it can be shown that DESSCA is a simple yet effective algorithmic extension to the established ES approach.",0
"The current focus of research in control engineering is on reinforcement learning (RL), which has the potential to be applied in industrial and commercial settings. RL controllers learn through direct interaction with the controlled system, resulting in data-driven and performance-oriented solutions. The standard method of using exploring starts (ES) to randomly select initial states during training may produce biased results if the system's dynamics and constraints lead to uneven sample distributions in the state space. To address this issue, a kernel density estimation-based state-space coverage acceleration (DESSCA) has been proposed. DESSCA prioritizes infrequently visited states to achieve a more balanced coverage of the state space during training. The effectiveness of DESSCA was tested in various scenarios, including mountain car, cartpole, and electric motor control environments, using DQN and DDPG as examples of RL algorithms. The results demonstrate that DESSCA is a simple and effective extension to the established ES approach.",1
"Patients with severe Coronavirus disease 19 (COVID-19) typically require supplemental oxygen as an essential treatment. We developed a machine learning algorithm, based on a deep Reinforcement Learning (RL), for continuous management of oxygen flow rate for critical ill patients under intensive care, which can identify the optimal personalized oxygen flow rate with strong potentials to reduce mortality rate relative to the current clinical practice. Basically, we modeled the oxygen flow trajectory of COVID-19 patients and their health outcomes as a Markov decision process. Based on individual patient characteristics and health status, a reinforcement learning based oxygen control policy is learned and real-time recommends the oxygen flow rate to reduce the mortality rate. We assessed the performance of proposed methods through cross validation by using a retrospective cohort of 1,372 critically ill patients with COVID-19 from New York University Langone Health ambulatory care with electronic health records from April 2020 to January 2021. The mean mortality rate under the RL algorithm is lower than standard of care by 2.57% (95% CI: 2.08- 3.06) reduction (P<0.001) from 7.94% under the standard of care to 5.37 % under our algorithm and the averaged recommended oxygen flow rate is 1.28 L/min (95% CI: 1.14-1.42) lower than the rate actually delivered to patients. Thus, the RL algorithm could potentially lead to better intensive care treatment that can reduce mortality rate, while saving the oxygen scarce resources. It can reduce the oxygen shortage issue and improve public health during the COVID-19 pandemic.",0
"Supplemental oxygen is a necessary treatment for severe COVID-19 patients. Our team developed a deep Reinforcement Learning (RL) algorithm to continuously manage oxygen flow rate for critically ill patients in intensive care. The algorithm models oxygen flow trajectory and health outcomes as a Markov decision process, learning a personalized oxygen control policy based on individual patient characteristics and health status. We tested the algorithm on a retrospective cohort of 1,372 critically ill COVID-19 patients from New York University Langone Health ambulatory care using electronic health records from April 2020 to January 2021. Our results showed that the RL algorithm has the potential to reduce mortality rate by 2.57% compared to the current standard of care, with an average recommended oxygen flow rate 1.28 L/min lower than the rate delivered to patients. This algorithm could lead to better intensive care treatment, reduce mortality rate, and save scarce oxygen resources, improving public health during the COVID-19 pandemic.",1
"Inspired by the cache replacement problem, we propose and solve a new variant of the well-known multi-armed bandit (MAB), thus providing a solution for improving existing state-of-the-art cache management methods. Each arm (or expert) represents a distinct cache replacement policy, which advises on the page to evict from the cache when needed. Feedback on the eviction comes in the form of a ""miss"", but at an indeterminate time after the action is taken, and the cost of the eviction is set to be inversely proportional to the response time. The feedback is ignored if it comes after a threshold value for the delay, which we set to be equal to the size of the page eviction history. Thus, for delays beyond the threshold, its cost is assumed to be zero. Consequently, we call this problem with delayed feedback and decaying costs. We introduce an adaptive reinforcement learning algorithm EXP4-DFDC that provides a solution to the problem. We derive an optimal learning rate for EXP4-DFDC that defines the balance between exploration and exploitation and proves theoretically that the expected regret of our algorithm is a vanishing quantity as a function of time. As an application, we show that LeCaR, a recent top-performing machine learning algorithm for cache replacement, can be enhanced with adaptive learning using our formulations. We present an improved adaptive version of LeCaR, called OLeCaR, with the learning rate set as determined by the theoretical derivation presented here to minimize regret for EXP4-DFDC. It then follows that LeCaR and OLeCaR are theoretically guaranteed to have vanishing regret over time.",0
"Our proposed solution tackles the cache replacement problem by introducing a new variant of the multi-armed bandit (MAB) approach. Each arm represents a unique cache replacement policy that advises on page eviction. A ""miss"" serves as feedback on the eviction, but the cost of eviction is inversely proportional to the response time and ignored if feedback comes after a threshold delay. We call this problem with delayed feedback and decaying costs and propose an adaptive reinforcement learning algorithm, EXP4-DFDC, to solve it. We derive an optimal learning rate for EXP4-DFDC, which minimizes regret over time and balances exploration and exploitation. Our algorithm is theoretically proven to have a vanishing regret as a function of time. We apply our method to LeCaR, a top-performing machine learning algorithm for cache replacement, and present an improved adaptive version, OLeCaR. The theoretical derivation of the optimal learning rate ensures OLeCaR's vanishing regret over time.",1
"Decomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic manner to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the selected modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.",0
"When faced with changes in distribution, breaking knowledge into interchangeable parts can lead to a generalization advantage. This is particularly relevant for a learning agent that interacts with its environment and needs to combine existing knowledge in novel ways. To investigate this idea, we propose a training framework where an agent's necessary knowledge and reward function are stationary and reusable across tasks. An attention mechanism chooses which modules can be adapted to the current task, with the module parameters changing rapidly while the attention parameters remain stable. We focus on sparse communication between modules through an attention bottleneck and find that meta-learning the modular aspects leads to faster adaptation in a reinforcement learning setup. Reversing the role of parameters and meta-parameters is not as effective, indicating the importance of fast adaptation of selected modules.",1
"In many decision-making tasks, some specific actions are limited in their frequency or total amounts, such as ""fire"" in the gunfight game and ""buy/sell"" in the stock trading. We name such actions as ""sparse action"". Sparse action often plays a crucial role in achieving good performance. However, their Q-values, estimated by \emph{classical Bellman update}, usually suffer from a large estimation error due to the sparsity of their samples. The \emph{greedy} policy could be greatly misled by the biased Q-function and takes sparse action aggressively, which leads to a huge sub-optimality. This paper constructs a reference distribution that assigns a low probability to sparse action and proposes a regularized objective with an explicit constraint to the reference distribution. Furthermore, we derive a regularized Bellman operator and a regularized optimal policy that can slow down the propagation of error and guide the agent to take sparse action more carefully. The experiment results demonstrate that our method achieves state-of-the-art performance on typical sparse action tasks.",0
"When making decisions, certain actions may have limitations on their frequency or total amount, such as ""fire"" in a gunfight game or ""buy/sell"" in stock trading. These actions are referred to as ""sparse action"" and can significantly impact performance. However, their Q-values, estimated through classical Bellman update, often have a large estimation error due to the lack of samples. This can lead to the ""greedy"" policy aggressively taking sparse action, resulting in suboptimal outcomes. To address this, we propose a reference distribution that assigns a low probability to sparse action and a regularized objective with a constraint to the reference distribution. We also derive a regularized Bellman operator and optimal policy to guide the agent to take sparse action more carefully. Our experiment results show that our approach outperforms existing methods on tasks involving sparse action.",1
"Gym-ANM is a Python package that facilitates the design of reinforcement learning (RL) environments that model active network management (ANM) tasks in electricity networks. Here, we describe how to implement new environments and how to write code to interact with pre-existing ones. We also provide an overview of ANM6-Easy, an environment designed to highlight common ANM challenges. Finally, we discuss the potential impact of Gym-ANM on the scientific community, both in terms of research and education. We hope this package will facilitate collaboration between the power system and RL communities in the search for algorithms to control future energy systems.",0
"The Python package called Gym-ANM is useful for creating reinforcement learning environments that simulate active network management tasks in electricity networks. This article outlines the process of developing new environments and interacting with existing ones, as well as introducing ANM6-Easy, an environment designed to showcase common ANM difficulties. Additionally, the article considers the potential benefits of Gym-ANM for the scientific community, including research and education opportunities. The authors hope that this package will encourage collaboration between the power system and RL communities in the development of algorithms for managing future energy systems.",1
"Many real-world domains are subject to a structured non-stationarity which affects the agent's goals and the environmental dynamics. Meta-reinforcement learning (RL) has been shown successful for training agents that quickly adapt to related tasks. However, most of the existing meta-RL algorithms for non-stationary domains either make strong assumptions on the task generation process or require sampling from it at training time. In this paper, we propose a novel algorithm (TRIO) that optimizes for the future by explicitly tracking the task evolution through time. At training time, TRIO learns a variational module to quickly identify latent parameters from experience samples. This module is learned jointly with an optimal exploration policy that takes task uncertainty into account. At test time, TRIO tracks the evolution of the latent parameters online, hence reducing the uncertainty over future tasks and obtaining fast adaptation through the meta-learned policy. Unlike most existing methods, TRIO does not assume Markovian task-evolution processes, it does not require information about the non-stationarity at training time, and it captures complex changes undergoing in the environment. We evaluate our algorithm on different simulated problems and show it outperforms competitive baselines.",0
"Structured non-stationarity in many real-world domains affects both the agent's goals and environmental dynamics. Meta-reinforcement learning (RL) has proven successful in training agents to rapidly adapt to related tasks. However, most current meta-RL algorithms for non-stationary domains either have strong assumptions on task generation or require sampling during training. This paper presents a new algorithm (TRIO) that optimizes for the future by explicitly tracking task evolution over time. TRIO learns a variational module to quickly identify latent parameters and an optimal exploration policy that accounts for task uncertainty during training. At test time, TRIO tracks the evolution of latent parameters online, reducing uncertainty over future tasks and enabling rapid adaptation through the meta-learned policy. Unlike most existing methods, TRIO does not assume Markovian task-evolution processes, does not require non-stationarity information during training, and captures complex environmental changes. We evaluate TRIO on various simulated problems and demonstrate its superiority over competitive baselines.",1
"With rapid advances in computing systems, there is an increasing demand for more effective and efficient access control (AC) approaches. Recently, Attribute Based Access Control (ABAC) approaches have been shown to be promising in fulfilling the AC needs of such emerging complex computing environments. An ABAC model grants access to a requester based on attributes of entities in a system and an authorization policy; however, its generality and flexibility come with a higher cost. Further, increasing complexities of organizational systems and the need for federated accesses to their resources make the task of AC enforcement and management much more challenging. In this paper, we propose an adaptive ABAC policy learning approach to automate the authorization management task. We model ABAC policy learning as a reinforcement learning problem. In particular, we propose a contextual bandit system, in which an authorization engine adapts an ABAC model through a feedback control loop; it relies on interacting with users/administrators of the system to receive their feedback that assists the model in making authorization decisions. We propose four methods for initializing the learning model and a planning approach based on attribute value hierarchy to accelerate the learning process. We focus on developing an adaptive ABAC policy learning model for a home IoT environment as a running example. We evaluate our proposed approach over real and synthetic data. We consider both complete and sparse datasets in our evaluations. Our experimental results show that the proposed approach achieves performance that is comparable to ones based on supervised learning in many scenarios and even outperforms them in several situations.",0
"The demand for more effective and efficient access control (AC) approaches is increasing due to rapid advances in computing systems. Attribute Based Access Control (ABAC) has been proven to be a promising solution for fulfilling the AC needs of complex computing environments. Nonetheless, the flexibility and generality of ABAC come with a higher cost, especially with the increasing complexities of organizational systems and the need for federated accesses. To address this challenge, we propose an adaptive ABAC policy learning approach that automates the authorization management task. We model ABAC policy learning as a reinforcement learning problem and introduce a contextual bandit system that adapts an ABAC model through a feedback control loop. Our approach relies on user/administrator feedback to assist the model in making authorization decisions. We present four methods for initializing the learning model and a planning approach based on attribute value hierarchy to accelerate the learning process. To demonstrate the effectiveness of our proposed approach, we focus on developing an adaptive ABAC policy learning model for a home IoT environment. We evaluate our approach using both real and synthetic data, considering both complete and sparse datasets. Our results show that our approach achieves comparable performance to supervised learning in many scenarios and even outperforms it in several situations.",1
"Recent work for image captioning mainly followed an extract-then-generate paradigm, pre-extracting a sequence of object-based features and then formulating image captioning as a single sequence-to-sequence task. Although promising, we observed two problems in generated captions: 1) content inconsistency where models would generate contradicting facts; 2) not informative enough where models would miss parts of important information. From a causal perspective, the reason is that models have captured spurious statistical correlations between visual features and certain expressions (e.g., visual features of ""long hair"" and ""woman""). In this paper, we propose a dependent multi-task learning framework with the causal intervention (DMTCI). Firstly, we involve an intermediate task, bag-of-categories generation, before the final task, image captioning. The intermediate task would help the model better understand the visual features and thus alleviate the content inconsistency problem. Secondly, we apply Pearl's do-calculus on the model, cutting off the link between the visual features and possible confounders and thus letting models focus on the causal visual features. Specifically, the high-frequency concept set is considered as the proxy confounders where the real confounders are inferred in the continuous space. Finally, we use a multi-agent reinforcement learning (MARL) strategy to enable end-to-end training and reduce the inter-task error accumulations. The extensive experiments show that our model outperforms the baseline models and achieves competitive performance with state-of-the-art models.",0
"Current approaches for image captioning typically involve extracting object-based features and then using a sequence-to-sequence model to generate captions. However, this approach has led to two issues: inconsistent content and insufficient information. These problems are caused by models capturing spurious correlations between visual features and specific expressions. To address these issues, we propose a dependent multi-task learning framework with causal intervention (DMTCI). Our framework involves an intermediate task of generating a bag-of-categories before the final image captioning task, which helps the model better understand visual features and reduce content inconsistency. We also use Pearl's do-calculus to cut off links between visual features and potential confounders, allowing models to focus on causal visual features. Finally, we apply a multi-agent reinforcement learning strategy to reduce inter-task error accumulation during training. Our extensive experiments demonstrate that our model outperforms baseline models and achieves competitive performance with state-of-the-art models.",1
"We study combinatorial problems with real world applications such as machine scheduling, routing, and assignment. We propose a method that combines Reinforcement Learning (RL) and planning. This method can equally be applied to both the offline, as well as online, variants of the combinatorial problem, in which the problem components (e.g., jobs in scheduling problems) are not known in advance, but rather arrive during the decision-making process. Our solution is quite generic, scalable, and leverages distributional knowledge of the problem parameters. We frame the solution process as an MDP, and take a Deep Q-Learning approach wherein states are represented as graphs, thereby allowing our trained policies to deal with arbitrary changes in a principled manner. Though learned policies work well in expectation, small deviations can have substantial negative effects in combinatorial settings. We mitigate these drawbacks by employing our graph-convolutional policies as non-optimal heuristics in a compatible search algorithm, Monte Carlo Tree Search, to significantly improve overall performance. We demonstrate our method on two problems: Machine Scheduling and Capacitated Vehicle Routing. We show that our method outperforms custom-tailored mathematical solvers, state of the art learning-based algorithms, and common heuristics, both in computation time and performance.",0
"Our research focuses on solving combinatorial problems that have practical applications such as machine scheduling, routing, and assignment. To address these challenges, we propose a hybrid approach that combines Reinforcement Learning (RL) and planning. Our method can handle both offline and online variants of the problem, where the components of the problem are unknown until the decision-making process. Our solution is designed to be scalable, generic, and uses distributional knowledge of the problem parameters. We represent the solution process as a Markov Decision Process (MDP), and use a Deep Q-Learning approach that uses graphs to handle arbitrary changes. Although our learned policies work well, we acknowledge that small deviations can have significant negative effects in combinatorial settings. To address this, we use our graph-convolutional policies as non-optimal heuristics in a Monte Carlo Tree Search algorithm, which significantly improves overall performance. We demonstrate the effectiveness of our method on two problems, Machine Scheduling and Capacitated Vehicle Routing, and show that it outperforms custom-tailored mathematical solvers, state-of-the-art learning-based algorithms, and common heuristics in both computation time and performance.",1
"In this paper we propose a Deep Reinforcement Learning approach to solve a multimodal transportation planning problem, in which containers must be assigned to a truck or to trains that will transport them to their destination. While traditional planning methods work ""offline"" (i.e., they take decisions for a batch of containers before the transportation starts), the proposed approach is ""online"", in that it can take decisions for individual containers, while transportation is being executed. Planning transportation online helps to effectively respond to unforeseen events that may affect the original transportation plan, thus supporting companies in lowering transportation costs. We implemented different container selection heuristics within the proposed Deep Reinforcement Learning algorithm and we evaluated its performance for each heuristic using data that simulate a realistic scenario, designed on the basis of a real case study at a logistics company. The experimental results revealed that the proposed method was able to learn effective patterns of container assignment. It outperformed tested competitors in terms of total transportation costs and utilization of train capacity by 20.48% to 55.32% for the cost and by 7.51% to 20.54% for the capacity. Furthermore, it obtained results within 2.7% for the cost and 0.72% for the capacity of the optimal solution generated by an Integer Linear Programming solver in an offline setting.",0
"Our paper introduces a Deep Reinforcement Learning approach for solving a multimodal transportation planning dilemma, where containers need to be assigned to trucks or trains. Unlike conventional planning methods that operate ""offline,"" our approach is ""online,"" enabling it to make decisions for individual containers while transportation is in progress. This capability allows for better responses to unforeseen events that might impact the original plan, thereby aiding businesses in lowering transportation costs. We integrated different container selection heuristics into our Deep Reinforcement Learning algorithm and assessed its performance in a simulated real-world scenario based on a logistics company case study. Our experimental findings indicated that the approach learned effective container assignment patterns, outperforming its competitors by 20.48% to 55.32% in terms of total transportation costs and by 7.51% to 20.54% in terms of train capacity utilization. Moreover, it achieved results that were within 2.7% for the cost and 0.72% for the capacity of the optimal solution provided by an Integer Linear Programming solver in an offline setting.",1
"Multi-agent reinforcement learning (MARL) becomes more challenging in the presence of more agents, as the capacity of the joint state and action spaces grows exponentially in the number of agents. To address such a challenge of scale, we identify a class of cooperative MARL problems with permutation invariance, and formulate it as a mean-field Markov decision processes (MDP). To exploit the permutation invariance therein, we propose the mean-field proximal policy optimization (MF-PPO) algorithm, at the core of which is a permutation-invariant actor-critic neural architecture. We prove that MF-PPO attains the globally optimal policy at a sublinear rate of convergence. Moreover, its sample complexity is independent of the number of agents. We validate the theoretical advantages of MF-PPO with numerical experiments in the multi-agent particle environment (MPE). In particular, we show that the inductive bias introduced by the permutation-invariant neural architecture enables MF-PPO to outperform existing competitors with a smaller number of model parameters, which is the key to its generalization performance.",0
"As the number of agents increases, Multi-agent reinforcement learning (MARL) becomes more difficult due to the exponential growth of the joint state and action spaces. To overcome this scaling challenge, we have identified a group of cooperative MARL issues with permutation invariance and have transformed them into mean-field Markov decision processes (MDP). We have introduced the mean-field proximal policy optimization (MF-PPO) algorithm to take advantage of the permutation invariance. The algorithm is based on a permutation-invariant actor-critic neural architecture, and we have proven that it achieves the globally optimal policy with a sublinear rate of convergence. Additionally, its sample complexity is independent of the number of agents. We have conducted numerical experiments in the multi-agent particle environment (MPE) to validate the theoretical benefits of MF-PPO. Specifically, we have demonstrated that the inductive bias introduced by the permutation-invariant neural architecture allows MF-PPO to outperform existing competitors with fewer model parameters, which is critical for its generalization performance.",1
"We apply reinforcement learning to video compressive sensing to adapt the compression ratio. Specifically, video snapshot compressive imaging (SCI), which captures high-speed video using a low-speed camera is considered in this work, in which multiple (B) video frames can be reconstructed from a snapshot measurement. One research gap in previous studies is how to adapt B in the video SCI system for different scenes. In this paper, we fill this gap utilizing reinforcement learning (RL). An RL model, as well as various convolutional neural networks for reconstruction, are learned to achieve adaptive sensing of video SCI systems. Furthermore, the performance of an object detection network using directly the video SCI measurements without reconstruction is also used to perform RL-based adaptive video compressive sensing. Our proposed adaptive SCI method can thus be implemented in low cost and real time. Our work takes the technology one step further towards real applications of video SCI.",0
"This study explores the use of reinforcement learning to adjust the compression ratio in video compressive sensing. Specifically, the focus is on video snapshot compressive imaging (SCI), which captures high-speed video using a low-speed camera and can reconstruct multiple video frames from a snapshot measurement. Previous research has not addressed how to adapt the number of frames (B) in the video SCI system for different scenes. To fill this gap, the authors employ RL and various convolutional neural networks for reconstruction to achieve adaptive sensing in video SCI systems. Additionally, an object detection network is used to perform RL-based adaptive video compressive sensing without reconstruction. The proposed adaptive SCI method can be implemented in real time at low cost, bringing the technology closer to practical applications.",1
"When working to understand usage of a data format, examples of the data format are often more representative than the format's specification. For example, two different applications might use very different JSON representations, or two PDF-writing applications might make use of very different areas of the PDF specification to realize the same rendered content. The complexity arising from these distinct origins can lead to large, difficult-to-understand attack surfaces, presenting a security concern when considering both exfiltration and data schizophrenia. Grammar inference can aid in describing the practical language generator behind examples of a data format. However, most grammar inference research focuses on natural language, not data formats, and fails to support crucial features such as type recursion. We propose a novel set of mechanisms for grammar inference, RL-GRIT, and apply them to understanding de facto data formats. After reviewing existing grammar inference solutions, it was determined that a new, more flexible scaffold could be found in Reinforcement Learning (RL). Within this work, we lay out the many algorithmic changes required to adapt RL from its traditional, sequential-time environment to the highly interdependent environment of parsing. The result is an algorithm which can demonstrably learn recursive control structures in simple data formats, and can extract meaningful structure from fragments of the PDF format. Whereas prior work in grammar inference focused on either regular languages or constituency parsing, we show that RL can be used to surpass the expressiveness of both classes, and offers a clear path to learning context-sensitive languages. The proposed algorithm can serve as a building block for understanding the ecosystems of de facto data formats.",0
"To comprehend the usage of a data format, examples of the format are often more representative than its specification. For instance, different applications may use distinct JSON representations or different areas of the PDF specification to produce the same content. This complexity can lead to security issues related to data schizophrenia and exfiltration. To address this, grammar inference can describe the practical language generator behind the format's examples. However, current grammar inference research does not support critical features like type recursion and mostly focuses on natural language. Therefore, we introduce RL-GRIT, a novel set of mechanisms for grammar inference, which adapts reinforcement learning to understand de facto data formats. This algorithm can learn recursive control structures in simple data formats and extract meaningful structure from PDF fragments, surpassing the expressiveness of both regular languages and constituency parsing. By using RL, we offer a way to understand the ecosystems of de facto data formats.",1
"Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2 - 20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",0
"The main hindrance to the widespread use of RL in real-world scenarios is safety. The uncertain nature of learning new tasks in such environments necessitates extensive exploration, which is at odds with the imperative of ensuring safety by limiting exploration. To tackle this issue, we propose the Recovery RL algorithm, which addresses this tradeoff in two ways: firstly, by utilizing offline data to learn about zones that violate constraints before policy learning, and secondly, by separating the goals of improving task performance and constraint satisfaction across two policies. Specifically, the task policy solely focuses on optimizing the task reward, while the recovery policy guides the agent to safety when constraint violation is likely. To evaluate the effectiveness of Recovery RL, we tested it on six simulation domains, encompassing two contact-rich manipulation tasks, an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. In comparison to five prior safe RL methods that jointly optimize for task performance and safety via constrained optimization or reward shaping, we found that Recovery RL outperforms the next best prior method across all domains. Our results indicate that Recovery RL is 2 to 20 times more efficient in simulation domains and three times more efficient in physical experiments in trading off constraint violations and task successes. For more details, including videos and supplementary material, please refer to https://tinyurl.com/rl-recovery.",1
"Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",0
"The idea of Offline Reinforcement Learning is to develop effective policies using pre-existing, static datasets without the need for exploration. The challenge with current Q-learning and actor-critic based off-policy RL algorithms is that they struggle to bootstrap from out-of-distribution (OOD) actions or states. Our hypothesis is that the missing piece of the puzzle is a proper handling of uncertainty in the offline setting. To address this, we propose the Uncertainty Weighted Actor-Critic (UWAC) algorithm, which identifies OOD state-action pairs and adjusts their contribution to the training objectives accordingly. We use a practical and efficient dropout-based uncertainty estimation approach for implementation, which adds minimal overhead to existing RL algorithms. Our empirical results show that UWAC significantly improves model stability during training, and outperforms existing offline RL methods on various competitive tasks. Moreover, UWAC achieves notable performance gains over the state-of-the-art baseline on datasets featuring sparse demonstrations collected from human experts.",1
"Low-complexity models such as linear function representation play a pivotal role in enabling sample-efficient reinforcement learning (RL). The current paper pertains to a scenario with value-based linear representation, which postulates the linear realizability of the optimal Q-function (also called the ""linear $Q^{\star}$ problem""). While linear realizability alone does not allow for sample-efficient solutions in general, the presence of a large sub-optimality gap is a potential game changer, depending on the sampling mechanism in use. Informally, sample efficiency is achievable with a large sub-optimality gap when a generative model is available but is unfortunately infeasible when we turn to standard online RL settings.   In this paper, we make progress towards understanding this linear $Q^{\star}$ problem by investigating a new sampling protocol, which draws samples in an online/exploratory fashion but allows one to backtrack and revisit previous states in a controlled and infrequent manner. This protocol is more flexible than the standard online RL setting, while being practically relevant and far more restrictive than the generative model. We develop an algorithm tailored to this setting, achieving a sample complexity that scales polynomially with the feature dimension, the horizon, and the inverse sub-optimality gap, but not the size of the state/action space. Our findings underscore the fundamental interplay between sampling protocols and low-complexity structural representation in RL.",0
"Efficient reinforcement learning heavily relies on simple models such as linear function representation. The focus of this study is on value-based linear representation, which assumes that the optimal Q-function can be represented linearly (known as ""linear $Q^{\star}$ problem""). Although linear representation alone does not lead to efficient solutions in general, a considerable sub-optimality gap can potentially change the game depending on the sampling mechanism. Achieving sample efficiency with a large sub-optimality gap is feasible with a generative model, but infeasible in standard online RL settings. In this paper, we present a new sampling protocol that allows for online/exploratory sampling while enabling controlled and infrequent backtracking to previous states. This protocol is more adaptable than standard online RL but less flexible than a generative model. We have developed an algorithm tailored to this setting, scaling polynomially with the feature dimension, horizon, and inverse sub-optimality gap, but not the size of the state/action space. Our study highlights the crucial relationship between sampling protocols and low-complexity structural representation in RL.",1
"Reinforcement Learning(RL) with sparse rewards is a major challenge. We propose \emph{Hindsight Trust Region Policy Optimization}(HTRPO), a new RL algorithm that extends the highly successful TRPO algorithm with \emph{hindsight} to tackle the challenge of sparse rewards. Hindsight refers to the algorithm's ability to learn from information across goals, including ones not intended for the current task. HTRPO leverages two main ideas. It introduces QKL, a quadratic approximation to the KL divergence constraint on the trust region, leading to reduced variance in KL divergence estimation and improved stability in policy update. It also presents Hindsight Goal Filtering(HGF) to select conductive hindsight goals. In experiments, we evaluate HTRPO in various sparse reward tasks, including simple benchmarks, image-based Atari games, and simulated robot control. Ablation studies indicate that QKL and HGF contribute greatly to learning stability and high performance. Comparison results show that in all tasks, HTRPO consistently outperforms both TRPO and HPG, a state-of-the-art algorithm for RL with sparse rewards.",0
"Dealing with sparse rewards in Reinforcement Learning(RL) poses a significant challenge. To address this issue, we introduce a new RL algorithm called \emph{Hindsight Trust Region Policy Optimization}(HTRPO). This algorithm extends the highly successful TRPO algorithm using \emph{hindsight} to overcome the challenge of sparse rewards. The term hindsight refers to the algorithm's ability to learn from information across goals, even those not intended for the current task. HTRPO leverages two key concepts: QKL, which is a quadratic approximation to the KL divergence constraint on the trust region, leading to reduced variance in KL divergence estimation and improved stability in policy update; and Hindsight Goal Filtering(HGF), which selects favorable hindsight goals. We evaluate HTRPO in various sparse reward tasks, including simple benchmarks, image-based Atari games, and simulated robot control. Our ablation studies show that QKL and HGF significantly contribute to learning stability and high performance. Comparison results demonstrate that HTRPO consistently outperforms both TRPO and HPG, which is a state-of-the-art algorithm for RL with sparse rewards in all tasks.",1
"Our team is proposing to run a full-scale energy demand response experiment in an office building. Although this is an exciting endeavor which will provide value to the community, collecting training data for the reinforcement learning agent is costly and will be limited. In this work, we apply a meta-learning architecture to warm start the experiment with simulated tasks, to increase sample efficiency. We present results that demonstrate a similar a step up in complexity still corresponds with better learning.",0
"Our team aims to conduct a comprehensive energy demand response experiment in an office building. While this endeavor is thrilling and beneficial to the community, gathering training data for the reinforcement learning agent is expensive and restricted. Therefore, we employ a meta-learning framework to initiate the experiment with simulated tasks, enhancing sample efficiency. Our findings reveal that as the complexity level rises, the learning process improves.",1
"One principled approach for provably efficient exploration is incorporating the upper confidence bound (UCB) into the value function as a bonus. However, UCB is specified to deal with linear and tabular settings and is incompatible with Deep Reinforcement Learning (DRL). In this paper, we propose a principled exploration method for DRL through Optimistic Bootstrapping and Backward Induction (OB2I). OB2I constructs a general-purpose UCB-bonus through non-parametric bootstrap in DRL. The UCB-bonus estimates the epistemic uncertainty of state-action pairs for optimistic exploration. We build theoretical connections between the proposed UCB-bonus and the LSVI-UCB in a linear setting. We propagate future uncertainty in a time-consistent manner through episodic backward update, which exploits the theoretical advantage and empirically improves the sample-efficiency. Our experiments in the MNIST maze and Atari suite suggest that OB2I outperforms several state-of-the-art exploration approaches.",0
"The upper confidence bound (UCB) is a technique that can be used to ensure efficient exploration, but it is only useful for linear and tabular settings and cannot be applied to Deep Reinforcement Learning (DRL). Thus, we present a new exploration method for DRL called Optimistic Bootstrapping and Backward Induction (OB2I). OB2I uses non-parametric bootstrap to create a general-purpose UCB-bonus that estimates the uncertainty of state-action pairs for optimistic exploration. We establish a connection between the proposed UCB-bonus and LSVI-UCB in a linear setting and use episodic backward update to propagate future uncertainty in a time-consistent manner. Our experiments in the MNIST maze and Atari suite show that OB2I outperforms other exploration methods.",1
"In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at https://github.com/astooke/rlpyt/tree/master/rlpyt/ul.",0
"To address the limitations of reward-based learning in deep reinforcement learning (RL) from images, we suggest separating the process of representation learning from policy learning. Our approach involves introducing a new unsupervised learning (UL) task called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations with a short time difference using image augmentations and a contrastive loss. Our online RL experiments demonstrate that exclusively training the encoder with ATC matches or outperforms end-to-end RL in most environments. Furthermore, we benchmarked various leading UL algorithms by pre-training encoders on expert demonstrations, and found that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from several environments and demonstrate their generalization to different downstream RL tasks. Our experiments span a range of visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is accessible at https://github.com/astooke/rlpyt/tree/master/rlpyt/ul. Additionally, we investigate the components of ATC and introduce new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation.",1
"Ensemble learning is a very prevalent method employed in machine learning. The relative success of ensemble methods is attributed to their ability to tackle a wide range of instances and complex problems that require different low-level approaches. However, ensemble methods are relatively less popular in reinforcement learning owing to the high sample complexity and computational expense involved in obtaining a diverse ensemble. We present a novel training and model selection framework for model-free reinforcement algorithms that use ensembles of policies obtained from a single training run. These policies are diverse in nature and are learned through directed perturbation of the model parameters at regular intervals. We show that learning and selecting an adequately diverse set of policies is required for a good ensemble while extreme diversity can prove detrimental to overall performance. Selection of an adequately diverse set of policies is done through our novel policy selection framework. We evaluate our approach on challenging discrete and continuous control tasks and also discuss various ensembling strategies. Our framework is substantially sample efficient, computationally inexpensive and is seen to outperform state-of-the-art (SOTA) scores in Atari 2600 and Mujoco.",0
"Ensemble learning is a widely used technique in machine learning that is known for its ability to handle complex problems that require different low-level approaches. However, it is not commonly used in reinforcement learning due to the high sample complexity and computational expense involved in creating a diverse ensemble. To address this issue, we propose a unique training and model selection framework for model-free reinforcement algorithms that employ ensembles of policies obtained from a single training run. Our approach involves creating a diverse set of policies through directed perturbation of the model parameters at regular intervals. We demonstrate that learning and selecting a well-diversified set of policies is crucial for achieving good ensemble performance, but excessively diverse policies can have a negative impact. To select an appropriate set of policies, we introduce a novel policy selection framework. We evaluate our framework on challenging discrete and continuous control tasks and compare it to other ensembling strategies. Our approach is highly sample efficient, computationally inexpensive, and outperforms state-of-the-art scores in Atari 2600 and Mujoco.",1
"We introduce biased gradient oracles to capture a setting where the function measurements have an estimation error that can be controlled through a batch size parameter. Our proposed oracles are appealing in several practical contexts, for instance, risk measure estimation from a batch of independent and identically distributed (i.i.d.) samples, or simulation optimization, where the function measurements are `biased' due to computational constraints. In either case, increasing the batch size reduces the estimation error. We highlight the applicability of our biased gradient oracles in a risk-sensitive reinforcement learning setting. In the stochastic non-convex optimization context, we analyze a variant of the randomized stochastic gradient (RSG) algorithm with a biased gradient oracle. We quantify the convergence rate of this algorithm by deriving non-asymptotic bounds on its performance. Next, in the stochastic convex optimization setting, we derive non-asymptotic bounds for the last iterate of a stochastic gradient descent (SGD) algorithm with a biased gradient oracle.",0
"Our proposal is to use biased gradient oracles to account for function measurement errors that can be controlled through a batch size parameter. These oracles have practical applications, such as estimating risk measures from a batch of i.i.d. samples or optimizing simulations with biased function measurements due to computational limitations. Increasing batch size can reduce estimation error in either case. We demonstrate the usefulness of our biased gradient oracles in a risk-sensitive reinforcement learning setting. We analyze a variant of the RSG algorithm with a biased gradient oracle in the stochastic non-convex optimization context and derive non-asymptotic bounds on its performance. Similarly, in the stochastic convex optimization setting, we derive non-asymptotic bounds for the last iterate of an SGD algorithm with a biased gradient oracle.",1
"Growing concerns regarding the operational usage of AI models in the real-world has caused a surge of interest in explaining AI models' decisions to humans. Reinforcement Learning is not an exception in this regard. In this work, we propose a method for offering local explanations on risk in reinforcement learning. Our method only requires a log of previous interactions between the agent and the environment to create a state-transition model. It is designed to work on RL environments with either continuous or discrete state and action spaces. After creating the model, actions of any agent can be explained in terms of the features most influential in increasing or decreasing risk or any other desirable objective function in the locality of the agent. Through experiments, we demonstrate the effectiveness of the proposed method in providing such explanations.",0
"Increasing concerns about the practical application of AI models in the real world has resulted in a heightened interest in explaining the decisions made by AI models to humans. This trend also applies to Reinforcement Learning. Our study introduces a technique that offers local explanations on risk in reinforcement learning. By utilizing a state-transition model created from a log of past interactions between the agent and the environment, our method is capable of functioning in RL environments with either continuous or discrete state and action spaces. With this model, we can explain the actions of any agent in terms of the most influential features for increasing or decreasing risk or any other desired objective function in the agent's vicinity. Experimental results demonstrate the effectiveness of our proposed method in providing such explanations.",1
"Explaining the behavior of black box machine learning models through human interpretable rules is an important research area. Recent work has focused on explaining model behavior locally i.e. for specific predictions as well as globally across the fields of vision, natural language, reinforcement learning and data science. We present a novel model-agnostic approach that derives rules to globally explain the behavior of classification models trained on numerical and/or categorical data. Our approach builds on top of existing local model explanation methods to extract conditions important for explaining model behavior for specific instances followed by an evolutionary algorithm that optimizes an information theory based fitness function to construct rules that explain global model behavior. We show how our approach outperforms existing approaches on a variety of datasets. Further, we introduce a parameter to evaluate the quality of interpretation under the scenario of distributional shift. This parameter evaluates how well the interpretation can predict model behavior for previously unseen data distributions. We show how existing approaches for interpreting models globally lack distributional robustness. Finally, we show how the quality of the interpretation can be improved under the scenario of distributional shift by adding out of distribution samples to the dataset used to learn the interpretation and thereby, increase robustness. All of the datasets used in our paper are open and publicly available. Our approach has been deployed in a leading digital marketing suite of products.",0
"The field of research that focuses on explaining the behavior of black box machine learning models using human interpretable rules is crucial. Recent studies have concentrated on explaining model behavior locally, for specific predictions, as well as globally across various domains including vision, natural language, reinforcement learning, and data science. In this paper, we propose a new model-agnostic approach that generates rules to explain the behavior of classification models trained on numerical and/or categorical data globally. Our approach utilizes existing local model explanation techniques to extract important conditions for explaining behavior for specific instances. It then employs an evolutionary algorithm that optimizes a fitness function based on information theory to construct rules that explain the global behavior of the model. Our approach has been demonstrated to outperform existing methods on several datasets. Furthermore, we introduce a parameter to assess the quality of interpretation under distributional shift, which evaluates how well the interpretation can predict model behavior for previously unseen data distributions. Existing approaches for interpreting models globally lack distributional robustness, and we demonstrate how the quality of interpretation can be enhanced under the scenario of distributional shift by adding out-of-distribution samples to the dataset used for learning. All of the datasets used in our study are publicly available. Our approach has been implemented in a leading suite of digital marketing products.",1
"Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.",0
"Despite significant research into distributional reinforcement learning (DRL) in recent years, there are still two unanswered questions regarding the validity of the learned quantile function and how to effectively use distributional information. This paper aims to provide new insights into these two areas and encourage further investigation. To ensure the accuracy of the quantile estimates, we propose a non-decreasing quantile function network (NDQFN), while a general exploration framework named distributional prediction error (DPE) is developed to efficiently utilize the entire distribution of the quantile function. Our approach is both theoretically sound and practically effective, as demonstrated by its superior performance compared to competitors on Atari 2600 games, particularly in challenging environments.",1
"The recent growth of web video sharing platforms has increased the demand for systems that can efficiently browse, retrieve and summarize video content. Query-aware multi-video summarization is a promising technique that caters to this demand. In this work, we introduce a novel Query-Aware Hierarchical Pointer Network for Multi-Video Summarization, termed DeepQAMVS, that jointly optimizes multiple criteria: (1) conciseness, (2) representativeness of important query-relevant events and (3) chronological soundness. We design a hierarchical attention model that factorizes over three distributions, each collecting evidence from a different modality, followed by a pointer network that selects frames to include in the summary. DeepQAMVS is trained with reinforcement learning, incorporating rewards that capture representativeness, diversity, query-adaptability and temporal coherence. We achieve state-of-the-art results on the MVS1K dataset, with inference time scaling linearly with the number of input video frames.",0
"The demand for efficient systems that can browse, retrieve and summarize video content has increased due to the recent growth of web video sharing platforms. Query-aware multi-video summarization is a technique that addresses this demand. This study introduces DeepQAMVS, a novel Query-Aware Hierarchical Pointer Network for Multi-Video Summarization. DeepQAMVS optimizes multiple criteria, including conciseness, representativeness of important query-relevant events and chronological soundness. A hierarchical attention model factors over three distributions, while a pointer network selects frames for the summary. DeepQAMVS is trained using reinforcement learning, with rewards capturing representativeness, diversity, query-adaptability and temporal coherence. State-of-the-art results are achieved on the MVS1K dataset and inference time scales linearly with the number of input video frames.",1
"This paper studies model-based bandit and reinforcement learning (RL) with nonlinear function approximations. We propose to study convergence to approximate local maxima because we show that global convergence is statistically intractable even for one-layer neural net bandit with a deterministic reward. For both nonlinear bandit and RL, the paper presents a model-based algorithm, Virtual Ascent with Online Model Learner (ViOlin), which provably converges to a local maximum with sample complexity that only depends on the sequential Rademacher complexity of the model class. Our results imply novel global or local regret bounds on several concrete settings such as linear bandit with finite or sparse model class, and two-layer neural net bandit. A key algorithmic insight is that optimism may lead to over-exploration even for two-layer neural net model class. On the other hand, for convergence to local maxima, it suffices to maximize the virtual return if the model can also reasonably predict the size of the gradient and Hessian of the real return.",0
"The objective of this research is to examine nonlinear function approximations in model-based bandit and reinforcement learning (RL). Rather than focusing on global convergence, which we have determined to be statistically impractical, we are proposing a study of convergence to approximate local maxima. Our paper offers a model-based algorithm for both nonlinear bandit and RL, which we have named Virtual Ascent with Online Model Learner (ViOlin). Our algorithm has been proven to converge to a local maximum, with a sample complexity that is dependent on the sequential Rademacher complexity of the model class. Our findings have resulted in new global and local regret bounds for various scenarios, including linear bandit with finite or sparse model class, and two-layer neural net bandit. Our research has also shown that optimism can result in excessive exploration for two-layer neural net model class. However, for convergence to local maxima, maximizing the virtual return is sufficient, provided the model is capable of reasonably predicting the gradient and Hessian of the real return.",1
"In this paper, we address the anomaly detection problem where the objective is to find the anomalous processes among a given set of processes. To this end, the decision-making agent probes a subset of processes at every time instant and obtains a potentially erroneous estimate of the binary variable which indicates whether or not the corresponding process is anomalous. The agent continues to probe the processes until it obtains a sufficient number of measurements to reliably identify the anomalous processes. In this context, we develop a sequential selection algorithm that decides which processes to be probed at every instant to detect the anomalies with an accuracy exceeding a desired value while minimizing the delay in making the decision and the total number of measurements taken. Our algorithm is based on active inference which is a general framework to make sequential decisions in order to maximize the notion of free energy. We define the free energy using the objectives of the selection policy and implement the active inference framework using a deep neural network approximation. Using numerical experiments, we compare our algorithm with the state-of-the-art method based on deep actor-critic reinforcement learning and demonstrate the superior performance of our algorithm.",0
"The focus of this article is on identifying anomalous processes within a given set of processes. The decision-making agent collects data by probing a subset of processes at each time instant, resulting in a potentially erroneous estimate of whether the corresponding process is anomalous or not. The agent continues to collect data until a sufficient number of measurements are obtained to accurately identify the anomalous processes. We introduce a sequential selection algorithm that minimizes the delay in decision-making and the total number of measurements taken while achieving a desired level of accuracy. Our algorithm is based on the active inference framework, which involves making sequential decisions to maximize the notion of free energy. We define the free energy using the objectives of the selection policy and implement the active inference framework using a deep neural network approximation. We conduct numerical experiments to compare our algorithm with a state-of-the-art method that uses deep actor-critic reinforcement learning, and demonstrate that our algorithm outperforms this method.",1
"We address the problem of sequentially selecting and observing processes from a given set to find the anomalies among them. The decision-maker observes one process at a time and obtains a noisy binary indicator of whether or not the corresponding process is anomalous. In this setting, we develop an anomaly detection algorithm that chooses the process to be observed at a given time instant, decides when to stop taking observations, and makes a decision regarding the anomalous processes. The objective of the detection algorithm is to arrive at a decision with an accuracy exceeding a desired value while minimizing the delay in decision making. Our algorithm relies on a Markov decision process defined using the marginal probability of each process being normal or anomalous, conditioned on the observations. We implement the detection algorithm using the deep actor-critic reinforcement learning framework. Unlike prior work on this topic that has exponential complexity in the number of processes, our algorithm has computational and memory requirements that are both polynomial in the number of processes. We demonstrate the efficacy of our algorithm using numerical experiments by comparing it with the state-of-the-art methods.",0
"The problem we tackle involves selecting and observing processes sequentially from a given set to identify anomalies. The decision-maker receives a binary signal that is noisy and indicates whether a process is anomalous or not, one at a time. We have developed an algorithm for anomaly detection that selects the process to observe, decides when to stop observing, and determines which processes are anomalous. Our objective is to achieve high accuracy in decision making while minimizing the delay. Our algorithm uses a Markov decision process that is based on the probability of each process being normal or anomalous, conditioned on the observations. We have implemented the algorithm using the deep actor-critic reinforcement learning framework. Unlike previous approaches that have exponential complexity, our algorithm has polynomial computational and memory requirements. Numerical experiments demonstrate the effectiveness of our algorithm compared to state-of-the-art methods.",1
"Deep reinforcement learning (DRL) has recently been used to perform efficient resource allocation in wireless communications. In this paper, the vulnerabilities of such DRL agents to adversarial attacks is studied. In particular, we consider multiple DRL agents that perform both dynamic channel access and power control in wireless interference channels. For these victim DRL agents, we design a jammer, which is also a DRL agent. We propose an adversarial jamming attack scheme that utilizes a listening phase and significantly degrades the users' sum rate. Subsequently, we develop an ensemble policy defense strategy against such a jamming attacker by reloading models (saved during retraining) that have minimum transition correlation.",0
"Recent use of deep reinforcement learning (DRL) has shown its potential for efficient resource allocation in wireless communications. However, this paper investigates the susceptibility of DRL agents to adversarial attacks. Specifically, we examine the vulnerabilities of multiple DRL agents that perform dynamic channel access and power control in wireless interference channels. To test their vulnerability, we design an adversarial jamming attack scheme using a DRL agent as the jammer, which significantly degrades the users' sum rate. To defend against such jamming attacks, we propose an ensemble policy defense strategy that utilizes models saved during retraining with minimum transition correlation.",1
"In past years model-agnostic meta-learning (MAML) has been one of the most promising approaches in meta-learning. It can be applied to different kinds of problems, e.g., reinforcement learning, but also shows good results on few-shot learning tasks. Besides their tremendous success in these tasks, it has still not been fully revealed yet, why it works so well. Recent work proposes that MAML rather reuses features than rapidly learns. In this paper, we want to inspire a deeper understanding of this question by analyzing MAML's representation. We apply representation similarity analysis (RSA), a well-established method in neuroscience, to the few-shot learning instantiation of MAML. Although some part of our analysis supports their general results that feature reuse is predominant, we also reveal arguments against their conclusion. The similarity-increase of layers closer to the input layers arises from the learning task itself and not from the model. In addition, the representations after inner gradient steps make a broader change to the representation than the changes during meta-training.",0
"Model-agnostic meta-learning (MAML) has been a successful approach in meta-learning for various problems, including reinforcement learning and few-shot learning tasks. However, the reason for its effectiveness is not fully understood. Recent research suggests that MAML reuses features instead of rapid learning. In this study, the authors aim to gain a deeper understanding of MAML's representation by using representation similarity analysis (RSA) on its few-shot learning instantiation. Although some findings support the idea of feature reuse, others argue against it. The increase in similarity of layers closer to input layers is due to the learning task, not the model. Additionally, the changes during inner gradient steps have a broader impact on representation than the changes during meta-training.",1
"We study offline reinforcement learning (RL), which aims to learn an optimal policy based on a dataset collected a priori. Due to the lack of further interactions with the environment, offline RL suffers from the insufficient coverage of the dataset, which eludes most existing theoretical analysis. In this paper, we propose a pessimistic variant of the value iteration algorithm (PEVI), which incorporates an uncertainty quantifier as the penalty function. Such a penalty function simply flips the sign of the bonus function for promoting exploration in online RL, which makes it easily implementable and compatible with general function approximators.   Without assuming the sufficient coverage of the dataset, we establish a data-dependent upper bound on the suboptimality of PEVI for general Markov decision processes (MDPs). When specialized to linear MDPs, it matches the information-theoretic lower bound up to multiplicative factors of the dimension and horizon. In other words, pessimism is not only provably efficient but also minimax optimal. In particular, given the dataset, the learned policy serves as the ""best effort"" among all policies, as no other policies can do better. Our theoretical analysis identifies the critical role of pessimism in eliminating a notion of spurious correlation, which emerges from the ""irrelevant"" trajectories that are less covered by the dataset and not informative for the optimal policy.",0
"The focus of our research is on offline reinforcement learning (RL), which involves using a pre-collected dataset to develop an optimal policy. Due to the lack of further interaction with the environment, the dataset used in offline RL is often insufficient, which poses a challenge for existing theoretical analyses. To address this challenge, we introduce a pessimistic approach to the value iteration algorithm, referred to as PEVI. PEVI incorporates an uncertainty quantifier as a penalty function, which is the opposite of the bonus function used to promote exploration in online RL. This approach is easy to implement and compatible with general function approximators. Our research establishes a data-dependent upper bound on the suboptimality of PEVI for general Markov decision processes (MDPs) without assuming sufficient coverage of the dataset. When applied to linear MDPs, the bound matches the information-theoretic lower bound, up to multiplicative factors of the dimension and horizon. This means that pessimism is not only provably efficient but also minimax optimal. Our theoretical analysis identifies the key role of pessimism in eliminating spurious correlations that arise from less-covered and irrelevant trajectories, which are not informative for developing an optimal policy. As a result, the policy learned through PEVI serves as the ""best effort"" among all policies, as no other policies can perform better given the dataset.",1
"Neural architecture search (NAS) has recently been addressed from various directions, including discrete, sampling-based methods and efficient differentiable approaches. While the former are notoriously expensive, the latter suffer from imposing strong constraints on the search space. Architecture optimization from a learned embedding space for example through graph neural network based variational autoencoders builds a middle ground and leverages advantages from both sides. Such approaches have recently shown good performance on several benchmarks. Yet, their stability and predictive power heavily depends on their capacity to reconstruct networks from the embedding space. In this paper, we propose a two-sided variational graph autoencoder, which allows to smoothly encode and accurately reconstruct neural architectures from various search spaces. We evaluate the proposed approach on neural architectures defined by the ENAS approach, the NAS-Bench-101 and the NAS-Bench-201 search space and show that our smooth embedding space allows to directly extrapolate the performance prediction to architectures outside the seen domain (e.g. with more operations). Thus, it facilitates to predict good network architectures even without expensive Bayesian optimization or reinforcement learning.",0
"Various approaches have been taken to address neural architecture search (NAS), such as discrete, sampling-based methods and efficient differentiable approaches. However, the former can be costly, while the latter impose strong constraints on the search space. A middle ground can be achieved through architecture optimization from a learned embedding space, which utilizes graph neural network based variational autoencoders and has recently shown promising results on several benchmarks. However, the predictive power of these approaches relies heavily on their ability to reconstruct networks from the embedding space. This paper proposes a two-sided variational graph autoencoder that can smoothly encode and accurately reconstruct neural architectures from different search spaces. The proposed approach is evaluated on various search spaces and is shown to facilitate the prediction of good network architectures outside the seen domain without the need for expensive Bayesian optimization or reinforcement learning.",1
"Offline reinforcement learning (RL) has increasingly become the focus of the artificial intelligent research due to its wide real-world applications where the collection of data may be difficult, time-consuming, or costly. In this paper, we first propose a two-fold taxonomy for existing offline RL algorithms from the perspective of exploration and exploitation tendency. Secondly, we derive the explicit expression of the upper bound of extrapolation error and explore the correlation between the performance of different types of algorithms and the distribution of actions under states. Specifically, we relax the strict assumption on the sufficiently large amount of state-action tuples. Accordingly, we provably explain why batch constrained Q-learning (BCQ) performs better than other existing techniques. Thirdly, after identifying the weakness of BCQ on dataset of low mean episode returns, we propose a modified variant based on top return selection mechanism, which is proved to be able to gain state-of-the-art performance on various datasets. Lastly, we create a benchmark platform on the Atari domain, entitled RL easy go (RLEG), at an estimated cost of more than 0.3 million dollars. We make it open-source for fair and comprehensive competitions between offline RL algorithms with complete datasets and checkpoints being provided.",0
"Due to its practical applications in scenarios where data collection is difficult, time-consuming or expensive, offline reinforcement learning (RL) has become an increasingly important focus in artificial intelligence research. This paper proposes a two-fold taxonomy for existing offline RL algorithms that considers their exploration and exploitation tendencies. Additionally, it explores the relationship between algorithm performance and action distribution under states, relaxing the assumption of a sufficiently large number of state-action tuples and offering a provable explanation for why batch constrained Q-learning (BCQ) outperforms other techniques. A modified variant of BCQ is introduced, addressing its weakness on datasets with low mean episode returns, and is shown to achieve state-of-the-art performance on various datasets. Finally, a benchmark platform called RL easy go (RLEG) is introduced for comprehensive and fair competitions between offline RL algorithms. This platform is open-source and has an estimated cost of over 0.3 million dollars. Checkpoints and complete datasets are provided for all participants.",1
"Scaling issues are mundane yet irritating for practitioners of reinforcement learning. Error scales vary across domains, tasks, and stages of learning; sometimes by many orders of magnitude. This can be detrimental to learning speed and stability, create interference between learning tasks, and necessitate substantial tuning. We revisit this topic for agents based on temporal-difference learning, sketch out some desiderata and investigate scenarios where simple fixes fall short. The mechanism we propose requires neither tuning, clipping, nor adaptation. We validate its effectiveness and robustness on the suite of Atari games. Our scaling method turns out to be particularly helpful at mitigating interference, when training a shared neural network on multiple targets that differ in reward scale or discounting.",0
"Practitioners of reinforcement learning often encounter scaling issues that can be frustrating. The scales of errors can vary greatly depending on the domain, task, and stage of learning, sometimes differing by orders of magnitude. These discrepancies can negatively affect the speed and stability of learning, cause interference between tasks, and require extensive tuning. In this study, we focus on agents that use temporal-difference learning and explore potential solutions to these problems. We outline some desirable features and examine cases where simple solutions are insufficient. Our proposed mechanism does not require tuning, clipping, or adaptation, and we confirm its effectiveness and resilience using the Atari game suite. Our scaling method is especially useful in reducing interference when training a shared neural network on multiple targets with varying reward scales or discounting.",1
"Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the performance of these developments not by changing the objective, but by regularising the value-function estimator. Constraining the Lipschitz constant of a single layer using spectral normalisation is sufficient to elevate the performance of a Categorical-DQN agent to that of a more elaborated \rainbow{} agent on the challenging Atari domain. We conduct ablation studies to disentangle the various effects normalisation has on the learning dynamics and show that is sufficient to modulate the parameter updates to recover most of the performance of spectral normalisation. These findings hint towards the need to also focus on the neural component and its learning dynamics to tackle the peculiarities of Deep Reinforcement Learning.",0
"Recent advancements in deep reinforcement learning have primarily focused on refining the training objective from an RL-centric perspective. However, our approach diverges from this view as we demonstrate the ability to recover the performance of these developments without altering the objective. Instead, we achieve this by regularising the value-function estimator through the constraint of the Lipschitz constant of a single layer using spectral normalisation. Our research shows that this is sufficient to enhance the performance of a Categorical-DQN agent to that of a more complex \rainbow{} agent on the challenging Atari domain. Furthermore, we conduct ablation studies to separate the various effects of normalisation on the learning dynamics and conclude that modulating the parameter updates through spectral normalisation can recover most of the performance. These findings suggest that the neural component and its learning dynamics should also be a focus to address the unique characteristics of Deep Reinforcement Learning.",1
"We design a multi-purpose environment for autonomous UAVs offering different communication services in a variety of application contexts (e.g., wireless mobile connectivity services, edge computing, data gathering). We develop the environment, based on OpenAI Gym framework, in order to simulate different characteristics of real operational environments and we adopt the Reinforcement Learning to generate policies that maximize some desired performance.The quality of the resulting policies are compared with a simple baseline to evaluate the system and derive guidelines to adopt this technique in different use cases. The main contribution of this paper is a flexible and extensible OpenAI Gym environment, which allows to generate, evaluate, and compare policies for autonomous multi-drone systems in multi-service applications. This environment allows for comparative evaluation and benchmarking of different approaches in a variety of application contexts.",0
"Our team has created a versatile environment for independent unmanned aerial vehicles that provides various communication services for a range of applications. These services include wireless mobile connectivity, edge computing, and data collection. To simulate real-world scenarios, we developed the environment using the OpenAI Gym framework and implemented Reinforcement Learning to generate policies that optimize performance. We compared the quality of the policies to a basic baseline to assess the system and provide recommendations for implementing the technique in various use cases. This paper's primary contribution is an adaptable and expandable OpenAI Gym environment that facilitates policy creation, evaluation, and comparison for multi-drone systems in multi-service applications. This environment allows for the comparative evaluation and benchmarking of various approaches in different application contexts.",1
"Reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. In this paper, we classify RL into direct and indirect RL according to how they seek the optimal policy of the Markov decision process problem. The former solves the optimal policy by directly maximizing an objective function using gradient descent methods, in which the objective function is usually the expectation of accumulative future rewards. The latter indirectly finds the optimal policy by solving the Bellman equation, which is the sufficient and necessary condition from Bellman's principle of optimality. We study policy gradient forms of direct and indirect RL and show that both of them can derive the actor-critic architecture and can be unified into a policy gradient with the approximate value function and the stationary state distribution, revealing the equivalence of direct and indirect RL. We employ a Gridworld task to verify the influence of different forms of policy gradient, suggesting their differences and relationships experimentally. Finally, we classify current mainstream RL algorithms using the direct and indirect taxonomy, together with other ones including value-based and policy-based, model-based and model-free.",0
"A variety of complex decision-making and control tasks have been successfully tackled using reinforcement learning (RL) algorithms. This study proposes a classification of RL into two categories, namely direct and indirect RL, based on their methods for seeking the optimal policy of the Markov decision process problem. Direct RL employs gradient descent methods to maximize an objective function, which is typically the expectation of future rewards. Indirect RL, on the other hand, solves the Bellman equation, which is a necessary and sufficient condition from Bellman's principle of optimality, to determine the optimal policy. The paper explores the policy gradient forms of both direct and indirect RL and demonstrates that they can be unified using the actor-critic architecture, approximate value function, and stationary state distribution. A Gridworld task is used to experimentally examine the impact of different policy gradient forms. The study also categorizes mainstream RL algorithms, including value-based and policy-based, model-based and model-free, using the direct and indirect taxonomy.",1
"At present, attention mechanism has been widely applied to the fields of deep learning models. Structural models that based on attention mechanism can not only record the relationships between features position, but also can measure the importance of different features based on their weights. By establishing dynamically weighted parameters for choosing relevant and irrelevant features, the key information can be strengthened, and the irrelevant information can be weakened. Therefore, the efficiency of deep learning algorithms can be significantly elevated and improved. Although transformers have been performed very well in many fields including reinforcement learning, there are still many problems and applications can be solved and made with transformers within this area. MARL (known as Multi-Agent Reinforcement Learning) can be recognized as a set of independent agents trying to adapt and learn through their way to reach the goal. In order to emphasize the relationship between each MDP decision in a certain time period, we applied the hierarchical coding method and validated the effectiveness of this method. This paper proposed a hierarchical transformers MADDPG based on RNN which we call it Hierarchical RNNs-Based Transformers MADDPG(HRTMADDPG). It consists of a lower level encoder based on RNNs that encodes multiple step sizes in each time sequence, and it also consists of an upper sequence level encoder based on transformer for learning the correlations between multiple sequences so that we can capture the causal relationship between sub-time sequences and make HRTMADDPG more efficient.",0
"Currently, attention mechanism is widely utilized in deep learning models. Structural models that employ attention mechanism can not only record feature relationships but also measure feature importance based on their weights. By establishing dynamically weighted parameters that select relevant and irrelevant features, key information can be strengthened while irrelevant information is weakened. This significantly improves the efficiency of deep learning algorithms. Despite the success of transformers in multiple fields, including reinforcement learning, there are still many problems and applications that can be addressed with transformers in this area. Multi-Agent Reinforcement Learning (MARL) is a set of independent agents that adapt and learn to reach goals. To emphasize the relationship between each decision in a certain time period, a hierarchical coding method was applied and validated in this paper. The proposed Hierarchical RNNs-Based Transformers MADDPG (HRTMADDPG) utilizes a lower level encoder based on RNNs to encode multiple step sizes and an upper sequence level encoder based on transformer to learn correlations between multiple sequences. This allows for the capture of causal relationships between sub-time sequences and enhances the efficiency of HRTMADDPG.",1
"Decision and control are core functionalities of high-level automated vehicles. Current mainstream methods, such as functionality decomposition and end-to-end reinforcement learning (RL), either suffer high time complexity or poor interpretability and adaptability on real-world autonomous driving tasks. In this paper, we present an interpretable and computationally efficient framework called integrated decision and control (IDC) for automated vehicles, which decomposes the driving task into static path planning and dynamic optimal tracking that are structured hierarchically. First, the static path planning generates several candidate paths only considering static traffic elements. Then, the dynamic optimal tracking is designed to track the optimal path while considering the dynamic obstacles. To that end, we formulate a constrained optimal control problem (OCP) for each candidate path, optimize them separately and follow the one with the best tracking performance. To unload the heavy online computation, we propose a model-based reinforcement learning (RL) algorithm that can be served as an approximate constrained OCP solver. Specifically, the OCPs for all paths are considered together to construct a single complete RL problem and then solved offline in the form of value and policy networks, for real-time online path selecting and tracking respectively. We verify our framework in both simulations and the real world. Results show that compared with baseline methods IDC has an order of magnitude higher online computing efficiency, as well as better driving performance including traffic efficiency and safety. In addition, it yields great interpretability and adaptability among different driving tasks. The effectiveness of the proposed method is also demonstrated in real road tests with complicated traffic conditions.",0
"High-level automated vehicles require decision-making and control capabilities as their core functionalities. However, mainstream methods such as functionality decomposition and end-to-end reinforcement learning (RL) either suffer from high time complexity or lack interpretability and adaptability in real-world autonomous driving tasks. This paper presents an efficient and interpretable framework called integrated decision and control (IDC) for automated vehicles. IDC decomposes the driving task into two hierarchical parts: static path planning and dynamic optimal tracking. First, the static path planning generates several candidate paths considering only static traffic elements. Next, the dynamic optimal tracking is designed to track the optimal path while considering dynamic obstacles. To achieve this, a constrained optimal control problem (OCP) is formulated for each candidate path, optimized separately, and the one with the best tracking performance is followed. To reduce online computation, the proposed method uses a model-based reinforcement learning (RL) algorithm as an approximate constrained OCP solver. The OCPs for all paths are considered together to construct a single complete RL problem, which is solved offline in the form of value and policy networks for real-time online path selection and tracking, respectively. The framework is verified in both simulations and the real world, demonstrating an order of magnitude higher online computing efficiency and better driving performance, including traffic efficiency and safety compared to baseline methods. Additionally, the proposed method yields great interpretability and adaptability among different driving tasks.",1
"Efficient and robust policy transfer remains a key challenge for reinforcement learning to become viable for real-wold robotics. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning tasks. However, this seems far from how skill transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation and exploration lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can ""Adapt-to-Learn"", that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. We show that the presented method learns to seamlessly combine learning from adaptation and exploration and leads to a robust policy transfer algorithm with significantly reduced sample complexity in transferring skills between related tasks.",0
"The challenge of ensuring efficient and robust policy transfer is fundamental for reinforcement learning to be practical for real-world robotics. While various techniques such as warm initialization, imitation, and interacting with a large set of randomized agents have been used to address this issue, they do not reflect how skill transfer occurs in the biological world. In contrast, humans and animals can quickly adapt learned behaviors to similar tasks and acquire new skills in novel situations. This leads us to consider whether a combination of adaptation and exploration can enhance policy transfer between domains. Our proposed approach, called ""Adapt-to-Learn,"" offers a systematic mechanism for adapting the source policy to enable learning for a target task with significant differences and uncertainties. By seamlessly integrating adaptation and exploration, our method achieves a robust policy transfer algorithm with significantly reduced sample complexity for related tasks.",1
"Designing efficient exploration is central to Reinforcement Learning due to the fundamental problem posed by the exploration-exploitation dilemma. Bayesian exploration strategies like Thompson Sampling resolve this trade-off in a principled way by modeling and updating the distribution of the parameters of the the action-value function, the outcome model of the environment. However, this technique becomes infeasible for complex environments due to the difficulty of representing and updating probability distributions over parameters of outcome models of corresponding complexity. Moreover, the approximation techniques introduced to mitigate this issue typically result in poor exploration-exploitation trade-offs, as observed in the case of deep neural network models with approximate posterior methods that have been shown to underperform in the deep bandit scenario.   In this paper we introduce Sample Average Uncertainty (SAU), a simple and efficient uncertainty measure for contextual bandits. While Bayesian approaches like Thompson Sampling estimate outcomes uncertainty indirectly by first quantifying the variability over the parameters of the outcome model, SAU is a frequentist approach that directly estimates the uncertainty of the outcomes based on the value predictions. Importantly, we show theoretically that the uncertainty measure estimated by SAU asymptotically matches the uncertainty provided by Thompson Sampling, as well as its regret bounds. Because of its simplicity SAU can be seamlessly applied to deep contextual bandits as a very scalable drop-in replacement for epsilon-greedy exploration. Finally, we empirically confirm our theory by showing that SAU-based exploration outperforms current state-of-the-art deep Bayesian bandit methods on several real-world datasets at modest computation cost.",0
"Reinforcement Learning requires efficient exploration design to address the exploration-exploitation dilemma. Bayesian exploration strategies, such as Thompson Sampling, tackle this issue by modeling and updating the distribution of the parameters of the action-value function and the environment's outcome model. However, for complex environments, updating probability distributions becomes challenging, and approximation techniques often result in poor exploration-exploitation trade-offs. To address this, the paper introduces Sample Average Uncertainty (SAU), a simple and efficient uncertainty measure for contextual bandits that directly estimates outcome uncertainty based on value predictions. SAU is a frequentist approach that asymptotically matches the uncertainty provided by Thompson Sampling and its regret bounds. SAU can be applied to deep contextual bandits as a scalable drop-in replacement for epsilon-greedy exploration. Empirical results show that SAU-based exploration outperforms deep Bayesian bandit methods on real-world datasets at modest computation cost.",1
"Classical value iteration approaches are not applicable to environments with continuous states and actions. For such environments, the states and actions are usually discretized, which leads to an exponential increase in computational complexity. In this paper, we propose continuous fitted value iteration (cFVI). This algorithm enables dynamic programming for continuous states and actions with a known dynamics model. Leveraging the continuous-time formulation, the optimal policy can be derived for non-linear control-affine dynamics. This closed-form solution enables the efficient extension of value iteration to continuous environments. We show in non-linear control experiments that the dynamic programming solution obtains the same quantitative performance as deep reinforcement learning methods in simulation but excels when transferred to the physical system. The policy obtained by cFVI is more robust to changes in the dynamics despite using only a deterministic model and without explicitly incorporating robustness in the optimization. Videos of the physical system are available at \url{https://sites.google.com/view/value-iteration}.",0
"Classical value iteration methods are not suitable for environments where states and actions are continuous. To overcome this issue, states and actions are often discretized, leading to a significant increase in computational complexity. In this study, we introduce continuous fitted value iteration (cFVI), a novel algorithm that enables dynamic programming for continuous states and actions with a known dynamics model. By utilizing the continuous-time formulation, cFVI allows for the derivation of optimal policies for non-linear control-affine dynamics in a closed-form solution, enabling the efficient extension of value iteration to continuous environments. Our experiments demonstrate that cFVI achieves comparable performance to deep reinforcement learning methods in simulations while outperforming them when transferred to physical systems. Interestingly, cFVI's policy is more resilient to changes in the dynamics, even though it utilizes only a deterministic model and does not explicitly incorporate robustness in the optimization. Videos of the physical system can be found at \url{https://sites.google.com/view/value-iteration}.",1
"Continual learning tackles the setting of learning different tasks sequentially. Despite the lots of previous solutions, most of them still suffer significant forgetting or expensive memory cost. In this work, targeted at these problems, we first study the continual learning process through the lens of information theory and observe that forgetting of a model stems from the loss of \emph{information gain} on its parameters from the previous tasks when learning a new task. From this viewpoint, we then propose a novel continual learning approach called Bit-Level Information Preserving (BLIP) that preserves the information gain on model parameters through updating the parameters at the bit level, which can be conveniently implemented with parameter quantization. More specifically, BLIP first trains a neural network with weight quantization on the new incoming task and then estimates information gain on each parameter provided by the task data to determine the bits to be frozen to prevent forgetting. We conduct extensive experiments ranging from classification tasks to reinforcement learning tasks, and the results show that our method produces better or on par results comparing to previous state-of-the-arts. Indeed, BLIP achieves close to zero forgetting while only requiring constant memory overheads throughout continual learning.",0
"The process of continual learning involves learning different tasks in a sequential manner. Despite many previous solutions, most still suffer from significant forgetting or high memory costs. To address these issues, this study examines the continual learning process through the lens of information theory. It is observed that forgetting occurs when a model loses information gain on its parameters from previous tasks when learning a new one. To overcome this, a novel approach called Bit-Level Information Preserving (BLIP) is proposed. BLIP updates parameters at the bit level, which can be easily implemented with parameter quantization. The method first trains a neural network with weight quantization on the new task, then estimates the information gain on each parameter provided by the task data to determine the bits that should be frozen to prevent forgetting. The study includes extensive experiments ranging from classification to reinforcement learning tasks. The results demonstrate that BLIP produces better or comparable results to previous state-of-the-art methods, achieving close to zero forgetting while only requiring a constant memory overhead throughout continual learning.",1
"Transfer learning (TL) is a promising way to improve the sample efficiency of reinforcement learning. However, how to efficiently transfer knowledge across tasks with different state-action spaces is investigated at an early stage. Most previous studies only addressed the inconsistency across different state spaces by learning a common feature space, without considering that similar actions in different action spaces of related tasks share similar semantics. In this paper, we propose a method to learning action embeddings by leveraging this idea, and a framework that learns both state embeddings and action embeddings to transfer policy across tasks with different state and action spaces. Our experimental results on various tasks show that the proposed method can not only learn informative action embeddings but accelerate policy learning.",0
"The utilization of transfer learning (TL) has shown potential in enhancing the efficiency of reinforcement learning by reducing the number of samples needed. However, the exploration of effective techniques for transferring knowledge across tasks with state-action spaces that differ remains in its early stages. Prior research mainly focused on tackling the inconsistency across various state spaces by learning a shared feature space, without considering that related tasks' similar actions in different action spaces hold comparable meanings. Our study proposes a method that relies on this concept to learn action embeddings, along with a framework that learns both state embeddings and action embeddings to transfer policies across tasks with varying state and action spaces. Our experimental outcomes demonstrate that the suggested method can not only acquire informative action embeddings but also expedite policy learning.",1
"The performance of deep reinforcement learning methods prone to degenerate when applied to environments with non-stationary dynamics. In this paper, we utilize the latent context recurrent encoders motivated by recent Meta-RL materials, and propose the Latent Context-based Soft Actor Critic (LC-SAC) method to address aforementioned issues. By minimizing the contrastive prediction loss function, the learned context variables capture the information of the environment dynamics and the recent behavior of the agent. Then combined with the soft policy iteration paradigm, the LC-SAC method alternates between soft policy evaluation and soft policy improvement until it converges to the optimal policy. Experimental results show that the performance of LC-SAC is significantly better than the SAC algorithm on the MetaWorld ML1 tasks whose dynamics changes drasticly among different episodes, and is comparable to SAC on the continuous control benchmark task MuJoCo whose dynamics changes slowly or doesn't change between different episodes. In addition, we also conduct relevant experiments to determine the impact of different hyperparameter settings on the performance of the LC-SAC algorithm and give the reasonable suggestions of hyperparameter setting.",0
"When applied to non-stationary environments, deep reinforcement learning methods can become degenerate. In this study, we developed the Latent Context-based Soft Actor Critic (LC-SAC) method, using latent context recurrent encoders inspired by recent Meta-RL research, to address these issues. By minimizing the contrastive prediction loss function, the learned context variables capture information about the environment dynamics and the agent's recent behavior. Combined with the soft policy iteration paradigm, the LC-SAC method alternates between soft policy evaluation and improvement until it reaches the optimal policy. Our experiments demonstrate that LC-SAC significantly outperforms SAC on MetaWorld ML1 tasks with rapidly changing dynamics, while performing comparably on the MuJoCo continuous control benchmark task, which has slower or constant dynamics between episodes. We also conducted experiments to determine the impact of different hyperparameter settings on LC-SAC performance and provide reasonable suggestions for these settings.",1
"Reinforcement learning lies at the intersection of several challenges. Many applications of interest involve extremely large state spaces, requiring function approximation to enable tractable computation. In addition, the learner has only a single stream of experience with which to evaluate a large number of possible courses of action, necessitating algorithms which can learn off-policy. However, the combination of off-policy learning with function approximation leads to divergence of temporal difference methods. Recent work into gradient-based temporal difference methods has promised a path to stability, but at the cost of expensive hyperparameter tuning. In parallel, progress in online learning has provided parameter-free methods that achieve minimax optimal guarantees up to logarithmic terms, but their application in reinforcement learning has yet to be explored. In this work, we combine these two lines of attack, deriving parameter-free, gradient-based temporal difference algorithms. Our algorithms run in linear time and achieve high-probability convergence guarantees matching those of GTD2 up to $\log$ factors. Our experiments demonstrate that our methods maintain high prediction performance relative to fully-tuned baselines, with no tuning whatsoever.",0
"Reinforcement learning presents various challenges, including the need for function approximation to handle large state spaces and the use of off-policy algorithms with a single stream of experience. However, combining off-policy learning with function approximation can lead to temporal difference methods divergence. Gradient-based temporal difference methods have shown promise, but require expensive hyperparameter tuning. On the other hand, parameter-free methods based on online learning have not been explored in reinforcement learning. This study combines these two approaches to develop parameter-free, gradient-based temporal difference algorithms that achieve high-probability convergence guarantees up to logarithmic terms. The proposed algorithms run in linear time and maintain high prediction performance without the need for tuning.",1
"In recent years, quantitative investment methods combined with artificial intelligence have attracted more and more attention from investors and researchers. Existing related methods based on the supervised learning are not very suitable for learning problems with long-term goals and delayed rewards in real futures trading. In this paper, therefore, we model the price prediction problem as a Markov decision process (MDP), and optimize it by reinforcement learning with expert trajectory. In the proposed method, we employ more than 100 short-term alpha factors instead of price, volume and several technical factors in used existing methods to describe the states of MDP. Furthermore, unlike DQN (deep Q-learning) and BC (behavior cloning) in related methods, we introduce expert experience in training stage, and consider both the expert-environment interaction and the agent-environment interaction to design the temporal difference error so that the agents are more adaptable for inevitable noise in financial data. Experimental results evaluated on share price index futures in China, including IF (CSI 300) and IC (CSI 500), show that the advantages of the proposed method compared with three typical technical analysis and two deep leaning based methods.",0
"Investors and researchers are increasingly interested in combining quantitative investment methods with artificial intelligence. However, existing supervised learning methods are not ideal for real futures trading learning problems with long-term goals and delayed rewards. In this study, we propose a Markov decision process (MDP) model for the price prediction problem and optimize it using reinforcement learning with expert trajectory. Instead of relying on price, volume, and technical factors, we use over 100 short-term alpha factors to describe MDP states. Our method incorporates expert experience during the training stage and considers both the expert-environment and agent-environment interactions to design the temporal difference error. This makes our agents more adaptable to financial data noise. The proposed method outperformed three typical technical analysis and two deep learning-based methods in experiments conducted on share price index futures in China, including IF (CSI 300) and IC (CSI 500).",1
"While Adversarial Imitation Learning (AIL) algorithms have recently led to state-of-the-art results on various imitation learning benchmarks, it is unclear as to what impact various design decisions have on performance. To this end, we present here an organizing, modular framework called Reinforcement-learning-based Adversarial Imitation Learning (RAIL) that encompasses and generalizes a popular subclass of existing AIL approaches. Using the view espoused by RAIL, we create two new IfO (Imitation from Observation) algorithms, which we term SAIfO: SAC-based Adversarial Imitation from Observation and SILEM (Skeletal Feature Compensation for Imitation Learning with Embodiment Mismatch). We go into greater depth about SILEM in a separate technical report. In this paper, we focus on SAIfO, evaluating it on a suite of locomotion tasks from OpenAI Gym, and showing that it outperforms contemporaneous RAIL algorithms that perform IfO.",0
"The use of Adversarial Imitation Learning (AIL) algorithms has resulted in impressive outcomes on various imitation learning benchmarks, but the effects of different design decisions on performance are uncertain. To address this, we introduce a comprehensive and modular framework called Reinforcement-learning-based Adversarial Imitation Learning (RAIL), which encompasses and extends a popular subset of AIL approaches. By adopting the perspective of RAIL, we develop two novel IfO (Imitation from Observation) algorithms: SAC-based Adversarial Imitation from Observation (SAIfO) and Skeletal Feature Compensation for Imitation Learning with Embodiment Mismatch (SILEM). In a separate technical report, we delve deeper into SILEM. In this study, we concentrate on SAIfO and evaluate its effectiveness on a range of locomotion tasks from OpenAI Gym. Our findings demonstrate that SAIfO surpasses RAIL algorithms that conduct IfO and are currently contemporaneous.",1
"Cooperative multi-agent tasks require agents to deduce their own contributions with shared global rewards, known as the challenge of credit assignment. General methods for policy based multi-agent reinforcement learning to solve the challenge introduce differentiate value functions or advantage functions for individual agents. In multi-agent system, polices of different agents need to be evaluated jointly. In order to update polices synchronously, such value functions or advantage functions also need synchronous evaluation. However, in current methods, value functions or advantage functions use counter-factual joint actions which are evaluated asynchronously, thus suffer from natural estimation bias. In this work, we propose the approximatively synchronous advantage estimation. We first derive the marginal advantage function, an expansion from single-agent advantage function to multi-agent system. Further more, we introduce a policy approximation for synchronous advantage estimation, and break down the multi-agent policy optimization problem into multiple sub-problems of single-agent policy optimization. Our method is compared with baseline algorithms on StarCraft multi-agent challenges, and shows the best performance on most of the tasks.",0
"When multiple agents work together on a task, it can be difficult to determine how much credit each agent deserves for the shared success. This is known as the challenge of credit assignment. To solve this problem, policy-based multi-agent reinforcement learning methods often use value functions or advantage functions for individual agents. However, in order to update these functions together, they need to be evaluated synchronously. Current methods use counter-factual joint actions for evaluation, which can lead to bias. Our proposed method is called approximatively synchronous advantage estimation. We expand the single-agent advantage function to a multi-agent system and introduce a policy approximation for synchronous evaluation. We break down the multi-agent policy optimization problem into sub-problems of single-agent policy optimization. Our method outperforms baseline algorithms in StarCraft multi-agent challenges.",1
"We propose FACtored Multi-Agent Centralised policy gradients (FACMAC), a new method for cooperative multi-agent reinforcement learning in both discrete and continuous action spaces. Like MADDPG, a popular multi-agent actor-critic method, our approach uses deep deterministic policy gradients to learn policies. However, FACMAC learns a centralised but factored critic, which combines per-agent utilities into the joint action-value function via a non-linear monotonic function, as in QMIX, a popular multi-agent Q-learning algorithm. However, unlike QMIX, there are no inherent constraints on factoring the critic. We thus also employ a nonmonotonic factorisation and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics. In addition, FACMAC uses a centralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent's action space separately as in MADDPG. This allows for more coordinated policy changes and fully reaps the benefits of a centralised critic. We evaluate FACMAC on variants of the multi-agent particle environments, a novel multi-agent MuJoCo benchmark, and a challenging set of StarCraft II micromanagement tasks. Empirical results demonstrate FACMAC's superior performance over MADDPG and other baselines on all three domains.",0
"A new cooperative multi-agent reinforcement learning method for both discrete and continuous action spaces is proposed, named FACtored Multi-Agent Centralised policy gradients (FACMAC). Similar to the MADDPG multi-agent actor-critic method, FACMAC employs deep deterministic policy gradients to learn policies. However, FACMAC uses a centralised but factored critic to combine per-agent utilities into the joint action-value function through a non-linear monotonic function, similar to the QMIX multi-agent Q-learning algorithm. Unlike QMIX, no inherent constraints exist on factoring the critic, and a nonmonotonic factorisation is employed to increase the representational capacity and solve tasks that cannot be solved with monolithic or monotonically factored critics. Moreover, FACMAC uses a centralised policy gradient estimator to optimise the entire joint action space, enabling more coordinated policy changes and fully exploiting the advantages of a centralised critic. FACMAC is evaluated on multi-agent particle environments, a multi-agent MuJoCo benchmark, and a set of challenging StarCraft II micromanagement tasks, demonstrating superior performance over MADDPG and other baselines in all three domains.",1
"One of the fundamental challenges in reinforcement learning (RL) is the one of data efficiency: modern algorithms require a very large number of training samples, especially compared to humans, for solving environments with high-dimensional observations. The severity of this problem is increased when the reward signal is sparse. In this work, we propose learning a state representation in a self-supervised manner for reward prediction. The reward predictor learns to estimate either a raw or a smoothed version of the true reward signal in environment with a single, terminating, goal state. We augment the training of out-of-the-box RL agents by shaping the reward using our reward predictor during policy learning. Using our representation for preprocessing high-dimensional observations, as well as using the predictor for reward shaping, is shown to significantly enhance Actor Critic using Kronecker-factored Trust Region and Proximal Policy Optimization in single-goal environments with visual inputs.",0
"Reinforcement learning (RL) faces a major hurdle in terms of data efficiency, as modern algorithms require a large number of training samples to solve high-dimensional observation environments, particularly when the reward signal is scarce. To address this issue, this study proposes a self-supervised learning approach to develop a state representation for reward prediction. The reward predictor estimates a raw or smoothed version of the true reward signal in an environment with a single, terminating goal state. The reward shaping technique is used to enhance the training of RL agents during policy learning. The proposed method is demonstrated to significantly improve Actor Critic using Kronecker-factored Trust Region and Proximal Policy Optimization in single-goal environments with visual inputs by preprocessing high-dimensional observations using our representation and using the predictor for reward shaping.",1
"In many deep reinforcement learning settings, when an agent takes an action, it repeats the same action a predefined number of times without observing the states until the next action-decision point. This technique of action repetition has several merits in training the agent, but the data between action-decision points (i.e., intermediate frames) are, in effect, discarded. Since the amount of training data is inversely proportional to the interval of action repeats, they can have a negative impact on the sample efficiency of training. In this paper, we propose a simple but effective approach to alleviate to this problem by introducing the concept of pseudo-actions. The key idea of our method is making the transition between action-decision points usable as training data by considering pseudo-actions. Pseudo-actions for continuous control tasks are obtained as the average of the action sequence straddling an action-decision point. For discrete control tasks, pseudo-actions are computed from learned action embeddings. This method can be combined with any model-free reinforcement learning algorithm that involves the learning of Q-functions. We demonstrate the effectiveness of our approach on both continuous and discrete control tasks in OpenAI Gym.",0
"When an agent takes an action in deep reinforcement learning, it often repeats that action a fixed number of times without observing the states until the next action-decision point. Although this technique has benefits in training the agent, it discards intermediate frames, which can cause a decrease in sample efficiency due to the inverse proportionality between the amount of training data and the interval of action repeats. To address this issue, we propose a straightforward yet efficient approach that introduces the concept of pseudo-actions to make the transition between action-decision points a usable source of training data. We obtain pseudo-actions for continuous control tasks by averaging the action sequence surrounding an action-decision point, while for discrete control tasks, we calculate them from learned action embeddings. Our method can complement any model-free reinforcement learning algorithm that involves Q-function learning. We validate our approach on both continuous and discrete control tasks in OpenAI Gym.",1
"It has recently been shown that reinforcement learning can be used to train generators capable of producing high-quality game levels, with quality defined in terms of some user-specified heuristic. To ensure that these generators' output is sufficiently diverse (that is, not amounting to the reproduction of a single optimal level configuration), the generation process is constrained such that the initial seed results in some variance in the generator's output. However, this results in a loss of control over the generated content for the human user. We propose to train generators capable of producing controllably diverse output, by making them ""goal-aware."" To this end, we add conditional inputs representing how close a generator is to some heuristic, and also modify the reward mechanism to incorporate that value. Testing on multiple domains, we show that the resulting level generators are capable of exploring the space of possible levels in a targeted, controllable manner, producing levels of comparable quality as their goal-unaware counterparts, that are diverse along designer-specified dimensions.",0
"Recent research has demonstrated that reinforcement learning can be used to train generators that can create high-quality game levels, as defined by specific user heuristics. In order to prevent these generators from merely replicating a single optimal level configuration, the generation process is restricted to include some variability in the initial seed. However, this causes a loss of control for the human user over the generated content. Our proposal is to train generators that can produce controllably diverse output by making them ""goal-aware."" We achieve this by adding conditional inputs that represent the proximity of the generator to a heuristic and modifying the reward mechanism to include that value. By testing this approach in multiple domains, we demonstrate that these level generators can explore the space of possible levels in a targeted and controllable manner, producing levels of similar quality to their goal-unaware counterparts that are diverse in ways specified by designers.",1
"We present new policy mirror descent (PMD) methods for solving reinforcement learning (RL) problems with either strongly convex or general convex regularizers. By exploring the structural properties of these overall highly nonconvex problems we show that the PMD methods exhibit fast linear rate of convergence to the global optimality. We develop stochastic counterparts of these methods, and establish an ${\cal O}(1/\epsilon)$ (resp., ${\cal O}(1/\epsilon^2)$) sampling complexity for solving these RL problems with strongly (resp., general) convex regularizers using different sampling schemes, where $\epsilon$ denote the target accuracy. We further show that the complexity for computing the gradients of these regularizers, if necessary, can be bounded by ${\cal O}\{(\log_\gamma \epsilon) [(1-\gamma)L/\mu]^{1/2}\log (1/\epsilon)\}$ (resp., ${\cal O} \{(\log_\gamma \epsilon ) (L/\epsilon)^{1/2}\}$)for problems with strongly (resp., general) convex regularizers. Here $\gamma$ denotes the discounting factor. To the best of our knowledge, these complexity bounds, along with our algorithmic developments, appear to be new in both optimization and RL literature. The introduction of these convex regularizers also greatly expands the flexibility and applicability of RL models.",0
"New policy mirror descent (PMD) methods are introduced for solving reinforcement learning (RL) problems with either strongly convex or general convex regularizers. The PMD methods exhibit a fast linear rate of convergence to the global optimality by exploring the structural properties of these overall highly nonconvex problems. Stochastic counterparts of these methods are developed and different sampling schemes are used to establish an ${\cal O}(1/\epsilon)$ (resp., ${\cal O}(1/\epsilon^2)$) sampling complexity for solving these RL problems with strongly (resp., general) convex regularizers, where $\epsilon$ denotes the target accuracy. The complexity for computing the gradients of these regularizers, if necessary, can be bounded by ${\cal O}\{(\log_\gamma \epsilon) [(1-\gamma)L/\mu]^{1/2}\log (1/\epsilon)\}$ (resp., ${\cal O} \{(\log_\gamma \epsilon ) (L/\epsilon)^{1/2}\}$) for problems with strongly (resp., general) convex regularizers. Here, $\gamma$ denotes the discounting factor. These complexity bounds, along with the algorithmic developments, appear to be new in both optimization and RL literature. The introduction of these convex regularizers also greatly expands the flexibility and applicability of RL models.",1
"Policy gradient methods are an attractive approach to multi-agent reinforcement learning problems due to their convergence properties and robustness in partially observable scenarios. However, there is a significant performance gap between state-of-the-art policy gradient and value-based methods on the popular StarCraft Multi-Agent Challenge (SMAC) benchmark. In this paper, we introduce semi-on-policy (SOP) training as an effective and computationally efficient way to address the sample inefficiency of on-policy policy gradient methods. We enhance two state-of-the-art policy gradient algorithms with SOP training, demonstrating significant performance improvements. Furthermore, we show that our methods perform as well or better than state-of-the-art value-based methods on a variety of SMAC tasks.",0
"Multi-agent reinforcement learning problems can be tackled through policy gradient methods which are preferred due to their convergence properties and robustness in partially observable scenarios. However, the StarCraft Multi-Agent Challenge (SMAC) benchmark highlights a considerable performance gap between state-of-the-art policy gradient and value-based methods. As a solution to the sample inefficiency of on-policy policy gradient methods, we propose semi-on-policy (SOP) training, which is both effective and computationally efficient. Our study enhances two state-of-the-art policy gradient algorithms with SOP training, resulting in significant performance improvements. We also demonstrate that our methods perform equally well or better than state-of-the-art value-based methods on various SMAC tasks.",1
"For NP-hard combinatorial optimization problems, it is usually difficult to find high-quality solutions in polynomial time. The design of either an exact algorithm or an approximate algorithm for these problems often requires significantly specialized knowledge. Recently, deep learning methods provide new directions to solve such problems. In this paper, an end-to-end deep reinforcement learning framework is proposed to solve this type of combinatorial optimization problems. This framework can be applied to different problems with only slight changes of input (for example, for a traveling salesman problem (TSP), the input is the two-dimensional coordinates of nodes; while for a capacity-constrained vehicle routing problem (CVRP), the input is simply changed to three-dimensional vectors including the two-dimensional coordinates and the customer demands of nodes), masks and decoder context vectors. The proposed framework is aiming to improve the models in literacy in terms of the neural network model and the training algorithm. The solution quality of TSP and the CVRP up to 100 nodes are significantly improved via our framework. Specifically, the average optimality gap is reduced from 4.53\% (reported best \cite{R22}) to 3.67\% for TSP with 100 nodes and from 7.34\% (reported best \cite{R22}) to 6.68\% for CVRP with 100 nodes when using the greedy decoding strategy. Furthermore, our framework uses about 1/3$\sim$3/4 training samples compared with other existing learning methods while achieving better results. The results performed on randomly generated instances and the benchmark instances from TSPLIB and CVRPLIB confirm that our framework has a linear running time on the problem size (number of nodes) during the testing phase, and has a good generalization performance from random instance training to real-world instance testing.",0
"It is often challenging to find high-quality solutions in polynomial time for NP-hard combinatorial optimization problems, and designing an exact or approximate algorithm for these problems typically requires specialized knowledge. However, deep learning methods have recently provided new avenues to address these issues. This paper proposes an end-to-end deep reinforcement learning framework to solve combinatorial optimization problems, which can be adapted to different problems with minimal changes to the input, masks, and decoder context vectors. The framework aims to improve the neural network model and training algorithm, resulting in significantly improved solution quality for problems such as the traveling salesman problem and the capacity-constrained vehicle routing problem. Our framework achieves better results while using fewer training samples compared to existing methods and has a linear running time on the problem size during testing, as demonstrated by the results obtained from randomly generated and benchmark instances.",1
"Deep Reinforcement Learning (DRL) has shown outstanding performance on inducing effective action policies that maximize expected long-term return on many complex tasks. Much of DRL work has been focused on sequences of events with discrete time steps and ignores the irregular time intervals between consecutive events. Given that in many real-world domains, data often consists of temporal sequences with irregular time intervals, and it is important to consider the time intervals between temporal events to capture latent progressive patterns of states. In this work, we present a general Time-Aware RL framework: Time-aware Q-Networks (TQN), which takes into account physical time intervals within a deep RL framework. TQN deals with time irregularity from two aspects: 1) elapsed time in the past and an expected next observation time for time-aware state approximation, and 2) action time window for the future for time-aware discounting of rewards. Experimental results show that by capturing the underlying structures in the sequences with time irregularities from both aspects, TQNs significantly outperform DQN in four types of contexts with irregular time intervals. More specifically, our results show that in classic RL tasks such as CartPole and MountainCar and Atari benchmark with randomly segmented time intervals, time-aware discounting alone is more important while in the real-world tasks such as nuclear reactor operation and septic patient treatment with intrinsic time intervals, both time-aware state and time-aware discounting are crucial. Moreover, to improve the agent's learning capacity, we explored three boosting methods: Double networks, Dueling networks, and Prioritized Experience Replay, and our results show that for the two real-world tasks, combining all three boosting methods with TQN is especially effective.",0
"Deep Reinforcement Learning (DRL) has demonstrated excellent performance in developing effective action policies that maximize expected long-term return on complex tasks. However, much of the research has focused on sequences of events with discrete time steps, disregarding the irregular time intervals between them. This approach is inadequate for real-world domains where data often consists of temporal sequences with irregular time intervals, and it is crucial to consider the time intervals between temporal events to capture latent progressive patterns of states. To address this issue, we propose a general Time-Aware RL framework: Time-aware Q-Networks (TQN), which incorporates physical time intervals within a deep RL framework. TQN deals with time irregularity from two perspectives: elapsed time in the past and an expected next observation time for time-aware state approximation, and action time window for the future for time-aware discounting of rewards. Our experimental results demonstrate that TQNs significantly outperform DQNs in four types of contexts with irregular time intervals by capturing the underlying structures in the sequences with time irregularities from both aspects. Specifically, our findings suggest that in classic RL tasks such as CartPole and MountainCar and Atari benchmark with randomly segmented time intervals, time-aware discounting alone is more critical, while in real-world tasks such as nuclear reactor operation and septic patient treatment with intrinsic time intervals, both time-aware state and time-aware discounting are essential. Furthermore, to enhance the agent's learning capacity, we investigated three boosting methods: Double networks, Dueling networks, and Prioritized Experience Replay, and our results indicate that combining all three boosting methods with TQN is particularly effective for the two real-world tasks.",1
"We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.",0
"Our focus is on the problem of offline meta-reinforcement learning (OMRL), which allows reinforcement learning (RL) algorithms to adapt to new tasks quickly without interacting with the environment. This approach is practical for real-world applications, but remains partially understood due to two key challenges. First, bootstrapping errors often lead to value function divergence in offline RL. Second, meta-RL requires robust task inference learned jointly with control policy. To address these issues, we propose a general approach of enforcing behavior regularization on learned policy and using a deterministic context encoder for efficient task inference. Our novel negative-power distance metric on the bounded context embedding space is detached from the Bellman backup for gradient propagation. Our analysis shows that simple design choices can improve over recent approaches involving meta-RL and distance metric learning. Our end-to-end OMRL algorithm is model-free, computationally efficient, and outperforms prior algorithms on several meta-RL benchmarks.",1
"This paper investigates reinforcement learning with constraints, which are indispensable in safety-critical environments. To drive the constraint violation monotonically decrease, we take the constraints as Lyapunov functions and impose new linear constraints on the policy parameters' updating dynamics. As a result, the original safety set can be forward-invariant. However, because the new guaranteed-feasible constraints are imposed on the updating dynamics instead of the original policy parameters, classic optimization algorithms are no longer applicable. To address this, we propose to learn a generic deep neural network (DNN)-based optimizer to optimize the objective while satisfying the linear constraints. The constraint-satisfaction is achieved via projection onto a polytope formulated by multiple linear inequality constraints, which can be solved analytically with our newly designed metric. To the best of our knowledge, this is the \textit{first} DNN-based optimizer for constrained optimization with the forward invariance guarantee. We show that our optimizer trains a policy to decrease the constraint violation and maximize the cumulative reward monotonically. Results on numerical constrained optimization and obstacle-avoidance navigation validate the theoretical findings.",0
"The purpose of this study is to explore reinforcement learning in the context of safety-critical environments where constraints are crucial. To ensure that constraint violation consistently decreases, we adopt the Lyapunov function approach and enforce linear constraints on the dynamics of policy parameter updates. This approach allows us to establish a forward-invariant safety set. However, the introduction of additional constraints on the updating dynamics renders traditional optimization algorithms unsuitable. To overcome this, we propose a deep neural network (DNN)-based optimizer that can learn to optimize objectives while adhering to the linear constraints. We achieve constraint satisfaction by projecting onto a polytope formed by multiple linear inequality constraints, which we can solve analytically using our newly developed metric. Our approach is the first DNN-based optimizer for constrained optimization with a forward invariance guarantee. We demonstrate that our optimizer trains policies that decrease constraint violation and increase cumulative rewards over time. Numerical experiments in constrained optimization and obstacle-avoidance navigation confirm the validity of our theoretical results.",1
"We study regenerative stopping problems in which the system starts anew whenever the controller decides to stop and the long-term average cost is to be minimized. Traditional model-based solutions involve estimating the underlying process from data and computing strategies for the estimated model. In this paper, we compare such solutions to deep reinforcement learning and imitation learning which involve learning a neural network policy from simulations. We evaluate the different approaches on a real-world problem of shipping consolidation in logistics and demonstrate that deep learning can be effectively used to solve such problems.",0
"The focus of our research is on regenerative stopping problems, where the system initiates a fresh start upon the controller's decision to stop, with the aim of minimizing the long-term average cost. The conventional model-based methods entail estimating the underlying process through data analysis and devising tactics for the estimated model. In contrast, we examine deep reinforcement learning and imitation learning techniques, which involve training a neural network policy using simulations. Our investigation centers on a practical logistics issue of shipping consolidation, and our findings reveal the effectiveness of deep learning in resolving such problems.",1
"Recent advances in reinforcement learning have inspired increasing interest in learning user modeling adaptively through dynamic interactions, e.g., in reinforcement learning based recommender systems. Reward function is crucial for most of reinforcement learning applications as it can provide the guideline about the optimization. However, current reinforcement-learning-based methods rely on manually-defined reward functions, which cannot adapt to dynamic and noisy environments. Besides, they generally use task-specific reward functions that sacrifice generalization ability. We propose a generative inverse reinforcement learning for user behavioral preference modelling, to address the above issues. Instead of using predefined reward functions, our model can automatically learn the rewards from user's actions based on discriminative actor-critic network and Wasserstein GAN. Our model provides a general way of characterizing and explaining underlying behavioral tendencies, and our experiments show our method outperforms state-of-the-art methods in a variety of scenarios, namely traffic signal control, online recommender systems, and scanpath prediction.",0
"The increasing interest in learning user modeling adaptively through dynamic interactions, such as in reinforcement learning based recommender systems, has been inspired by recent advancements in reinforcement learning. The reward function is critical for most reinforcement learning applications as it guides optimization. However, current reinforcement-learning-based methods rely on manually-defined reward functions that cannot adapt to dynamic and noisy environments, and task-specific reward functions that sacrifice generalization ability. To address these issues, we propose a generative inverse reinforcement learning approach for user behavioral preference modelling. Instead of using predefined reward functions, our model automatically learns rewards from user actions using a discriminative actor-critic network and Wasserstein GAN. Our model provides a general way of characterizing and explaining underlying behavioral tendencies, and our experiments demonstrate that our method outperforms state-of-the-art methods in various scenarios, including traffic signal control, online recommender systems, and scanpath prediction.",1
"Sequential decision-making under cost-sensitive tasks is prohibitively daunting, especially for the problem that has a significant impact on people's daily lives, such as malaria control, treatment recommendation. The main challenge faced by policymakers is to learn a policy from scratch by interacting with a complex environment in a few trials. This work introduces a practical, data-efficient policy learning method, named Variance-Bonus Monte Carlo Tree Search~(VB-MCTS), which can copy with very little data and facilitate learning from scratch in only a few trials. Specifically, the solution is a model-based reinforcement learning method. To avoid model bias, we apply Gaussian Process~(GP) regression to estimate the transitions explicitly. With the GP world model, we propose a variance-bonus reward to measure the uncertainty about the world. Adding the reward to the planning with MCTS can result in more efficient and effective exploration. Furthermore, the derived polynomial sample complexity indicates that VB-MCTS is sample efficient. Finally, outstanding performance on a competitive world-level RL competition and extensive experimental results verify its advantage over the state-of-the-art on the challenging malaria control task.",0
"It is a daunting task to make sequential decisions for cost-sensitive tasks, especially when the problem has a significant impact on people's daily lives, such as malaria control and treatment recommendation. Policymakers face the challenge of learning a policy from scratch by interacting with a complex environment in a few trials. This study presents a practical and data-efficient policy learning method called Variance-Bonus Monte Carlo Tree Search (VB-MCTS), which can facilitate learning from scratch with very little data. The method is a model-based reinforcement learning approach that applies Gaussian Process regression to estimate transitions explicitly to avoid model bias. The study proposes a variance-bonus reward to measure the uncertainty about the world and incorporate it into the planning with MCTS to improve exploration efficiency and effectiveness. The study shows that VB-MCTS is sample efficient based on the derived polynomial sample complexity. The method also demonstrates outstanding performance in a competitive world-level RL competition and extensive experimental results on the challenging malaria control task.",1
"Logic optimization is an NP-hard problem commonly approached through hand-engineered heuristics. We propose to combine graph convolutional networks with reinforcement learning and a novel, scalable node embedding method to learn which local transforms should be applied to the logic graph. We show that this method achieves a similar size reduction as ABC on smaller circuits and outperforms it by 1.5-1.75x on larger random graphs.",0
"The issue of logic optimization, which is classified as NP-hard, is typically tackled through manual heuristics. Our proposal involves utilizing reinforcement learning, graph convolutional networks, and an innovative, easily-expandable node embedding technique to determine the appropriate local transformations for the logic graph. Our findings indicate that this approach is as effective as ABC in reducing size for smaller circuits, and even surpasses it by 1.5-1.75 times in the case of larger random graphs.",1
"Action and observation delays commonly occur in many Reinforcement Learning applications, such as remote control scenarios. We study the anatomy of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.",0
"In Reinforcement Learning applications, particularly in remote control scenarios, it is not uncommon to experience delays in both action and observation. Our research focuses on analyzing randomly delayed environments and proposes a solution of partially resampling trajectory fragments in hindsight for off-policy multi-step value estimation. This principle has been applied to develop the Delay-Correcting Actor-Critic (DCAC) algorithm, which is an enhancement of Soft Actor-Critic and performs significantly better in delay-prone environments. The efficacy of DCAC is demonstrated both theoretically and practically using a delay-augmented version of the MuJoCo continuous control benchmark.",1
"Reinforcement learning (RL) has achieved impressive performance in a variety of online settings in which an agent's ability to query the environment for transitions and rewards is effectively unlimited. However, in many practical applications, the situation is reversed: an agent may have access to large amounts of undirected offline experience data, while access to the online environment is severely limited. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning. Primitives extracted in this way serve two purposes: they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains. Visualizations are available at https://sites.google.com/view/opal-iclr",0
"Impressive results have been achieved by reinforcement learning (RL) in online settings where an agent can freely query the environment for transitions and rewards. However, in practical applications, the agent's access to the online environment may be severely limited while it may have access to large amounts of undirected offline experience data. This work focuses on the offline setting and proposes the extraction of a continuous space of recurring and temporally extended primitive behaviors from the offline data to leverage it effectively for downstream task learning. The extracted primitives serve two purposes: distinguishing supported behaviors from unsupported ones to avoid distributional shift in offline RL, and providing temporal abstraction to reduce the effective horizon for better learning in theory and improved offline RL in practice. The proposed offline primitive learning also benefits few-shot imitation learning, exploration, and transfer in online RL on various benchmark domains. Visualizations are available at https://sites.google.com/view/opal-iclr.",1
"Poisoning attacks on Reinforcement Learning (RL) systems could take advantage of RL algorithm's vulnerabilities and cause failure of the learning. However, prior works on poisoning RL usually either unrealistically assume the attacker knows the underlying Markov Decision Process (MDP), or directly apply the poisoning methods in supervised learning to RL. In this work, we build a generic poisoning framework for online RL via a comprehensive investigation of heterogeneous poisoning models in RL. Without any prior knowledge of the MDP, we propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for most policy-based deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. VA2C-P uses a novel metric, stability radius in RL, that measures the vulnerability of RL algorithms. Experiments on multiple deep RL agents and multiple environments show that our poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget.",0
"The susceptibility of Reinforcement Learning (RL) systems to poisoning attacks can lead to their failure. However, previous studies on poisoning RL often make unrealistic assumptions about the attacker's knowledge of the Markov Decision Process (MDP) or apply poisoning techniques from supervised learning to RL. In this research, we have developed a general poisoning framework for online RL by analyzing various poisoning models in RL. Our strategic poisoning algorithm, called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), can be applied to most policy-based deep RL agents, even without prior knowledge of the MDP. VA2C-P employs a new metric, stability radius in RL, to evaluate the vulnerability of RL algorithms. Our experiments on multiple deep RL agents in different environments demonstrate that our poisoning algorithm can effectively prevent agents from learning a good policy or teach them to converge to a specific policy with a limited attacking budget.",1
"Historically, artificial intelligence has drawn much inspiration from neuroscience to fuel advances in the field. However, current progress in reinforcement learning is largely focused on benchmark problems that fail to capture many of the aspects that are of interest in neuroscience today. We illustrate this point by extending a T-maze task from neuroscience for use with reinforcement learning algorithms, and show that state-of-the-art algorithms are not capable of solving this problem. Finally, we point out where insights from neuroscience could help explain some of the issues encountered.",0
"In the past, artificial intelligence has relied heavily on insights from neuroscience to make advancements. However, the current progress in reinforcement learning is mainly centered around benchmark problems that do not encompass the aspects of neuroscience that are currently of interest. To demonstrate this, we adapted a T-maze task from neuroscience to be used with reinforcement learning algorithms and found that even the most advanced algorithms were unable to solve it. We conclude by highlighting areas where neuroscience can provide valuable insights into the challenges faced.",1
"We analyze the DQN reinforcement learning algorithm as a stochastic approximation scheme using the o.d.e. (for 'ordinary differential equation') approach and point out certain theoretical issues. We then propose a modified scheme called Full Gradient DQN (FG-DQN, for short) that has a sound theoretical basis and compare it with the original scheme on sample problems. We observe a better performance for FG-DQN.",0
"The DQN reinforcement learning algorithm is examined as a stochastic approximation scheme through the use of the o.d.e. approach, and some theoretical concerns are highlighted. To address these concerns, we suggest a revised approach known as Full Gradient DQN (FG-DQN), which is supported by a solid theoretical foundation. We test the original and revised approaches on sample problems and find that FG-DQN outperforms the original method.",1
"Markov Decision Processes are classically solved using Value Iteration and Policy Iteration algorithms. Recent interest in Reinforcement Learning has motivated the study of methods inspired by optimization, such as gradient ascent. Among these, a popular algorithm is the Natural Policy Gradient, which is a mirror descent variant for MDPs. This algorithm forms the basis of several popular Reinforcement Learning algorithms such as Natural actor-critic, TRPO, PPO, etc, and so is being studied with growing interest. It has been shown that Natural Policy Gradient with constant step size converges with a sublinear rate of O(1/k) to the global optimal. In this paper, we present improved finite time convergence bounds, and show that this algorithm has geometric (also known as linear) asymptotic convergence rate. We further improve this convergence result by introducing a variant of Natural Policy Gradient with adaptive step sizes. Finally, we compare different variants of policy gradient methods experimentally.",0
"Value Iteration and Policy Iteration algorithms are traditionally employed to solve Markov Decision Processes. However, the emergence of Reinforcement Learning has led to the exploration of optimization-inspired techniques, such as gradient ascent. A well-liked algorithm in this category is the Natural Policy Gradient, which is a variant of mirror descent for MDPs. This algorithm is the foundation of various popular Reinforcement Learning algorithms such as TRPO, PPO, and Natural actor-critic, and is gaining increased interest. The algorithm exhibits sublinear convergence with a constant step size of O(1/k) to the global optimal. In this study, we present improved finite time convergence bounds and demonstrate that the algorithm has a geometric (linear) asymptotic convergence rate. We also introduce a Natural Policy Gradient variant with adaptive step sizes to further improve the convergence rate. Finally, we experimentally compare different policy gradient method variants.",1
"Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.",0
"To achieve goals in complex environments, intelligent agents rely on generalization from past experiences. World models play a crucial role in this process, as they allow for the learning of behaviors through imagined outcomes, which ultimately increases sample-efficiency. While some recent advancements have made it possible to learn world models from image inputs for certain tasks, modeling Atari games accurately enough to derive successful behaviors has remained challenging for many years. However, with the introduction of DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model, this challenge has been overcome. DreamerV2 is the first agent to achieve human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. In fact, with the same computational budget and wall-clock time, DreamerV2 surpasses the final performance of the top single-GPU agents IQN and Rainbow by reaching 200M frames. Furthermore, DreamerV2 is also applicable to tasks with continuous actions, as it can learn an accurate world model of a complex humanoid robot and solve stand-up and walking from only pixel inputs.",1
"In many biomedical, science, and engineering problems, one must sequentially decide which action to take next so as to maximize rewards. One general class of algorithms for optimizing interactions with the world, while simultaneously learning how the world operates, is the multi-armed bandit setting and, in particular, the contextual bandit case. In this setting, for each executed action, one observes rewards that are dependent on a given 'context', available at each interaction with the world. The Thompson sampling algorithm has recently been shown to enjoy provable optimality properties for this set of problems, and to perform well in real-world settings. It facilitates generative and interpretable modeling of the problem at hand. Nevertheless, the design and complexity of the model limit its application, since one must both sample from the distributions modeled and calculate their expected rewards. We here show how these limitations can be overcome using variational inference to approximate complex models, applying to the reinforcement learning case advances developed for the inference case in the machine learning community over the past two decades. We consider contextual multi-armed bandit applications where the true reward distribution is unknown and complex, which we approximate with a mixture model whose parameters are inferred via variational inference. We show how the proposed variational Thompson sampling approach is accurate in approximating the true distribution, and attains reduced regrets even with complex reward distributions. The proposed algorithm is valuable for practical scenarios where restrictive modeling assumptions are undesirable.",0
"In order to maximize rewards in various biomedical, science, and engineering problems, one must make sequential decisions about which action to take. The multi-armed bandit setting, particularly the contextual bandit case, is a general class of algorithms used to optimize interactions with the world while simultaneously learning how the world operates. Each executed action in this setting results in reward observation that is context-dependent. The Thompson sampling algorithm has been shown to be optimal for these types of problems and perform well in real-world settings. However, the design and complexity of the model limit its application as it requires sampling from modeled distributions and calculating expected rewards. This paper proposes using variational inference to approximate complex models, an approach that has been developed in the machine learning community over the past two decades for inference cases. The proposed variational Thompson sampling approach is accurate in approximating the true distribution and attains reduced regrets even with complex reward distributions. This algorithm is valuable for practical scenarios in which restrictive modeling assumptions are undesirable, especially in contextual multi-armed bandit applications where the true reward distribution is unknown and complex.",1
"In this paper, we present a comprehensive, in-depth survey of the literature on reinforcement learning approaches to ridesharing problems. Papers on the topics of rideshare matching, vehicle repositioning, ride-pooling, and dynamic pricing are covered. Popular data sets and open simulation environments are also introduced. Subsequently, we discuss a number of challenges and opportunities for reinforcement learning research on this important domain.",0
"The focus of this paper is to provide a thorough and detailed examination of the various reinforcement learning methods applied to the issue of ridesharing. The topics of rideshare matching, vehicle repositioning, ride-pooling, and dynamic pricing are extensively explored, along with popular data sets and simulation environments. Additionally, challenges and opportunities for further reinforcement learning research in this crucial domain are discussed.",1
"We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods.",0
"A technique is being proposed to meta-learn reinforcement learning algorithms through exploration of the computational graph space responsible for the loss function calculation of a model-free RL agent that optimizes values. The algorithms learned are not specific to any domain and can be applied to new environments beyond the training set. Our method allows for learning from scratch or building upon existing algorithms such as DQN, with the added benefit of making modifications to enhance performance. In simple classical control and gridworld tasks, our method discovered the TD algorithm when learning from scratch. When bootstrapped from DQN, our method identified two algorithms that exhibited excellent generalization performance in various classical control tasks, gridworld-type tasks, and Atari games. The behavior of the learned algorithms showed similarities to recent RL algorithms that deal with overestimation in value-based methods.",1
"In this paper, we present a Model-Based Reinforcement Learning algorithm named Monte Carlo Probabilistic Inference for Learning COntrol (MC-PILCO). The algorithm relies on Gaussian Processes (GPs) to model the system dynamics and on a Monte Carlo approach to estimate the policy gradient. This defines a framework in which we ablate the choice of the following components: (i) the selection of the cost function, (ii) the optimization of policies using dropout, (iii) an improved data efficiency through the use of structured kernels in the GP models. The combination of the aforementioned aspects affects dramatically the performance of MC-PILCO. Numerical comparisons in a simulated cart-pole environment show that MC-PILCO exhibits better data-efficiency and control performance w.r.t. state-of-the-art GP-based MBRL algorithms. Finally, we apply MC-PILCO to real systems, considering in particular systems with partially measurable states. We discuss the importance of modeling both the measurement system and the state estimators during policy optimization. The effectiveness of the proposed solutions has been tested in simulation and in two real systems, a Furuta pendulum and a ball-and-plate.",0
"In this article, we introduce MC-PILCO, a Model-Based Reinforcement Learning algorithm that uses Gaussian Processes to model system dynamics and a Monte Carlo approach to estimate policy gradient. By evaluating the cost function selection, policy optimization using dropout, and the use of structured kernels in GP models, we analyze MC-PILCO's framework. Our results show that MC-PILCO outperforms state-of-the-art GP-based MBRL algorithms in terms of data efficiency and control performance in a simulated cart-pole environment. We also apply MC-PILCO to real systems with partially measurable states, discussing the importance of modeling both the measurement system and the state estimators during policy optimization. Our proposed solutions have been successfully tested in simulations and two real systems, a Furuta pendulum, and a ball-and-plate.",1
"Off-policy evaluation (OPE) in reinforcement learning is notoriously difficult in long- and infinite-horizon settings due to diminishing overlap between behavior and target policies. In this paper, we study the role of Markovian and time-invariant structure in efficient OPE. We first derive the efficiency bounds for OPE when one assumes each of these structures. This precisely characterizes the curse of horizon: in time-variant processes, OPE is only feasible in the near-on-policy setting, where behavior and target policies are sufficiently similar. But, in time-invariant Markov decision processes, our bounds show that truly-off-policy evaluation is feasible, even with only just one dependent trajectory, and provide the limits of how well we could hope to do. We develop a new estimator based on Double Reinforcement Learning (DRL) that leverages this structure for OPE using the efficient influence function we derive. Our DRL estimator simultaneously uses estimated stationary density ratios and $q$-functions and remains efficient when both are estimated at slow, nonparametric rates and remains consistent when either is estimated consistently. We investigate these properties and the performance benefits of leveraging the problem structure for more efficient OPE.",0
"Reinforcement learning faces challenges in accurately evaluating policies that differ from the ones used during training, particularly in long- and infinite-horizon scenarios where there is limited overlap between the behavior and target policies. This study explores the potential of Markovian and time-invariant structures to improve off-policy evaluation (OPE) efficiency. The paper establishes OPE efficiency bounds for each structure, revealing that time-variant processes only allow feasible OPE in near-on-policy settings, while time-invariant Markov decision processes enable truly off-policy evaluation with just one dependent trajectory. The study proposes a novel estimator based on Double Reinforcement Learning (DRL) that utilizes the efficient influence function derived from the structures for OPE. The DRL estimator leverages estimated stationary density ratios and $q$-functions and retains efficiency and consistency even when the estimation rates are slow and nonparametric. The study investigates the benefits of leveraging the problem structure for more efficient OPE.",1
"Double Q-learning is a popular reinforcement learning algorithm in Markov decision process (MDP) problems. Clipped Double Q-learning, as an effective variant of Double Q-learning, employs the clipped double estimator to approximate the maximum expected action value. Due to the underestimation bias of the clipped double estimator, performance of clipped Double Q-learning may be degraded in some stochastic environments. In this paper, in order to reduce the underestimation bias, we propose an action candidate based clipped double estimator for Double Q-learning. Specifically, we first select a set of elite action candidates with the high action values from one set of estimators. Then, among these candidates, we choose the highest valued action from the other set of estimators. Finally, we use the maximum value in the second set of estimators to clip the action value of the chosen action in the first set of estimators and the clipped value is used for approximating the maximum expected action value. Theoretically, the underestimation bias in our clipped Double Q-learning decays monotonically as the number of the action candidates decreases. Moreover, the number of action candidates controls the trade-off between the overestimation and underestimation biases. In addition, we also extend our clipped Double Q-learning to continuous action tasks via approximating the elite continuous action candidates. We empirically verify that our algorithm can more accurately estimate the maximum expected action value on some toy environments and yield good performance on several benchmark problems.",0
"Double Q-learning is a well-known reinforcement learning algorithm that is often used in Markov decision process (MDP) problems. Clipped Double Q-learning is a variation of Double Q-learning that utilizes the clipped double estimator to approximate the maximum expected action value. However, this method can sometimes result in a decrease in performance due to the underestimation bias of the estimator in certain stochastic environments. To address this issue, we introduce a new approach to reduce underestimation bias in Clipped Double Q-learning. This method involves selecting a set of elite action candidates with high action values from one set of estimators, and then choosing the highest-valued action from a second set of estimators among these candidates. The maximum value in the second set of estimators is then used to clip the action value of the chosen action in the first set of estimators, resulting in an approximation of the maximum expected action value. We demonstrate that our approach can effectively reduce underestimation bias and achieve better performance on a range of benchmark problems, including continuous action tasks. Our findings also suggest that the number of action candidates selected can help control the trade-off between overestimation and underestimation biases.",1
"The temporal Credit Assignment Problem (CAP) is a well-known and challenging task in AI. While Reinforcement Learning (RL), especially Deep RL, works well when immediate rewards are available, it can fail when only delayed rewards are available or when the reward function is noisy. In this work, we propose delegating the CAP to a Neural Network-based algorithm named InferNet that explicitly learns to infer the immediate rewards from the delayed rewards. The effectiveness of InferNet was evaluated on two online RL tasks: a simple GridWorld and 40 Atari games; and two offline RL tasks: GridWorld and a real-life Sepsis treatment task. For all tasks, the effectiveness of using the InferNet inferred rewards is compared against the immediate and the delayed rewards with two settings: with noisy rewards and without noise. Overall, our results show that the effectiveness of InferNet is robust against noisy reward functions and is an effective add-on mechanism for solving temporal CAP in a wide range of RL tasks, from classic RL simulation environments to a real-world RL problem and for both online and offline learning.",0
"AI faces a challenging task in the temporal Credit Assignment Problem (CAP), which poses a difficulty for Reinforcement Learning (RL) and Deep RL when only delayed rewards or noisy reward functions are available. To address this issue, we propose InferNet, a Neural Network-based algorithm that infers immediate rewards from delayed rewards. We evaluate the effectiveness of InferNet on four RL tasks: two online and two offline, including a real-life Sepsis treatment task. Our results demonstrate that InferNet is a robust and effective add-on mechanism for solving temporal CAP in a broad range of RL tasks, including noisy reward functions.",1
"To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: https://sites.google.com/view/latent-skill-planning/",0
"In order to quickly resolve new challenges in complicated surroundings, intelligent agents must establish reusable knowledge. One instance of this is a learned world model, which captures knowledge about the environment that can be applied to new tasks. Likewise, skills capture general behaviors that can also be applied to new tasks. In this article, we explore how these two approaches can be combined into a single reinforcement learning agent. We use the concept of partial amortization to facilitate rapid adaptation at test time. To achieve this, a policy that is learned over time produces actions while the skills it relies on are chosen using online planning. We demonstrate the advantages of our design choices through a series of demanding locomotion tasks, and we show that our method improves sample efficiency in both single tasks and transfer from one task to another, compared to competing methods. Videos of our work can be found at: https://sites.google.com/view/latent-skill-planning/",1
"Deep-learning-based video processing has yielded transformative results in recent years. However, the video analytics pipeline is energy-intensive due to high data rates and reliance on complex inference algorithms, which limits its adoption in energy-constrained applications. Motivated by the observation of high and variable spatial redundancy and temporal dynamics in video data streams, we design and evaluate an adaptive-resolution optimization framework to minimize the energy use of multi-task video analytics pipelines. Instead of heuristically tuning the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically govern the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, hence the accuracy of video analytics results, the proposed end-to-end optimization framework learns the best non-myopic policy for dynamically controlling the resolution of input video streams to globally optimize energy efficiency. Governed by reinforcement learning, optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. The proposed framework is applied to video instance segmentation which is one of the most challenging computer vision tasks, and achieves better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.",0
"In recent years, deep-learning-based video processing has produced significant results. However, the energy consumption of the video analytics pipeline is high due to the complex inference algorithms and large data rates, which limits its use in applications with energy constraints. To address this issue, we have developed an adaptive-resolution optimization framework that utilizes deep reinforcement learning to dynamically control the input resolution and computation of the entire video analytics pipeline. Instead of adjusting the resolution of individual tasks, our framework learns the best policy for controlling the input resolution through monitoring the impact of varying resolution on the quality of video analytics features. Optical flow is integrated into the framework to minimize redundant computations while maintaining accuracy. Our framework is applied to video instance segmentation and achieves better energy efficiency than all other similar baseline methods on the YouTube-VIS dataset.",1
"Humans can abstract prior knowledge from very little data and use it to boost skill learning. In this paper, we propose routine-augmented policy learning (RAPL), which discovers routines composed of primitive actions from a single demonstration and uses discovered routines to augment policy learning. To discover routines from the demonstration, we first abstract routine candidates by identifying grammar over the demonstrated action trajectory. Then, the best routines measured by length and frequency are selected to form a routine library. We propose to learn policy simultaneously at primitive-level and routine-level with discovered routines, leveraging the temporal structure of routines. Our approach enables imitating expert behavior at multiple temporal scales for imitation learning and promotes reinforcement learning exploration. Extensive experiments on Atari games demonstrate that RAPL improves the state-of-the-art imitation learning method SQIL and reinforcement learning method A2C. Further, we show that discovered routines can generalize to unseen levels and difficulties on the CoinRun benchmark.",0
"The ability of humans to learn from minimal information and apply this knowledge to enhance their skills is well-known. This article introduces a new technique called routine-augmented policy learning (RAPL), which identifies basic actions from a single demonstration and uses them to improve policy learning. Routines are discovered by recognizing patterns in the demonstrated action sequence, and the most effective routines are selected based on frequency and length. Policy learning takes place both at the primitive and routine levels, using the structure of routines to facilitate learning. This approach is useful for developing imitation learning and reinforcement learning strategies and has been shown to outperform existing methods such as SQIL and A2C. Furthermore, the discovered routines have been demonstrated to be effective even when applied to new and more challenging environments.",1
"We present a framework that enables the discovery of diverse and natural-looking motion strategies for athletic skills such as the high jump. The strategies are realized as control policies for physics-based characters. Given a task objective and an initial character configuration, the combination of physics simulation and deep reinforcement learning (DRL) provides a suitable starting point for automatic control policy training. To facilitate the learning of realistic human motions, we propose a Pose Variational Autoencoder (P-VAE) to constrain the actions to a subspace of natural poses. In contrast to motion imitation methods, a rich variety of novel strategies can naturally emerge by exploring initial character states through a sample-efficient Bayesian diversity search (BDS) algorithm. A second stage of optimization that encourages novel policies can further enrich the unique strategies discovered. Our method allows for the discovery of diverse and novel strategies for athletic jumping motions such as high jumps and obstacle jumps with no motion examples and less reward engineering than prior work.",0
"Our approach introduces a framework that uncovers a range of authentic and varied motion techniques for athletic skills, such as the high jump. These techniques manifest as control policies for characters based on physics. By leveraging physics simulation and deep reinforcement learning (DRL), we establish a suitable foundation for automatic control policy training, where a task objective and an initial character configuration are given. To ensure the acquisition of realistic human movements, we propose a Pose Variational Autoencoder (P-VAE) that limits actions to a subset of natural poses. Rather than relying on motion imitation methods, the Bayesian diversity search (BDS) algorithm allows for the exploration of initial character states, producing several new and innovative strategies. A second phase of optimization encourages the discovery of unique policies, further augmenting the range of techniques uncovered. Our approach requires no motion examples and less reward engineering than previous methods, making it an efficient means to discover novel strategies for athletic motions like high jumps and obstacle jumps.",1
"Partially Observable Stochastic Games (POSGs) are the most general and common model of games used in Multi-Agent Reinforcement Learning (MARL). We argue that the POSG model is conceptually ill suited to software MARL environments, and offer case studies from the literature where this mismatch has led to severely unexpected behavior. In response to this, we introduce the Agent Environment Cycle Games (AEC Games) model, which is more representative of software implementation. We then prove it's as an equivalent model to POSGs. The AEC games model is also uniquely useful in that it can elegantly represent both all forms of MARL environments, whereas for example POSGs cannot elegantly represent strictly turn based games like chess.",0
"POSGs are commonly used in MARL as they are the most general game model. However, we believe that this model is not suitable for software environments, as demonstrated by examples from the literature where it resulted in unexpected behavior. As a solution, we propose the AEC Games model, which better aligns with software implementation and is equivalent to POSGs. AEC Games is also advantageous as it can represent all types of MARL environments, whereas POSGs struggle with turn-based games like chess.",1
"We consider the problem of scheduling in constrained queueing networks with a view to minimizing packet delay. Modern communication systems are becoming increasingly complex, and are required to handle multiple types of traffic with widely varying characteristics such as arrival rates and service times. This, coupled with the need for rapid network deployment, render a bottom up approach of first characterizing the traffic and then devising an appropriate scheduling protocol infeasible.   In contrast, we formulate a top down approach to scheduling where, given an unknown network and a set of scheduling policies, we use a policy gradient based reinforcement learning algorithm that produces a scheduler that performs better than the available atomic policies. We derive convergence results and analyze finite time performance of the algorithm. Simulation results show that the algorithm performs well even when the arrival rates are nonstationary and can stabilize the system even when the constituent policies are unstable.",0
"The objective of this study is to minimize packet delay in constrained queueing networks. Due to the increasing complexity of modern communication systems and the need for swift network deployment, a bottom up approach is not practical. This is because it involves characterizing the traffic before devising a suitable scheduling protocol. Instead, we propose a top down approach that utilizes a policy gradient based reinforcement learning algorithm to develop a scheduler that outperforms existing atomic policies. Our research includes convergence results and an analysis of the algorithm's performance over a finite time. The simulation results indicate that the algorithm is effective even when dealing with nonstationary arrival rates and can stabilize the system in the presence of unstable constituent policies.",1
"Reinforcement Learning (RL) is essentially a trial-and-error learning procedure which may cause unsafe behavior during the exploration-and-exploitation process. This hinders the application of RL to real-world control problems, especially to those for safety-critical systems. In this paper, we introduce a framework for safe RL that is based on integration of a RL algorithm with an add-on safety supervision module, called the Robust Action Governor (RAG), which exploits set-theoretic techniques and online optimization to manage safety-related requirements during learning. We illustrate this proposed safe RL framework through an application to automotive adaptive cruise control.",0
"The trial-and-error learning procedure of Reinforcement Learning (RL) can result in unsafe behavior during the exploration-and-exploitation process, which poses a challenge for its application to real-world control problems, particularly those involving safety-critical systems. To address this issue, we present a safe RL framework that incorporates an additional safety supervision module called the Robust Action Governor (RAG). This module utilizes set-theoretic techniques and online optimization to ensure that safety-related requirements are met during the learning process. To demonstrate the effectiveness of this proposed safe RL framework, we apply it to automotive adaptive cruise control.",1
"Recently, Intelligent Transportation Systems are leveraging the power of increased sensory coverage and computing power to deliver data-intensive solutions achieving higher levels of performance than traditional systems. Within Traffic Signal Control (TSC), this has allowed the emergence of Machine Learning (ML) based systems. Among this group, Reinforcement Learning (RL) approaches have performed particularly well. Given the lack of industry standards in ML for TSC, literature exploring RL often lacks comparison against commercially available systems and straightforward formulations of how the agents operate. Here we attempt to bridge that gap. We propose three different architectures for TSC RL agents and compare them against the currently used commercial systems MOVA, SurTrac and Cyclic controllers and provide pseudo-code for them. The agents use variations of Deep Q-Learning and Actor Critic, using states and rewards based on queue lengths. Their performance is compared in across different map scenarios with variable demand, assessing them in terms of the global delay and average queue length. We find that the RL-based systems can significantly and consistently achieve lower delays when compared with existing commercial systems.",0
"In recent times, Intelligent Transportation Systems have utilized enhanced sensory coverage and computing power to develop data-driven solutions that surpass the capabilities of traditional systems. This progress has facilitated the emergence of Machine Learning (ML) based systems in Traffic Signal Control (TSC), with Reinforcement Learning (RL) approaches proving particularly effective. However, due to the absence of industry standards in ML for TSC, literature exploring RL often lacks comparison with commercially available systems and simple explanations of how the agents function. Our study aims to address this gap by proposing three different architectures for TSC RL agents and comparing them with existing commercial systems MOVA, SurTrac, and Cyclic controllers, and providing pseudo-code for each. The agents we developed use variations of Deep Q-Learning and Actor Critic with states and rewards based on queue lengths. We assessed their performance across different map scenarios with varying demand and evaluated them based on global delay and average queue length. Our investigation revealed that the RL-based systems consistently achieved lower delays compared to existing commercial systems.",1
"Multi-agent reinforcement learning methods have shown remarkable potential in solving complex multi-agent problems but mostly lack theoretical guarantees. Recently, mean field control and mean field games have been established as a tractable solution for large-scale multi-agent problems with many agents. In this work, driven by a motivating scheduling problem, we consider a discrete-time mean field control model with common environment states. We rigorously establish approximate optimality as the number of agents grows in the finite agent case and find that a dynamic programming principle holds, resulting in the existence of an optimal stationary policy. As exact solutions are difficult in general due to the resulting continuous action space of the limiting mean field Markov decision process, we apply established deep reinforcement learning methods to solve the associated mean field control problem. The performance of the learned mean field control policy is compared to typical multi-agent reinforcement learning approaches and is found to converge to the mean field performance for sufficiently many agents, verifying the obtained theoretical results and reaching competitive solutions.",0
"Multi-agent reinforcement learning techniques exhibit significant potential in addressing intricate multi-agent problems, but they often lack theoretical assurances. Recently, mean field control and mean field games have emerged as a practical solution for multi-agent problems with numerous agents. This study examines a discrete-time mean field control model with shared environment states, motivated by a scheduling problem. We rigorously establish nearly optimal outcomes as the number of agents increases in finite agent situations and identify the presence of an optimal stationary policy based on a dynamic programming principle. Since exact solutions are generally difficult due to the continuous action space of the limiting mean field Markov decision process, we employ deep reinforcement learning methods to solve the associated mean field control problem. The acquired mean field control policy's performance is compared to typical multi-agent reinforcement learning techniques and is shown to converge to the mean field performance for a sufficient number of agents, validating the obtained theoretical outcomes and producing competitive solutions.",1
"Zero-shot action recognition is the task of recognizingaction classes without visual examples, only with a seman-tic embedding which relates unseen to seen classes. Theproblem can be seen as learning a function which general-izes well to instances of unseen classes without losing dis-crimination between classes. Neural networks can modelthe complex boundaries between visual classes, which ex-plains their success as supervised models. However, inzero-shot learning, these highly specialized class bound-aries may not transfer well from seen to unseen classes.In this paper we propose a centroid-based representation,which clusters visual and semantic representation, consid-ers all training samples at once, and in this way generaliz-ing well to instances from unseen classes. We optimize theclustering using Reinforcement Learning which we show iscritical for our approach to work. We call the proposedmethod CLASTER and observe that it consistently outper-forms the state-of-the-art in all standard datasets, includ-ing UCF101, HMDB51 and Olympic Sports; both in thestandard zero-shot evaluation and the generalized zero-shotlearning. Further, we show that our model performs com-petitively in the image domain as well, outperforming thestate-of-the-art in many settings.",0
"The task of zero-shot action recognition involves recognizing action classes without any visual examples. Instead, it relies on a semantic embedding that connects unseen and seen classes. The challenge is to develop a function that can generalize well to instances of unseen classes without losing discrimination between classes. While neural networks are effective in modeling complex boundaries between visual classes in supervised models, this may not transfer well to zero-shot learning. To address this issue, we propose a centroid-based representation that clusters visual and semantic representations and considers all training samples simultaneously. We optimize the clustering using Reinforcement Learning, which is critical for our approach. Our proposed method, CLASTER, consistently outperforms the state-of-the-art in all standard datasets, including UCF101, HMDB51, and Olympic Sports, in both standard zero-shot evaluation and generalized zero-shot learning. Additionally, our model performs competitively in the image domain, outperforming the state-of-the-art in many settings.",1
"Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. In this review article, we have focused on presenting recent approaches on Multi-Agent Reinforcement Learning (MARL) algorithms. In particular, we have focused on five common approaches on modeling and solving cooperative multi-agent reinforcement learning problems: (I) independent learners, (II) fully observable critic, (III) value function factorization, (IV) consensus, and (IV) learn to communicate. First, we elaborate on each of these methods, possible challenges, and how these challenges were mitigated in the relevant papers. If applicable, we further make a connection among different papers in each category. Next, we cover some new emerging research areas in MARL along with the relevant recent papers. Due to the recent success of MARL in real-world applications, we assign a section to provide a review of these applications and corresponding articles.   Also, a list of available environments for MARL research is provided in this survey. Finally, the paper is concluded with proposals on the possible research directions.",0
"The progress of Deep Reinforcement Learning in multi-agent systems has been noteworthy in recent times. This review article's primary focus is to present recent approaches of Multi-Agent Reinforcement Learning (MARL) algorithms. The article highlights five common approaches to modeling and solving cooperative multi-agent reinforcement learning problems: independent learners, fully observable critic, value function factorization, consensus, and learn to communicate. Each approach is elaborated in detail, including possible challenges and how the relevant papers mitigated them. Where applicable, we establish connections among different papers in each category. Additionally, we discuss some of the new emerging research areas in MARL, along with the relevant recent papers. The paper also reviews the real-world applications of MARL and the corresponding articles. Additionally, we provide a list of available environments for MARL research in this survey. Finally, the paper concludes with proposals on possible research directions.",1
"Recurrent meta reinforcement learning (meta-RL) agents are agents that employ a recurrent neural network (RNN) for the purpose of ""learning a learning algorithm"". After being trained on a pre-specified task distribution, the learned weights of the agent's RNN are said to implement an efficient learning algorithm through their activity dynamics, which allows the agent to quickly solve new tasks sampled from the same distribution. However, due to the black-box nature of these agents, the way in which they work is not yet fully understood. In this study, we shed light on the internal working mechanisms of these agents by reformulating the meta-RL problem using the Partially Observable Markov Decision Process (POMDP) framework. We hypothesize that the learned activity dynamics is acting as belief states for such agents. Several illustrative experiments suggest that this hypothesis is true, and that recurrent meta-RL agents can be viewed as agents that learn to act optimally in partially observable environments consisting of multiple related tasks. This view helps in understanding their failure cases and some interesting model-based results reported in the literature.",0
"Agents known as recurrent meta reinforcement learning (meta-RL) agents use a recurrent neural network (RNN) to ""learn a learning algorithm"". Once trained on a task distribution, the agent's RNN weights develop an efficient learning algorithm that allows the agent to solve new tasks from the same distribution quickly. However, due to the opaque nature of these agents, their functioning is not yet fully comprehended. Therefore, we redefined the meta-RL problem using the Partially Observable Markov Decision Process (POMDP) framework to illuminate the internal workings of these agents. We hypothesized that the learned activity dynamics serve as belief states for these agents. Our experiments suggest that this hypothesis is valid, and that recurrent meta-RL agents can be viewed as agents that learn to act optimally in partially observable environments with multiple related tasks. This perspective aids in comprehending their failures and some intriguing model-based findings in the literature.",1
"This paper proposes a distributed Reinforcement Learning (RL) based framework that can be used for synthesizing MAC layer wireless protocols in IoT networks with low-complexity wireless transceivers. The proposed framework does not rely on complex hardware capabilities such as carrier sensing and its associated algorithmic complexities that are often not supported in wireless transceivers of low-cost and low-energy IoT devices. In this framework, the access protocols are first formulated as Markov Decision Processes (MDP) and then solved using RL. A distributed and multi-Agent RL framework is used as the basis for protocol synthesis. Distributed behavior makes the nodes independently learn optimal transmission strategies without having to rely on full network level information and direct knowledge of behavior of other nodes. The nodes learn to minimize packet collisions such that optimal throughput can be attained and maintained for loading conditions that are higher than what the known benchmark protocols (such as ALOHA) for IoT devices without complex transceivers. In addition, the nodes are observed to be able to learn to act optimally in the presence of heterogeneous loading and network topological conditions. Finally, the proposed learning approach allows the wireless bandwidth to be fairly distributed among network nodes in a way that is not dependent on such heterogeneities. Via simulation experiments, the paper demonstrates the performance of the learning paradigm and its abilities to make nodes adapt their optimal transmission strategies on the fly in response to various network dynamics.",0
"A framework for synthesizing MAC layer wireless protocols in IoT networks with low-complexity wireless transceivers is proposed in this paper using distributed Reinforcement Learning (RL). The framework does not require complex hardware capabilities like carrier sensing and its associated algorithmic complexities, which are not supported in low-cost and low-energy IoT devices' wireless transceivers. In this approach, the access protocols are formulated as Markov Decision Processes (MDP) and solved using RL. The proposed distributed and multi-Agent RL framework enables nodes to learn optimal transmission strategies independently, minimizing packet collisions to attain and maintain optimal throughput for higher loading conditions than known benchmark protocols like ALOHA for IoT devices without complex transceivers. The approach also enables nodes to act optimally in the presence of heterogeneous loading and network topological conditions, and the wireless bandwidth to be fairly distributed among network nodes, independent of heterogeneities. Simulation experiments demonstrate the learning paradigm's performance and ability to adapt nodes' optimal transmission strategies on the fly in response to various network dynamics.",1
"Data preparation, i.e. the process of transforming raw data into a format that can be used for training effective machine learning models, is a tedious and time-consuming task. For image data, preprocessing typically involves a sequence of basic transformations such as cropping, filtering, rotating or flipping images. Currently, data scientists decide manually based on their experience which transformations to apply in which particular order to a given image data set. Besides constituting a bottleneck in real-world data science projects, manual image data preprocessing may yield suboptimal results as data scientists need to rely on intuition or trial-and-error approaches when exploring the space of possible image transformations and thus might not be able to discover the most effective ones. To mitigate the inefficiency and potential ineffectiveness of manual data preprocessing, this paper proposes a deep reinforcement learning framework to automatically discover the optimal data preprocessing steps for training an image classifier. The framework takes as input sets of labeled images and predefined preprocessing transformations. It jointly learns the classifier and the optimal preprocessing transformations for individual images. Experimental results show that the proposed approach not only improves the accuracy of image classifiers, but also makes them substantially more robust to noisy inputs at test time.",0
"The process of converting raw data into a suitable format for training efficient machine learning models is known as data preparation, which is an arduous and time-consuming task. When it comes to image data, preprocessing entails a series of fundamental transformations such as cropping, filtering, rotating, or flipping images. Currently, data scientists manually decide which transformations to apply to a given image dataset based on their expertise. However, this manual image data preprocessing can be a bottleneck in real-world data science projects, and may not yield optimal results as data scientists rely on intuition or trial-and-error methods to explore the space of possible image transformations. As a solution to the inefficiency and potential ineffectiveness of manual data preprocessing, this study introduces a deep reinforcement learning framework that automatically identifies the most effective data preprocessing steps for training an image classifier. The framework takes in labeled images and predefined preprocessing transformations as inputs, and learns the optimal classifier and preprocessing transformations for individual images simultaneously. Experimental findings indicate that the proposed approach not only enhances the accuracy of image classifiers, but also makes them considerably more resilient to noisy inputs during testing.",1
"We address the problem of forecasting pedestrian and vehicle trajectories in unknown environments, conditioned on their past motion and scene structure. Trajectory forecasting is a challenging problem due to the large variation in scene structure and the multimodal distribution of future trajectories. Unlike prior approaches that directly learn one-to-many mappings from observed context to multiple future trajectories, we propose to condition trajectory forecasts on plans sampled from a grid based policy learned using maximum entropy inverse reinforcement learning (MaxEnt IRL). We reformulate MaxEnt IRL to allow the policy to jointly infer plausible agent goals, and paths to those goals on a coarse 2-D grid defined over the scene. We propose an attention based trajectory generator that generates continuous valued future trajectories conditioned on state sequences sampled from the MaxEnt policy. Quantitative and qualitative evaluation on the publicly available Stanford drone and NuScenes datasets shows that our model generates trajectories that are diverse, representing the multimodal predictive distribution, and precise, conforming to the underlying scene structure over long prediction horizons.",0
"Our focus is on predicting the trajectories of pedestrians and vehicles in unfamiliar surroundings, using their prior movements and scene layout as guidance. The complexity of forecasting arises from the diverse scene structures and the multiple possibilities for future trajectories. Instead of previous strategies that map observed context directly to multiple future trajectories, we propose relying on plans derived from a grid-based policy learned through Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL). We redefine MaxEnt IRL to allow the policy to determine both plausible agent goals and paths to those goals on a coarse 2-D grid that encompasses the scene. Our attention-based trajectory generator creates continuous-valued future trajectories based on state sequences drawn from the MaxEnt policy. Our model produces trajectories that are both diverse, reflecting multiple predictive options, and precise, conforming to the underlying scene structure for extended prediction periods. Our quantitative and qualitative assessments on the publicly available Stanford drone and NuScenes datasets support the validity of our approach.",1
"Visual domain randomization in simulated environments is a widely used method to transfer policies trained in simulation to real robots. However, domain randomization and augmentation hamper the training of a policy. As reinforcement learning struggles with a noisy training signal, this additional nuisance can drastically impede training. For difficult tasks it can even result in complete failure to learn. To overcome this problem we propose to pre-train a perception encoder that already provides an embedding invariant to the randomization. We demonstrate that this yields consistently improved results on a randomized version of DeepMind control suite tasks and a stacking environment on arbitrary backgrounds with zero-shot transfer to a physical robot.",0
"The technique of visual domain randomization is frequently employed to transfer policies from simulation to real robots. Unfortunately, this approach can impede policy training due to the added complications of domain randomization and augmentation. These factors create a noisy training signal that can significantly hinder reinforcement learning, and in some cases, lead to an inability to learn altogether. To address this issue, we propose a solution involving the pre-training of a perception encoder that can provide an invariant embedding to the randomization. Our approach has consistently yielded improved results in randomized DeepMind control suite tasks and a stacking environment on arbitrary backgrounds, with the added benefit of zero-shot transfer to a physical robot.",1
"Network dismantling aims to degrade the connectivity of a network by removing an optimal set of nodes and has been widely adopted in many real-world applications such as epidemic control and rumor containment. However, conventional methods usually focus on simple network modeling with only pairwise interactions, while group-wise interactions modeled by hypernetwork are ubiquitous and critical. In this work, we formulate the hypernetwork dismantling problem as a node sequence decision problem and propose a deep reinforcement learning (DRL)-based hypernetwork dismantling framework. Besides, we design a novel inductive hypernetwork embedding method to ensure the transferability to various real-world hypernetworks. Generally, our framework builds an agent. It first generates small-scale synthetic hypernetworks and embeds the nodes and hypernetworks into a low dimensional vector space to represent the action and state space in DRL, respectively. Then trial-and-error dismantling tasks are conducted by the agent on these synthetic hypernetworks, and the dismantling strategy is continuously optimized. Finally, the well-optimized strategy is applied to real-world hypernetwork dismantling tasks. Experimental results on five real-world hypernetworks demonstrate the effectiveness of our proposed framework.",0
"The aim of network dismantling is to reduce the connectivity of a network by taking out a specific set of nodes. This approach has been widely used in real-life situations, such as controlling epidemics and containing rumors. However, traditional methods only consider pairwise interactions, while hypernetworks, which involve group interactions, are critical and commonly used. This study presents a deep reinforcement learning-based framework for hypernetwork dismantling, which formulates the hypernetwork dismantling problem as a node sequence decision problem. The framework includes a novel inductive hypernetwork embedding method that guarantees its applicability to various real-world hypernetworks. The proposed framework builds an agent that uses trial-and-error dismantling tasks on small-scale synthetic hypernetworks to optimize dismantling strategies. The optimized strategy is then applied to actual hypernetwork dismantling tasks. Five real-world hypernetwork experiments demonstrate the effectiveness of the proposed framework.",1
"The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing problem formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose PolyGym, a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found transformations that led to a speedup of 3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our generic MDP formulation enables using reinforcement learning to learn optimization policies over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.",0
"The structured approach of the polyhedral model enables semantics-preserving transformations that enhance the performance of many loops. However, finding profitable points in this space is a challenging task that is typically tackled by heuristics that rely on domain-expert knowledge. The problem with existing heuristics is that they are dependent on the shape of specific loops, limiting the ability to use more powerful optimization techniques from the machine learning domain. To address this issue, we present PolyGym, which is a shape-agnostic formulation for the space of legal transformations in the polyhedral model that uses a Markov Decision Process (MDP). Instead of transformations, PolyGym is based on an abstract space of possible schedules, where states model partial schedules constructed by reusable actions across different loops. By using a simple heuristic to navigate the space, we demonstrate that our formulation is more powerful than state-of-the-art heuristics. Our MDP formulation is generic, allowing for reinforcement learning to learn optimization policies over a wide range of loops, which contributes to the field of machine learning in compilers. Furthermore, our approach exposes a novel problem formulation that can expand the limits of existing methods. We found that PolyGym produced a speedup of 3.39x over LLVM O3 and outperformed ISL with a speedup of 1.83x on the Polybench benchmark suite.",1
"In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.",0
"This article examines reinforcement learning tasks that involve rewards that increase in magnitude over time. The authors suggest that this type of task may pose challenges for value-based deep reinforcement learning agents, especially if the agent must first succeed in less rewarding areas of the task before reaching more rewarding areas. To address this issue, the authors present Spectral DQN, a technique that breaks down the reward into frequencies so that high frequencies only activate when large rewards are obtained. This allows for a more balanced training loss that considers both small and large reward regions. Spectral DQN was tested on two domains with extreme reward progressivity, where standard value-based methods struggled significantly. The results showed that Spectral DQN was able to make much greater progress. Additionally, when evaluated on a set of six standard Atari games, Spectral DQN performed well and surpassed benchmarks in three games. These findings suggest that Spectral DQN may have advantages beyond addressing reward progressivity and is not limited to the target problem.",1
"Model-based reinforcement learning (MBRL) algorithms can attain significant sample efficiency but require an appropriate network structure to represent system dynamics. Current approaches include white-box modeling using analytic parameterizations and black-box modeling using deep neural networks. However, both can suffer from a bias-variance trade-off in the learning process, and neither provides a structured method for injecting domain knowledge into the network. As an alternative, gray-box modeling leverages prior knowledge in neural network training but only for simple systems. In this paper, we devise a nested mixture of experts (NMOE) for representing and learning hybrid dynamical systems. An NMOE combines both white-box and black-box models while optimizing bias-variance trade-off. Moreover, an NMOE provides a structured method for incorporating various types of prior knowledge by training the associative experts cooperatively or competitively. The prior knowledge includes information on robots' physical contacts with the environments as well as their kinematic and dynamic properties. In this paper, we demonstrate how to incorporate prior knowledge into our NMOE in various continuous control domains, including hybrid dynamical systems. We also show the effectiveness of our method in terms of data-efficiency, generalization to unseen data, and bias-variance trade-off. Finally, we evaluate our NMOE using an MBRL setup, where the model is integrated with a model-based controller and trained online.",0
"Although model-based reinforcement learning (MBRL) algorithms possess significant sample efficiency, they necessitate a suitable network structure to represent system dynamics. Currently, white-box modeling using analytic parameterizations and black-box modeling using deep neural networks are prevalent approaches. Yet, both encounter a bias-variance trade-off during the learning process, and neither employs a structured approach for infusing domain knowledge into the network. Gray-box modeling, which leverages prior knowledge for neural network training, is an alternative, but it only suits simple systems. This research proposes a nested mixture of experts (NMOE) for learning and representing hybrid dynamical systems. NMOE combines both black-box and white-box models while optimizing the bias-variance trade-off. Additionally, NMOE provides a systematic approach to integrate various forms of prior knowledge by training the associative experts cooperatively or competitively, including information regarding robots' physical contacts with the environment and their kinematic and dynamic characteristics. This research showcases how prior knowledge can be incorporated into NMOE in various continuous control domains, including hybrid dynamical systems. The research also establishes the efficiency of the proposed method in terms of data-efficiency, generalization to unseen data, and bias-variance trade-off. Finally, the NMOE is assessed in an MBRL setup, where the model is integrated with a model-based controller and trained online.",1
"Communication lays the foundation for human cooperation. It is also crucial for multi-agent cooperation. However, existing work focuses on broadcast communication, which is not only impractical but also leads to information redundancy that could even impair the learning process. To tackle these difficulties, we propose Individually Inferred Communication (I2C), a simple yet effective model to enable agents to learn a prior for agent-agent communication. The prior knowledge is learned via causal inference and realized by a feed-forward neural network that maps the agent's local observation to a belief about who to communicate with. The influence of one agent on another is inferred via the joint action-value function in multi-agent reinforcement learning and quantified to label the necessity of agent-agent communication. Furthermore, the agent policy is regularized to better exploit communicated messages. Empirically, we show that I2C can not only reduce communication overhead but also improve the performance in a variety of multi-agent cooperative scenarios, comparing to existing methods. The code is available at https://github.com/PKU-AI-Edge/I2C.",0
"Human cooperation is built upon communication, which is also vital for multi-agent cooperation. However, current approaches to communication focus on broadcast communication, which is not practical and can lead to information redundancy that hinders learning. To address these challenges, we propose Individually Inferred Communication (I2C), a straightforward yet effective model that helps agents learn a prior for agent-agent communication. This prior knowledge is acquired using causal inference and implemented through a feed-forward neural network that maps an agent's local observation to a belief on whom to communicate with. The effect of one agent on another is inferred through the joint action-value function in multi-agent reinforcement learning, and the need for agent-agent communication is quantified. Additionally, the agent policy is regulated to better leverage communicated messages. Our empirical results demonstrate that I2C can reduce communication overhead and enhance performance in various multi-agent cooperative scenarios compared to existing methods. The code is available on https://github.com/PKU-AI-Edge/I2C.",1
"The antagonistic behavior of the crowd often exacerbates the seriousness of the situation in sudden riots, where the spreading of antagonistic emotion and behavioral decision making in the crowd play very important roles. However, the mechanism of complex emotion influencing decision making, especially in the environment of sudden confrontation, has not yet been explored clearly. In this paper, we propose one new antagonistic crowd simulation model by combing emotional contagion and deep reinforcement learning (ACSED). Firstly, we build a group emotional contagion model based on the improved SIS contagion disease model, and estimate the emotional state of the group at each time step during the simulation. Then, the tendency of group antagonistic behavior is modeled based on Deep Q Network (DQN), where the agent can learn the combat behavior autonomously, and leverages the mean field theory to quickly calculate the influence of other surrounding individuals on the central one. Finally, the rationality of the predicted behaviors by the DQN is further analyzed in combination with group emotion, and the final combat behavior of the agent is determined. The method proposed in this paper is verified through several different settings of experiments. The results prove that emotions have a vital impact on the group combat, and positive emotional states are more conducive to combat. Moreover, by comparing the simulation results with real scenes, the feasibility of the method is further verified, which can provide good reference for formulating battle plans and improving the winning rate of righteous groups battles in a variety of situations.",0
"In sudden riots, the behavior of the crowd can worsen the situation due to the spread of antagonistic emotions and decision making. However, the mechanism behind this complex emotional influence on decision making in confrontational situations is not well understood. To address this gap, we propose a new model called ACSED, which combines emotional contagion and deep reinforcement learning. We first develop a group emotional contagion model based on an improved SIS contagion disease model to estimate the emotional state of the group during the simulation. Next, we model the tendency of group antagonistic behavior using Deep Q Network (DQN), which allows the agent to learn combat behavior autonomously. We also leverage the mean field theory to quickly calculate the influence of surrounding individuals on the central one. Finally, we analyze the rationality of the predicted behaviors by the DQN in combination with group emotion to determine the final combat behavior of the agent. We conduct several experiments to verify the proposed method, and the results demonstrate that positive emotional states are more conducive to combat. By comparing the simulation results with real scenes, we further verify the feasibility of the method, which can provide valuable guidance for formulating battle plans and improving the winning rate of righteous groups battles in various situations.",1
"Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.",0
"Reinforcement learning (RL) has made significant progress in recent years, achieving notable success in solving various sequential decision-making problems in machine learning, such as the games of Go and Poker, robotics, and autonomous driving. Multi-agent RL (MARL) has emerged as a natural extension of RL, as many successful applications involve more than one agent. Although empirical success has been achieved, theoretical foundations for MARL are relatively lacking in the literature. This chapter provides a selective overview of MARL, focusing on algorithms that are backed by theoretical analysis. The theoretical results of MARL algorithms are reviewed mainly within two frameworks: Markov/stochastic games and extensive-form games. The types of tasks addressed include fully cooperative, fully competitive, and a mix of the two. Several significant but challenging applications of these algorithms are also introduced. This chapter highlights several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, and (non-)convergence of policy-based methods for learning in games. The overall goal is to identify fruitful future research directions on theoretical studies of MARL, and to serve as a continuing stimulus for researchers interested in this exciting but challenging topic.",1
"The celebrated Monte Carlo method estimates an expensive-to-compute quantity by random sampling. Bandit-based Monte Carlo optimization is a general technique for computing the minimum of many such expensive-to-compute quantities by adaptive random sampling. The technique converts an optimization problem into a statistical estimation problem which is then solved via multi-armed bandits. We apply this technique to solve the problem of high-dimensional $k$-nearest neighbors, developing an algorithm which we prove is able to identify exact nearest neighbors with high probability. We show that under regularity assumptions on a dataset of $n$ points in $d$-dimensional space, the complexity of our algorithm scales logarithmically with the dimension of the data as $O\left((n+d)\log^2 \left(\frac{nd}{\delta}\right)\right)$ for error probability $\delta$, rather than linearly as in exact computation requiring $O(nd)$. We corroborate our theoretical results with numerical simulations, showing that our algorithm outperforms both exact computation and state-of-the-art algorithms such as kGraph, NGT, and LSH on real datasets.",0
"An expensive-to-compute quantity can be estimated by random sampling using the renowned Monte Carlo method. Bandit-based Monte Carlo optimization is a versatile technique that utilizes adaptive random sampling to calculate the minimum of numerous expensive-to-compute quantities. This technique transforms an optimization issue into a statistical estimation problem that can be solved using multi-armed bandits. Our study applies this technique to address the challenge of high-dimensional $k$-nearest neighbors and establishes an algorithm that can identify exact nearest neighbors with high probability. We demonstrate that our algorithm's complexity scales logarithmically with the dimension of the data, as opposed to linearly as in exact computation, and we prove it under regularity assumptions on a dataset of $n$ points in $d$-dimensional space. The complexity is $O\left((n+d)\log^2 \left(\frac{nd}{\delta}\right)\right)$ for error probability $\delta$. We support our theoretical findings with numerical simulations, which indicate that our algorithm performs better than exact computation and state-of-the-art algorithms like kGraph, NGT, and LSH on real datasets.",1
"This paper considers the problem of reward design for autonomous driving (AD), with insights that are also applicable to the design of cost functions and performance metrics more generally. Herein we develop 8 simple sanity checks for identifying flaws in reward functions. The sanity checks are applied to reward functions from past work on reinforcement learning (RL) for autonomous driving, revealing near-universal flaws in reward design for AD that might also exist pervasively across reward design for other tasks. Lastly, we explore promising directions that may help future researchers design reward functions for AD.",0
"This article examines the challenge of creating appropriate rewards for autonomous driving (AD) and how this can inform the development of cost functions and performance metrics more broadly. The authors present eight straightforward checks to identify weaknesses in reward functions and apply them to past studies on reinforcement learning (RL) for AD. The results highlight widespread issues in reward design for AD that may extend to other tasks. Finally, the authors suggest potential avenues for future research to improve reward function design for AD.",1
"Many reinforcement learning algorithms rely on value estimation. However, the most widely used algorithms -- namely temporal difference algorithms -- can diverge under both off-policy sampling and nonlinear function approximation. Many algorithms have been developed for off-policy value estimation which are sound under linear function approximation, based on the linear mean-squared projected Bellman error (PBE). Extending these methods to the non-linear case has been largely unsuccessful. Recently, several methods have been introduced that approximate a different objective, called the mean-squared Bellman error (BE), which naturally facilities nonlinear approximation. In this work, we build on these insights and introduce a new generalized PBE, that extends the linear PBE to the nonlinear setting. We show how this generalized objective unifies previous work, including previous theory, and obtain new bounds for the value error of the solutions of the generalized objective. We derive an easy-to-use, but sound, algorithm to minimize the generalized objective which is more stable across runs, is less sensitive to hyperparameters, and performs favorably across four control domains with neural network function approximation.",0
"Value estimation is a crucial component of many reinforcement learning algorithms. However, the commonly used temporal difference algorithms can become problematic when dealing with off-policy sampling and nonlinear function approximation. While several off-policy value estimation algorithms have been developed for linear function approximation based on the linear mean-squared projected Bellman error, extending these methods to the non-linear case has been challenging. Recently, new methods have been introduced to approximate the mean-squared Bellman error, which is more suitable for nonlinear approximation. This study builds on these insights and introduces a new generalized PBE that extends the linear PBE to the nonlinear setting. The generalized objective unifies previous work and theory and provides new bounds for the value error of the solutions. The study also presents an easy-to-use and reliable algorithm to minimize the generalized objective, which outperforms previous methods across four control domains with neural network function approximation.",1
"The combination of deep learning and Monte Carlo Tree Search (MCTS) has shown to be effective in various domains, such as board and video games. AlphaGo represented a significant step forward in our ability to learn complex board games, and it was rapidly followed by significant advances, such as AlphaGo Zero and AlphaZero. Recently, MuZero demonstrated that it is possible to master both Atari games and board games by directly learning a model of the environment, which is then used with MCTS to decide what move to play in each position. During tree search, the algorithm simulates games by exploring several possible moves and then picks the action that corresponds to the most promising trajectory. When training, limited use is made of these simulated games since none of their trajectories are directly used as training examples. Even if we consider that not all trajectories from simulated games are useful, there are thousands of potentially useful trajectories that are discarded. Using information from these trajectories would provide more training data, more quickly, leading to faster convergence and higher sample efficiency. Recent work introduced an off-policy value target for AlphaZero that uses data from simulated games. In this work, we propose a way to obtain off-policy targets using data from simulated games in MuZero. We combine these off-policy targets with the on-policy targets already used in MuZero in several ways, and study the impact of these targets and their combinations in three environments with distinct characteristics. When used in the right combinations, our results show that these targets can speed up the training process and lead to faster convergence and higher rewards than the ones obtained by MuZero.",0
"The application of deep learning and Monte Carlo Tree Search (MCTS) has been successful in various fields, including board and video games. AlphaGo was a significant breakthrough in our ability to master complex board games, and subsequent advances such as AlphaGo Zero and AlphaZero have followed. MuZero has recently demonstrated that it can learn both Atari games and board games by learning a model of the environment, which is used in conjunction with MCTS to determine the best move in each position. During the tree search, the algorithm explores multiple possible moves, simulating games and selecting the most promising trajectory. However, this approach discards potentially useful trajectories from simulated games, which could offer more training data and improve convergence and sample efficiency. Recent work has introduced an off-policy value target for AlphaZero that uses data from simulated games, and we propose a similar approach for MuZero. By combining these off-policy targets with the on-policy targets already used in MuZero, we can speed up the training process and achieve faster convergence and higher rewards in various environments.",1
"Reinforcement learning (RL) enjoyed significant progress over the last years. One of the most important steps forward was the wide application of neural networks. However, architectures of these neural networks are typically constructed manually. In this work, we study recently proposed neural architecture search (NAS) methods for optimizing the architecture of RL agents. We carry out experiments on the Atari benchmark and conclude that modern NAS methods find architectures of RL agents outperforming a manually selected one.",0
"Significant progress has been made in Reinforcement Learning (RL) in recent years, largely due to the widespread use of neural networks. However, the architecture of these networks is typically created manually. This study examines newly developed Neural Architecture Search (NAS) methods for optimizing RL agent architecture. We conducted experiments on the Atari benchmark and found that modern NAS methods discovered RL agent architectures that outperformed manually selected ones.",1
"This paper presents new machine learning approaches to approximate the solution of optimal stopping problems. The key idea of these methods is to use neural networks, where the hidden layers are generated randomly and only the last layer is trained, in order to approximate the continuation value. Our approaches are applicable for high dimensional problems where the existing approaches become increasingly impractical. In addition, since our approaches can be optimized using a simple linear regression, they are very easy to implement and theoretical guarantees can be provided. In Markovian examples our randomized reinforcement learning approach and in non-Markovian examples our randomized recurrent neural network approach outperform the state-of-the-art and other relevant machine learning approaches.",0
"New methods are presented in this paper for approximating the solution of optimal stopping problems using machine learning. These methods utilize neural networks, with randomly generated hidden layers and training only on the last layer, to approximate the continuation value. They are particularly useful for high dimensional problems, where existing methods are impractical. Furthermore, these approaches are easy to implement and provide theoretical guarantees through simple linear regression optimization. Our randomized reinforcement learning approach performs well in Markovian examples, while our randomized recurrent neural network approach outperforms state-of-the-art and other relevant machine learning approaches in non-Markovian examples.",1
"An important topic in the autonomous driving research is the development of maneuver planning systems. Vehicles have to interact and negotiate with each other so that optimal choices, in terms of time and safety, are taken. For this purpose, we present a maneuver planning module able to negotiate the entering in busy roundabouts. The proposed module is based on a neural network trained to predict when and how entering the roundabout throughout the whole duration of the maneuver. Our model is trained with a novel implementation of A3C, which we will call Delayed A3C (D-A3C), in a synthetic environment where vehicles move in a realistic manner with interaction capabilities. In addition, the system is trained such that agents feature a unique tunable behavior, emulating real world scenarios where drivers have their own driving styles. Similarly, the maneuver can be performed using different aggressiveness levels, which is particularly useful to manage busy scenarios where conservative rule-based policies would result in undefined waits.",0
"The development of maneuver planning systems is a crucial aspect of autonomous driving research. In order to make optimal decisions regarding time and safety, vehicles must be able to interact and negotiate with one another. Our focus is on the creation of a maneuver planning module that can handle the complexities of entering a busy roundabout. Our module is based on a neural network that has been trained using Delayed A3C (D-A3C) in a synthetic environment that mimics realistic vehicle interactions. We have also ensured that our system can emulate real-world scenarios where drivers have unique styles of driving, and can adjust the level of aggressiveness used in maneuvering to effectively manage busy situations where rule-based policies would result in extended waits.",1
"Learning a disentangled representation of the latent space has become one of the most fundamental problems studied in computer vision. Recently, many Generative Adversarial Networks (GANs) have shown promising results in generating high fidelity images. However, studies to understand the semantic layout of the latent space of pre-trained models are still limited. Several works train conditional GANs to generate faces with required semantic attributes. Unfortunately, in these attempts, the generated output is often not as photo-realistic as the unconditional state-of-the-art models. Besides, they also require large computational resources and specific datasets to generate high fidelity images. In our work, we have formulated a Markov Decision Process (MDP) over the latent space of a pre-trained GAN model to learn a conditional policy for semantic manipulation along specific attributes under defined identity bounds. Further, we have defined a semantic age manipulation scheme using a locally linear approximation over the latent space. Results show that our learned policy samples high fidelity images with required age alterations, while preserving the identity of the person.",0
"The computer vision field has been focusing on acquiring a disentangled representation of the latent space, which is considered one of the most essential issues. Various Generative Adversarial Networks (GANs) have demonstrated their potential in producing high-quality images. Nevertheless, there is still a lack of research on comprehending the semantic structure of pre-trained models' latent space. Some researchers have utilized conditional GANs to create faces with specific semantic attributes. However, these experiments have not been as successful in generating realistic images as the unconditional state-of-the-art models, and they require extensive computational resources and specific data sets. Our study introduces a Markov Decision Process (MDP) that develops a conditional policy for manipulating specific semantic attributes within defined identity boundaries. We have also created a semantic age manipulation scheme by utilizing a locally linear approximation over the latent space. Our results indicate that our learned policy generates high-quality images with accurate age alterations while maintaining the person's identity.",1
"Meta Reinforcement Learning (MRL) enables an agent to learn from a limited number of past trajectories and extrapolate to a new task. In this paper, we attempt to improve the robustness of MRL. We build upon model-agnostic meta-learning (MAML) and propose a novel method to generate adversarial samples for MRL by using Generative Adversarial Network (GAN). That allows us to enhance the robustness of MRL to adversal attacks by leveraging these attacks during meta training process.",0
"The objective of this study is to enhance the resilience of Meta Reinforcement Learning (MRL) by exploring the potential of model-agnostic meta-learning (MAML). We aim to achieve this by leveraging Generative Adversarial Network (GAN) to generate adversarial samples for MRL. By incorporating adversarial attacks into the meta training procedure, we intend to improve the ability of MRL to learn from a limited number of past trajectories and generalize to new tasks.",1
"Building embodied autonomous agents capable of participating in social interactions with humans is one of the main challenges in AI. This problem motivated many research directions on embodied language use. Current approaches focus on language as a communication tool in very simplified and non diverse social situations: the ""naturalness"" of language is reduced to the concept of high vocabulary size and variability. In this paper, we argue that aiming towards human-level AI requires a broader set of key social skills: 1) language use in complex and variable social contexts; 2) beyond language, complex embodied communication in multimodal settings within constantly evolving social worlds. In this work we explain how concepts from cognitive sciences could help AI to draw a roadmap towards human-like intelligence, with a focus on its social dimensions. We then study the limits of a recent SOTA Deep RL approach when tested on a first grid-world environment from the upcoming SocialAI, a benchmark to assess the social skills of Deep RL agents. Videos and code are available at https://sites.google.com/view/socialai01 .",0
"Developing autonomous agents that are embodied and capable of social interactions with humans is a significant challenge in the field of AI. As a result, many research directions have focused on embodied language use. However, current approaches only consider language as a communication tool in simplistic and limited social situations, reducing the ""naturalness"" of language to vocabulary size and variability. To achieve human-level AI, it is necessary to expand the range of social skills required, including language use in complex and diverse social contexts, as well as complex embodied communication in constantly evolving multimodal settings. In this paper, we propose that adopting concepts from cognitive sciences can help AI progress towards human-like intelligence, with particular emphasis on social dimensions. We also investigate the limitations of a recent state-of-the-art Deep RL approach when tested on a grid-world environment from the upcoming SocialAI benchmark, which assesses the social skills of Deep RL agents. Videos and code related to this work can be found at https://sites.google.com/view/socialai01.",1
"From cutting costs to improving customer experience, forecasting is the crux of retail supply chain management (SCM) and the key to better supply chain performance. Several retailers are using AI/ML models to gather datasets and provide forecast guidance in applications such as Cognitive Demand Forecasting, Product End-of-Life, Forecasting, and Demand Integrated Product Flow. Early work in these areas looked at classical algorithms to improve on a gamut of challenges such as network flow and graphs. But the recent disruptions have made it critical for supply chains to have the resiliency to handle unexpected events. The biggest challenge lies in matching supply with demand.   Reinforcement Learning (RL) with its ability to train systems to respond to unforeseen environments, is being increasingly adopted in SCM to improve forecast accuracy, solve supply chain optimization challenges, and train systems to respond to unforeseen circumstances. Companies like UPS and Amazon have developed RL algorithms to define winning AI strategies and keep up with rising consumer delivery expectations. While there are many ways to build RL algorithms for supply chain use cases, the OpenAI Gym toolkit is becoming the preferred choice because of the robust framework for event-driven simulations.   This white paper explores the application of RL in supply chain forecasting and describes how to build suitable RL models and algorithms by using the OpenAI Gym toolkit.",0
"Forecasting is the foundation of retail supply chain management (SCM) and is crucial for improving customer experience and reducing costs. Retailers are now using AI/ML models to collect datasets and generate forecast guidance for Cognitive Demand Forecasting, Product End-of-Life Forecasting, and Demand Integrated Product Flow. While earlier work focused on classical algorithms to tackle network flow and graph-related challenges, recent disruptions have made it essential for supply chains to be resilient to handle unexpected events. The biggest hurdle is matching supply with demand. To improve forecast accuracy, solve supply chain optimization issues, and prepare systems to respond to unforeseen situations, Reinforcement Learning (RL) is being increasingly employed in SCM. Companies like UPS and Amazon have developed RL algorithms to keep up with evolving consumer delivery expectations. The OpenAI Gym toolkit is becoming a preferred choice for building RL algorithms for supply chain use cases due to its robust simulation framework. This white paper examines the use of RL in supply chain forecasting and provides guidance on creating appropriate RL models and algorithms using the OpenAI Gym toolkit.",1
"Safe exploration presents a major challenge in reinforcement learning (RL): when active data collection requires deploying partially trained policies, we must ensure that these policies avoid catastrophically unsafe regions, while still enabling trial and error learning. In this paper, we target the problem of safe exploration in RL by learning a conservative safety estimate of environment states through a critic, and provably upper bound the likelihood of catastrophic failures at every training iteration. We theoretically characterize the tradeoff between safety and policy improvement, show that the safety constraints are likely to be satisfied with high probability during training, derive provable convergence guarantees for our approach, which is no worse asymptotically than standard RL, and demonstrate the efficacy of the proposed approach on a suite of challenging navigation, manipulation, and locomotion tasks. Empirically, we show that the proposed approach can achieve competitive task performance while incurring significantly lower catastrophic failure rates during training than prior methods. Videos are at this url https://sites.google.com/view/conservative-safety-critics/home",0
"Reinforcement learning (RL) faces a significant challenge in ensuring safe exploration. Deploying partially trained policies for data collection requires avoiding unsafe areas while allowing for trial and error learning. This paper addresses the problem of safe exploration in RL by using a critic to learn a conservative safety estimate of environment states. The likelihood of catastrophic failures is upper bounded at every training iteration. The tradeoff between safety and policy improvement is theoretically characterized, with high probability of safety constraints being satisfied during training. The approach is no worse asymptotically than standard RL, with provable convergence guarantees, and is demonstrated on challenging tasks. The proposed approach achieves competitive task performance with significantly lower catastrophic failure rates during training than prior methods. Videos can be viewed at this URL: https://sites.google.com/view/conservative-safety-critics/home.",1
"Federated learning is a new learning paradigm that decouples data collection and model training via multi-party computation and model aggregation. As a flexible learning setting, federated learning has the potential to integrate with other learning frameworks. We conduct a focused survey of federated learning in conjunction with other learning algorithms. Specifically, we explore various learning algorithms to improve the vanilla federated averaging algorithm and review model fusion methods such as adaptive aggregation, regularization, clustered methods, and Bayesian methods. Following the emerging trends, we also discuss federated learning in the intersection with other learning paradigms, termed as federated x learning, where x includes multitask learning, meta-learning, transfer learning, unsupervised learning, and reinforcement learning. This survey reviews the state of the art, challenges, and future directions.",0
"Federated learning is a novel approach to learning that separates the data collection and model training through multi-party computation and model aggregation. This adaptable learning method can be combined with other learning frameworks, which we investigate in our focused survey of federated learning. Our study delves into various learning algorithms that enhance the vanilla federated averaging algorithm and examines model fusion techniques like adaptive aggregation, regularization, clustered methods, and Bayesian methods. Additionally, we explore the intersection between federated learning and other learning paradigms, dubbed as federated x learning. These include multitask learning, meta-learning, transfer learning, unsupervised learning, and reinforcement learning. Our comprehensive survey evaluates the current state of the art, obstacles, and future prospects.",1
"We introduce a new unsupervised pre-training method for reinforcement learning called APT, which stands for Active Pre-Training. APT learns behaviors and representations by actively searching for novel states in reward-free environments. The key novel idea is to explore the environment by maximizing a non-parametric entropy computed in an abstract representation space, which avoids the challenging density modeling and consequently allows our approach to scale much better in environments that have high-dimensional observations (e.g., image observations). We empirically evaluate APT by exposing task-specific reward after a long unsupervised pre-training phase. On Atari games, APT achieves human-level performance on 12 games and obtains highly competitive performance compared to canonical fully supervised RL algorithms. On DMControl suite, APT beats all baselines in terms of asymptotic performance and data efficiency and dramatically improves performance on tasks that are extremely difficult to train from scratch.",0
"We present a new technique for unsupervised pre-training in reinforcement learning, which we call Active Pre-Training (APT). APT seeks out new states in environments without rewards to learn behaviors and representations. A key innovation is exploring the environment by maximizing a non-parametric entropy in an abstract representation space, making it easier to scale in high-dimensional observation environments. To evaluate APT, we reward it after extensive unsupervised pre-training. APT achieves human-level performance on 12 Atari games and competes well with fully supervised RL algorithms. On DMControl suite, APT outperforms all baselines in asymptotic performance and data efficiency and significantly improves performance on challenging scratch-trained tasks.",1
"This research is concerned with the novel application and investigation of `Soft Actor Critic' (SAC) based Deep Reinforcement Learning (DRL) to control the cooling setpoint (and hence cooling loads) of a large commercial building to harness energy flexibility. The research is motivated by the challenge associated with the development and application of conventional model-based control approaches at scale to the wider building stock. SAC is a model-free DRL technique that is able to handle continuous action spaces and which has seen limited application to real-life or high-fidelity simulation implementations in the context of automated and intelligent control of building energy systems. Such control techniques are seen as one possible solution to supporting the operation of a smart, sustainable and future electrical grid. This research tests the suitability of the SAC DRL technique through training and deployment of the agent on an EnergyPlus based environment of the office building. The SAC DRL was found to learn an optimal control policy that was able to minimise energy costs by 9.7% compared to the default rule-based control (RBC) scheme and was able to improve or maintain thermal comfort limits over a test period of one week. The algorithm was shown to be robust to the different hyperparameters and this optimal control policy was learnt through the use of a minimal state space consisting of readily available variables. The robustness of the algorithm was tested through investigation of the speed of learning and ability to deploy to different seasons and climates. It was found that the SAC DRL requires minimal training sample points and outperforms the RBC after three months of operation and also without disruption to thermal comfort during this period. The agent is transferable to other climates and seasons although further retraining or hyperparameter tuning is recommended.",0
"The study aims to explore the innovative application and analysis of a Deep Reinforcement Learning method called Soft Actor Critic (SAC) in regulating the cooling setpoint and managing cooling loads of a large commercial building to leverage energy flexibility. The study is motivated by the difficulties in implementing traditional model-based control techniques on a larger scale to a wide range of buildings. SAC is a model-free DRL technique that is capable of managing continuous action spaces and has been sparingly applied to real-life or high-fidelity simulation environments in the context of intelligent control of building energy systems. This approach is considered as a possible solution to support a smart, eco-friendly, and future electrical grid. To evaluate the effectiveness of the SAC DRL method, the agent was trained and deployed on an EnergyPlus-based office building environment. The results showed that the SAC DRL technique learned an optimal control policy that considerably reduced energy costs by 9.7% compared to the rule-based control scheme. Moreover, the SAC DRL method was successful in maintaining thermal comfort limits over a test period of one week. The algorithm proved to be robust to different hyperparameters and was trained using a minimal state space consisting of readily available variables. The study also demonstrated that the SAC DRL technique requires minimal training sample points and outperforms the rule-based control scheme after three months of operation, without disrupting thermal comfort during this period. The technique is also adaptable to different climates and seasons, although retraining or hyperparameter tuning may be necessary.",1
"Reinforcement learning (RL) provides an appealing formalism for learning control policies from experience. However, the classic active formulation of RL necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings such as robotic control. If we can instead allow RL algorithms to effectively use previously collected data to aid the online learning process, such applications could be made substantially more practical: the prior data would provide a starting point that mitigates challenges due to exploration and sample complexity, while the online training enables the agent to perfect the desired skill. Such prior data could either constitute expert demonstrations or sub-optimal prior data that illustrates potentially useful transitions. While a number of prior methods have either used optimal demonstrations to bootstrap RL, or have used sub-optimal data to train purely offline, it remains exceptionally difficult to train a policy with offline data and actually continue to improve it further with online RL. In this paper we analyze why this problem is so challenging, and propose an algorithm that combines sample efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of RL policies. We show that our method, advantage weighted actor critic (AWAC), enables rapid learning of skills with a combination of prior demonstration data and online experience. We demonstrate these benefits on simulated and real-world robotics domains, including dexterous manipulation with a real multi-fingered hand, drawer opening with a robotic arm, and rotating a valve. Our results show that incorporating prior data can reduce the time required to learn a range of robotic skills to practical time-scales.",0
"Learning control policies from experience is facilitated by reinforcement learning (RL), which has become a popular formalism. However, the traditional active approach of RL requires time-consuming exploration for each behavior, making it impractical for real-world applications like robotic control. To overcome this challenge, RL algorithms could effectively utilize previously collected data to assist the online learning process, making such applications more feasible. This prior data could consist of expert demonstrations or sub-optimal data that demonstrates potentially useful transitions. Although previous methods have used optimal demonstrations to bootstrap RL or sub-optimal data to train purely offline, it is still challenging to train a policy with offline data and continue to enhance it further with online RL. This paper proposes an algorithm that combines sample-efficient dynamic programming with maximum likelihood policy updates, providing a straightforward and effective framework that can leverage large amounts of offline data and then quickly perform online fine-tuning of RL policies. The advantage weighted actor-critic (AWAC) method enables rapid learning of skills with a combination of prior demonstration data and online experience. The benefits of incorporating prior data are demonstrated on simulated and real-world robotics domains, including dexterous manipulation with a real multi-fingered hand, drawer opening with a robotic arm, and rotating a valve. The results show that incorporating prior data can reduce the time required to learn a range of robotic skills to practical timescales.",1
"The classic objective in a reinforcement learning (RL) problem is to find a policy that minimizes, in expectation, a long-run objective such as the infinite-horizon cumulative discounted or long-run average cost. In many practical applications, optimizing the expected value alone is not sufficient, and it may be necessary to include a risk measure in the optimization process, either in the objective or as a constraint. Various risk measures have been proposed in the literature, e.g., variance, exponential utility, percentile performance, chance constraints, value at risk (quantile), conditional value-at-risk, coherent risk measure, prospect theory and its later enhancement, cumulative prospect theory. In this article, we focus on the combination of risk criteria and reinforcement learning in a constrained optimization framework, i.e., a setting where the goal to find a policy that optimizes the usual objective of infinite-horizon discounted/average cost, while ensuring that an explicit risk constraint is satisfied. We introduce the risk-constrained RL framework, cover popular risk measures based on variance, conditional value-at-risk, and chance constraints, and present a template for a risk-sensitive RL algorithm. Next, we study risk-sensitive RL with the objective of minimizing risk in an unconstrained framework, and cover cumulative prospect theory and coherent risk measures as special cases. We survey some of the recent work on this topic, covering problems encompassing discounted cost, average cost, and stochastic shortest path settings, together with the aforementioned risk measures, in constrained as well as unconstrained frameworks. This non-exhaustive survey is aimed at giving a flavor of the challenges involved in solving risk-sensitive RL problems, and outlining some potential future research directions.",0
"The primary aim of reinforcement learning (RL) is to discover a policy that can minimize a long-term objective such as the cumulative discounted or long-run average cost. However, in practical scenarios, optimizing the expected value is often inadequate, and it becomes necessary to integrate a risk measure in the optimization process either as a constraint or in the objective. Different risk measures such as variance, conditional value-at-risk, and chance constraints have been proposed in literature. This article concentrates on the combination of risk criteria and RL in a constrained optimization framework, where the objective is to find a policy that optimizes the infinite-horizon discounted/average cost objective and meets an explicit risk constraint. The risk-constrained RL framework is introduced, and a popular template for a risk-sensitive RL algorithm is presented. Furthermore, the article discusses risk-sensitive RL in an unconstrained framework, where the aim is to minimize risk, and it covers cumulative prospect theory and coherent risk measures as special cases. Recent research work on this topic is surveyed, encompassing problems in discounted cost, average cost, and stochastic shortest path settings, along with the aforementioned risk measures, in constrained as well as unconstrained frameworks. The survey aims to provide an overview of the challenges involved in solving risk-sensitive RL problems and outline some potential areas for future research.",1
"We consider a meal delivery service fulfilling dynamic customer requests given a set of couriers over the course of a day. A courier's duty is to pick-up an order from a restaurant and deliver it to a customer. We model this service as a Markov decision process and use deep reinforcement learning as the solution approach. We experiment with the resulting policies on synthetic and real-world datasets and compare those with the baseline policies. We also examine the courier utilization for different numbers of couriers. In our analysis, we specifically focus on the impact of the limited available resources in the meal delivery problem. Furthermore, we investigate the effect of intelligent order rejection and re-positioning of the couriers. Our numerical experiments show that, by incorporating the geographical locations of the restaurants, customers, and the depot, our model significantly improves the overall service quality as characterized by the expected total reward and the delivery times. Our results present valuable insights on both the courier assignment process and the optimal number of couriers for different order frequencies on a given day. The proposed model also shows a robust performance under a variety of scenarios for real-world implementation.",0
"We examine a meal delivery service that caters to dynamic customer requests with a team of couriers over the course of a day. The courier's task is to collect an order from a restaurant and deliver it to a customer. Our solution approach models this service as a Markov decision process and leverages deep reinforcement learning. We conduct experiments using synthetic and real-world datasets and compare the results with baseline policies. Our analysis focuses on the impact of limited resources in the meal delivery problem and investigates the effect of intelligent order rejection and courier re-positioning. We specifically consider courier utilization for different numbers of couriers. Our numerical experiments reveal that incorporating the geographical locations of restaurants, customers, and the depot significantly improves overall service quality, as measured by expected total reward and delivery times. Our results provide valuable insights on the optimal courier assignment process and the number of couriers required for different order frequencies on a given day. The proposed model exhibits robust performance under various real-world scenarios.",1
"Can we use reinforcement learning to learn general-purpose policies that can perform a wide range of different tasks, resulting in flexible and reusable skills? Contextual policies provide this capability in principle, but the representation of the context determines the degree of generalization and expressivity. Categorical contexts preclude generalization to entirely new tasks. Goal-conditioned policies may enable some generalization, but cannot capture all tasks that might be desired. In this paper, we propose goal distributions as a general and broadly applicable task representation suitable for contextual policies. Goal distributions are general in the sense that they can represent any state-based reward function when equipped with an appropriate distribution class, while the particular choice of distribution class allows us to trade off expressivity and learnability. We develop an off-policy algorithm called distribution-conditioned reinforcement learning (DisCo RL) to efficiently learn these policies. We evaluate DisCo RL on a variety of robot manipulation tasks and find that it significantly outperforms prior methods on tasks that require generalization to new goal distributions.",0
"Is it possible to utilize reinforcement learning to acquire versatile policies that are capable of performing diverse tasks, leading to adaptable and reusable abilities? Although contextual policies offer this potential in theory, the context representation determines the level of generalization and expressiveness. Categorical contexts prevent generalization to completely novel tasks. While goal-conditioned policies may allow for some generalization, they are unable to encompass all desired tasks. The present study suggests goal distributions as a universal and widely applicable task representation suitable for contextual policies. Goal distributions are universal because they can represent any state-based reward function when equipped with a suitable distribution class, and the distribution class selection enables us to balance expressiveness and learnability. We introduce an off-policy algorithm called distribution-conditioned reinforcement learning (DisCo RL) to efficiently learn these policies. We assess DisCo RL on various robot manipulation tasks, and we observe that it surpasses previous techniques on tasks that necessitate generalization to new goal distributions.",1
"Reinforcement Learning (RL) controllers have generated excitement within the control community. The primary advantage of RL controllers relative to existing methods is their ability to optimize uncertain systems independently of explicit assumption of process uncertainty. Recent focus on engineering applications has been directed towards the development of safe RL controllers. Previous works have proposed approaches to account for constraint satisfaction through constraint tightening from the domain of stochastic model predictive control. Here, we extend these approaches to account for plant-model mismatch. Specifically, we propose a data-driven approach that utilizes Gaussian processes for the offline simulation model and use the associated posterior uncertainty prediction to account for joint chance constraints and plant-model mismatch. The method is benchmarked against nonlinear model predictive control via case studies. The results demonstrate the ability of the methodology to account for process uncertainty, enabling satisfaction of joint chance constraints even in the presence of plant-model mismatch.",0
"RL controllers are generating excitement in the control community due to their ability to optimize uncertain systems without explicit assumptions of process uncertainty. However, there is a recent focus on developing safe RL controllers, which has led to proposed approaches for constraint satisfaction through constraint tightening from the domain of stochastic model predictive control. In this study, the authors extend these approaches to account for plant-model mismatch by proposing a data-driven approach that uses Gaussian processes for offline simulation models and associated posterior uncertainty predictions to account for joint chance constraints and plant-model mismatch. The method is benchmarked against nonlinear model predictive control through case studies, which demonstrate its ability to account for process uncertainty and satisfy joint chance constraints even in the presence of plant-model mismatch.",1
"Nitrogen fertilizers have a detrimental effect on the environment, which can be reduced by optimizing fertilizer management strategies. We implement an OpenAI Gym environment where a reinforcement learning agent can learn fertilization management policies using process-based crop growth models and identify policies with reduced environmental impact. In our environment, an agent trained with the Proximal Policy Optimization algorithm is more successful at reducing environmental impacts than the other baseline agents we present.",0
"By optimizing fertilizer management strategies, the adverse impact of nitrogen fertilizers on the environment can be minimized. To achieve this, we created an OpenAI Gym environment that employs process-based crop growth models to train a reinforcement learning agent in fertilization management policies. This agent can identify policies that have less negative environmental impact. Out of the baseline agents we tested, the Proximal Policy Optimization algorithm proved to be the most effective in reducing environmental impact.",1
"Reinforcement Learning (RL) algorithms can in principle acquire complex robotic skills by learning from large amounts of data in the real world, collected via trial and error. However, most RL algorithms use a carefully engineered setup in order to collect data, requiring human supervision and intervention to provide episodic resets. This is particularly evident in challenging robotics problems, such as dexterous manipulation. To make data collection scalable, such applications require reset-free algorithms that are able to learn autonomously, without explicit instrumentation or human intervention. Most prior work in this area handles single-task learning. However, we might also want robots that can perform large repertoires of skills. At first, this would appear to only make the problem harder. However, the key observation we make in this work is that an appropriately chosen multi-task RL setting actually alleviates the reset-free learning challenge, with minimal additional machinery required. In effect, solving a multi-task problem can directly solve the reset-free problem since different combinations of tasks can serve to perform resets for other tasks. By learning multiple tasks together and appropriately sequencing them, we can effectively learn all of the tasks together reset-free. This type of multi-task learning can effectively scale reset-free learning schemes to much more complex problems, as we demonstrate in our experiments. We propose a simple scheme for multi-task learning that tackles the reset-free learning problem, and show its effectiveness at learning to solve complex dexterous manipulation tasks in both hardware and simulation without any explicit resets. This work shows the ability to learn dexterous manipulation behaviors in the real world with RL without any human intervention.",0
"In theory, Reinforcement Learning (RL) algorithms can learn intricate robotic skills by using data collected through trial and error in the real world. However, the current RL algorithms require a specific setup for data collection, which necessitates human supervision and intervention for episodic resets. This limitation is particularly challenging in complex robotics problems, such as dexterous manipulation. To overcome this challenge, reset-free algorithms that can learn autonomously without explicit instrumentation or human intervention are essential. Previous research in this area has only focused on single-task learning, but we need robots that can perform multiple skills. Although this may seem like a harder problem, we propose that multi-task RL can alleviate the reset-free learning challenge and scale it to more complex problems. By learning multiple tasks simultaneously and sequencing them appropriately, we can learn all the tasks together reset-free. This multi-task approach can effectively solve the reset-free problem and scale it to more intricate problems. Our experiments demonstrate that our proposed multi-task learning scheme can solve complex dexterous manipulation tasks without any explicit resets, both in hardware and simulation. This research showcases the ability to learn dexterous manipulation behaviors in the real world using RL without any human intervention.",1
"Due to recent breakthroughs, reinforcement learning (RL) has demonstrated impressive performance in challenging sequential decision-making problems. However, an open question is how to make RL cope with partial observability which is prevalent in many real-world problems. Contrary to contemporary RL approaches, which focus mostly on improved memory representations or strong assumptions about the type of partial observability, we propose a simple but efficient approach that can be applied together with a wide variety of RL methods. Our main insight is that smoothly transitioning from full observability to partial observability during the training process yields a high performance policy. The approach, called partially observable guided reinforcement learning (PO-GRL), allows to utilize full state information during policy optimization without compromising the optimality of the final policy. A comprehensive evaluation in discrete partially observableMarkov decision process (POMDP) benchmark problems and continuous partially observable MuJoCo and OpenAI gym tasks shows that PO-GRL improves performance. Finally, we demonstrate PO-GRL in the ball-in-the-cup task on a real Barrett WAM robot under partial observability.",0
"Recent breakthroughs have led to impressive performance by reinforcement learning (RL) in challenging sequential decision-making problems. However, the issue of how to enable RL to handle partial observability, which is common in many real-world problems, remains unresolved. Rather than relying on improved memory representations or strong assumptions about the type of partial observability, our proposed approach, partially observable guided reinforcement learning (PO-GRL), is simple yet efficient and can be used with a wide range of RL methods. We have discovered that gradually transitioning from full observability to partial observability during the training process results in a high-performance policy. PO-GRL allows full state information to be utilized during policy optimization without compromising the optimality of the final policy. Comprehensive evaluations of the approach in discrete partially observable Markov decision process (POMDP) benchmark problems and continuous partially observable MuJoCo and OpenAI gym tasks have shown that PO-GRL improves performance. Finally, we demonstrate the effectiveness of PO-GRL in the ball-in-the-cup task performed by a real Barrett WAM robot under partial observability.",1
"Model-Free Reinforcement Learning has achieved meaningful results in stable environments but, to this day, it remains problematic in regime changing environments like financial markets. In contrast, model-based RL is able to capture some fundamental and dynamical concepts of the environment but suffer from cognitive bias. In this work, we propose to combine the best of the two techniques by selecting various model-based approaches thanks to Model-Free Deep Reinforcement Learning. Using not only past performance and volatility, we include additional contextual information such as macro and risk appetite signals to account for implicit regime changes. We also adapt traditional RL methods to real-life situations by considering only past data for the training sets. Hence, we cannot use future information in our training data set as implied by K-fold cross validation. Building on traditional statistical methods, we use the traditional ""walk-forward analysis"", which is defined by successive training and testing based on expanding periods, to assert the robustness of the resulting agent.   Finally, we present the concept of statistical difference's significance based on a two-tailed T-test, to highlight the ways in which our models differ from more traditional ones. Our experimental results show that our approach outperforms traditional financial baseline portfolio models such as the Markowitz model in almost all evaluation metrics commonly used in financial mathematics, namely net performance, Sharpe and Sortino ratios, maximum drawdown, maximum drawdown over volatility.",0
"Although Model-Free Reinforcement Learning has been successful in stable environments, it still faces challenges in dynamic environments like financial markets. Model-based RL, on the other hand, can capture important and changing environmental concepts but can be affected by cognitive bias. In this study, we propose a combination of model-based methods and Model-Free Deep Reinforcement Learning, utilizing contextual information such as macro and risk appetite signals to account for implicit regime changes. To adapt traditional RL methods to real-life situations, we train models using only past data, without future information, and use the ""walk-forward analysis"" technique to ensure model robustness. Additionally, we introduce the concept of statistical difference's significance based on a two-tailed T-test to highlight the differences between our models and more traditional ones. Our experimental results demonstrate that our approach outperforms traditional financial baseline models in various evaluation metrics, including net performance, Sharpe and Sortino ratios, maximum drawdown, and maximum drawdown over volatility.",1
"The adaptive traffic signal control (ATSC) problem can be modeled as a multiagent cooperative game among urban intersections, where intersections cooperate to optimize their common goal. Recently, reinforcement learning (RL) has achieved marked successes in managing sequential decision making problems, which motivates us to apply RL in the ASTC problem. Here we use independent reinforcement learning (IRL) to solve a complex traffic cooperative control problem in this study. One of the largest challenges of this problem is that the observation information of intersection is typically partially observable, which limits the learning performance of IRL algorithms. To this, we model the traffic control problem as a partially observable weak cooperative traffic model (PO-WCTM) to optimize the overall traffic situation of a group of intersections. Different from a traditional IRL task that averages the returns of all agents in fully cooperative games, the learning goal of each intersection in PO-WCTM is to reduce the cooperative difficulty of learning, which is also consistent with the traffic environment hypothesis. We also propose an IRL algorithm called Cooperative Important Lenient Double DQN (CIL-DDQN), which extends Double DQN (DDQN) algorithm using two mechanisms: the forgetful experience mechanism and the lenient weight training mechanism. The former mechanism decreases the importance of experiences stored in the experience reply buffer, which deals with the problem of experience failure caused by the strategy change of other agents. The latter mechanism increases the weight experiences with high estimation and `leniently' trains the DDQN neural network, which improves the probability of the selection of cooperative joint strategies. Experimental results show that CIL-DDQN outperforms other methods in almost all performance indicators of the traffic control problem.",0
"In this study, we aim to use reinforcement learning (RL) to solve the adaptive traffic signal control (ATSC) problem, which involves multiple urban intersections collaborating to achieve a common goal. However, the challenge lies in the fact that observation information is often incomplete, which affects the learning performance of independent reinforcement learning (IRL) algorithms. To address this, we introduce the partially observable weak cooperative traffic model (PO-WCTM) to optimize the overall traffic situation. In PO-WCTM, the learning goal of each intersection is to reduce the difficulty of learning, in line with the traffic environment hypothesis. We also propose a new IRL algorithm called Cooperative Important Lenient Double DQN (CIL-DDQN) that extends the Double DQN (DDQN) algorithm with two mechanisms - forgetful experience and lenient weight training - to improve the selection of cooperative joint strategies. Our experimental results indicate that CIL-DDQN performs better than other methods in multiple performance indicators.",1
"Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a generative DRL framework training with an auxiliary task of observational interferences such as artificial noises. Under this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ). We evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences as auxiliary labels. Our experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.",0
"Various gaming simulators and real-world applications have shown impressive performance in deep reinforcement learning (DRL). However, DRL agents may encounter faulty observations due to sudden interruptions like black-outs, frozen-screens, and adversarial perturbations. It is crucial to design a resilient DRL algorithm to handle these rare but critical situations. This paper proposes a generative DRL framework that trains with auxiliary tasks, including artificial noises, to prepare for observational interferences. The importance of the causal relation is discussed, and a causal inference-based DRL algorithm called causal inference Q-network (CIQ) is introduced. The performance of CIQ is evaluated in several benchmark DRL environments with different types of interferences as auxiliary labels. Experimental results show that CIQ achieves higher performance and more resilience against observational interferences.",1
"Human gaze is known to be an intention-revealing signal in human demonstrations of tasks. In this work, we use gaze cues from human demonstrators to enhance the performance of agents trained via three popular imitation learning methods -- behavioral cloning (BC), behavioral cloning from observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on similarities between the attention of reinforcement learning agents and human gaze, we propose a novel approach for utilizing gaze data in a computationally efficient manner, as part of an auxiliary loss function, which guides a network to have higher activations in image regions where the human's gaze fixated. This work is a step towards augmenting any existing convolutional imitation learning agent's training with auxiliary gaze data. Our auxiliary coverage-based gaze loss (CGL) guides learning toward a better reward function or policy, without adding any additional learnable parameters and without requiring gaze data at test time. We find that our proposed approach improves the performance by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over 20 different Atari games. We also find that compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL), our method achieves better performance, and is more efficient in terms of learning with fewer demonstrations. We further interpret trained CGL agents with a saliency map visualization method to explain their performance. At last, we show that CGL can help alleviate a well-known causal confusion problem in imitation learning.",0
"The human gaze is a signal that reveals intentions during demonstrations of tasks. This study utilizes gaze cues from human demonstrators to enhance the performance of agents trained using three popular imitation learning methods. A novel approach is proposed that utilizes gaze data in a computationally efficient manner as part of an auxiliary loss function to guide a network to have higher activations in image regions where the human's gaze fixated. This work improves the performance of the agents trained by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over 20 different Atari games. The proposed approach achieves better performance compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL) and is more efficient in terms of learning with fewer demonstrations. The trained agents are interpreted using a saliency map visualization method to explain their performance. Lastly, the proposed approach helps alleviate a well-known causal confusion problem in imitation learning.",1
"In this article we study the problem of training intelligent agents using Reinforcement Learning for the purpose of game development. Unlike systems built to replace human players and to achieve super-human performance, our agents aim to produce meaningful interactions with the player, and at the same time demonstrate behavioral traits as desired by game designers. We show how to combine distinct behavioral policies to obtain a meaningful ""fusion"" policy which comprises all these behaviors. To this end, we propose four different policy fusion methods for combining pre-trained policies. We further demonstrate how these methods can be used in combination with Inverse Reinforcement Learning in order to create intelligent agents with specific behavioral styles as chosen by game designers, without having to define many and possibly poorly-designed reward functions. Experiments on two different environments indicate that entropy-weighted policy fusion significantly outperforms all others. We provide several practical examples and use-cases for how these methods are indeed useful for video game production and designers.",0
"The problem of training intelligent agents for game development using Reinforcement Learning is explored in this article. Rather than replacing human players and achieving super-human performance, the focus is on creating agents that produce meaningful interactions and demonstrate desired behavioral traits as determined by game designers. The article presents four policy fusion methods for combining pre-trained policies to create a ""fusion"" policy that encompasses all desired behaviors. These fusion methods can be used in conjunction with Inverse Reinforcement Learning to create intelligent agents with specific behavioral styles, without needing to define multiple reward functions. The article provides practical examples and use-cases to demonstrate how these methods are valuable for video game production and designers. Experiments conducted on two environments indicate that entropy-weighted policy fusion is the most effective method.",1
"Reinforcement learning techniques achieved human-level performance in several tasks in the last decade. However, in recent years, the need for interpretability emerged: we want to be able to understand how a system works and the reasons behind its decisions. Not only we need interpretability to assess the safety of the produced systems, we also need it to extract knowledge about unknown problems. While some techniques that optimize decision trees for reinforcement learning do exist, they usually employ greedy algorithms or they do not exploit the rewards given by the environment. This means that these techniques may easily get stuck in local optima. In this work, we propose a novel approach to interpretable reinforcement learning that uses decision trees. We present a two-level optimization scheme that combines the advantages of evolutionary algorithms with the advantages of Q-learning. This way we decompose the problem into two sub-problems: the problem of finding a meaningful and useful decomposition of the state space, and the problem of associating an action to each state. We test the proposed method on three well-known reinforcement learning benchmarks, on which it results competitive with respect to the state-of-the-art in both performance and interpretability. Finally, we perform an ablation study that confirms that using the two-level optimization scheme gives a boost in performance in non-trivial environments with respect to a one-layer optimization technique.",0
"Over the past decade, reinforcement learning techniques have achieved human-level performance in various tasks. However, the need for interpretability has recently arisen as we strive to comprehend how a system operates and the rationale behind its decisions. Interpretability is crucial not only for evaluating the safety of produced systems but also for extracting knowledge about unfamiliar issues. Although some techniques that optimize decision trees for reinforcement learning exist, they typically employ greedy algorithms or do not exploit rewards from the environment, resulting in susceptibility to local optima. This study proposes a novel approach to interpretable reinforcement learning using decision trees, employing a two-level optimization scheme that combines the advantages of evolutionary algorithms with Q-learning. This approach divides the problem into two sub-problems: finding a significant and beneficial decomposition of the state space and associating an action with each state. The proposed method is tested on three well-known reinforcement learning benchmarks, demonstrating competitiveness with the state-of-the-art in both performance and interpretability. Additionally, an ablation study confirms that the two-level optimization scheme provides a performance boost in non-trivial environments compared to a one-layer optimization technique.",1
"This paper develops a reinforcement learning (RL) scheme for adaptive traffic signal control (ATSC), called ""CVLight"", that leverages data collected only from connected vehicles (CV). Seven types of RL models are proposed within this scheme that contain various state and reward representations, including incorporation of CV delay and green light duration into state and the usage of CV delay as reward. To further incorporate information of both CV and non-CV into CVLight, an algorithm based on actor-critic, A2C-Full, is proposed where both CV and non-CV information is used to train the critic network, while only CV information is used to update the policy network and execute optimal signal timing. These models are compared at an isolated intersection under various CV market penetration rates. A full model with the best performance (i.e., minimum average travel delay per vehicle) is then selected and applied to compare with state-of-the-art benchmarks under different levels of traffic demands, turning proportions, and dynamic traffic demands, respectively. Two case studies are performed on an isolated intersection and a corridor with three consecutive intersections located in Manhattan, New York, to further demonstrate the effectiveness of the proposed algorithm under real-world scenarios. Compared to other baseline models that use all vehicle information, the trained CVLight agent can efficiently control multiple intersections solely based on CV data and can achieve a similar or even greater performance when the CV penetration rate is no less than 20%.",0
"The objective of this paper is to introduce ""CVLight"", an adaptive traffic signal control (ATSC) reinforcement learning (RL) scheme that uses data exclusively collected from connected vehicles (CV). The scheme includes seven types of RL models with different state and reward representations, such as incorporating CV delay and green light duration into state and using CV delay as reward. To incorporate information from both CV and non-CV, an actor-critic algorithm, A2C-Full, is proposed, where both types of data are used to train the critic network, but only CV information is used to update the policy network and execute optimal signal timing. The models are tested at an isolated intersection under various CV market penetration rates, and the best performing model is selected and compared with state-of-the-art benchmarks under different traffic conditions. Two case studies are conducted on an isolated intersection and a corridor with three consecutive intersections in Manhattan, New York, to demonstrate the effectiveness of the proposed algorithm under real-world scenarios. Results show that the trained CVLight agent can efficiently control multiple intersections solely based on CV data and can achieve similar or even better performance than baseline models when the CV penetration rate is at least 20%.",1
"Snake robots, comprised of sequentially connected joint actuators, have recently gained increasing attention in the industrial field, like life detection in narrow space. Such robots can navigate through the complex environment via the cooperation of multiple motors located on the backbone. However, controlling the robots in an unknown environment is challenging, and conventional control strategies can be energy inefficient or even fail to navigate to the destination. In this work, a snake locomotion gait policy is developed via deep reinforcement learning (DRL) for energy-efficient control. We apply proximal policy optimization (PPO) to each joint motor parameterized by angular velocity and the DRL agent learns the standard serpenoid curve at each timestep. The robot simulator and task environment are built upon PyBullet. Comparing to conventional control strategies, the snake robots controlled by the trained PPO agent can achieve faster movement and more energy-efficient locomotion gait. This work demonstrates that DRL provides an energy-efficient solution for robot control.",0
"Recently, snake robots have been receiving more attention in the industrial field due to their ability to detect life in narrow spaces. These robots can navigate complex environments through the collaboration of multiple motors on their backbone. However, controlling them in an unknown environment is difficult, and traditional control methods can be inefficient or even fail to reach the destination. This study uses deep reinforcement learning (DRL) to create an energy-efficient snake locomotion gait policy. Proximal policy optimization (PPO) is applied to each joint motor, and the DRL agent learns the standard serpenoid curve at each timestep. The robot simulator and task environment are built using PyBullet. Compared to traditional control methods, the snake robots controlled by the trained PPO agent move faster and have a more energy-efficient locomotion gait. The study shows that DRL is an energy-efficient solution for robot control.",1
"High dropout rates in tertiary education expose a lack of efficiency that causes frustration of expectations and financial waste. Predicting students at risk is not enough to avoid student dropout. Usually, an appropriate aid action must be discovered and applied in the proper time for each student. To tackle this sequential decision-making problem, we propose a decision support method to the selection of aid actions for students using offline reinforcement learning to support decision-makers effectively avoid student dropout. Additionally, a discretization of student's state space applying two different clustering methods is evaluated. Our experiments using logged data of real students shows, through off-policy evaluation, that the method should achieve roughly 1.0 to 1.5 times as much cumulative reward as the logged policy. So, it is feasible to help decision-makers apply appropriate aid actions and, possibly, reduce student dropout.",0
"The high number of students dropping out of tertiary education indicates a lack of efficiency, leading to disappointment and financial loss. Identifying students who may be at risk of dropping out is insufficient to prevent it. To address this issue, we suggest a decision support technique that employs offline reinforcement learning to aid decision-makers in selecting the most suitable aid actions for each student. We also examine the state space of students by using two clustering methods to discretize it. Our experiments indicate that this approach could achieve a cumulative reward of approximately 1.0 to 1.5 times greater than the logged policy, demonstrating its potential to reduce student dropout rates.",1
"While reinforcement learning algorithms provide automated acquisition of optimal policies, practical application of such methods requires a number of design decisions, such as manually designing reward functions that not only define the task, but also provide sufficient shaping to accomplish it. In this paper, we discuss a new perspective on reinforcement learning, recasting it as the problem of inferring actions that achieve desired outcomes, rather than a problem of maximizing rewards. To solve the resulting outcome-directed inference problem, we establish a novel variational inference formulation that allows us to derive a well-shaped reward function which can be learned directly from environment interactions. From the corresponding variational objective, we also derive a new probabilistic Bellman backup operator reminiscent of the standard Bellman backup operator and use it to develop an off-policy algorithm to solve goal-directed tasks. We empirically demonstrate that this method eliminates the need to design reward functions and leads to effective goal-directed behaviors.",0
"Although reinforcement learning algorithms can automatically acquire optimal policies, their practical application requires several design decisions. These decisions include manually designing reward functions, which not only define the task but also provide enough shaping to achieve it. However, this paper presents a new perspective on reinforcement learning by framing it as the problem of inferring actions that produce desired outcomes, rather than maximizing rewards. To address this outcome-directed inference problem, a novel variational inference formulation is established, enabling the derivation of a well-shaped reward function that can be learned directly from environment interactions. Additionally, a new probabilistic Bellman backup operator is derived from the corresponding variational objective, similar to the standard Bellman backup operator, and used to develop an off-policy algorithm for solving goal-directed tasks. Empirical evidence shows that this approach eliminates the need to design reward functions and produces effective goal-directed behaviors.",1
"The design of efficient and generic algorithms for solving combinatorial optimization problems has been an active field of research for many years. Standard exact solving approaches are based on a clever and complete enumeration of the solution set. A critical and non-trivial design choice with such methods is the branching strategy, directing how the search is performed. The last decade has shown an increasing interest in the design of machine learning-based heuristics to solve combinatorial optimization problems. The goal is to leverage knowledge from historical data to solve similar new instances of a problem. Used alone, such heuristics are only able to provide approximate solutions efficiently, but cannot prove optimality nor bounds on their solution. Recent works have shown that reinforcement learning can be successfully used for driving the search phase of constraint programming (CP) solvers. However, it has also been shown that this hybridization is challenging to build, as standard CP frameworks do not natively include machine learning mechanisms, leading to some sources of inefficiencies. This paper presents the proof of concept for SeaPearl, a new CP solver implemented in Julia, that supports machine learning routines in order to learn branching decisions using reinforcement learning. Support for modeling the learning component is also provided. We illustrate the modeling and solution performance of this new solver on two problems. Although not yet competitive with industrial solvers, SeaPearl aims to provide a flexible and open-source framework in order to facilitate future research in the hybridization of constraint programming and machine learning.",0
"For numerous years, researchers have been actively engaged in designing effective and general algorithms to tackle combinatorial optimization problems. Traditional exact solving methods involve a comprehensive enumeration of the solution set and rely heavily on a suitable branching strategy to guide the search process. Recently, machine learning-based heuristics have garnered significant interest in solving combinatorial optimization problems by utilizing historical data to solve new instances of the problem. These heuristics are solely capable of providing approximate solutions without proof of optimality or solution bounds. Several studies have shown that reinforcement learning can be effectively employed in driving the search phase of constraint programming (CP) solvers. However, incorporating machine learning mechanisms into standard CP frameworks poses some challenges, leading to inefficiencies. This paper introduces SeaPearl, a new CP solver developed in Julia that supports machine learning routines to learn branching decisions using reinforcement learning. It also provides support for modeling the learning component. The paper demonstrates the modeling and solution performance of SeaPearl on two problems, although the solver is not yet competitive with industrial solvers. SeaPearl aims to serve as a flexible and open-source framework to facilitate future research in the hybridization of constraint programming and machine learning.",1
"We explore the feasibility of combining Graph Neural Network-based policy architectures with Deep Reinforcement Learning as an approach to problems in systems. This fits particularly well with operations on networks, which naturally take the form of graphs. As a case study, we take the idea of data-driven routing in intradomain traffic engineering, whereby the routing of data in a network can be managed taking into account the data itself. The particular subproblem which we examine is minimising link congestion in networks using knowledge of historic traffic flows. We show through experiments that an approach using Graph Neural Networks (GNNs) performs at least as well as previous work using Multilayer Perceptron architectures. GNNs have the added benefit that they allow for the generalisation of trained agents to different network topologies with no extra work. Furthermore, we believe that this technique is applicable to a far wider selection of problems in systems research.",0
"In this study, we investigate the possibility of using Graph Neural Network-based policy architectures in conjunction with Deep Reinforcement Learning to tackle issues in systems, with a particular focus on network operations. Since networks are naturally represented as graphs, this combination is an ideal solution. To illustrate this, we examine the concept of data-driven routing in intradomain traffic engineering, which involves managing data routing in a network while considering the data itself. Our specific focus is on minimizing link congestion in networks by leveraging historical traffic flow data. Our experiments demonstrate that utilizing Graph Neural Networks (GNNs) performs at least as well as previous studies that used Multilayer Perceptron architectures. Additionally, GNNs enable trained agents to be applied to various network topologies without additional effort, making them even more beneficial. We are convinced that this method has broad applications in systems research.",1
"Q-learning (QL), a common reinforcement learning algorithm, suffers from over-estimation bias due to the maximization term in the optimal Bellman operator. This bias may lead to sub-optimal behavior. Double-Q-learning tackles this issue by utilizing two estimators, yet results in an under-estimation bias. Similar to over-estimation in Q-learning, in certain scenarios, the under-estimation bias may degrade performance. In this work, we introduce a new bias-reduced algorithm called Ensemble Bootstrapped Q-Learning (EBQL), a natural extension of Double-Q-learning to ensembles. We analyze our method both theoretically and empirically. Theoretically, we prove that EBQL-like updates yield lower MSE when estimating the maximal mean of a set of independent random variables. Empirically, we show that there exist domains where both over and under-estimation result in sub-optimal performance. Finally, We demonstrate the superior performance of a deep RL variant of EBQL over other deep QL algorithms for a suite of ATARI games.",0
"QL, a popular algorithm in reinforcement learning, has a tendency to overestimate due to the maximization term in the optimal Bellman operator, resulting in sub-optimal behavior. Double-Q-learning addresses this issue by using two estimators, but it may cause an underestimation bias in certain situations, which can also lead to performance degradation. To solve this problem, we present Ensemble Bootstrapped Q-Learning (EBQL), a new bias-reduced algorithm that extends Double-Q-learning to ensembles. We provide both theoretical and empirical analyses, proving that EBQL-like updates yield lower MSE when estimating the maximal mean of a set of independent random variables. We also show that in some domains, both over and underestimation can lead to sub-optimal performance. Finally, we demonstrate the superior performance of a deep RL variant of EBQL over other deep QL algorithms in a suite of ATARI games.",1
"This paper deals with distributed policy optimization in reinforcement learning, which involves a central controller and a group of learners. In particular, two typical settings encountered in several applications are considered: multi-agent reinforcement learning (RL) and parallel RL, where frequent information exchanges between the learners and the controller are required. For many practical distributed systems, however, the overhead caused by these frequent communication exchanges is considerable, and becomes the bottleneck of the overall performance. To address this challenge, a novel policy gradient approach is developed for solving distributed RL. The novel approach adaptively skips the policy gradient communication during iterations, and can reduce the communication overhead without degrading learning performance. It is established analytically that: i) the novel algorithm has convergence rate identical to that of the plain-vanilla policy gradient; while ii) if the distributed learners are heterogeneous in terms of their reward functions, the number of communication rounds needed to achieve a desirable learning accuracy is markedly reduced. Numerical experiments corroborate the communication reduction attained by the novel algorithm compared to alternatives.",0
"The focus of this paper is on the optimization of policies in reinforcement learning through a distributed approach. This involves a central controller and a group of learners, which is applicable to both multi-agent reinforcement learning and parallel reinforcement learning. However, frequent information exchanges between the learners and the controller can lead to significant overhead, which can hinder the overall performance. To tackle this issue, a new policy gradient approach has been developed to effectively manage communication during iterations without compromising learning performance. This approach has been shown to converge at the same rate as the plain-vanilla policy gradient, while also reducing the number of communication rounds needed in cases where learners have varying reward functions. Numerical experiments have validated the effectiveness of this approach in reducing communication overhead compared to other alternatives.",1
"This work focuses on object goal visual navigation, aiming at finding the location of an object from a given class, where in each step the agent is provided with an egocentric RGB image of the scene. We propose to learn the agent's policy using a reinforcement learning algorithm. Our key contribution is a novel attention probability model for visual navigation tasks. This attention encodes semantic information about observed objects, as well as spatial information about their place. This combination of the ""what"" and the ""where"" allows the agent to navigate toward the sought-after object effectively. The attention model is shown to improve the agent's policy and to achieve state-of-the-art results on commonly-used datasets.",0
"The focus of this study is on visual navigation with the goal of locating objects from a specific class. The agent is provided with an egocentric RGB image of the scene at each step. We suggest using a reinforcement learning algorithm to teach the agent's policy. Our main contribution is a new attention probability model for visual navigation tasks. This attention model captures both semantic and spatial information about observed objects, enabling the agent to navigate towards the desired object with ease. The attention model enhances the agent's policy and performs better than other existing models on popular datasets.",1
"Recent years have witnessed the popularity and success of graph neural networks (GNN) in various scenarios. To obtain data-specific GNN architectures, researchers turn to neural architecture search (NAS), which has made impressive success in discovering effective architectures in convolutional neural networks. However, it is non-trivial to apply NAS approaches to GNN due to challenges in search space design and the expensive searching cost of existing NAS methods. In this work, to obtain the data-specific GNN architectures and address the computational challenges facing by NAS approaches, we propose a framework, which tries to Search to Aggregate NEighborhood (SANE), to automatically design data-specific GNN architectures. By designing a novel and expressive search space, we propose a differentiable search algorithm, which is more efficient than previous reinforcement learning based methods. Experimental results on four tasks and seven real-world datasets demonstrate the superiority of SANE compared to existing GNN models and NAS approaches in terms of effectiveness and efficiency. (Code is available at: https://github.com/AutoML-4Paradigm/SANE).",0
"The popularity and success of graph neural networks (GNN) in various scenarios have been witnessed in recent years. Researchers are turning to neural architecture search (NAS) to obtain data-specific GNN architectures, which has been successful in discovering effective architectures in convolutional neural networks. However, there are challenges in applying NAS approaches to GNN due to search space design and the expensive searching cost of existing NAS methods. To address these computational challenges and obtain data-specific GNN architectures, we propose a framework called Search to Aggregate NEighborhood (SANE). Our approach uses a novel and expressive search space and a differentiable search algorithm that is more efficient than previous reinforcement learning-based methods. We demonstrate the superiority of SANE over existing GNN models and NAS approaches in terms of effectiveness and efficiency through experiments on four tasks and seven real-world datasets. (Code is available at: https://github.com/AutoML-4Paradigm/SANE).",1
"We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.",0
"The focus of our research is on forecasting and regulating the future state distribution of an independent agent. This issue can be interpreted as a different version of goal-oriented reinforcement learning (RL), which revolves around acquiring knowledge about a conditional probability density function for future states. Rather than directly estimating this density function, we indirectly estimate it by training a classifier to determine whether an observation is from the future. Using Bayes' rule, the classifier's predictions can be converted into predictions for future states. Our algorithm's off-policy variant is crucial because it enables us to anticipate the future state distribution of a new policy without obtaining new experience. This variant allows us to optimize policy functionals that rely on future state distribution, such as the likelihood of achieving a specific goal state. Our work provides a theoretical basis for goal-oriented RL as density estimation, which justifies the use of goal-oriented methods in prior research. Our foundation establishes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we verify experimentally. Additionally, our proposed approach is comparable to previous goal-oriented RL methods.",1
"As more machine learning agents interact with humans, it is increasingly a prospect that an agent trained to perform a task optimally, using only a measure of task performance as feedback, can violate societal norms for acceptable behavior or cause harm. Value alignment is a property of intelligent agents wherein they solely pursue non-harmful behaviors or human-beneficial goals. We introduce an approach to value-aligned reinforcement learning, in which we train an agent with two reward signals: a standard task performance reward, plus a normative behavior reward. The normative behavior reward is derived from a value-aligned prior model previously shown to classify text as normative or non-normative. We show how variations on a policy shaping technique can balance these two sources of reward and produce policies that are both effective and perceived as being more normative. We test our value-alignment technique on three interactive text-based worlds; each world is designed specifically to challenge agents with a task as well as provide opportunities to deviate from the task to engage in normative and/or altruistic behavior.",0
"As machine learning agents increasingly interact with humans, there is a growing concern that these agents, trained solely for optimal task performance, may violate societal norms or cause harm. To address this, value alignment is crucial, whereby intelligent agents prioritize non-harmful or human-beneficial goals. We propose a value-aligned reinforcement learning approach that involves training an agent with two reward signals: a standard task performance reward and a normative behavior reward. The normative behavior reward is derived from a prior model that distinguishes normative from non-normative text. By utilizing a policy shaping technique, we can balance these rewards to produce effective policies that are also perceived as more normative. Our approach was tested in three text-based worlds that challenged agents to complete tasks while providing opportunities for normative and altruistic behavior.",1
"Active screening is a common approach in controlling the spread of recurring infectious diseases such as tuberculosis and influenza. In this approach, health workers periodically select a subset of population for screening. However, given the limited number of health workers, only a small subset of the population can be visited in any given time period. Given the recurrent nature of the disease and rapid spreading, the goal is to minimize the number of infections over a long time horizon. Active screening can be formalized as a sequential combinatorial optimization over the network of people and their connections. The main computational challenges in this formalization arise from i) the combinatorial nature of the problem, ii) the need of sequential planning and iii) the uncertainties in the infectiousness states of the population.   Previous works on active screening fail to scale to large time horizon while fully considering the future effect of current interventions. In this paper, we propose a novel reinforcement learning (RL) approach based on Deep Q-Networks (DQN), with several innovative adaptations that are designed to address the above challenges. First, we use graph convolutional networks (GCNs) to represent the Q-function that exploit the node correlations of the underlying contact network. Second, to avoid solving a combinatorial optimization problem in each time period, we decompose the node set selection as a sub-sequence of decisions, and further design a two-level RL framework that solves the problem in a hierarchical way. Finally, to speed-up the slow convergence of RL which arises from reward sparseness, we incorporate ideas from curriculum learning into our hierarchical RL approach. We evaluate our RL algorithm on several real-world networks.",0
"Controlling the spread of infectious diseases like tuberculosis and influenza often involves active screening, where a subset of the population is periodically screened by health workers. However, because of limited resources, only a small portion of the population can be screened at any given time. The aim of active screening is to minimize the number of infections over a long period. The process involves a sequential combinatorial optimization over the network of people and their connections, and is challenged by the problem's combinatorial nature, the need for sequential planning, and uncertainties in the infectiousness of the population. Prior studies on active screening have not fully considered the future effects of current interventions or scaled well to long time horizons. This paper proposes a novel reinforcement learning (RL) approach using Deep Q-Networks (DQN) that addresses these challenges through graph convolutional networks (GCNs), a two-level RL framework, and curriculum learning. The approach is evaluated on real-world networks.",1
"Object-centric representations have recently enabled significant progress in tackling relational reasoning tasks. By building a strong object-centric inductive bias into neural architectures, recent efforts have improved generalization and data efficiency of machine learning algorithms for these problems. One problem class involving relational reasoning that still remains under-explored is multi-agent reinforcement learning (MARL). Here we investigate whether object-centric representations are also beneficial in the fully cooperative MARL setting. Specifically, we study two ways of incorporating an agent-centric inductive bias into our RL algorithm: 1. Introducing an agent-centric attention module with explicit connections across agents 2. Adding an agent-centric unsupervised predictive objective (i.e. not using action labels), to be used as an auxiliary loss for MARL, or as the basis of a pre-training step. We evaluate these approaches on the Google Research Football environment as well as DeepMind Lab 2D. Empirically, agent-centric representation learning leads to the emergence of more complex cooperation strategies between agents as well as enhanced sample efficiency and generalization.",0
"Recent progress in tackling relational reasoning tasks has been made possible by object-centric representations. By incorporating a strong object-centric inductive bias into neural architectures, machine learning algorithms for these problems have experienced improvements in generalization and data efficiency. However, multi-agent reinforcement learning (MARL) involving relational reasoning remains an under-explored problem class. This study investigates whether object-centric representations can also be beneficial in the fully cooperative MARL setting. Two approaches are studied, including introducing an agent-centric attention module with explicit connections across agents and adding an agent-centric unsupervised predictive objective. The evaluation is conducted on the Google Research Football environment and DeepMind Lab 2D. The results show that agent-centric representation learning leads to the emergence of more complex cooperation strategies between agents, as well as enhanced sample efficiency and generalization.",1
"Deep reinforcement learning (DRL) has successfully solved various problems recently, typically with a unimodal policy representation. However, grasping distinguishable skills for some tasks with non-unique optima can be essential for further improving its learning efficiency and performance, which may lead to a multimodal policy represented as a mixture-of-experts (MOE). To our best knowledge, present DRL algorithms for general utility do not deploy this method as policy function approximators due to the potential challenge in its differentiability for policy learning. In this work, we propose a probabilistic mixture-of-experts (PMOE) implemented with a Gaussian mixture model (GMM) for multimodal policy, together with a novel gradient estimator for the indifferentiability problem, which can be applied in generic off-policy and on-policy DRL algorithms using stochastic policies, e.g., Soft Actor-Critic (SAC) and Proximal Policy Optimisation (PPO). Experimental results testify the advantage of our method over unimodal polices and two different MOE methods, as well as a method of option frameworks, based on the above two types of DRL algorithms, on six MuJoCo tasks. Different gradient estimations for GMM like the reparameterisation trick (Gumbel-Softmax) and the score-ratio trick are also compared with our method. We further empirically demonstrate the distinguishable primitives learned with PMOE and show the benefits of our method in terms of exploration.",0
"Recently, Deep Reinforcement Learning (DRL) has been successful in solving various problems using a unimodal policy representation. However, for tasks with non-unique optima, it is essential to grasp distinguishable skills to improve learning efficiency and performance, which can be achieved through a multimodal policy represented as a mixture-of-experts (MOE). However, current DRL algorithms do not use this method as policy function approximators due to potential challenges in its differentiability for policy learning. In this study, we propose a probabilistic mixture-of-experts (PMOE) implemented with a Gaussian mixture model (GMM) for multimodal policy, along with a novel gradient estimator to address the indifferentiability problem. Our method can be applied in generic off-policy and on-policy DRL algorithms using stochastic policies, such as Soft Actor-Critic (SAC) and Proximal Policy Optimisation (PPO). We conducted experiments on six MuJoCo tasks and our results demonstrate the advantages of our method over unimodal policies, two different MOE methods, and a method of option frameworks, based on the above two types of DRL algorithms. We also compared different gradient estimations for GMM, such as the reparameterisation trick (Gumbel-Softmax) and the score-ratio trick with our method. Furthermore, we empirically demonstrate the distinguishable primitives learned with PMOE and show the benefits of our method in terms of exploration.",1
"Applications of reinforcement learning (RL) to stabilization problems of real systems are restricted since an agent needs many experiences to learn an optimal policy and may determine dangerous actions during its exploration. If we know a mathematical model of a real system, a simulator is useful because it predicates behaviors of the real system using the mathematical model with a given system parameter vector. We can collect many experiences more efficiently than interactions with the real system. However, it is difficult to identify the system parameter vector accurately. If we have an identification error, experiences obtained by the simulator may degrade the performance of the learned policy. Thus, we propose a practical RL algorithm that consists of two stages. At the first stage, we choose multiple system parameter vectors. Then, we have a mathematical model for each system parameter vector, which is called a virtual system. We obtain optimal Q-functions for multiple virtual systems using the continuous deep Q-learning algorithm. At the second stage, we represent a Q-function for the real system by a linear approximated function whose basis functions are optimal Q-functions learned at the first stage. The agent learns the Q-function through interactions with the real system online. By numerical simulations, we show the usefulness of our proposed method.",0
"The use of reinforcement learning (RL) in stabilizing real systems is limited due to the need for an agent to gain numerous experiences to acquire an optimal policy, which can lead to hazardous actions during exploration. However, if we possess a mathematical model of the real system, a simulator can be employed to predict its behavior, enabling us to gather experiences more efficiently than direct interactions. Nevertheless, accurately identifying the system parameter vector is challenging, and errors can cause the simulator's experiences to hinder learned policy performance. Therefore, we propose a two-stage RL algorithm that involves selecting multiple system parameter vectors, creating virtual systems and obtaining optimal Q-functions using deep Q-learning. In the second stage, a Q-function for the real system is represented by an approximated linear function with optimal Q-functions learned in the first stage as its basis functions. The agent then learns the Q-function by interacting with the real system. Our proposed method is demonstrated to be useful through numerical simulations.",1
"Detection and localization of actions in videos is an important problem in practice. A traffic analyst might be interested in studying the patterns in which vehicles move at a given intersection. State-of-the-art video analytics systems are unable to efficiently and effectively answer such action queries. The reasons are threefold. First, action detection and localization tasks require computationally expensive deep neural networks. Second, actions are often rare events. Third, actions are spread across a sequence of frames. It is important to take the entire sequence of frames into context for effectively answering the query. It is critical to quickly skim through the irrelevant parts of the video to answer the action query efficiently.   In this paper, we present Zeus, a video analytics system tailored for answering action queries. We propose a novel technique for efficiently answering these queries using a deep reinforcement learning agent. Zeus trains an agent that learns to adaptively modify the input video segments to an action classification network. The agent alters the input segments along three dimensions -- sampling rate, segment length, and resolution. Besides efficiency, Zeus is capable of answering the query at a user-specified target accuracy using a query optimizer that trains the agent based on an accuracy-aware reward function. Our evaluation of Zeus on a novel action localization dataset shows that it outperforms the state-of-the-art frame- and window-based techniques by up to 1.4x and 3x, respectively. Furthermore, unlike the frame-based technique, it satisfies the user-specified target accuracy across all the queries, at up to 2x higher accuracy, than frame-based methods.",0
"The identification and positioning of actions in videos is a significant challenge in practical applications. For instance, a traffic analyst might be interested in examining the movement patterns of vehicles at a particular intersection. Despite modern video analytics systems, action queries remain challenging to efficiently and effectively answer for three reasons. Firstly, deep neural networks that require significant computational power are required for action detection and localization. Secondly, actions are typically uncommon occurrences. Lastly, actions are dispersed throughout a sequence of frames, and it is necessary to consider the entire sequence of frames to provide an effective response to the query. To respond to action queries efficiently, it is essential to quickly sift through irrelevant video parts. This paper introduces Zeus, a video analytics system designed to respond to action queries. A novel approach using a deep reinforcement learning agent is proposed to efficiently answer these queries. Zeus trains an agent that modifies the input video segments to an action classification network along three dimensions: sampling rate, segment length, and resolution. In addition to efficiency, Zeus can answer the query at a user-specified target accuracy using a query optimizer that trains the agent based on an accuracy-aware reward function. Our evaluation of Zeus on a novel action localization dataset demonstrates that it outperforms existing frame- and window-based techniques by up to 1.4x and 3x, respectively. Moreover, it satisfies the user-specified target accuracy across all queries, with up to 2x higher accuracy than frame-based methods.",1
"Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, with little development of the general framework. In this paper, we identify a fundamental issue of the standard MBRL framework -- what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t.~the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of one-step ahead predictions is not always correlated with control performance. This observation highlights a critical limitation in the MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of research for addressing this issue.",0
"The framework of model-based reinforcement learning (MBRL) has proven to be effective in efficiently learning control of continuous tasks. However, recent advancements in MBRL have mainly focused on using more sophisticated function approximators and planning techniques, with little attention given to the overall framework. This paper identifies a fundamental issue with the standard MBRL framework, which we refer to as the objective mismatch issue. This issue arises when one objective is optimized in hopes of also optimizing a second, often unrelated metric. In the context of MBRL, we examine the objective mismatch between training the forward dynamics model based on the likelihood of one-step ahead prediction and the ultimate goal of improving performance on a downstream control task. We demonstrate that dynamics models effective for a specific task may not necessarily be globally accurate, and vice versa. Our experiments reveal that the likelihood of one-step ahead predictions is not always correlated with control performance, highlighting a critical limitation in the MBRL framework that requires further research to be fully understood and resolved. We propose a preliminary method for mitigating the mismatch issue by re-weighting dynamics model training and discuss potential research directions for addressing this issue.",1
"Two key challenges within Reinforcement Learning involve improving (a) agent learning within environments with sparse extrinsic rewards and (b) the explainability of agent actions. We describe a curious subgoal focused agent to address both these challenges. We use a novel method for curiosity produced from a Generative Adversarial Network (GAN) based model of environment transitions that is robust to stochastic environment transitions. Additionally, we use a subgoal generating network to guide navigation. The explainability of the agent's behavior is increased by decomposing complex tasks into a sequence of interpretable subgoals that do not require any manual design. We show that this method also enables the agent to solve challenging procedurally-generated tasks that contain stochastic transitions above other state-of-the-art methods.",0
"Reinforcement Learning faces two main obstacles, namely, enhancing the ability of agents to learn within environments that offer sparse extrinsic rewards, and improving the comprehensibility of agent actions. To tackle these issues, we introduce a curious subgoal-oriented agent. Our approach employs a unique technique for stimulating curiosity derived from a Generative Adversarial Network (GAN)-based model of environment transitions that remains robust even when confronted with stochastic environment transitions. Moreover, we employ a subgoal-generating network to direct navigation. By breaking down complicated tasks into a sequence of understandable subgoals that do not require any manual design, we increase the comprehensibility of the agent's behavior. Our method also allows the agent to solve complex procedurally-generated tasks that involve stochastic transitions, surpassing other state-of-the-art techniques.",1
"Reinforcement learning (RL) algorithms based on high-dimensional function approximation have achieved tremendous empirical success in large-scale problems with an enormous number of states. However, most analysis of such algorithms gives rise to error bounds that involve either the number of states or the number of features. This paper considers the situation where the function approximation is made either using the kernel method or the two-layer neural network model, in the context of a fitted Q-iteration algorithm with explicit regularization. We establish an $\tilde{O}(H^3|\mathcal {A}|^{\frac14}n^{-\frac14})$ bound for the optimal policy with $Hn$ samples, where $H$ is the length of each episode and $|\mathcal {A}|$ is the size of action space. Our analysis hinges on analyzing the $L^2$ error of the approximated Q-function using $n$ data points. Even though this result still requires a finite-sized action space, the error bound is independent of the dimensionality of the state space.",0
"Algorithms using reinforcement learning (RL) and high-dimensional function approximation have achieved great success in large-scale problems with a vast number of states. However, most analyses of such algorithms involve error bounds that depend on either the number of states or the number of features. This study focuses on the kernel method or the two-layer neural network model for function approximation within a fitted Q-iteration algorithm with explicit regularization. The paper establishes a bound of $\tilde{O}(H^3|\mathcal{A}|^{\frac14}n^{-\frac14})$ for the optimal policy with $Hn$ samples, where $H$ is the episode length and $|\mathcal{A}|$ is the action space size. The analysis is based on examining the $L^2$ error of the approximated Q-function using $n$ data points. Although the result still requires a finite-sized action space, the error bound is independent of the state space's dimensionality.",1
"It is essential for an automated vehicle in the field to perform discretionary lane changes with appropriate roadmanship - driving safely and efficiently without annoying or endangering other road users - under a wide range of traffic cultures and driving conditions. While deep reinforcement learning methods have excelled in recent years and been applied to automated vehicle driving policy, there are concerns about their capability to quickly adapt to unseen traffic with new environment dynamics. We formulate this challenge as a multi-Markov Decision Processes (MDPs) adaptation problem and developed Meta Reinforcement Learning (MRL) driving policies to showcase their quick learning capability. Two types of distribution variation in environments were designed and simulated to validate the fast adaptation capability of resulting MRL driving policies which significantly outperform a baseline RL.",0
"Performing discretionary lane changes appropriately is crucial for automated vehicles operating in the field. They must drive safely and efficiently without disturbing or endangering other road users, regardless of traffic cultures and driving conditions. Although deep reinforcement learning methods have excelled in recent years and have been implemented in automated vehicle driving policies, there are concerns about their ability to adapt quickly to new environment dynamics and unforeseen traffic. We have approached this challenge by formulating it as a multi-Markov Decision Processes (MDPs) adaptation problem and have developed Meta Reinforcement Learning (MRL) driving policies to demonstrate their rapid learning capabilities. To validate the fast adaptation ability of the resulting MRL driving policies, we have designed and simulated two types of distribution variation in environments, which outperform a baseline RL.",1
"Physics-based reinforcement learning tasks can benefit from simplified physics simulators as they potentially allow near-optimal policies to be learned in simulation. However, such simulators require the latent factors (e.g. mass, friction coefficient etc.) of the associated objects and other environment-specific factors (e.g. wind speed, air density etc.) to be accurately specified, without which, it could take considerable additional learning effort to adapt the learned simulation policy to the real environment. As such a complete specification can be impractical, in this paper, we instead, focus on learning task-specific estimates of latent factors which allow the approximation of real world trajectories in an ideal simulation environment. Specifically, we propose two new concepts: a) action grouping - the idea that certain types of actions are closely associated with the estimation of certain latent factors, and; b) partial grounding - the idea that simulation of task-specific dynamics may not need precise estimation of all the latent factors. We first introduce intuitive action groupings based on human physics knowledge and experience, which is then used to design novel strategies for interacting with the real environment. Next, we describe how prior knowledge of a task in a given environment can be used to extract the relative importance of different latent factors, and how this can be used to inform partial grounding, which enables efficient learning of the task in any arbitrary environment. We demonstrate our approach in a range of physics based tasks, and show that it achieves superior performance relative to other baselines, using only a limited number of real-world interactions.",0
"The utilization of simplified physics simulators in physics-based reinforcement learning tasks has the potential to facilitate the learning of near-optimal policies in simulations. However, these simulators necessitate the precise specification of latent factors, such as mass and friction coefficient, and environment-specific factors, including air density and wind speed. Failure to specify these factors accurately results in additional learning effort required to adapt the simulation policy to the genuine environment. Given that complete specification can be impractical, we propose learning task-specific estimates of latent factors to approximate real-world trajectories in an ideal simulation environment. Our approach includes two novel concepts: action grouping and partial grounding. Action grouping involves pairing certain actions with particular latent factor estimation, based on human physics knowledge and experience. Partial grounding recognizes that not all latent factors need precise estimation to simulate task-specific dynamics. We also describe how prior knowledge of a task and environment can inform partial grounding and facilitate efficient learning of the task in any arbitrary environment. Our approach achieves superior performance in a range of physics-based tasks compared to other baselines, utilizing only a limited number of real-world interactions.",1
"In this paper we propose several novel distributed gradient-based temporal difference algorithms for multi-agent off-policy learning of linear approximation of the value function in Markov decision processes with strict information structure constraints, limiting inter-agent communications to small neighborhoods. The algorithms are composed of: 1) local parameter updates based on single-agent off-policy gradient temporal difference learning algorithms, including eligibility traces with state dependent parameters, and 2) linear stochastic time varying consensus schemes, represented by directed graphs. The proposed algorithms differ by their form, definition of eligibility traces, selection of time scales and the way of incorporating consensus iterations. The main contribution of the paper is a convergence analysis based on the general properties of the underlying Feller-Markov processes and the stochastic time varying consensus model. We prove, under general assumptions, that the parameter estimates generated by all the proposed algorithms weakly converge to the corresponding ordinary differential equations (ODE) with precisely defined invariant sets. It is demonstrated how the adopted methodology can be applied to temporal-difference algorithms under weaker information structure constraints. The variance reduction effect of the proposed algorithms is demonstrated by formulating and analyzing an asymptotic stochastic differential equation. Specific guidelines for communication network design are provided. The algorithms' superior properties are illustrated by characteristic simulation results.",0
"This paper introduces a set of distributed gradient-based temporal difference algorithms for multi-agent off-policy learning of linear approximation of the value function in Markov decision processes. The algorithms are designed to work within strict information structure constraints that limit inter-agent communications to small neighborhoods. They consist of local parameter updates based on single-agent off-policy gradient temporal difference learning algorithms, with eligibility traces that have state-dependent parameters. Additionally, the algorithms include linear stochastic time varying consensus schemes, represented by directed graphs. There are variations in the form, definition of eligibility traces, selection of time scales, and incorporation of consensus iterations. The paper's main contribution is a convergence analysis based on the general properties of the underlying Feller-Markov processes and the stochastic time varying consensus model. The authors prove that the parameter estimates generated by all proposed algorithms weakly converge to the corresponding ordinary differential equations with defined invariant sets, under general assumptions. The methodology can be applied to temporal-difference algorithms under weaker information structure constraints, and specific guidelines for communication network design are provided. The paper also demonstrates the variance reduction effect of the proposed algorithms by formulating and analyzing an asymptotic stochastic differential equation and provides simulation results to show superior properties of the algorithms.",1
"System operators are faced with increasingly volatile operating conditions. In order to manage system reliability in a cost-effective manner, control room operators are turning to computerised decision support tools based on AI and machine learning. Specifically, Reinforcement Learning (RL) is a promising technique to train agents that suggest grid control actions to operators. In this paper, a simple baseline approach is presented using RL to represent an artificial control room operator that can operate a IEEE 14-bus test case for a duration of 1 week. This agent takes topological switching actions to control power flows on the grid, and is trained on only a single well-chosen scenario. The behaviour of this agent is tested on different time-series of generation and demand, demonstrating its ability to operate the grid successfully in 965 out of 1000 scenarios. The type and variability of topologies suggested by the agent are analysed across the test scenarios, demonstrating efficient and diverse agent behaviour.",0
"Control room operators are facing more unpredictable operating conditions nowadays. To manage system reliability in a cost-effective way, they are utilizing computerized decision support tools that are based on AI and machine learning. Among these techniques, Reinforcement Learning (RL) shows promise in training agents that provide suggested grid control actions to operators. In this study, a straightforward baseline method is presented, which employs RL to create an artificial control room operator. This agent can operate an IEEE 14-bus test case for a week by taking topological switching actions to regulate power flows on the grid. It is trained for a single well-chosen scenario, and its performance is tested on different generation and demand time-series. The agent's ability to successfully manage the grid is demonstrated in 965 out of 1000 scenarios. The study also examines the type and variability of topologies suggested by the agent across the test scenarios, showing effective and varied agent behavior.",1
"Action advising is a peer-to-peer knowledge exchange technique built on the teacher-student paradigm to alleviate the sample inefficiency problem in deep reinforcement learning. Recently proposed student-initiated approaches have obtained promising results. However, due to being in the early stages of development, these also have some substantial shortcomings. One of the abilities that are absent in the current methods is further utilising advice by reusing, which is especially crucial in the practical settings considering the budget and cost constraints in peer-to-peer. In this study, we present an approach to enable the student agent to imitate previously acquired advice to reuse them directly in its exploration policy, without any interventions in the learning mechanism itself. In particular, we employ a behavioural cloning module to imitate the teacher policy and use dropout regularisation to have a notion of epistemic uncertainty to keep track of which state-advice pairs are actually collected. As the results of experiments we conducted in three Atari games show, advice reusing via generalisation is indeed a feasible option in deep RL and our approach can successfully achieve this while significantly improving the learning performance, even when paired with a simple early advising heuristic.",0
"The technique of action advising involves exchanging knowledge between peers in order to solve the inefficiency problem in deep reinforcement learning. While recent student-initiated approaches have shown promise, they are still in the early stages of development and have significant drawbacks. One such drawback is the inability to reuse previously acquired advice, which is important in practical peer-to-peer settings with budget and cost constraints. This study presents an approach that allows student agents to imitate and reuse previously acquired advice in their exploration policy without changing the learning mechanism itself. The approach uses a behavioural cloning module to imitate the teacher policy and dropout regularisation to keep track of collected state-advice pairs. The results of experiments conducted in three Atari games demonstrate the feasibility of advice reusing via generalisation and the success of the proposed approach in significantly improving learning performance, even with a simple early advising heuristic.",1
"Recent years have seen a rise in interest in terms of using machine learning, particularly reinforcement learning (RL), for production scheduling problems of varying degrees of complexity. The general approach is to break down the scheduling problem into a Markov Decision Process (MDP), whereupon a simulation implementing the MDP is used to train an RL agent. Since existing studies rely on (sometimes) complex simulations for which the code is unavailable, the experiments presented are hard, or, in the case of stochastic environments, impossible to reproduce accurately. Furthermore, there is a vast array of RL designs to choose from. To make RL methods widely applicable in production scheduling and work out their strength for the industry, the standardization of model descriptions - both production setup and RL design - and validation scheme are a prerequisite. Our contribution is threefold: First, we standardize the description of production setups used in RL studies based on established nomenclature. Secondly, we classify RL design choices from existing publications. Lastly, we propose recommendations for a validation scheme focusing on reproducibility and sufficient benchmarking.",0
"In recent times, there has been an increase in interest in the use of machine learning, particularly reinforcement learning (RL), for tackling production scheduling problems of different levels of complexity. The standard approach involves breaking down the scheduling issue into a Markov Decision Process (MDP), followed by training an RL agent via a simulation that implements the MDP. However, existing research relies on complex simulations with unavailable code, making it challenging to reproduce experiments accurately, especially in stochastic environments. Additionally, there is a wide range of RL designs to choose from. Therefore, to make RL methods more widely applicable in production scheduling and determine their industry strength, it is necessary to standardize model descriptions and validation schemes. Our contribution has three parts: Firstly, we standardize the description of production setups using established terminology. Secondly, we classify RL design choices from previous studies. Finally, we recommend a validation scheme that emphasizes reproducibility and sufficient benchmarking.",1
"This paper studies the problem of developing an approximate dynamic programming (ADP) framework for learning online the value function of an infinite-horizon optimal problem while obeying safety constraints expressed as control barrier functions (CBFs). Our approach is facilitated by the development of a novel class of CBFs, termed Lyapunov-like CBFs (LCBFs), that retain the beneficial properties of CBFs for developing minimally-invasive safe control policies while also possessing desirable Lyapunov-like qualities such as positive semi-definiteness. We show how these LCBFs can be used to augment a learning-based control policy so as to guarantee safety and then leverage this approach to develop a safe exploration framework in a model-based reinforcement learning setting. We demonstrate that our developed approach can handle more general safety constraints than state-of-the-art safe ADP methods through a variety of numerical examples.",0
"The aim of this paper is to explore the development of an approximate dynamic programming (ADP) framework that can learn the value function of an optimal problem with infinite-horizon while adhering to safety constraints expressed as control barrier functions (CBFs). To achieve this, the authors introduce a new category of CBFs, known as Lyapunov-like CBFs (LCBFs), which offer the benefits of CBFs while also possessing desirable Lyapunov-like features such as positive semi-definiteness. The authors demonstrate how LCBFs can be employed to augment a learning-based control policy, ensuring safety, and leveraging this method to create a safe exploration framework in a model-based reinforcement learning environment. Numerical examples are used to demonstrate the effectiveness of the proposed approach in handling a more diverse range of safety constraints than current safe ADP methodologies.",1
"Explainable AI (XAI) is a research area whose objective is to increase trustworthiness and to enlighten the hidden mechanism of opaque machine learning techniques. This becomes increasingly important in case such models are applied to the chemistry domain, for its potential impact on humans' health, e.g, toxicity analysis in pharmacology. In this paper, we present a novel approach to tackle explainability of deep graph networks in the context of molecule property prediction t asks, named MEG (Molecular Explanation Generator). We generate informative counterfactual explanations for a specific prediction under the form of (valid) compounds with high structural similarity and different predicted properties. Given a trained DGN, we train a reinforcement learning based generator to output counterfactual explanations. At each step, MEG feeds the current candidate counterfactual into the DGN, collects the prediction and uses it to reward the RL agent to guide the exploration. Furthermore, we restrict the action space of the agent in order to only keep actions that maintain the molecule in a valid state. We discuss the results showing how the model can convey non-ML experts with key insights into the learning model focus in the neighbourhood of a molecule.",0
"The research area of Explainable AI (XAI) aims to enhance trustworthiness and shed light on the opaque machine learning techniques that are increasingly being used in domains such as chemistry, where their potential impact on human health is significant. For instance, in pharmacology, these techniques are used for toxicity analysis. This paper introduces MEG (Molecular Explanation Generator), a novel approach to addressing the explainability of deep graph networks in the context of molecule property prediction tasks. MEG generates informative counterfactual explanations in the form of valid compounds with high structural similarity but different predicted properties. To achieve this, a reinforcement learning-based generator is trained to output counterfactual explanations, and MEG feeds the current candidate counterfactual into the DGN, collects the prediction, and rewards the RL agent to guide exploration. Additionally, the agent's action space is restricted to maintain the molecule in a valid state. The results show that the model can provide non-ML experts with important insights into the learning model's focus in the neighborhood of a molecule.",1
"Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.",0
"The process of Adversarial Imitation Learning involves developing a discriminator to distinguish between expert and generated demonstrations, as well as a generator's policy to create trajectories that can deceive the discriminator. However, this approach can be problematic due to the combination of unstable adversarial training and inefficient reinforcement learning. To address this, we propose a new discriminator formulation that eliminates the need for policy optimization steps. Our discriminator is conditioned on both the previous generator's policy and a learnable policy, allowing it to learn the optimal generator's policy directly. As a result, the generator's optimization problem is solved during the discriminator's update, removing the Reinforcement Learning phase and reducing the burden on implementation and computation by half. We demonstrate that our simplified approach is competitive with common Imitation Learning methods across a range of tasks.",1
"Accurate models of mechanical system dynamics are often critical for model-based control and reinforcement learning. Fully data-driven dynamics models promise to ease the process of modeling and analysis, but require considerable amounts of data for training and often do not generalize well to unseen parts of the state space. Combining data-driven modelling with prior analytical knowledge is an attractive alternative as the inclusion of structural knowledge into a regression model improves the model's data efficiency and physical integrity. In this article, we survey supervised regression models that combine rigid-body mechanics with data-driven modelling techniques. We analyze the different latent functions (such as kinetic energy or dissipative forces) and operators (such as differential operators and projection matrices) underlying common descriptions of rigid-body mechanics. Based on this analysis, we provide a unified view on the combination of data-driven regression models, such as neural networks and Gaussian processes, with analytical model priors. Further, we review and discuss key techniques for designing structured models such as automatic differentiation.",0
"The accurate representation of mechanical system dynamics is crucial for model-based control and reinforcement learning. Although fully data-driven dynamics models offer a more streamlined modeling and analysis process, they often require substantial amounts of data for training and may not be applicable to areas of the state space that have not been observed. Combining data-driven modeling with prior analytical knowledge is a promising alternative, as incorporating structural knowledge into a regression model can enhance its data efficiency and physical accuracy. This article explores supervised regression models that combine data-driven methods, like neural networks and Gaussian processes, with rigid-body mechanics. We examine the distinct latent functions and operators underlying common descriptions of rigid-body mechanics. Additionally, we provide a comprehensive overview of how to combine data-driven regression models with analytical model priors, including techniques like automatic differentiation, and discuss their importance.",1
"Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data, typically through message passing among nodes by aggregating their neighborhood information via different operations. While promising, most existing GNNs oversimplified the complexity and diversity of the edges in the graph, and thus inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this paper, we propose RioGNN, a novel Reinforced, recursive and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism.",0
"The use of Graph Neural Networks (GNNs) is widespread in learning the representation of various structured graph data. This is achieved through message passing among nodes using different techniques to aggregate neighborhood information. However, most existing GNNs are limited in their ability to handle the complexity and diversity of edges in ubiquitous heterogeneous graphs, which are represented as multi-relational graphs. In this paper, we propose RioGNN, a new Reinforced, recursive, and flexible neighborhood selection guided multi-relational Graph Neural Network architecture to address these limitations. We create a multi-relational graph that reflects the heterogeneity of nodes, edges, attributes, and labels for the task at hand. To avoid embedding over-assimilation between different types of nodes, we use a label-aware neural similarity measure to identify the most similar neighbors based on node attributes. We also develop a reinforced relation-aware neighbor selection mechanism to choose the most similar neighbors within a relation before aggregating all neighborhood information from different relations to obtain the final node embedding. To improve the efficiency of neighbor selection, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with improved explainability by recognizing the importance of each relation through the filtering threshold mechanism.",1
"Unmanned aerial vehicles (UAVs) are expected to be an integral part of wireless networks. In this paper, we aim to find collision-free paths for multiple cellular-connected UAVs, while satisfying requirements of connectivity with ground base stations (GBSs) in the presence of a dynamic jammer. We first formulate the problem as a sequential decision making problem in discrete domain, with connectivity, collision avoidance, and kinematic constraints. We, then, propose an offline temporal difference (TD) learning algorithm with online signal-to-interference-plus-noise ratio (SINR) mapping to solve the problem. More specifically, a value network is constructed and trained offline by TD method to encode the interactions among the UAVs and between the UAVs and the environment; and an online SINR mapping deep neural network (DNN) is designed and trained by supervised learning, to encode the influence and changes due to the jammer. Numerical results show that, without any information on the jammer, the proposed algorithm can achieve performance levels close to that of the ideal scenario with the perfect SINR-map. Real-time navigation for multi-UAVs can be efficiently performed with high success rates, and collisions are avoided.",0
"The use of unmanned aerial vehicles (UAVs) is expected to be a crucial aspect of wireless networks. The objective of this research is to identify routes that are free of collisions for multiple cellular-connected UAVs, while also fulfilling the connectivity requirements with ground base stations (GBSs) in the presence of a dynamic jammer. The issue is first formulated as a sequential decision-making problem in a discrete domain, with the inclusion of connectivity, collision avoidance, and kinematic constraints. Subsequently, an offline temporal difference (TD) learning algorithm is proposed, along with online signal-to-interference-plus-noise ratio (SINR) mapping, to resolve the problem. A value network is created and trained offline by TD method to encode the interactions among the UAVs and between the UAVs and the environment. Additionally, an online SINR mapping deep neural network (DNN) is designed and trained by supervised learning to encode the influence and changes due to the jammer. The numerical findings suggest that the proposed algorithm can achieve performance levels comparable to those of an ideal scenario with a perfect SINR-map, even without any knowledge about the jammer. Real-time navigation for multi-UAVs can be efficiently performed with high success rates, and collisions can be avoided.",1
"Using insight from numerical approximation of ODEs and the problem formulation and solution methodology of TD learning through a Galerkin relaxation, I propose a new class of TD learning algorithms. After applying the improved numerical methods, the parameter being approximated has a guaranteed order of magnitude reduction in the Taylor Series error of the solution to the ODE for the parameter $\theta(t)$ that is used in constructing the linearly parameterized value function. Predictor-Corrector Temporal Difference (PCTD) is what I call the translated discrete time Reinforcement Learning(RL) algorithm from the continuous time ODE using the theory of Stochastic Approximation(SA). Both causal and non-causal implementations of the algorithm are provided, and simulation results are listed for an infinite horizon task to compare the original TD(0) algorithm against both versions of PCTD(0).",0
"A new category of TD learning algorithms is proposed using insights from numerical approximation of ODEs and the problem formulation and solution methodology of TD learning through a Galerkin relaxation. The application of improved numerical methods guarantees a significant reduction in the Taylor Series error of the solution to the ODE for the parameter $\theta(t)$ used in constructing the linearly parameterized value function. The translated discrete time Reinforcement Learning (RL) algorithm from the continuous time ODE is referred to as Predictor-Corrector Temporal Difference (PCTD) and is based on the theory of Stochastic Approximation (SA). Both causal and non-causal implementations of the algorithm are available, and simulation results are presented comparing the original TD(0) algorithm to both versions of PCTD(0) for an infinite horizon task.",1
"Mobile ad hoc computing (MAHC), which allows mobile devices to directly share their computing resources, is a promising solution to address the growing demands for computing resources required by mobile devices. However, offloading a computation task from a mobile device to other mobile devices is a challenging task due to frequent topology changes and link failures because of node mobility, unstable and unknown communication environments, and the heterogeneous nature of these devices. To address these challenges, in this paper, we introduce a novel coded computation scheme based on multi-agent reinforcement learning (MARL), which has many promising features such as adaptability to network changes, high efficiency and robustness to uncertain system disturbances, consideration of node heterogeneity, and decentralized load allocation. Comprehensive simulation studies demonstrate that the proposed approach can outperform state-of-the-art distributed computing schemes.",0
"Mobile ad hoc computing (MAHC) is a promising solution for meeting the increasing demand for computing resources on mobile devices. However, offloading computation tasks between mobile devices is challenging due to frequent topology changes and link failures caused by node mobility, unstable and unknown communication environments, and the heterogeneous nature of the devices. This paper presents a novel coded computation scheme based on multi-agent reinforcement learning (MARL) to address these challenges. The scheme is adaptable to network changes, efficient, robust to uncertain system disturbances, considers node heterogeneity, and features decentralized load allocation. Comprehensive simulation studies show that this approach outperforms existing distributed computing schemes.",1
"Training with Reinforcement Learning requires a reward function that is used to guide the agent towards achieving its objective. However, designing smooth and well-behaved rewards is in general not trivial and requires significant human engineering efforts. Generating rewards in self-supervised way, by inspiring the agent with an intrinsic desire to learn and explore the environment, might induce more general behaviours. In this work, we propose a curiosity-based bonus as intrinsic reward for Reinforcement Learning, computed as the Bayesian surprise with respect to a latent state variable, learnt by reconstructing fixed random features. We extensively evaluate our model by measuring the agent's performance in terms of environment exploration, for continuous tasks, and looking at the game scores achieved, for video games. Our model is computationally cheap and empirically shows state-of-the-art performance on several problems. Furthermore, experimenting on an environment with stochastic actions, our approach emerged to be the most resilient to simple stochasticity. Further visualization is available on the project webpage.(https://lbsexploration.github.io/)",0
"The use of Reinforcement Learning for training an agent requires a reward function to direct it towards accomplishing its goal. However, creating a smooth and well-designed reward function is a challenging task that requires substantial human effort. Alternatively, generating rewards in a self-supervised way by motivating the agent to learn and explore the environment may lead to more general behaviors. This study proposes an intrinsic reward based on curiosity, computed as the Bayesian surprise concerning a latent state variable, learned by reconstructing random features. The model's performance was evaluated extensively by measuring the agent's exploration of the environment for continuous tasks and examining the scores achieved in video games. Our approach is cost-effective and exhibited state-of-the-art performance on various problems. Additionally, our method was the most resilient to simple stochasticity when tested in an environment with stochastic actions. More visualizations are accessible on the project webpage (https://lbsexploration.github.io/).",1
"Overdetermined linear systems are common in reinforcement learning, e.g., in Q and value function estimation with function approximation. The standard least-squares criterion, however, leads to a solution that is unduly influenced by rows with large norms. This is a serious issue, especially when the matrices in these systems are beyond user control. To address this, we propose a scale-invariant criterion that we then use to develop two novel algorithms for value function estimation: Normalized Monte Carlo and Normalized TD(0). Separately, we also introduce a novel adaptive stepsize that may be useful beyond this work as well. We use simulations and theoretical guarantees to demonstrate the efficacy of our ideas.",0
"Reinforcement learning often involves overdetermined linear systems, such as in the estimation of Q and value functions using function approximation. However, the conventional least-squares approach can produce biased solutions due to the influence of rows with large norms, which is particularly problematic when the matrices in these systems are not within the user's control. To address this issue, we propose a scale-invariant criterion and utilize it to develop two new algorithms for value function estimation, namely Normalized Monte Carlo and Normalized TD(0). Additionally, we introduce a novel adaptive stepsize that may have broader applications beyond this study. Our ideas are supported by simulations and theoretical evidence of their effectiveness.",1
"There are relatively few conventions followed in reinforcement learning (RL) environments to structure the action spaces. As a consequence the application of RL algorithms to tasks with large action spaces with multiple components require additional effort to adjust to different formats. In this paper we introduce {\em Conditional Action Trees} with two main objectives: (1) as a method of structuring action spaces in RL to generalise across several action space specifications, and (2) to formalise a process to significantly reduce the action space by decomposing it into multiple sub-spaces, favoring a multi-staged decision making approach. We show several proof-of-concept experiments validating our scheme, ranging from environments with basic discrete action spaces to those with large combinatorial action spaces commonly found in RTS-style games.",0
"Reinforcement learning (RL) environments lack conventions for structuring action spaces, making it difficult to apply RL algorithms to tasks with large action spaces. To address this issue, we propose the use of Conditional Action Trees for two purposes: (1) to structure action spaces in RL and ensure generalisation across multiple specifications, and (2) to decompose the action space into sub-spaces, allowing for a multi-staged decision making approach and significantly reducing the action space. We demonstrate the effectiveness of our method through proof-of-concept experiments in environments with both basic and combinatorial action spaces, such as RTS-style games.",1
"We tackle a common scenario in imitation learning (IL), where agents try to recover the optimal policy from expert demonstrations without further access to the expert or environment reward signals. Except the simple Behavior Cloning (BC) that adopts supervised learning followed by the problem of compounding error, previous solutions like inverse reinforcement learning (IRL) and recent generative adversarial methods involve a bi-level or alternating optimization for updating the reward function and the policy, suffering from high computational cost and training instability. Inspired by recent progress in energy-based model (EBM), in this paper, we propose a simplified IL framework named Energy-Based Imitation Learning (EBIL). Instead of updating the reward and policy iteratively, EBIL breaks out of the traditional IRL paradigm by a simple and flexible two-stage solution: first estimating the expert energy as the surrogate reward function through score matching, then utilizing such a reward for learning the policy by reinforcement learning algorithms. EBIL combines the idea of both EBM and occupancy measure matching, and via theoretic analysis we reveal that EBIL and Max-Entropy IRL (MaxEnt IRL) approaches are two sides of the same coin, and thus EBIL could be an alternative of adversarial IRL methods. Extensive experiments on qualitative and quantitative evaluations indicate that EBIL is able to recover meaningful and interpretative reward signals while achieving effective and comparable performance against existing algorithms on IL benchmarks.",0
"This paper addresses a common issue in imitation learning (IL) where agents aim to learn the optimal policy from expert demonstrations without access to the expert or environment rewards. Previous solutions, such as inverse reinforcement learning (IRL) and generative adversarial methods, involve complex optimization processes that are computationally expensive and prone to training instability. Inspired by recent developments in energy-based models (EBM), we propose a simplified IL framework called Energy-Based Imitation Learning (EBIL). Instead of iteratively updating the reward and policy, EBIL uses a two-stage approach: estimating the expert energy as the surrogate reward function through score matching, and then using this reward to learn the policy through reinforcement learning algorithms. We show that EBIL and Max-Entropy IRL (MaxEnt IRL) approaches are similar, and EBIL can be used as an alternative to adversarial IRL methods. Our experiments show that EBIL effectively recovers meaningful and interpretable reward signals and performs comparably to existing algorithms on IL benchmarks.",1
"We propose a simple, practical, and intuitive approach for domain adaptation in reinforcement learning. Our approach stems from the idea that the agent's experience in the source domain should look similar to its experience in the target domain. Building off of a probabilistic view of RL, we formally show that we can achieve this goal by compensating for the difference in dynamics by modifying the reward function. This modified reward function is simple to estimate by learning auxiliary classifiers that distinguish source-domain transitions from target-domain transitions. Intuitively, the modified reward function penalizes the agent for visiting states and taking actions in the source domain which are not possible in the target domain. Said another way, the agent is penalized for transitions that would indicate that the agent is interacting with the source domain, rather than the target domain. Our approach is applicable to domains with continuous states and actions and does not require learning an explicit model of the dynamics. On discrete and continuous control tasks, we illustrate the mechanics of our approach and demonstrate its scalability to high-dimensional tasks.",0
"A practical and intuitive approach for domain adaptation in reinforcement learning is proposed, based on the concept that the agent's experience in the source domain should resemble its experience in the target domain. Using a probabilistic view of RL, we demonstrate that this can be achieved by modifying the reward function to account for differences in dynamics. This modified reward function is simple to estimate, as auxiliary classifiers can be used to distinguish between source-domain and target-domain transitions. The modified reward function penalizes the agent for actions that are not possible in the target domain, which prevents the agent from interacting with the source domain. This approach is suitable for continuous state and action domains and does not require an explicit model of the dynamics. We illustrate the mechanics of our approach on both discrete and continuous control tasks and demonstrate its ability to scale to high-dimensional tasks.",1
"While deep reinforcement learning (RL) promises freedom from hand-labeled data, great successes, especially for Embodied AI, require significant work to create supervision via carefully shaped rewards. Indeed, without shaped rewards, i.e., with only terminal rewards, present-day Embodied AI results degrade significantly across Embodied AI problems from single-agent Habitat-based PointGoal Navigation (SPL drops from 55 to 0) and two-agent AI2-THOR-based Furniture Moving (success drops from 58% to 1%) to three-agent Google Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1). As training from shaped rewards doesn't scale to more realistic tasks, the community needs to improve the success of training with terminal rewards. For this we propose GridToPix: 1) train agents with terminal rewards in gridworlds that generically mirror Embodied AI environments, i.e., they are independent of the task; 2) distill the learned policy into agents that reside in complex visual worlds. Despite learning from only terminal rewards with identical models and RL algorithms, GridToPix significantly improves results across tasks: from PointGoal Navigation (SPL improves from 0 to 64) and Furniture Moving (success improves from 1% to 25%) to football gameplay (game score improves from 0.1 to 0.6). GridToPix even helps to improve the results of shaped reward training.",0
"Although deep reinforcement learning (RL) offers the potential to eliminate the need for hand-labeled data, achieving significant successes in Embodied AI requires substantial effort in creating supervision through carefully designed rewards. Without these shaped rewards, performance in current Embodied AI tasks deteriorates considerably, including single-agent Habitat-based PointGoal Navigation (SPL drops from 55 to 0), two-agent AI2-THOR-based Furniture Moving (success drops from 58% to 1%), and three-agent Google Football-based 3 vs. 1 with Keeper (game score drops from 0.6 to 0.1). As shaped reward training is not scalable for more complex tasks, there is a need to enhance training success with terminal rewards. To address this, GridToPix proposes training agents with terminal rewards in gridworlds that mimic Embodied AI environments and distilling the learned policy into agents that operate in complex visual worlds. Remarkably, GridToPix significantly enhances results across tasks, even outperforming shaped reward training in some cases. For instance, in PointGoal Navigation, SPL improves from 0 to 64, Furniture Moving success improves from 1% to 25%, and football gameplay score improves from 0.1 to 0.6.",1
"The applicability of reinforcement learning (RL) algorithms in real-world domains often requires adherence to safety constraints, a need difficult to address given the asymptotic nature of the classic RL optimization objective. In contrast to the traditional RL objective, safe exploration considers the maximization of expected returns under safety constraints expressed in expected cost returns. We introduce a model-based safe exploration algorithm for constrained high-dimensional control to address the often prohibitively high sample complexity of model-free safe exploration algorithms. Further, we provide theoretical and empirical analyses regarding the implications of model-usage on constrained policy optimization problems and introduce a practical algorithm that accelerates policy search with model-generated data. The need for accurate estimates of a policy's constraint satisfaction is in conflict with accumulating model-errors. We address this issue by quantifying model-uncertainty as the expected Kullback-Leibler divergence between predictions of an ensemble of probabilistic dynamics models and constrain this error-measure, resulting in an adaptive resampling scheme and dynamically limited rollout horizons. We evaluate this approach on several simulated constrained robot locomotion tasks with high-dimensional action- and state-spaces. Our empirical studies find that our algorithm reaches model-free performances with a 10-20 fold reduction of training samples while maintaining approximate constraint satisfaction levels of model-free methods.",0
"To apply reinforcement learning (RL) algorithms in real-world situations, it is crucial to adhere to safety constraints, which is difficult because the classic RL optimization objective is asymptotic in nature. Safe exploration, on the other hand, aims to maximize expected returns while meeting safety constraints expressed in expected cost returns. We present a model-based safe exploration algorithm for high-dimensional control, which addresses the high sample complexity of model-free safe exploration algorithms. We also introduce a practical algorithm that accelerates policy search by using model-generated data. However, accurately estimating a policy's constraint satisfaction conflicts with accumulating model-errors. To address this issue, we quantify model-uncertainty using an expected Kullback-Leibler divergence and constrain this error-measure, resulting in an adaptive resampling scheme and dynamically limited rollout horizons. We evaluate our approach on several simulated constrained robot locomotion tasks and find that our algorithm achieves model-free performances with a 10-20 fold reduction in training samples while maintaining approximate constraint satisfaction levels of model-free methods.",1
"Reinforcement learning (RL) is a control approach that can handle nonlinear stochastic optimal control problems. However, despite the promise exhibited, RL has yet to see marked translation to industrial practice primarily due to its inability to satisfy state constraints. In this work we aim to address this challenge. We propose an 'oracle'-assisted constrained Q-learning algorithm that guarantees the satisfaction of joint chance constraints with a high probability, which is crucial for safety critical tasks. To achieve this, constraint tightening (backoffs) are introduced and adjusted using Broyden's method, hence making them self-tuned. This results in a general methodology that can be imbued into approximate dynamic programming-based algorithms to ensure constraint satisfaction with high probability. Finally, we present case studies that analyze the performance of the proposed approach and compare this algorithm with model predictive control (MPC). The favorable performance of this algorithm signifies a step toward the incorporation of RL into real world optimization and control of engineering systems, where constraints are essential in ensuring safety.",0
"Reinforcement learning (RL) is a control method suitable for handling nonlinear stochastic optimal control problems. Despite its potential, RL has yet to be widely adopted in industry because it cannot comply with state constraints. Our objective is to address this issue by proposing an 'oracle'-assisted constrained Q-learning algorithm. This algorithm ensures that joint chance constraints are met with a high probability, which is crucial for safety-critical tasks. We use Broyden's method to introduce and adjust constraint tightening (backoffs), making them self-tuned. This methodology can be used in approximate dynamic programming-based algorithms to guarantee constraint satisfaction with high probability. We present case studies that demonstrate the effectiveness of the proposed approach and compare it with model predictive control (MPC). Our algorithm's favorable performance represents a significant step forward in incorporating RL into the optimization and control of engineering systems where constraints are essential for safety.",1
"Depth adjustment aims to enhance the visual experience of stereoscopic 3D (S3D) images, which accompanied with improving visual comfort and depth perception. For a human expert, the depth adjustment procedure is a sequence of iterative decision making. The human expert iteratively adjusts the depth until he is satisfied with the both levels of visual comfort and the perceived depth. In this work, we present a novel deep reinforcement learning (DRL)-based approach for depth adjustment named VCA-RL (Visual Comfort Aware Reinforcement Learning) to explicitly model human sequential decision making in depth editing operations. We formulate the depth adjustment process as a Markov decision process where actions are defined as camera movement operations to control the distance between the left and right cameras. Our agent is trained based on the guidance of an objective visual comfort assessment metric to learn the optimal sequence of camera movement actions in terms of perceptual aspects in stereoscopic viewing. With extensive experiments and user studies, we show the effectiveness of our VCA-RL model on three different S3D databases.",0
"The goal of depth adjustment is to enhance the overall experience of viewing stereoscopic 3D (S3D) images, which includes improving visual comfort and depth perception. Typically, the process of adjusting the depth is a series of decisions made by a human expert, who iteratively modifies the depth until they are satisfied with both the level of visual comfort and the perceived depth. In this study, we introduce a new approach called VCA-RL (Visual Comfort Aware Reinforcement Learning) that utilizes deep reinforcement learning (DRL) to model the sequential decision-making process involved in depth editing operations. We use a Markov decision process to define actions as camera movements, which control the distance between the left and right cameras. Our agent is trained with the guidance of an objective visual comfort assessment metric to learn the optimal sequence of camera movements for perceptual aspects in stereoscopic viewing. Through extensive experiments and user studies, we demonstrate the effectiveness of VCA-RL on three different S3D databases.",1
"Recently, we have struck the balance between the information freshness, in terms of age of information (AoI), experienced by users and energy consumed by sensors, by appropriately activating sensors to update their current status in caching enabled Internet of Things (IoT) networks [1]. To solve this problem, we cast the corresponding status update procedure as a continuing Markov Decision Process (MDP) (i.e., without termination states), where the number of state-action pairs increases exponentially with respect to the number of considered sensors and users. Moreover, to circumvent the curse of dimensionality, we have established a methodology for designing deep reinforcement learning (DRL) algorithms to maximize (resp. minimize) the average reward (resp. cost), by integrating R-learning, a tabular reinforcement learning (RL) algorithm tailored for maximizing the long-term average reward, and traditional DRL algorithms, initially developed to optimize the discounted long-term cumulative reward rather than the average one. In this technical report, we would present detailed discussions on the technical contributions of this methodology.",0
"We recently found a way to balance the freshness of information experienced by users, measured as the age of information (AoI), and the energy consumed by sensors in IoT networks with caching capabilities. We achieved this by activating sensors to update their current status appropriately. To address this issue, we formulated the corresponding status update process as a Markov Decision Process (MDP) without termination states. However, the number of state-action pairs increased exponentially as we considered more sensors and users, leading to the curse of dimensionality. To overcome this, we developed a methodology for designing deep reinforcement learning (DRL) algorithms that integrate R-learning, a tabular reinforcement learning (RL) algorithm that maximizes the long-term average reward, and traditional DRL algorithms that optimize the discounted long-term cumulative reward. Our technical report presents detailed discussions on the technical contributions of this methodology.",1
"Multitask Reinforcement Learning is a promising way to obtain models with better performance, generalisation, data efficiency, and robustness. Most existing work is limited to compatible settings, where the state and action space dimensions are the same across tasks. Graph Neural Networks (GNN) are one way to address incompatible environments, because they can process graphs of arbitrary size. They also allow practitioners to inject biases encoded in the structure of the input graph. Existing work in graph-based continuous control uses the physical morphology of the agent to construct the input graph, i.e., encoding limb features as node labels and using edges to connect the nodes if their corresponded limbs are physically connected. In this work, we present a series of ablations on existing methods that show that morphological information encoded in the graph does not improve their performance. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, we also propose Amorpheus, a transformer-based approach. Further results show that, while Amorpheus ignores the morphological information that GNNs encode, it nonetheless substantially outperforms GNN-based methods that use the morphological information to define the message-passing scheme.",0
"Multitask Reinforcement Learning shows promise in improving model performance, generalisation, data efficiency, and robustness. However, existing work is limited to compatible settings where state and action space dimensions remain the same across tasks. To address incompatible environments, Graph Neural Networks (GNN) can process graphs of varying sizes and incorporate biases encoded in the input graph structure. Previous graph-based continuous control work has used physical morphology to construct the input graph, but our study finds that incorporating morphological information does not improve performance. We propose a transformer-based approach called Amorpheus, which ignores morphological information that GNNs encode, yet still outperforms GNN-based methods that utilize morphological information in message-passing schemes. Our results suggest that while GNNs may extract benefits from graph structure, the difficulties they create for message passing outweigh these benefits.",1
"Reinforcement learning algorithms are highly sensitive to the choice of hyperparameters, typically requiring significant manual effort to identify hyperparameters that perform well on a new domain. In this paper, we take a step towards addressing this issue by using metagradients to automatically adapt hyperparameters online by meta-gradient descent (Xu et al., 2018). We apply our algorithm, Self-Tuning Actor-Critic (STAC), to self-tune all the differentiable hyperparameters of an actor-critic loss function, to discover auxiliary tasks, and to improve off-policy learning using a novel leaky V-trace operator. STAC is simple to use, sample efficient and does not require a significant increase in compute. Ablative studies show that the overall performance of STAC improved as we adapt more hyperparameters. When applied to the Arcade Learning Environment (Bellemare et al. 2012), STAC improved the median human normalized score in 200M steps from 243% to 364%. When applied to the DM Control suite (Tassa et al., 2018), STAC improved the mean score in 30M steps from 217 to 389 when learning with features, from 108 to 202 when learning from pixels, and from 195 to 295 in the Real-World Reinforcement Learning Challenge (Dulac-Arnold et al., 2020).",0
"The choice of hyperparameters greatly impacts the performance of reinforcement learning algorithms, often requiring extensive manual effort to find the best ones for new domains. To address this problem, we developed a method using metagradients to automatically adapt hyperparameters through meta-gradient descent. Our algorithm, called Self-Tuning Actor-Critic (STAC), can self-tune all differentiable hyperparameters of an actor-critic loss function, discover auxiliary tasks, and improve off-policy learning using a leaky V-trace operator. STAC is user-friendly, efficient, and does not require a significant increase in computational power. Our studies demonstrate that as more hyperparameters are adapted, the overall performance of STAC improves. We applied STAC to the Arcade Learning Environment and the DM Control suite, resulting in significant improvements in median human normalized score and mean score, respectively.",1
"Deep reinforcement learning (DRL) has achieved great successes in many simulated tasks. The sample inefficiency problem makes applying traditional DRL methods to real-world robots a great challenge. Generative Adversarial Imitation Learning (GAIL) -- a general model-free imitation learning method, allows robots to directly learn policies from expert trajectories in large environments. However, GAIL shares the limitation of other imitation learning methods that they can seldom surpass the performance of demonstrations. In this paper, to address the limit of GAIL, we propose GAN-Based Interactive Reinforcement Learning (GAIRL) from demonstration and human evaluative feedback by combining the advantages of GAIL and interactive reinforcement learning. We tested our proposed method in six physics-based control tasks, ranging from simple low-dimensional control tasks -- Cart Pole and Mountain Car, to difficult high-dimensional tasks -- Inverted Double Pendulum, Lunar Lander, Hopper and HalfCheetah. Our results suggest that with both optimal and suboptimal demonstrations, a GAIRL agent can always learn a more stable policy with optimal or close to optimal performance, while the performance of the GAIL agent is upper bounded by the performance of demonstrations or even worse than it. In addition, our results indicate the reason that GAIRL is superior over GAIL is the complementary effect of demonstrations and human evaluative feedback.",0
"While Deep Reinforcement Learning (DRL) has been successful in simulated tasks, its traditional methods are challenged when applied to real-world robots due to sample inefficiency. Generative Adversarial Imitation Learning (GAIL) is a model-free imitation learning method that allows robots to learn policies from expert trajectories in large environments. However, GAIL, like other imitation learning methods, has a limitation where it cannot surpass the performance of demonstrations. In response to this challenge, this paper proposes a new method called GAN-Based Interactive Reinforcement Learning (GAIRL) that combines the advantages of GAIL and interactive reinforcement learning to learn from demonstrations and human evaluative feedback. Our experiments on six physics-based control tasks show that GAIRL outperforms GAIL due to the complementary effect of demonstrations and human evaluative feedback. Specifically, GAIRL can always learn a more stable policy with optimal or near-optimal performance, while GAIL's performance is limited by the performance of demonstrations or even worse.",1
"By searching for shared inductive biases across tasks, meta-learning promises to accelerate learning on novel tasks, but with the cost of solving a complex bilevel optimization problem. We introduce and rigorously define the trade-off between accurate modeling and optimization ease in meta-learning. At one end, classic meta-learning algorithms account for the structure of meta-learning but solve a complex optimization problem, while at the other end domain randomized search (otherwise known as joint training) ignores the structure of meta-learning and solves a single level optimization problem. Taking MAML as the representative meta-learning algorithm, we theoretically characterize the trade-off for general non-convex risk functions as well as linear regression, for which we are able to provide explicit bounds on the errors associated with modeling and optimization. We also empirically study this trade-off for meta-reinforcement learning benchmarks.",0
"Meta-learning offers the potential to speed up learning on new tasks by identifying shared inductive biases across tasks. However, this comes at the cost of solving a complex two-level optimization problem. Our study aims to define and quantify the trade-off between accurate modeling and optimization ease in meta-learning. Traditional meta-learning algorithms consider meta-learning structure but require complex optimization, whereas domain randomized search (or joint training) ignores meta-learning structure and solves a single-level optimization problem. Using MAML as an example, we theoretically characterize this trade-off for general non-convex risk functions and linear regression, providing explicit error bounds for modeling and optimization. We also examine the trade-off empirically in meta-reinforcement learning benchmarks.",1
"Model-Predictive Control (MPC) is a powerful tool for controlling complex, real-world systems that uses a model to make predictions about future behavior. For each state encountered, MPC solves an online optimization problem to choose a control action that will minimize future cost. This is a surprisingly effective strategy, but real-time performance requirements warrant the use of simple models. If the model is not sufficiently accurate, then the resulting controller can be biased, limiting performance. We present a framework for improving on MPC with model-free reinforcement learning (RL). The key insight is to view MPC as constructing a series of local Q-function approximations. We show that by using a parameter $\lambda$, similar to the trace decay parameter in TD($\lambda$), we can systematically trade-off learned value estimates against the local Q-function approximations. We present a theoretical analysis that shows how error from inaccurate models in MPC and value function estimation in RL can be balanced. We further propose an algorithm that changes $\lambda$ over time to reduce the dependence on MPC as our estimates of the value function improve, and test the efficacy our approach on challenging high-dimensional manipulation tasks with biased models in simulation. We demonstrate that our approach can obtain performance comparable with MPC with access to true dynamics even under severe model bias and is more sample efficient as compared to model-free RL.",0
"The utilization of Model-Predictive Control (MPC) has proven to be an effective means of managing complicated, real-world systems by employing a model to anticipate future behavior. To minimize future cost, MPC solves an optimization problem for each state and selects a control action. However, due to real-time performance demands, uncomplicated models are utilized, which may lead to a biased controller, impacting performance. Our paper introduces a framework that improves on MPC with model-free reinforcement learning (RL). We demonstrate that by using a parameter $\lambda$, similar to the trace decay parameter in TD($\lambda$), we can systematically balance learned value estimates against local Q-function approximations. A theoretical analysis is provided, showing how inaccuracies in models and value function estimation in RL can be balanced. We propose an algorithm that modifies $\lambda$ over time to decrease the dependence on MPC as value function estimates improve and test the effectiveness of our approach on challenging high-dimensional manipulation tasks with biased models in simulation. Our approach can achieve performance comparable to MPC with access to true dynamics, even with severe model bias, and is more sample efficient than model-free RL.",1
"Virtual character animation control is a problem for which Reinforcement Learning (RL) is a viable approach. While current work have applied RL effectively to portray physics-based skills, social behaviours are challenging to design reward functions for, due to their lack of physical interaction with the world. On the other hand, data-driven implementations for these skills have been limited to supervised learning methods which require extensive training data and carry constraints on generalisability. In this paper, we propose RLAnimate, a novel data-driven deep RL approach to address this challenge, where we combine the strengths of RL together with an ability to learn from a motion dataset when creating agents. We formalise a mathematical structure for training agents by refining the conceptual roles of elements such as agents, environments, states and actions, in a way that leverages attributes of the character animation domain and model-based RL. An agent trained using our approach learns versatile animation dynamics to portray multiple behaviours, using an iterative RL training process, which becomes aware of valid behaviours via representations learnt from motion capture clips. We demonstrate, by training agents that portray realistic pointing and waving behaviours, that our approach requires a significantly lower training time, and substantially fewer sample episodes to be generated during training relative to state-of-the-art physics-based RL methods. Also, compared to existing supervised learning-based animation agents, RLAnimate needs a limited dataset of motion clips to generate representations of valid behaviours during training.",0
"Reinforcement Learning (RL) is a promising solution to the challenge of controlling virtual character animation. Although RL has been successful in simulating physics-based skills, designing rewards for social behaviors is difficult due to the lack of physical interactions. Supervised learning methods have been used for data-driven implementations of these skills, but they require extensive training data and have limitations in generalization. In this research paper, we present a new approach called RLAnimate, which combines the benefits of RL with the ability to learn from a motion dataset. We formalize a mathematical structure for training agents, refining the roles of agents, environments, states, and actions specific to character animation and model-based RL. Our approach trains agents to exhibit multiple behaviors by using an iterative RL process that identifies valid behaviors through representations learned from motion capture clips. We demonstrate that RLAnimate requires significantly less training time and fewer sample episodes than state-of-the-art physics-based RL methods to train agents that display realistic pointing and waving behaviors. Furthermore, compared to existing supervised learning-based animation agents, RLAnimate requires only a limited dataset of motion clips to generate representations of valid behaviors during training.",1
"Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.",0
"Enumerating all possible actions for many important real-world problems is not feasible due to the high-dimensionality and continuity of their action spaces. Hence, only a small subset of actions can be sampled for policy evaluation and improvement. This paper presents a sample-based policy iteration framework that can be applied to any reinforcement learning algorithm based upon policy iteration. We introduce Sampled MuZero, an extension of the MuZero algorithm that can learn in domains with complex action spaces by planning over sampled actions. Our approach is demonstrated on the board game of Go and two benchmark domains: DeepMind Control Suite and Real-World RL Suite.",1
"Learning efficiently from small amounts of data has long been the focus of model-based reinforcement learning, both for the online case when interacting with the environment and the offline case when learning from a fixed dataset. However, to date no single unified algorithm could demonstrate state-of-the-art results in both settings. In this work, we describe the Reanalyse algorithm which uses model-based policy and value improvement operators to compute new improved training targets on existing data points, allowing efficient learning for data budgets varying by several orders of magnitude. We further show that Reanalyse can also be used to learn entirely from demonstrations without any environment interactions, as in the case of offline Reinforcement Learning (offline RL). Combining Reanalyse with the MuZero algorithm, we introduce MuZero Unplugged, a single unified algorithm for any data budget, including offline RL. In contrast to previous work, our algorithm does not require any special adaptations for the off-policy or offline RL settings. MuZero Unplugged sets new state-of-the-art results in the RL Unplugged offline RL benchmark as well as in the online RL benchmark of Atari in the standard 200 million frame setting.",0
"For a long time, model-based reinforcement learning has focused on efficiently learning from small amounts of data, both in online and offline scenarios. However, no algorithm has been able to excel in both settings. The Reanalyse algorithm, which uses model-based policy and value improvement operators, can improve training targets on existing data points, enabling efficient learning with data budgets varying by several orders of magnitude. Reanalyse can also learn entirely from demonstrations without any environment interactions, making it suitable for offline reinforcement learning. By combining Reanalyse with the MuZero algorithm, we introduce MuZero Unplugged, a single unified algorithm that can handle any data budget, including offline RL, without requiring any special adaptations. MuZero Unplugged sets new state-of-the-art results in the RL Unplugged offline RL benchmark, as well as in the online RL benchmark of Atari in the standard 200 million frame setting.",1
"Supporting state-of-the-art AI research requires balancing rapid prototyping, ease of use, and quick iteration, with the ability to deploy experiments at a scale traditionally associated with production systems.Deep learning frameworks such as TensorFlow, PyTorch and JAX allow users to transparently make use of accelerators, such as TPUs and GPUs, to offload the more computationally intensive parts of training and inference in modern deep learning systems. Popular training pipelines that use these frameworks for deep learning typically focus on (un-)supervised learning. How to best train reinforcement learning (RL) agents at scale is still an active research area. In this report we argue that TPUs are particularly well suited for training RL agents in a scalable, efficient and reproducible way. Specifically we describe two architectures designed to make the best use of the resources available on a TPU Pod (a special configuration in a Google data center that features multiple TPU devices connected to each other by extremely low latency communication channels).",0
"To support advanced AI research, it is important to balance rapid prototyping, user-friendliness, and quick iteration, while also being able to deploy experiments on a large scale. Deep learning frameworks like TensorFlow, PyTorch, and JAX enable users to utilize accelerators such as TPUs and GPUs for computationally intensive parts of training and inference. These frameworks are commonly used for (un-)supervised learning, but training reinforcement learning (RL) agents at scale is still an active research area. This report suggests that TPUs are particularly suitable for scalable, efficient, and reproducible training of RL agents. The report describes two architectures designed to optimize the resources available on a TPU Pod, which is a special configuration in a Google data center featuring multiple TPU devices connected by low latency communication channels.",1
"Reinforcement learning, which acquires a policy maximizing long-term rewards, has been actively studied. Unfortunately, this learning type is too slow and difficult to use in practical situations because the state-action space becomes huge in real environments. Many studies have incorporated human knowledge into reinforcement Learning. Though human knowledge on trajectories is often used, a human could be asked to control an AI agent, which can be difficult. Knowledge on subgoals may lessen this requirement because humans need only to consider a few representative states on an optimal trajectory in their minds. The essential factor for learning efficiency is rewards. Potential-based reward shaping is a basic method for enriching rewards. However, it is often difficult to incorporate subgoals for accelerating learning over potential-based reward shaping. This is because the appropriate potentials are not intuitive for humans. We extend potential-based reward shaping and propose a subgoal-based reward shaping. The method makes it easier for human trainers to share their knowledge of subgoals. To evaluate our method, we obtained a subgoal series from participants and conducted experiments in three domains, four-rooms(discrete states and discrete actions), pinball(continuous and discrete), and picking(both continuous). We compared our method with a baseline reinforcement learning algorithm and other subgoal-based methods, including random subgoal and naive subgoal-based reward shaping. As a result, we found out that our reward shaping outperformed all other methods in learning efficiency.",0
"The study of reinforcement learning, in which a policy is acquired to maximize long-term rewards, has been active. However, this type of learning is often too slow and difficult to use in practical situations due to the vast state-action space in real environments. To address this, many studies have incorporated human knowledge into reinforcement learning, often utilizing human knowledge on trajectories. However, controlling an AI agent can be challenging for humans. Knowledge on subgoals may alleviate this issue, as humans only need to consider a few representative states on an optimal trajectory. Reward efficiency is crucial for learning, and potential-based reward shaping is a common method for enriching rewards. However, incorporating subgoals for accelerated learning is often difficult due to unintuitive potentials for humans. To overcome this, the authors propose a subgoal-based reward shaping method that makes it easier for human trainers to share their knowledge of subgoals. The authors evaluated their method through experiments in three domains and compared it to baseline algorithms and other subgoal-based methods. The results showed that their reward shaping method outperformed all other methods in learning efficiency.",1
"Reinforcement learning, which acquires a policy maximizing long-term rewards, has been actively studied. Unfortunately, this learning type is too slow and difficult to use in practical situations because the state-action space becomes huge in real environments. The essential factor for learning efficiency is rewards. Potential-based reward shaping is a basic method for enriching rewards. This method is required to define a specific real-value function called a potential function for every domain. It is often difficult to represent the potential function directly. SARSA-RS learns the potential function and acquires it. However, SARSA-RS can only be applied to the simple environment. The bottleneck of this method is the aggregation of states to make abstract states since it is almost impossible for designers to build an aggregation function for all states. We propose a trajectory aggregation that uses subgoal series. This method dynamically aggregates states in an episode during trial and error with only the subgoal series and subgoal identification function. It makes designer effort minimal and the application to environments with high-dimensional observations possible. We obtained subgoal series from participants for experiments. We conducted the experiments in three domains, four-rooms(discrete states and discrete actions), pinball(continuous and discrete), and picking(both continuous). We compared our method with a baseline reinforcement learning algorithm and other subgoal-based methods, including random subgoal and naive subgoal-based reward shaping. As a result, our reward shaping outperformed all other methods in learning efficiency.",0
"Reinforcement learning has been a focal point of research, as it aims to acquire a policy that maximizes long-term rewards. However, this learning type has proven to be slow and challenging to implement in practical scenarios due to the vast state-action space in real environments. The efficiency of learning is highly dependent on rewards, and potential-based reward shaping is a fundamental approach to enriching rewards. This method requires a specific real-value function called a potential function to be defined for each domain, but it can be difficult to represent directly. While SARSA-RS learns the potential function, it can only be applied to simple environments and struggles to aggregate states to create abstract states. We propose a trajectory aggregation method that uses subgoal series to dynamically aggregate states during trial and error. This method minimizes designer effort and enables application to environments with high-dimensional observations. We conducted experiments in three domains and compared our method with a baseline reinforcement learning algorithm and other subgoal-based methods. Our reward shaping outperformed all other methods in learning efficiency, demonstrating its effectiveness.",1
"In the past decade, model-free reinforcement learning (RL) has provided solutions to challenging domains such as robotics. Model-based RL shows the prospect of being more sample-efficient than model-free methods in terms of agent-environment interactions, because the model enables to extrapolate to unseen situations. In the more recent past, model-based methods have shown superior results compared to model-free methods in some challenging domains with non-linear state transitions. At the same time, it has become apparent that RL is not market-ready yet and that many real-world applications are going to require model-based approaches, because model-free methods are too sample-inefficient and show poor performance in early stages of training. The latter is particularly important in industry, e.g. in production systems that directly impact a company's revenue. This demonstrates the necessity for a toolbox to push the boundaries for model-based RL. While there is a plethora of toolboxes for model-free RL, model-based RL has received little attention in terms of toolbox development. Bellman aims to fill this gap and introduces the first thoroughly designed and tested model-based RL toolbox using state-of-the-art software engineering practices. Our modular approach enables to combine a wide range of environment models with generic model-based agent classes that recover state-of-the-art algorithms. We also provide an experiment harness to compare both model-free and model-based agents in a systematic fashion w.r.t. user-defined evaluation metrics (e.g. cumulative reward). This paves the way for new research directions, e.g. investigating uncertainty-aware environment models that are not necessarily neural-network-based, or developing algorithms to solve industrially-motivated benchmarks that share characteristics with real-world problems.",0
"Over the past decade, model-free reinforcement learning has been successful in solving complex domains such as robotics. However, model-based RL has shown potential in being more efficient in terms of agent-environment interactions due to its ability to extrapolate to unseen situations. Recent developments have demonstrated that model-based methods outperform model-free methods in domains with non-linear state transitions. Although RL is not yet market-ready, it has become clear that real-world applications will require model-based approaches because model-free methods are too sample-inefficient and perform poorly in early stages of training, which is especially important in industry. Therefore, there is a need for a toolbox to advance model-based RL, which has received little attention in terms of development compared to model-free RL. Bellman aims to address this gap by introducing the first tested and designed model-based RL toolbox using advanced software engineering practices. Our modular approach allows for a wide range of environment models to be combined with generic model-based agent classes that recover state-of-the-art algorithms. Additionally, we provide an experiment harness to systematically compare model-free and model-based agents using user-defined evaluation metrics. This toolkit opens up new research possibilities, such as investigating uncertainty-aware environment models and developing algorithms to solve industrially-motivated benchmarks that share characteristics with real-world problems.",1
"Model extraction increasingly attracts research attentions as keeping commercial AI models private can retain a competitive advantage. In some scenarios, AI models are trained proprietarily, where neither pre-trained models nor sufficient in-distribution data is publicly available. Model extraction attacks against these models are typically more devastating. Therefore, in this paper, we empirically investigate the behaviors of model extraction under such scenarios. We find the effectiveness of existing techniques significantly affected by the absence of pre-trained models. In addition, the impacts of the attacker's hyperparameters, e.g. model architecture and optimizer, as well as the utilities of information retrieved from queries, are counterintuitive. We provide some insights on explaining the possible causes of these phenomena. With these observations, we formulate model extraction attacks into an adaptive framework that captures these factors with deep reinforcement learning. Experiments show that the proposed framework can be used to improve existing techniques, and show that model extraction is still possible in such strict scenarios. Our research can help system designers to construct better defense strategies based on their scenarios.",0
"The attention of researchers has increasingly been drawn towards model extraction as keeping AI models private can give a competitive advantage. In situations where AI models are trained privately, neither pre-trained models nor sufficient in-distribution data is publicly available, making model extraction attacks particularly damaging. This paper empirically investigates the behavior of model extraction under such scenarios and finds that the effectiveness of existing techniques is significantly affected by the absence of pre-trained models. The study also highlights the counterintuitive impact of certain hyperparameters and utilities of information retrieved from queries. To address these issues, the paper proposes an adaptive framework using deep reinforcement learning to capture these factors. The experiments demonstrate that the proposed framework can improve existing techniques and that model extraction is still possible in such strict scenarios. The findings can aid system designers in developing better defense strategies for their specific scenarios.",1
"The upper confidence reinforcement learning (UCRL2) algorithm introduced in (Jaksch et al., 2010) is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this algorithm and its variants have remained until now mostly theoretical as numerical experiments in simple environments exhibit long burn-in phases before the learning takes place. In pursuit of practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities to compute confidence sets on the reward and (component-wise) transition distributions for each state-action pair. Furthermore, to tighten exploration, it uses an adaptive computation of the support of each transition distribution, which in turn enables us to revisit the extended value iteration procedure of UCRL2 to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments in standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable us to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear notions of local diameter and local effective support, thanks to variance-aware concentration bounds.",0
"The UCRL2 algorithm, as presented in (Jaksch et al., 2010), is widely used for minimizing regret in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its strong theoretical guarantees, numerical experiments in simple environments have shown that there is often a lengthy burn-in period before learning takes place, limiting its practical applicability. To address this, we introduce UCRL3, which builds on UCRL2 by incorporating two key modifications. Firstly, we use state-of-the-art time-uniform concentration inequalities to compute confidence sets on the reward and transition distributions for each state-action pair. Secondly, we provide an adaptive computation of the support of each transition distribution, which allows for tighter exploration. Through numerical experiments, we demonstrate that these modifications result in substantial improvements compared to UCRL2 and its variants. Furthermore, we derive a regret bound for UCRL3 that improves on UCRL2 and incorporates notions of local diameter and local effective support, thanks to variance-aware concentration bounds.",1
"Deep Q-Learning is an important reinforcement learning algorithm, which involves training a deep neural network, called Deep Q-Network (DQN), to approximate the well-known Q-function. Although wildly successful under laboratory conditions, serious gaps between theory and practice as well as a lack of formal guarantees prevent its use in the real world. Adopting a dynamical systems perspective, we provide a theoretical analysis of a popular version of Deep Q-Learning under realistic and verifiable assumptions. More specifically, we prove an important result on the convergence of the algorithm, characterizing the asymptotic behavior of the learning process. Our result sheds light on hitherto unexplained properties of the algorithm and helps understand empirical observations, such as performance inconsistencies even after training. Unlike previous theories, our analysis accommodates state Markov processes with multiple stationary distributions. In spite of the focus on Deep Q-Learning, we believe that our theory may be applied to understand other deep learning algorithms",0
"The Deep Q-Learning algorithm is significant in reinforcement learning as it trains a deep neural network, referred to as the Deep Q-Network (DQN), to approximate the Q-function. While it has been successful in controlled settings, its application in the real world is hindered by gaps between theory and practice and a lack of guarantees. This study offers a theoretical analysis of a popular version of Deep Q-Learning, using a dynamical systems approach, to overcome these issues. The research provides insights into the algorithm's properties and explains performance inconsistencies. Furthermore, the study accommodates state Markov processes with multiple stationary distributions, making it applicable to other deep learning algorithms. The result of this research is a characterization of the learning process's asymptotic behavior, proving the algorithm's convergence under realistic and verifiable assumptions.",1
"One-shot weight sharing methods have recently drawn great attention in neural architecture search due to high efficiency and competitive performance. However, weight sharing across models has an inherent deficiency, i.e., insufficient training of subnetworks in hypernetworks. To alleviate this problem, we present a simple yet effective architecture distillation method. The central idea is that subnetworks can learn collaboratively and teach each other throughout the training process, aiming to boost the convergence of individual models. We introduce the concept of prioritized path, which refers to the architecture candidates exhibiting superior performance during training. Distilling knowledge from the prioritized paths is able to boost the training of subnetworks. Since the prioritized paths are changed on the fly depending on their performance and complexity, the final obtained paths are the cream of the crop. We directly select the most promising one from the prioritized paths as the final architecture, without using other complex search methods, such as reinforcement learning or evolution algorithms. The experiments on ImageNet verify such path distillation method can improve the convergence ratio and performance of the hypernetwork, as well as boosting the training of subnetworks. The discovered architectures achieve superior performance compared to the recent MobileNetV3 and EfficientNet families under aligned settings. Moreover, the experiments on object detection and more challenging search space show the generality and robustness of the proposed method. Code and models are available at https://github.com/microsoft/cream.git.",0
"One-shot weight sharing techniques have become popular in the field of neural architecture search due to their efficiency and competitive performance. However, weight sharing has a drawback in that it leads to insufficient training of subnetworks in hypernetworks. To address this issue, we present a straightforward and effective approach to architecture distillation. The main concept is that subnetworks can learn collaboratively and teach each other during training, with the goal of enhancing the convergence of individual models. Our method uses prioritized paths, which are architecture candidates that exhibit superior performance during training. Distilling knowledge from the prioritized paths can improve the training of subnetworks. By dynamically changing the prioritized paths based on their performance and complexity, we obtain the best paths without using complex search methods like reinforcement learning or evolution algorithms. Experiments on ImageNet demonstrate that our path distillation method can improve the convergence ratio and performance of the hypernetwork, as well as boosting the training of subnetworks. Our discovered architectures outperform recent MobileNetV3 and EfficientNet families under aligned settings. Furthermore, experiments on object detection and more challenging search spaces demonstrate the generalizability and robustness of our proposed method. Code and models are available at https://github.com/microsoft/cream.git.",1
"In this paper, we introduce ChainerRL, an open-source deep reinforcement learning (DRL) library built using Python and the Chainer deep learning framework. ChainerRL implements a comprehensive set of DRL algorithms and techniques drawn from state-of-the-art research in the field. To foster reproducible research, and for instructional purposes, ChainerRL provides scripts that closely replicate the original papers' experimental settings and reproduce published benchmark results for several algorithms. Lastly, ChainerRL offers a visualization tool that enables the qualitative inspection of trained agents. The ChainerRL source code can be found on GitHub: https://github.com/chainer/chainerrl.",0
"ChainerRL is a DRL library developed using the Chainer deep learning framework and Python. The library consists of a wide range of DRL techniques and algorithms based on the latest research in the field. To ensure reproducibility and facilitate instruction, ChainerRL contains scripts that recreate the original experimental settings and replicate benchmark results for numerous algorithms. Additionally, the library includes a visualization tool that allows for qualitative analysis of trained agents. The ChainerRL source code is available on GitHub at https://github.com/chainer/chainerrl.",1
"It is of significance for an agent to learn a widely applicable and general-purpose policy that can achieve diverse goals including images and text descriptions. Considering such perceptually-specific goals, the frontier of deep reinforcement learning research is to learn a goal-conditioned policy without hand-crafted rewards. To learn this kind of policy, recent works usually take as the reward the non-parametric distance to a given goal in an explicit embedding space. From a different viewpoint, we propose a novel unsupervised learning approach named goal-conditioned policy with intrinsic motivation (GPIM), which jointly learns both an abstract-level policy and a goal-conditioned policy. The abstract-level policy is conditioned on a latent variable to optimize a discriminator and discovers diverse states that are further rendered into perceptually-specific goals for the goal-conditioned policy. The learned discriminator serves as an intrinsic reward function for the goal-conditioned policy to imitate the trajectory induced by the abstract-level policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of our proposed GPIM method which substantially outperforms prior techniques.",0
"Learning a versatile and general policy that can achieve a variety of goals, including those based on images and text descriptions, is important for agents. The frontier of deep reinforcement learning research involves developing a goal-conditioned policy that can achieve perceptually-specific goals without the need for hand-crafted rewards. Recent works use non-parametric distance to a given goal in an explicit embedding space as the reward. Our proposed unsupervised learning approach, called goal-conditioned policy with intrinsic motivation (GPIM), jointly learns an abstract-level policy and a goal-conditioned policy. The abstract-level policy is conditioned on a latent variable to optimize a discriminator, which discovers diverse states that are further transformed into perceptually-specific goals for the goal-conditioned policy. The learned discriminator serves as an intrinsic reward function for the goal-conditioned policy. Experiments on various robotic tasks demonstrate the effectiveness and efficiency of our proposed GPIM method, which outperforms prior techniques.",1
"Reinforcement learning has made great strides in recent years due to the success of methods using deep neural networks. However, such neural networks act as a black box, obscuring the inner workings. While reinforcement learning has the potential to solve unique problems, a lack of trust and understanding of reinforcement learning algorithms could prevent their widespread adoption. Here, we present a library that attaches a ""data scraper"" to deep reinforcement learning agents, acting as an observer, and then show how the data collected by the Atari Data Scraper can be used to understand and interpret deep reinforcement learning agents. The code for the Atari Data Scraper can be found here: https://github.com/IRLL/Atari-Data-Scraper",0
"The success of deep neural network methods has propelled reinforcement learning forward in recent years. However, the black box nature of neural networks can obscure their workings. This lack of transparency and comprehension could impede the widespread acceptance of reinforcement learning algorithms, despite their potential to address unique problems. To address this issue, we have developed a library that adds a ""data scraper"" to deep reinforcement learning agents to act as an observer. We have also demonstrated how the data collected by the Atari Data Scraper can be utilized to better understand and interpret deep reinforcement learning agents. The Atari Data Scraper code is available at https://github.com/IRLL/Atari-Data-Scraper.",1
"When making decisions, people often overlook critical information or are overly swayed by irrelevant information. A common approach to mitigate these biases is to provide decision-makers, especially professionals such as medical doctors, with decision aids, such as decision trees and flowcharts. Designing effective decision aids is a difficult problem. We propose that recently developed reinforcement learning methods for discovering clever heuristics for good decision-making can be partially leveraged to assist human experts in this design process. One of the biggest remaining obstacles to leveraging the aforementioned methods is that the policies they learn are opaque to people. To solve this problem, we introduce AI-Interpret: a general method for transforming idiosyncratic policies into simple and interpretable descriptions. Our algorithm combines recent advances in imitation learning and program induction with a new clustering method for identifying a large subset of demonstrations that can be accurately described by a simple, high-performing decision rule. We evaluate our new algorithm and employ it to translate information-acquisition policies discovered through metalevel reinforcement learning. The results of large behavioral experiments showed that prividing the decision rules generated by AI-Interpret as flowcharts significantly improved people's planning strategies and decisions across three diferent classes of sequential decision problems. Moreover, another experiment revealed that this approach is significantly more effective than training people by giving them performance feedback. Finally, a series of ablation studies confirmed that AI-Interpret is critical to the discovery of interpretable decision rules. We conclude that the methods and findings presented herein are an important step towards leveraging automatic strategy discovery to improve human decision-making.",0
"People often fail to consider important information or are influenced by irrelevant information when making decisions. Decision aids, such as decision trees and flowcharts, are commonly used to address these biases, particularly in professional fields like medicine. However, designing effective decision aids is a challenging task. We propose that reinforcement learning methods can help human experts in this process by discovering useful heuristics for decision-making. One challenge of using these methods is that the resulting policies are often hard for humans to understand. To overcome this, we introduce AI-Interpret, a method for transforming complex policies into simple and interpretable descriptions. Our algorithm combines recent advances in imitation learning and program induction with a new clustering method to identify simple decision rules that accurately describe a large subset of demonstrations. We tested our approach in behavioral experiments and found that providing the decision rules generated by AI-Interpret as flowcharts improved people's decision-making across different types of problems. This approach was also more effective than training people through performance feedback. Our findings demonstrate the importance of using automatic strategy discovery to enhance human decision-making.",1
"Deep reinforcement learning (RL) is a data-driven, model-free method capable of discovering complex control strategies for macroscopic objectives in high-dimensional systems, making its application towards flow control promising. Many systems of flow control interest possess symmetries that, when neglected, can significantly inhibit the learning and performance of a naive deep RL approach. Using a test-bed consisting of the Kuramoto-Sivashinsky Equation (KSE), equally spaced actuators, and a goal of minimizing dissipation and power cost, we demonstrate that by moving the deep RL problem to a symmetry-reduced space, we can alleviate limitations inherent in the naive application of deep RL. We demonstrate that symmetry-reduced deep RL yields improved data efficiency as well as improved control policy efficacy compared to policies found by naive deep RL. Interestingly, the policy learned by the the symmetry aware control agent drives the system toward an equilibrium state of the forced KSE that is connected by continuation to an equilibrium of the unforced KSE, despite having been given no explicit information regarding its existence. I.e., to achieve its goal, the RL algorithm discovers and stabilizes an equilibrium state of the system. Finally, we demonstrate that the symmetry-reduced control policy is robust to observation and actuation signal noise, as well as to system parameters it has not observed before.",0
"Deep reinforcement learning is a model-free technique that can identify intricate control strategies for high-dimensional systems with macroscopic objectives, which makes it ideal for flow control. However, many flow control systems have symmetries that can hinder the performance and learning of a basic deep RL approach. To address these limitations, we utilized the Kuramoto-Sivashinsky Equation (KSE), equally spaced actuators, and a goal of minimizing dissipation and power cost as a test-bed. By shifting the deep RL problem to a space with reduced symmetry, we were able to overcome the challenges associated with naive deep RL. The symmetry-reduced deep RL approach was more efficient and effective in discovering control policies compared to the basic deep RL method. Notably, the learned policy directed the system towards an equilibrium state of the forced KSE, which was connected to an equilibrium state of the unforced KSE, without any explicit information. This means that the RL algorithm discovered and stabilized an equilibrium state to achieve its objective. Furthermore, we found that the symmetry-reduced control policy was robust to observation and actuation signal noise, as well as to unknown system parameters.",1
"Inferring the intent of an intelligent agent from demonstrations and subsequently predicting its behavior, is a critical task in many collaborative settings. A common approach to solve this problem is the framework of inverse reinforcement learning (IRL), where the observed agent, e.g., a human demonstrator, is assumed to behave according to an intrinsic cost function that reflects its intent and informs its control actions. In this work, we reformulate the IRL inference problem to learning control Lyapunov functions (CLF) from demonstrations by exploiting the inverse optimality property, which states that every CLF is also a meaningful value function. Moreover, the derived CLF formulation directly guarantees stability of inferred control policies. We show the flexibility of our proposed method by learning from goal-directed movement demonstrations in a continuous environment.",0
"In collaborative settings, accurately predicting the behavior of an intelligent agent is crucial and can be achieved by inferring its intent from demonstrations. The Inverse Reinforcement Learning (IRL) framework is commonly used for this purpose, assuming that the observed agent operates according to an intrinsic cost function that informs its control actions. This study proposes a reformulation of the IRL inference problem using the Learning Control Lyapunov Functions (CLF) approach, which exploits the inverse optimality property and guarantees stability of the control policies. By learning from goal-directed movement demonstrations in a continuous environment, the proposed method demonstrates its flexibility.",1
"In this work, we propose Behavior-Guided Actor-Critic (BAC), an off-policy actor-critic deep RL algorithm. BAC mathematically formulates the behavior of the policy through autoencoders by providing an accurate estimation of how frequently each state-action pair was visited while taking into consideration state dynamics that play a crucial role in determining the trajectories produced by the policy. The agent is encouraged to change its behavior consistently towards less-visited state-action pairs while attaining good performance by maximizing the expected discounted sum of rewards, resulting in an efficient exploration of the environment and good exploitation of all high reward regions. One prominent aspect of our approach is that it is applicable to both stochastic and deterministic actors in contrast to maximum entropy deep reinforcement learning algorithms. Results show considerably better performances of BAC when compared to several cutting-edge learning algorithms.",0
"The proposed algorithm in this study is called Behavior-Guided Actor-Critic (BAC), which is an off-policy actor-critic deep RL approach. BAC utilizes autoencoders to mathematically express policy behavior and estimate the frequency of state-action pair visits. This considers state dynamics that are crucial in determining the produced trajectories. The agent is then motivated to change its behavior towards less-visited state-action pairs while maximizing rewards. This results in efficient exploration and exploitation of high reward regions. Unlike maximum entropy deep reinforcement learning algorithms, our approach is applicable to both stochastic and deterministic actors. The results demonstrate that BAC outperforms several advanced learning algorithms.",1
"Zeroth-order (ZO, also known as derivative-free) methods, which estimate the gradient only by two function evaluations, have attracted much attention recently because of its broad applications in machine learning community. The two function evaluations are normally generated with random perturbations from standard Gaussian distribution. To speed up ZO methods, many methods, such as variance reduced stochastic ZO gradients and learning an adaptive Gaussian distribution, have recently been proposed to reduce the variances of ZO gradients. However, it is still an open problem whether there is a space to further improve the convergence of ZO methods. To explore this problem, in this paper, we propose a new reinforcement learning based ZO algorithm (ZO-RL) with learning the sampling policy for generating the perturbations in ZO optimization instead of using random sampling. To find the optimal policy, an actor-critic RL algorithm called deep deterministic policy gradient (DDPG) with two neural network function approximators is adopted. The learned sampling policy guides the perturbed points in the parameter space to estimate a more accurate ZO gradient. To the best of our knowledge, our ZO-RL is the first algorithm to learn the sampling policy using reinforcement learning for ZO optimization which is parallel to the existing methods. Especially, our ZO-RL can be combined with existing ZO algorithms that could further accelerate the algorithms. Experimental results for different ZO optimization problems show that our ZO-RL algorithm can effectively reduce the variances of ZO gradient by learning a sampling policy, and converge faster than existing ZO algorithms in different scenarios.",0
"Recently, there has been much interest in Zeroth-order (ZO) methods, also referred to as derivative-free methods. These methods estimate the gradient by only using two function evaluations, which are generated with random perturbations from a standard Gaussian distribution. ZO methods have a wide range of applications in the machine learning community. To enhance the performance of ZO methods, various approaches have been proposed, such as variance reduced stochastic ZO gradients and learning an adaptive Gaussian distribution. Nonetheless, it remains an open problem whether there is space for further improving the convergence of ZO methods. In this paper, we propose a new reinforcement learning based ZO algorithm referred to as ZO-RL. ZO-RL learns the sampling policy for generating perturbations in ZO optimization. This is achieved by using an actor-critic reinforcement learning (RL) algorithm called deep deterministic policy gradient (DDPG) with two neural network function approximators. The learned policy guides the perturbed points to estimate a more accurate ZO gradient. ZO-RL is the first algorithm to learn the sampling policy using reinforcement learning for ZO optimization and can be combined with existing ZO algorithms to further speed up the process. Experimental results demonstrate that ZO-RL effectively reduces the variances of ZO gradient by learning a sampling policy and converges faster than existing ZO algorithms in various scenarios.",1
"Disparate access to resources by different subpopulations is a prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial groups to more easily access resources such as high-quality schools, grocery stores, and polling places. Similarly, social networks within universities and organizations may enable certain groups to more easily access people with valuable information or influence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to enhance equity in networked systems by editing graph edges under budget constraints. We prove such problems are NP-hard, and cannot be approximated within a factor of $(1-\tfrac{1}{3e})$. We develop a principled, sample- and time- efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms baselines on a diverse set of synthetic graphs. We further demonstrate the method on real-world networks, by merging public census, school, and transportation datasets for the city of Chicago and applying our algorithm to find human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. Further experiments on Facebook networks of universities yield sets of new social connections that would increase equitable access to certain attributed nodes across gender groups.",0
"Inequality in accessing resources is a common problem in societal and sociotechnical networks. For instance, certain racial groups may have better access to high-quality schools, grocery stores, and polling places in urban infrastructure networks. Similarly, some groups may have an advantage in accessing people with valuable information or influence within social networks in organizations and universities. To address this issue, we propose a new class of problems called Graph Augmentation for Equitable Access (GAEA), which involves editing graph edges under budget constraints to promote fairness in networked systems. We prove that solving such problems is NP-hard and cannot be approximated within a factor of $(1-\tfrac{1}{3e})$. To tackle this challenge, we present a principled, efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms existing methods on synthetic graphs and real-world networks, as demonstrated by our case study on the city of Chicago. By merging public census, school, and transportation datasets, we identify human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. We also conduct experiments on Facebook networks of universities and find sets of new social connections that would increase equitable access to certain attributed nodes across gender groups.",1
"Modern navigation algorithms based on deep reinforcement learning (RL) show promising efficiency and robustness. However, most deep RL algorithms operate in a risk-neutral manner, making no special attempt to shield users from relatively rare but serious outcomes, even if such shielding might cause little loss of performance. Furthermore, such algorithms typically make no provisions to ensure safety in the presence of inaccuracies in the models on which they were trained, beyond adding a cost-of-collision and some domain randomization while training, in spite of the formidable complexity of the environments in which they operate. In this paper, we present a novel distributional RL algorithm that not only learns an uncertainty-aware policy, but can also change its risk measure without expensive fine-tuning or retraining. Our method shows superior performance and safety over baselines in partially-observed navigation tasks. We also demonstrate that agents trained using our method can adapt their policies to a wide range of risk measures at run-time.",0
"Navigation algorithms utilizing deep reinforcement learning (RL) have displayed impressive efficiency and robustness. However, most deep RL algorithms function in a risk-neutral manner, neglecting to protect users from rare but potentially severe outcomes, even if it would have minimal impact on performance. Additionally, these algorithms typically do not ensure safety in the presence of inaccuracies in the models used for training, aside from implementing a cost-of-collision and domain randomization during training. This is despite the complex environments in which they operate. This paper introduces a new distributional RL algorithm that learns an uncertainty-aware policy and can modify its risk measure without the need for costly fine-tuning or retraining. Our method outperforms baselines in partially-observed navigation tasks in terms of both performance and safety. We also demonstrate that agents trained with our method can adjust their policies to various risk measures during run-time.",1
"Visual defect detection (VDD) for high-mix low-volume production of non-convex metal objects, such as high-pressure cylindrical piping joint parts (VDD-HPPPs), is challenging because subtle difference in domain (e.g., metal objects, imaging device, viewpoints, lighting) significantly affects the specular reflection characteristics of individual metal object types. In this paper, we address this issue by introducing a tailor-made VDD framework that can be automatically adapted to a new domain. Specifically, we formulate this adaptation task as the problem of network architecture search (NAS) on a deep object-detection network, in which the network architecture is searched via reinforcement learning. We demonstrate the effectiveness of the proposed framework using the VDD-HPPPs task as a factory case study. Experimental results show that the proposed method achieved higher burr detection accuracy compared with the baseline method for data with different training/test domains for the non-convex HPPPs, which are particularly affected by domain shifts.",0
"Detecting visual defects in non-convex metal objects, such as high-pressure cylindrical piping joint parts (VDD-HPPPs), is a complex task due to the variations in domain. Differences in metal objects, imaging devices, viewpoints, and lighting can significantly impact the specular reflection characteristics of each metal object type. To address this issue, we propose a custom VDD framework that can automatically adapt to new domains. Our approach involves using reinforcement learning to search for network architectures in a deep object-detection network. We test the effectiveness of our framework by applying it to the VDD-HPPPs task in a factory case study. Our experimental results demonstrate that our method outperformed the baseline method in detecting burrs in non-convex HPPPs with different training/test domains that are particularly affected by domain shifts.",1
"Few-shot learning aims to correctly recognize query samples from unseen classes given a limited number of support samples, often by relying on global embeddings of images. In this paper, we propose to equip the backbone network with an attention agent, which is trained by reinforcement learning. The policy gradient algorithm is employed to train the agent towards adaptively localizing the representative regions on feature maps over time. We further design a reward function based on the prediction of the held-out data, thus helping the attention mechanism to generalize better across the unseen classes. The extensive experiments show, with the help of the reinforced attention, that our embedding network has the capability to progressively generate a more discriminative representation in few-shot learning. Moreover, experiments on the task of image classification also show the effectiveness of the proposed design.",0
"The objective of few-shot learning is to accurately identify query samples from unknown categories, using a limited number of support samples and relying on global embeddings of images. Our paper introduces the idea of incorporating an attention agent in the backbone network, trained through reinforcement learning. By using the policy gradient algorithm, the agent can adaptively identify representative regions on feature maps over time. To improve the attention mechanism's ability to generalize across unseen classes, we design a reward function based on the prediction of the held-out data. Our extensive experiments demonstrate that the reinforced attention in our embedding network can progressively create a more distinctive representation in few-shot learning. Additionally, our proposed design proves to be effective in image classification tasks.",1
"Model-based reinforcement learning (RL) is more sample efficient than model-free RL by using imaginary trajectories generated by the learned dynamics model. When the model is inaccurate or biased, imaginary trajectories may be deleterious for training the action-value and policy functions. To alleviate such problem, this paper proposes to adaptively reweight the imaginary transitions, so as to reduce the negative effects of poorly generated trajectories. More specifically, we evaluate the effect of an imaginary transition by calculating the change of the loss computed on the real samples when we use the transition to train the action-value and policy functions. Based on this evaluation criterion, we construct the idea of reweighting each imaginary transition by a well-designed meta-gradient algorithm. Extensive experimental results demonstrate that our method outperforms state-of-the-art model-based and model-free RL algorithms on multiple tasks. Visualization of our changing weights further validates the necessity of utilizing reweight scheme.",0
"The utilization of model-based reinforcement learning (RL) is a more competent method than model-free RL as it employs hypothetical trajectories created by the acquired dynamics model. However, if the model is erroneous or biased, these hypothetical trajectories may impede the training of the action-value and policy functions. To address this issue, this study introduces an adaptive method to reevaluate the hypothetical transitions and reduce the negative impact of poorly generated trajectories. The study evaluates each hypothetical transition by measuring the change in loss computed on real samples when training the action-value and policy functions with the transition. The study then uses a meta-gradient algorithm to reweight each hypothetical transition based on this evaluation criterion. The results of the study show that our approach surpasses current state-of-the-art model-based and model-free RL methods in multiple tasks. Furthermore, the changing weights visualization confirms the necessity of the reweighting scheme.",1
"In most real world scenarios, a policy trained by reinforcement learning in one environment needs to be deployed in another, potentially quite different environment. However, generalization across different environments is known to be hard. A natural solution would be to keep training after deployment in the new environment, but this cannot be done if the new environment offers no reward signal. Our work explores the use of self-supervision to allow the policy to continue training after deployment without using any rewards. While previous methods explicitly anticipate changes in the new environment, we assume no prior knowledge of those changes yet still obtain significant improvements. Empirical evaluations are performed on diverse simulation environments from DeepMind Control suite and ViZDoom, as well as real robotic manipulation tasks in continuously changing environments, taking observations from an uncalibrated camera. Our method improves generalization in 31 out of 36 environments across various tasks and outperforms domain randomization on a majority of environments.",0
Deploying a policy trained by reinforcement learning in a different environment can be challenging as generalization across environments is difficult. It may not be feasible to continue training if the new environment does not provide a reward signal. Our research investigates the use of self-supervision to allow policies to continue training post-deployment without rewards. We assume no prior knowledge of environmental changes and achieved significant improvements across diverse simulation environments and real robotic manipulation tasks with observations from an uncalibrated camera. Our method outperforms domain randomization on most environments and enhances generalization in 31 out of 36 environments across multiple tasks.,1
"Extensive efforts have been made to improve the generalization ability of Reinforcement Learning (RL) methods via domain randomization and data augmentation. However, as more factors of variation are introduced during training, optimization becomes increasingly challenging, and empirically may result in lower sample efficiency and unstable training. Instead of learning policies directly from augmented data, we propose SOft Data Augmentation (SODA), a method that decouples augmentation from policy learning. Specifically, SODA imposes a soft constraint on the encoder that aims to maximize the mutual information between latent representations of augmented and non-augmented data, while the RL optimization process uses strictly non-augmented data. Empirical evaluations are performed on diverse tasks from DeepMind Control suite as well as a robotic manipulation task, and we find SODA to significantly advance sample efficiency, generalization, and stability in training over state-of-the-art vision-based RL methods.",0
"Numerous attempts have been made to enhance the ability of Reinforcement Learning (RL) techniques to generalize through the application of domain randomization and data augmentation. However, the introduction of additional factors of variation during training can increase the difficulty of optimization, potentially leading to lower sample efficiency and unstable training. Our proposal, SOft Data Augmentation (SODA), separates augmentation from policy learning by imposing a soft constraint on the encoder to maximize the mutual information between latent representations of augmented and non-augmented data. Meanwhile, the RL optimization process solely employs non-augmented data. We conducted empirical evaluations on diverse tasks from DeepMind Control suite and a robotic manipulation task, and observed that SODA significantly enhances sample efficiency, generalization, and stability in training compared to state-of-the-art vision-based RL methods.",1
"We apply Reinforcement Learning algorithms to solve the classic quantitative finance Market Making problem, in which an agent provides liquidity to the market by placing buy and sell orders while maximizing a utility function. The optimal agent has to find a delicate balance between the price risk of her inventory and the profits obtained by capturing the bid-ask spread. We design an environment with a reward function that determines an order relation between policies equivalent to the original utility function. When comparing our agents with the optimal solution and a benchmark symmetric agent, we find that the Deep Q-Learning algorithm manages to recover the optimal agent.",0
"Our approach involves utilizing Reinforcement Learning algorithms to tackle the Market Making problem in quantitative finance. The agent's role is to offer liquidity to the market by placing buy and sell orders while simultaneously maximizing a utility function. The ideal agent must strike a balance between the price risk of their inventory and the profits gained from the bid-ask spread. We have created an environment with a reward function that establishes an order relation between policies that is comparable to the initial utility function. After comparing our agents to an optimal solution and a benchmark symmetric agent, we have discovered that the Deep Q-Learning algorithm successfully restores the optimal agent.",1
"Natural policy gradient (NPG) methods are among the most widely used policy optimization algorithms in contemporary reinforcement learning. This class of methods is often applied in conjunction with entropy regularization -- an algorithmic scheme that encourages exploration -- and is closely related to soft policy iteration and trust region policy optimization. Despite the empirical success, the theoretical underpinnings for NPG methods remain limited even for the tabular setting. This paper develops $\textit{non-asymptotic}$ convergence guarantees for entropy-regularized NPG methods under softmax parameterization, focusing on discounted Markov decision processes (MDPs). Assuming access to exact policy evaluation, we demonstrate that the algorithm converges linearly -- or even quadratically once it enters a local region around the optimal policy -- when computing optimal value functions of the regularized MDP. Moreover, the algorithm is provably stable vis-\`a-vis inexactness of policy evaluation. Our convergence results accommodate a wide range of learning rates, and shed light upon the role of entropy regularization in enabling fast convergence.",0
"Policy optimization algorithms in contemporary reinforcement learning often use Natural Policy Gradient (NPG) methods. These methods are often combined with entropy regularization, which encourages exploration, and are related to soft policy iteration and trust region policy optimization. Despite being empirically successful, there is limited theoretical understanding of NPG methods, even for the tabular setting. This paper provides non-asymptotic convergence guarantees for entropy-regularized NPG methods under softmax parameterization, specifically for discounted Markov decision processes. The algorithm is shown to converge linearly, and even quadratically once it enters a local region around the optimal policy, assuming access to exact policy evaluation. The algorithm is also stable in the face of inexactness of policy evaluation. These results apply to a wide range of learning rates and help explain the role of entropy regularization in enabling fast convergence.",1
"We propose a framework for reinforcement learning (RL) in fine time discretization and a learning algorithm in this framework. One of the main goals of RL is to provide a way for physical machines to learn optimal behavior instead of being programmed. However, the machines are usually controlled in fine time discretization. The most common RL methods apply independent random elements to each action, which is not suitable in that setting. It is not feasible because it causes the controlled system to jerk, and does not ensure sufficient exploration since a single action is not long enough to create a significant experience that could be translated into policy improvement. In the RL framework introduced in this paper, policies are considered that produce actions based on states and random elements autocorrelated in subsequent time instants. The RL algorithm introduced here approximately optimizes such a policy. The efficiency of this algorithm is verified against three other RL methods (PPO, SAC, ACER) in four simulated learning control problems (Ant, HalfCheetah, Hopper, and Walker2D) in diverse time discretization. The algorithm introduced here outperforms the competitors in most cases considered.",0
"We present a framework and learning algorithm for reinforcement learning (RL) that is applicable to fine time discretization. The primary objective of RL is to enable machines to learn optimal behavior without explicit programming. However, in practice, machines are often controlled using fine time discretization, which makes it challenging to apply the traditional RL methods that use independent random elements for each action. This approach causes the system to jerk and does not allow for sufficient exploration. The novel RL framework proposed in this study incorporates policies that generate actions based on states and autocorrelated random elements at successive time intervals. The proposed RL algorithm optimizes this policy with high accuracy. Using four simulated learning control problems (Ant, HalfCheetah, Hopper, and Walker2D) with varying time discretization, we demonstrate that our algorithm outperforms three other RL methods (PPO, SAC, and ACER) in most cases.",1
"In this paper, we present a state-of-the-art reinforcement learning method for autonomous driving. Our approach employs temporal difference learning in a Bayesian framework to learn vehicle control signals from sensor data. The agent has access to images from a forward facing camera, which are preprocessed to generate semantic segmentation maps. We trained our system using both ground truth and estimated semantic segmentation input. Based on our observations from a large set of experiments, we conclude that training the system on ground truth input data leads to better performance than training the system on estimated input even if estimated input is used for evaluation. The system is trained and evaluated in a realistic simulated urban environment using the CARLA simulator. The simulator also contains a benchmark that allows for comparing to other systems and methods. The required training time of the system is shown to be lower and the performance on the benchmark superior to competing approaches.",0
"This paper introduces a cutting-edge reinforcement learning technique for self-driving cars. Our method utilizes Bayesian temporal difference learning to acquire vehicle control signals from sensor data. The agent is supplied with forward-facing camera images, which are preprocessed to create semantic segmentation maps. Our system was trained with both ground truth and estimated semantic segmentation data. After conducting extensive experiments, we found that training the system on ground truth data outperforms training it on estimated input, even during evaluation. We used the CARLA simulator to train and test the system in a realistic urban environment. Additionally, the simulator includes a benchmark for comparing our approach to other methods. Our method's required training time is lower, and its benchmark performance is superior to competing techniques.",1
"Scheduling is a fundamental task occurring in various automated systems applications, e.g., optimal schedules for machines on a job shop allow for a reduction of production costs and waste. Nevertheless, finding such schedules is often intractable and cannot be achieved by Combinatorial Optimization Problem (COP) methods within a given time limit. Recent advances of Deep Reinforcement Learning (DRL) in learning complex behavior enable new COP application possibilities. This paper presents an efficient DRL environment for Job-Shop Scheduling -- an important problem in the field. Furthermore, we design a meaningful and compact state representation as well as a novel, simple dense reward function, closely related to the sparse make-span minimization criteria used by COP methods. We demonstrate that our approach significantly outperforms existing DRL methods on classic benchmark instances, coming close to state-of-the-art COP approaches.",0
"Various automated systems applications involve scheduling, which is a crucial task. For instance, job shop machines' optimal schedules can reduce production costs and waste. However, Combinatorial Optimization Problem (COP) methods cannot find such schedules within a given time limit, making it an intractable task. The recent developments of Deep Reinforcement Learning (DRL) in learning complex behavior have opened new doors for COP applications. This paper presents an efficient DRL environment for Job-Shop Scheduling, a crucial problem in the field. Additionally, we have designed a compact and meaningful state representation and a simple dense reward function that is closely linked to the sparse make-span minimization criteria used by COP methods. We have demonstrated that our approach outperforms existing DRL methods on classic benchmark instances, approaching the state-of-the-art COP approaches.",1
"Safe interaction with the environment is one of the most challenging aspects of Reinforcement Learning (RL) when applied to real-world problems. This is particularly important when unsafe actions have a high or irreversible negative impact on the environment. In the context of network management operations, Remote Electrical Tilt (RET) optimisation is a safety-critical application in which exploratory modifications of antenna tilt angles of base stations can cause significant performance degradation in the network. In this paper, we propose a modular Safe Reinforcement Learning (SRL) architecture which is then used to address the RET optimisation in cellular networks. In this approach, a safety shield continuously benchmarks the performance of RL agents against safe baselines, and determines safe antenna tilt updates to be performed on the network. Our results demonstrate improved performance of the SRL agent over the baseline while ensuring the safety of the performed actions.",0
"When applying Reinforcement Learning (RL) to real-world issues, ensuring safe interaction with the environment is a major challenge. This is especially crucial when hazardous actions can have severe or irreversible consequences on the environment. In network management, Remote Electrical Tilt (RET) optimization is a safety-critical application that can result in significant performance degradation in the network when exploratory modifications of antenna tilt angles of base stations are made. To address this issue, we present a modular Safe Reinforcement Learning (SRL) architecture that is used to tackle RET optimization in cellular networks. In this approach, a safety shield continuously compares the performance of RL agents to safe baselines and determines safe antenna tilt updates for the network. Our results indicate that the SRL agent outperforms the baseline while ensuring the safety of the actions performed.",1
"Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into the desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently; 2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks.",0
"Skill learning algorithms are often developed and evaluated using simulated virtual environments, which have limitations in simulating rigid body physics and providing useful gradients. To address this, we introduce a new differentiable physics benchmark called PasticineLab, featuring soft body manipulation tasks where agents use manipulators to deform plasticine into desired configurations. The underlying physics engine supports differentiable elastic and plastic deformation, posing challenges to robotic agents. We evaluated existing reinforcement learning and gradient-based methods on this benchmark, finding that RL-based approaches struggle and gradient-based approaches can rapidly find solutions within tens of iterations but fall short on multi-stage tasks. We believe that PlasticineLab will inspire the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks.",1
"Experience replay, which enables the agents to remember and reuse experience from the past, has played a significant role in the success of off-policy reinforcement learning (RL). To utilize the experience replay efficiently, the existing sampling methods allow selecting out more meaningful experiences by imposing priorities on them based on certain metrics (e.g. TD-error). However, they may result in sampling highly biased, redundant transitions since they compute the sampling rate for each transition independently, without consideration of its importance in relation to other transitions. In this paper, we aim to address the issue by proposing a new learning-based sampling method that can compute the relative importance of transition. To this end, we design a novel permutation-equivariant neural architecture that takes contexts from not only features of each transition (local) but also those of others (global) as inputs. We validate our framework, which we refer to as Neural Experience Replay Sampler (NERS), on multiple benchmark tasks for both continuous and discrete control tasks and show that it can significantly improve the performance of various off-policy RL methods. Further analysis confirms that the improvements of the sample efficiency indeed are due to sampling diverse and meaningful transitions by NERS that considers both local and global contexts.",0
"Experience replay has been crucial for the success of off-policy reinforcement learning, allowing agents to recall and reuse past experiences. However, the conventional selection methods for meaningful experiences based on metrics like TD-error can result in biased and redundant transitions. This is because they do not consider the importance of each transition in relation to others. In this study, we propose a new learning-based sampling method called Neural Experience Replay Sampler (NERS) that computes the relative importance of transitions by utilizing a novel permutation-equivariant neural architecture. NERS considers both local and global contexts from the features of each transition and others. Our validation on various control tasks shows that NERS significantly improves the sample efficiency of off-policy RL methods by sampling diverse and meaningful transitions.",1
"To improve policy robustness of deep reinforcement learning agents, a line of recent works focus on producing disturbances of the environment. Existing approaches of the literature to generate meaningful disturbances of the environment are adversarial reinforcement learning methods. These methods set the problem as a two-player game between the protagonist agent, which learns to perform a task in an environment, and the adversary agent, which learns to disturb the protagonist via modifications of the considered environment. Both protagonist and adversary are trained with deep reinforcement learning algorithms. Alternatively, we propose in this paper to build on gradient-based adversarial attacks, usually used for classification tasks for instance, that we apply on the critic network of the protagonist to identify efficient disturbances of the environment. Rather than learning an attacker policy, which usually reveals as very complex and unstable, we leverage the knowledge of the critic network of the protagonist, to dynamically complexify the task at each step of the learning process. We show that our method, while being faster and lighter, leads to significantly better improvements in policy robustness than existing methods of the literature.",0
"Recent research has focused on enhancing the policy robustness of deep reinforcement learning agents by producing disturbances in the environment. Adversarial reinforcement learning methods have been employed to generate significant disturbances in the environment. This approach involves a two-player game between the protagonist agent, which learns to perform a task in an environment, and the adversary agent, which learns to disturb the protagonist through modifications of the environment. Both agents are trained using deep reinforcement learning algorithms. In contrast, this paper proposes an alternative approach that utilizes gradient-based adversarial attacks, commonly used in classification tasks, to identify efficient disturbances in the critic network of the protagonist. Rather than training an attacker policy, which tends to be complex and unstable, this method leverages the knowledge of the protagonist's critic network to dynamically complicate the task during the learning process. The proposed method is faster, lighter, and leads to significantly better improvements in policy robustness compared to existing methods in the literature.",1
"Efficient hyperparameter or architecture search methods have shown remarkable results, but each of them is only applicable to searching for either hyperparameters (HPs) or architectures. In this work, we propose a unified pipeline, AutoHAS, to efficiently search for both architectures and hyperparameters. AutoHAS learns to alternately update the shared network weights and a reinforcement learning (RL) controller, which learns the probability distribution for the architecture candidates and HP candidates. A temporary weight is introduced to store the updated weight from the selected HPs (by the controller), and a validation accuracy based on this temporary weight serves as a reward to update the controller. In experiments, we show AutoHAS is efficient and generalizable to different search spaces, baselines and datasets. In particular, AutoHAS can improve the accuracy over popular network architectures, such as ResNet and EfficientNet, on CIFAR-10/100, ImageNet, and four more other datasets.",0
"Although efficient methods for searching hyperparameters or architectures have been successful, each one is only suitable for searching for either hyperparameters or architectures. This study introduces a unified pipeline, AutoHAS, that can search for both architectures and hyperparameters efficiently. AutoHAS uses a reinforcement learning (RL) controller and learns to update shared network weights alternately. The controller learns the probability distribution for the architecture and HP candidates, and a temporary weight stores the updated weight from the chosen HPs to update the controller. A validation accuracy based on this temporary weight serves as a reward to update the controller. The experiments demonstrate that AutoHAS is efficient and generalizable to various search spaces, datasets, and baselines. Furthermore, AutoHAS can enhance the accuracy of well-known network architectures such as ResNet and EfficientNet on CIFAR-10/100, ImageNet, and four other datasets.",1
"We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.",0
"Our research focuses on the application of representation learning to reinforcement learning using rich observations, such as images. Our objective is to develop representations that can facilitate downstream control while being invariant to irrelevant details. We propose using bisimulation metrics to measure behavioral similarity between states in continuous Markov Decision Processes (MDPs) and learn robust latent representations that encode only relevant information from observations. Our approach involves training encoders to ensure that distances in latent space are equivalent to bisimulation distances in state space. We demonstrate the effectiveness of our method in ignoring irrelevant information by modifying visual MuJoCo tasks and achieving state-of-the-art (SOTA) performance. We also evaluate our method in a first-person highway driving task and show that it can learn invariance to clouds, weather, and time of day. Additionally, we provide generalization results based on bisimulation metrics properties and discuss links to causal inference.",1
"Black-box AI induction methods such as deep reinforcement learning (DRL) are increasingly being used to find optimal policies for a given control task. Although policies represented using a black-box AI are capable of efficiently executing the underlying control task and achieving optimal closed-loop performance, the developed control rules are often complex and neither interpretable nor explainable. In this paper, we use a recently proposed nonlinear decision-tree (NLDT) approach to find a hierarchical set of control rules in an attempt to maximize the open-loop performance for approximating and explaining the pre-trained black-box DRL (oracle) agent using the labelled state-action dataset. Recent advances in nonlinear optimization approaches using evolutionary computation facilitates finding a hierarchical set of nonlinear control rules as a function of state variables using a computationally fast bilevel optimization procedure at each node of the proposed NLDT. Additionally, we propose a re-optimization procedure for enhancing closed-loop performance of an already derived NLDT. We evaluate our proposed methodologies (open and closed-loop NLDTs) on different control problems having multiple discrete actions. In all these problems our proposed approach is able to find relatively simple and interpretable rules involving one to four non-linear terms per rule, while simultaneously achieving on par closed-loop performance when compared to a trained black-box DRL agent. A post-processing approach for simplifying the NLDT is also suggested. The obtained results are inspiring as they suggest the replacement of complicated black-box DRL policies involving thousands of parameters (making them non-interpretable) with relatively simple interpretable policies. Results are encouraging and motivating to pursue further applications of proposed approach in solving more complex control tasks.",0
"The use of black-box AI induction methods, such as deep reinforcement learning (DRL), has become more common in finding optimal policies for control tasks. These policies are efficient at executing the task and achieving optimal performance, but they are often complex and cannot be interpreted or explained. This paper proposes a nonlinear decision-tree (NLDT) approach to find a hierarchical set of control rules that maximizes open-loop performance and explains the pre-trained black-box DRL agent using a labelled state-action dataset. Recent advances in nonlinear optimization with evolutionary computation allow for a computationally fast bilevel optimization procedure at each node of the proposed NLDT. Additionally, the paper suggests a re-optimization procedure to enhance closed-loop performance of an already-derived NLDT. The proposed methodologies are evaluated on multiple control problems and are able to find simple and interpretable rules involving one to four nonlinear terms per rule while achieving comparable closed-loop performance to a trained black-box DRL agent. A post-processing approach for simplifying the NLDT is also suggested. These promising results suggest that the proposed approach can replace complicated black-box DRL policies with simpler and more interpretable policies, motivating further applications in solving complex control tasks.",1
"The rapid growth of ride-hailing platforms has created a highly competitive market where businesses struggle to make profits, demanding the need for better operational strategies. However, real-world experiments are risky and expensive for these platforms as they deal with millions of users daily. Thus, a need arises for a simulated environment where they can predict users' reactions to changes in the platform-specific parameters such as trip fares and incentives. Building such a simulation is challenging, as these platforms exist within dynamic environments where thousands of users regularly interact with one another. This paper presents a framework to mimic and predict user, specifically driver, behaviors in ride-hailing services. We use a data-driven hybrid reinforcement learning and imitation learning approach for this. First, the agent utilizes behavioral cloning to mimic driver behavior using a real-world data set. Next, reinforcement learning is applied on top of the pre-trained agents in a simulated environment, to allow them to adapt to changes in the platform. Our framework provides an ideal playground for ride-hailing platforms to experiment with platform-specific parameters to predict drivers' behavioral patterns.",0
"The ride-hailing industry has become fiercely competitive, with companies struggling to generate profits and seeking improved operational strategies. However, conducting real-world experiments is both costly and risky due to the high number of daily users. This has created a need for a simulated environment that can predict user reactions to changes in platform-specific parameters such as trip fares and incentives. Developing such a simulation is challenging, given the dynamic nature of ride-hailing platforms and the thousands of user interactions that occur daily. This paper proposes a framework for predicting driver behavior in ride-hailing services using a hybrid approach combining reinforcement and imitation learning. The framework employs a data-driven approach that utilizes behavioral cloning to mimic driver behavior, followed by reinforcement learning in a simulated environment to enable agents to adapt to platform changes. Our framework provides ride-hailing platforms with a safe and controlled environment to experiment with platform-specific parameters and predict drivers' behavioral patterns.",1
"Directed exploration strategies for reinforcement learning are critical for learning an optimal policy in a minimal number of interactions with the environment. Many algorithms use optimism to direct exploration, either through visitation estimates or upper confidence bounds, as opposed to data-inefficient strategies like \epsilon-greedy that use random, undirected exploration. Most data-efficient exploration methods require significant computation, typically relying on a learned model to guide exploration. Least-squares methods have the potential to provide some of the data-efficiency benefits of model-based approaches -- because they summarize past interactions -- with the computation closer to that of model-free approaches. In this work, we provide a novel, computationally efficient, incremental exploration strategy, leveraging this property of least-squares temporal difference learning (LSTD). We derive upper confidence bounds on the action-values learned by LSTD, with context-dependent (or state-dependent) noise variance. Such context-dependent noise focuses exploration on a subset of variable states, and allows for reduced exploration in other states. We empirically demonstrate that our algorithm can converge more quickly than other incremental exploration strategies using confidence estimates on action-values.",0
"Exploration strategies that are directed are crucial for reinforcement learning to achieve an optimal policy in a minimal number of interactions with the environment. To direct exploration, many algorithms use optimism through visitation estimates or upper confidence bounds rather than random, undirected exploration strategies like \epsilon-greedy that are not data-efficient. However, data-efficient exploration methods require significant computation and usually rely on a learned model to guide exploration. Least-squares methods have the potential to provide data-efficiency benefits similar to model-based approaches but with computation closer to model-free approaches due to their ability to summarize past interactions. In this study, we propose a new, computationally efficient, incremental exploration strategy taking advantage of this property of least-squares temporal difference learning (LSTD). We establish upper confidence bounds on the action-values learned by LSTD with context-dependent noise variance, which focuses exploration on a subset of variable states and reduces exploration in other states. Our algorithm has been empirically demonstrated to converge more quickly than other incremental exploration strategies that use confidence estimates on action-values.",1
"This paper proposes a reinforcement learning approach for nightly offline rebalancing operations in free-floating electric vehicle sharing systems (FFEVSS). Due to sparse demand in a network, FFEVSS require relocation of electrical vehicles (EVs) to charging stations and demander nodes, which is typically done by a group of drivers. A shuttle is used to pick up and drop off drivers throughout the network. The objective of this study is to solve the shuttle routing problem to finish the rebalancing work in the minimal time. We consider a reinforcement learning framework for the problem, in which a central controller determines the routing policies of a fleet of multiple shuttles. We deploy a policy gradient method for training recurrent neural networks and compare the obtained policy results with heuristic solutions. Our numerical studies show that unlike the existing solutions in the literature, the proposed methods allow to solve the general version of the problem with no restrictions on the urban EV network structure and charging requirements of EVs. Moreover, the learned policies offer a wide range of flexibility resulting in a significant reduction in the time needed to rebalance the network.",0
"In this paper, we suggest a reinforcement learning method to address the nightly offline rebalancing operations in free-floating electric vehicle sharing systems (FFEVSS). The FFEVSS require the movement of electrical vehicles (EVs) to charging stations and demander nodes due to sparse demand in the network, which is usually carried out by a group of drivers using a shuttle to pick them up and drop them off throughout the network. Our goal is to find a solution to the shuttle routing problem that completes the rebalancing work in the shortest possible time. To achieve this, we have employed a reinforcement learning framework, where a central controller determines the routing policies of a fleet of multiple shuttles. We have used a policy gradient method to train recurrent neural networks and compared the policy results with heuristic solutions. Our numerical studies reveal that our proposed method can address the general version of the problem without limitations on the urban EV network structure or charging requirements of EVs. Furthermore, the learned policies offer a high degree of flexibility, resulting in a significant reduction in the time required to rebalance the network.",1
"We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for each particular task, the proposed framework, DISH, distills a hierarchical policy from a set of tasks by representation and reinforcement learning. The framework is based on the idea of latent variable models that represent high-dimensional observations using low-dimensional latent variables. The resulting policy consists of two levels of hierarchy: (i) a planning module that reasons a sequence of latent intentions that would lead to an optimistic future and (ii) a feedback control policy, shared across the tasks, that executes the inferred intention. Because the planning is performed in low-dimensional latent space, the learned policy can immediately be used to solve or adapt to new tasks without additional training. We demonstrate the proposed framework can learn compact representations (3- and 1-dimensional latent states and commands for a humanoid with 197- and 36-dimensional state features and actions) while solving a small number of imitation tasks, and the resulting policy is directly applicable to other types of tasks, i.e., navigation in cluttered environments. Video: https://youtu.be/HQsQysUWOhg",0
"Our framework, DISH, offers a solution to the challenge of enabling an agent to perform various tasks and adjust to new ones with ease. Rather than creating a policy for each task, DISH uses representation and reinforcement learning to distill a hierarchical policy from a set of tasks. The policy comprises two levels of hierarchy: a planning module that reasons a sequence of latent intentions leading to an optimistic future, and a feedback control policy that executes the inferred intention, shared across tasks. DISH uses latent variable models to represent high-dimensional observations using low-dimensional latent variables, enabling planning to occur in low-dimensional latent space. This approach allows the learned policy to solve or adapt to new tasks without extra training. We demonstrate how DISH can learn compact representations and solve imitation tasks, making it directly applicable to other types of tasks, including navigation in cluttered environments. A video is available to demonstrate our framework in action.",1
"This report summarizes work performed as part of an internship at INRIA, in partial requirement for the completion of a master degree in math and informatics. The goal of the internship was to develop a software environment to simulate electricity transmission in a power grid and actions performed by operators to maintain this grid in security. Our environment lends itself to automate the control of the power grid with reinforcement learning agents, assisting human operators. It is amenable to organizing benchmarks, including a challenge in machine learning planned by INRIA and RTE for 2019. Our framework, built on top of open-source libraries, is available at https://github.com/MarvinLer/pypownet. In this report we present intermediary results and its usage in the context of a reinforcement learning game.",0
"As part of the requirements for a master's degree in math and informatics, this report outlines the work carried out during an internship at INRIA. The objective of the internship was to develop a software environment that could simulate the transmission of electricity in a power grid, as well as the actions taken by operators to keep the grid secure. Our environment is designed to automate the control of the power grid using reinforcement learning agents, which can assist human operators. Moreover, it can be used to organize benchmarks, including a machine learning challenge that is scheduled to take place in 2019, as part of a collaboration between INRIA and RTE. Our framework has been built using open-source libraries and is available for download at https://github.com/MarvinLer/pypownet. In this report, we present the intermediate results obtained and demonstrate how the framework was used in the context of a reinforcement learning game.",1
"Can we learn how to explore unknown spaces efficiently? To answer this question, we study the problem of Online Graph Exploration, the online version of the Traveling Salesperson Problem. We reformulate graph exploration as a reinforcement learning problem and apply Direct Future Prediction (Dosovitskiy and Koltun, 2017) to solve it. As the graph is discovered online, the corresponding Markov Decision Process entails a dynamic state space, namely the observable graph and a dynamic action space, namely the nodes forming the graph's frontier. To the best of our knowledge, this is the first attempt to solve online graph exploration in a data-driven way. We conduct experiments on six data sets of procedurally generated graphs and three real city road networks. We demonstrate that our agent can learn strategies superior to many well known graph traversal algorithms, confirming that exploration can be learned.",0
"Is it possible to efficiently navigate uncharted territories? To address this question, we investigate the issue of exploring graphs in an online setting, which is akin to the Traveling Salesperson Problem. We reframe graph exploration as a problem of reinforcement learning and utilize Direct Future Prediction (Dosovitskiy and Koltun, 2017) to tackle it. Since the graph is uncovered in real-time, the corresponding Markov Decision Process involves a dynamic state space - the observable graph - and a dynamic action space - the nodes that make up the graph's periphery. This is the first time, to the best of our knowledge, that online graph exploration has been approached in a data-driven manner. We conduct experiments on six datasets of procedurally generated graphs and three actual city road networks. Our findings demonstrate that our agent can acquire tactics that outperform numerous well-known graph traversal algorithms, which verifies that exploration can be learned.",1
"Reinforcement learning (RL) is always the preferred embodiment to construct the control strategy of complex tasks, like asymmetric assembly tasks. However, the convergence speed of reinforcement learning severely restricts its practical application. In this paper, the convergence is first accelerated by combining RL and compliance control. Then a completely innovative progressive extension of action dimension (PEAD) mechanism is proposed to optimize the convergence of RL algorithms. The PEAD method is verified in DDPG and PPO. The results demonstrate the PEAD method will enhance the data-efficiency and time-efficiency of RL algorithms as well as increase the stable reward, which provides more potential for the application of RL.",0
"When it comes to complex tasks such as asymmetric assembly tasks, reinforcement learning (RL) is the ideal choice for constructing control strategies. However, the slow convergence of RL has limited its practical use. This study proposes a new approach to accelerate convergence by combining RL and compliance control, followed by introducing a novel mechanism called the progressive extension of action dimension (PEAD) to optimize the RL algorithm's convergence. The effectiveness of the PEAD method is validated in DDPG and PPO, showing improved data and time efficiency, stable rewards, and increased potential for RL application.",1
"In this paper, we are interested in optimal control problems with purely economic costs, which often yield optimal policies having a (nearly) bang-bang structure. We focus on policy approximations based on Model Predictive Control (MPC) and the use of the deterministic policy gradient method to optimize the MPC closed-loop performance in the presence of unmodelled stochasticity or model error. When the policy has a (nearly) bang-bang structure, we observe that the policy gradient method can struggle to produce meaningful steps in the policy parameters. To tackle this issue, we propose a homotopy strategy based on the interior-point method, providing a relaxation of the policy during the learning. We investigate a specific well-known battery storage problem, and show that the proposed method delivers a homogeneous and faster learning than a classical policy gradient approach.",0
"This article delves into optimal control problems that have only economic costs, leading to optimal policies with a nearly bang-bang structure. The focus is on approximating policies using Model Predictive Control (MPC) and optimizing the MPC closed-loop performance using the deterministic policy gradient method in the presence of unmodeled stochasticity or model error. However, when the policy has a nearly bang-bang structure, the policy gradient method may struggle to produce meaningful policy parameter steps. To address this problem, a homotopy strategy based on the interior-point method is proposed, which provides a relaxation of the policy during the learning phase. The study examines a well-known battery storage problem and shows that the proposed method results in homogeneous and faster learning than the classical policy gradient approach.",1
"Ensemble learning combines several individual models to obtain better generalization performance. Currently, deep learning models with multilayer processing architecture is showing better performance as compared to the shallow or traditional classification models. Deep ensemble learning models combine the advantages of both the deep learning models as well as the ensemble learning such that the final model has better generalization performance. This paper reviews the state-of-art deep ensemble models and hence serves as an extensive summary for the researchers. The ensemble models are broadly categorised into ensemble models like bagging, boosting and stacking, negative correlation based deep ensemble models, explicit/implicit ensembles, homogeneous /heterogeneous ensemble, decision fusion strategies, unsupervised, semi-supervised, reinforcement learning and online/incremental, multilabel based deep ensemble models. Application of deep ensemble models in different domains is also briefly discussed. Finally, we conclude this paper with some future recommendations and research directions.",0
"Ensemble learning is the technique of combining multiple individual models to improve generalization performance. Currently, deep learning models with multilayer processing architecture are outperforming traditional classification models. Deep ensemble learning models merge the benefits of both deep learning and ensemble learning to enhance the final model's generalization performance. This review paper comprehensively summarizes the state-of-the-art deep ensemble models for researchers. The ensemble models are grouped into categories such as bagging, boosting, stacking, negative correlation based, explicit/implicit ensembles, homogeneous/heterogeneous ensembles, decision fusion strategies, unsupervised, semi-supervised, reinforcement learning, online/incremental, and multilabel-based deep ensemble models. Additionally, the paper briefly discusses the application of deep ensemble models in different domains. Finally, the paper suggests future research directions and recommendations.",1
"The next-generation of wireless networks will enable many machine learning (ML) tools and applications to efficiently analyze various types of data collected by edge devices for inference, autonomy, and decision making purposes. However, due to resource constraints, delay limitations, and privacy challenges, edge devices cannot offload their entire collected datasets to a cloud server for centrally training their ML models or inference purposes. To overcome these challenges, distributed learning and inference techniques have been proposed as a means to enable edge devices to collaboratively train ML models without raw data exchanges, thus reducing the communication overhead and latency as well as improving data privacy. However, deploying distributed learning over wireless networks faces several challenges including the uncertain wireless environment, limited wireless resources (e.g., transmit power and radio spectrum), and hardware resources. This paper provides a comprehensive study of how distributed learning can be efficiently and effectively deployed over wireless edge networks. We present a detailed overview of several emerging distributed learning paradigms, including federated learning, federated distillation, distributed inference, and multi-agent reinforcement learning. For each learning framework, we first introduce the motivation for deploying it over wireless networks. Then, we present a detailed literature review on the use of communication techniques for its efficient deployment. We then introduce an illustrative example to show how to optimize wireless networks to improve its performance. Finally, we introduce future research opportunities. In a nutshell, this paper provides a holistic set of guidelines on how to deploy a broad range of distributed learning frameworks over real-world wireless communication networks.",0
"The upcoming wireless networks will allow machine learning (ML) tools and applications to analyze various data collected by edge devices for inference, autonomy, and decision making purposes. Due to resource constraints, delay limitations, and privacy challenges, edge devices cannot send their entire collected datasets to a cloud server for centralized training of their ML models or inference purposes. To address this, distributed learning and inference techniques have been proposed to enable edge devices to collaboratively train ML models without raw data exchanges, thus reducing communication overhead and latency, and enhancing data privacy. However, deploying distributed learning over wireless networks is challenging due to uncertain wireless environments, limited wireless resources, and hardware resources. This paper comprehensively studies the efficient and effective deployment of distributed learning over wireless edge networks, providing an overview of several emerging distributed learning paradigms, including federated learning, federated distillation, distributed inference, and multi-agent reinforcement learning. For each learning framework, the motivation for deploying it over wireless networks is introduced, followed by a literature review on using communication techniques for its efficient deployment. An illustrative example is presented to demonstrate how to optimize wireless networks to improve performance, and future research opportunities are highlighted. In summary, this paper offers a comprehensive set of guidelines on deploying various distributed learning frameworks over real-world wireless communication networks.",1
"Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.",0
"Convolutional neural networks (CNNs) have shown exceptional ability in distinguishing features, but they tend to have poor generalization skills when dealing with unseen domains. To tackle this issue, domain generalization aims to create a model that can be generalized to any new domain by learning from a set of source domains. This paper introduces a new method called MixStyle, which proposes a unique approach by mixing instance-level feature statistics of training samples across source domains. The idea behind MixStyle is to recognize that the visual domain of an image is closely related to its style, and the proposed style-mixing takes place in the bottom layers of a CNN. By mixing styles of training instances, new domains are synthesized implicitly, which increases the domain diversity of the source domains and enhances the generalizability of the trained model. MixStyle is easily implemented and fits into mini-batch training efficiently. The effectiveness of MixStyle is demonstrated on a variety of tasks, including category classification, instance retrieval, and reinforcement learning.",1
"This paper introduces a dual-critic reinforcement learning (RL) framework to address the problem of frame-level bit allocation in HEVC/H.265. The objective is to minimize the distortion of a group of pictures (GOP) under a rate constraint. Previous RL-based methods tackle such a constrained optimization problem by maximizing a single reward function that often combines a distortion and a rate reward. However, the way how these rewards are combined is usually ad hoc and may not generalize well to various coding conditions and video sequences. To overcome this issue, we adapt the deep deterministic policy gradient (DDPG) reinforcement learning algorithm for use with two critics, with one learning to predict the distortion reward and the other the rate reward. In particular, the distortion critic works to update the agent when the rate constraint is satisfied. By contrast, the rate critic makes the rate constraint a priority when the agent goes over the bit budget. Experimental results on commonly used datasets show that our method outperforms the bit allocation scheme in x265 and the single-critic baseline by a significant margin in terms of rate-distortion performance while offering fairly precise rate control.",0
"The paper presents a new approach to tackle the issue of frame-level bit allocation in HEVC/H.265 using a dual-critic reinforcement learning framework. The aim is to minimize distortion within a rate constraint. Previous methods have used a single reward function that combines distortion and rate rewards, but this may not be effective across different coding conditions and video sequences. To overcome this, the study uses the deep deterministic policy gradient (DDPG) reinforcement learning algorithm with two critics, where one predicts distortion rewards and the other rate rewards. The distortion critic updates the agent when the rate constraint is met, while the rate critic prioritizes the rate constraint when the bit budget is exceeded. The results show that this method outperforms x265's bit allocation scheme and the single-critic baseline in terms of rate-distortion performance, while also providing accurate rate control.",1
"Many real-world applications such as robotics provide hard constraints on power and compute that limit the viable model complexity of Reinforcement Learning (RL) agents. Similarly, in many distributed RL settings, acting is done on un-accelerated hardware such as CPUs, which likewise restricts model size to prevent intractable experiment run times. These ""actor-latency"" constrained settings present a major obstruction to the scaling up of model complexity that has recently been extremely successful in supervised learning. To be able to utilize large model capacity while still operating within the limits imposed by the system during acting, we develop an ""Actor-Learner Distillation"" (ALD) procedure that leverages a continual form of distillation that transfers learning progress from a large capacity learner model to a small capacity actor model. As a case study, we develop this procedure in the context of partially-observable environments, where transformer models have had large improvements over LSTMs recently, at the cost of significantly higher computational complexity. With transformer models as the learner and LSTMs as the actor, we demonstrate in several challenging memory environments that using Actor-Learner Distillation recovers the clear sample-efficiency gains of the transformer learner model while maintaining the fast inference and reduced total training time of the LSTM actor model.",0
"Reinforcement Learning (RL) agents used in practical applications like robotics face constraints on power and compute that limit their model complexity. In distributed RL settings, acting is carried out on CPUs, which also restricts model size to prevent intractable experiment run times. These limitations pose a significant obstacle to scaling up model complexity, which has been successful in supervised learning. To address this, we introduce an ""Actor-Learner Distillation"" (ALD) procedure, using a continual form of distillation that transfers learning progress from a large capacity learner model to a small capacity actor model. We apply this procedure in the context of partially-observable environments, where transformer models have shown improved performance over LSTMs but at a higher computational cost. By using transformer models as the learner and LSTMs as the actor, we demonstrate that Actor-Learner Distillation recovers the sample-efficiency gains of the transformer learner model while maintaining the fast inference and reduced training time of the LSTM actor model in challenging memory environments.",1
"Positive affect has been linked to increased interest, curiosity and satisfaction in human learning. In reinforcement learning, extrinsic rewards are often sparse and difficult to define, intrinsically motivated learning can help address these challenges. We argue that positive affect is an important intrinsic reward that effectively helps drive exploration that is useful in gathering experiences. We present a novel approach leveraging a task-independent reward function trained on spontaneous smile behavior that reflects the intrinsic reward of positive affect. To evaluate our approach we trained several downstream computer vision tasks on data collected with our policy and several baseline methods. We show that the policy based on our affective rewards successfully increases the duration of episodes, the area explored and reduces collisions. The impact is the increased speed of learning for several downstream computer vision tasks.",0
"Increased interest, curiosity, and satisfaction in human learning have been associated with positive affect. However, in reinforcement learning, defining extrinsic rewards can be challenging due to their scarcity. Intrinsic motivation in learning can help overcome this issue. We propose that positive affect serves as a crucial intrinsic reward, which facilitates exploration and helps acquire valuable experiences. To demonstrate this, we introduce a unique approach that employs a task-independent reward function modeled on spontaneous smiling behavior as a reflection of positive affect. We evaluate our approach by training various downstream computer vision tasks using our policy and several baseline methods. Our results show that using affective rewards in our policy enhances the duration of episodes, the explored area, and reduces collisions. Importantly, this approach accelerates learning in several downstream computer vision tasks.",1
"In most machine learning algorithms, training data is assumed to be independent and identically distributed (iid). When it is not the case, the algorithm's performances are challenged, leading to the famous phenomenon of catastrophic forgetting. Algorithms dealing with it are gathered in the Continual Learning research field. In this paper, we study the regularization based approaches to continual learning and show that those approaches can not learn to discriminate classes from different tasks in an elemental continual benchmark: the class-incremental scenario. We make theoretical reasoning to prove this shortcoming and illustrate it with examples and experiments. Moreover, we show that it can have some important consequences on continual multi-tasks reinforcement learning or in pre-trained models used for continual learning. We believe that highlighting and understanding the shortcomings of regularization strategies will help us to use them more efficiently.",0
"The majority of machine learning algorithms assume that the training data is independent and identically distributed (iid), but if this is not the case, the algorithm's performance may be challenged, resulting in the widely recognized problem of catastrophic forgetting. To tackle this issue, researchers in the field of Continual Learning have developed various algorithms. This paper examines regularization-based approaches to continual learning and demonstrates that such approaches struggle to differentiate classes from different tasks in a basic continual benchmark known as the class-incremental scenario. We provide theoretical reasoning and examples to confirm this deficiency and highlight its potential impact on continual multi-tasks reinforcement learning and pre-trained models used for continual learning. Recognizing and comprehending the limitations of regularization strategies can aid in their more effective application.",1
"Experience replay (ER) improves the data efficiency of off-policy reinforcement learning (RL) algorithms by allowing an agent to store and reuse its past experiences in a replay buffer. While many techniques have been proposed to enhance ER by biasing how experiences are sampled from the buffer, thus far they have not considered strategies for refreshing experiences inside the buffer. In this work, we introduce Lucid Dreaming for Experience Replay (LiDER), a conceptually new framework that allows replay experiences to be refreshed by leveraging the agent's current policy. LiDER consists of three steps: First, LiDER moves an agent back to a past state. Second, from that state, LiDER then lets the agent execute a sequence of actions by following its current policy -- as if the agent were ""dreaming"" about the past and can try out different behaviors to encounter new experiences in the dream. Third, LiDER stores and reuses the new experience if it turned out better than what the agent previously experienced, i.e., to refresh its memories. LiDER is designed to be easily incorporated into off-policy, multi-worker RL algorithms that use ER; we present in this work a case study of applying LiDER to an actor-critic based algorithm. Results show LiDER consistently improves performance over the baseline in six Atari 2600 games. Our open-source implementation of LiDER and the data used to generate all plots in this work are available at github.com/duyunshu/lucid-dreaming-for-exp-replay.",0
"By utilizing a replay buffer, experience replay (ER) can enhance the data efficiency of off-policy reinforcement learning (RL) algorithms. Although various techniques have been suggested to enhance ER by manipulating how experiences are selected from the buffer, none have focused on refreshing experiences within the buffer. This paper introduces a new concept, Lucid Dreaming for Experience Replay (LiDER), which can refresh replay experiences by utilizing an agent's current policy. LiDER has three steps: first, it returns the agent to a previous state, then it enables the agent to execute a sequence of actions using its current policy, as if the agent were dreaming about the past and could experiment with different behaviors to encounter novel experiences. Finally, LiDER stores and reuses the new experience if it is better than the previous experience, thus refreshing the agent's memories. LiDER is simple to integrate into off-policy, multi-worker RL algorithms that use ER. A case study of applying LiDER to an actor-critic based algorithm is presented, and results show that LiDER consistently enhances performance over the baseline in six Atari 2600 games. All plots produced in this study, as well as our open-source implementation of LiDER, are available on github.com/duyunshu/lucid-dreaming-for-exp-replay.",1
