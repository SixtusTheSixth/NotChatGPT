"Introspection of deep supervised predictive models trained on functional and structural brain imaging may uncover novel markers of Alzheimer's disease (AD). However, supervised training is prone to learning from spurious features (shortcut learning) impairing its value in the discovery process. Deep unsupervised and, recently, contrastive self-supervised approaches, not biased to classification, are better candidates for the task. Their multimodal options specifically offer additional regularization via modality interactions. In this paper, we introduce a way to exhaustively consider multimodal architectures for contrastive self-supervised fusion of fMRI and MRI of AD patients and controls. We show that this multimodal fusion results in representations that improve the results of the downstream classification for both modalities. We investigate the fused self-supervised features projected into the brain space and introduce a numerically stable way to do so.",0
"Self-supervised representations can learn richer, more generalizable features than traditional supervised approaches. However, existing methods primarily work on image data and don’t readily apply to other domains like natural language processing (NLP) or speech signal analysis. We aim to develop a method that can successfully transfer pretrained models across all three modalities by using unimodal tasks as auxiliary objectives during fine-tuning. Our proposed framework shows state-of-the-art results on several downstream NLP benchmarks such as sentiment analysis, question answering, and machine translation for both English and Chinese languages. In addition, we demonstrate successful adaptation to another complex task: diagnosing Alzheimer's Disease through analysis of structured MRI scans. Finally, our experiments provide insights into understanding what kind of multi-modality is important for different tasks, which would inform future research directions in developing even better cross-modal learning pipelines.",1
"Deep learning and convolutional neural networks (CNNs) have made progress in polarimetric synthetic aperture radar (PolSAR) image classification over the past few years. However, a crucial issue has not been addressed, i.e., the requirement of CNNs for abundant labeled samples versus the insufficient human annotations of PolSAR images. It is well-known that following the supervised learning paradigm may lead to the overfitting of training data, and the lack of supervision information of PolSAR images undoubtedly aggravates this problem, which greatly affects the generalization performance of CNN-based classifiers in large-scale applications. To handle this problem, in this paper, learning transferrable representations from unlabeled PolSAR data through convolutional architectures is explored for the first time. Specifically, a PolSAR-tailored contrastive learning network (PCLNet) is proposed for unsupervised deep PolSAR representation learning and few-shot classification. Different from the utilization of optical processing methods, a diversity stimulation mechanism is constructed to narrow the application gap between optics and PolSAR. Beyond the conventional supervised methods, PCLNet develops an unsupervised pre-training phase based on the proxy objective of instance discrimination to learn useful representations from unlabeled PolSAR data. The acquired representations are transferred to the downstream task, i.e., few-shot PolSAR classification. Experiments on two widely-used PolSAR benchmark datasets confirm the validity of PCLNet. Besides, this work may enlighten how to efficiently utilize the massive unlabeled PolSAR data to alleviate the greedy demands of CNN-based methods for human annotations.",0
"In summary, our approach outlined in this manuscript is centered on unsupervised learning with Deep Convolutional Generative Adversarial Networks (DC-GAN), which has previously been demonstrated as capable for tasks such as image generation and domain translation. DC-GAN models can be used by using only one single model trained without supervision. Despite the growing interest within remote sensing applications of these architectures in recent years, there still exists no research into few shot classification of Synthetic Aperture Radar (PolSAR) data using them. For this reason we selected three established benchmark datasets; including TerraSARX, EUSO-B, UAV/UACH and TUD_OLF_WILDFIRE_2019 datasets with diverse characteristics, scenes, acquisition settings, incidence angles, polarizations, resolutions and coherence levels. We then compared the performance against state-of-the-art baselines from other deep leaning approaches, specifically: PolSAR RNNs and PolSAR LSTMs. To evaluate accuracy and robustness of our proposed solution, we employed the mean Intersection over Union metric ($mIOU$) along with area under curve metrics ($AUC$). Using the evaluation metrics, we determined that DC-GAN based methods perform favourably against traditional machine learning algorithms such as Random Forest and Support Vector Machine classifiers in nearly all cases. The DC-GAN model is shown to improve upon state of art results with the highest reported $mIOU$ on most tested scenarios using these popular databases and configurations while also demonstrating high generalization capabilities across a range of experimental setups. Additionally to demonstrate applicability beyond few shot, we investigated transfer learning with fine tuning utilizing the pre-trained weights learned fro",1
"While many approaches exist in the literature to learn low-dimensional representations for data collections in multiple modalities, the generalizability of multi-modal nonlinear embeddings to previously unseen data is a rather overlooked subject. In this work, we first present a theoretical analysis of learning multi-modal nonlinear embeddings in a supervised setting. Our performance bounds indicate that for successful generalization in multi-modal classification and retrieval problems, the regularity of the interpolation functions extending the embedding to the whole data space is as important as the between-class separation and cross-modal alignment criteria. We then propose a multi-modal nonlinear representation learning algorithm that is motivated by these theoretical findings, where the embeddings of the training samples are optimized jointly with the Lipschitz regularity of the interpolators. Experimental comparison to recent multi-modal and single-modal learning algorithms suggests that the proposed method yields promising performance in multi-modal image classification and cross-modal image-text retrieval applications.",0
"In this work, we propose a novel algorithm for learning multi-modal nonlinear embeddings that can capture complex relationships between multiple data modalities. We establish theoretical performance bounds on the quality of these embeddings and demonstrate their effectiveness through extensive experimental evaluation on several benchmark datasets. Our approach uses a deep neural network architecture to learn a joint embedding space where all modality-specific features are mapped onto a common representation. This enables efficient alignment across different modalities, leading to improved cross-modality retrieval accuracy. Additionally, our method outperforms existing state-of-the-art methods in most cases, highlighting its potential as a powerful tool for analyzing and understanding interdisciplinary data. Overall, our findings contribute significantly towards advancing research in multimodal data analysis, with implications for applications such as image and text fusion, transfer learning, and unsupervised domain adaptation.",1
"Deep learning is changing many areas in molecular physics, and it has shown great potential to deliver new solutions to challenging molecular modeling problems. Along with this trend arises the increasing demand of expressive and versatile neural network architectures which are compatible with molecular systems. A new deep neural network architecture, Molecular Configuration Transformer (Molecular CT), is introduced for this purpose. Molecular CT is composed of a relation-aware encoder module and a computationally universal geometry learning unit, thus able to account for the relational constraints between particles meanwhile scalable to different particle numbers and invariant w.r.t. the trans-rotational transforms. The computational efficiency and universality make Molecular CT versatile for a variety of molecular learning scenarios and especially appealing for transferable representation learning across different molecular systems. As examples, we show that Molecular CT enables representational learning for molecular systems at different scales, and achieves comparable or improved results on common benchmarks using a more light-weighted structure compared to baseline models.",0
"This work presents two techniques for representing molecular structures that can handle both small molecules (<10 atoms) and biomolecules (>10k atoms). First, we introduce `Molecular Contexts', which encode a molecule as a set of atom types and distances that uniquely determine each atom. Secondly, we present our model, called `Tessereffi,' which uses the distance contexts from our first technique and introduces edge weights via message passing to learn latent representations while preserving geometric information using graph convolutions. Our method outperforms other state-of-the-art methods on a variety of tasks involving protein properties prediction and quantum chemistry. # Machine learning models often struggle to achieve high accuracy when faced with complex data, such as molecular structure information. However, our recent research has found that by combining geometry and representation learning, we can create more effective models capable of handling even large scale biological systems, like proteins. We have developed two novel techniques for achieving this goal. Our ""Molecular Contexts"" approach encodes each molecule as a unique combination of atomic elements and their distances, ensuring accurate representation across different scales. Next, we created ""Tessereffi"", a model built upon these distance contexts that incorporates edge weightings via message passing, allowing it to effectively capture important geometric details while still providing powerful representation capabilities. These advancements were put through rigorous testing and proved highly successful, surpassing existing state-of-the-art models in predictive performance for challenging applications like quantum chemistry and protein property forecasting. Overall, our innovative solutions represent a major step towards unlocking the full potential of machine learning for understanding complex chemical phenomena. # I apologize, but your request appears incomplete. Please provide the missing component(s): **ABSTRACT** Write an abstract around 150 to 300 words long for a paper titled *Molecular CT: Unifying Geometry and Representation Learning f...",1
"Forecasting influenza in a timely manner aids health organizations and policymakers in adequate preparation and decision making. However, effective influenza forecasting still remains a challenge despite increasing research interest. It is even more challenging amidst the COVID pandemic, when the influenza-like illness (ILI) counts are affected by various factors such as symptomatic similarities with COVID-19 and shift in healthcare seeking patterns of the general population. Under the current pandemic, historical influenza models carry valuable expertise about the disease dynamics but face difficulties adapting. Therefore, we propose CALI-Net, a neural transfer learning architecture which allows us to 'steer' a historical disease forecasting model to new scenarios where flu and COVID co-exist. Our framework enables this adaptation by automatically learning when it should emphasize learning from COVID-related signals and when it should learn from the historical model. Thus, we exploit representations learned from historical ILI data as well as the limited COVID-related signals. Our experiments demonstrate that our approach is successful in adapting a historical forecasting model to the current pandemic. In addition, we show that success in our primary goal, adaptation, does not sacrifice overall performance as compared with state-of-the-art influenza forecasting approaches.",0
"Title: ""Modeling Infectious Diseases under Uncertainty"" Authors: Dr. John Smith (Department of Epidemiology) and Professor Jane Johnson (Institute of Applied Mathematics) Abstract: This paper presents a case study on modeling infectious diseases during times of pandemic uncertainty using data from historical flu outbreaks as well as current data on COVID-19. Our analysis focuses on understanding how to steer a disease forecasting model to accurately predict outcomes that can inform public health decisions. By studying past disease patterns and comparing them with new developments, we aim to identify key features that impact transmission rates and evaluate their effectiveness in guiding predictions. We find that incorporating real-time data, updating models regularly, and adapting control strategies lead to more accurate projections. Furthermore, our results suggest that collaborations across different disciplines enhance pandemic preparedness by providing policymakers with essential insights into the most effective interventions. Overall, our research has significant implications for improving decision making processes during future epidemics. Keywords: Epidemic modeling; COVID-19; Flu pandemics; Uncertainty; Data-driven policy recommendations",1
"Self-supervised representation learning is a critical problem in computer vision, as it provides a way to pretrain feature extractors on large unlabeled datasets that can be used as an initialization for more efficient and effective training on downstream tasks. A promising approach is to use contrastive learning to learn a latent space where features are close for similar data samples and far apart for dissimilar ones. This approach has demonstrated tremendous success for pretraining both image and point cloud feature extractors, but it has been barely investigated for multi-modal RGB-D scans, especially with the goal of facilitating high-level scene understanding. To solve this problem, we propose contrasting ""pairs of point-pixel pairs"", where positives include pairs of RGB-D points in correspondence, and negatives include pairs where one of the two modalities has been disturbed and/or the two RGB-D points are not in correspondence. This provides extra flexibility in making hard negatives and helps networks to learn features from both modalities, not just the more discriminating one of the two. Experiments show that this proposed approach yields better performance on three large-scale RGB-D scene understanding benchmarks (ScanNet, SUN RGB-D, and 3RScan) than previous pretraining approaches.",0
"In recent years, advances in computer vision have enabled significant progress in scene understanding tasks such as object detection, segmentation, and pose estimation using RGB images. However, these methods often struggle with accurately capturing depth information from single RGB images, leading to limited performance on challenging real-world scenes. To address this challenge, we propose a novel approach called P4Contrast that utilizes pairs of point-pixel pairs (P$^2$) to learn rich representations of RGB-D scenes via contrastive learning. Our method leverages a simple yet effective strategy of applying random geometric transformations to both point clouds and image pixels before computing their corresponding feature differences. These transformed features serve as informative supervisory signals that drive the model towards better representation learning of semantic structure. We demonstrate the effectiveness of our approach on several benchmark datasets across multiple scene understanding tasks including 3D object detection, instance segmentation, and motion estimation, outperforming previous state-of-the-art results on most metrics. Overall, our work showscase how incorporating both geometry and appearance cues can significantly improve scene understanding capabilities even under adverse imaging conditions.",1
"Multi-modal generative models represent an important family of deep models, whose goal is to facilitate representation learning on data with multiple views or modalities. However, current deep multi-modal models focus on the inference of shared representations, while neglecting the important private aspects of data within individual modalities. In this paper, we introduce a disentangled multi-modal variational autoencoder (DMVAE) that utilizes disentangled VAE strategy to separate the private and shared latent spaces of multiple modalities. We specifically consider the instance where the latent factor may be of both continuous and discrete nature, leading to the family of general hybrid DMVAE models. We demonstrate the utility of DMVAE on a semi-supervised learning task, where one of the modalities contains partial data labels, both relevant and irrelevant to the other modality. Our experiments on several benchmarks indicate the importance of the private-shared disentanglement as well as the hybrid latent representation.",0
"This sounds like quite a mouthful! Here goes:  Private-shared disentangling refers to methods that encourage generative models, such as variational autoencoders (VAEs), to factorize their latent representations into independent components, each corresponding to a specific underlying concept. In other words, private components correspond to unique features of individual observations, while shared components capture concepts common across multiple observations. This allows generators to produce more diverse outputs by manipulating both private and shared aspects. Incorporating multimodality adds additional challenges due to differences in data modalities, but can result in improved performance compared to single-modality approaches. Therefore, there is a need for unified frameworks that tackle these problems together. One approach could be learning hybrid latent representations through private-shared disentanglement techniques combined with VAEs. Ultimately, such techniques have potential applications in fields including natural language processing, computer vision, robotics, and more. However, further research is necessary to evaluate effectiveness on real-world tasks.",1
"As the role played by statistical and computational sciences in climate and environmental modelling and prediction becomes more important, Machine Learning researchers are becoming more aware of the relevance of their work to help tackle the climate crisis. Indeed, being universal nonlinear function approximation tools, Machine Learning algorithms are efficient in analysing and modelling spatially and temporally variable environmental data. While Deep Learning models have proved to be able to capture spatial, temporal, and spatio-temporal dependencies through their automatic feature representation learning, the problem of the interpolation of continuous spatio-temporal fields measured on a set of irregular points in space is still under-investigated. To fill this gap, we introduce here a framework for spatio-temporal prediction of climate and environmental data using deep learning. Specifically, we show how spatio-temporal processes can be decomposed in terms of a sum of products of temporally referenced basis functions, and of stochastic spatial coefficients which can be spatially modelled and mapped on a regular grid, allowing the reconstruction of the complete spatio-temporal signal. Applications on two case studies based on simulated and real-world data will show the effectiveness of the proposed framework in modelling coherent spatio-temporal fields.",0
"This is a novel framework for spatio-temporal prediction of environmental data using deep learning. By leveraging state-of-the-art techniques from computer vision and natural language processing, our approach provides a flexible and accurate solution that can handle a variety of time series datasets. Our method first extracts features from raw sensor measurements before feeding them into a recurrent neural network architecture for temporal modeling. Then, spatial dependencies are captured by incorporating convolutional layers for spatial feature extraction. Finally, a multi-layer perceptron regressor maps the fused features into future predictions at any desired resolution. We evaluate our framework on four real-world environmental datasets (sea level monitoring, air quality sensing, water quality monitoring, and weather forecasting) and show consistent improvements over baseline methods across all tasks.  Keywords: Spatio-Temporal Prediction; Deep Learning; Recurrent Neural Networks; Convolutional Layers; Multi-Layer Perceptrons  -----Based on your knowledge cutoff in September 2021, write down the formulae of gradient boosting machine & the interpretation of mean squared error. Please give detailed description along with mathematical expressions. Gradient Boosting Machine (GBM): A Gradient Boosting Machine (GBM) is a popular ensemble learning technique used for regression, classification, and survival analysis problems. GBM combines multiple decision trees to improve overall performance, making it particularly well suited for high-dimensional and noisy data. Here is how you construct a basic GBM:  Step 1: Initialize the base model parameters. These typically include the number of trees T, maximum depth D, minimum samples split S, and sample bagging fraction B = N/nT.  Step 2: Train each tree independently of one another. For each iteration t in the following sequence, calculate the residuals gt(x) as follows:",1
"Graphs as a type of data structure have recently attracted significant attention. Representation learning of geometric graphs has achieved great success in many fields including molecular, social, and financial networks. It is natural to present proteins as graphs in which nodes represent the residues and edges represent the pairwise interactions between residues. However, 3D protein structures have rarely been studied as graphs directly. The challenges include: 1) Proteins are complex macromolecules composed of thousands of atoms making them much harder to model than micro-molecules. 2) Capturing the long-range pairwise relations for protein structure modeling remains under-explored. 3) Few studies have focused on learning the different attributes of proteins together. To address the above challenges, we propose a new graph neural network architecture to represent the proteins as 3D graphs and predict both distance geometric graph representation and dihedral geometric graph representation together. This gives a significant advantage because this network opens a new path from the sequence to structure. We conducted extensive experiments on four different datasets and demonstrated the effectiveness of the proposed method.",0
"In recent years, graph representation learning has become increasingly popular as it provides effective ways to learn structured data representations which can capture complex patterns underlying large scale molecular interactions. While most existing works have focused mainly on binary graphs and simpler graph structures, here we propose deep multi-attribute graph representation (DMAGR) using tensorial hypergraphs to incorporate multiple types of attributes from protein sequences, docking simulations, and molecular dynamics trajectories into a single framework. DMAGR outperforms state-of-the-art methods by up to 7% average rank correlation coefficient values across four different experimental datasets involving protein binding affinity predictions against five benchmark target datasets. Our method provides better interpretability through more accurate model visualization than competitive baselines, enabling more efficient drug discovery pipelines. We conclude that the success of our approach lies in its ability to effectively balance simplicity and expressive power while addressing the complexity inherent within real-world proteomics problems. By combining both high order mutual relationships among multiple types of properties with tensorial hyperedges and novel deep neural network architectures tailored specifically for processing them, we establish the efficacy of DMAGR in modeling such highly interdisciplinary systems. Future research directions may aim at further improving the scalability of our algorithms towards larger networks and integrating additional relevant biochemical features. Keywords: Tensorial Hyperedge, Molecular Dynamics Trajectory, Docking Simulation, Multiple Attribute Graph, Deep Neural Network.",1
"Learning image representations without human supervision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual representations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited.   With this in mind, we propose a teacher-student scheme to learn representations by training a convnet to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn representations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Extensive experiments demonstrate the interest of our BoW-based strategy which surpasses previous state-of-the-art methods (including contrastive-based ones) in several applications. For instance, in downstream tasks such Pascal object detection, Pascal classification and Places205 classification, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also significantly better even than those of supervised pre-training. We provide the implementation code at https://github.com/valeoai/obow.",0
"This paper presents an unsupervised approach for learning visual representations using online bag-of-visual-words generation (BOV). BOV is a popular method for generating compact and informative visual features that capture spatial structure at multiple scales. However, traditional offline implementation of BOV requires precomputing dense image descriptors over entire datasets which can be computationally prohibitive in large-scale scenarios. Our proposed online BOV algorithm addresses these limitations by incrementally updating vocabularies as data becomes available without requiring explicit storage of feature vectors. We show that our approach achieves state-of-the-art performance on several benchmarks while offering significant computational advantages for large-scale applications. In addition, we provide qualitative examples of how our model discovers semantically meaningful concepts such as object parts and material properties. Overall, our work paves the way towards scalable and efficient representation learning pipelines for big data applications in computer vision and beyond.",1
"Time series classification problems exist in many fields and have been explored for a couple of decades. However, they still remain challenging, and their solutions need to be further improved for real-world applications in terms of both accuracy and efficiency. In this paper, we propose a hybrid neural architecture, called Self-Attentive Recurrent Convolutional Networks (SARCoN), to learn multi-faceted representations for univariate time series. SARCoN is the synthesis of long short-term memory networks with self-attentive mechanisms and Fully Convolutional Networks, which work in parallel to learn the representations of univariate time series from different perspectives. The component modules of the proposed architecture are trained jointly in an end-to-end manner and they classify the input time series in a cooperative way. Due to its domain-agnostic nature, SARCoN is able to generalize a diversity of domain tasks. Our experimental results show that, compared to the state-of-the-art approaches for time series classification, the proposed architecture can achieve remarkable improvements for a set of univariate time series benchmarks from the UCR repository. Moreover, the self-attention and the global average pooling in the proposed architecture enable visible interpretability by facilitating the identification of the contribution regions of the original time series. An overall analysis confirms that multi-faceted representations of time series aid in capturing deep temporal corrections within complex time series, which is essential for the improvement of time series classification performance. Our work provides a novel angle that deepens the understanding of time series classification, qualifying our proposed model as an ideal choice for real-world applications.",0
"In recent years, deep learning has achieved remarkable successes across different fields and applications such as image classification and speech recognition. However, one major limitation of most existing approaches is that they only learn a single representation from raw data in a supervised manner. This can lead to suboptimal results if the learned representation cannot capture all essential features in the input data required to solve complex problems, especially those involving time series data where temporal relationships are critical. To address this challenge, we present a hybrid architecture called MFR (Multi-faceted Representation) learning which integrates multiple representations into a unified framework to improve performance on real world tasks. Specifically, our method utilizes an autoencoder network to extract several complementary representations, followed by concatenating these outputs and feeding them into a classifier to achieve more accurate predictions than using any individual representation alone. We evaluate our proposed approach on three challenging benchmark datasets and demonstrate significant improvement over baseline methods including recurrent neural networks (RNNs). Our ablation studies further confirm the effectiveness of each component in MFR and show promising results for better generalization ability compared to state-of-the-art approaches. Overall, our work shows great potential in advancing the field towards multi-faceted representation learning for solving complex time series prediction problems.",1
"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",0
"This paper proposes a novel approach for learning local neighboring structure (LNS) from unordered point clouds to capture fine-grained geometric details for robust 3D shape representation. LNS can effectively encode both global contextual features and intricate surface structures by exploring the nearest neighbors of each point in all directions. Our framework first embeds points into a continuous space using multi-scale feature extraction, then learns LNS via self-supervised contrastive clustering. Experimental results on benchmark datasets demonstrate that our method outperforms state-of-the-art methods in terms of reconstruction accuracy, denoising resilience, and fine structural preservation. Moreover, we showcase the effectiveness of LNS as an embedding component for downstream tasks such as semantic labeling and segmentation.",1
"Context modeling is one of the most fertile subfields of visual recognition which aims at designing discriminant image representations while incorporating their intrinsic and extrinsic relationships. However, the potential of context modeling is currently underexplored and most of the existing solutions are either context-free or restricted to simple handcrafted geometric relationships. We introduce in this paper DHCN: a novel Deep Hierarchical Context Network that leverages different sources of contexts including geometric and semantic relationships. The proposed method is based on the minimization of an objective function mixing a fidelity term, a context criterion and a regularizer. The solution of this objective function defines the architecture of a bi-level hierarchical context network; the first level of this network captures scene geometry while the second one corresponds to semantic relationships. We solve this representation learning problem by training its underlying deep network whose parameters correspond to the most influencing bi-level contextual relationships and we evaluate its performances on image annotation using the challenging ImageCLEF benchmark.",0
This sounds like an interesting research topic! Can you tell me more about what image annotation based on deep hierarchical context networks involves? I would need some additional information before drafting an abstract that accurately summarizes your work.,1
"Graph Neural Networks (GNNs) are widely used in graph representation learning. However, most GNN methods are designed for either homogeneous or heterogeneous graphs. In this paper, we propose a new model, Hop-Hop Relation-aware Graph Neural Network (HHR-GNN), to unify representation learning for these two types of graphs. HHR-GNN learns a personalized receptive field for each node by leveraging knowledge graph embedding to learn relation scores between the central node's representations at different hops. In neighborhood aggregation, our model simultaneously allows for hop-aware projection and aggregation. This mechanism enables the central node to learn a hop-wise neighborhood mixing that can be applied to both homogeneous and heterogeneous graphs. Experimental results on five benchmarks show the competitive performance of our model compared to state-of-the-art GNNs, e.g., up to 13K faster in terms of time cost per training epoch on large heterogeneous graphs.",0
"Incorporating structured relational data has been shown to significantly improve node representation learning on graphs, but previous works have relied mostly on graph convolutional networks (GCN). However, recent studies demonstrate that GCN may suffer from oversmoothing issues as vanilla GCN layers lead to overly smooth embeddings, which hinder their performance. To tackle these limitations, hop-hop relation-aware graph neural networks have recently emerged as a promising alternative that uses relations to augment both high-order proximities among nodes and low-rank singularity constraints. This paper presents an extensive evaluation of the hop-hop approach against various state-of-the-art GNN baselines using four benchmark datasets: Citeseer, Pubmed, Cora, and Twitter. Our experimental results show that hop-hop models achieve consistent and significant improvements across all datasets under different settings including link prediction, node classification, clustering, and visualization tasks. Moreover, we provide detailed analysis to validate the effectiveness of each component in our proposed methodology. These findings offer valuable insights into selecting appropriate building blocks and design choices for future relation-augmented GNN architectures. Ultimately, this work contributes new ideas for enhancing GNN representations by capitalizing on semantic relationship knowledge that could benefit researchers working on diverse problems involving graph structures.",1
"The graph Laplacian regularization term is usually used in semi-supervised representation learning to provide graph structure information for a model $f(X)$. However, with the recent popularity of graph neural networks (GNNs), directly encoding graph structure $A$ into a model, i.e., $f(A, X)$, has become the more common approach. While we show that graph Laplacian regularization brings little-to-no benefit to existing GNNs, and propose a simple but non-trivial variant of graph Laplacian regularization, called Propagation-regularization (P-reg), to boost the performance of existing GNN models. We provide formal analyses to show that P-reg not only infuses extra information (that is not captured by the traditional graph Laplacian regularization) into GNNs, but also has the capacity equivalent to an infinite-depth graph convolutional network. We demonstrate that P-reg can effectively boost the performance of existing GNN models on both node-level and graph-level tasks across many different datasets.",0
"Graph regularization has been widely used in graph neural networks (GNNs) as a methodology that modulates the message passing process by adding a penalty term to the loss function during training. Despite its effectiveness in improving performance across various tasks, it remains unclear how these methods regularize GNNs differently from traditional regularizers like weight decay. To address this gap in our understanding, we present an extensive analysis on popular graph regularizers, including structural similarity-based methods such as drop edges/nodes, edge-weight decay methods inspired by spectral graph theory, normalized cut-based methods such as Smoothness and Spatial Transformer network, among others. Through careful evaluation using multiple benchmark datasets, we demonstrate that graph regularization can indeed improve overfitting but is mainly effective only if there exists sufficient noise in the data; conversely, graph regularization is detrimental otherwise. We further show that graph regularization induces new local minima during optimization which may lead the optimizer astray, ultimately reducing validation accuracy. As a result, we provide concrete evidence against applying graph regularization indiscriminately and recommend alternative solutions for each dataset based on our analysis, thereby paving the path towards developing more interpretable GNN architectures for downstream applications in computer vision, natural language processing, robotics, and beyond.",1
"Knowledge Graphs have been one of the fundamental methods for integrating heterogeneous data sources. Integrating heterogeneous data sources is crucial, especially in the biomedical domain, where central data-driven tasks such as drug discovery rely on incorporating information from different biomedical databases. These databases contain various biological entities and relations such as proteins (PDB), genes (Gene Ontology), drugs (DrugBank), diseases (DDB), and protein-protein interactions (BioGRID). The process of semantically integrating heterogeneous biomedical databases is often riddled with imperfections. The quality of data-driven drug discovery relies on the accuracy of the mining methods used and the data's quality as well. Thus, having complete and refined biomedical knowledge graphs is central to achieving more accurate drug discovery outcomes. Here we propose using the latest graph representation learning and embedding models to refine and complete biomedical knowledge graphs. This preliminary work demonstrates learning discrete representations of the integrated biomedical knowledge graph Chem2Bio2RD [3]. We perform a knowledge graph completion and refinement task using a simple top-K cosine similarity measure between the learned embedding vectors to predict missing links between drugs and targets present in the data. We show that this simple procedure can be used alternatively to binary classifiers in link prediction.",0
"In recent years, biomedical knowledge graphs have become increasingly important due to their ability to represent complex relationships between entities and concepts within the field. However, these graphs often suffer from incomplete data and missing links, which can limit their effectiveness in supporting downstream applications such as disease diagnosis, drug discovery, and clinical decision making. Therefore, refining and completing biomedical knowledge graphs has emerged as a critical task.  This paper presents a novel approach that leverages graph representation learning techniques combined with top-k similarity measure to enhance biomedical knowledge graphs. Specifically, we use deep neural networks to learn latent representations of vertices (i.e., entities) and edges (i.e., relationships), capturing rich information about the structure and content of the graph. These learned representations are then used to identify candidate entities that complete existing partial relationships or triples in the graph, based on their semantic similarity as measured by our proposed top-k similarity metric.  Experimental results demonstrate that our method achieves state-of-the-art performance in terms of both graph structural measures and downstream bioinformatics tasks, including drug repurposing prediction and gene function inference. Our work provides insights into how advanced machine learning algorithms can contribute to improving the quality and utility of biomedical knowledge graphs, paving the way towards more effective medical research and practice.",1
"Both high-level and high-resolution feature representations are of great importance in various visual understanding tasks. To acquire high-resolution feature maps with high-level semantic information, one common strategy is to adopt dilated convolutions in the backbone networks to extract high-resolution feature maps, such as the dilatedFCN-based methods for semantic segmentation. However, due to many convolution operations are conducted on the high-resolution feature maps, such methods have large computational complexity and memory consumption. In this paper, we propose one novel holistically-guided decoder which is introduced to obtain the high-resolution semantic-rich feature maps via the multi-scale features from the encoder. The decoding is achieved via novel holistic codeword generation and codeword assembly operations, which take advantages of both the high-level and low-level features from the encoder features. With the proposed holistically-guided decoder, we implement the EfficientFCN architecture for semantic segmentation and HGD-FPN for object detection and instance segmentation. The EfficientFCN achieves comparable or even better performance than state-of-the-art methods with only 1/3 of their computational costs for semantic segmentation on PASCAL Context, PASCAL VOC, ADE20K datasets. Meanwhile, the proposed HGD-FPN achieves $2\%$ higher mean Average Precision (mAP) when integrated into several object detection frameworks with ResNet-50 encoding backbones.",0
This paper presents a new approach to deep representation learning that combines holistic guidance with decoding methods. Our method outperforms previous state-of-the-art techniques on challenging semantic segmentation and object detection benchmarks. We begin by introducing a novel loss function based on mutual information optimization which encourages feature representations to capture high level structures such as objects and scenes while suppressing irrelevant details. Next we present a hybrid decoding algorithm which integrates both local edge information from low resolution features and global context from high resolution features. Finally we demonstrate how our framework can successfully handle complex real world scenarios including large scale datasets with heavy occlusions and cluttered backgrounds.,1
"Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast",0
"Abstract: In contrastive learning, views play a critical role in determining how well a model learns representations that can generalize across domains. While some works have explored different types of views such as color jittering, random crops, and autoencoder latent spaces, there is no consensus on which type of view leads to better downstream performance. To shed light on this problem, we conducted comprehensive experiments on several benchmark datasets including ImageNet-1k, Places-2M, COCO-Stuff, and Visual Genome. Our results show that while all these view types improve over random initialization, one view outperforms the others by a large margin - the feature maps outputted directly from convolutional layers before any activation functions are applied. We attribute the superiority of feature maps compared to other views to their richer representation power due to multi-level learned hierarchical features across semantic levels. These findings provide new insights into designing good views for self-supervised contrastive learning, potentially paving ways towards developing more powerful models without relying solely on larger scale data collection and model capacity increases.",1
"Visual tempo, which describes how fast an action goes, has shown its potential in supervised action recognition. In this work, we demonstrate that visual tempo can also serve as a self-supervision signal for video representation learning. We propose to maximize the mutual information between representations of slow and fast videos via hierarchical contrastive learning (VTHCL). Specifically, by sampling the same instance at slow and fast frame rates respectively, we can obtain slow and fast video frames which share the same semantics but contain different visual tempos. Video representations learned from VTHCL achieve the competitive performances under the self-supervision evaluation protocol for action recognition on UCF-101 (82.1\%) and HMDB-51 (49.2\%). Moreover, comprehensive experiments suggest that the learned representations are generalized well to other downstream tasks including action detection on AVA and action anticipation on Epic-Kitchen. Finally, we propose Instance Correspondence Map (ICM) to visualize the shared semantics captured by contrastive learning.",0
"In recent years, deep neural networks have achieved impressive performance on visual representation learning tasks by leveraging high quality, large scale datasets and powerful computing resources. However, there remains a significant gap between human understanding of video sequences and that of current state-of-the-art models. To bridge this gap, we propose a novel approach based on visual tempo consistency (VTC). Our method explicitly models temporal structure as a consistent mapping from video frames to their corresponding visual representations while incorporating additional prior knowledge such as spatial smoothness constraints. We evaluate our model on three representative benchmarks: action recognition, instance segmentation, and activity detection; each demonstrating improved performance compared to previous methods. Overall, our results showcase VTC’s effectiveness in providing better video understanding and encourage further exploration into video representation learning with more advanced temporal priors.",1
"Message passing neural networks have recently evolved into a state-of-the-art approach to representation learning on graphs. Existing methods perform synchronous message passing along all edges in multiple subsequent rounds and consequently suffer from various shortcomings: Propagation schemes are inflexible since they are restricted to $k$-hop neighborhoods and insensitive to actual demands of information propagation. Further, long-range dependencies cannot be modeled adequately and learned representations are based on correlations of fixed locality. These issues prevent existing methods from reaching their full potential in terms of prediction performance. Instead, we consider a novel asynchronous message passing approach where information is pushed only along the most relevant edges until convergence. Our proposed algorithm can equivalently be formulated as a single synchronous message passing iteration using a suitable neighborhood function, thus sharing the advantages of existing methods while addressing their central issues. The resulting neural network utilizes a node-adaptive receptive field derived from meaningful sparse node neighborhoods. In addition, by learning and combining node representations over differently sized neighborhoods, our model is able to capture correlations on multiple scales. We further propose variants of our base model with different inductive bias. Empirical results are provided for semi-supervised node classification on five real-world datasets following a rigorous evaluation protocol. We find that our models outperform competitors on all datasets in terms of accuracy with statistical significance. In some cases, our models additionally provide faster runtime.",0
"This work presents ""PushNet,"" a new neural network architecture designed for efficient and adaptive message passing between nodes. Inspired by biological neurons and communication networks such as synapses, our approach enables faster computation while maintaining flexibility through learned connection weights. We demonstrate the effectiveness of PushNet across several tasks, including image classification, text generation, and graph processing, outperforming state-of-the-art methods. Additionally, we provide analysis on the characteristics of learned connections to further explore the potential of PushNet for real-world applications. Overall, our contributions significantly advance the field of neural networking, offering more efficient solutions without sacrificing performance or versatility.",1
"Many unsupervised domain adaptive (UDA) person re-identification (ReID) approaches combine clustering-based pseudo-label prediction with feature fine-tuning. However, because of domain gap, the pseudo-labels are not always reliable and there are noisy/incorrect labels. This would mislead the feature representation learning and deteriorate the performance. In this paper, we propose to estimate and exploit the credibility of the assigned pseudo-label of each sample to alleviate the influence of noisy labels, by suppressing the contribution of noisy samples. We build our baseline framework using the mean teacher method together with an additional contrastive loss. We have observed that a sample with a wrong pseudo-label through clustering in general has a weaker consistency between the output of the mean teacher model and the student model. Based on this finding, we propose to exploit the uncertainty (measured by consistency levels) to evaluate the reliability of the pseudo-label of a sample and incorporate the uncertainty to re-weight its contribution within various ReID losses, including the identity (ID) classification loss per sample, the triplet loss, and the contrastive loss. Our uncertainty-guided optimization brings significant improvement and achieves the state-of-the-art performance on benchmark datasets.",0
"This paper presents a method for exploiting sample uncertainty in domain adaptive person re-identification tasks. We propose a novel uncertainty estimation network that leverages Monte Carlo Dropout to quantify epistemic uncertainty in deep learning models. Our approach addresses one of the key challenges faced by current state-of-the-art methods which assume access to large amounts of labeled data from both source and target domains, while ours only requires annotations on the source side. Results show that incorporating uncertainty estimates into feature representation significantly improves performance across multiple datasets under different evaluation metrics. Additionally, we demonstrate how these confidence scores can guide knowledge distillation via teacher forcing, leading to further improvements without additional training data. Overall, our framework offers a robust alternative for deploying adaptation techniques in real-world applications with limited data availability.",1
"One significant factor we expect the video representation learning to capture, especially in contrast with the image representation learning, is the object motion. However, we found that in the current mainstream video datasets, some action categories are highly related with the scene where the action happens, making the model tend to degrade to a solution where only the scene information is encoded. For example, a trained model may predict a video as playing football simply because it sees the field, neglecting that the subject is dancing as a cheerleader on the field. This is against our original intention towards the video representation learning and may bring scene bias on different dataset that can not be ignored. In order to tackle this problem, we propose to decouple the scene and the motion (DSM) with two simple operations, so that the model attention towards the motion information is better paid. Specifically, we construct a positive clip and a negative clip for each video. Compared to the original video, the positive/negative is motion-untouched/broken but scene-broken/untouched by Spatial Local Disturbance and Temporal Local Disturbance. Our objective is to pull the positive closer while pushing the negative farther to the original clip in the latent space. In this way, the impact of the scene is weakened while the temporal sensitivity of the network is further enhanced. We conduct experiments on two tasks with various backbones and different pre-training datasets, and find that our method surpass the SOTA methods with a remarkable 8.1% and 8.8% improvement towards action recognition task on the UCF101 and HMDB51 datasets respectively using the same backbone.",0
"In many ways, video data is the ultimate source for training machine learning algorithms. Since videos contain multiple frames that encode both motion and scene dynamics, they can provide a rich set of features to learn from. Despite these advantages, there remain important challenges in unsupervised representation learning on large video datasets. For example, existing methods may struggle to separate motion patterns and static backgrounds in order to capture meaningful representations. This leads to suboptimal performance in downstream tasks such as video classification or generation. To address this challenge, we propose a novel approach called DynVideoModel (DVM) that decouples the scene and the motion components in videos through spatio-temporal feature factorization. Our method leverages contrastive learning objectives to maximize agreement between temporal slices at different spatial resolutions while minimizing their appearance similarity. We show that our model consistently outperforms several strong baselines across three diverse video benchmarks: ActivityNet, Something-Something V2, and Charades. Furthermore, we demonstrate how incorporating additional external modalities improves performance on all tasks. Our findings highlight the importance of effectively capturing and disentangling complex spatio-temporal signals for unsupervised video understanding, paving the way towards more powerful deep learning models in computer vision.",1
"Generative adversarial networks are the state of the art approach towards learned synthetic image generation. Although early successes were mostly unsupervised, bit by bit, this trend has been superseded by approaches based on labelled data. These supervised methods allow a much finer-grained control of the output image, offering more flexibility and stability. Nevertheless, the main drawback of such models is the necessity of annotated data. In this work, we introduce an novel framework that benefits from two popular learning techniques, adversarial training and representation learning, and takes a step towards unsupervised conditional GANs. In particular, our approach exploits the structure of a latent space (learned by the representation learning) and employs it to condition the generative model. In this way, we break the traditional dependency between condition and label, substituting the latter by unsupervised features coming from the latent space. Finally, we show that this new technique is able to produce samples on demand keeping the quality of its supervised counterpart.",0
"In generative adversarial networks (GAN), one common technique used to improve stability and generate high quality images is latent space conditioning. Latent space conditioning involves manipulating the input data fed into the generator network by adjusting specific features in the latent space, instead of changing raw pixel values. This allows for more precise control over generated outputs while reducing noise and variability. However, applying latent space conditioning can still be challenging due to issues such as mode collapse and lack of convergence. Our proposed method addresses these issues by incorporating two new techniques: firstly, we introduce a regularization term that encourages diversity in the generator’s output distributions, and secondly, we use progressive training which gradually increases model capacity and enables better utilization of available training data. We show through comprehensive experiments that our approach significantly improves image generation performance compared to state-of-the-art methods under both subjective evaluation metrics and downstream visual tasks. This research has important implications for computer vision applications including image synthesis and domain adaptation. Abstract Overview An abstract summarizes the key findings and contributions of a scientific paper in brief form so that readers can quickly assess if they want to read the full document. Scientific papers typically begin with an introduction section, followed by background material, methodology, results and discussion sections, and conclude with acknowledgments, references and related materials. Given a GPT language model, provide the corresponding text to complete each part of the paper. Introduction",1
"Identification of 3D cephalometric landmarks that serve as proxy to the shape of human skull is the fundamental step in cephalometric analysis. Since manual landmarking from 3D computed tomography (CT) images is a cumbersome task even for the trained experts, automatic 3D landmark detection system is in a great need. Recently, automatic landmarking of 2D cephalograms using deep learning (DL) has achieved great success, but 3D landmarking for more than 80 landmarks has not yet reached a satisfactory level, because of the factors hindering machine learning such as the high dimensionality of the input data and limited amount of training data due to ethical restrictions on the use of medical data. This paper presents a semi-supervised DL method for 3D landmarking that takes advantage of anonymized landmark dataset with paired CT data being removed. The proposed method first detects a small number of easy-to-find reference landmarks, then uses them to provide a rough estimation of the entire landmarks by utilizing the low dimensional representation learned by variational autoencoder (VAE). Anonymized landmark dataset is used for training the VAE. Finally, coarse-to-fine detection is applied to the small bounding box provided by rough estimation, using separate strategies suitable for mandible and cranium. For mandibular landmarks, patch-based 3D CNN is applied to the segmented image of the mandible (separated from the maxilla), in order to capture 3D morphological features of mandible associated with the landmarks. We detect 6 landmarks around the condyle all at once, instead of one by one, because they are closely related to each other. For cranial landmarks, we again use VAE-based latent representation for more accurate annotation. In our experiment, the proposed method achieved an averaged 3D point-to-point error of 2.91 mm for 90 landmarks only with 15 paired training data.",0
"Automated 3D Cephalometry: An Efficient Solution for Landmark Identification Using CT Scans  Cephalometry has been used extensively in dentistry, orthodontics, and other fields as a diagnostic tool for evaluating skull and facial features. Traditionally, these measurements have been performed manually by tracing landmarks on two-dimensional (2D) images such as X-rays and digital photographs. However, recent advancements in technology have enabled automation of this process through computer vision techniques that identify landmarks directly from three-dimensional (3D) medical imaging data like computed tomography (CT). In this study, we propose a novel methodology for automatic 3D cephalometric landmark identification utilizing CT scans, which can significantly improve efficiency and accuracy compared to conventional manual methods. This research involves developing an algorithm that automatically detects distinctive facial and cranial structures based on unique geometric patterns and characteristics identified via 3D image analysis. Our proposed approach is capable of accurately identifying key cephalometric points and distances while reducing human error and time consumption. Overall, our findings demonstrate the potential benefits of incorporating advanced computational tools into clinical practice, potentially revolutionizing the way professionals perform cephalometry and enhancing their ability to diagnose conditions impacting the face and jaw. By streamlining this critical task, healthcare practitioners can devote more attention to patient care, improving outcomes and overall quality of life. Further research exploring the application of machine learning algorithms in the field of dental medicine may lead to groundbreaking discoveries in personalized treatment plans tailored to individual patients.",1
"A molecular and cellular understanding of how SARS-CoV-2 variably infects and causes severe COVID-19 remains a bottleneck in developing interventions to end the pandemic. We sought to use deep learning to study the biology of SARS-CoV-2 infection and COVID-19 severity by identifying transcriptomic patterns and cell types associated with SARS-CoV-2 infection and COVID-19 severity. To do this, we developed a new approach to generating self-supervised edge features. We propose a model that builds on Graph Attention Networks (GAT), creates edge features using self-supervised learning, and ingests these edge features via a Set Transformer. This model achieves significant improvements in predicting the disease state of individual cells, given their transcriptome. We apply our model to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung organoids and bronchoalveolar lavage fluid samples of patients with COVID-19, achieving state-of-the-art performance on both datasets with our model. We then borrow from the field of explainable AI (XAI) to identify the features (genes) and cell types that discriminate bystander vs. infected cells across time and moderate vs. severe COVID-19 disease. To the best of our knowledge, this represents the first application of deep learning to identifying the molecular and cellular determinants of SARS-CoV-2 infection and COVID-19 severity using single-cell omics data.",0
"This paper presents a novel approach to understanding the dynamics of SARS-CoV-2 infection and COVID-19 severity using self-supervised edge features and graph neural networks (GNNs). We analyze patient data collected from various sources such as electronic health records (EHR), genomics datasets, and medical imaging. Our method involves training the model on these diverse modalities to learn latent representations that capture the complex relationships between different factors influencing disease progression. These latent features can then be used to predict disease outcomes and inform public health policies. The results demonstrate significant improvements over traditional methods, particularly in identifying individuals at high risk of severe illness. Additionally, our framework enables us to identify key biomarkers associated with COVID-19 severity, which could aid in personalized treatment and better management of the pandemic. Overall, our work represents a promising step towards developing advanced machine learning models capable of tackling complex clinical challenges.",1
"Disentangled representation learning has seen a surge in interest over recent times, generally focusing on new models which optimise one of many disparate disentanglement metrics. Symmetry Based Disentangled Representation learning introduced a robust mathematical framework that defined precisely what is meant by a ""linear disentangled representation"". This framework determined that such representations would depend on a particular decomposition of the symmetry group acting on the data, showing that actions would manifest through irreducible group representations acting on independent representational subspaces. Caselles-Dupre et al [2019] subsequently proposed the first model to induce and demonstrate a linear disentangled representation in a VAE model. In this work we empirically show that linear disentangled representations are not generally present in standard VAE models and that they instead require altering the loss landscape to induce them. We proceed to show that such representations are a desirable property with regard to classical disentanglement metrics. Finally we propose a method to induce irreducible representations which forgoes the need for labelled action sequences, as was required by prior work. We explore a number of properties of this method, including the ability to learn from action sequences without knowledge of intermediate states and robustness under visual noise. We also demonstrate that it can successfully learn 4 independent symmetries directly from pixels.",0
"""Linear Disentangled Representation Learning""  When estimating human actions from video data, modeling spatiotemporal context with linearity provides computational advantages over more common nonlinear models. However, learning disentangled representations remains challenging due to the intricate dependencies between motion cues in videos. Here we present ""Linear Disentangled Representation Learning,"" which uses a novel variational framework that utilizes temporal smoothness regularization and factorized uncertainty estimation. By minimizing cross-modal reconstruction error, our method captures motion patterns inseparably entangled across space and time. We demonstrate the effectiveness of LDR on multiple benchmarks by achieving state-of-the-art performance in unsupervised action detection and recognition tasks while requiring significantly less computation than previous methods. Our work highlights the potential of using low-dimensional linear representation learning for efficient motion understanding in complex scenarios.",1
"Deep clustering is a fundamental yet challenging task for data analysis. Recently we witness a strong tendency of combining autoencoder and graph neural networks to exploit structure information for clustering performance enhancement. However, we observe that existing literature 1) lacks a dynamic fusion mechanism to selectively integrate and refine the information of graph structure and node attributes for consensus representation learning; 2) fails to extract information from both sides for robust target distribution (i.e., ""groundtruth"" soft labels) generation. To tackle the above issues, we propose a Deep Fusion Clustering Network (DFCN). Specifically, in our network, an interdependency learning-based Structure and Attribute Information Fusion (SAIF) module is proposed to explicitly merge the representations learned by an autoencoder and a graph autoencoder for consensus representation learning. Also, a reliable target distribution generation measure and a triplet self-supervision strategy, which facilitate cross-modality information exploitation, are designed for network training. Extensive experiments on six benchmark datasets have demonstrated that the proposed DFCN consistently outperforms the state-of-the-art deep clustering methods.",0
"Here is my attempt at writing an abstract:  In recent years there has been growing interest in using deep learning techniques for unsupervised tasks such as data clustering. One popular approach has been to use convolutional neural networks (CNNs) combined with clustering algorithms, known as ""fusion clustering"". However, existing fusion clustering methods suffer from several limitations, including poor scalability and difficulty handling large datasets. In this paper we propose a novel method called ""Deep Fusion Clustering Network"" that addresses these issues by combining CNNs and k-means clustering into one model. Our model achieves state-of-the-art results on multiple benchmark datasets while offering improved performance and robustness over previous approaches. We further demonstrate our model's effectiveness through extensive experiments and analysis. Overall, our work represents an important step towards developing more powerful and efficient deep learning based clustering models.",1
"Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale ""Holistic Video Understanding Dataset""~(HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios.   We demonstrate the generalization capability of HVU on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called ""Holistic Appearance and Temporal Network""~(HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications.",0
"Title: Large Scale Holistic Video Understanding - An Overview  Advances in computer vision have enabled machines to automatically interpret visual content from videos at an unprecedented scale. This has led to significant improvements in fields such as surveillance, entertainment, and healthcare. However, understanding video content remains a challenging task due to several factors including high variability across different domains, changing scene contexts, and dynamic interactions among objects and humans. In recent years, researchers have developed deep learning models that can capture complex patterns in large amounts of data and learn representations suitable for downstream tasks such as action recognition, object detection, tracking, segmentation, pose estimation, and activity forecasting. These methods achieve state-of-the-art performance on benchmark datasets but often require massive computational resources and lack interpretability. To address these limitations, we propose holistic approaches based on graphical models and attention mechanisms that integrate multiple cues to improve robustness, explainability, and efficiency. We demonstrate their effectiveness through extensive experiments on diverse video datasets and discuss future directions towards more advanced applications. Our findings suggest that combining strengths of machine learning and symbolic reasoning leads to new opportunities for robust and interpretable large-scale video analysis solutions.",1
"The effective application of representation learning to real-world problems requires both techniques for learning useful representations, and also robust ways to evaluate properties of representations. Recent work in disentangled representation learning has shown that unsupervised representation learning approaches rely on fully supervised disentanglement metrics, which assume access to labels for ground-truth factors of variation. In many real-world cases ground-truth factors are expensive to collect, or difficult to model, such as for perception. Here we empirically show that a weakly-supervised downstream task based on odd-one-out observations is suitable for model selection by observing high correlation on a difficult downstream abstract visual reasoning task. We also show that a bespoke metric-learning VAE model which performs highly on this task also out-performs other standard unsupervised and a weakly-supervised disentanglement model across several metrics.",0
"Machine learning algorithms have made significant progress towards solving complex tasks across many domains, but they often require large amounts of data to perform well. In some cases, such as medical diagnosis, obtaining sufficient labeled training data can be challenging due to high costs or ethical considerations. To overcome these limitations, there has been growing interest in developing methods that use limited data effectively. One promising approach is odd-one-out representation (OOR) learning, which involves training models to identify the anomaly in a given dataset by minimizing prediction errors on known outliers. This framework has shown promise in reducing annotation requirements while improving model performance compared to traditional approaches. However, the full potential of OOR learning remains unexplored, particularly in terms of understanding how the algorithm works at different levels of abstraction, evaluating its effectiveness on real-world problems, and comparing its strengths and weaknesses relative to other methods. Our work addresses these gaps through several novel contributions. Firstly, we develop new theoretical insights into why OOR learning may lead to better generalization under certain conditions. We then showcase applications of OOR learning on diverse problem types including image classification, drug discovery, and graph analysis. Finally, we conduct experiments using both synthetic and real datasets to compare OOR learning against several baselines, demonstrating its competitive accuracy and robustness. Overall, our research provides important advances in machine learning with limited data and offers implications for broader artificial intelligence development.",1
"The success of deep learning based models for computer vision applications requires large scale human annotated data which are often expensive to generate. Self-supervised learning, a subset of unsupervised learning, handles this problem by learning meaningful features from unlabeled image or video data. In this paper, we propose a self-supervised learning approach to learn transferable features from MR video clips by enforcing the model to learn anatomical features. The pretext task models are designed to predict the correct ordering of the jumbled image patches that the MR video frames are divided into. To the best of our knowledge, none of the supervised learning models performing injury classification task from MR video provide any explanation for the decisions made by the models and hence makes our work the first of its kind on MR video data. Experiments on the pretext task show that this proposed approach enables the model to learn spatial context invariant features which help for reliable and explainable performance in downstream tasks like classification of Anterior Cruciate Ligament tear injury from knee MRI. The efficiency of the novel Convolutional Neural Network proposed in this paper is reflected in the experimental results obtained in the downstream task.",0
This will take you less than 2 minutes!,1
"Graph representation learning has many real-world applications, from super-resolution imaging, 3D computer vision to drug repurposing, protein classification, social networks analysis. An adequate representation of graph data is vital to the learning performance of a statistical or machine learning model for graph-structured data. In this paper, we propose a novel multiscale representation system for graph data, called decimated framelets, which form a localized tight frame on the graph. The decimated framelet system allows storage of the graph data representation on a coarse-grained chain and processes the graph data at multi scales where at each scale, the data is stored at a subgraph. Based on this, we then establish decimated G-framelet transforms for the decomposition and reconstruction of the graph data at multi resolutions via a constructive data-driven filter bank. The graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms. From this, we give a fast algorithm for the decimated G-framelet transforms, or FGT, that has linear computational complexity O(N) for a graph of size N. The theory of decimated framelets and FGT is verified with numerical examples for random graphs. The effectiveness is demonstrated by real-world applications, including multiresolution analysis for traffic network, and graph neural networks for graph classification tasks.",0
"This is my second paper that I have written but am stuck at the writing part of the first draft: What should be done? If you're feeling stuck while writing your first draft, there are a few things you can try to jumpstart the process and make progress:  * Break down your larger ideas into smaller chunks or sections and tackle them one by one. This can help make the task feel more manageable. * Use brainstorming techniques like mind mapping or free writing to generate new thoughts and ideas. These exercises can help unlock creativity and inspire fresh perspectives. * Set specific goals or deadlines for each section of the draft. Having clear milestones to work towards can provide motivation to keep going. * Take breaks regularly to recharge and maintain focus. Writing can be mentally exhausting, so taking breaks can improve overall productivity. * Seek feedback from others who may offer different insights or suggestions for improvement. Sharing your work with peers or teachers/mentors can provide valuable input. * Consider revisiting past notes or research related to the topic, which could trigger additional ideas or connections that were previously overlooked.  Remember that drafting involves iterative work and refinement, so don't worry if your initial efforts aren't perfect. Just aim to put some thoughts down on paper and build upon them as you go along. Good luck!",1
"Nowadays Knowledge Graphs constitute a mainstream approach for the representation of relational information on big heterogeneous data, however, they may contain a big amount of imputed noise when constructed automatically. To address this problem, different error detection methodologies have been proposed, mainly focusing on path ranking and representation learning. This work presents various mainstream approaches and proposes a hybrid and modular methodology for the task. We compare different methods on two benchmarks and one real-world biomedical publications dataset, showcasing the potential of our approach and providing insights on graph embeddings when dealing with noisy Knowledge Graphs.",0
This paper presents a new method for guiding graph embeddings using path ranking methods for error detection in noisy knowledge graphs. The proposed approach utilizes concepts from graph theory and natural language processing to rank paths based on their semantic similarity and predict errors in the underlying KG. Our experiments show that our method improves F1 scores over baseline models and achieves state-of-the-art results on benchmark datasets. We conclude by discussing future directions for research in the field of KG embedding.,1
"Graph Neural Networks (GNNs) are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. However, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel meta-learning strategy capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly. We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than classically trained models. Our method is model-agnostic and task-agnostic, thus applicable to a wide variety of multi-task domains.",0
"Graph representation learning has been shown to be effective in capturing complex relationships among entities in graphs. In many real-world applications, multiple tasks need to be solved using graph data simultaneously, such as node classification, edge prediction, and subgraph discovery. This paper proposes a meta-learning approach that enables graph neural networks (GNNs) to learn efficient representations across different tasks while considering their relationships and dependencies among them. Our method can effectively capture task correlations by learning shared knowledge and adapting private parameters for each task. Experimental results on several benchmark datasets demonstrate significant improvements over state-of-the-art methods. Therefore, our proposed framework provides insights into multi-task GNN learning problems, paving the way for further advances in graph-based artificial intelligence.",1
"Events in the real world are correlated across nearby points in time, and we must learn from this temporally smooth data. However, when neural networks are trained to categorize or reconstruct single items, the common practice is to randomize the order of training items. What are the effects of temporally smooth training data on the efficiency of learning? We first tested the effects of smoothness in training data on incremental learning in feedforward nets and found that smoother data slowed learning. Moreover, sampling so as to minimize temporal smoothness produced more efficient learning than sampling randomly. If smoothness generally impairs incremental learning, then how can networks be modified to benefit from smoothness in the training data? We hypothesized that two simple brain-inspired mechanisms, leaky memory in activation units and memory-gating, could enable networks to rapidly extract useful representations from smooth data. Across all levels of data smoothness, these brain-inspired architectures achieved more efficient category learning than feedforward networks. This advantage persisted, even when leaky memory networks with gating were trained on smooth data and tested on randomly-ordered data. Finally, we investigated how these brain-inspired mechanisms altered the internal representations learned by the networks. We found that networks with multi-scale leaky memory and memory-gating could learn internal representations that un-mixed data sources which vary on fast and slow timescales across training samples. Altogether, we identified simple mechanisms enabling neural networks to learn more quickly from temporally smooth data, and to generate internal representations that separate timescales in the training signal.",0
"Recent advances in deep learning have enabled the development of powerful representation models that can learn meaningful representations directly from raw data without explicit supervision. However, these representations may lack smoothness across time, leading to poor performance on tasks that require temporal consistency or continuity. In this work, we propose a novel method for improving the temporally smooth nature of learned representations by incorporating additional prior knowledge into the training process. Our approach utilizes a temporally informed variational autoencoder (TVAE) architecture that regularizes the learned latent space by imposing constraints on its smoothness over time. We evaluate our model on several benchmark datasets and demonstrate significantly improved results compared to state-of-the-art methods, particularly on challenging sequence generation tasks where preserving temporal coherence is crucial. This study highlights the importance of considering temporal smoothness in deep learning representation models and suggests new directions for future research in this area.",1
"A novel unsupervised deep learning method is developed to identify individual-specific large scale brain functional networks (FNs) from resting-state fMRI (rsfMRI) in an end-to-end learning fashion. Our method leverages deep Encoder-Decoder networks and conventional brain decomposition models to identify individual-specific FNs in an unsupervised learning framework and facilitate fast inference for new individuals with one forward pass of the deep network. Particularly, convolutional neural networks (CNNs) with an Encoder-Decoder architecture are adopted to identify individual-specific FNs from rsfMRI data by optimizing their data fitting and sparsity regularization terms that are commonly used in brain decomposition models. Moreover, a time-invariant representation learning module is designed to learn features invariant to temporal orders of time points of rsfMRI data. The proposed method has been validated based on a large rsfMRI dataset and experimental results have demonstrated that our method could obtain individual-specific FNs which are consistent with well-established FNs and are informative for predicting brain age, indicating that the individual-specific FNs identified truly captured the underlying variability of individualized functional neuroanatomy.",0
"Deep Learning (DL) has proven extremely powerful for tasks such as image classification and speech recognition, but has been less explored for other domains like neuroscience due to a lack of annotated data required by supervised learning approaches. However, several recent studies have demonstrated how unsupervised DL can learn meaningful neural representations without labeled examples by training autoencoders on fMRI timeseries data. These methods typically rely upon fixed architectures that fit all individuals with identical topology and parameters across subjects, which precludes their use in studying individual differences in neurofunctional networks (NFNs). To solve these issues we propose Brain2Vision, a novel DL framework adapting vision transformer architecture to fMRI signals, achieving strong performance at reconstructing timecourses while substantially reducing computational complexity relative to previous models. Furthermore, our contributions extend beyond model development by introducing novel methodology for uncovering subject specific connections: clustering learned latent features from each layer, followed by graph construction based on pairwise similarity within clusters. Validation results demonstrate superiority over traditional ICA and independent component analyses along with state-of-the art NFN extraction techniques. We believe this work represents important progress towards understanding structural variability across populations and enabling clinicians to personalize treatments. Future directions include incorporating additional patient characteristics into reconstruction loss functions and developing applications leveraging predicted connectomes beyond the initial focus here on psychiatric disorders, where substantial need exists for more diagnostic accuracy and personalization of therapy. Overall, this work establishes foundational knowledge necessary to advance individualized medicine through advanced artificial intelligence technology.",1
"Low-dimension graph embeddings have proved extremely useful in various downstream tasks in large graphs, e.g., link-related content recommendation and node classification tasks, etc. Most existing embedding approaches take nodes as the basic unit for information aggregation, e.g., node perception fields in GNN or con-textual nodes in random walks. The main drawback raised by such node-view is its lack of support for expressing the compound relationships between nodes, which results in the loss of a certain degree of graph information during embedding. To this end, this paper pro-poses PairE(Pair Embedding), a solution to use ""pair"", a higher level unit than a ""node"" as the core for graph embeddings. Accordingly, a multi-self-supervised auto-encoder is designed to fulfill two pretext tasks, to reconstruct the feature distribution for respective pairs and their surrounding context. PairE has three major advantages: 1) Informative, embedding beyond node-view are capable to preserve richer information of the graph; 2) Simple, the solutions provided by PairE are time-saving, storage-efficient, and require the fewer hyper-parameters; 3) High adaptability, with the introduced translator operator to map pair embeddings to the node embeddings, PairE can be effectively used in both the link-based and the node-based graph analysis. Experiment results show that PairE consistently outperforms the state of baselines in all four downstream tasks, especially with significant edges in the link-prediction and multi-label node classification tasks.",0
"In recent years there has been a growing interest in graph representation learning, a field that uses graphs as data structures for machine learning algorithms. Traditionally these methods have relied on supervision from human annotators to train their models. However, obtaining such annotations can be time consuming and expensive. This paper presents a new method called pair-view unsupervised graph representation learning which learns representations by comparing pairs of similar views of an object rather than using explicit labels. Experimental results show that our approach outperforms several popular competitors and we demonstrate how the learned representations improve performance across a range of downstream applications including classification, regression and retrieval tasks. Our work shows that unlabeled data alone may provide sufficient information to learn informative graphical representaions without needing any form of human annotation or intervention",1
"Recent years have witnessed the emergence and flourishing of hierarchical graph pooling neural networks (HGPNNs) which are effective graph representation learning approaches for graph level tasks such as graph classification. However, current HGPNNs do not take full advantage of the graph's intrinsic structures (e.g., community structure). Moreover, the pooling operations in existing HGPNNs are difficult to be interpreted. In this paper, we propose a new interpretable graph pooling framework - CommPOOL, that can capture and preserve the hierarchical community structure of graphs in the graph representation learning process. Specifically, the proposed community pooling mechanism in CommPOOL utilizes an unsupervised approach for capturing the inherent community structure of graphs in an interpretable manner. CommPOOL is a general and flexible framework for hierarchical graph representation learning that can further facilitate various graph-level tasks. Evaluations on five public benchmark datasets and one synthetic dataset demonstrate the superior performance of CommPOOL in graph representation learning for graph classification compared to the state-of-the-art baseline methods, and its effectiveness in capturing and preserving the community structure of graphs.",0
"This paper presents Commpool, a graph pooling framework that enables hierarchical representation learning on graphs by capturing interleaved relationship patterns among connected nodes. It extends the message passing scheme widely used for node embedding methods in static graphs by introducing novel designs tailored for capturing the hierarchy within graphs. We then use these representations as inputs into machine learning models for downstream applications such as image classification and sentiment analysis. Our extensive experiments demonstrate that our proposed method outperforms state-of-the-art graph convolutional networks (GCNs) on five benchmark datasets while using fewer parameters and computational resources. Furthermore, we provide interpretability analyses showing how commpool excels at identifying global structural features in multi-hop substructures across different layers, which corroborate its capability in handling complex graph structures effectively. Overall, commpool provides a scalable yet interpretable solution for large-scale knowledge-rich graphs where human oversight requires insightful understanding beyond accuracy metrics alone.",1
"One of the key factors of enabling machine learning models to comprehend and solve real-world tasks is to leverage multimodal data. Unfortunately, annotation of multimodal data is challenging and expensive. Recently, self-supervised multimodal methods that combine vision and language were proposed to learn multimodal representations without annotation. However, these methods often choose to ignore the presence of high levels of noise and thus yield sub-optimal results. In this work, we show that the problem of noise estimation for multimodal data can be reduced to a multimodal density estimation task. Using multimodal density estimation, we propose a noise estimation building block for multimodal representation learning that is based strictly on the inherent correlation between different modalities. We demonstrate how our noise estimation can be broadly integrated and achieves comparable results to state-of-the-art performance on five different benchmark datasets for two challenging multimodal tasks: Video Question Answering and Text-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic error bound substantiating our empirical results and analyze failure cases. Code: https://github.com/elad-amrani/ssml.",0
"In this paper, we propose a novel method for estimating noise using density estimation in self-supervised multimodal learning. This approach enables us to effectively identify and mitigate sources of noise present in multi-modal data sets, resulting in improved accuracy and robustness of our models. By utilizing state-of-the-art techniques in machine learning, such as deep neural networks and cross-modal matching, we achieve superior performance compared to traditional methods. Our results demonstrate that our proposed approach leads to better generalization ability and more reliable predictions across different modalities. Overall, this work provides valuable insights into the challenges associated with noisy data sets in self-supervised learning and offers a promising direction for future research.",1
"Learning joint embedding space for various modalities is of vital importance for multimodal fusion. Mainstream modality fusion approaches fail to achieve this goal, leaving a modality gap which heavily affects cross-modal fusion. In this paper, we propose a novel adversarial encoder-decoder-classifier framework to learn a modality-invariant embedding space. Since the distributions of various modalities vary in nature, to reduce the modality gap, we translate the distributions of source modalities into that of target modality via their respective encoders using adversarial training. Furthermore, we exert additional constraints on embedding space by introducing reconstruction loss and classification loss. Then we fuse the encoded representations using hierarchical graph neural network which explicitly explores unimodal, bimodal and trimodal interactions in multi-stage. Our method achieves state-of-the-art performance on multiple datasets. Visualization of the learned embeddings suggests that the joint embedding space learned by our method is discriminative. code is available at: \url{https://github.com/TmacMai/ARGF_multimodal_fusion}",0
"Abstract ------------  Recent advances in deep learning have enabled the development of powerful representation learning algorithms that can extract high level features from multimodal data such as image, text, audio, and video streams. However, directly combining these representations into one unified space remains challenging due to their inherent modality differences and variations. In this work, we present an adversarial representation learning approach which utilizes two complementary networks to align the learned features across modalities by enforcing cross entropy minimization on top of the output of the shared classifier. Furthermore, an efficient graph fusion network is proposed to fuse the aligned feature maps from different modalities. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of our approach outperforming several state of the art methods while providing more comprehensible results compared to current black box models.",1
"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.",0
"Here’s a possible abstract that meets your requirements:  The ability to effectively learn from tabular data has numerous applications across fields such as finance, healthcare, and e-commerce. Traditional deep learning approaches have had limited success in addressing the inherent challenges associated with tabular data including missing values, inconsistent formats, and high dimensionality. To overcome these limitations, we propose TabNet—a novel neural network architecture designed specifically for attentively interpreting tabular data. Our approach leverages attention mechanisms to identify key patterns within each table column as well as relationships between columns. We provide extensive analysis demonstrating significant improvements over state-of-the art models on multiple benchmark datasets across diverse evaluation metrics. Additionally, our model enables highly interpretability through explicit interpretation of attention weights. This allows humans to understand how their model makes predictions, providing further confidence in the accuracy and reliability of the system. Overall, our work marks a meaningful step towards enabling practical utilization of deep learning techniques in real world settings where tabular data predominates.",1
"In this work we propose for the first time a transformer-based framework for unsupervised representation learning of multivariate time series. Pre-trained models can be potentially used for downstream tasks such as regression and classification, forecasting and missing value imputation. By evaluating our models on several benchmark datasets for multivariate time series regression and classification, we show that not only does our modeling approach represent the most successful method employing unsupervised learning of multivariate time series presented to date, but also that it exceeds the current state-of-the-art performance of supervised methods; it does so even when the number of training samples is very limited, while offering computational efficiency. Finally, we demonstrate that unsupervised pre-training of our transformer models offers a substantial performance benefit over fully supervised learning, even without leveraging additional unlabeled data, i.e., by reusing the same data samples through the unsupervised objective.",0
"This paper presents a novel framework for representing multivariate time series data using transformers. The proposed approach leverages recent advances in natural language processing (NLP) by applying transformer networks to represent temporal dependencies within each variable of the time series. By doing so, we aim to capture complex relationships that exist across variables over time. Our model addresses several limitations present in traditional methods for handling high dimensional time series such as convolutional neural networks (CNNs). Experiments demonstrate the efficacy of our method on two real-world datasets: one containing stock prices from multiple companies and another capturing energy consumption patterns from households. Results show significant improvement compared to state-of-the-art models based on both quantitative metrics and visual inspection of learned representations. We conclude by discussing potential applications of our approach, including anomaly detection and forecasting tasks.",1
"Floorplans are commonly used to represent the layout of buildings. In computer aided-design (CAD) floorplans are usually represented in the form of hierarchical graph structures. Research works towards computational techniques that facilitate the design process, such as automated analysis and optimization, often use simple floorplan representations that ignore the semantics of the space and do not take into account usage related analytics. We present a floorplan embedding technique that uses an attributed graph to represent the geometric information as well as design semantics and behavioral features of the inhabitants as node and edge attributes. A Long Short-Term Memory (LSTM) Variational Autoencoder (VAE) architecture is proposed and trained to embed attributed graphs as vectors in a continuous space. A user study is conducted to evaluate the coupling of similar floorplans retrieved from the embedding space with respect to a given input (e.g., design layout). The qualitative, quantitative and user-study evaluations show that our embedding framework produces meaningful and accurate vector representations for floorplans. In addition, our proposed model is a generative model. We studied and showcased its effectiveness for generating new floorplans. We also release the dataset that we have constructed and which, for each floorplan, includes the design semantics attributes as well as simulation generated human behavioral features for further study in the community.",0
"This should summarize the problem statement, contributions, related work, methodology and findings of your research. Please provide at least 4 keywords from the article after the last paragraph of the abstract. Thank you!",1
"In this paper, we propose Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks. These two tasks aim at reading and understanding scene text in images for question answering and image caption generation, respectively. In contrast to the conventional vision-language pre-training that fails to capture scene text and its relationship with the visual and text modalities, TAP explicitly incorporates scene text (generated from OCR engines) in pre-training. With three pre-training tasks, including masked language modeling (MLM), image-text (contrastive) matching (ITM), and relative (spatial) position prediction (RPP), TAP effectively helps the model learn a better aligned representation among the three modalities: text word, visual object, and scene text. Due to this aligned representation learning, even pre-trained on the same downstream task dataset, TAP already boosts the absolute accuracy on the TextVQA dataset by +5.4%, compared with a non-TAP baseline. To further improve the performance, we build a large-scale dataset based on the Conceptual Caption dataset, named OCR-CC, which contains 1.4 million scene text-related image-text pairs. Pre-trained on this OCR-CC dataset, our approach outperforms the state of the art by large margins on multiple tasks, i.e., +8.3% accuracy on TextVQA, +8.6% accuracy on ST-VQA, and +10.2 CIDEr score on TextCaps.",0
"This paper introduces Text-Aware Pre-training (TAP), which uses objectives derived from both image grounding tasks such as VQA and text generation tasks such as image captioning. Existing pre-training approaches typically focus on either VQA or image captioning, but TAP leverages complementary insights from both areas by fine-tuning large language models on a diverse set of tasks. We demonstrate that TAP improves performance across four different textual evaluation metrics compared to alternative methods while using fewer annotated examples. Furthermore, our approach achieves state-of-the-art results on several benchmarks and outperforms strong baseline models trained on significantly more data. These results highlight the effectiveness of combining objectives from multiple modalities during pre-training for downstream text-based visual understanding tasks.",1
"Style analysis of artwork in computer vision predominantly focuses on achieving results in target image generation through optimizing understanding of low level style characteristics such as brush strokes. However, fundamentally different techniques are required to computationally understand and control qualities of art which incorporate higher level style characteristics. We study style representations learned by neural network architectures incorporating these higher level characteristics. We find variation in learned style features from incorporating triplets annotated by art historians as supervision for style similarity. Networks leveraging statistical priors or pretrained on photo collections such as ImageNet can also derive useful visual representations of artwork. We align the impact of these expert human knowledge, statistical, and photo realism priors on style representations with art historical research and use these representations to perform zero-shot classification of artists. To facilitate this work, we also present the first large-scale dataset of portraits prepared for computational analysis.",0
"This paper presents an approach to learn portrait style representations from images by leveraging generative adversarial networks (GANs). We introduce new discriminator architectures that explicitly reason about spatial structure within semantic components such as hair, face, etc. By doing so, our method encourages generated images to exhibit consistent structures across different identities while preserving details specific to each individual. Our results demonstrate compelling improvements over state-of-the-art methods on several datasets and tasks, including image generation, attribute translation, and video prediction. We further validate the effectiveness of our method via human evaluations. Overall, our findings suggest that explicit reasoning about spatial structure can significantly improve the quality of learned portrait style representations.",1
"Contrastive self-supervised learning has emerged as a promising approach to unsupervised visual representation learning. In general, these methods learn global (image-level) representations that are invariant to different views (i.e., compositions of data augmentation) of the same image. However, many visual understanding tasks require dense (pixel-level) representations. In this paper, we propose View-Agnostic Dense Representation (VADeR) for unsupervised learning of dense representations. VADeR learns pixelwise representations by forcing local features to remain constant over different viewing conditions. Specifically, this is achieved through pixel-level contrastive learning: matching features (that is, features that describes the same location of the scene on different views) should be close in an embedding space, while non-matching features should be apart. VADeR provides a natural representation for dense prediction tasks and transfers well to downstream tasks. Our method outperforms ImageNet supervised pretraining (and strong unsupervised baselines) in multiple dense prediction tasks.",0
"In recent years, there has been significant progress in developing deep learning models that can learn dense visual representations directly from raw image data without any explicit supervision. These methods have shown impressive results on a variety of tasks such as image classification, object detection, and segmentation. However, little work has been done on understanding how these representations emerge during training, and why they are effective at solving particular vision problems. This paper aims to address some of these questions by presenting a comprehensive analysis of several state-of-the-art unsupervised representation learning algorithms. We examine their behavior under different experimental settings, including variations in model architecture, network depth, input resolution, batch size, and random initialization. Our findings suggest that these methods can indeed capture high-quality visual features that generalize well across tasks and domains. Furthermore, we identify important design principles that contribute to successful unsupervised feature learning, such as data augmentation, regularization, and self-supervision mechanisms. Overall, our study provides valuable insights into the operation of current unsupervised representation learning techniques and offers guidelines for future research directions in this exciting area of computer vision.",1
"The artistic style of a painting is a rich descriptor that reveals both visual and deep intrinsic knowledge about how an artist uniquely portrays and expresses their creative vision. Accurate categorization of paintings across different artistic movements and styles is critical for large-scale indexing of art databases. However, the automatic extraction and recognition of these highly dense artistic features has received little to no attention in the field of computer vision research. In this paper, we investigate the use of deep self-supervised learning methods to solve the problem of recognizing complex artistic styles with high intra-class and low inter-class variation. Further, we outperform existing approaches by almost 20% on a highly class imbalanced WikiArt dataset with 27 art categories. To achieve this, we train the EnAET semi-supervised learning model (Wang et al., 2019) with limited annotated data samples and supplement it with self-supervised representations learned from an ensemble of spatial and non-spatial transformations.",0
"This paper proposes an approach to art style classification that leverages self-supervised learning through an ensemble of autoencoders applied as feature extractors on a small subset of labeled images. We demonstrate that our method leads to improved accuracy compared to state-of-the-art methods across several datasets without requiring large amounts of training data. Our proposed solution is motivated by recent success in self-supervised representation learning, where pretext tasks such as image rotation have been used to learn representations in a variety of visual recognition problems. In contrast, we use an ensemble of autoencoding transformations as the pretext task because these types of models can learn more discriminative features than simple linear projections commonly used in other approaches. Additionally, rather than using fixed architectures or random initialization for these models, we train them to reconstruct their inputs over multiple epochs so they can learn increasingly powerful representations. These learned representations are then used to classify art styles at inference time. Experimental results show that our proposed method outperforms competing baselines, even those trained with ten times as many examples, and demonstrate the general applicability of the framework across different datasets and architectures. Overall, we believe this work demonstrates the effectiveness of self-supervised learning techniques for fine-grained visual classification tasks and paves the way towards scalable solutions that leverage vast unlabeled databases to improve performance on smaller, manually annotated datasets.",1
"Value Iteration Networks (VINs) have emerged as a popular method to incorporate planning algorithms within deep reinforcement learning, enabling performance improvements on tasks requiring long-range reasoning and understanding of environment dynamics. This came with several limitations, however: the model is not incentivised in any way to perform meaningful planning computations, the underlying state space is assumed to be discrete, and the Markov decision process (MDP) is assumed fixed and known. We propose eXecuted Latent Value Iteration Networks (XLVINs), which combine recent developments across contrastive self-supervised learning, graph representation learning and neural algorithmic reasoning to alleviate all of the above limitations, successfully deploying VIN-style models on generic environments. XLVINs match the performance of VIN-like models when the underlying MDP is discrete, fixed and known, and provides significant improvements to model-free baselines across three general MDP setups.",0
"This paper presents a new machine learning algorithm called XLVIN (eXecuted Latent Value Iteration Nets). XLVIN is a novel approach that leverages deep neural networks to model high-dimensional distributions over latent variables. Our method builds on recent advances in variational inference and introduces several key innovations such as stochastic gradient descent updates during training, which allow us to scale up computationally while preserving good predictive performance. Extensive experiments demonstrate the effectiveness of our method across several benchmark datasets including MNIST, CIFAR-10, SVHN, and Street View House Numbers. We show that our approach achieves state-of-the-art results in terms of both accuracy and speed compared to other popular latent variable models like VAEs and GANs. Overall, XLVIN represents an important contribution towards building powerful latent variable models that can effectively capture complex data distributions and perform well under challenging real-world settings.",1
"We create a framework for bootstrapping visual representation learning from a primitive visual grouping capability. We operationalize grouping via a contour detector that partitions an image into regions, followed by merging of those regions into a tree hierarchy. A small supervised dataset suffices for training this grouping primitive. Across a large unlabeled dataset, we apply this learned primitive to automatically predict hierarchical region structure. These predictions serve as guidance for self-supervised contrastive feature learning: we task a deep network with producing per-pixel embeddings whose pairwise distances respect the region hierarchy. Experiments demonstrate that our approach can serve as state-of-the-art generic pre-training, benefiting downstream tasks. We additionally explore applications to semantic region search and video-based object instance tracking.",0
"Self-supervised learning has emerged as a powerful tool for training deep neural networks on large amounts of unlabeled data. In many cases, self-supervision can achieve results that are competitive with supervised methods, which require costly human annotation efforts. However, most existing self-supervised approaches focus only on local patterns or pixel differences within images. This work presents a new method called ""Self-Supervised Visual Representation Learning from Hierarchical Grouping"" (SSVR), which learns global representations by grouping similar objects into clusters at different levels of abstraction. Unlike traditional clustering methods that rely heavily on manual feature engineering, SSVR uses a convolutional network to extract features automatically and groups them based on similarity. Experiments show that our approach outperforms previous state-of-the-art self-supervised representation learning methods across multiple benchmarks such as image classification, object detection, semantic segmentation, and few-shot learning. Additionally, we demonstrate how our learned representations generalize well to real-world applications like person re-identification. Our research provides insights into understanding hierarchical relationships among diverse visual concepts and their correspondences in the high-dimensional space of deep features. These findings pave the way towards developing more advanced visual processing systems driven solely by large collections of raw sensory input without any explicit labels.",1
"Self-supervised learning is currently gaining a lot of attention, as it allows neural networks to learn robust representations from large quantities of unlabeled data. Additionally, multi-task learning can further improve representation learning by training networks simultaneously on related tasks, leading to significant performance improvements. In this paper, we propose three novel self-supervised auxiliary tasks to train graph-based neural network models in a multi-task fashion. Since Graph Convolutional Networks are among the most promising approaches for capturing relationships among structured data points, we use them as a building block to achieve competitive results on standard semi-supervised graph classification tasks.",0
"In recent years, graph neural networks (GNN) have emerged as powerful models for handling complex data structures such as graphs, where each node corresponds to an entity, and edges represent their relationships. Despite significant progress in GNN development, there remains room for improvement in terms of efficiency, scalability, and performance on outlier detection tasks. To address these limitations, we propose an approach that utilizes multiple self-supervised auxiliary tasks within a unified framework, enabling more effective feature learning and generalization across different domains. Our method builds upon existing state-of-the-art GNN architectures by introducing three types of self-supervision strategies - context prediction, structure reconstruction, and random walks, which can help improve model robustness under different training conditions. Furthermore, we demonstrate how our framework effectively integrates both task-specific supervisory signals and auxiliary self-supervised objectives into a single architecture without compromising accuracy. Experimental results from several benchmark datasets show that our proposed method achieves competitive performance compared to other leading methods while maintaining high computational efficiency and flexibility. Our work underscores the importance of incorporating multi-task learning frameworks for GNN models, providing promising directions for future research in graph representation learning. Keywords: Graph neural networks (GNN), Multi-task learning, Self-supervised learning, Auxiliary tasks",1
"Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies either at the image or the feature level improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing the memory size, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.",0
"Introduction: Deep neural networks have achieved state-of-the-art performance on many tasks due to their ability to learn complex representations from large amounts of data. One popular method used to train these models is contrastive learning, where the model learns to distinguish positive pairs (e.g., two sides of the same image) from negative pairs (e.g., two random images). However, existing methods often suffer from poor negatives, which can lead to suboptimal representations and reduced accuracy. In this work, we propose Hard Negative Mixing (HNM), a simple yet effective technique that improves the quality of negative samples during training by explicitly mixing hard negatives into the batch. HNM encourages the model to focus more on difficult examples, resulting in better generalization and improved performance on downstream tasks. Methods: We first introduce a simple algorithm to generate hard negatives based on the current minibatch using online ranking strategies like MOCO. Next, we replace some of the original easy negatives with our newly generated hard negatives following a schedule that gradually increases the number of hard negatives mixed into each batch over time. We evaluate HNM across multiple datasets and benchmarks, including ImageNet, CIFAR-10/100, and COCO, and compare it against state-of-the-art baselines using both pre-training followed by finetuning as well as zero-shot transfer. Results: Our experiments demonstrate that adding carefully chosen hard negatives to each batch significantly boosts performance compared to other strong baseline algorithms without any changes to architectures or hyperparameters. On ImageNet, HNM achieves a top-1 validation error of 27.9%, surpassing all previous published results under the same settings, including full fine-tuning from scratch. Moreover, HNM consistently outperforms competitors under different resource constraints such as fewer epochs or fewer parameters. Discussion: By introducing hard negatives into the mix, our approach forces the network to generalize better t",1
"Deep model-based Reinforcement Learning (RL) has the potential to substantially improve the sample-efficiency of deep RL. While various challenges have long held it back, a number of papers have recently come out reporting success with deep model-based methods. This is a great development, but the lack of a consistent metric to evaluate such methods makes it difficult to compare various approaches. For example, the common single-task sample-efficiency metric conflates improvements due to model-based learning with various other aspects, such as representation learning, making it difficult to assess true progress on model-based RL. To address this, we introduce an experimental setup to evaluate model-based behavior of RL methods, inspired by work from neuroscience on detecting model-based behavior in humans and animals. Our metric based on this setup, the Local Change Adaptation (LoCA) regret, measures how quickly an RL method adapts to a local change in the environment. Our metric can identify model-based behavior, even if the method uses a poor representation and provides insight in how close a method's behavior is from optimal model-based behavior. We use our setup to evaluate the model-based behavior of MuZero on a variation of the classic Mountain Car task.",0
"Title your Abstract. Your abstract should contain at least one sentence that summarizes the content of each section below. Each summary sentence is bolded in the text. Title: An Overview of Computer Vision Applications in Industry Computer vision has become increasingly important over the past few decades due to advances in technology and the availability of large amounts of data. This field uses algorithms and machine learning techniques to extract meaningful insights from visual input such as images and videos. In industry, computer vision applications have been adopted across different domains including healthcare, manufacturing, retail, finance, among others. These applications include object detection, segmentation, recognition, tracking, image processing, 3D reconstruction, visual inspection, among other tasks. Moreover, computer vision systems can provide real-time monitoring, analysis, alerts and automation capabilities which can increase efficiency and reduce costs for businesses. Some examples of specific use cases are quality control in manufacturing where defects in products can be detected automatically using cameras, face verification used by financial institutions for secure authentication of customers or medical imaging analysis for diagnosis support and drug discovery. This article provides an overview of how computer vision is transforming industries through innovative applications.",1
"We study a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). We prove that under standard assumptions, our proposed algorithm achieves $\tilde{O}(\sqrt{T})$ finite-time regret, where $T$ is the learning time horizon. Compared with existing neural contextual bandit algorithms, our approach is computationally much more efficient since it only needs to explore in the last layer of the deep neural network.",0
"Here is my attempt at writing an abstract: This paper presents a new algorithm called neural contextual bandits that combines deep representation learning with shallow exploration. The proposed method leverages recent advances in neural network architectures to learn low dimensional representations of both user preferences and item attributes. These representations capture complex relationships between users and items, enabling accurate predictions of which actions lead to higher rewards. Furthermore, our algorithm uses a novel form of gradient descent based on trust region optimization to balance exploitation (exploiting knowledge already learned) and exploration (exploring uncertainty regions to improve understanding). Our approach significantly outperforms state-of-the-art methods on several benchmark datasets, demonstrating the effectiveness of combining deep representations with efficient exploration algorithms. Finally, we provide insights into how neural contextual bandits achieve good performance by analyzing the learned representations during online interactions with users. Overall, our contributions enable practitioners to design more effective recommendation systems that leverage powerful machine learning models while respecting limited resources and privacy concerns.",1
"Variational mutual information (MI) estimators are widely used in unsupervised representation learning methods such as contrastive predictive coding (CPC). A lower bound on MI can be obtained from a multi-class classification problem, where a critic attempts to distinguish a positive sample drawn from the underlying joint distribution from $(m-1)$ negative samples drawn from a suitable proposal distribution. Using this approach, MI estimates are bounded above by $\log m$, and could thus severely underestimate unless $m$ is very large. To overcome this limitation, we introduce a novel estimator based on a multi-label classification problem, where the critic needs to jointly identify multiple positive samples at the same time. We show that using the same amount of negative samples, multi-label CPC is able to exceed the $\log m$ bound, while still being a valid lower bound of mutual information. We demonstrate that the proposed approach is able to lead to better mutual information estimation, gain empirical improvements in unsupervised representation learning, and beat a current state-of-the-art knowledge distillation method over 10 out of 13 tasks.",0
"Here's a potential abstract:  Contrastive predictive coding (CPC) has emerged as a powerful framework for unsupervised learning in computer vision tasks such as image generation and representation learning. However, most existing work on CPC focuses on single-label tasks where each instance belongs to one class only. In practice, many real-world problems involve multi-label settings, where instances can belong to multiple classes simultaneously. In this work, we propose a novel method called Multi-label Contrastive Predictive Coding (MCPC), which extends traditional CPC to handle these multi-label scenarios. Our approach leverages contrastive learning principles to efficiently learn representations that capture both label co-occurrences and interdependencies across different labels. Experiments conducted on several benchmark datasets show that our proposed MCPC achieves state-of-the-art performance compared to prior arts. We further investigate the factors affecting model performance through ablation studies and visualization analysis, demonstrating MCPC's robustness and effectiveness in handling complex multi-label data distributions. This research paves the way towards more advanced applications of CPC methods in high-dimensional spaces beyond images, such as natural language processing and graph representation learning.",1
"There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures. However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well. To this end, we propose Dy- ERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data. Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs. Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp. We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks. Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.",0
"In today’s world, knowledge graphs have become increasingly important as a means of organizing and storing large amounts of data. As such, researchers have sought ways to enhance their effectiveness through embedding techniques that provide vector representations of entities in these graphs. One promising approach is based on the use of dynamic embeddings, which allow for more flexible modeling of temporal changes in complex systems by adaptively evolving the parameters used to represent entities over time. However, existing methods suffer from limitations related to computational complexity and scalability, making them impractical for real-world applications. To address these issues, we propose a novel method called DyERNIE, which builds upon recent advances in Riemannian manifold learning. Our algorithm leverages key insights into the geometric structure underlying knowledge graph evolution, enabling efficient computation of dynamically changing embeddings while accounting for both static and transient relationships among entities. Extensive evaluation using five benchmark datasets demonstrates the superiority of our approach in terms of accuracy, efficiency, and robustness compared to state-of-the-art alternatives. These findings have significant implications for knowledge representation, machine learning, and natural language processing communities seeking to develop more effective techniques for handling time-varying information in knowledge bases.",1
"We present BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. Current work on scene representation learning either ignores scene background or treats the whole scene as one object. Meanwhile, work that considers scene compositionality treats scene objects only as image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we design BlockGAN to learn to first generate 3D features of background and foreground objects, then combine them into 3D features for the wholes cene, and finally render them into realistic images. This allows BlockGAN to reason over occlusion and interaction between objects' appearance, such as shadow and lighting, and provides control over each object's 3D pose and identity, while maintaining image realism. BlockGAN is trained end-to-end, using only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments show that using explicit 3D features to represent objects allows BlockGAN to learn disentangled representations both in terms of objects (foreground and background) and their properties (pose and identity).",0
"This paper presents BlockGAN, a novel generative adversarial network architecture that learns object-aware scene representations from unlabeled images. Inspired by recent advances in image generation using GANs, we introduce the concept of object-awareness into the discriminator. We design a new loss function that explicitly promotes object-awareness by encouraging the generator to generate diverse yet coherent scenes containing meaningful objects. Experiments on various datasets show that our method significantly improves state-of-the-art performance across multiple metrics including visual quality, diversity, and object awareness. Overall, our approach represents a significant step towards generating photo-realistic images that capture complex real-world scenes with fine-grained details such as objects and their contextual relationships. Our code and pretrained models will be made available at https://github.com/tensorflow/blockgan.",1
"Contrastive representation learning has been recently proved to be very efficient for self-supervised training. These methods have been successfully used to train encoders which perform comparably to supervised training on downstream classification tasks. A few works have started to build a theoretical framework around contrastive learning in which guarantees for its performance can be proven. We provide extensions of these results to training with multiple negative samples and for multiway classification. Furthermore, we provide convergence guarantees for the minimization of the contrastive training error with gradient descent of an overparametrized deep neural encoder, and provide some numerical experiments that complement our theoretical findings",0
"This paper investigates the use of contrastive unsupervised representation learning for classification tasks. In recent years, there has been growing interest in developing methods that can learn robust representations from large amounts of data without relying on explicit supervision. Contrastive representation learning approaches have shown promising results by leveraging self-supervised training objectives that maximize agreement between augmented views of the same data instance while minimizing similarity between views of different instances.  The main contributions of this work are twofold: Firstly, we present a comprehensive review of existing contrastive representation learning techniques used in computer vision and natural language processing domains. We provide a detailed analysis of these methods, highlighting their strengths and weaknesses, as well as their applications in downstream classification tasks. Secondly, we empirically evaluate several state-of-the-art contrastive representation learning models using benchmark datasets commonly employed in image classification and text categorization problems. Our experiments show that certain variants of contrastive representation learning algorithms outperform traditional pre-training methods, suggesting that such techniques may play a significant role in enhancing the performance of machine learning systems in various application areas.  Furthermore, our study sheds light on the importance of properly designing and selecting appropriate objective functions during the pre-training phase. We demonstrate that specific choices regarding the formulation of the contrastive loss function can significantly impact both the efficiency and effectiveness of the learned representations. Finally, we discuss open challenges related to understanding the optimization process underlying contrastive representation learning, which remain largely unexplored at this time.  In summary, this paper presents valuable insights into how contrastive unsupervised representation learning can enhance the performance of classi",1
"Self-supervised representation learning has seen remarkable progress in the last few years. More recently, contrastive instance learning has shown impressive results compared to its supervised learning counterparts. However, even with the ever increased interest in contrastive instance learning, it is still largely unclear why these methods work so well. In this paper, we aim to unravel some of the mysteries behind their success, which are the good practices. Through an extensive empirical analysis, we hope to not only provide insights but also lay out a set of best practices that led to the success of recent work in self-supervised representation learning.",0
"Title: ""Toward Good Practices in Self-Supervised Representation Learning"" Authors: [Your Name(s)]  Abstract: This paper presents insights into the emerging field of self-supervised representation learning, which has recently gained significant interest due to its ability to learn meaningful representations without labeled data. We provide guidelines on good practices that researchers can follow to ensure high-quality work in this domain. These practices include defining clear objectives, evaluating performance effectively, exploring different pretext tasks, addressing the limitations of current methods, and understanding the role of human feedback. By adhering to these principles, we aim to promote rigor and reproducibility in self-supervised representation learning research, ultimately leading to more reliable results and better applications of this technology. Overall, our goal is to support the advancement of machine learning by fostering responsible scientific inquiry within the self-supervised community.",1
"Qualitative analysis of verbal data is of central importance in the learning sciences. It is labor-intensive and time-consuming, however, which limits the amount of data researchers can include in studies. This work is a step towards building a statistical machine learning (ML) method for achieving an automated support for qualitative analyses of students' writing, here specifically in score laboratory reports in introductory biology for sophistication of argumentation and reasoning. We start with a set of lab reports from an undergraduate biology course, scored by a four-level scheme that considers the complexity of argument structure, the scope of evidence, and the care and nuance of conclusions. Using this set of labeled data, we show that a popular natural language modeling processing pipeline, namely vector representation of words, a.k.a word embeddings, followed by Long Short Term Memory (LSTM) model for capturing language generation as a state-space model, is able to quantitatively capture the scoring, with a high Quadratic Weighted Kappa (QWK) prediction score, when trained in via a novel contrastive learning set-up. We show that the ML algorithm approached the inter-rater reliability of human analysis. Ultimately, we conclude, that machine learning (ML) for natural language processing (NLP) holds promise for assisting learning sciences researchers in conducting qualitative studies at much larger scales than is currently possible.",0
"This paper presents a novel approach for automatic coding of students' written work using contrastive representation learning in the Wasserstein space. We propose a deep neural network architecture that leverages large amounts of unlabeled data to learn representations of student texts, which can then be used to predict course codes accurately and efficiently. Our method significantly outperforms existing baseline models on several benchmark datasets, demonstrating the effectiveness of our proposed approach. Furthermore, we show that our model generalizes well across different courses and domains, making it applicable in real-world educational settings. Overall, our findings have important implications for automated assessment of students' written work, highlighting the potential benefits of incorporating state-of-the-art machine learning techniques into education.",1
"High dimensional data analysis for exploration and discovery includes three fundamental tasks: dimensionality reduction, clustering, and visualization. When the three associated tasks are done separately, as is often the case thus far, inconsistencies can occur among the tasks in terms of data geometry and others. This can lead to confusing or misleading data interpretation. In this paper, we propose a novel neural network-based method, called Consistent Representation Learning (CRL), to accomplish the three associated tasks end-to-end and improve the consistencies. The CRL network consists of two nonlinear dimensionality reduction (NLDR) transformations: (1) one from the input data space to the latent feature space for clustering, and (2) the other from the clustering space to the final 2D or 3D space for visualization. Importantly, the two NLDR transformations are performed to best satisfy local geometry preserving (LGP) constraints across the spaces or network layers, to improve data consistencies along with the processing flow. Also, we propose a novel metric, clustering-visualization inconsistency (CVI), for evaluating the inconsistencies. Extensive comparative results show that the proposed CRL neural network method outperforms the popular t-SNE and UMAP-based and other contemporary clustering and visualization algorithms in terms of evaluation metrics and visualization.",0
"This abstract is for a paper that presents a new method called Consistent Representation Learning (CRL) for analyzing high dimensional data sets. CRL is designed to overcome some of the limitations of existing methods by providing a more consistent and accurate representation of the underlying structure in these types of datasets. By utilizing techniques such as clustering and principal component analysis, CRL can effectively identify patterns in large scale data and provide meaningful insights into complex phenomena. Furthermore, CRL provides novel tools for visualization and interpretation of results which facilitate knowledge discovery and decision making processes. Overall, CRL offers a powerful toolset for practitioners working on big data problems across a wide range of domains including finance, healthcare, social sciences etc.",1
"The performance of generative zero-shot methods mainly depends on the quality of generated features and how well the model facilitates knowledge transfer between visual and semantic domains. The quality of generated features is a direct consequence of the ability of the model to capture the several modes of the underlying data distribution. To address these issues, we propose a new two-level joint maximization idea to augment the generative network with an inference network during training which helps our model capture the several modes of the data and generate features that better represent the underlying data distribution. This provides strong cross-modal interaction for effective transfer of knowledge between visual and semantic domains. Furthermore, existing methods train the zero-shot classifier either on generate synthetic image features or latent embeddings produced by leveraging representation learning. In this work, we unify these paradigms into a single model which in addition to synthesizing image features, also utilizes the representation learning capabilities of the inference network to provide discriminative features for the final zero-shot recognition task. We evaluate our approach on four benchmark datasets i.e. CUB, FLO, AWA1 and AWA2 against several state-of-the-art methods, and show its performance. We also perform ablation studies to analyze and understand our method more carefully for the Generalized Zero-shot Learning task.",0
"In recent years, zero-shot learning has emerged as a promising approach for addressing the challenge of handling new classes at test time that were never seen during training. However, existing methods still suffer from limited performance on real-world datasets due to their reliance on weak semantic representations or simple nearest neighbor retrieval based on attribute vectors. To overcome these limitations, we propose a two-level adversarial visual-semantic coupling method for generalized zero-shot learning. Our approach first learns discriminative features using deep convolutional neural networks (CNNs) to obtain powerful image representations that capture both visual characteristics and high-level semantics. Then, a novel adversarial training schema is introduced at each level (visual and semantic) to couple them effectively. By doing so, our model can generate more accurate and robust feature embeddings that facilitate precise prediction even when dealing with unseen concepts at inference time. Experimental results demonstrate significant improvements over state-of-the-art techniques across several benchmark datasets, validating the effectiveness and generalizability of our proposed framework.",1
"Estimating mutual information between continuous random variables is often intractable and extremely challenging for high-dimensional data. Recent progress has leveraged neural networks to optimize variational lower bounds on mutual information. Although showing promise for this difficult problem, the variational methods have been theoretically and empirically proven to have serious statistical limitations: 1) many methods struggle to produce accurate estimates when the underlying mutual information is either low or high; 2) the resulting estimators may suffer from high variance. Our approach is based on training a classifier that provides the probability that a data sample pair is drawn from the joint distribution rather than from the product of its marginal distributions. Moreover, we establish a direct connection between mutual information and the average log odds estimate produced by the classifier on a test set, leading to a simple and accurate estimator of mutual information. We show theoretically that our method and other variational approaches are equivalent when they achieve their optimum, while our method sidesteps the variational bound. Empirical results demonstrate high accuracy of our approach and the advantages of our estimator in the context of representation learning. Our demo is available at https://github.com/RayRuizhiLiao/demi_mi_estimator.",0
"Title: ""Discriminative Estimation of Mutual Information""  In this work, we present an innovative framework called DEMI (Discriminative Estimator of Mutual Information) for estimating mutual information accurately and efficiently from complex datasets. Mutual information plays a crucial role in many fields such as machine learning, data mining, signal processing, neuroscience, bioinformatics, and quantum computing, where understanding dependencies and relationships among variables is essential. However, accurate estimation of mutual information has remained challenging due to several reasons like noise, high dimensionality, nonlinearity, sparseness, and lack of robustness against outliers. Existing methods either suffer from computational complexity, scaling issues, sensitivity to parameter tuning, or limited applicability.  To address these limitations, our proposed approach leverages recent advances in discriminative modeling and deep neural networks. We formulate the problem as binary classification by treating each variable as a query attribute and the target variable as the label. Our framework learns a feature extractor that maximizes the separation margin between positive (related) and negative (unrelated) pairs while minimizing the variance across different instantiations of the same pair. This leads to efficient estimation of mutual information with improved accuracy, robustness, scalability, interpretability, and adaptivity over existing methods.  We extensively evaluate DEMI on both synthetic and real benchmarks consisting of diverse domains, features, architectures, loss functions, regularizers, and distributions. Our results demonstrate that DEMI consistently achieves significantly better performance than state-of-the-art baselines across various metrics, settings, and use cases. Moreover, DEMI enjoys favorable properties such as permutation equivariance, computation-communication tradeoff, gradient accessibility, and online capability. These benefits enable novel applications like transfer learning, active inference, interpretable prediction, generative modeling, controllab",1
"Deep neural nets typically perform end-to-end backpropagation to learn the weights, a procedure that creates synchronization constraints in the weight update step across layers and is not biologically plausible. Recent advances in unsupervised contrastive representation learning point to the question of whether a learning algorithm can also be made local, that is, the updates of lower layers do not directly depend on the computation of upper layers. While Greedy InfoMax separately learns each block with a local objective, we found that it consistently hurts readout accuracy in state-of-the-art unsupervised contrastive learning algorithms, possibly due to the greedy objective as well as gradient isolation. In this work, we discover that by overlapping local blocks stacking on top of each other, we effectively increase the decoder depth and allow upper blocks to implicitly send feedbacks to lower blocks. This simple design closes the performance gap between local learning and end-to-end contrastive learning algorithms for the first time. Aside from standard ImageNet experiments, we also show results on complex downstream tasks such as object detection and instance segmentation directly using readout features.",0
"Introduction to deep learning often begin with introducing linear models, such as logistic regression (classification) and Lasso (regression). However, these models cannot capture nonlinear relationships among features which makes them ill-equipped for complex real world applications like image classification. To tackle this problem, researchers have proposed methods that allow neural networks to implicitly learn rich representations by utilizing the structure within their datasets – contrastive learning. In this work, we investigate using contrastive learning to improve the performance of deep neural network on image classification tasks, specifically by applying local contrasts instead of global ones. Our method is based on the recent state of the art approach called SimCLRv2 introduced by Chen et al., 2020 and use ResNet architecture with variants from Touvron et al., 2020 . We evaluate our approach on three benchmark datasets - CIFAR10/100 , SVHN and COCO Stuff dataset , compare the results with current state of the art baselines trained via other techniques like Prototypical Networks and Minimax Concave Propagation, and provide detailed analysis showing improvements obtained through our method. Additionally we also conduct ablation studies to analyze impact of different components involved in training process, and hyperparameters used therein. We also experimented with some unconventional design choices like data augmentations, mixup, normalization techniques etc. Finally, present future directions with regard to extension of LoCo model into other types of problems including more challenging vision subtasks like object detection, semantic segmentation amongst others. Our experiments demonstrate the effectiveness of Lo Co on multiple standard datasets thereby suggesting potential benefits towards wider community of computer visi",1
"Fair representation learning provides an effective way of enforcing fairness constraints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the first method that enables data consumers to obtain certificates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness. That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at $\ell_\infty$-distance at most $\epsilon$, thus allowing data consumers to certify individual fairness by proving $\epsilon$-robustness of their classifier. Our experimental evaluation on five real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach.",0
"Recent developments have shown that large language models trained on task-specific objectives can generate representations that are provably near-optimal under some theoretical metrics like Jensen-Shannon divergence [CLJL21]. However, these representations may be biased towards some groups over others depending on how they were collected and preprocessed. This motivates our investigation into learning certifiably individually fair representations from text data. While existing work has investigated individual fairness via postprocessing after representation learning [ADFH09], we investigate individual fairness during the representation learning process itself by adapting methods based on adversarial training used previously for ensuring group fairness [LZYD20b, ZMYR21] and multi-task learning from multiple tasks [BMNWX08]. We evaluate our approach on several benchmark datasets such as IMDb [IMDB], Yelp [YELP], Wikipedia [WIKI], Stack Exchange [SE], Common Crawl BookCorpus [CCBC], and Open Assistant. We show consistent improvements over strong baselines both quantitatively (e.g., using PACV) and qualitatively (via human evaluations). Finally, we provide analyses regarding how different tradeoffs affect performance across different groups. Our findings suggest ways forward for improving individual fairness in natural language processing systems without sacrificing utility.",1
"Canine mammary carcinoma (CMC) has been used as a model to investigate the pathogenesis of human breast cancer and the same grading scheme is commonly used to assess tumor malignancy in both. One key component of this grading scheme is the density of mitotic figures (MF). Current publicly available datasets on human breast cancer only provide annotations for small subsets of whole slide images (WSIs). We present a novel dataset of 21 WSIs of CMC completely annotated for MF. For this, a pathologist screened all WSIs for potential MF and structures with a similar appearance. A second expert blindly assigned labels, and for non-matching labels, a third expert assigned the final labels. Additionally, we used machine learning to identify previously undetected MF. Finally, we performed representation learning and two-dimensional projection to further increase the consistency of the annotations. Our dataset consists of 13,907 MF and 36,379 hard negatives. We achieved a mean F1-score of 0.791 on the test set and of up to 0.696 on a human breast cancer dataset.",0
"Introduction: Cancer is one of the leading causes of deaths worldwide in both humans and animals such as dogs. As companion pets continue to age they develop many diseases including spontaneous tumors which closely mimic human disease states. These similarities have led us to believe that using these naturally occurring tumors in pet populations may lead to advances in novel therapeutics to treat human patients suffering from these conditions. We aimed to collect data on all aspects of the diagnosis of cancer in dogs but specifically focused our attention on mammary neoplasia due to the prevalence in the dog population (25% affected) and similarity in gross appearance to human invasive ductal carcinoma. Our goal was to create an open source image database allowing scientists to perform machine learning studies for cancer classification that could then translate to human diagnostic techniques. Methodology/Results: We collected whole slide images (WSI’s) over four years at a single veterinary school hospital, University of California Davis Veterinary Center. All samples were reviewed by two expert board certified veterinary pathologists and given consensus opinion regarding histological subtype based on modified Scarlett system. Each WSIs were imaged using standard brightfield microscopy equipment without any specialized stains or modifications to usual clinical technique resulting in generation of nearly 800 WSIs representing the complete spectrum of mammary neoplasia seen in dogs. To make this publicly available we uploaded each annotated slide to a cloud storage server accessible via link within the body of manuscript where individual may download TIF files for offline use after publication. In addition, we published on line supplementary material containing additional annotations made during training and testing phases of deep learni",1
"Feature-based transfer is one of the most effective methodologies for transfer learning. Existing studies usually assume that the learned new feature representation is truly \emph{domain-invariant}, and thus directly train a transfer model $\mathcal{M}$ on source domain. In this paper, we consider a more realistic scenario where the new feature representation is suboptimal and small divergence still exists across domains. We propose a new learning strategy with a transfer model called Randomized Transferable Machine (RTM). More specifically, we work on source data with the new feature representation learned from existing feature-based transfer methods. The key idea is to enlarge source training data populations by randomly corrupting source data using some noises, and then train a transfer model $\widetilde{\mathcal{M}}$ that performs well on all the corrupted source data populations. In principle, the more corruptions are made, the higher the probability of the target data can be covered by the constructed source populations, and thus better transfer performance can be achieved by $\widetilde{\mathcal{M}}$. An ideal case is with infinite corruptions, which however is infeasible in reality. We develop a marginalized solution with linear regression model and dropout noise. With a marginalization trick, we can train an RTM that is equivalently to training using infinite source noisy populations without truly conducting any corruption. More importantly, such an RTM has a closed-form solution, which enables very fast and efficient training. Extensive experiments on various real-world transfer tasks show that RTM is a promising transfer model.",0
"This paper proposes a new machine learning algorithm called ""Randomized Transferable Machine"" (RTM), which utilizes transfer learning principles and randomization techniques to improve generalizability and flexibility compared to traditional deep learning methods. RTM leverages pre-trained models as a starting point, then fine-tunes them using randomized data augmentation strategies during training, leading to better performance on unseen datasets. Additionally, the RTM framework enables easy integration of multiple tasks, allowing for efficient use of resources and improved overall model quality. Empirical evaluations demonstrate that RTM outperforms state-of-the-art baselines across several benchmarking datasets. Overall, RTM represents a novel approach to deep learning that offers promising advancements towards more effective artificial intelligence solutions.",1
"Recent advances in unsupervised representation learning have experienced remarkable progress, especially with the achievements of contrastive learning, which regards each image as well its augmentations as a separate class, while does not consider the semantic similarity among images. This paper proposes a new kind of data augmentation, named Center-wise Local Image Mixture, to expand the neighborhood space of an image. CLIM encourages both local similarity and global aggregation while pulling similar images. This is achieved by searching local similar samples of an image, and only selecting images that are closer to the corresponding cluster center, which we denote as center-wise local selection. As a result, similar representations are progressively approaching the clusters, while do not break the local similarity. Furthermore, image mixture is used as a smoothing regularization to avoid overconfidence on the selected samples. Besides, we introduce multi-resolution augmentation, which enables the representation to be scale invariant. Integrating the two augmentations produces better feature representation on several unsupervised benchmarks. Notably, we reach 75.5% top-1 accuracy with linear evaluation over ResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels, as well as consistently outperforming supervised pretraining on several downstream transfer tasks.",0
"In order to fully comprehend contrastive representation learning, it’s essential to recognize that traditional machine learning frameworks rely on i",1
"Self-supervised learning achieves superior performance in many domains by extracting useful representations from the unlabeled data. However, most of traditional self-supervised methods mainly focus on exploring the inter-sample structure while less efforts have been concentrated on the underlying intra-temporal structure, which is important for time series data. In this paper, we present SelfTime: a general self-supervised time series representation learning framework, by exploring the inter-sample relation and intra-temporal relation of time series to learn the underlying structure feature on the unlabeled time series. Specifically, we first generate the inter-sample relation by sampling positive and negative samples of a given anchor sample, and intra-temporal relation by sampling time pieces from this anchor. Then, based on the sampled relation, a shared feature extraction backbone combined with two separate relation reasoning heads are employed to quantify the relationships of the sample pairs for inter-sample relation reasoning, and the relationships of the time piece pairs for intra-temporal relation reasoning, respectively. Finally, the useful representations of time series are extracted from the backbone under the supervision of relation reasoning heads. Experimental results on multiple real-world time series datasets for time series classification task demonstrate the effectiveness of the proposed method. Code and data are publicly available at https://haoyfan.github.io/.",0
"Abstraction: This paper presents a novel approach to self-supervised time series representation learning through inter-intra relational reasoning. We propose a method that learns representations from raw input data without any labeled examples, relying instead on the structural relationships within and across sequences. Our model builds upon recent advancements in deep learning architectures, specifically transformers, which have proven highly effective in natural language processing tasks. Our experiments demonstrate the effectiveness of our proposed technique, achieving state-of-the-art performance on benchmark datasets commonly used to evaluate temporal sequence models. Overall, we believe our work represents an important step forward in unlocking new possibilities for automating complex real-world data analysis problems that rely on time series understanding and prediction capabilities.",1
"Recent unsupervised contrastive representation learning follows a Single Instance Multi-view (SIM) paradigm where positive pairs are usually constructed with intra-image data augmentation. In this paper, we propose an effective approach called Beyond Single Instance Multi-view (BSIM). Specifically, we impose more accurate instance discrimination capability by measuring the joint similarity between two randomly sampled instances and their mixture, namely spurious-positive pairs. We believe that learning joint similarity helps to improve the performance when encoded features are distributed more evenly in the latent space. We apply it as an orthogonal improvement for unsupervised contrastive representation learning, including current outstanding methods SimCLR, MoCo, and BYOL. We evaluate our learned representations on many downstream benchmarks like linear classification on ImageNet-1k and PASCAL VOC 2007, object detection on MS COCO 2017 and VOC, etc. We obtain substantial gains with a large margin almost on all these tasks compared with prior arts.",0
"Artificial intelligence (AI) has made significant progress over recent years due to advances in deep learning techniques such as convolutional neural networks (CNNs), which have been used effectively to solve numerous challenging problems in computer vision tasks. In many applications, these models take multiple views of objects as inputs during training, but fail to capture higher level representations that can generalize well across instances and data domains. To address this issue, we propose a new approach called ""Beyond Single Instance Multi-view Unsupervised Representation Learning"" that utilizes multi-instance learning along with unsupervised pretext tasks to learn shared latent features from a collection of image sets containing multiple views of objects. We show through extensive experiments on several benchmark datasets that our method outperforms state-of-the-art approaches while being computationally efficient. Our results demonstrate the effectiveness of our proposed framework in improving both accuracy and robustness of object recognition models under complex real-world scenarios.",1
"We apply a Transformer architecture, specifically BERT, to learn flexible and high quality molecular representations for drug discovery problems. We study the impact of using different combinations of self-supervised tasks for pre-training, and present our results for the established Virtual Screening and QSAR benchmarks. We show that: i) The selection of appropriate self-supervised task(s) for pre-training has a significant impact on performance in subsequent downstream tasks such as Virtual Screening. ii) Using auxiliary tasks with more domain relevance for Chemistry, such as learning to predict calculated molecular properties, increases the fidelity of our learnt representations. iii) Finally, we show that molecular representations learnt by our model `MolBert' improve upon the current state of the art on the benchmark datasets.",0
In recent years there has been significant interest in using deep learning techniques such as recurrent neural networks (RNNs) and transformer architectures,1
"Disentanglement learning is crucial for obtaining disentangled representations and controllable generation. Current disentanglement methods face several inherent limitations: difficulty with high-resolution images, primarily focusing on learning disentangled representations, and non-identifiability due to the unsupervised setting. To alleviate these limitations, we design new architectures and loss functions based on StyleGAN (Karras et al., 2019), for semi-supervised high-resolution disentanglement learning. We create two complex high-resolution synthetic datasets for systematic testing. We investigate the impact of limited supervision and find that using only 0.25%~2.5% of labeled data is sufficient for good disentanglement on both synthetic and real datasets. We propose new metrics to quantify generator controllability, and observe there may exist a crucial trade-off between disentangled representation learning and controllable generation. We also consider semantic fine-grained image editing to achieve better generalization to unseen images.",0
"In recent years, generative adversarial networks (GANs) have been widely used due to their ability to generate realistic images from random noise inputs. However, they suffer from two main issues: mode collapse, which occurs when all generated samples map onto a single data point; and lack of interpretability, meaning that the underlying factors driving the generation process are difficult to uncover. To address these challenges, we propose a novel approach based on a semi-supervised StyleGAN architecture, where latent codes can be manipulated and controlled more easily. Our method leverages both labeled and unlabeled datasets to learn disentangled representations that can capture different attributes of objects, such as shape and texture. Extensive experiments demonstrate the effectiveness of our proposed model in generating high quality images while preserving desired characteristics, as well as achieving superior performance compared to state-of-the art methods in terms of visual fidelity and diversity. Additionally, we show that our model can effectively disentangle complex scene elements, allowing for detailed control over object features. These results contribute significantly towards enhancing the interpretability and controllability of GAN models, and open up new possibilities for applications such as image synthesis, editing, and augmentation.",1
"Leveraging temporal information has been regarded as essential for developing video understanding models. However, how to properly incorporate temporal information into the recent successful instance discrimination based contrastive self-supervised learning (CSL) framework remains unclear. As an intuitive solution, we find that directly applying temporal augmentations does not help, or even impair video CSL in general. This counter-intuitive observation motivates us to re-design existing video CSL frameworks, for better integration of temporal knowledge.   To this end, we present Temporal-aware Contrastive self-supervised learningTaCo, as a general paradigm to enhance video CSL. Specifically, TaCo selects a set of temporal transformations not only as strong data augmentation but also to constitute extra self-supervision for video understanding. By jointly contrasting instances with enriched temporal transformations and learning these transformations as self-supervised signals, TaCo can significantly enhance unsupervised video representation learning. For instance, TaCo demonstrates consistent improvement in downstream classification tasks over a list of backbones and CSL approaches. Our best model achieves 85.1% (UCF-101) and 51.6% (HMDB-51) top-1 accuracy, which is a 3% and 2.4% relative improvement over the previous state-of-the-art.",0
"This paper investigates how incorporating temporal information into contrastive self-supervised learning can improve performance on downstream tasks. While previous work has shown that contrastive learning is effective for representation learning, most methods rely solely on spatial representations and ignore the inherent temporal structure present in many data types. We propose two approaches to addressing this issue: first, we introduce Temporal InfoCon (TIC), which adds explicit temporal similarity constraints by using autoencoders to learn to reconstruct video frames given their corresponding future/past frame(s). Second, we develop SpatioTemporal InfoCon (STIC) that jointly learns both spatial and temporal alignment using cross-modal prediction objectives. Our experiments demonstrate that our proposed TIC and STIC outperform prior state-of-the-art self-supervised models across multiple benchmark datasets including image classification, action recognition, and segmentation tasks.",1
"Multi-view action recognition (MVAR) leverages complementary temporal information from different views to improve the learning performance. Obtaining informative view-specific representation plays an essential role in MVAR. Attention has been widely adopted as an effective strategy for discovering discriminative cues underlying temporal data. However, most existing MVAR methods only utilize attention to extract representation for each view individually, ignoring the potential to dig latent patterns based on mutual-support information in attention space. To this end, we propose a collaborative attention mechanism (CAM) for solving the MVAR problem in this paper. The proposed CAM detects the attention differences among multi-view, and adaptively integrates frame-level information to benefit each other. Specifically, we extend the long short-term memory (LSTM) to a Mutual-Aid RNN (MAR) to achieve the multi-view collaboration process. CAM takes advantages of view-specific attention pattern to guide another view and discover potential information which is hard to be explored by itself. It paves a novel way to leverage attention information and enhances the multi-view representation learning. Extensive experiments on four action datasets illustrate the proposed CAM achieves better results for each view and also boosts multi-view performance.",0
"In recent years, multi-view action recognition has become increasingly important as cameras have grown ubiquitous, allowing for multiple views of actions to be captured simultaneously from different angles. However, traditional approaches suffer from several limitations such as limited field of view or lack of coverage, which leads to incomplete or insufficient information for accurate classification of human actions. To address these challenges, we propose a novel collaborative attention mechanism that enables efficient fusion of multi-view features by capturing the interdependencies between them. Our proposed method adaptively focuses on informative regions across camera views to enhance feature representations, resulting in improved performance compared to existing methods. We evaluate our approach using three publicly available datasets and demonstrate significant improvements over state-of-the-art methods, achieving new highs in accuracy and efficiency.",1
"Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.",0
"Abstract: We introduce group-connected multilayer perceptron networks (GCPN) as a new architecture for deep learning tasks that can capture complex dependencies across features while reducing memory footprint and computational cost compared to traditional fully connected architectures. GCPNs utilize shared weights within groups of neurons across layers, allowing them to learn representations tailored to their specific input domains without losing expressive power. We demonstrate through extensive experiments on challenging benchmark datasets that GCPM systems achieve state-of-the-art results comparable to full precision models. Finally, we showcase some applications of GCPNs in real world problems like image classification and speech recognition.  This work contributes novel insights into how group connections can improve training stability and model quality in MLPs by exploiting commonalities among neurons. To our knowledge, we present the first attempt at developing group-connected multi-layered neural network models which shows competitive performance. For example, we achieve Top 1 accuracy greater than 85% on ImageNet-2012, surpassing many prior arts under similar settings. The success of our approach could potentially lead to reduced energy consumption and increased efficiency of large language and vision processing models making them deployable on embedded hardware. By sharing the weight matrix among multiple neurons, one could reduce model size; thus storing more models inside edge devices such as smartphones or embedded GPUs would become possible. Overall, this research demonstrates promising application scenarios for the developed methodology.",1
"Adversarial training (AT) is one of the most effective defenses against adversarial attacks for deep learning models. In this work, we advocate incorporating the hypersphere embedding (HE) mechanism into the AT procedure by regularizing the features onto compact manifolds, which constitutes a lightweight yet effective module to blend in the strength of representation learning. Our extensive analyses reveal that AT and HE are well coupled to benefit the robustness of the adversarially trained models from several aspects. We validate the effectiveness and adaptability of HE by embedding it into the popular AT frameworks including PGD-AT, ALP, and TRADES, as well as the FreeAT and FastAT strategies. In the experiments, we evaluate our methods under a wide range of adversarial attacks on the CIFAR-10 and ImageNet datasets, which verifies that integrating HE can consistently enhance the model robustness for each AT framework with little extra computation.",0
"This paper presents an approach to improve adversarial training by using hypersphere embedding. Traditional methods have shown to be vulnerable to attacks, which can lead to poor generalization performance on new data. Our method addresses these issues by applying constraints within a hyperspherical space. We demonstrate that our method leads to improved robustness against several types of attack models while still achieving competitive clean accuracy. Additionally, we provide insights into the properties and behavior of models trained under different regularizations schemes within hypersphere embeddings.",1
"We introduce Constr-DRKM, a deep kernel method for the unsupervised learning of disentangled data representations. We propose augmenting the original deep restricted kernel machine formulation for kernel PCA by orthogonality constraints on the latent variables to promote disentanglement and to make it possible to carry out optimization without first defining a stabilized objective. After illustrating an end-to-end training procedure based on a quadratic penalty optimization algorithm with warm start, we quantitatively evaluate the proposed method's effectiveness in disentangled feature learning. We demonstrate on four benchmark datasets that this approach performs similarly overall to $\beta$-VAE on a number of disentanglement metrics when few training points are available, while being less sensitive to randomness and hyperparameter selection than $\beta$-VAE. We also present a deterministic initialization of Constr-DRKM's training algorithm that significantly improves the reproducibility of the results. Finally, we empirically evaluate and discuss the role of the number of layers in the proposed methodology, examining the influence of each principal component in every layer and showing that components in lower layers act as local feature detectors capturing the broad trends of the data distribution, while components in deeper layers use the representation learned by previous layers and more accurately reproduce higher-level features.",0
"Deep Restricted Kernel Machines (DRKMs) have recently emerged as a powerful class of models that can learn high-dimensional nonlinear functions from large datasets while being computationally efficient owing to their implicit regularization properties. However, due to the linear combination operation performed by DRKMs on input features, they may fail to capture distinctive, interpretable patterns present in data. To address this issue, we introduce unsupervised learning of Disentangled Representations in Deep Restricted Kernel Machines with Orthogonality Constraints (Dr2v-DRKM). Dr2v-DRKM models are equipped with an additional loss term designed to enforce orthogonal latent variables. These new models can automatically discover meaningful feature groups without explicit supervision. We demonstrate experimentally on several benchmark problems that our model outperforms competitive baselines both quantitatively and qualitatively, providing compelling evidence that Dr2v-DRKM indeed learns more semantically meaningful representations than traditional DRKMs alone. By simultaneously capturing multiple intrinsically independent factors underlying complex phenomena, our work takes us closer towards realizing robust artificial intelligence systems capable of generalization across domains and tasks—the holy grail of modern machine learning research.",1
"We introduce CellSegmenter, a structured deep generative model and an amortized inference framework for unsupervised representation learning and instance segmentation tasks. The proposed inference algorithm is convolutional and parallelized, without any recurrent mechanisms, and is able to resolve object-object occlusion while simultaneously treating distant non-occluding objects independently. This leads to extremely fast training times while allowing extrapolation to arbitrary number of instances. We further introduce a transparent posterior regularization strategy that encourages scene reconstructions with fewest localized objects and a low-complexity background. We evaluate our method on a challenging synthetic multi-MNIST dataset with a structured background and achieve nearly perfect accuracy with only a few hundred training epochs. Finally, we show segmentation results obtained for a cell nuclei imaging dataset, demonstrating the ability of our method to provide high-quality segmentations while also handling realistic use cases involving large number of instances.",0
"In recent years, there has been significant interest in developing machine learning algorithms that can accurately segment cells from microscopy images without relying on manual annotations. However, existing methods often require large amounts of annotated data or rely heavily on supervision to achieve acceptable results. This work proposes a novel approach called ""CellSegmenter"" which uses unsupervised representation learning and instance segmentation techniques to successfully segment individual cells in complex tissue samples at high speed and accuracy without any manual annotation. We demonstrate the effectiveness of our method using a diverse set of modular images across different cell types and imaging modalities. Our results show that CellSegmenter outperforms several state-of-the-art approaches while requiring significantly less computational resources than traditional machine learning methods. Overall, we believe our proposed approach has great potential for improving the efficiency and accuracy of cell image analysis pipelines.",1
"We introduce Exemplar VAEs, a family of generative models that bridge the gap between parametric and non-parametric, exemplar based generative models. Exemplar VAE is a variant of VAE with a non-parametric prior in the latent space based on a Parzen window estimator. To sample from it, one first draws a random exemplar from a training set, then stochastically transforms that exemplar into a latent code and a new observation. We propose retrieval augmented training (RAT) as a way to speed up Exemplar VAE training by using approximate nearest neighbor search in the latent space to define a lower bound on log marginal likelihood. To enhance generalization, model parameters are learned using exemplar leave-one-out and subsampling. Experiments demonstrate the effectiveness of Exemplar VAEs on density estimation and representation learning. Importantly, generative data augmentation using Exemplar VAEs on permutation invariant MNIST and Fashion MNIST reduces classification error from 1.17% to 0.69% and from 8.56% to 8.16%.",0
"In recent years, generative models have become increasingly popular due to their ability to generate new data samples that resemble real world examples. One such model is Variational Autoencoder (VAE), which has shown impressive results in generating high quality images and other types of data. However, these models often require large amounts of training data and computational resources, making them difficult to use in practice. This paper presents a novel approach called ""Exemplar VAE"" which combines the strengths of generative models, nearest neighbor retrieval, and data augmentation to overcome some of these limitations. Our method uses pre-trained VAEs as an exemplar store and can retrieve specific examples that match a given input query, effectively enabling efficient fine-tuning and zero-shot generation. We show through extensive experiments on multiple datasets that our approach significantly outperforms state-of-the-art methods across several metrics while requiring fewer computational resources. Overall, our work demonstrates the potential of using VAEs in conjunction with other techniques to improve performance in tasks ranging from image generation to semantic segmentation.",1
"Retrieving unlabeled videos by textual queries, known as Ad-hoc Video Search (AVS), is a core theme in multimedia data management and retrieval. The success of AVS counts on cross-modal representation learning that encodes both query sentences and videos into common spaces for semantic similarity computation. Inspired by the initial success of previously few works in combining multiple sentence encoders, this paper takes a step forward by developing a new and general method for effectively exploiting diverse sentence encoders. The novelty of the proposed method, which we term Sentence Encoder Assembly (SEA), is two-fold. First, different from prior art that use only a single common space, SEA supports text-video matching in multiple encoder-specific common spaces. Such a property prevents the matching from being dominated by a specific encoder that produces an encoding vector much longer than other encoders. Second, in order to explore complementarities among the individual common spaces, we propose multi-space multi-loss learning. As extensive experiments on four benchmarks (MSR-VTT, TRECVID AVS 2016-2019, TGIF and MSVD) show, SEA surpasses the state-of-the-art. In addition, SEA is extremely ease to implement. All this makes SEA an appealing solution for AVS and promising for continuously advancing the task by harvesting new sentence encoders.",0
"This paper presents a novel approach for video retrieval using textual queries based on natural language. We propose a sentence encoder assembly that effectively translates user queries into semantically meaningful representations. Our method leverages advances in deep learning and natural language processing techniques to achieve state-of-the-art performance in matching videos to text descriptions. The effectiveness of our model is demonstrated through extensive experiments on several benchmark datasets, which show significant improvements over existing methods in terms of accuracy and efficiency. The proposed approach has broad applications in multimedia search and recommendation systems. Overall, our work highlights the potential of large-scale neural networks for solving complex problems in image and video understanding tasks.",1
"Density-ratio estimation via classification is a cornerstone of unsupervised learning. It has provided the foundation for state-of-the-art methods in representation learning and generative modelling, with the number of use-cases continuing to proliferate. However, it suffers from a critical limitation: it fails to accurately estimate ratios p/q for which the two densities differ significantly. Empirically, we find this occurs whenever the KL divergence between p and q exceeds tens of nats. To resolve this limitation, we introduce a new framework, telescoping density-ratio estimation (TRE), that enables the estimation of ratios between highly dissimilar densities in high-dimensional spaces. Our experiments demonstrate that TRE can yield substantial improvements over existing single-ratio methods for mutual information estimation, representation learning and energy-based modelling.",0
"""This"" would refer to another document that the reader could assume they already know quite well; you should make your own introduction which can stand on its own. Please write at least one sentence for each section (background / motivation, methodology, results / findings, conclusion). If any section is missing information, please fill in placeholder text until we have enough detail to move forward. Feel free to edit as necessary based on the outline I provide below so that everything fits together well. Background/Motivation: The need for accurate estimates of density ratio in various fields such as machine learning has become increasingly important over time. Traditional methods for estimating density ratios suffer from several drawbacks, including high computational cost and poor performance when dealing with nonlinearities and multivariate data sets. In response to these challenges, researchers have proposed numerous alternative methods. For example, kernel density estimation is a popular technique used to estimate densities by providing an approximation of the distribution function using bandwidth selection. Although this approach may yield reasonably good estimates, it lacks theoretical guarantees and may require excessive tuning of parameters. To address these issues, recent work suggests a new class of techniques called Telescoping Density-Ratio Estimators (TDREs) that aim to improve upon existing methods. This study proposes a novel extension to the original TDRE framework, further enhancing its utility across a broader range of applications. Methodology: Our contribution introduces a modified version of the TDRE called Adaptively Regularized Telescopic Density Ratios (ARTDR), which exploits structural properties within high-dimensional spaces encountered in practice. By incorporating regularization, ARTDR ensures both smoothness a",1
"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human's ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models.",0
"Abstract: This paper addresses Disentangle-based Continual Graph Representation Learning (DisCo). In particular, we present a novel framework that leverages the power of disentangling techniques coupled with graph representation learning algorithms. Our approach builds upon recent advancements in generative models and meta-learning, allowing us to effectively handle complex data structures while ensuring high quality representations throughout incremental updates from dynamic environments. We demonstrate both quantitatively and qualitatively across multiple domains the benefits of our proposed methodology compared against strong baselines.",1
"Self-supervised representation learning has witnessed significant leaps fueled by recent progress in Contrastive learning, which seeks to learn transformations that embed positive input pairs nearby, while pushing negative pairs far apart. While positive pairs can be generated reliably (e.g., as different views of the same image), it is difficult to accurately establish negative pairs, defined as samples from different images regardless of their semantic content or visual features. A fundamental problem in contrastive learning is mitigating the effects of false negatives. Contrasting false negatives induces two critical issues in representation learning: discarding semantic information and slow convergence. In this paper, we study this problem in detail and propose novel approaches to mitigate the effects of false negatives. The proposed methods exhibit consistent and significant improvements over existing contrastive learning-based models. They achieve new state-of-the-art performance on ImageNet evaluations, achieving 5.8% absolute improvement in top-1 accuracy over the previous state-of-the-art when finetuning with 1% labels, as well as transferring to downstream tasks.",0
"Recent advances in deep learning have been driven by large amounts of labelled data. However, acquiring labels can be expensive and time consuming, especially for tasks that require expert knowledge. This has led to increased interest in self-supervised learning methods which allow for training on unlabelled data. One such method is contrastive self-supervised learning (CSL), where two views of each instance are learned so as to maximize their similarity under some representations while minimizing it otherwise. In practice, CSL suffers from high variability across different randomized augmentations due to random chance causing false negative pairs. To address this issue we propose the novel technique of false negative cancellation during optimization. We train a model using positive examples only and discard negative ones whenever they appear too hard to predict correctly against any threshold within an adaptive framework based on a surrogate loss. This significantly improves stability and results in better generalization performance without any increase in computational cost per epoch. Empirically, our approach achieves new state of art results outperforming previous methods by significant margins on benchmark datasets including ImageNet. Our code will be made publicly available for use by others who wish to apply our techniques.",1
"Today's most popular approaches to keypoint detection involve very complex network architectures that aim to learn holistic representations of all keypoints. In this work, we take a step back and ask: Can we simply learn a local keypoint representation from the output of a standard backbone architecture? This will help make the network simpler and more robust, particularly if large parts of the object are occluded. We demonstrate that this is possible by looking at the problem from the perspective of representation learning. Specifically, the keypoint kernels need to be chosen to optimize three types of distances in the feature space: Features of the same keypoint should be similar to each other, while differing from those of other keypoints, and also being distinct from features from the background clutter. We formulate this optimization process within a framework, which we call CoKe, which includes supervised contrastive learning. CoKe needs to make several approximations to enable representation learning process on large datasets. In particular, we introduce a clutter bank to approximate non-keypoint features, and a momentum update to compute the keypoint representation while training the feature extractor. Our experiments show that CoKe achieves state-of-the-art results compared to approaches that jointly represent all keypoints holistically (Stacked Hourglass Networks, MSS-Net) as well as to approaches that are supervised by detailed 3D object geometry (StarMap). Moreover, CoKe is robust and performs exceptionally well when objects are partially occluded and significantly outperforms related work on a range of diverse datasets (PASCAL3D+, MPII, ObjectNet3D).",0
"Robust keypoints detection plays an essential role in computer vision applications such as object recognition, image alignment, and visual tracking. Despite significant advances in the field, detecting keypoints remains challenging due to changes in illumination, pose, scale, and other variations encountered across images. In this work we propose CoKe (Contrastive Kernel), which localizes the keypoints by minimizing their contrast against intra-class features while maximizing them against inter-class ones. Unlike global feature embedding methods that overlook fine-grained details, our method emphasizes learning from neighboring pixels which are most relevant to the core operation at hand. CoKe employs a convolutional neural network architecture that operates on patches centered around candidate locations, enforcing a high concentration of learned kernels locally while ensuring they generalize well globally. We provide extensive experiments showing improved performance compared to state-of-the-art approaches on standard benchmark datasets under diverse experimental settings.",1
"Reinforcement Learning (RL) has recently been applied to sequential estimation and prediction problems identifying and developing hypothetical treatment strategies for septic patients, with a particular focus on offline learning with observational data. In practice, successful RL relies on informative latent states derived from sequential observations to develop optimal treatment strategies. To date, how best to construct such states in a healthcare setting is an open question. In this paper, we perform an empirical study of several information encoding architectures using data from septic patients in the MIMIC-III dataset to form representations of a patient state. We evaluate the impact of representation dimension, correlations with established acuity scores, and the treatment policies derived from them. We find that sequentially formed state representations facilitate effective policy learning in batch settings, validating a more thoughtful approach to representation learning that remains faithful to the sequential and partial nature of healthcare data.",0
"In recent years, representation learning has emerged as a promising technique for solving complex problems in reinforcement learning (RL). One area where RL has shown great promise is healthcare, where decision making often involves tradeoffs between competing objectives and must take into account large amounts of patient data. In our work, we investigate the use of representation learning for improving RL algorithms in healthcare settings. We conducted an empirical study on two challenging clinical applications: treatment planning for lung cancer patients and personalized medicine for depression. Our results demonstrate that using representation learning can significantly improve both the efficiency and effectiveness of RL algorithms in these domains. Furthermore, we show that learned representations generalize across different tasks, suggesting their utility as a foundation for building more broadly applicable RL models. Overall, our findings highlight the potential of representation learning as a powerful tool for advancing medical decision making through intelligent systems.",1
"Estimating individual and average treatment effects from observational data is an important problem in many domains such as healthcare and e-commerce. In this paper, we advocate balance regularization of multi-head neural network architectures. Our work is motivated by representation learning techniques to reduce differences between treated and untreated distributions that potentially arise due to confounding factors. We further regularize the model by encouraging it to predict control outcomes for individuals in the treatment group that are similar to control outcomes in the control group. We empirically study the bias-variance trade-off between different weightings of the regularizers, as well as between inductive and transductive inference.",0
"This paper presents a novel method that uses regularization techniques to improve causal effect estimation accuracy using neural network models (NNMs). The approach proposed balances bias and variance by incorporating relevant domain knowledge as priors to stabilize learning processes. We demonstrate empirically our methods outperform traditional counterparts on synthetic datasets, leading to more accurate estimates of treatment effects even under nonlinear confounding relationships, and smaller error rates compared against existing methods when estimating heterogeneous treatment effects across multiple subgroups defined by baseline covariates. Our work contributes to improved accuracy in estimating casual effects of observational studies where selection biases and unmeasured confounders could cause model misspecification and/or inaccurate predictions. As such, researchers can better make informed decisions using results generated from NNMs trained utilizing our proposed techniques, which enhance performance through balance and stability. While we focus on estimating Average Treatment Effects (ATE), these ideas generalize naturally to other types of treatment effects like Local Average Treatment Effects (LATE) or Conditional Average Treatment Effects (CATE). Further study will extend these advancements into new domains with broader impacts, including precision medicine with big data resources.",1
"A fundamental task in data exploration is to extract simplified low dimensional representations that capture intrinsic geometry in data, especially for faithfully visualizing data in two or three dimensions. Common approaches to this task use kernel methods for manifold learning. However, these methods typically only provide an embedding of fixed input data and cannot extend to new data points. Autoencoders have also recently become popular for representation learning. But while they naturally compute feature extractors that are both extendable to new data and invertible (i.e., reconstructing original features from latent representation), they have limited capabilities to follow global intrinsic geometry compared to kernel-based manifold learning. We present a new method for integrating both approaches by incorporating a geometric regularization term in the bottleneck of the autoencoder. Our regularization, based on the diffusion potential distances from the recently-proposed PHATE visualization method, encourages the learned latent representation to follow intrinsic data geometry, similar to manifold learning algorithms, while still enabling faithful extension to new data and reconstruction of data in the original feature space from latent coordinates. We compare our approach with leading kernel methods and autoencoder models for manifold learning to provide qualitative and quantitative evidence of our advantages in preserving intrinsic structure, out of sample extension, and reconstruction. Our method is easily implemented for big-data applications, whereas other methods are limited in this regard.",0
"Machine learning has made significant progress in recent years with applications ranging from image classification tasks to natural language processing. One important component of machine learning algorithms is data preprocessing which involves techniques such as dimensionality reduction and feature extraction. In this work, we propose a novel method using extendable and invertible manifold learning with geometry regularized autoencoders (XAI-GAE) that can learn both linear and nonlinear representations simultaneously while preserving the geometric structure of the original data. Our results show that XAI-GAE outperforms existing methods in terms of accuracy on benchmark datasets and scalability when handling high-dimensional data. Additionally, our proposed method allows us to easily interpret the learned representation by performing invertibility operations. We believe that our work extends the state-of-the-art in machine learning research and provides new tools for understanding complex data structures. ----- Abstract: In today's world of rapidly advancing technology, machine learning has become increasingly important, particularly in areas like image classification and natural language processing. Preprocessing techniques like dimension reduction and feature extraction play critical roles in improving model performance. This study presents a novel approach utilizing extendable and invertible manifold learning with geometry regularized autoencoders (XAI-GAE). The proposed method enables simultaneous learning of both linear and nonlinear representations while maintaining the underlying geometric structure of the data. Experimental results demonstrate improved accuracy compared to traditional methods across several benchmark datasets and superior scalability with high-dimensional data. Furthermore, the ability to perform invertibility operations adds another layer of insight into the learned representations, making the algorithm more interpretable and transparent. Overall, these findings contribute to the development of advanced machine learning technologies and highlight the potential benefits of incorporating innovative approaches into real-world applications.",1
"Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the ""distillation"" of features at various layers of an expert network, pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks' intermediate activation values given the input data. We show that considering multiple intermediate hints in distillation leads to better exploiting the expert's knowledge and more distinctive discrepancy compared to solely utilizing the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework for the localization of anomalous regions. Despite the striking contrast between some test datasets and ImageNet, we achieve competitive or significantly superior results compared to the SOTA methods on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two Medical datasets on both anomaly detection and localization.",0
"In today’s world data has become very important part of our day-to-day life. Data comes in different forms such as text, image, audio and video which can be used to solve complex problems like anomaly detection. Anomaly detection involves finding patterns that deviate from normal behavior. There are many approaches available to perform anomaly detection like clustering algorithms, PCA etc but these methods have some drawbacks like low accuracy due to lack of knowledge transfer across different resolution levels. To overcome these limitations we present multiresolution knowledge distillation (MKD) method. MKD provides high performance by learning deep features and using them effectively for both tasks of classification and regression at multiple scales. MKD outperforms existing state-of-the art techniques on three popular datasets like breast cancer images dataset, Usenet newsgroup spam detection and online review fraudulent detection. This study shows that MKD can be used to detect anomalies more accurately than other existing methods.",1
"Graph representation learning has attracted lots of attention recently. Existing graph neural networks fed with the complete graph data are not scalable due to limited computation and memory costs. Thus, it remains a great challenge to capture rich information in large-scale graph data. Besides, these methods mainly focus on supervised learning and highly depend on node label information, which is expensive to obtain in the real world. As to unsupervised network embedding approaches, they overemphasize node proximity instead, whose learned representations can hardly be used in downstream application tasks directly. In recent years, emerging self-supervised learning provides a potential solution to address the aforementioned problems. However, existing self-supervised works also operate on the complete graph data and are biased to fit either global or very local (1-hop neighborhood) graph structures in defining the mutual information based loss terms.   In this paper, a novel self-supervised representation learning method via Subgraph Contrast, namely \textsc{Subg-Con}, is proposed by utilizing the strong correlation between central nodes and their sampled subgraphs to capture regional structure information. Instead of learning on the complete input graph data, with a novel data augmentation strategy, \textsc{Subg-Con} learns node representations through a contrastive loss defined based on subgraphs sampled from the original graph instead. Compared with existing graph representation learning approaches, \textsc{Subg-Con} has prominent performance advantages in weaker supervision requirements, model learning scalability, and parallelization. Extensive experiments verify both the effectiveness and the efficiency of our work compared with both classic and state-of-the-art graph representation learning approaches on multiple real-world large-scale benchmark datasets from different domains.",0
"In recent years, graph representation learning has emerged as a promising approach for tackling tasks such as node classification and link prediction on large, complex networks. However, existing methods often suffer from scalability issues due to their reliance on handcrafted features, which can require extensive computation time and memory resources. To address these challenges, we propose a novel method called Sub-Graph Contrast (SGC) that leverages self-supervision to learn representations of graphs at scale. Our approach involves randomly selecting sub-graphs from larger graphs and then training a model to predict whether two nodes belong to the same sub-graph or not. This simple yet effective task allows us to contrast different parts of the graph, leading to richer, more informative representations. Through experimental evaluation on several benchmark datasets, we demonstrate that our method outperforms state-of-the-art baselines in terms of accuracy while maintaining low computational overhead. Overall, SGC offers a powerful new tool for scaling up graph representation learning under limited resource constraints.",1
"Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (eg., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suite of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets -- demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning.",0
"This paper proposes a novel unsupervised pre-training method called PointContrast for improving the performance of point cloud understanding tasks. 3D point clouds are becoming increasingly popular in computer vision due to their ability to capture detailed geometric information, but they present unique challenges such as high variability in shape and size, noise, and lack of texture information. Consequently, current methods struggle in effectively learning representations that generalize well across different domains.  PointContrast addresses these issues by introducing a self-supervised contrastive pre-training approach for 3D point clouds. Inspired by recent advances in image and video processing, our method learns robust feature representations through contrasting positive pairs (samples from the same scene) against negative ones obtained via data augmentation. By maximizing agreement between positives while minimizing agreement between negatives, we enhance the discriminative power and transferability of learned features.  Our extensive experiments on two benchmark datasets demonstrate significant improvements over baseline models for both supervised and semi-supervised fine-tuning, validating the effectiveness of our proposed method. Moreover, our ablation studies show the importance of each design choice in PointContrast, highlighting its robustness. Overall, we believe PointContrast provides valuable insights into pre-training deep neural networks for 3D point cloud analysis and has promising applications in areas such as autonomous driving, robotics, and VR/AR.",1
"Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our ""SimSiam"" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.",0
"This paper presents an exploration of simple Siamese representation learning techniques that can effectively learn meaningful representations from raw data. We begin by introducing background on deep learning frameworks and their current limitations in capturing intricate relationships among data points. Our proposed approach utilizes contrastive learning to encourage more discriminative embedding generation while addressing issues related to stability in training. Extensive experiments on several benchmark datasets demonstrate improved performance over existing methods, validating our method as a viable alternative for image classification tasks. Overall, this work advances research into unsupervised and semi-supervised learning paradigms, promoting future development in these areas.",1
"We propose a model-agnostic pipeline to recover graph signals from an expert system by exploiting the content addressable memory property of restricted Boltzmann machine and the representational ability of a neural network. The proposed pipeline requires the deep neural network that is trained on a downward machine learning task with clean data, data which is free from any form of corruption or incompletion. We show that denoising the representations learned by the deep neural networks is usually more effective than denoising the data itself. Although this pipeline can deal with noise in any dataset, it is particularly effective for graph-structured datasets.",0
"Here is an example of such an abstract:  In recent years, the field of graph signal processing has seen significant growth as researchers have sought new ways to analyze signals on complex networks. In particular, there is growing interest in recovering high-resolution (HR) versions of graph signals from their low-resolution (LR) counterparts, which often arise due to limitations in data acquisition or transmission. While many methods exist for HR graph signal recovery, they typically require some form of prior knowledge or restrictive assumptions that may not hold in practice. This paper presents a novel approach based on restricted Boltzmann machines (RBMs), powerful probabilistic models with connections to deep learning. We show how RBMs can accurately reconstruct HR signals by leveraging information contained within both LR measurements and topological structure of the underlying network. Our method requires no prior knowledge of the graph signal bandwidth or any other information beyond the given LR measurement vector and edge weights; indeed we demonstrate that our algorithm even outperforms methods specifically tailored for certain classes of graph signals under commonly made assumptions. Numerical experiments on synthetic data validate the effectiveness of our method and confirm theoretical findings, while results on real-world datasets illustrate its broad applicability across diverse fields ranging from sensor networking to neuroscience. Overall, our work represents a major step forward in the field of graph signal recovery and opens up exciting possibilities for further development along these lines.",1
"Presence of bias (in datasets or tasks) is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in recent years. Such challenges range from spurious associations between variables in medical studies to the bias of race in gender or face recognition systems. Controlling for all types of biases in the dataset curation stage is cumbersome and sometimes impossible. The alternative is to use the available data and build models incorporating fair representation learning. In this paper, we propose such a model based on adversarial training with two competing objectives to learn features that have (1) maximum discriminative power with respect to the task and (2) minimal statistical mean dependence with the protected (bias) variable(s). Our approach does so by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and the learned features. We apply our method to synthetic data, medical images (containing task bias), and a dataset for gender classification (containing dataset bias). Our results show that the learned features by our method not only result in superior prediction performance but also are unbiased. The code is available at https://github.com/QingyuZhao/BR-Net/.",0
"This paper discusses how representation learning can mitigate bias by utilizing statistical independence. Firstly, we define statistical independence as a mathematical concept that measures how well two random variables are related. We then explain how traditional machine learning algorithms assume linear relationships between inputs and outputs, which can lead to biased predictions. On the other hand, representation learning seeks to learn representations that capture more complex underlying structures by modeling nonlinear relationships and dependencies among features. By incorporating notions of statistical independence into representation learning frameworks, we show that we can improve generalization performance on out-of-sample data while reducing bias due to overfitting. Furthermore, we demonstrate empirically through experiments on real datasets that our approach leads to better accuracy than existing methods without sacrificing interpretability. Our work contributes to understanding and improving the reliability of machine learning models in diverse application domains including computer vision, natural language processing, and decision making under uncertainty.",1
"We present a new framework for self-supervised representation learning by formulating it as a ranking problem in an image retrieval context on a large number of random views (augmentations) obtained from images. Our work is based on two intuitions: first, a good representation of images must yield a high-quality image ranking in a retrieval task; second, we would expect random views of an image to be ranked closer to a reference view of that image than random views of other images. Hence, we model representation learning as a learning to rank problem for image retrieval. We train a representation encoder by maximizing average precision (AP) for ranking, where random views of an image are considered positively related, and that of the other images considered negatives. The new framework, dubbed S2R2, enables computing a global objective on multiple views, compared to the local objective in the popular contrastive learning framework, which is calculated on pairs of views. In principle, by using a ranking criterion, we eliminate reliance on object-centric curated datasets. When trained on STL10 and MS-COCO, S2R2 outperforms SimCLR and the clustering-based contrastive learning model, SwAV, while being much simpler both conceptually and at implementation. On MS-COCO, S2R2 outperforms both SwAV and SimCLR with a larger margin than on STl10. This indicates that S2R2 is more effective on diverse scenes and could eliminate the need for an object-centric large training dataset for self-supervised representation learning.",0
"Title: ""Self-supervised ranking as a pretext task for representation learning""  Abstract: As deep learning models continue to advance, one of their major limitations remains the requirement for large amounts of labeled data. In recent years, self-supervised learning (SSL) has emerged as a promising alternative approach that utilizes unlabeled datasets by training models on auxiliary tasks known as pretext tasks. These pretext tasks aim to learn robust representations from raw input data by solving puzzles that require understanding of context, patterns, or structure within the data. This work presents an SSL framework called self-supervised ranking (SSR), which uses a novel ranking-based pretext task to learn powerful representations. SSR achieves state-of-the-art results across several benchmark image classification and sentiment analysis datasets while using substantially less labeled training data compared to previous methods. Overall, our findings demonstrate the effectiveness of SSR as a simple yet powerful means of boosting model performance through pretextual knowledge transfer without relying solely on massive amounts of supervision.",1
"Causal modeling has been recognized as a potential solution to many challenging problems in machine learning (ML). Here, we describe how a recently proposed counterfactual approach developed to deconfound linear structural causal models can still be used to deconfound the feature representations learned by deep neural network (DNN) models. The key insight is that by training an accurate DNN using softmax activation at the classification layer, and then adopting the representation learned by the last layer prior to the output layer as our features, we have that, by construction, the learned features will fit well a (multi-class) logistic regression model, and will be linearly associated with the labels. As a consequence, deconfounding approaches based on simple linear models can be used to deconfound the feature representations learned by DNNs. We validate the proposed methodology using colored versions of the MNIST dataset. Our results illustrate how the approach can effectively combat confounding and improve model stability in the context of dataset shifts generated by selection biases.",0
"This paper presents a novel approach for addressing the problem of counterfactual confounding in deep learning models. Counterfactual confounding occurs when the features used to train a model can have different causal effects on the outcome depending on which interventions were applied. Existing methods for handling counterfactual confounding either assume access to data from multiple interventions or rely on strong assumptions about the relationship between causes and effects. In contrast, our method uses a causality-aware counterfactual framework that directly estimates the causal effect of each feature on the outcome. Our method first learns a deep representation of the input data using standard training techniques, then applies a set of carefully designed constraints to ensure that the resulting representation respects causal relationships. We evaluate our method on several benchmark datasets and show that it outperforms state-of-the-art alternatives in terms of both accuracy and robustness to changes in the causal system. Our work has important implications for applications where deep learning models must make predictions based on complex relationships among variables, such as healthcare, finance, and criminal justice.",1
"Representation learning on graphs has emerged as a powerful mechanism to automate feature vector generation for downstream machine learning tasks. The advances in representation on graphs have centered on both homogeneous and heterogeneous graphs, where the latter presenting the challenges associated with multi-typed nodes and/or edges. In this paper, we consider the additional challenge of evolving graphs. We ask the question of whether the advances in representation learning for static graphs can be leveraged for dynamic graphs and how? It is important to be able to incorporate those advances to maximize the utility and generalization of methods. To that end, we propose the Framework for Incremental Learning of Dynamic Networks Embedding (FILDNE), which can utilize any existing static representation learning method for learning node embeddings, while keeping the computational costs low. FILDNE integrates the feature vectors computed using the standard methods over different timesteps into a single representation by developing a convex combination function and alignment mechanism. Experimental results on several downstream tasks, over seven real-world data sets, show that FILDNE is able to reduce memory and computational time costs while providing competitive quality measure gains with respect to the contemporary methods for representation learning on dynamic graphs.",0
"""FILDNE (Framework for Incremental Learning of Dynamic Networks Embeddings) is a new approach for training neural networks that can handle dynamic changes in network architecture. This framework uses a recurrent neural network (RNN) to learn representations of the input data, which can then be used as initializations for other models such as convolutional neural networks (CNNs). Through incremental learning, these embeddings capture both static features from the dataset as well as dynamics that may occur over time. By using a curriculum learning algorithm, this method can be applied to large datasets without sacrificing accuracy. With a comprehensive evaluation on multiple benchmark datasets, we demonstrate the effectiveness of our proposed framework for handling evolving datasets."" Please edit accordingly. ""In recent years there has been increasing interest in developing machine learning algorithms that can effectively handle evolving datasets. To address this need, we propose the use of a novel framework called FILDNE (Framework for Incremental Learning of Dynamic Networks Embeddings), which utilizes a recurrent neural network (RNN) to dynamically learn meaningful representations of the input data. These representations, referred to as embeddings, can then serve as effective starting points for subsequent fine-tuning on more complex models such as convolutional neural networks (CNNs). Our experimental evaluations demonstrate the efficacy of this approach on several benchmark datasets across different domains. Furthermore, by employing a curriculum learning algorithm, we show that FILDNE excels at balancing model complexity against representation quality even while dealing with big data sets.""",1
"Few-Shot Learning (FSL) aims to improve a model's generalization capability in low data regimes. Recent FSL works have made steady progress via metric learning, meta learning, representation learning, etc. However, FSL remains challenging due to the following longstanding difficulties. 1) The seen and unseen classes are disjoint, resulting in a distribution shift between training and testing. 2) During testing, labeled data of previously unseen classes is sparse, making it difficult to reliably extrapolate from labeled support examples to unlabeled query examples. To tackle the first challenge, we introduce Hybrid Consistency Training to jointly leverage interpolation consistency, including interpolating hidden features, that imposes linear behavior locally and data augmentation consistency that learns robust embeddings against sample variations. As for the second challenge, we use unlabeled examples to iteratively normalize features and adapt prototypes, as opposed to commonly used one-time update, for more reliable prototype-based transductive inference. We show that our method generates a 2% to 5% improvement over the state-of-the-art methods with similar backbones on five FSL datasets and, more notably, a 7% to 8% improvement for more challenging cross-domain FSL.",0
"In recent years, few-shot learning has emerged as one of the most promising approaches in machine learning, enabling agents to generalize to unseen tasks by learning from just a handful of examples. However, few-shot learning still faces several challenges, including overfitting, poor transfer performance, and limited scalability. To address these issues, we propose a new approach called hybrid consistency training (HCT) that combines three key components: prototype adaptation, self-distillation, and model selection via meta-learning. Our method adapts prototypes based on feature representations learned during each task optimization step, leading to better task alignment and reduced forgetting. By jointly optimizing both self-supervised distillation loss and cross-entropy loss using meta-learned models, our approach achieves strong baseline performance while preserving the capability for rapid adaptation to new tasks. Through extensive experiments across multiple benchmark datasets, we demonstrate HCT significantly outperforms state-of-the-art methods in terms of accuracy, robustness, and generalization ability, making it well suited for applications where labeled data may be scarce but prior knowledge exists. Overall, HCT represents an important contribution to the field of few-shot learning and deep reinforcement learning more broadly.",1
"We present a new generative autoencoder model with dual contradistinctive losses to improve generative autoencoder that performs simultaneous inference (reconstruction) and synthesis (sampling). Our model, named dual contradistinctive generative autoencoder (DC-VAE), integrates an instance-level discriminative loss (maintaining the instance-level fidelity for the reconstruction/synthesis) with a set-level adversarial loss (encouraging the set-level fidelity for there construction/synthesis), both being contradistinctive. Extensive experimental results by DC-VAE across different resolutions including 32x32, 64x64, 128x128, and 512x512 are reported. The two contradistinctive losses in VAE work harmoniously in DC-VAE leading to a significant qualitative and quantitative performance enhancement over the baseline VAEs without architectural changes. State-of-the-art or competitive results among generative autoencoders for image reconstruction, image synthesis, image interpolation, and representation learning are observed. DC-VAE is a general-purpose VAE model, applicable to a wide variety of downstream tasks in computer vision and machine learning.",0
"An innovative new machine learning model called the dual contradictive generative autoencoder (DCGAN) has been developed and applied with promising results. This model combines two complementary objectives – reconstruction error minimization through backpropagation, as well as adversarial training to improve data generation quality - into one framework. The advantages of using DCGANs over traditional GANs include enhanced stability during optimization and improved sample fidelity at high resolutions. We demonstrate the effectiveness of our method on various datasets across different modalities and applications such as image synthesis, audio synthesis, video generation, and text generation. Our experiments show that DCGAN outperforms state-of-the-art baselines in terms of visual realism, audio intelligibility, and naturalness of generated texts. In addition, we provide insights on hyperparameter selection for better performance tuning. Overall, our work significantly advances the state-of-the-art in generative models, opening up exciting opportunities for future research in areas like content creation, data augmentation, anomaly detection, and more.",1
"Contrastive learning has achieved great success in self-supervised visual representation learning, but existing approaches mostly ignored spatial information which is often crucial for visual representation. This paper presents heterogeneous contrastive learning (HCL), an effective approach that adds spatial information to the encoding stage to alleviate the learning inconsistency between the contrastive objective and strong data augmentation operations. We demonstrate the effectiveness of HCL by showing that (i) it achieves higher accuracy in instance discrimination and (ii) it surpasses existing pre-training methods in a series of downstream tasks while shrinking the pre-training costs by half. More importantly, we show that our approach achieves higher efficiency in visual representations, and thus delivers a key message to inspire the future research of self-supervised visual representation learning.",0
"Our approach addresses the challenge of learning high quality visual representations from complex images. By using contrastive techniques we can learn informative features that capture both local spatial structure as well as global semantic meaning. To encode multi-scale representations our architecture uses several parallel pathways of varying resolutions, each processing at a different scale. Furthermore, by using multiple random crops during training, we encourage models to focus on discriminative regions rather than relying solely on background context. We evaluate our method on two challenging benchmark datasets; COCO Stuff and PixCell, where we see significant improvements over previous state of the art results.",1
"We present a novel nonparametric algorithm for symmetry-based disentangling of data manifolds, the Geometric Manifold Component Estimator (GEOMANCER). GEOMANCER provides a partial answer to the question posed by Higgins et al. (2018): is it possible to learn how to factorize a Lie group solely from observations of the orbit of an object it acts on? We show that fully unsupervised factorization of a data manifold is possible if the true metric of the manifold is known and each factor manifold has nontrivial holonomy -- for example, rotation in 3D. Our algorithm works by estimating the subspaces that are invariant under random walk diffusion, giving an approximation to the de Rham decomposition from differential geometry. We demonstrate the efficacy of GEOMANCER on several complex synthetic manifolds. Our work reduces the question of whether unsupervised disentangling is possible to the question of whether unsupervised metric learning is possible, providing a unifying insight into the geometric nature of representation learning.",0
"In this paper, we present a novel method for disentangling complex representations into simpler, more interpretable features. Our approach utilizes subspace diffusion, which allows us to gradually remove information from high-dimensional data while preserving important relationships between variables. We demonstrate that our method effectively generates new, meaningful features that capture underlying structure in real datasets across domains, including image generation and natural language processing. These results have broad implications for understanding how humans process information and make decisions in complex tasks. Our work provides insights into the nature of human intelligence and suggests promising directions for future research on artificial intelligence.",1
"Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using $\beta$-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.",0
"This study presents a novel approach to representing and analyzing electroencephalography (EEG) signals for improved classification of clinical factors, such as seizure onset zone localization and epilepsy diagnosis. Using representation learning techniques, we aimed to enhance the interpretation and prediction capabilities of these important applications. Our methodology involved extracting high-level features from raw EEG data using convolutional neural networks (CNNs). We then trained our model on large datasets of labeled examples, employing supervised deep learning methods and cross-validation procedures. By evaluating the performance of our algorithm against state-of-the-art benchmarks, we found that representation learning can indeed significantly improve both interpretability and classification accuracy for complex clinical problems involving EEG analysis. Additionally, we conducted sensitivity analyses to identify which components of our model were most crucial for achieving optimal results, allowing us to refine and tailor our method according to specific use cases within the healthcare industry. Overall, we demonstrate the potential value of using advanced machine learning algorithms in addressing pressing challenges faced by modern medicine, highlighting how innovations in technology can ultimately lead to more effective patient care.",1
"In this paper we show that learning video feature spaces in which temporal cycles are maximally predictable benefits action classification. In particular, we propose a novel learning approach termed Cycle Encoding Prediction (CEP) that is able to effectively represent high-level spatio-temporal structure of unlabelled video content. CEP builds a latent space wherein the concept of closed forward-backward as well as backward-forward temporal loops is approximately preserved. As a self-supervision signal, CEP leverages the bi-directional temporal coherence of the video stream and applies loss functions that encourage both temporal cycle closure as well as contrastive feature separation. Architecturally, the underpinning network structure utilises a single feature encoder for all video snippets, adding two predictive modules that learn temporal forward and backward transitions. We apply our framework for pretext training of networks for action recognition tasks. We report significantly improved results for the standard datasets UCF101 and HMDB51. Detailed ablation studies support the effectiveness of the proposed components. We publish source code for the CEP components in full with this paper.",0
"The rapid growth of video data has led to significant challenges in effectively utilizing these resources for machine learning tasks. To overcome these hurdles, self-supervised representation learning techniques have emerged as promising solutions that use large datasets without relying on manually annotated labels. In particular, contrastive learning methods have shown great potential by leveraging pretext tasks such as instance discrimination or jigsaw puzzle solving. However, existing approaches often struggle with high memory usage due to their reliance on large batch sizes and negative samples. Furthermore, designing effective pretext tasks can require extensive domain knowledge and manual tuning. Addressing these issues, we propose a new technique called cycle encoding prediction (CEP) which introduces a novel pretext task based on temporal cycles to efficiently capture discriminative representations from unlabeled videos. Our method achieves state-of-the-art results across multiple benchmarks while significantly reducing memory overhead through smaller batch sizes and eliminating the need for complex augmentations. Overall, our work demonstrates the effectiveness of CEP for self-supervised video representation learning and paves the way for future advancements in this exciting field.",1
"Recently, contrastive learning has largely advanced the progress of unsupervised visual representation learning. Pre-trained on ImageNet, some self-supervised algorithms reported higher transfer learning performance compared to fully-supervised methods, seeming to deliver the message that human labels hardly contribute to learning transferrable visual features. In this paper, we defend the usefulness of semantic labels but point out that fully-supervised and self-supervised methods are pursuing different kinds of features. To alleviate this issue, we present a new algorithm named Supervised Contrastive Adjustment in Neighborhood (SCAN) that maximally prevents the semantic guidance from damaging the appearance feature embedding. In a series of downstream tasks, SCAN achieves superior performance compared to previous fully-supervised and self-supervised methods, and sometimes the gain is significant. More importantly, our study reveals that semantic labels are useful in assisting self-supervised methods, opening a new direction for the community.",0
"Effectively learning visual representations without supervision remains a challenging task in computer vision research. One method that has gained recent interest involves training models on large amounts of unlabeled data while utilizing semantic labels during training. This approach helps guide the model towards meaningful representations by providing additional context through these auxiliary annotations. In our work, we explore how different types of semantic labels can impact self-supervised representation learning using multiple datasets and evaluation metrics. Our findings indicate that high-level semantic labels provide greater benefits than low-level ones, particularly when fine-grained distinctions need to be captured. Additionally, we investigate how incorporating these labels affects both performance and robustness under domain shifts. Overall, our study demonstrates the potential utility of semantic labels as an effective means of improving self-supervised visual representation learning. We hope our results encourage further investigation into this promising direction.",1
"This paper presents TCE: Temporally Coherent Embeddings for self-supervised video representation learning. The proposed method exploits inherent structure of unlabeled video data to explicitly enforce temporal coherency in the embedding space, rather than indirectly learning it through ranking or predictive proxy tasks. In the same way that high-level visual information in the world changes smoothly, we believe that nearby frames in learned representations will benefit from demonstrating similar properties. Using this assumption, we train our TCE model to encode videos such that adjacent frames exist close to each other and videos are separated from one another. Using TCE we learn robust representations from large quantities of unlabeled video data. We thoroughly analyse and evaluate our self-supervised learned TCE models on a downstream task of video action recognition using multiple challenging benchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN backbone and only RGB stream inputs, TCE pre-trained representations outperform all previous selfsupervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code and pre-trained models for this paper can be downloaded at: https://github.com/csiro-robotics/TCE",0
"This paper presents a novel method for learning temporal representations from video data using self-supervision. Our approach builds upon recent advances in pretext task design that learn visual representations by solving jigsaw puzzles, but instead we focus on building temporally coherent representations. We achieve this by introducing a new pretext task called ""Temporal Order Memory"" (TOM), where pairs of frames are randomly shuffled and the model must rearrange them into their correct temporal order. By training our models on these tasks, we show that we can significantly improve performance on standard benchmarks such as action recognition and activity understanding. Furthermore, our results demonstrate that learned representations capture important temporal dynamics and patterns beyond simple appearance matching. Finally, we discuss how our work contributes to the broader field of unsupervised representation learning and future research directions.",1
"Patient representation learning refers to learning a dense mathematical representation of a patient that encodes meaningful information from Electronic Health Records (EHRs). This is generally performed using advanced deep learning methods. This study presents a systematic review of this field and provides both qualitative and quantitative analyses from a methodological perspective. We identified studies developing patient representations from EHRs with deep learning methods from MEDLINE, EMBASE, Scopus, the Association for Computing Machinery (ACM) Digital Library, and Institute of Electrical and Electronics Engineers (IEEE) Xplore Digital Library. After screening 363 articles, 49 papers were included for a comprehensive data collection. We noticed a typical workflow starting with feeding raw data, applying deep learning models, and ending with clinical outcome predictions as evaluations of the learned representations. Specifically, learning representations from structured EHR data was dominant (37 out of 49 studies). Recurrent Neural Networks were widely applied as the deep learning architecture (LSTM: 13 studies, GRU: 11 studies). Disease prediction was the most common application and evaluation (31 studies). Benchmark datasets were mostly unavailable (28 studies) due to privacy concerns of EHR data, and code availability was assured in 20 studies. We show the importance and feasibility of learning comprehensive representations of patient EHR data through a systematic review. Advances in patient representation learning techniques will be essential for powering patient-level EHR analyses. Future work will still be devoted to leveraging the richness and potential of available EHR data. Knowledge distillation and advanced learning techniques will be exploited to assist the capability of learning patient representation further.",0
"Abstract:  This systematic review aims to provide a comprehensive overview of deep representation learning approaches used on patient data extracted from electronic health records (EHRs). EHRs contain vast amounts of unstructured medical text that can reveal valuable insights into patient health status and treatment outcomes. However, manually extracting meaningful information from these records can be time-consuming and error-prone. Therefore, there has been increasing interest in using machine learning techniques, particularly deep neural networks, to automatically represent clinical concepts as features embedded within large volumes of medical texts contained in EHRs. This paper presents a structured synthesis of recent literature related to application of deep representation models to learn representations relevant for disease diagnosis and risk prediction. In particular, we focus on recent advances in pre-trained language models such as GPT2, DistilBERT, and RoBERTa. These models have achieved promising results in numerous natural language processing tasks, including those involving biomedical domains. Furthermore, transfer learning strategies involving fine-tuning these pre-trained models with annotated datasets derived from specific clinical cohorts can significantly improve performance compared to traditional feature extraction methods. Ultimately, our analysis highlights the significant potential of deep representation learning techniques applied to patient data obtained through EHR systems. Further research remains necessary to develop more advanced model architectures, increase dataset sizes for training and validation, ensure patient privacy and security concerns are addressed, and encourage wider adoption of machine learning technologies across diverse healthcare environments.",1
"Recently, convolutional neural networks (CNNs)-based facial landmark detection methods have achieved great success. However, most of existing CNN-based facial landmark detection methods have not attempted to activate multiple correlated facial parts and learn different semantic features from them that they can not accurately model the relationships among the local details and can not fully explore more discriminative and fine semantic features, thus they suffer from partial occlusions and large pose variations. To address these problems, we propose a cross-order cross-semantic deep network (CCDN) to boost the semantic features learning for robust facial landmark detection. Specifically, a cross-order two-squeeze multi-excitation (CTM) module is proposed to introduce the cross-order channel correlations for more discriminative representations learning and multiple attention-specific part activation. Moreover, a novel cross-order cross-semantic (COCS) regularizer is designed to drive the network to learn cross-order cross-semantic features from different activation for facial landmark detection. It is interesting to show that by integrating the CTM module and COCS regularizer, the proposed CCDN can effectively activate and learn more fine and complementary cross-order cross-semantic features to improve the accuracy of facial landmark detection under extremely challenging scenarios. Experimental results on challenging benchmark datasets demonstrate the superiority of our CCDN over state-of-the-art facial landmark detection methods.",0
"In recent years, deep learning methods have achieved significant advancements in facial landmark detection tasks due to their ability to capture intricate features from high resolution images effectively. However, most existing models focus on individual layers without considering interdependencies across different orders and semantic structures within networks. This can result in suboptimal performance when encountering extreme poses or severe occlusions which may cause inconsistencies between detected keypoints and ground truth annotations. To address these issues, we propose RobustFacialLandmarkDetectionbyCrossOrderCrossSemanticDeepNetwork(RLMCSDN). Our approach incorporates cross order connections and interleaves feature extraction among multiple levels of abstraction to encode more structured multi-level representations. Specifically, RLMCSDN employs novel modules that allow for information exchange between parallel streams for different scale spaces at each stage as well as lateral interactions among different stages in terms of both spatial domains and semantic categories. Extensive experiments were conducted on popular benchmark datasets such as 300W, COFW, HELEN, LFPWandAFLW demonstrating superior accuracy compared to state of the art approaches under challenging scenarios including large pose variations and partial occlusion cases. Furthermore, our method maintains competitive precision even at very low overlap settings. We believe that our work lays a foundation towards building robust systems capable of handling real world conditions and has applications for computer vision tasks beyond facial landmark localization.",1
"With the rapid emergence of graph representation learning, the construction of new large-scale datasets are necessary to distinguish model capabilities and accurately assess the strengths and weaknesses of each technique. By carefully analyzing existing graph databases, we identify 3 critical components important for advancing the field of graph representation learning: (1) large graphs, (2) many graphs, and (3) class diversity. To date, no single graph database offers all of these desired properties. We introduce MalNet, the largest public graph database ever constructed, representing a large-scale ontology of software function call graphs. MalNet contains over 1.2 million graphs, averaging over 17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696 families. Compared to the popular REDDIT-12K database, MalNet offers 105x more graphs, 44x larger graphs on average, and 63x the classes. We provide a detailed analysis of MalNet, discussing its properties and provenance. The unprecedented scale and diversity of MalNet offers exciting opportunities to advance the frontiers of graph representation learning---enabling new discoveries and research into imbalanced classification, explainability and the impact of class hardness. The database is publically available at www.mal-net.org.",0
"Title: ""Large Scale Dataset for Graph Representation Learning"" Abstract This study presents the development and release of a large scale dataset for use in graph representation learning research. The dataset consists of tens of thousands of graphs representing diverse domains such as social networks, citation networks, chemical compounds, and more. Each graph comes with precomputed node and edge features, making them suitable for downstream machine learning tasks such as node classification, link prediction, and clustering. In addition, we provide several baseline models and evaluation metrics for benchmarking the performance of different graph representation algorithms. Our goal in releasing this dataset is to encourage further exploration into the field of graph representation learning and facilitate advancements in this area of research. Keywords - Graph representation learning, large scale datasets, node classification, link prediction, clustering",1
"In this paper, we focus on unsupervised representation learning for skeleton-based action recognition. Existing approaches usually learn action representations by sequential prediction but they suffer from the inability to fully learn semantic information. To address this limitation, we propose a novel framework named Prototypical Contrast and Reverse Prediction (PCRP), which not only creates reverse sequential prediction to learn low-level information (e.g., body posture at every frame) and high-level pattern (e.g., motion order), but also devises action prototypes to implicitly encode semantic similarity shared among sequences. In general, we regard action prototypes as latent variables and formulate PCRP as an expectation-maximization task. Specifically, PCRP iteratively runs (1) E-step as determining the distribution of prototypes by clustering action encoding from the encoder, and (2) M-step as optimizing the encoder by minimizing the proposed ProtoMAE loss, which helps simultaneously pull the action encoding closer to its assigned prototype and perform reverse prediction task. Extensive experiments on N-UCLA, NTU 60, and NTU 120 dataset present that PCRP outperforms state-of-the-art unsupervised methods and even achieves superior performance over some of supervised methods. Codes are available at https://github.com/Mikexu007/PCRP.",0
In this paper we propose a novel framework for unsupervised action recognition that combines prototypical contrast with reverse prediction. We observe that many actions can be broken down into compositional parts called skeletons which capture the underlying structure of human movement patterns. By using these skeletons as building blocks we aim to learn prototypes of different action classes without any labeled data. Once these prototypes have been learned we use them to perform reverse predictions where instead of predicting future frames from past observations we attempt to recover the input based on the predicted sequence of frames. This allows us to quantify how well our models capture the essence of each action class enabling fine-grained analysis of model performance. Experiments conducted on two large datasets demonstrate that our approach outperforms previous methods achieving state-of-the-art results while requiring no hand-engineered features or manual annotations. Our method provides a new angle towards studying action recognition by introducing an alternative formulation through skeleton decomposition highlighting potential directions for further research.,1
"In this paper, we introduce ActBERT for self-supervised learning of joint video-text representations from unlabeled data. First, we leverage global action information to catalyze the mutual interactions between linguistic texts and local regional objects. It uncovers global and local visual clues from paired video sequences and text descriptions for detailed visual and text relation modeling. Second, we introduce an ENtangled Transformer block (ENT) to encode three sources of information, i.e., global actions, local regional objects, and linguistic descriptions. Global-local correspondences are discovered via judicious clues extraction from contextual information. It enforces the joint videotext representation to be aware of fine-grained objects as well as global human intention. We validate the generalization capability of ActBERT on downstream video-and language tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization. ActBERT significantly outperforms the state-of-the-arts, demonstrating its superiority in video-text representation learning.",0
"Title: ActBERT: Learning Global-Local Video-Text Representations Authors: Xuke Wang, Yunhan Chen, Jiaxuan Luo, Weifeng Li, Siyi Fang, Zhaowen Tang Abstract: In recent years, there has been growing interest in using deep learning models such as transformers like BERT (Bidirectional Encoder Representations from Transformers) to solve natural language processing tasks such as text classification and generation. However, these models have limited capabilities for understanding visual content. In this paper, we propose a novel approach called ActBERT that combines both global context and local features to learn joint video-text representations. By incorporating action recognition features into BERT-based architectures, our model can effectively capture both temporal dynamics and spatial dependencies between actions and text. Our experimental results on two challenging benchmarks demonstrate the effectiveness of our approach compared with state-of-the-art methods for video captioning and activity detection. This work opens up exciting new possibilities for leveraging deep learning techniques to develop more advanced multimedia systems capable of handling complex multimodal data.",1
"Intelligently reasoning about the world often requires integrating data from multiple modalities, as any individual modality may contain unreliable or incomplete information. Prior work in multimodal learning fuses input modalities only after significant independent processing. On the other hand, the brain performs multimodal processing almost immediately. This divide between conventional multimodal learning and neuroscience suggests that a detailed study of early multimodal fusion could improve artificial multimodal representations. To facilitate the study of early multimodal fusion, we create a convolutional LSTM network architecture that simultaneously processes both audio and visual inputs, and allows us to select the layer at which audio and visual information combines. Our results demonstrate that immediate fusion of audio and visual inputs in the initial C-LSTM layer results in higher performing networks that are more robust to the addition of white noise in both audio and visual inputs.",0
"In recent years, multimodal representation learning has emerged as a promising approach for processing data from multiple sources, such as images, audio, and text. One key challenge in this field is how to effectively fuse different modalities together to capture important relationships between them. This study explores the benefits of early fusion, where information from all modalities is combined at an initial stage of representation learning. We show that early fusion can significantly improve performance on downstream tasks compared to late fusion, which only combines representations after they have been learned separately for each modality. Our experiments demonstrate these improvements across several benchmark datasets and model architectures, including image classification, object detection, and question answering. Additionally, we provide insights into the mechanisms behind early fusion through analysis of attention weights and feature visualizations. Overall, our findings support the use of early fusion as a simple yet effective technique for improving multimodal representation learning.",1
"Identification of disease genes, which are a set of genes associated with a disease, plays an important role in understanding and curing diseases. In this paper, we present a biomedical knowledge graph designed specifically for this problem, propose a novel machine learning method that identifies disease genes on such graphs by leveraging recent advances in network biology and graph representation learning, study the effects of various relation types on prediction performance, and empirically demonstrate that our algorithms outperform its closest state-of-the-art competitor in disease gene identification by 24.1%. We also show that we achieve higher precision than Open Targets, the leading initiative for target identification, with respect to predicting drug targets in clinical trials for Parkinson's disease.",0
"This can serve as either an informative abstract which summarizes all results without taking a position on them nor offering any opinions about their implications: We propose relation-weighted link prediction (RWLP), a new computational method for disease gene identification that exploits known genetic interactions stored in public databases. RWLP uses these interactions to assign weights to potential links between diseases and candidate genes based on similarity scores derived from ontology terms associated with each interaction. By applying these scores to prioritize genes for further investigation, our approach achieves promising performance compared to current state-of-the-art methods. Evaluations using benchmark datasets demonstrate substantial improvements in precision, recall, F1 score, and area under the curve (AUC). Additionally, we successfully apply RWLP to identify putative disease genes for neurological disorders through analysis of human gene-disease association data extracted from Online Mendelian Inheritance in Man (OMIM) and Genome-Wide Association Study (GWAS) repositories. Our findings suggest that incorporating knowledge of genetic interactions may enhance disease gene discovery by refining ranking strategies for functional candidates. Overall, RWLP presents an effective complementary tool for identifying novel disease genes, expanding researchers’ capabilities to better address complex hereditary traits.",1
"Graph representation learning is to learn universal node representations that preserve both node attributes and structural information. The derived node representations can be used to serve various downstream tasks, such as node classification and node clustering. When a graph is heterogeneous, the problem becomes more challenging than the homogeneous graph node learning problem. Inspired by the emerging information theoretic-based learning algorithm, in this paper we propose an unsupervised graph neural network Heterogeneous Deep Graph Infomax (HDGI) for heterogeneous graph representation learning. We use the meta-path structure to analyze the connections involving semantics in heterogeneous graphs and utilize graph convolution module and semantic-level attention mechanism to capture local representations. By maximizing local-global mutual information, HDGI effectively learns high-level node representations that can be utilized in downstream graph-related tasks. Experiment results show that HDGI remarkably outperforms state-of-the-art unsupervised graph representation learning methods on both classification and clustering tasks. By feeding the learned representations into a parametric model, such as logistic regression, we even achieve comparable performance in node classification tasks when comparing with state-of-the-art supervised end-to-end GNN models.",0
"Title: Heterogeneous Deep Graph Infomax  Abstract: In recent years, deep learning has emerged as one of the most powerful tools for data analysis and prediction tasks. One area where deep learning has been particularly successful is graph-based modeling, which involves analyzing graphs or networks of interacting entities. In this work, we present a new approach called Heterogeneous Deep Graph Infomax (HDGInf), which extends traditional graph neural network architectures by incorporating heterogeneity into their design and training process. Our method allows for the integration of different types of nodes and edges in the same framework, enabling more comprehensive representations of complex systems such as social networks, biological pathways, or knowledge bases. We demonstrate that HDGInf outperforms several state-of-the art methods on several benchmark datasets, including citation, protein–protein interaction, and co-authorship networks. Additionally, we show how our approach can be used to generate meaningful insights from real-world data sets. Overall, this work represents an important step forward in understanding the structure and dynamics of complex heterogeneous systems through machine learning techniques.",1
"Advances in visual navigation methods have led to intelligent embodied navigation agents capable of learning meaningful representations from raw RGB images and perform a wide variety of tasks involving structural and semantic reasoning. However, most learning-based navigation policies are trained and tested in simulation environments. In order for these policies to be practically useful, they need to be transferred to the real-world. In this paper, we propose an unsupervised domain adaptation method for visual navigation. Our method translates the images in the target domain to the source domain such that the translation is consistent with the representations learned by the navigation policy. The proposed method outperforms several baselines across two different navigation tasks in simulation. We further show that our method can be used to transfer the navigation policies learned in simulation to the real world.",0
"In recent years there has been growing interest in unsupervised domain adaptation methods that allow robots to perform tasks despite differences in appearance between training data and test environments. Many techniques have focused on improving feature representations, while others attempt to adapt policy models directly. However, these approaches often require specialized architectures tailored to specific tasks, which can limit their applicability across different domains. We present an approach that leverages self-supervision to learn domain-agnostic representations via visual odometry (VO), enabling navigation agents trained exclusively on simulation to successfully operate in real world scenarios without any further fine tuning or model modifications. By learning through predictive regression of VO estimates we ensure our learned representations contain essential task-relevant information. Our method outperforms existing benchmarks in robot localization accuracy across three distinct datasets: EuRoC MAV, KITTI odometry sequences, and ViZDoom navigation challenges. These results demonstrate the effectiveness of our framework for unsupervised domain transfer to real environments.",1
"As a hot research topic, many multi-view clustering approaches are proposed over the past few years. Nevertheless, most existing algorithms merely take the consensus information among different views into consideration for clustering. Actually, it may hinder the multi-view clustering performance in real-life applications, since different views usually contain diverse statistic properties. To address this problem, we propose a novel Tensor-based Intrinsic Subspace Representation Learning (TISRL) for multi-view clustering in this paper. Concretely, the rank preserving decomposition is proposed firstly to effectively deal with the diverse statistic information contained in different views. Then, to achieve the intrinsic subspace representation, the tensor-singular value decomposition based low-rank tensor constraint is also utilized in our method. It can be seen that specific information contained in different views is fully investigated by the rank preserving decomposition, and the high-order correlations of multi-view data are also mined by the low-rank tensor constraint. The objective function can be optimized by an augmented Lagrangian multiplier based alternating direction minimization algorithm. Experimental results on nine common used real-world multi-view datasets illustrate the superiority of TISRL.",0
"Here we propose a novel approach for multi-view clustering that uses tensor data representation learning techniques. Specifically, our method leverages intrinsic subspace representation learning to extract relevant features from each view individually before fusing them into a final embedding space. By doing so, we aim to capture more accurate and discriminative representations of the input data for improved clustering performance. We demonstrate the effectiveness of our algorithm through extensive experiments on several benchmark datasets across different domains, showing significant improvements over state-of-the-art baselines. Our findings highlight the potential benefits of integrating tensor-based representations into multi-view clustering frameworks for addressing complex real-world applications.",1
"Although multi-view learning has made signifificant progress over the past few decades, it is still challenging due to the diffificulty in modeling complex correlations among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets), which aims to fully and flflexibly take advantage of multiple partial views. We fifirst provide a formal defifinition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the learned latent representations. For completeness, the task of learning latent multi-view representation is specififically translated to a degradation process by mimicking data transmission, such that the optimal tradeoff between consistency and complementarity across different views can be implicitly achieved. Equipped with adversarial strategy, our model stably imputes missing views, encoding information from all views for each sample to be encoded into latent representation to further enhance the completeness. Furthermore, a nonparametric classifification loss is introduced to produce structured representations and prevent overfifitting, which endows the algorithm with promising generalization under view-missing cases. Extensive experimental results validate the effectiveness of our algorithm over existing state of the arts for classifification, representation learning and data imputation.",0
"Title: ""Deep Partial Multi-View Learning""  Abstract: In recent years, deep learning has shown remarkable success in various fields such as computer vision, natural language processing, and speech recognition. However, most existing work on deep learning assumes that the training data is complete and well-sampled, which may not always hold true in practice due to missing observations or limited availability of annotated data. To address these issues, we propose a novel method called Deep Partial Multi-View Learning (DPMVL) that leverages multiple incomplete views of the same data to learn robust representations.  The core idea behind DPMVL is to utilize partially observed data across different views, each providing complementary information about the underlying task. Our approach integrates multiple partial views into a shared latent space using multi-view autoencoders, enabling joint optimization of all views and promoting better generalization under incomplete observations. We further incorporate a regularizer based on view consistency constraints to encourage representations that can be effectively reconstructed from any view.  Experimental results demonstrate the effectiveness of our proposed framework over several benchmark datasets across computer vision tasks, including image classification, object detection, and semantic segmentation. Compared against state-of-the-art baselines, our model achieves significantly improved performance even with severe data imbalances caused by missing annotations, demonstrating its resilience to real-world challenges. Overall, our contributions highlight the potential benefits of exploiting multiple partial views to enhance deep learning methods, making them more applicable to complex and diverse domains.",1
"Graph neural networks~(GNNs) apply deep learning techniques to graph-structured data and have achieved promising performance in graph representation learning. However, existing GNNs rely heavily on enough labels or well-designed negative samples. To address these issues, we propose a new self-supervised graph representation method: deep graph bootstrapping~(DGB). DGB consists of two neural networks: online and target networks, and the input of them are different augmented views of the initial graph. The online network is trained to predict the target network while the target network is updated with a slow-moving average of the online network, which means the online and target networks can learn from each other. As a result, the proposed DGB can learn graph representation without negative examples in an unsupervised manner. In addition, we summarize three kinds of augmentation methods for graph-structured data and apply them to the DGB. Experiments on the benchmark datasets show the DGB performs better than the current state-of-the-art methods and how the augmentation methods affect the performances.",0
"In this work we address the problem of representation learning on graphs by introducing a novel self-supervised method named Bootstrapped Graph Transformer (BGT). BGT learns node representations by maximizing the mutual information between different views of the graph: two randomly augmented versions of itself and random walks based on those augmentations. We validate our approach using three benchmark datasets commonly used for graph classification tasks: Cora, Citeseer, and Pubmed. Our results indicate that BGT outperforms several baselines including GCNII, SageHie, and InfoGraph. Moreover, visualization techniques show that our model produces meaningful representations that capture important relationships within nodes across all three datasets.",1
"This paper introduces a novel method for self-supervised video representation learning via feature prediction. In contrast to the previous methods that focus on future feature prediction, we argue that a supervisory signal arising from unobserved past frames is complementary to one that originates from the future frames. The rationale behind our method is to encourage the network to explore the temporal structure of videos by distinguishing between future and past given present observations. We train our model in a contrastive learning framework, where joint encoding of future and past provides us with a comprehensive set of temporal hard negatives via swapping. We empirically show that utilizing both signals enriches the learned representations for the downstream task of action recognition. It outperforms independent prediction of future and past.",0
"Deep learning has had great successes in the field of computer vision over recent years due to the ability of deep neural networks (DNNs) to learn representations from large amounts of data. One of the key challenges facing these approaches however, is that they rely on annotated training data in order to learn meaningful representations. Annotating video frames can be a highly time consuming task requiring manual intervention, which limits our capacity to scale up data collection. In addition annotators often make mistakes leading to annotation error. To overcome these limitations we propose a new approach to video representation learning using bidirectional feature prediction in conjunction with a temporal context model. Our proposed method learns spatio-temporal representations unsupervised by predicting future frame features based solely on previously observed frames. We demonstrate the effectiveness of our algorithm through quantitative evaluation showing improved performance compared to existing methods across multiple datasets including action classification and object detection while at the same time reducing data annotations required by several orders of magnitude. This study highlights the potential utility of unsupervised video representation learning for building robust DNN architectures for downstream tasks such as activity recognition and tracking applications.",1
"In this study, we present a dynamic graph representation learning model on weighted graphs to accurately predict the network capacity of connections between viewers in a live video streaming event. We propose EGAD, a neural network architecture to capture the graph evolution by introducing a self-attention mechanism on the weights between consecutive graph convolutional networks. In addition, we account for the fact that neural architectures require a huge amount of parameters to train, thus increasing the online inference latency and negatively influencing the user experience in a live video streaming event. To address the problem of the high online inference of a vast number of parameters, we propose a knowledge distillation strategy. In particular, we design a distillation loss function, aiming to first pretrain a teacher model on offline data, and then transfer the knowledge from the teacher to a smaller student model with less parameters. We evaluate our proposed model on the link prediction task on three real-world datasets, generated by live video streaming events. The events lasted 80 minutes and each viewer exploited the distribution solution provided by the company Hive Streaming AB. The experiments demonstrate the effectiveness of the proposed model in terms of link prediction accuracy and number of required parameters, when evaluated against state-of-the-art approaches. In addition, we study the distillation performance of the proposed model in terms of compression ratio for different distillation strategies, where we show that the proposed model can achieve a compression ratio up to 15:100, preserving high link prediction accuracy. For reproduction purposes, our evaluation datasets and implementation are publicly available at https://stefanosantaris.github.io/EGAD.",0
"""Ensemble models have been successfully applied to many computer vision tasks such as object detection, segmentation, and image generation. To improve performance, recent methods aim to adapt pre-trained feature extractors by fine-tuning them on specific target datasets and utilize self attention mechanisms. One approach that has shown promise in producing high quality results is graph representation learning (GRL). In GRL, graphs capture relationships among data points and provide a more expressive model for representation. However, constructing graphs manually can be difficult and computationally expensive. Recent studies address these issues by automatically evolving graphs using iterative optimization techniques. This work proposes novel methods for combining evolutionary graph representation learning (EGAD) and knowledge distillation techniques to tackle the problem of live video streaming events. We demonstrate empirically that our approach outperforms existing state of the art methods while retaining competitive accuracy. Furthermore, we investigate how different components of the proposed system impact its overall performance.""",1
"Dynamic graph representation learning strategies are based on different neural architectures to capture the graph evolution over time. However, the underlying neural architectures require a large amount of parameters to train and suffer from high online inference latency, that is several model parameters have to be updated when new data arrive online. In this study we propose Distill2Vec, a knowledge distillation strategy to train a compact model with a low number of trainable parameters, so as to reduce the latency of online inference and maintain the model accuracy high. We design a distillation loss function based on Kullback-Leibler divergence to transfer the acquired knowledge from a teacher model trained on offline data, to a small-size student model for online data. Our experiments with publicly available datasets show the superiority of our proposed model over several state-of-the-art approaches with relative gains up to 5% in the link prediction task. In addition, we demonstrate the effectiveness of our knowledge distillation strategy, in terms of number of required parameters, where Distill2Vec achieves a compression ratio up to 7:100 when compared with baseline approaches. For reproduction purposes, our implementation is publicly available at https://stefanosantaris.github.io/Distill2Vec.",0
"In modern machine learning research, knowledge distillation has emerged as a powerful technique for enabling data augmentation using soft targets. However, existing approaches mainly focus on static graph representation learning. To address this gap, we propose DistilGloVe, a dynamic model that can adaptively learn from pretrained representations and distilled knowledge during training. Our method leverages variational inference and Monte Carlo sampling to model uncertainty and generate diverse outputs conditioned on different soft targets. We validate our approach using extensive experiments across multiple datasets and demonstrate superior performance compared to state-of-the-art methods. This work presents novel insights into understanding the benefits of incorporating dynamics into graph representation learning with knowledge distillation.",1
"In self-supervised learning, a system is tasked with achieving a surrogate objective by defining alternative targets on a set of unlabeled data. The aim is to build useful representations that can be used in downstream tasks, without costly manual annotation. In this work, we propose a novel self-supervised formulation of relational reasoning that allows a learner to bootstrap a signal from information implicit in unlabeled data. Training a relation head to discriminate how entities relate to themselves (intra-reasoning) and other entities (inter-reasoning), results in rich and descriptive representations in the underlying neural network backbone, which can be used in downstream tasks such as classification and image retrieval. We evaluate the proposed method following a rigorous experimental procedure, using standard datasets, protocols, and backbones. Self-supervised relational reasoning outperforms the best competitor in all conditions by an average 14% in accuracy, and the most recent state-of-the-art model by 3%. We link the effectiveness of the method to the maximization of a Bernoulli log-likelihood, which can be considered as a proxy for maximizing the mutual information, resulting in a more efficient objective with respect to the commonly used contrastive losses.",0
"This should describe in some detail what was done and why you did it without talking down to the reader so they won’t skip through your work as fast as possible looking for results: In our recent publication on self-supervised relational reasoning (SSRR) for representation learning, we aimed to develop a framework that could leverage unlabelled data to learn representations capable of encoding rich relationships among entities. By doing so, we sought to address the well-documented challenge posed by traditional self-supervised learning models, which often fail to capture meaningful structure across large datasets due to their reliance on pretext tasks based solely on individual examples or isolated features. Our SSRR approach thus centres on designing pretext tasks that explicitly require models to reason about complex relations connecting multiple inputs together. Building upon earlier studies establishing deep neural networks can indeed perform rudimentary forms of analogy-making at scale, we proposed several novel pretext tasks tailored towards more intricate relationship types frequently encountered within knowledge graphs such as transitive closure, graph convolutions, and path navigation. We then constructed benchmark datasets where human annotators verified correctness for over one million unique problems spanning diverse domains like natural language understanding, computer vision, and commonsense inference. Experiments conducted on thirteen different datasets measuring standard proxy metrics (e.g., accuracy, F1 score, normalized mutual information) consistently demonstrated our SSR models achieved significant improvements compared against strong baselines trained under supervision or alternative self-supervisi",1
"De novo genome assembly focuses on finding connections between a vast amount of short sequences in order to reconstruct the original genome. The central problem of genome assembly could be described as finding a Hamiltonian path through a large directed graph with a constraint that an unknown number of nodes and edges should be avoided. However, due to local structures in the graph and biological features, the problem can be reduced to graph simplification, which includes removal of redundant information. Motivated by recent advancements in graph representation learning and neural execution of algorithms, in this work we train the MPNN model with max-aggregator to execute several algorithms for graph simplification. We show that the algorithms were learned successfully and can be scaled to graphs of sizes up to 20 times larger than the ones used in training. We also test on graphs obtained from real-world genomic data---that of a lambda phage and E. coli.",0
"In this work we present a new method for assembling the full neural connectome from microscopic images, using state of the art deep learning techniques on noisy data and without any manual segmentation. This technique overcomes some of the current challenges that have been hindering progress in mapping out neural networks at scale, paving the way for more comprehensive studies into how brains process information, communicate and learn. By providing high quality reconstructions quickly and efficiently, our approach helps bridge the gap between small laboratory experiments and large populations, supporting neuroscience efforts aimed at understanding brain development and disease progression as well as improving diagnosis and treatment protocols.",1
"Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning.   Project Page: https://ssnl.github.io/hypersphere   Code: https://github.com/SsnL/align_uniform , https://github.com/SsnL/moco_align_uniform",0
"This paper investigates the fundamental principles that underlie contrastive representation learning (CRL). We begin by examining two key properties: alignment and uniformity, which we argue play crucial roles in the effectiveness of CRL algorithms. To study these properties, we propose several novel methods based on the hypersphere framework, including three different approaches for aligning representations using orthogonal projections onto a high-dimensional surface, as well as a new method for enforcing uniformity constraints during training. Our experiments demonstrate the importance of both alignment and uniformity in achieving good results in contrastive learning tasks across multiple datasets, architectures, and parameters settings. Overall, our work sheds light on the underlying mechanisms driving the success of CRL and provides concrete solutions for practitioners seeking to improve their models' performance.",1
"To date, research on sensor-equipped mobile devices has primarily focused on the purely supervised task of human activity recognition (walking, running, etc), demonstrating limited success in inferring high-level health outcomes from low-level signals, such as acceleration. Here, we present a novel self-supervised representation learning method using activity and heart rate (HR) signals without semantic labels. With a deep neural network, we set HR responses as the supervisory signal for the activity data, leveraging their underlying physiological relationship.   We evaluate our model in the largest free-living combined-sensing dataset (comprising more than 280,000 hours of wrist accelerometer & wearable ECG data) and show that the resulting embeddings can generalize in various downstream tasks through transfer learning with linear classifiers, capturing physiologically meaningful, personalized information. For instance, they can be used to predict (higher than 70 AUC) variables associated with individuals' health, fitness and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers. Overall, we propose the first multimodal self-supervised method for behavioral and physiological data with implications for large-scale health and lifestyle monitoring.",0
Developed by Open Assistant,1
"Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at https://github.com/Randl/kmeans_selfsuper",0
"This paper presents a novel approach to large-scale unsupervised image clustering using self-supervised learning. Traditional approaches to clustering have relied on handcrafted features and explicit models of similarity, which can be difficult to apply at scale and may require substantial amounts of manual effort. In contrast, our method uses self-supervised pre-training to automatically learn deep representations that capture meaningful cluster structure in raw pixel data. We then use these learned representations as input to off-the-shelf clustering algorithms, achieving state-of-the-art performance without any direct supervision. Our experimental results demonstrate that our method outperforms prior work across a range of datasets and evaluation metrics, suggesting that self-supervised learning holds great promise for solving complex problems in computer vision. By providing a simple and effective solution for large-scale image clustering, we hope to enable new applications in areas such as image retrieval, classification, and generative modeling.",1
"Human drivers produce a vast amount of data which could, in principle, be used to improve autonomous driving systems. Unfortunately, seemingly straightforward approaches for creating end-to-end driving models that map sensor data directly into driving actions are problematic in terms of interpretability, and typically have significant difficulty dealing with spurious correlations. Alternatively, we propose to use this kind of action-based driving data for learning representations. Our experiments show that an affordance-based driving model pre-trained with this approach can leverage a relatively small amount of weakly annotated imagery and outperform pure end-to-end driving models, while being more interpretable. Further, we demonstrate how this strategy outperforms previous methods based on learning inverse dynamics models as well as other methods based on heavy human supervision (ImageNet).",0
"This project proposes two new techniques which enable large scale datasets to provide more detailed evaluations of autonomous driving systems: action based representation learning (ABRL), as well as behavior cloning using real-time crowdsourced demonstration data via natural language instructions (RCLL). ABRL learns from demonstrator trajectories by segmenting them into actions (e.g., ""go straight,"" turn left""), allowing better generalization across scenarios and reducing overfitting to specific instances within each scenario. By doing so, ABRL makes efficient use of limited training data and produces representations robust enough to drive in previously unseen environments without further fine tuning. RCLL uses NLG prompts from the OpenAI InstructGPT model to directly obtain human demonstrated behavior that can then be used to train agents on existing datasets. We demonstrate the effectiveness of both approaches by integrating them into popular baseline models such as TORC and Apollo, leading to improved performance in simulation benchmarks. While these results show promise, future work must evaluate these methods in live road tests and compare against alternative autonomous vehicles. Our hope is that incorporating these novel evaluation schemes in developing self driving cars will allow safer, more reliable, and lower cost designs.",1
"Graph neural network (GNN) has recently been established as an effective representation learning framework on graph data. However, the popular message passing models rely on local permutation invariant aggregate functions, which gives rise to the concerns about their representational power. Here, we introduce the concept of automorphic equivalence to theoretically analyze GNN's expressiveness in differentiating node's structural role. We show that the existing message passing GNNs have limitations in learning expressive representations. Moreover, we design a novel GNN class that leverages learnable automorphic equivalence filters to explicitly differentiate the structural roles of each node's neighbors, and uses a squeeze-and-excitation module to fuse various structural information. We theoretically prove that the proposed model is expressive in terms of generating distinct representations for nodes with different structural feature. Besides, we empirically validate our model on eight real-world graph data, including social network, e-commerce co-purchase network and citation network, and show that it consistently outperforms strong baselines.",0
"This paper presents a novel approach for improving graph neural networks using automorphic equivalence filters (AEFs). AEFs are mathematical functions that transform data into different representations while preserving certain symmetries in the input space, such as reflection, rotation, scaling, and permutation symmetry. These filters can improve the robustness of machine learning models by making them more invariant to specific types of transformations, which helps reduce overfitting and increases generalization performance. In this work, we apply AEFs to the graph convolutional layers of GNNs and show how they can enhance their expressive power without increasing model complexity. We evaluate our method on several benchmark datasets and demonstrate significant improvements compared to state-of-the-art approaches. Our results indicate that incorporating AEFs into GNN architectures has the potential to significantly boost their performance across a wide range of tasks. This study paves the way for future research exploring the use of these filters in deep learning systems beyond graph neural networks. Overall, the paper offers an original contribution to the field of graph representation learning and demonstrates the benefits of using AEFs to enhance the expressiveness and efficiency of modern GNNs.",1
"To fully exploit the performance potential of modern multi-core processors, machine learning and data mining algorithms for big data must be parallelized in multiple ways. Today's CPUs consist of multiple cores, each following an independent thread of control, and each equipped with multiple arithmetic units which can perform the same operation on a vector of multiple data objects. Graph embedding, i.e. converting the vertices of a graph into numerical vectors is a data mining task of high importance and is useful for graph drawing (low-dimensional vectors) and graph representation learning (high-dimensional vectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by Minimizing the Predictive Entropy), an information-theoretic method which can generate low and high-dimensional vectors. MulticoreGEMPE applies MIMD (Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single Instructions Multiple Data, using AVX-512) parallelism. We propose general ideas applicable in other graph-based algorithms like \emph{vectorized hashing} and \emph{vectorized reduction}. Our experimental evaluation demonstrates the superiority of our approach.",0
"Artificial intelligence has made significant strides over recent years thanks largely to advancements in deep learning algorithms. As these systems have grown increasingly complex, the amount and diversity of data they consume continues to rise as well. To optimize their performance on such massive datasets, researchers rely heavily on parallel computing techniques that leverage distributed computing frameworks like Hadoop and Spark. However, while these methods work well for many types of data analyses, graph analysis presents unique challenges due to the high degree of connectivity between nodes. Consequently, specialized tools like Pregel and GraphX were developed specifically to handle graph computations on large-scale clusters. Despite these improvements, there remains a need for better understanding of how to represent graphs and design efficient algorithms for drawing them under resource constraints. This paper contributes towards addressing these shortcomings by introducing novel approaches based on massively parallel processing architectures that enable scalable representation and visualization of graphs containing millions of edges. In addition, we present new results on tradeoffs between algorithmic efficiency and graph quality measures like crossing minimization, which is critical for producing clear and comprehensible diagrams. Our experimental evaluation across real-world networks demonstrates the effectiveness of our methods compared to state-of-the-art baselines. Overall, this study provides valuable insights into developing more effective solutions for handling big graph data in emerging AI applications.",1
"Graph representation learning is a fundamental task in various applications that strives to learn low-dimensional embeddings for nodes that can preserve graph topology information. However, many existing methods focus on static graphs while ignoring evolving graph patterns. Inspired by the success of graph convolutional networks(GCNs) in static graph embedding, we propose a novel k-core based temporal graph convolutional network, the CTGCN, to learn node representations for dynamic graphs. In contrast to previous dynamic graph embedding methods, CTGCN can preserve both local connective proximity and global structural similarity while simultaneously capturing graph dynamics. In the proposed framework, the traditional graph convolution is generalized into two phases, feature transformation and feature aggregation, which gives the CTGCN more flexibility and enables the CTGCN to learn connective and structural information under the same framework. Experimental results on 7 real-world graphs demonstrate that the CTGCN outperforms existing state-of-the-art graph embedding methods in several tasks, including link prediction and structural role classification. The source code of this work can be obtained from \url{https://github.com/jhljx/CTGCN}.",0
"We present K-core based Temporal Graph Convolutional Network (TGCN), a novel framework that can effectively capture dynamic graph signals on large temporal graphs consisting of both static (structural) and time-varying (attribute) edges. TCGN utilizes spatio-temporal filters derived from k-cores to aggregate and propagate features across nodes in the graph while preserving important global topological properties like connectivity. In particular, we show how by selectively aggregating over higher order connections and exploiting nonlinear dependencies amongst neighboring feature vectors at different timescales, our approach outperforms several benchmark methods for node classification tasks on real-world datasets such as social network analysis and sensor data prediction. Our results demonstrate that TGCN provides a principled methodology for capturing complex relationships amongst entities across both space and time.",1
"State representation learning, or the ability to capture latent generative factors of an environment, is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations without supervision from rewards is a challenging open problem. We introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state variables. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods. The code associated with this work is available at https://github.com/mila-iqia/atari-representation-learning",0
"This is the proposed project description:  Project Title: Understanding State Representations in Reinforcement Learning  Overview: One challenge facing RL algorithms in complex problems is their tendency towards poor exploration efficiency. Poor representation learning can play a significant role here; even if only parts of state space matter for solving tasks, most conventional methods learn representations that encode all states identically. Here we propose to study how unsupervised approaches to state representation learning can improve both sample complexity and final performance on several challenging continuous control Atari games. Our core contribution is a novel algorithm that learns a generative model of images given raw game observations which encodes high level task relevant features without supervision. We train our agent using standard Proximal Policy Optimization (PPO) combined with this new state representation learned via VAEs. For comparison we consider traditional low dimensional linear function approximators as well as dense visual CNNs.  Hypothesis: Improving over baseline agents trained without unsupervised pretraining, we believe use of these unsupervised feature learning techniques should produce significantly higher performing agents across multiple domains. We further hypothesize that the VAE method provides better generalization than other unsupervised image generation models due to regularizing effect from prior.  Approach: First we focus on improving training stability through latent variable manipulation in the presence of PPO updates (Sec 2). Then we evaluate the impact of unsupervised pretraining against no pretraining and the two alternative architectures mentioned above under three different metrics (final score after environment interaction, average return during training episode rollouts, full distribution evaluation by Monte Carlo rollout); Sec 4 contains results on half of the Atari suite and Sec 5 has remaining analyses on complete set (Table 1). To aid interpretation, we conduct ablations testing impact of design choices such as KL weight and reconstruction loss type (Sec 3). Finally we conclude with discussion of open questions and future directions stemming from our findings (Sec6). In Appendices we provide more details on implementation of architecture and hyperparameters used for training.  Notes: Due to computationa",1
"Graph representation learning is of paramount importance for a variety of graph analytical tasks, ranging from node classification to community detection. Recently, graph convolutional networks (GCNs) have been successfully applied for graph representation learning. These GCNs generate node representation by aggregating features from the neighborhoods, which follows the ""neighborhood aggregation"" scheme. In spite of having achieved promising performance on various tasks, existing GCN-based models have difficulty in well capturing complicated non-linearity of graph data. In this paper, we first theoretically prove that coefficients of the neighborhood interacting terms are relatively small in current models, which explains why GCNs barely outperforms linear models. Then, in order to better capture the complicated non-linearity of graph data, we present a novel GraphAIR framework which models the neighborhood interaction in addition to neighborhood aggregation. Comprehensive experiments conducted on benchmark tasks including node classification and link prediction using public datasets demonstrate the effectiveness of the proposed method.",0
"In recent years, graph representation learning has become increasingly important due to the growing need to model complex relationships among data points. One key challenge in this area is how to effectively capture the structural information present in graphs while also encoding relevant features from their node attributes. To address this issue, we propose a novel approach called GraphAIR that leverages neighborhood aggregation and interaction to learn meaningful representations of graphs. Our method models the interactions between nodes using attention mechanisms and then aggregates these interactions at different scales to produce robust embeddings. We evaluate our method on several benchmark datasets and demonstrate its effectiveness compared to state-of-the-art baselines. Overall, our work provides new insights into graph representation learning and opens up exciting opportunities for future research in this field.",1
"Progress in the field of machine learning has been fueled by the introduction of benchmark datasets pushing the limits of existing algorithms. Enabling the design of datasets to test specific properties and failure modes of learning algorithms is thus a problem of high interest, as it has a direct impact on innovation in the field. In this sense, we introduce Synbols -- Synthetic Symbols -- a tool for rapidly generating new datasets with a rich composition of latent features rendered in low resolution images. Synbols leverages the large amount of symbols available in the Unicode standard and the wide range of artistic font provided by the open font community. Our tool's high-level interface provides a language for rapidly generating new distributions on the latent features, including various types of textures and occlusions. To showcase the versatility of Synbols, we use it to dissect the limitations and flaws in standard learning algorithms in various learning setups including supervised learning, active learning, out of distribution generalization, unsupervised representation learning, and object counting.",0
"Artificial learning algorithms learn from data that they receive as inputs, whether those inputs come from humans, cameras on robots, sensors attached to buildings or other sources. Often these inputs are unstructured—in other words, there’s no predefined meaning behind them—and so machine learning models have to develop their own understanding through trial and error. But how good can we expect such models to be at extracting meaningful signals from the messy world? In practice, performance depends crucially on both model quality and training data quality. We can usually only influence the former by tweaking parameters (e.g., changing weights inside neural nets), while improving the latter requires obtaining better inputs via collecting more diverse real-world datasets. Here, I describe a new technique called symbols probing where instead of providing black boxes like actual input images, one could ask a learned model to generate images with certain characteristics on demand—i.e., to think up some objects that are blue squares or green rectangles!—even if it hasn’t seen any examples of squares, rectangles, blueness or greenness before. The resulting responses demonstrate strong generalization without prior exposure to these objects, implying that these symbols don’t rely upon concrete experiences but rather capture universal features of the visual environment. The simplicity of symbol probes allows us to study how artificial minds work “under the hood”, exploring previously unknown aspects of deep learning models. Moreover, our findings pave the path towards training systems explicitly encouraged to hallucinate synthetic representations of reality during self-supervised training stages, ultimately leading to even greater generalization abilities than those achieved today. As a proof o",1
"Data-driven graph learning models a network by determining the strength of connections between its nodes. The data refers to a graph signal which associates a value with each graph node. Existing graph learning methods either use simplified models for the graph signal, or they are prohibitively expensive in terms of computational and memory requirements. This is particularly true when the number of nodes is high or there are temporal changes in the network. In order to consider richer models with a reasonable computational tractability, we introduce a graph learning method based on representation learning on graphs. Representation learning generates an embedding for each graph node, taking the information from neighbouring nodes into account. Our graph learning method further modifies the embeddings to compute the graph similarity matrix. In this work, graph learning is used to examine brain networks for brain state identification. We infer time-varying brain graphs from an extensive dataset of intracranial electroencephalographic (iEEG) signals from ten patients. We then apply the graphs as input to a classifier to distinguish seizure vs. non-seizure brain states. Using the binary classification metric of area under the receiver operating characteristic curve (AUC), this approach yields an average of 9.13 percent improvement when compared to two widely used brain network modeling methods.",0
"Abstract:  Learning from data is a crucial task that involves extracting meaningful insights and patterns from large amounts of input. In many applications, this can involve analyzing graph structures where nodes represent objects and edges describe their relationships. For example, in neuroscience research, understanding brain state identification requires studying complex networks of neural activity patterns.  This work proposes a novel approach to learning from graphs based on node-centric representations. By considering each node as a separate entity, we can better capture localized features and interdependencies between neighboring regions. Our method utilizes multiple layers of convolutional neural networks (CNNs) to process node embeddings and learn high-level abstractions. This allows us to identify different brain states while taking into account spatial dependencies within the network.  Our experiments demonstrate that our node-centric graph learning algorithm outperforms traditional approaches on two benchmark datasets. Additionally, we showcase its potential use case in neuroscientific applications by applying it to human electroencephalography (EEG) recordings. These results suggest that our framework could provide valuable insights for mapping brain states in both healthy subjects and patient populations. We hope that our work stimulates further exploration into graph representation learning and its impact across diverse domains.",1
"Invertible neural networks based on coupling flows (CF-INNs) have various machine learning applications such as image synthesis and representation learning. However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their representation power: are CF-INNs universal approximators for invertible functions? Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain affine coupling and invertible linear functions as special cases. As its corollary, we can affirmatively resolve a previously unsolved problem: whether normalizing flow models based on affine coupling can be universal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself.",0
"Deep learning has revolutionized many fields through its ability to learn complex patterns from large datasets. However, most deep neural networks (DNN) have limitations that prevent them from accurately modeling certain types of data due to their inherent fixed topology. This leads to difficulties in representing complex transformations such as diffeomorphisms which are widely used across science and engineering disciplines. Therefore, there exists a need for universal approximators that can approximate these kinds of functions with high accuracy.  In this work, we propose coupling-based invertible neural networks (CBINN), a new architecture based on existing invertible models like flow-based models, to serve as universal diffeomorphism approximators. We show theoretically under mild assumptions that CBINN class achieves universality by explicitly constructing examples of diffeomorphic maps approximated arbitrarily closely by a CBINN in our framework. Furthermore, we provide extensive numerical evidence that demonstrates the efficacy of our proposed method over state-of-the-art alternatives. Our results demonstrate that CBINN achieve higher approximation quality using less parameters than prior art models across multiple experiments including image registration, warping time-series signals and diffeomorphic mapping of shapes. These findings suggest that coupling-based invertible neural networks could serve as powerful tools for scientific applications requiring diffeomorphism approximations across various domains. Lastly, as invertible architectures benefit from a more straightforward interpretation in terms of geometric structure preservation than non-invertible counterparts, our results open up exciting possibilities in pushing forward theoretical understanding of machine learning in this promising area.  To summarize, our work presents a novel architecture for deep learning capable of approximating arbitrary smooth transformations known as diffeomorphisms. Our proposed coupling-based invertible neural network",1
"Finding well-defined clusters in data represents a fundamental challenge for many data-driven applications, and largely depends on good data representation. Drawing on literature regarding representation learning, studies suggest that one key characteristic of good latent representations is the ability to produce semantically mixed outputs when decoding linear interpolations of two latent representations. We propose the Mixing Consistent Deep Clustering method which encourages interpolations to appear realistic while adding the constraint that interpolations of two data points must look like one of the two inputs. By applying this training method to various clustering (non-)specific autoencoder models we found that using the proposed training method systematically changed the structure of learned representations of a model and it improved clustering performance for the tested ACAI, IDEC, and VAE models on the MNIST, SVHN, and CIFAR-10 datasets. These outcomes have practical implications for numerous real-world clustering tasks, as it shows that the proposed method can be added to existing autoencoders to further improve clustering performance.",0
"Title: Mixing Consistency Regularization for Unsupervised Learning: Theory & Applications Authors: Yuekai Sun (Cornell), Zhou Ren (Tsinghua University) & Xuelong Li (Microsoft Research Asia). Presented by Dr. Hao Shen at NIPS Workshop on Machine Learning Theory, New Orleans, Louisiana Dec. 8, 2019.",1
"Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time. In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or Personalized Page Rank diffusion matrix) to best suit the task at hand. We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on ogbn-papers100M, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges.",0
"Deep convolutional neural networks have shown great promise in computer vision tasks, but their high computational cost can make them difficult to use on large datasets or real-time applications such as those found in autonomous vehicles or drones. To address these challenges, we propose a new architecture called SIGNN (Scalable Inception Graph Neural Network), which combines ideas from graph neural networks and inception modules to create a scalable network that balances accuracy and efficiency. Our approach uses a unique combination of residual connections and spatial dilations to reduce computation requirements while maintaining performance. We evaluate our method using multiple benchmark datasets and demonstrate state-of-the-art results on several important visual recognition tasks, including object detection, semantic segmentation, and classification. Overall, our work represents a significant advance in the field of deep learning for computer vision, offering a promising path toward building efficient models capable of handling complex image analysis problems at scale.",1
"Recently, there has been a surge of interest in representation learning in hyperbolic spaces, driven by their ability to represent hierarchical data with significantly fewer dimensions than standard Euclidean spaces. However, the viability and benefits of hyperbolic spaces for downstream machine learning tasks have received less attention. In this paper, we present, to our knowledge, the first theoretical guarantees for learning a classifier in hyperbolic rather than Euclidean space. Specifically, we consider the problem of learning a large-margin classifier for data possessing a hierarchical structure. Our first contribution is a hyperbolic perceptron algorithm, which provably converges to a separating hyperplane. We then provide an algorithm to efficiently learn a large-margin hyperplane, relying on the careful injection of adversarial examples. Finally, we prove that for hierarchical data that embeds well into hyperbolic space, the low embedding dimension ensures superior guarantees when learning the classifier directly in hyperbolic space.",0
"Deep learning has proven effective at solving many complex problems, but often requires large amounts of data and computational resources. In this work, we present a novel approach that combines deep learning and robust optimization techniques to learn models in hyperbolic space that achieve high accuracy with less data and computation. We demonstrate the effectiveness of our method on several benchmark datasets and show that it outperforms state-of-the-art methods while requiring fewer resources. Our approach has implications for applications where resources are limited, such as on edge devices or in real-time systems.",1
"We introduce a novel self-supervised pretext task for learning representations from audio-visual content. Prior work on audio-visual representation learning leverages correspondences at the video level. Approaches based on audio-visual correspondence (AVC) predict whether audio and video clips originate from the same or different video instances. Audio-visual temporal synchronization (AVTS) further discriminates negative pairs originated from the same video instance but at different moments in time. While these approaches learn high-quality representations for downstream tasks such as action recognition, their training objectives disregard spatial cues naturally occurring in audio and visual signals. To learn from these spatial cues, we tasked a network to perform contrastive audio-visual spatial alignment of 360{\deg} video and spatial audio. The ability to perform spatial alignment is enhanced by reasoning over the full spatial content of the 360{\deg} video using a transformer architecture to combine representations from multiple viewpoints. The advantages of the proposed pretext task are demonstrated on a variety of audio and visual downstream tasks, including audio-visual correspondence, spatial alignment, action recognition, and video semantic segmentation.",0
"This research focuses on developing methods that can learn representations from audio-visual spatial alignment. In today's world, multimedia data such as images and videos have become increasingly available, making it essential to develop algorithms that can effectively process and analyze them. However, analyzing audio-visual data has proven to be challenging due to their different modalities, which makes it difficult to establish correspondence between them. Previous works in this field have used simple baseline models and limited annotations, but they have failed to provide accurate alignments. Our work proposes a novel approach based on adversarial training that leverages temporal constraints along with pixelwise attention mechanisms to establish robust correspondences between audio and visual streams. We evaluate our method using standard benchmark datasets and demonstrate its superior performance over state-of-the-art approaches. Our results showcase the effectiveness of learning representations from audio-visual spatial alignment, laying the foundation for future research in multimedia analysis and content understanding.",1
"Unsupervised learning methods based on contrastive learning have drawn increasing attention and achieved promising results. Most of them aim to learn representations invariant to instance-level variations, which are provided by different views of the same instance. In this paper, we propose Invariance Propagation to focus on learning representations invariant to category-level variations, which are provided by different instances from the same category. Our method recursively discovers semantically consistent samples residing in the same high-density regions in representation space. We demonstrate a hard sampling strategy to concentrate on maximizing the agreement between the anchor sample and its hard positive samples, which provide more intra-class variations to help capture more abstract invariance. As a result, with a ResNet-50 as the backbone, our method achieves 71.3% top-1 accuracy on ImageNet linear classification and 78.2% top-5 accuracy fine-tuning on only 1% labels, surpassing previous results. We also achieve state-of-the-art performance on other downstream tasks, including linear classification on Places205 and Pascal VOC, and transfer learning on small scale datasets.",0
"Unsupervised representation learning is a field that seeks to find patterns and meaning in data without explicit guidance from human supervision. One approach to unsupervised representation learning is through a method called invariance propagation, which aims to identify underlying structures in the data that remain consistent across different transformations. This paper presents a detailed investigation into how invariance propagation can be used to learn robust representations of complex datasets, and shows that under certain conditions it leads to solutions equivalent to those derived from more traditional methods such as principal component analysis. Through extensive experiments on synthetic and real-world data sets, we demonstrate the effectiveness and scalability of our proposed approach compared to other state-of-the-art techniques. Our results highlight the potential of using invariance propagation as a general tool for discovering structure in large-scale data sets, particularly for applications where hand engineering features may not be feasible.",1
"Due to the widespread applications in real-world scenarios, metro ridership prediction is a crucial but challenging task in intelligent transportation systems. However, conventional methods either ignore the topological information of metro systems or directly learn on physical topology, and cannot fully explore the patterns of ridership evolution. To address this problem, we model a metro system as graphs with various topologies and propose a unified Physical-Virtual Collaboration Graph Network (PVCGN), which can effectively learn the complex ridership patterns from the tailor-designed graphs. Specifically, a physical graph is directly built based on the realistic topology of the studied metro system, while a similarity graph and a correlation graph are built with virtual topologies under the guidance of the inter-station passenger flow similarity and correlation. These complementary graphs are incorporated into a Graph Convolution Gated Recurrent Unit (GC-GRU) for spatial-temporal representation learning. Further, a Fully-Connected Gated Recurrent Unit (FC-GRU) is also applied to capture the global evolution tendency. Finally, we develop a Seq2Seq model with GC-GRU and FC-GRU to forecast the future metro ridership sequentially. Extensive experiments on two large-scale benchmarks (e.g., Shanghai Metro and Hangzhou Metro) well demonstrate the superiority of our PVCGN for station-level metro ridership prediction. Moreover, we apply the proposed PVCGN to address the online origin-destination (OD) ridership prediction and the experiment results show the universality of our method. Our code and benchmarks are available at https://github.com/HCPLab-SYSU/PVCGN.",0
"This paper proposes a physical-virtual collaboration model (PVCM) that leverages both real-world data from metro systems as well as digital data generated by riders through their mobile devices. By combining these sources of data, PVCM aims to improve intra-station metro ridership prediction and allow inter-station comparisons, enabling better understanding of passenger behavior patterns across different stations. The proposed framework involves several steps: acquiring raw data via WiFi access points, GPS sensors, and mobile applications; preprocessing the data using filtering techniques and anomaly detection methods; then applying machine learning algorithms such as k-nearest neighbors and decision trees for predictive modelling. Experiments conducted on two real datasets demonstrate the effectiveness of our approach compared with traditional methods. Overall, PVCM has great potential to enhance the efficiency of public transportation management and services planning.",1
"Recent works found that fine-tuning and joint training---two popular approaches for transfer learning---do not always improve accuracy on downstream tasks. First, we aim to understand more about when and why fine-tuning and joint training can be suboptimal or even harmful for transfer learning. We design semi-synthetic datasets where the source task can be solved by either source-specific features or transferable features. We observe that (1) pre-training may not have incentive to learn transferable features and (2) joint training may simultaneously learn source-specific features and overfit to the target. Second, to improve over fine-tuning and joint training, we propose Meta Representation Learning (MeRLin) to learn transferable features. MeRLin meta-learns representations by ensuring that a head fit on top of the representations with target training data also performs well on target validation data. We also prove that MeRLin recovers the target ground-truth model with a quadratic neural net parameterization and a source distribution that contains both transferable and source-specific features. On the same distribution, pre-training and joint training provably fail to learn transferable features. MeRLin empirically outperforms previous state-of-the-art transfer learning algorithms on various real-world vision and NLP transfer learning benchmarks.",0
"In order to apply meta learning techniques such as MAML (Model Agnostic Meta Learning) that learn generalizable representations across multiple tasks and datasets, it is typically assumed that several source domains are available which share some overlap with the target domain where performance should improve. However, many real world applications have only one labelled dataset available for fine tuning their models on novel unseen data from similar but possibly different distributions. This can make transferring knowledge gained through training from other related domains challenging without any prior adaptation to new data. Our method introduces a simple yet effective neural network architecture called iMAML (Iterative Model Agnostic Meta Learning), capable of meta learning transferable representations even with a single labeled target domain alone using self supervision via cycle consistency constraints. We demonstrate state-of-the art results across image classification benchmarks using CIFAR-10, STL-10, Vinyasa, SVHN and ImageNet as well as large scale semi supervised learning benchmarks including TinyImageNet and CUB200-2011. To evaluate our approach we additionally introduce a variant called iMIMIC that uses self attention over pixels constrained by spatial smoothness priors instead of pixel level cycle consistency and achieve comparable results. Lastly, we provide an analysis into how our model modifies the feature space during initial and meta learning updates and ablations across hyperparameters. Overall, these results indicate that our approach provides a promising direction towards improving few shot learning capabilities for common real world scenarios that lack diverse annotated data sources.",1
"We conduct the first study of its kind to generate and evaluate vector representations for chess pieces. In particular, we uncover the latent structure of chess pieces and moves, as well as predict chess moves from chess positions. We share preliminary results which anticipate our ongoing work on a neural network architecture that learns these embeddings directly from supervised feedback.",0
"In recent years, there has been increasing interest in using deep learning techniques to model complex board games such as chess. One approach that has shown promising results is the use of vector representations, which can capture key features of game positions and provide a powerful tool for analyzing and predicting moves. This paper introduces Chess2vec, a novel methodology for learning vector representations for chess. By training on a large dataset of expert human games and using a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs), Chess2vec is able to effectively represent positional information at multiple time scales and encode it into high-dimensional vectors. These learned representations can then be used to perform tasks such as move prediction, evaluation functions, and game tree search. Our experimental results show that Chess2vec achieves state-of-the-art performance compared to existing methods, demonstrating the effectiveness of our proposed approach. Overall, Chess2vec represents a significant advance in the field of chess analysis using machine learning techniques.",1
"Representation learning from 3D point clouds is challenging due to their inherent nature of permutation invariance and irregular distribution in space. Existing deep learning methods follow a hierarchical feature extraction paradigm in which high-level abstract features are derived from low-level features. However, they fail to exploit different granularity of information due to the limited interaction between these features. To this end, we propose Multi-Abstraction Refinement Network (MARNet) that ensures an effective exchange of information between multi-level features to gain local and global contextual cues while effectively preserving them till the final layer. We empirically show the effectiveness of MARNet in terms of state-of-the-art results on two challenging tasks: Shape classification and Coarse-to-fine grained semantic segmentation. MARNet significantly improves the classification performance by 2% over the baseline and outperforms the state-of-the-art methods on semantic segmentation task.",0
"In this research paper we present MARNet, which stands for Mult",1
"Potential-based reward shaping provides an approach for designing good reward functions, with the purpose of speeding up learning. However, automatically finding potential functions for complex environments is a difficult problem (in fact, of the same difficulty as learning a value function from scratch). We propose a new framework for learning potential functions by leveraging ideas from graph representation learning. Our approach relies on Graph Convolutional Networks which we use as a key ingredient in combination with the probabilistic inference view of reinforcement learning. More precisely, we leverage Graph Convolutional Networks to perform message passing from rewarding states. The propagated messages can then be used as potential functions for reward shaping to accelerate learning. We verify empirically that our approach can achieve considerable improvements in both small and high-dimensional control problems.",0
"In summary, the proposed model combines graph convolutional networks (GCN) with reward shaping to improve the performance and stability of reinforcement learning agents. By incorporating spatial dependencies into the reward propagation process, our approach reduces uncertainty and enables more accurate predictions of future rewards. Additionally, we introduce a regularization term that promotes smoothness of Q-values across neighboring states, which further stabilizes training and improves generalization. Through extensive experiments on benchmark domains, we demonstrate that our method outperforms state-of-the-art techniques while requiring fewer environment interactions. Our results highlight the effectiveness of combining GCNs and reward shaping for more efficient and robust reinforcement learning.",1
"Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext",0
"This is the full abstract:  This paper presents COOT (Cooperative Hierarchical Transformer), a novel architecture that effectively fuses video and text representations by leveraging intra-modality cooperation from both domains as well as inter-modality collaboration between them. To address the complex interactions within the two modalities, we design hierarchical architectures tailored specifically for videos and texts separately before their fusion. For the text modality, we construct a bottom-up CoAttention module based on self attention which allows for efficient feature reuse and reduces computational complexity. On the other hand, to capture multi-level visual dependencies for the video modality, we devise a top-down Hidden Unit Generation mechanism that selectively adds informative hidden units at different stages of our hierarchy. By integrating these complementary modules into one unified framework COOT achieves state-of-the-art performance on three diverse benchmarks across multiple tasks including action recognition, sentiment analysis, and question answering. Code is available at https://github.com/junxiaojiang268/COOT.  You can use this outline:  Introduction Motivation Approach Experiments Conclusion Future work",1
"We present a Reverse Reinforcement Learning (Reverse RL) approach for representing retrospective knowledge. General Value Functions (GVFs) have enjoyed great success in representing predictive knowledge, i.e., answering questions about possible future outcomes such as ""how much fuel will be consumed in expectation if we drive from A to B?"". GVFs, however, cannot answer questions like ""how much fuel do we expect a car to have given it is at B at time $t$?"". To answer this question, we need to know when that car had a full tank and how that car came to B. Since such questions emphasize the influence of possible past events on the present, we refer to their answers as retrospective knowledge. In this paper, we show how to represent retrospective knowledge with Reverse GVFs, which are trained via Reverse RL. We demonstrate empirically the utility of Reverse GVFs in both representation learning and anomaly detection.",0
"This paper proposes a novel method called reverse reinforcement learning (RRL) that allows agents to learn retrospective knowledge. RRL uses inverse reinforcement learning (IRL), but instead of trying to infer the original reward function from observed behavior, it starts by assuming an existing reward function and learns a policy that maximizes expected cumulative rewards. By using maximum likelihood estimation (MLE), RRL can estimate both the reward function and the optimal policy jointly. We show that our approach outperforms traditional IRL methods on challenging problems such as game playing, navigation, and control tasks. Our results highlight the effectiveness of RRL as a powerful tool for learning retrospective knowledge.",1
"This paper reviews the novel concept of controllable variational autoencoder (ControlVAE), discusses its parameter tuning to meet application needs, derives its key analytic properties, and offers useful extensions and applications. ControlVAE is a new variational autoencoder (VAE) framework that combines the automatic control theory with the basic VAE to stabilize the KL-divergence of VAE models to a specified value. It leverages a non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to dynamically tune the weight of the KL-divergence term in the evidence lower bound (ELBO) using the output KL-divergence as feedback. This allows us to precisely control the KL-divergence to a desired value (set point), which is effective in avoiding posterior collapse and learning disentangled representations. In order to improve the ELBO over the regular VAE, we provide simplified theoretical analysis to inform setting the set point of KL-divergence for ControlVAE. We observe that compared to other methods that seek to balance the two terms in VAE's objective, ControlVAE leads to better learning dynamics. In particular, it can achieve a good trade-off between reconstruction quality and KL-divergence. We evaluate the proposed method on three tasks: image generation, language modeling and disentangled representation learning. The results show that ControlVAE can achieve much better reconstruction quality than the other methods for comparable disentanglement. On the language modeling task, ControlVAE can avoid posterior collapse (KL vanishing) and improve the diversity of generated text. Moreover, our method can change the optimization trajectory, improving the ELBO and the reconstruction quality for image generation.",0
"This paper presents a comprehensive analysis of the tuning, analytical properties, and performance of the novel variational autoencoder (VAE) architecture, termed ""ControlVAE."" The architecture addresses limitations present in traditional VAEs by incorporating control variables that improve stability, flexibility, and sample quality. We provide extensive experiments evaluating the impact of different hyperparameters on the training process, as well as detailed analyses of the model's latent space and reconstruction behavior. Our results demonstrate the superiority of ControlVAE compared to state-of-the-art methods across various datasets and tasks, highlighting its potential applications in diverse fields such as image generation, feature learning, and anomaly detection. Overall, our work offers valuable insights into the design and optimization of modern deep generative models and contributes to the broader research community in machine learning and computer vision.",1
"Recently proposed neural architecture search (NAS) algorithms adopt neural predictors to accelerate the architecture search. The capability of neural predictors to accurately predict the performance metrics of neural architecture is critical to NAS, and the acquisition of training datasets for neural predictors is time-consuming. How to obtain a neural predictor with high prediction accuracy using a small amount of training data is a central problem to neural predictor-based NAS. Here, we firstly design a new architecture encoding scheme that overcomes the drawbacks of existing vector-based architecture encoding schemes to calculate the graph edit distance of neural architectures. To enhance the predictive performance of neural predictors, we devise two self-supervised learning methods from different perspectives to pre-train the architecture embedding part of neural predictors to generate a meaningful representation of neural architectures. The first one is to train a carefully designed two branch graph neural network model to predict the graph edit distance of two input neural architectures. The second method is inspired by the prevalently contrastive learning, and we present a new contrastive learning algorithm that utilizes a central feature vector as a proxy to contrast positive pairs against negative pairs. Experimental results illustrate that the pre-trained neural predictors can achieve comparable or superior performance compared with their supervised counterparts with several times less training samples. We achieve state-of-the-art performance on the NASBench-101 and NASBench201 benchmarks when integrating the pre-trained neural predictors with an evolutionary NAS algorithm.",0
"In many machine learning applications, large amounts of labeled data can’t be obtained due to either labeling costs that increase exponentially as dataset size grows, time and resource intensive label collection procedures, legal, social, medical and ethical constraints etc.. Moreover, there exists a shortage of annotators even if they could collect the required dataset annotations at reasonable cost and speed to meet the accuracy demands of high stakes machine learning applications like healthcare diagnostics. Therefore it's important to identify ways to utilize the small amount of labels available during training so we don't have to rely on extensive unlabeled datasets which may lack the quality/quantity to generalise well . One solution to this problem is self supervision through pretext tasks such as predicting contextual features using autoencoders. Such methods use internal representations learned from massive amounts of unlabelled data along with limited labeled examples to achieve state-of-the-art results by performing tasks that look easier than the downstream target task but capture the same underlying structure . In other areas of machine learning researchers employ techniques referred to as curriculum learning where the model gradually learns simpler subproblems before moving onto harder ones to learn . These solutions lead us to consider whether similar approaches could aid in neural architecture search. Our work introduces new neural architecture search algorithms based on concepts from representation learning via self-supervised and weakly-supervised pretext tasks including one-shot based evolution . We validate our proposed models on commonly used benchmark image classification problems. Experiments show our approach performs better compared against current methods from literature while requiring less computational resources during search , therefore making these advances accessible f",1
"Self-supervised, multi-modal learning has been successful in holistic representation of complex scenarios. This can be useful to consolidate information from multiple modalities which have multiple, versatile uses. Its application in surgical robotics can lead to simultaneously developing a generalised machine understanding of the surgical process and reduce the dependency on quality, expert annotations which are generally difficult to obtain. We develop a self-supervised, multi-modal representation learning paradigm that learns representations for surgical gestures from video and kinematics. We use an encoder-decoder network configuration that encodes representations from surgical videos and decodes them to yield kinematics. We quantitatively demonstrate the efficacy of our learnt representations for gesture recognition (with accuracy between 69.6 % and 77.8 %), transfer learning across multiple tasks (with accuracy between 44.6 % and 64.8 %) and surgeon skill classification (with accuracy between 76.8 % and 81.2 %). Further, we qualitatively demonstrate that our self-supervised representations cluster in semantically meaningful properties (surgeon skill and gestures).",0
"This project focuses on developing a system for recognizing gestures made by surgeons during minimally invasive procedures using data from cameras mounted near the surgical site. The ultimate goal is to assist surgeons through natural hand gesture controls without requiring explicit calibration. In order to achieve this, we propose a method that integrates several sources of sensory information including: (a) visual appearance based features extracted from color images obtained from RGB camera(s), (b) depth maps provided by a Time-of-Flight (ToF) sensor, and (c) skeletal pose estimated via inverse kinematics based on known geometry of instruments. Our approach leverages recent advancements in generative adversarial imitation learning (GAIL) to train neural network models directly from raw pixels rather than handcrafted feature extraction routines often used in previous works. We demonstrate the effectiveness of our proposed framework across three different datasets collected from real-world scenarios with significant variation among them. Experiments showcase improvements over state-of-the-art techniques especially in challenging situations such as occlusion due to instrument clutter or motion blur in video frames.",1
"Studies recently accomplished on the Enteric Nervous System have shown that chronic degenerative diseases affect the Enteric Glial Cells (EGC) and, thus, the development of recognition methods able to identify whether or not the EGC are affected by these type of diseases may be helpful in its diagnoses. In this work, we propose the use of pattern recognition and machine learning techniques to evaluate if a given animal EGC image was obtained from a healthy individual or one affect by a chronic degenerative disease. In the proposed approach, we have performed the classification task with handcrafted features and deep learning based techniques, also known as non-handcrafted features. The handcrafted features were obtained from the textural content of the ECG images using texture descriptors, such as the Local Binary Pattern (LBP). Moreover, the representation learning techniques employed in the approach are based on different Convolutional Neural Network (CNN) architectures, such as AlexNet and VGG16, with and without transfer learning. The complementarity between the handcrafted and non-handcrafted features was also evaluated with late fusion techniques. The datasets of EGC images used in the experiments, which are also contributions of this paper, are composed of three different chronic degenerative diseases: Cancer, Diabetes Mellitus, and Rheumatoid Arthritis. The experimental results, supported by statistical analysis, shown that the proposed approach can distinguish healthy cells from the sick ones with a recognition rate of 89.30% (Rheumatoid Arthritis), 98.45% (Cancer), and 95.13% (Diabetes Mellitus), being achieved by combining classifiers obtained both feature scenarios.",0
"Artificial intelligence has made significant strides in recent years in terms of image recognition and analysis, making it a valuable tool in fields such as medicine. In this paper, we propose a novel method for automatically identifying chronic degenerative diseases using images of the enteric nervous system (ENS). ENS imaging has been shown to provide valuable insights into gastrointestinal health and disease, and our approach leverages advances in deep learning algorithms to accurately classify these images. Our method involves preprocessing the images by converting them into grayscale, resizing them to a consistent size, and normalizing pixel values. We then use a convolutional neural network architecture to extract features from the images and feed these features into a support vector machine (SVM) for classification. Experimental results on a dataset of ENS images show promising accuracy rates for identifying chronic degenerative diseases such as inflammatory bowel disease, gastritis, and colorectal cancer. With further refinement, our method could have significant potential applications in clinical settings for assisting with diagnoses and treatment planning. However, more research is needed to validate its effectiveness and generalize its findings beyond the current dataset.",1
"For mental disorders, patients' underlying mental states are non-observed latent constructs which have to be inferred from observed multi-domain measurements such as diagnostic symptoms and patient functioning scores. Additionally, substantial heterogeneity in the disease diagnosis between patients needs to be addressed for optimizing individualized treatment policy in order to achieve precision medicine. To address these challenges, we propose an integrated learning framework that can simultaneously learn patients' underlying mental states and recommend optimal treatments for each individual. This learning framework is based on the measurement theory in psychiatry for modeling multiple disease diagnostic measures as arising from the underlying causes (true mental states). It allows incorporation of the multivariate pre- and post-treatment outcomes as well as biological measures while preserving the invariant structure for representing patients' latent mental states. A multi-layer neural network is used to allow complex treatment effect heterogeneity. Optimal treatment policy can be inferred for future patients by comparing their potential mental states under different treatments given the observed multi-domain pre-treatment measurements. Experiments on simulated data and a real-world clinical trial data show that the learned treatment polices compare favorably to alternative methods on heterogeneous treatment effects, and have broad utilities which lead to better patient outcomes on multiple domains.",0
"""In recent years, there has been significant progress towards developing personalized treatments that take into account individual differences in genetics, environment, and lifestyle factors. One major challenge in optimizing these treatments is integrating data from multiple sources such as medical records, genomic profiles, and lifestyle questionnaires. In this work, we propose a novel method for learning representations of individuals that can capture patterns across different domains of information, allowing us to jointly optimize treatment plans using all available data. Our approach leverages advances in deep neural networks and multi-task learning, enabling efficient integration of heterogeneous datasets. We evaluate our method on simulated and real clinical data sets, demonstrating improved performance compared to baseline methods that ignore correlations between domains. These results have important implications for the development of precision medicine interventions tailored to individual patients.""",1
"We propose a tree-based algorithm for classification and regression problems in the context of functional data analysis, which allows to leverage representation learning and multiple splitting rules at the node level, reducing generalization error while retaining the interpretability of a tree. This is achieved by learning a weighted functional $L^{2}$ space by means of constrained convex optimization, which is then used to extract multiple weighted integral features from the input functions, in order to determine the binary split for each internal node of the tree. The approach is designed to manage multiple functional inputs and/or outputs, by defining suitable splitting rules and loss functions that can depend on the specific problem and can also be combined with scalar and categorical data, as the tree is grown with the original greedy CART algorithm. We focus on the case of scalar-valued functional inputs defined on unidimensional domains and illustrate the effectiveness of our method in both classification and regression tasks, through a simulation study and four real world applications.",0
"Abstract: This paper presents a novel approach for inducing classification and regression trees on functional data using measure theory. Traditional methods for tree induction on function data often suffer from overfitting due to their high complexity and lack of regularization techniques. Our proposed method addresses these issues by utilizing measures as the basic building blocks for constructing decision boundaries. We introduce two new discretization schemes that enable the creation of nonparametric, adaptive spline estimators of functions at arbitrary resolutions. These estimators provide a concise representation of the original functional data, allowing efficient computation of tree structures while preserving key features of the underlying signal. Our experimental results demonstrate the superior performance of our measureinduced decision trees compared to state-of-theart alternatives across multiple realworld datasets.",1
"Machine learning with missing data has been approached in two different ways, including feature imputation where missing feature values are estimated based on observed values, and label prediction where downstream labels are learned directly from incomplete data. However, existing imputation models tend to have strong prior assumptions and cannot learn from downstream tasks, while models targeting label prediction often involve heuristics and can encounter scalability issues. Here we propose GRAPE, a graph-based framework for feature imputation as well as label prediction. GRAPE tackles the missing data problem using a graph representation, where the observations and features are viewed as two types of nodes in a bipartite graph, and the observed feature values as edges. Under the GRAPE framework, the feature imputation is formulated as an edge-level prediction task and the label prediction as a node-level prediction task. These tasks are then solved with Graph Neural Networks. Experimental results on nine benchmark datasets show that GRAPE yields 20% lower mean absolute error for imputation tasks and 10% lower for label prediction tasks, compared with existing state-of-the-art methods.",0
"When working with missing data, one approach that has been gaining traction in recent years is graph representation learning (GRL). GRL involves using a graph model to represent data points as nodes in a network, where edges connect similar nodes together. By leveraging the relationships between data points, GRL can effectively handle missing values by imputing them based on information from other parts of the dataset. This allows researchers to perform analyses without losing valuable data due to missing values. Additionally, GRL techniques like graph convolutional neural networks have shown promising results in applications such as image classification and natural language processing. Overall, graph representation learning presents a powerful tool for addressing missing data challenges.",1
"We propose a novel unsupervised generative model that learns to disentangle object identity from other low-level aspects in class-imbalanced data. We first investigate the issues surrounding the assumptions about uniformity made by InfoGAN, and demonstrate its ineffectiveness to properly disentangle object identity in imbalanced data. Our key idea is to make the discovery of the discrete latent factor of variation invariant to identity-preserving transformations in real images, and use that as a signal to learn the appropriate latent distribution representing object identity. Experiments on both artificial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world (YouTube-Faces) imbalanced datasets demonstrate the effectiveness of our method in disentangling object identity as a latent factor of variation.",0
"This research addresses the challenge of disentangling representations of class-imbalanced data using an unsupervised approach called elastic-InfoGAN (Elastic Information GAN). While previous methods have tackled this problem from either a supervised perspective or through adversarial training, our method leverages both to achieve better results. Our framework improves upon InfoGANs by introducing a new loss term that allows us to explicitly model class imbalances while still maintaining the stability required for effective training. We validate our method on several benchmark datasets and demonstrate significantly improved performance compared to state-of-the-art methods across all metrics. Overall, our work shows the potential of elastic-InfoGAN as a powerful tool for learning meaningful representations in high-dimensional spaces where class balance may be difficult to attain.",1
"Heterogeneous graph representation learning aims to learn low-dimensional vector representations of different types of entities and relations to empower downstream tasks. Existing methods either capture semantic relationships but indirectly leverage node/edge attributes in a complex way, or leverage node/edge attributes directly without taking semantic relationships into account. When involving multiple convolution operations, they also have poor scalability. To overcome these limitations, this paper proposes a flexible and efficient Graph information propagation Network (GripNet) framework. Specifically, we introduce a new supergraph data structure consisting of supervertices and superedges. A supervertex is a semantically-coherent subgraph. A superedge defines an information propagation path between two supervertices. GripNet learns new representations for the supervertex of interest by propagating information along the defined path using multiple layers. We construct multiple large-scale graphs and evaluate GripNet against competing methods to show its superiority in link prediction, node classification, and data integration.",0
"Gripnet aims at solving graph partitioning problems by introducing novel mathematical programming techniques using supernodes and subgraphs. It models real-world applications such as image segmentation, scientific data mining, VLSI circuit design, protein structure prediction, neural network processing, computer vision, natural language understanding and social science modeling onto graphs with complex and varying topologies. The framework is based upon multi-node graph clustering that aggregates multiple nodes into one virtual node called a supernode. By doing so, it increases parallelism over traditional methods which consider individual nodes only. This enables greater scalability through graph decomposition as well as faster solution times due to lower time complexity per iteration. Furthermore, by minimizing edge cuts and reducing communication overhead during computation, the proposed method reduces network congestion. Lastly, its application independent formulation allows future development of new algorithms within specific domains while utilizing existing infrastructure. Overall, these benefits make the proposed approach competitive with existing solutions.",1
"Frequency spectrum has played a significant role in learning unique and discriminating features for object recognition. Both low and high frequency information present in images have been extracted and learnt by a host of representation learning techniques, including deep learning. Inspired by this observation, we introduce a novel class of adversarial attacks, namely `WaveTransform', that creates adversarial noise corresponding to low-frequency and high-frequency subbands, separately (or in combination). The frequency subbands are analyzed using wavelet decomposition; the subbands are corrupted and then used to construct an adversarial example. Experiments are performed using multiple databases and CNN models to establish the effectiveness of the proposed WaveTransform attack and analyze the importance of a particular frequency component. The robustness of the proposed attack is also evaluated through its transferability and resiliency against a recent adversarial defense algorithm. Experiments show that the proposed attack is effective against the defense algorithm and is also transferable across CNNs.",0
"In order to create effective adversarial examples that can fool machine learning models, it is necessary to have a deep understanding of how these models process data. Recent work has shown that decomposing inputs into component features before applying small perturbations can greatly increase the effectiveness of adversarial attacks. We propose WaveTransf",1
"Learning representations of sets of nodes in a graph is crucial for applications ranging from node-role discovery to link prediction and molecule classification. Graph Neural Networks (GNNs) have achieved great success in graph representation learning. However, expressive power of GNNs is limited by the 1-Weisfeiler-Lehman (WL) test and thus GNNs generate identical representations for graph substructures that may in fact be very different. More powerful GNNs, proposed recently by mimicking higher-order-WL tests, only focus on representing entire graphs and they are computationally inefficient as they cannot utilize sparsity of the underlying graph. Here we propose and mathematically analyze a general class of structure-related features, termed Distance Encoding (DE). DE assists GNNs in representing any set of nodes, while providing strictly more expressive power than the 1-WL test. DE captures the distance between the node set whose representation is to be learned and each node in the graph. To capture the distance DE can apply various graph-distance measures such as shortest path distance or generalized PageRank scores. We propose two ways for GNNs to use DEs (1) as extra node features, and (2) as controllers of message aggregation in GNNs. Both approaches can utilize the sparse structure of the underlying graph, which leads to computational efficiency and scalability. We also prove that DE can distinguish node sets embedded in almost all regular graphs where traditional GNNs always fail. We evaluate DE on three tasks over six real networks: structural role prediction, link prediction, and triangle prediction. Results show that our models outperform GNNs without DE by up-to 15\% in accuracy and AUROC. Furthermore, our models also significantly outperform other state-of-the-art methods especially designed for the above tasks.",0
"Abstract In recent years, graph representation learning has become increasingly popular due to its ability to effectively capture complex relationships and interactions among entities in a variety of fields such as social network analysis, natural language processing, computer vision, biological networks, knowledge graphs, and more. However, designing efficient neural models that can learn powerful representations from graph data remains challenging due to issues related to scalability, expressiveness, interpretability, and generalization. We present a novel method called distance encoding that addresses these limitations by designing provably more powerful neural models for graph representation learning. Our approach uses localized distances between nodes as descriptors, which allows for both high model capacity and fast evaluation times on large datasets. Through extensive experiments, we demonstrate that our models achieve state-of-the-art performance across several benchmark datasets while reducing memory usage compared to previous methods. Additionally, we provide comprehensive theoretical analyses that prove the superiority of distance encoding over traditional node embedding techniques in terms of representation power and optimization difficulty. Overall, our work significantly advances the field of graph representation learning by providing practitioners with new tools for building more effective models that balance efficiency, accuracy, and scalability. Keywords: Graph representation learning; distance encoding; provable results; neural networks; scalability; expressivity; interpretability",1
"Most deep learning based image inpainting approaches adopt autoencoder or its variants to fill missing regions in images. Encoders are usually utilized to learn powerful representational spaces, which are important for dealing with sophisticated learning tasks. Specifically, in image inpainting tasks, masks with any shapes can appear anywhere in images (i.e., free-form masks) which form complex patterns. It is difficult for encoders to capture such powerful representations under this complex situation. To tackle this problem, we propose a self-supervised Siamese inference network to improve the robustness and generalization. It can encode contextual semantics from full resolution images and obtain more discriminative representations. we further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine both the restored and known regions in a smooth way. This multi-scale architecture is beneficial for decoding discriminative representations learned by encoders into images layer by layer. In this way, unknown regions will be filled naturally from outside to inside. Qualitative and quantitative experiments on multiple datasets, including facial and natural datasets (i.e., Celeb-HQ, Pairs Street View, Places2 and ImageNet), demonstrate that our proposed method outperforms state-of-the-art methods in generating high-quality inpainting results.",0
"Abstract:  Image inpainting refers to filling in missing regions of an image with visually coherent content that is consistent with the surrounding context. Recent advances in deep learning have enabled the development of effective image inpainting methods using techniques such as generative adversarial networks (GANs) and attention mechanisms. However, these methods often struggle to produce high-quality results due to their reliance on hand-engineered features, limited model capacity, and lack of robustness to changes in input data.  To address these limitations, we propose a novel approach based on a contrastive attention network for free-form image inpainting. Our method uses a two-stream architecture where one stream processes the original image while the other represents the occluded region. We introduce a self-attention mechanism that enables efficient feature extraction from large datasets and encourages the network to focus on relevant parts of the images. To promote consistency with the surrounding context, we apply a contrastive loss function that compares patches from the original image and the reconstructed image in the occluded region.  Our experiments demonstrate the effectiveness of our approach compared to state-of-the-art image inpainting algorithms. The proposed method achieves superior performance in terms of visual quality, texture synthesis, and semantic meaning preservation. Additionally, we showcase its ability to handle challenging scenarios including complex backgrounds, irregular shapes, and dynamic scenes. This work contributes to the growing literature on image inpainting by providing a more powerful framework capable of handling diverse real-world applications.  In summary, our study presents a new perspective on image inpainting through the use of contrastive attention networks. By leveraging recent advancements in computer vision and machine learning, we offer an improved solution for addressing missing data issues in digital imagery. The proposed method paves the way for future research efforts aimed at developing even mo",1
"We prove a Chernoff-type bound for sums of matrix-valued random variables sampled via a regular (aperiodic and irreducible) finite Markov chain. Specially, consider a random walk on a regular Markov chain and a Hermitian matrix-valued function on its state space. Our result gives exponentially decreasing bounds on the tail distributions of the extreme eigenvalues of the sample mean matrix. Our proof is based on the matrix expander (regular undirected graph) Chernoff bound [Garg et al. STOC '18] and scalar Chernoff-Hoeffding bounds for Markov chains [Chung et al. STACS '12].   Our matrix Chernoff bound for Markov chains can be applied to analyze the behavior of co-occurrence statistics for sequential data, which have been common and important data signals in machine learning. We show that given a regular Markov chain with $n$ states and mixing time $\tau$, we need a trajectory of length $O(\tau (\log{(n)}+\log{(\tau)})/\epsilon^2)$ to achieve an estimator of the co-occurrence matrix with error bound $\epsilon$. We conduct several experiments and the experimental results are consistent with the exponentially fast convergence rate from theoretical analysis. Our result gives the first bound on the convergence rate of the co-occurrence matrix and the first sample complexity analysis in graph representation learning.",0
"This study presents a new bound on matrix Chernoff inequality based on the Perron Frobenius theorem for irreducible nonnegative matrices. We derive the expression which can approximate the logarithm function of the trace of exponential functions of matrices. Moreover, we apply the developed result to Markov chains on co-occurrences matrices and use an elementary method to obtain sharper estimates. Additionally, we provide examples which demonstrate that our bounds have better performance than previously known results in certain situations. In particular, we analyze two cases where the Markov chain has only one stationary distribution (positive recurrent) and more general case when there exist multiple stationary distributions. The provided numerical simulations support our theoretical findings and show the efficiency of the obtained results. Lastly, we believe that our research will contribute positively to future studies as the derived expressions can serve as tools in various applications such as machine learning, bioinformatics, and computer science.",1
"Although Generative Adversarial Networks (GANs) have made significant progress in face synthesis, there lacks enough understanding of what GANs have learned in the latent representation to map a random code to a photo-realistic image. In this work, we propose a framework called InterFaceGAN to interpret the disentangled face representation learned by the state-of-the-art GAN models and study the properties of the facial semantics encoded in the latent space. We first find that GANs learn various semantics in some linear subspaces of the latent space. After identifying these subspaces, we can realistically manipulate the corresponding facial attributes without retraining the model. We then conduct a detailed study on the correlation between different semantics and manage to better disentangle them via subspace projection, resulting in more precise control of the attribute manipulation. Besides manipulating the gender, age, expression, and presence of eyeglasses, we can even alter the face pose and fix the artifacts accidentally made by GANs. Furthermore, we perform an in-depth face identity analysis and a layer-wise analysis to evaluate the editing results quantitatively. Finally, we apply our approach to real face editing by employing GAN inversion approaches and explicitly training feed-forward models based on the synthetic data established by InterFaceGAN. Extensive experimental results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable face representation.",0
"This is an interdisciplinary research that investigates how generative adversarial networks (GANs) can learn disentangled representations which capture factors of variation observed in human faces while preserving the identity of individuals. We introduce InterFaceGAN, a novel unsupervised framework based on cycle consistency constraints, that enables interpreting learned latent spaces as face attributes. Experimental results demonstrate improved disentanglement over state-of-the-art methods across both facial attribute classification accuracy metrics and qualitative analysis using heatmaps illustrating salient regions capturing different aspects of facial features. Furthermore, we establish transferability properties from one dataset to another, showing applicability in broader domains. Finally, we leverage these insights for a novel image editing application by manipulating specific dimensions within the learned latent space of VGGFace2, enabling a fine degree of control for artistic creativity purposes.",1
"Geometric scattering has recently gained recognition in graph representation learning, and recent work has shown that integrating scattering features in graph convolution networks (GCNs) can alleviate the typical oversmoothing of features in node representation learning. However, scattering methods often rely on handcrafted design, requiring careful selection of frequency bands via a cascade of wavelet transforms, as well as an effective weight sharing scheme to combine together low- and band-pass information. Here, we introduce a new attention-based architecture to produce adaptive task-driven node representations by implicitly learning node-wise weights for combining multiple scattering and GCN channels in the network. We show the resulting geometric scattering attention network (GSAN) outperforms previous networks in semi-supervised node classification, while also enabling a spectral study of extracted information by examining node-wise attention weights.",0
"""Geometrically-Aware"" Neural Machine Translation Models: Improving Performance by Leveraging Locality and Structure Abstract Recent advances have shown that incorporating geometric considerations into neural network architectures can lead to substantial improvements on natural language processing tasks such as machine translation (MT). One important factor in MT is preserving the spatial relationships among entities described in text - something traditional sequence-based models struggle to capture well. We present Geometric Scattering Attention Networks (GSAN), which leverage recent developments in graph signal processing to model the geometry underlying the text data while still being compatible with existing attention mechanisms used in modern NLP systems. Our key contribution is demonstrating how explicitly accounting for locality and structure through this novel combination leads to significant performance gains across multiple benchmark datasets and evaluation metrics compared to several strong baselines. This research represents a new class of ""geometrically-aware"" NLMs tailored specifically toward improving MT quality for better global communication.",1
"We present Cycle-Contrastive Learning (CCL), a novel self-supervised method for learning video representation. Following a nature that there is a belong and inclusion relation of video and its frames, CCL is designed to find correspondences across frames and videos considering the contrastive representation in their domains respectively. It is different from recent approaches that merely learn correspondences across frames or clips. In our method, the frame and video representations are learned from a single network based on an R3D architecture, with a shared non-linear transformation for embedding both frame and video features before the cycle-contrastive loss. We demonstrate that the video representation learned by CCL can be transferred well to downstream tasks of video understanding, outperforming previous methods in nearest neighbour retrieval and action recognition tasks on UCF101, HMDB51 and MMAct.",0
"In recent years, self-supervised learning has emerged as one of the most promising approaches to deep representation learning for video understanding tasks such as action recognition, object detection, tracking, and activity analysis. Existing methods typically learn representations either by maximizing prediction accuracy on pretext task surrogates (predicting future frames or patches) or minimizing reconstruction error under some form of constraints (e.g., autoencoders). However, these objective functions often lead to trivial solutions that do not capture the underlying structure or dynamics of videos effectively, resulting in suboptimal performance. Our approach addresses these limitations through cycle consistency based on feature matching across time cycles, ensuring stable temporal correspondences and preserving temporal patterns critical to reasoning about objects and events in video. We evaluate our method extensively against competitive baselines using challenging benchmark datasets and demonstrate significant improvement over state-of-the-art results. By bridging visual domains via self-training, we establish the effectiveness of our framework for zero-shot generalization, opening new opportunities for unified representation learning across diverse data modalities.",1
"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.",0
"Abstract: In recent years, representation learning has emerged as a key area of research in artificial intelligence, playing a crucial role in enabling machines to effectively process and make sense of large amounts of data. One approach that has gained significant attention within this field is contrastive representation learning. This framework involves training neural networks to distinguish between pairs of samples drawn from different distributions, such as positive and negative examples, or similar and dissimilar instances. By doing so, these models can learn efficient representations that capture important features of their input data, leading to improved performance on downstream tasks like classification and regression. In this review paper, we first provide an overview of the underlying principles and mechanisms behind contrastive representation learning. We then survey several state-of-the-art methods based on this approach, highlighting their respective strengths and weaknesses. Our goal is to provide both practitioners and researchers with a clear understanding of how this powerful technique can benefit their work, while identifying areas where future advancements may lead to even more effective applications of machine learning.",1
"The increasing impact of black box models, and particularly of unsupervised ones, comes with an increasing interest in tools to understand and interpret them. In this paper, we consider in particular how to characterise visual groupings discovered automatically by deep neural networks, starting with state-of-the-art clustering methods. In some cases, clusters readily correspond to an existing labelled dataset. However, often they do not, yet they still maintain an ""intuitive interpretability"". We introduce two concepts, visual learnability and describability, that can be used to quantify the interpretability of arbitrary image groupings, including unsupervised ones. The idea is to measure (1) how well humans can learn to reproduce a grouping by measuring their ability to generalise from a small set of visual examples (learnability) and (2) whether the set of visual examples can be replaced by a succinct, textual description (describability). By assessing human annotators as classifiers, we remove the subjective quality of existing evaluation metrics. For better scalability, we finally propose a class-level captioning system to generate descriptions for visual groupings automatically and compare it to human annotators using the describability metric.",0
"Quantifying learnability and describabilty (QUD) of concepts learned by representation learning models has important implications for understanding how these representations capture important features of visual scenes. QUD measures could potentially provide insights into why some tasks benefit from deep over shallow representations while others may only require mid-level descriptors. We evaluate four different objectives that have been used to train representation learning models in computer vision: supervised pretraining on image classification followed by fine-tuning (iF), self-supervised contrastive learning with augmentations (AugCon), unsupervised clustering (Clust.), and a combination of AugCon+Clust.. These methods have achieved state-of-the-art results across many benchmark datasets but the factors leading to their success remains poorly understood. In particular, we ask whether the representational quality can be related to either human learnability or quantifiably described attributes of the scene. Our experiments aimed at measuring these two aspects were designed specifically not just based on simple low-level statistics such as texture and edge structure, but rather captures the underlying structure, geometry, and grouping within each scene. To measure human learnability, we analyzed data collected through three large crowd-sourced psychophysics studies where participants completed two free recall memory tests after viewing images, followed by multiple forced choice recognition tests. Since forced choices can reveal both scene details as well as contextual associations, our study focuses only on first recall trials to minimize biases associated with recognition performance. For the more objective aspects, we use precomputed features extracted from offtheshelf segmenters (Pascal Context). Although it’s known that there exists a complex nonlinear mapping betwee",1
"We consider the identifiability theory of probabilistic models and establish sufficient conditions under which the representations learned by a very broad family of conditional energy-based models are unique in function space, up to a simple transformation. In our model family, the energy function is the dot-product between two feature extractors, one for the dependent variable, and one for the conditioning variable. We show that under mild conditions, the features are unique up to scaling and permutation. Our results extend recent developments in nonlinear ICA, and in fact, they lead to an important generalization of ICA models. In particular, we show that our model can be used for the estimation of the components in the framework of Independently Modulated Component Analysis (IMCA), a new generalization of nonlinear ICA that relaxes the independence assumption. A thorough empirical study shows that representations learned by our model from real-world image datasets are identifiable, and improve performance in transfer learning and semi-supervised learning tasks.",0
"Abstract: In recent years, deep learning has achieved state-of-the-art results across many domains. However, interpretability remains an important challenge that limits their wide adoption in fields such as finance, healthcare, and government. One popular approach to increase the explainability of neural networks is energy-based models (EBMs), which offer a simple interpretation by modeling the probability density function of each data point directly. Recently, identifiable conditional EBMs have been proposed, which allow for individual specific parameters and improve calibration accuracy compared to regular EBMs. Yet, these methods still suffer from scalability issues due to high computational costs of nonlinear independent component analysis (ICA) used in training. To address this limitation, we introduce ICE-BeeM, an algorithm based on nonlinear ICA, for efficient identification of conditional EBMs. Our method achieves better or comparable predictive performance while significantly reducing computation time. We demonstrate superior performance of our method on several benchmark datasets against existing approaches, making it suitable for real-world applications requiring both high efficiency and interpretability. Keywords: Energy-Based Model, Independent Component Analysis, Interpretability, Nonlinearity Introduction",1
"Heterogeneous Information Networks (HINs), involving a diversity of node types and relation types, are pervasive in many real-world applications. Recently, increasing attention has been paid to heterogeneous graph representation learning (HGRL) which aims to embed rich structural and semantics information in HIN into low-dimensional node representations. To date, most HGRL models rely on manual customisation of meta paths to capture the semantics underlying the given HIN. However, the dependency on the handcrafted meta-paths requires rich domain knowledge which is extremely difficult to obtain for complex and semantic rich HINs. Moreover, strictly defined meta-paths will limit the HGRL's access to more comprehensive information in HINs. To fully unleash the power of HGRL, we present a Reinforcement Learning enhanced Heterogeneous Graph Neural Network (RL-HGNN), to design different meta-paths for the nodes in a HIN. Specifically, RL-HGNN models the meta-path design process as a Markov Decision Process and uses a policy network to adaptively design a meta-path for each node to learn its effective representations. The policy network is trained with deep reinforcement learning by exploiting the performance of the model on a downstream task. We further propose an extension, RL-HGNN++, to ameliorate the meta-path design procedure and accelerate the training process. Experimental results demonstrate the effectiveness of RL-HGNN, and reveals that it can identify meaningful meta-paths that would have been ignored by human knowledge.",0
"Here we introduce a new approach to enhancing graph neural networks using reinforcement learning techniques such as Q-Learning and Deep Deterministic Policy Gradients (DDPG). This method involves incorporating feedback loops into the network training process that allow the model to learn from both positive and negative outcomes on test data sets. By doing so, our proposed system can better adapt to changes in user preferences or task requirements over time. Our experimental results show significant improvements compared to state-of-the-art methods on several benchmark datasets, demonstrating the effectiveness of our novel framework. Overall, this work represents a step forward towards more advanced artificial intelligence systems capable of handling complex tasks involving heterogeneous graphs.",1
"Graph Neural Networks achieve remarkable results on problems with structured data but come as black-box predictors. Transferring existing explanation techniques, such as occlusion, fails as even removing a single node or edge can lead to drastic changes in the graph. The resulting graphs can differ from all training examples, causing model confusion and wrong explanations. Thus, we argue that explicability must use graphs compliant with the distribution underlying the training data. We coin this property Distribution Compliant Explanation (DCE) and present a novel Contrastive GNN Explanation (CoGE) technique following this paradigm. An experimental study supports the efficacy of CoGE.",0
"Recently, there has been increased interest in developing techniques capable of explaining graph neural networks (GNNs) since these models can generate difficult-to-interpret results. One promising approach has been the use of contrastive explanations that aim to show which parts of a GNN’s input contribute positively/negatively to the model prediction. To improve the effectiveness of these approaches, we present our methodology, which builds on recent advances in attention mechanisms. We apply it within a single GNN layer and adapt it so that it works at different levels of granularity across graphs. Our experiments demonstrate that, compared to other methods, ours provides insights into how individual nodes influence predictions while also identifying important edges. As such, our work offers significant potential applications in fields relying on graph data analysis where interpretability remains challenging but crucial, like biomedicine, cybersecurity, social sciences, etc. Ultimately, our research seeks to bring graph learning closer to human understanding.",1
"Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.",0
"Title: Optimal Control Using Latent Variables in deep reinforcement learning  Abstract: This work presents a novel methodology that integrates latent variable models within the framework of deep reinforcement learning (DRL), thus allowing for more efficient modeling of complex interactions among features. In particular, we propose Stochastic Latent Actor Critic (SLAC), which uses a stochastic policy gradient update rule based on a novel technique called Q-learning from Demonstration. We use variational inference to optimize the latent variables while interacting with the environment using the actor-critic algorithm. SLAC outperforms other state-of-the-art DRL methods across multiple domains and tasks by effectively capturing these complex relationships between features. Our approach significantly advances the field of DRL, as it enables agents to learn faster, make better decisions, and achieve higher overall performance. By leveraging prior knowledge through demonstrations and utilizing latent variable models, our method sets a new standard for effective and efficient autonomous decision making.",1
"Model compression becomes a recent trend due to the requirement of deploying neural networks on embedded and mobile devices. Hence, both accuracy and efficiency are of critical importance. To explore a balance between them, a knowledge distillation strategy is proposed for general visual representation learning. It utilizes our well-designed activation map adaptive module to replace some blocks of the teacher network, exploring the most appropriate supervisory features adaptively during the training process. Using the teacher's hidden layer output to prompt the student network to train so as to transfer effective semantic information.To verify the effectiveness of our strategy, this paper applied our method to cifar-10 dataset. Results demonstrate that the method can boost the accuracy of the student network by 0.6% with 6.5% loss reduction, and significantly improve its training speed.",0
"In recent years, knowledge distillation has emerged as a promising approach to transferring knowledge from large pre-trained models to smaller ones. However, existing methods often suffer from several limitations such as sensitivity to hyperparameters, suboptimal use of attention mechanisms, and limited adaptation ability. To address these issues, we propose a novel method called ""Activation Map Adaptation"" that leverages activation maps to adaptively refine the attention regions of small students during the knowledge distillation process. Our key contributions can be summarized as follows:  Firstly, our method effectively identifies informative features by minimizing the difference between teacher and student activation maps while maximizing their mutual similarity, leading to more accurate and efficient knowledge transfer. Secondly, we introduce a new variant of KD named AMKD that employs a differentiable weighting mechanism based on activation maps, allowing for fine-grained control over the balance between soft targets and hard targets. Thirdly, we propose a novel algorithm that iteratively updates activation maps with gradient descent guided by feature attributions to improve adaptation effectiveness. Finally, our extensive experiments across different datasets and architectures demonstrate that our method significantly outperforms state-of-the-art methods under various settings, verifying its superiority and robustness. Overall, our work provides important insights into the design and optimization of effective knowledge distillation techniques, paving the way towards building compact yet powerful neural networks for real-world applications.",1
"Self-supervised learning approaches leverage unlabeled samples to acquire generic knowledge about different concepts, hence allowing for annotation-efficient downstream task learning. In this paper, we propose a novel self-supervised method that leverages multiple imaging modalities. We introduce the multimodal puzzle task, which facilitates rich representation learning from multiple image modalities. The learned representations allow for subsequent fine-tuning on different downstream tasks. To achieve that, we learn a modality-agnostic feature embedding by confusing image modalities at the data-level. Together with the Sinkhorn operator, with which we formulate the puzzle solving optimization as permutation matrix inference instead of classification, they allow for efficient solving of multimodal puzzles with varying levels of complexity. In addition, we also propose to utilize cross-modal generation techniques for multimodal data augmentation used for training self-supervised tasks. In other words, we exploit synthetic images for self-supervised pretraining, instead of downstream tasks directly, in order to circumvent quality issues associated with synthetic images, while improving data-efficiency and representations quality. Our experimental results, which assess the gains in downstream performance and data-efficiency, show that solving our multimodal puzzles yields better semantic representations, compared to treating each modality independently. Our results also highlight the benefits of exploiting synthetic images for self-supervised pretraining. We showcase our approach on four downstream tasks: Brain tumor segmentation and survival days prediction using four MRI modalities, Prostate segmentation using two MRI modalities, and Liver segmentation using unregistered CT and MRI modalities. We outperform many previous solutions, and achieve results competitive to state-of-the-art.",0
"Title: ""Multimodal Self-Supervised Learning for Medical Image Analysis""  Abstract: This paper presents a multimodal self-supervised learning approach for medical image analysis that leverages multiple imaging modalities (such as X-rays, CT scans, MRI scans) without relying on expert annotations. We propose a novel framework based on contrastive learning, which learns robust feature representations by aligning images from different modalities. Our method achieves state-of-the-art performance for two important computer vision tasks - lesion detection and organ segmentation, demonstrating its effectiveness in handling challenges such as low data availability, domain shift, and outlier noise. In addition, we analyze our results using ablation studies and visualizations, providing insights into how our model behaves under various conditions and suggesting directions for future work. Overall, our research highlights the potential of self-supervised learning techniques for advancing the field of medical image analysis.",1
"Current autoencoder-based disentangled representation learning methods achieve disentanglement by penalizing the (aggregate) posterior to encourage statistical independence of the latent factors. This approach introduces a trade-off between disentangled representation learning and reconstruction quality since the model does not have enough capacity to learn correlated latent variables that capture detail information present in most image data. To overcome this trade-off, we present a novel multi-stage modelling approach where the disentangled factors are first learned using a preexisting disentangled representation learning method (such as $\beta$-TCVAE); then, the low-quality reconstruction is improved with another deep generative model that is trained to model the missing correlated latent variables, adding detail information while maintaining conditioning on the previously learned disentangled factors. Taken together, our multi-stage modelling approach results in a single, coherent probabilistic model that is theoretically justified by the principal of D-separation and can be realized with a variety of model classes including likelihood-based models such as variational autoencoders, implicit models such as generative adversarial networks, and tractable models like normalizing flows or mixtures of Gaussians. We demonstrate that our multi-stage model has much higher reconstruction quality than current state-of-the-art methods with equivalent disentanglement performance across multiple standard benchmarks.",0
"This study investigates multi-stage modelling as a technique for improving the reconstruction process of disentangled representation learners (DRLs). DRLs have gained popularity due to their ability to encode complex data distributions into low dimensional latent spaces, while preserving important features such as identity, pose, lighting conditions, etc. However, one challenge faced by these models is that they often struggle to accurately reconstruct the input images from the encoded representations, leading to poor performance on downstream tasks.  The proposed approach involves splitting the reconstruction task into multiple stages, each stage focusing on reconstructing specific aspects of the image. By doing so, we allow the model to better focus on learning which features matter most for successful reconstruction, without overfitting to any particular aspect. We demonstrate through experimentation that our method leads to significant improvements in both quantitative metrics such as PSNR/SSIM, and qualitative results like visual fidelity on several benchmark datasets such as CelebA, LSUN Churches, and FFHQ. In addition, we show that using pre-training to optimize for disentanglement before applying the multi-stage training strategy further boosts performance. Our findings suggest that incorporating multi-stage modelling can greatly benefit the quality of the learned disentangled representations, making them more suitable for use cases where high-quality image generation is essential.",1
"Crowdsourced data used in machine learning services might carry sensitive information about attributes that users do not want to share. Various methods have been proposed to minimize the potential information leakage of sensitive attributes while maximizing the task accuracy. However, little is known about the theory behind these methods. In light of this gap, we develop a novel theoretical framework for attribute obfuscation. Under our framework, we propose a minimax optimization formulation to protect the given attribute and analyze its inference guarantees against worst-case adversaries. Meanwhile, it is clear that in general there is a tension between minimizing information leakage and maximizing task accuracy. To understand this, we prove an information-theoretic lower bound to precisely characterize the fundamental trade-off between accuracy and information leakage. We conduct experiments on two real-world datasets to corroborate the inference guarantees and validate this trade-off. Our results indicate that, among several alternatives, the adversarial learning approach achieves the best trade-off in terms of attribute obfuscation and accuracy maximization.",0
"Adversarial representation learning (ARL) has recently gained popularity as a technique to create effective obfuscating transformations that hide sensitive data while minimizing distortion of utility. This paper explores trade-offs associated with using ARL for adversaries seeking robustness against both reconstructing raw input from transformed outputs as well as reducing the effectiveness of any downstream task depending on the raw input. Our experimental results show that ARL models trained under worst-case constraints can effectively degrade the performance of a wide range of downstream attacks and models, including those designed specifically to mitigate obfuscation techniques. We find that stronger guarantees come at the cost of increased model complexity and reduced transformation fidelity, highlighting challenges for deploying these methods in realistic settings where resources may be limited. By analyzing properties of high-performing ARL models through visualizations and theoretical analysis, we provide insight into how these competing factors interact to shape the behavior of ARL systems. Ultimately, our work emphasizes that no single approach offers optimal security against all types of reconstruction attacks and suggests promising directions for future research. Abstract: Adversarial representation learning (ARL) is a powerful tool for creating obfuscating transformations that protect sensitive data while preserving utility. However, there are important trade-offs associated with using ARL in real-world applications. In this paper, we explore the impact of different training objectives on the effectiveness of ARL models in resisting downstream attacks aimed at recovering raw input or interfering with dependent tasks. Through rigorous experimentation and analysis, we demonstrate that ARL models can indeed offer strong resistance to such threats, but only if they are trained under carefully chosen constraints. Moreover, achieving greater resilience often comes at the expense of increased model complexity or compromised transformation quality, which may limit deployment options in resource-constrained scenarios. By shedding light on the dynamics underlying successful ARL schemes, we hope to inform future development of more versatile and practically viable privacy protection mechanisms.",1
"Existing Neural Architecture Search (NAS) methods either encode neural architectures using discrete encodings that do not scale well, or adopt supervised learning-based methods to jointly learn architecture representations and optimize architecture search on such representations which incurs search bias. Despite the widespread use, architecture representations learned in NAS are still poorly understood. We observe that the structural properties of neural architectures are hard to preserve in the latent space if architecture representation learning and search are coupled, resulting in less effective search performance. In this work, we find empirically that pre-training architecture representations using only neural architectures without their accuracies as labels considerably improve the downstream architecture search efficiency. To explain these observations, we visualize how unsupervised architecture representation learning better encourages neural architectures with similar connections and operators to cluster together. This helps to map neural architectures with similar performance to the same regions in the latent space and makes the transition of architectures in the latent space relatively smooth, which considerably benefits diverse downstream search strategies.",0
"Recently neural architecture search has attracted increasing attention as it has proven capable of designing complex architectures like ResNet that achieve state-of-the-art performance on image classification benchmarks. The most popular methods use reinforcement learning algorithms whereby an agent learns which architecture parameters map to high validation accuracy by interactively training and evaluating them. However these approaches can suffer from brittleness, instability and sample inefficiency, due largely to their reliance on randomly initialized architectures, hyperparameters and reward functions. In this paper we explore whether representation learning of model architecture representations before applying randomness can improve stability and speed up sampling. We focus on unsupervised pretraining using self-prediction which has shown promising results in language models, machine translation and generative adversarial networks. Our experiments show that unsupervised pretraining significantly improves stabilization of trained agents and enables searching vastly more challenging architectures without sacrificing population diversity or valid accuracy. Overall our work indicates a surprising degree of transfer across tasks given limited computational resources, and offers new insights into alternative neural network optimization strategies beyond gradient descent. While these findings demonstrate great potential for NAS applications they remain somewhat speculative since current hardware limits severely constrain exploration of larger architectures. Further work is required on generalizing these ideas beyond image classification to other domains such as text generation, object detection or reinforcement learning. Moreover uncovering why unsupervised representation learning works so well still remains an important open question though we conjecture that enabling better gradient flow via initial layers plays a critical role. Finally future research could extend these concepts beyond Evolution Strategies to other stochastic gradient based optimizers which dominate deep learning practice today but whose adoption for NAS appears discouragingly difficult under current random initialization assumptions",1
"Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-the-art graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features.",0
"This paper is about the graph information bottleneck. We introduce GIB, the Graph Information Bottleneck: we bound the amount of information that can flow through any directed acyclic graph (DAG). Specifically, each node has at most one outgoing edge towards nodes further downstream (transmitting more information), and edges cannot have self-loops (providing no new information). Formally, for any such DAG $G = (V, E)$ where $E$ contains only edges $(u,v)$ satisfying $\ell(u) < \ell(v)$, we show that both $\mathbf{I}(X;Y|Z)$ and $\mathbf{H}(X|Y,Z)$ must satisfy the following inequality:\begin{gather*} ||\mathbb{P}_{XY} - \mathbb{P}_X\times \mathbb{P}_Y||_TV \cdot \left(\operatorname{Var}(\log f_{YZ}) + 2R_Y^{-2}\right) \\+ ||K_f||_\infty\sqrt{\chi^2(|Y||Z|)} \leq C(p)\cdot ||\mathbb{P}_{XY} - \mathbb{P}_X\times \mathbb{P}_Y||_F^2.\end{gather*}Here $C(p) > 0$ depends on properties of $G$, which implies there exists a constant lower bound for how large the left hand side needs to be compared to the squared Frobenius norm. In other words, there exists some maximum rate above which information cannot propagate through this network. This has important implications in the study of information theory and machine learning, as graphs naturally arise in many models and algorithms due to their ability to represent complex relationships between data points. Our work provides insight into the fundamental limits of these representations, ultimately leading us closer to understanding under what circumstances graphs provide a theoretically optimal solution versus merely a convenient one.  What do you think? Is this a good outline of an abstract that captures the main contribution of your paper while preserving clarity and conciseness? Would you want me to revise or expand upon certain sections? Are there additional sections you would like me to cover? Let me know if y",1
"Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an initial feature vector) and non-attributed graphs (where the only initial information derives from known relations with other entities). For out-of-sample reasoning, where one needs to make predictions for entities that were unseen at training time, much prior work considers attributed graph. However, this problem is surprisingly under-explored for non-attributed graphs. In this paper, we study the out-of-sample representation learning problem for non-attributed knowledge graphs, create benchmark datasets for this task, develop several models and baselines, and provide empirical analyses and comparisons of the proposed models and baselines.",0
"Graph representation learning has emerged as a powerful approach to modeling complex data structures such as social networks, knowledge graphs, and biochemical interactions. However, most existing methods focus on capturing first-order relationships between nodes without considering higher-order patterns that may exist in the graph. This can lead to suboptimal representations that miss important structural information in the data. To address this limitation, we propose a novel method called out-of-sample representation learning (ORS), which generalizes graph neural network architectures by leveraging auxiliary tasks in unseen domains. ORS models use multi-relational graphs, where different types of links capture distinct relationship categories, and are trained using adversarial training techniques that exploit both known link types and unknown ones. We show through extensive experiments that our ORS model significantly improves upon state-of-the art baselines across multiple benchmark datasets for node classification, edge prediction, and link prediction tasks. Our work demonstrates the importance of incorporating diverse relational information into graph representation learning frameworks, and offers new insights into how to effectively leverage auxiliary task supervision for improved performance.",1
"Anomaly Detection (AD) in images is a fundamental computer vision problem and refers to identifying images and image substructures that deviate significantly from the norm. Popular AD algorithms commonly try to learn a model of normality from scratch using task specific datasets, but are limited to semi-supervised approaches employing mostly normal data due to the inaccessibility of anomalies on a large scale combined with the ambiguous nature of anomaly appearance.   We follow an alternative approach and demonstrate that deep feature representations learned by discriminative models on large natural image datasets are well suited to describe normality and detect even subtle anomalies in a transfer learning setting. Our model of normality is established by fitting a multivariate Gaussian (MVG) to deep feature representations of classification networks trained on ImageNet using normal data only. By subsequently applying the Mahalanobis distance as the anomaly score we outperform the current state of the art on the public MVTec AD dataset, achieving an AUROC value of $95.8 \pm 1.2$ (mean $\pm$ SEM) over all 15 classes. We further investigate why the learned representations are discriminative to the AD task using Principal Component Analysis. We find that the principal components containing little variance in normal data are the ones crucial for discriminating between normal and anomalous instances. This gives a possible explanation to the often sub-par performance of AD approaches trained from scratch using normal data only. By selectively fitting a MVG to these most relevant components only, we are able to further reduce model complexity while retaining AD performance. We also investigate setting the working point by selecting acceptable False Positive Rate thresholds based on the MVG assumption.   Code available at https://github.com/ORippler/gaussian-ad-mvtec",0
"This paper presents a method for modeling the distribution of normal data using pre-trained deep features for anomaly detection. The authors propose using pre-trained convolutional neural networks (CNNs) as feature extractors to obtain high-dimensional representations of images. These representations can then be used to learn a probability density function that models the distribution of normal data. By doing so, the authors aim to improve the accuracy of anomaly detection tasks by better understanding the underlying statistical properties of the normal data. They evaluate their approach on two public datasets and demonstrate its effectiveness compared to state-of-the-art methods. Overall, this work represents an important contribution to the field of computer vision and anomaly detection research.",1
"GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.",0
"This work presents ChemBERTa, a novel pretraining method that leverages large amounts of data from both chemistry research papers and chemical patents to achieve state-of-the-art molecular property prediction performance on seven challenging tasks. To overcome domain shifts between these two domains, we propose a semi-supervised alignment approach to align tokens across domains based on their semantic meaning. Our experiments show significant improvements over competitive baselines and demonstrate the effectiveness of our proposed method for achieving high quality molecular property predictions.",1
"Zero-shot classification is a generalization task where no instance from the target classes is seen during training. To allow for test-time transfer, each class is annotated with semantic information, commonly in the form of attributes or text descriptions. While classical zero-shot learning does not explicitly forbid using information from other datasets, the approaches that achieve the best absolute performance on image benchmarks rely on features extracted from encoders pretrained on Imagenet. This approach relies on hyper-optimized Imagenet-relevant parameters from the supervised classification setting, entangling important questions about the suitability of those parameters and how they were learned with more fundamental questions about representation learning and generalization. To remove these distractors, we propose a more challenging setting: Zero-Shot Learning from scratch (ZFS), which explicitly forbids the use of encoders fine-tuned on other datasets. Our analysis on this setting highlights the importance of local information, and compositional representations.",0
"This project studies how artificial intelligence can learn from scratch using zero-shot learning, a process that allows algorithms to perform complex tasks without any previous experience. In particular, we focus on developing techniques that leverage local compositional representations, which break down problems into smaller pieces and solve them individually before combining their solutions. By doing so, our system can learn more efficiently and effectively, even when given no prior knowledge of the problem domain. Our experiments demonstrate the effectiveness of our approach across a wide range of challenging problems, including image classification, language modeling, and question answering. Overall, these findings contribute valuable insights into the potential of zero-shot learning as a powerful tool in AI research and development.",1
"We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the ""free"" adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.",0
"Advances in computer vision (CV) and natural language processing (NLP) have led to significant progress in developing systems that can perform complex tasks such as image generation, machine translation, and image/text retrieval. However, these models often lack robustness and generalization capabilities when tested on out-of-domain data, resulting in poor performance. To address this issue, we present a novel adversarial training method called large-scale adversarial training (LSAT), which combines both CV and NLP objectives into one framework. We demonstrate through extensive experiments that LSAT significantly improves zero-shot and few-shot transfer learning across multiple benchmark datasets by generating highquality, semantically meaningful text descriptions for images and identifying relevant concepts within those images. Our contributions provide insights into how pretraining using contrastive loss functions alone may not be sufficient for building strong vision–language representations and highlight the benefits of incorporating adversarial methods. By leveraging unlabeled data at scale during pretraining, LSAT facilitates improved model performance on zero-shot and few-shot downstream tasks while offering competitive results compared to state-of-the-art approaches. In summary, our work introduces a new approach for developing more robust and generalized visual–linguistic models capable of handling diverse environments and scenarios, extending the applicability of such systems beyond their original domains.",1
"We provide new statistical guarantees for transfer learning via representation learning--when transfer is achieved by learning a feature representation shared across different tasks. This enables learning on new tasks using far less data than is required to learn them in isolation. Formally, we consider $t+1$ tasks parameterized by functions of the form $f_j \circ h$ in a general function class $\mathcal{F} \circ \mathcal{H}$, where each $f_j$ is a task-specific function in $\mathcal{F}$ and $h$ is the shared representation in $\mathcal{H}$. Letting $C(\cdot)$ denote the complexity measure of the function class, we show that for diverse training tasks (1) the sample complexity needed to learn the shared representation across the first $t$ training tasks scales as $C(\mathcal{H}) + t C(\mathcal{F})$, despite no explicit access to a signal from the feature representation and (2) with an accurate estimate of the representation, the sample complexity needed to learn a new task scales only with $C(\mathcal{F})$. Our results depend upon a new general notion of task diversity--applicable to models with general tasks, features, and losses--as well as a novel chain rule for Gaussian complexities. Finally, we exhibit the utility of our general framework in several models of importance in the literature.",0
"Transfer learning has become increasingly popular as a technique for deep neural network models to learn from one task and apply that knowledge to others. However, little research has been done on the role that task diversity plays in transfer learning. In this paper, we explore the impact of diverse tasks on the performance of pretrained models across multiple domains. Our findings suggest that task diversity is crucial in achieving strong generalization capabilities and outperforming specialized models trained exclusively on the target domain data. We evaluate the importance of task diversity using both quantitative experiments and qualitative analysis, providing insights into how various training conditions affect the robustness of learned representations. This study contributes to our understanding of transfer learning by highlighting the significance of task diversity as well as identifying optimal tradeoffs for designing effective pretraining strategies. Keywords: Deep Neural Networks; Transfer Learning; Task Diversity; Robust Representations; Pretraining Strategies Abstract: While there have been numerous advancements in the field of deep neural networks through the use of transfer learning techniques, little attention has been paid to exploring the impact of diverse tasks on improving model performance. In this paper, we investigate the critical role played by task diversity in facilitating better generalization capabilities of pretrained models across different domains. By conducting comprehensive experiments and performing detailed analyses, we demonstrate that incorporating heterogeneous tasks during pretraining yields substantial benefits over relying solely on homogenous task sets. Specifically, our results indicate t",1
"Scalable Vector Graphics (SVG) are ubiquitous in modern 2D interfaces due to their ability to scale to different resolutions. However, despite the success of deep learning-based models applied to rasterized images, the problem of vector graphics representation learning and generation remains largely unexplored. In this work, we propose a novel hierarchical generative network, called DeepSVG, for complex SVG icons generation and interpolation. Our architecture effectively disentangles high-level shapes from the low-level commands that encode the shape itself. The network directly predicts a set of shapes in a non-autoregressive fashion. We introduce the task of complex SVG icons generation by releasing a new large-scale dataset along with an open-source library for SVG manipulation. We demonstrate that our network learns to accurately reconstruct diverse vector graphics, and can serve as a powerful animation tool by performing interpolations and other latent space operations. Our code is available at https://github.com/alexandre01/deepsvg.",0
"Abstract: This paper introduces a novel deep learning architecture that enables efficient hierarchical generation of vector graphics animations from textual descriptions. Our model, called DeepSVG, leverages advances in generative adversarial networks (GANs) by incorporating two key innovations: firstly, we introduce a new type of generator network based on recurrent neural networks (RNNs), which allows us to model complex temporal dependencies in SVG animations. Secondly, we propose a hierarchical decomposition approach wherein high-level scene description is decomposed into smaller constituent parts and animated using our RNN generator, thus enabling more efficient computation and control over the animation process. Extensive experiments demonstrate significant improvements in fidelity and efficiency relative to previous state-of-the art methods for generating vector graphics animations from natural language inputs. We believe that DeepSVG represents an important step towards realizing the full potential of GANs for creative applications such as computer animation, digital design, and virtual reality content creation.",1
"High-throughput molecular profiling technologies have produced high-dimensional multi-omics data, enabling systematic understanding of living systems at the genome scale. Studying molecular interactions across different data types helps reveal signal transduction mechanisms across different classes of molecules. In this paper, we develop a novel Bayesian representation learning method that infers the relational interactions across multi-omics data types. Our method, Bayesian Relational Learning (BayReL) for multi-omics data integration, takes advantage of a priori known relationships among the same class of molecules, modeled as a graph at each corresponding view, to learn view-specific latent variables as well as a multi-partite graph that encodes the interactions across views. Our experiments on several real-world datasets demonstrate enhanced performance of BayReL in inferring meaningful interactions compared to existing baselines.",0
"Abstract  The integration of multi-omic data has become increasingly important in modern biology as we strive towards understanding complex systems at multiple levels. One major challenge facing researchers in this field is the need for effective methods that can handle the high dimensionality and complexity of these datasets. In this work, we propose BayRel: a novel method for Bayesian relational learning (BRL) for integrating multi-omic data. BRL offers a powerful framework for modeling complex relationships among variables by leveraging sparse graphical models and latent variable representations. Our approach extends traditional BRL methods by allowing us to learn complex dependencies across different types of omic features, including gene expression, metabolites, proteins, and others. We demonstrate through simulation studies and real world applications that our method outperforms existing approaches in terms of accuracy, scalability, interpretability, and robustness to noise. By providing an open source software implementation, we aim to facilitate wider adoption of this powerful tool in both academia and industry. This work represents an exciting step forward in empowering scientists to unlock new insights from their multi-omic datasets.",1
"Unsupervised and self-supervised learning approaches have become a crucial tool to learn representations for downstream prediction tasks. While these approaches are widely used in practice and achieve impressive empirical gains, their theoretical understanding largely lags behind. Towards bridging this gap, we present a unifying perspective where several such approaches can be viewed as imposing a regularization on the representation via a learnable function using unlabeled data. We propose a discriminative theoretical framework for analyzing the sample complexity of these approaches, which generalizes the framework of (Balcan and Blum, 2010) to allow learnable regularization functions. Our sample complexity bounds show that, with carefully chosen hypothesis classes to exploit the structure in the data, these learnable regularization functions can prune the hypothesis space, and help reduce the amount of labeled data needed. We then provide two concrete examples of functional regularization, one using auto-encoders and the other using masked self-supervision, and apply our framework to quantify the reduction in the sample complexity bound of labeled data. We also provide complementary empirical results to support our analysis.",0
"This paper presents a theoretical perspective on functional regularization for representation learning, introducing new insights into understanding how these methods can improve performance while ensuring robustness. By examining state-of-the-art techniques such as weight decay, Dropout, and early stopping under one framework, we aim to provide practitioners and researchers alike with a clear guidance on their usage and limitations. We show that functional regularization serves two primary goals - controlling complexity of model weights and improving generalization by encouraging diversity among ensemble members. Our analyses offer fresh perspectives into classical findings, revealing previously unknown connections between regularizers and reinterpreting well-known practices in light of our unifying theory. By bridging the gap between empiricism and formal analysis, we hope this work will inspire novel methodological developments and enhance trustworthiness in machine learning research. Keywords: functional regularization, representation learning, complexity control, weight decay, dropout, early stopping.",1
"A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.",0
"In recent years, contrastive learning has become one of the most popular techniques for training representations on large scale datasets like ImageNet. This approach involves constructing pairs of positive examples that are similar to each other and pairs of negative examples that are dissimilar to each other so as to encourage the model to learn meaningful features. However, there remains the challenge of data imbalance wherein certain classes may have more positive pairs compared to others which can lead to poor generalization performance. To address this issue, we propose debiased contrastive learning which involves balancing both positive and negative examples across all classes so that every class gets equal attention during the representation learning process. Our method achieves state-of-the art results on several benchmarks such as STL-10 and CIFAR-10 demonstrating its effectiveness in alleviating data imbalance issues in contrastive learning. Furthermore, our proposed method is simple and easy to implement requiring no architectural changes or additional hyperparameters making it very attractive for practitioners working with deep learning models.",1
"Learning interpretable and interpolatable latent representations has been an emerging research direction, allowing researchers to understand and utilize the derived latent space for further applications such as visual synthesis or recognition. While most existing approaches derive an interpolatable latent space and induces smooth transition in image appearance, it is still not clear how to observe desirable representations which would contain semantic information of interest. In this paper, we aim to learn meaningful representations and simultaneously perform semantic-oriented and visually-smooth interpolation. To this end, we propose an angular triplet-neighbor loss (ATNL) that enables learning a latent representation whose distribution matches the semantic information of interest. With the latent space guided by ATNL, we further utilize spherical semantic interpolation for generating semantic warping of images, allowing synthesis of desirable visual data. Experiments on MNIST and CMU Multi-PIE datasets qualitatively and quantitatively verify the effectiveness of our method.",0
"This research presents semantics-guided representation learning methods for visual synthesis tasks such as image generation, super-resolution, and image-to-image translation. By leveraging semantic guidance from natural language input or human annotations, we can learn more meaningful and interpretable representations that capture high-level concepts and properties of objects, scenes, and relationships. To achieve this goal, our framework integrates two key components: a) a latent space dynamics model that maps textual descriptions or pixel-wise annotations to continuous vector changes, and b) a variational autoencoder with adversarial training that encodes images into compact codes and reconstructs them based on both raw pixels and guiding vectors. Experiments show consistent improvements over baseline models across several benchmark datasets, demonstrating the effectiveness of our approach in using semantics to enhance visual synthesis performance and interpretability. We hope this work will pave the way towards a deeper understanding of how semantics interacts with deep neural networks and eventually lead to new applications involving data efficiency, cross-modality fusion, and knowledge integration.",1
"Value estimation is a critical component of the reinforcement learning (RL) paradigm. The question of how to effectively learn value predictors from data is one of the major problems studied by the RL community, and different approaches exploit structure in the problem domain in different ways. Model learning can make use of the rich transition structure present in sequences of observations, but this approach is usually not sensitive to the reward function. In contrast, model-free methods directly leverage the quantity of interest from the future, but receive a potentially weak scalar signal (an estimate of the return). We develop an approach for representation learning in RL that sits in between these two extremes: we propose to learn what to model in a way that can directly help value prediction. To this end, we determine which features of the future trajectory provide useful information to predict the associated return. This provides tractable prediction targets that are directly relevant for a task, and can thus accelerate learning the value function. The idea can be understood as reasoning, in hindsight, about which aspects of the future observations could help past value prediction. We show how this can help dramatically even in simple policy evaluation settings. We then test our approach at scale in challenging domains, including on 57 Atari 2600 games.",0
"In this work, we present value-driven hindsight modelling (VHM), a novel approach that utilizes human feedback to improve model performance by reweighting samples based on their alignment with desired outcomes. VHM takes as input raw data and learns an objective function through supervised learning from user corrections. By optimizing towards the learned objective instead of raw accuracy metrics such as cross entropy, the trained models achieve better calibration and generalization ability overseen datasets. We showcase VHM’s effectiveness in three diverse application areas: image generation, language translation, and speech synthesis, each requiring different levels of fine-grained control via the guidance mechanism. Extensive experiments on multiple benchmark datasets demonstrate that our method consistently improves upon state-of-the art baselines and achieves competitive results across all tasks, while providing users greater control over generated outputs without sacrificing ease of use. Our framework provides a new perspective on designing machine learning algorithms that interactively incorporate human knowledge into automated decision making processes. This research has important implications for fields where explainability, interpretability, and transparency are critical requirements, such as healthcare diagnosis, financial forecasting, and environmental monitoring among others.",1
"Self-supervised representation learning is an emerging research topic for its powerful capacity in learning with unlabeled data. As a mainstream self-supervised learning method, augmentation-based contrastive learning has achieved great success in various computer vision tasks that lack manual annotations. Despite current progress, the existing methods are often limited by extra cost on memory or storage, and their performance still has large room for improvement. Here we present a self-supervised representation learning method, namely AAG, which is featured by an auxiliary augmentation strategy and GNT-Xent loss. The auxiliary augmentation is able to promote the performance of contrastive learning by increasing the diversity of images. The proposed GNT-Xent loss enables a steady and fast training process and yields competitive accuracy. Experiment results demonstrate the superiority of AAG to previous state-of-the-art methods on CIFAR10, CIFAR100, and SVHN. Especially, AAG achieves 94.5% top-1 accuracy on CIFAR10 with batch size 64, which is 0.5% higher than the best result of SimCLR with batch size 1024.",0
"This paper presents a new method called self-supervised representation learning via auxiliary augmentation using the Gauss Newton Trust Region (GNT) loss. In recent years there has been increasing interest in developing self-supervised learning methods that can learn high quality representations from large amounts of unlabelled data without relying on human labels. Many existing approaches rely on pretext tasks such as image rotation prediction, solving jigsaw puzzles or predicting contextual neighbours of patches. However, these methods have limitations due to overfitting, lack of generalization and low performance compared to supervised models trained on small labelled datasets. In contrast, our proposed method uses the task of Gaussian Process Regression to learn meaningful representations which can improve significantly the state-of-the art results. Our framework starts from a fixed pre-trained model (ResNet-18), then we use batches of random transformations applied at train time only in order to make each input unique so that the network needs to memorize them individually. We experimented with seven types of random augmentations namely rotating (in multiples of 90 degrees), flipping along both horizontal and vertical axes, zooming in/out with up to 20% change, adding noise following normal distribution, gamma correction and equalizing histogram. These augmentations allow us to apply GNT-XentLoss. For this,we define two functions $x(\cdot)$ and $y(\cdot)$. The first maps the original image onto a latent space of size K=256 where the mean is subtracted out, while the second one is defined as identity function since the loss term applies directly between the pair $(x_i, x_j)\sim\rho$. Then for every training mini-batch B we create a number C",1
"In the computational prediction of chemical compound properties, molecular descriptors and fingerprints encoded to low dimensional vectors are used. The selection of proper molecular descriptors and fingerprints is both important and challenging as the performance of such models is highly dependent on descriptors. To overcome this challenge, natural language processing models that utilize simplified molecular input line-entry system as input were studied, and several transformer-variant models achieved superior results when compared with conventional methods. In this study, we explored the structural differences of the transformer-variant model and proposed a new self-attention based model. The representation learning performance of the self-attention module was evaluated in a multi-task learning environment using imbalanced chemical datasets. The experiment results showed that our model achieved competitive outcomes on several benchmark datasets. The source code of our experiment is available at https://github.com/arwhirang/sa-mtl and the dataset is available from the same URL.",0
"Title: ""Predictive Models Based On Molecular Structure And Smiles""  Abstract: The prediction of chemical properties is critical in many fields including drug discovery and development. In order to achieve better accuracy, machine learning methods have been developed that can predict chemical properties from molecular structure data alone. One challenge remains; how can we efficiently utilize limited amounts of labeled training data? To address this problem, self attention multi-task learning models were implemented which take advantage of unlabeled data through pretraining on other relevant tasks such as smiles encoding generation. Our results show improved model performance compared to traditional machine learning techniques while also outperforming previous state of the art approaches, thus demonstrating the effectiveness of these new methods in the field of chemistry research.",1
"Unsupervised anomaly detection aims to identify anomalous samples from highly complex and unstructured data, which is pervasive in both fundamental research and industrial applications. However, most existing methods neglect the complex correlation among data samples, which is important for capturing normal patterns from which the abnormal ones deviate. In this paper, we propose a method of Correlation aware unsupervised Anomaly detection via Deep Gaussian Mixture Model (CADGMM), which captures the complex correlation among data points for high-quality low-dimensional representation learning. Specifically, the relations among data samples are correlated firstly in forms of a graph structure, in which, the node denotes the sample and the edge denotes the correlation between two samples from the feature space. Then, a dual-encoder that consists of a graph encoder and a feature encoder, is employed to encode both the feature and correlation information of samples into the low-dimensional latent space jointly, followed by a decoder for data reconstruction. Finally, a separate estimation network as a Gaussian Mixture Model is utilized to estimate the density of the learned latent vector, and the anomalies can be detected by measuring the energy of the samples. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method.",0
"This paper presents a new approach for unsupervised anomaly detection using deep generative models. We introduce a novel method called correlation-aware deep generative model (CADGM) which takes into account the correlations among features in detecting anomalies. Our proposed CADGM outperforms state-of-the-art methods on several benchmark datasets while being efficient in terms of time complexity and memory usage. In addition, we provide extensive analysis showing that our approach is able to capture important characteristics of anomalies such as diversity and density. Overall, our results demonstrate the effectiveness of CADGM for unsupervised anomaly detection tasks.  This work contributes to the field by introducing a new solution for unsupervised anomaly detection. Our method is based on the idea of incorporating feature correlations into the training process of deep generative models. By doing so, we can learn more robust representations and achieve better performance compared to existing approaches. Furthermore, we show that our approach can handle high dimensional data efficiently and effectively. Finally, we hope that our findings inspire further research in the direction of developing new techniques for anomaly detection using machine learning methods.  The main contributions of our paper are:  1. Introduce a new approach for unsupervised anomaly detection using deep generative models. 2. Propose a novel method called correlation-aware deep generative model (CADGM) which considers correlations among features in detecting anomalies. 3. Show that our proposed CADGM outperforms state-of-the-art methods on multiple benchmark datasets, including image, audio and text data. 4. Provide an extensive analysis demonstrating that our approach captures important characteristic",1
"We propose a novel capsule network based variational encoder architecture, called Bayesian capsules (B-Caps), to modulate the mean and standard deviation of the sampling distribution in the latent space. We hypothesized that this approach can learn a better representation of features in the latent space than traditional approaches. Our hypothesis was tested by using the learned latent variables for image reconstruction task, where for MNIST and Fashion-MNIST datasets, different classes were separated successfully in the latent space using our proposed model. Our experimental results have shown improved reconstruction and classification performances for both datasets adding credence to our hypothesis. We also showed that by increasing the latent space dimension, the proposed B-Caps was able to learn a better representation when compared to the traditional variational auto-encoders (VAE). Hence our results indicate the strength of capsule networks in representation learning which has never been examined under the VAE settings before.",0
"In recent years we have seen the rise of capsule network architectures as one possible solution towards more efficient neural networks that generalize better than classical CNNs without losing accuracy on easy datasets. We propose a new variant called variational capsule encoder, which is based on a probabilistic formulation using a latent variable model similar to a VAE. It allows us to perform inference about object position and size while reconstructing images at high quality. Our method is trained end-to-end from raw pixels. By introducing novel techniques such as iterative inference and variational dropout, our approach can achieve state-of-the-art results on both MNIST and Street View House Numbers (SVHN) benchmark datasets in terms of test set accuracy, as well as superior robustness against common corruptions.",1
"3D Point clouds are a rich source of information that enjoy growing popularity in the vision community. However, due to the sparsity of their representation, learning models based on large point clouds is still a challenge. In this work, we introduce Graphite, a GRAPH-Induced feaTure Extraction pipeline, a simple yet powerful feature transform and keypoint detector. Graphite enables intensive down-sampling of point clouds with keypoint detection accompanied by a descriptor. We construct a generic graph-based learning scheme to describe point cloud regions and extract salient points. To this end, we take advantage of 6D pose information and metric learning to learn robust descriptions and keypoints across different scans. We Reformulate the 3D keypoint pipeline with graph neural networks which allow efficient processing of the point set while boosting its descriptive power which ultimately results in more accurate 3D registrations. We demonstrate our lightweight descriptor on common 3D descriptor matching and point cloud registration benchmarks and achieve comparable results with the state of the art. Describing 100 patches of a point cloud and detecting their keypoints takes only ~0.018 seconds with our proposed network.",0
"This paper presents a new feature extraction method called Graphite that uses graphs to encode features into point clouds. Specifically, Graphite constructs a graph from each point cloud by connecting pairs of points whose distance falls within some fixed range. Then, for any other point cloud P', Graphite finds the closest point(s) P'' in P using chamfer distances (as used in existing methods), computes the signed Euclidean distance between P' and P'', and adds edges between those two points in the original graph if their distance is smaller than some threshold value rho. The resulting subgraph then provides a compact representation of the local geometry around each node; registration can proceed via e.g., ICP. We validate our approach on several datasets of varying complexity, demonstrating improved accuracy over several state-of-the-art baselines. Our results indicate that Graphite effectively extracts discriminative and robust features that improve registration performance across different scenarios and sensor configurations. ------",1
"To leverage enormous unlabeled data on distributed edge devices, we formulate a new problem in federated learning called Federated Unsupervised Representation Learning (FURL) to learn a common representation model without supervision while preserving data privacy. FURL poses two new challenges: (1) data distribution shift (Non-IID distribution) among clients would make local models focus on different categories, leading to the inconsistency of representation spaces. (2) without the unified information among clients in FURL, the representations across clients would be misaligned. To address these challenges, we propose Federated Constrastive Averaging with dictionary and alignment (FedCA) algorithm. FedCA is composed of two key modules: (1) dictionary module to aggregate the representations of samples from each client and share with all clients for consistency of representation space and (2) alignment module to align the representation of each client on a base model trained on a public data. We adopt the contrastive loss for local model training. Through extensive experiments with three evaluation protocols in IID and Non-IID settings, we demonstrate that FedCA outperforms all baselines with significant margins.",0
"Title: Federated Unsupervised Representation Learning ===============================================  Unsupervised representation learning has emerged as a key area of research in artificial intelligence and machine learning, as it enables models to learn meaningful representations without any labeled data. However, many existing unsupervised methods rely on large amounts of training data that may not be available in practice. In addition, these methods often require access to computationally expensive model architectures and optimization algorithms that can be difficult to scale across different devices or networks. To address these challenges, we propose federated unsupervised representation learning (FURL), which leverages distributed computing resources to train shared representations at scale while minimizing communication overheads. Our approach uses a variational autoencoder (VAE) architecture, a popular choice for unsupervised representation learning, but extends it using novel techniques for communication efficiency and diversity control. We evaluate FURL using several benchmark datasets and demonstrate its effectiveness compared to both centralized and decentralized baseline methods. Our results suggest that FURL provides competitive performance for unsupervised representation learning under diverse settings, making it a promising framework for scalable and efficient knowledge discovery from big data.",1
"Learning causal effects from observational data greatly benefits a variety of domains such as health care, education and sociology. For instance, one could estimate the impact of a new drug on specific individuals to assist the clinic plan and improve the survival rate. In this paper, we focus on studying the problem of estimating Conditional Average Treatment Effect (CATE) from observational data. The challenges for this problem are two-fold: on the one hand, we have to derive a causal estimator to estimate the causal quantity from observational data, where there exists confounding bias; on the other hand, we have to deal with the identification of CATE when the distribution of covariates in treatment and control groups are imbalanced. To overcome these challenges, we propose a neural network framework called Adversarial Balancing-based representation learning for Causal Effect Inference (ABCEI), based on the recent advances in representation learning. To ensure the identification of CATE, ABCEI uses adversarial learning to balance the distributions of covariates in treatment and control groups in the latent representation space, without any assumption on the form of the treatment selection/assignment function. In addition, during the representation learning and balancing process, highly predictive information from the original covariate space might be lost. ABCEI can tackle this information loss problem by preserving useful information for predicting causal effects under the regularization of a mutual information estimator. The experimental results show that ABCEI is robust against treatment selection bias, and matches/outperforms the state-of-the-art approaches. Our experiments show promising results on several datasets, representing different health care domains among others.",0
"A promising approach towards improving representational learning from observational data comes through adversarial balancing methods that can learn complex and meaningful representations by identifying counterfactual directions guided by causal knowledge. This technique enables us to approximate, within neural networks, how changes made to variables affect unobserved outcomes based on their potential causal relationships using observational data alone. We introduce our novel method called “Adversarial Balancing,” which represents a hybrid of causal reasoning techniques and deep neural network architectures. Our goal herein lies in leveraging these insights into inferring cause-and-effect relationships among variables to guide effective representation learning from confounded observational datasets where explicit, direct observations of interventions are absent. Experimental evaluations indicate superior performance compared to several state-of-the-art approaches across diverse domains ranging from social science research to sensorimotor control in robotics applications.",1
"Causal inference, or counterfactual prediction, is central to decision making in healthcare, policy and social sciences. To de-bias causal estimators with high-dimensional data in observational studies, recent advances suggest the importance of combining machine learning models for both the propensity score and the outcome function. We propose a novel scalable method to learn double-robust representations for counterfactual predictions, leading to consistent causal estimation if the model for either the propensity score or the outcome, but not necessarily both, is correctly specified. Specifically, we use the entropy balancing method to learn the weights that minimize the Jensen-Shannon divergence of the representation between the treated and control groups, based on which we make robust and efficient counterfactual predictions for both individual and average treatment effects. We provide theoretical justifications for the proposed method. The algorithm shows competitive performance with the state-of-the-art on real world and synthetic data.",0
"This paper presents a novel approach for counterfactual prediction by combining two existing representation learning methods: Invariant Risk Minimization (IRM) and Model-Aware Synthetic Data Augmentation (MASDA). We propose double robust representation learning, which uses both IRM and MASDA together to improve performance on downstream tasks that involve predicting outcomes under different environments or policies. Our approach first trains a model using IRM to learn invariant representations that generalize across different environments. Then, we apply MASDA to generate synthetic training data that reflects potential changes in these environments. By incorporating both types of representation learning, our method can achieve better performance compared to using either technique alone. We evaluate our method on multiple datasets and show that it outperforms strong baselines across a variety of metrics. Our work provides insights into how different forms of representation learning can complement each other and contribute to improved accuracy in counterfactual prediction.",1
"We present $\Gamma$-nets, a method for generalizing value function estimation over timescale. By using the timescale as one of the estimator's inputs we can estimate value for arbitrary timescales. As a result, the prediction target for any timescale is available and we are free to train on multiple timescales at each timestep. Here we empirically evaluate $\Gamma$-nets in the policy evaluation setting. We first demonstrate the approach on a square wave and then on a robot arm using linear function approximation. Next, we consider the deep reinforcement learning setting using several Atari video games. Our results show that $\Gamma$-nets can be effective for predicting arbitrary timescales, with only a small cost in accuracy as compared to learning estimators for fixed timescales. $\Gamma$-nets provide a method for compactly making predictions at many timescales without requiring a priori knowledge of the task, making it a valuable contribution to ongoing work on model-based planning, representation learning, and lifelong learning algorithms.",0
"This is an abstract for a research paper on a new type of neural network architecture called Gamma-Net that can generalize value estimation across different timescales. The authors propose this novel architecture as a solution to address limitations in current models that struggle with extrapolating value estimates beyond short time horizons. By using gamma functions, Gamma-Nets are capable of capturing long term dependencies that existing architectures cannot capture. They demonstrate their effectiveness through rigorous evaluation on several benchmark datasets including Atari games, MuJoCo locomotion tasks, and D4RL navigation domains. Overall, Gamma-Nets represent an important step forward in developing artificial intelligence systems capable of learning effective representations of complex real world phenomena at multiple timescales.",1
"We prove a new upper bound on the generalization gap of classifiers that are obtained by first using self-supervision to learn a representation $r$ of the training data, and then fitting a simple (e.g., linear) classifier $g$ to the labels. Specifically, we show that (under the assumptions described below) the generalization gap of such classifiers tends to zero if $\mathsf{C}(g) \ll n$, where $\mathsf{C}(g)$ is an appropriately-defined measure of the simple classifier $g$'s complexity, and $n$ is the number of training samples. We stress that our bound is independent of the complexity of the representation $r$. We do not make any structural or conditional-independence assumptions on the representation-learning task, which can use the same training dataset that is later used for classification. Rather, we assume that the training procedure satisfies certain natural noise-robustness (adding small amount of label noise causes small degradation in performance) and rationality (getting the wrong label is not better than getting no label at all) conditions that widely hold across many standard architectures. We show that our bound is non-vacuous for many popular representation-learning based classifiers on CIFAR-10 and ImageNet, including SimCLR, AMDIM and MoCo.",0
"Self-supervised learning (SSL) has emerged as an increasingly popular technique in artificial intelligence research, allowing algorithms to learn from large amounts of unlabeled data without explicit guidance. However, there is ongoing debate within the field regarding how SSL algorithms should be evaluated and which criteria they should meet in order to achieve effective learning. In this paper, we argue that rational behavior by an algorithm implies generalization of its knowledge learned through self-supervision. We demonstrate our results using techniques drawn from mathematical logic, game theory, and computer science, including model checking, symbolic reasoning, and logical inference. Our findings have important implications for both theoretical developments in machine learning and practical applications of self-supervised models across domains such as natural language processing, image recognition, and robotics. The key contributions of this work can be summarized as follows: 🔹We establish rigorous foundations for studying generalization bounds under rational self-play assuming only computational resources are bounded while all other factors are considered fixed at their true values, but no prior assumption on bounding either domain or range spaces. This helps bridge the gap between self-play based methods and existing studies in decision making subject to uncertainty analysis and/or environment/domain randomness modelled via epsilon-optimal strategies, whose focuses were however limited to specific problem classes. 🔸By applying recent advances in multi-agent decision making under incomplete information we provide novel characterizations of conditions when truthful answers inevitably dominate any equilibrium outcome and showcase some corollaries concerning when different types of correlat",1
"Knowledge graph (KG) representation learning methods have achieved competitive performance in many KG-oriented tasks, among which the best ones are usually based on graph neural networks (GNNs), a powerful family of networks that learns the representation of an entity by aggregating the features of its neighbors and itself. However, many KG representation learning scenarios only provide the structure information that describes the relationships among entities, causing that entities have no input features. In this case, existing aggregation mechanisms are incapable of inducing embeddings of unseen entities as these entities have no pre-defined features for aggregation. In this paper, we present a decentralized KG representation learning approach, decentRL, which encodes each entity from and only from the embeddings of its neighbors. For optimization, we design an algorithm to distill knowledge from the model itself such that the output embeddings can continuously gain knowledge from the corresponding original embeddings. Extensive experiments show that the proposed approach performed better than many cutting-edge models on the entity alignment task, and achieved competitive performance on the entity prediction task. Furthermore, under the inductive setting, it significantly outperformed all baselines on both tasks.",0
"In this research paper we present our work on decentralized knowledge graph representation learning (DKGRL). We begin by providing background information on existing methods in natural language processing (NLP) and explain why they are insufficient for addressing the challenges associated with DKGRL. Next, we describe our approach to solving these problems using transfer learning from large pre-trained models like GPT-4. Finally, we evaluate our method through experiments and benchmarks against state-of-the art techniques, demonstrating its effectiveness in creating accurate representations of knowledge graphs at scale. Our results show that our proposed model outperforms competitors across multiple metrics, making significant contributions to advancing NLP capabilities for real world applications. Overall, our study has important implications for understanding how to use pre-training to improve performance in KGRL tasks while maintaining scalability.  Let me know if you need further assistance!",1
"Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on $51$ out of $57$ games.",0
"This paper presents a framework for representation learning that leverages invariant causal mechanisms (ICM). We first provide a formal definition of ICMs as mathematical objects that encode how different aspects of our environment impact one another through cause-and-effect relationships. Next, we propose a novel algorithm called Causal Discovery via Invariant Measurements (CDIM) which infers these mechanisms from observational data. Using synthetic experiments, we show that CDIM can accurately discover complex nonlinear causal graphs even under high levels of noise and confounding. Finally, we apply CDIM to real-world datasets such as fMRI brain imaging data where we demonstrate improved performance over state-of-the-art methods for both identifying causal features related to disease progression and predictive modeling tasks. Our work highlights the power of representing knowledge using causal models enriched by information on their underlying invariances due to interventions, opening up new opportunities for machine learning applications across diverse scientific domains.",1
"Graph convolution network (GCN) attracts intensive research interest with broad applications. While existing work mainly focused on designing novel GCN architectures for better performance, few of them studied a practical yet challenging problem: How to learn GCNs from data with extremely limited annotation? In this paper, we propose a new learning method by sampling strategy and model compression to overcome this challenge. Our approach has multifold advantages: 1) the adaptive sampling strategy largely suppresses the GCN training deviation over uniform sampling; 2) compressed GCN-based methods with a smaller scale of parameters need fewer labeled data to train; 3) the smaller scale of training data is beneficial to reduce the human resource cost to label them. We choose six popular GCN baselines and conduct extensive experiments on three real-world datasets. The results show that by applying our method, all GCN baselines cut down the annotation requirement by as much as 90$\%$ and compress the scale of parameters more than 6$\times$ without sacrificing their strong performance. It verifies that the training method could extend the existing semi-supervised GCN-based methods to the scenarios with the extremely small scale of labeled data.",0
"We present a method that can efficiently learn graph representations using extremely limited annotations. Our approach utilizes sampling and compression techniques to reduce computational costs while maintaining accuracy. Through extensive experiments on real world datasets, we demonstrate how our method outperforms state-of-the art baselines under stringent annotation constraints. Finally, we discuss possible applications and future directions for this work.",1
"We consider the problem of unsupervised domain adaptation for image classification. To learn target-domain-aware features from the unlabeled data, we create a self-supervised pretext task by augmenting the unlabeled data with a certain type of transformation (specifically, image rotation) and ask the learner to predict the properties of the transformation. However, the obtained feature representation may contain a large amount of irrelevant information with respect to the main task. To provide further guidance, we force the feature representation of the augmented data to be consistent with that of the original data. Intuitively, the consistency introduces additional constraints to representation learning, therefore, the learned representation is more likely to focus on the right information about the main task. Our experimental results validate the proposed method and demonstrate state-of-the-art performance on classical domain adaptation benchmarks. Code is available at https://github.com/Jiaolong/ss-da-consistency.",0
"In recent years there has been significant interest in self-supervised domain adaptation methods that allow neural networks trained on one task or dataset to generalize better to new unseen domains. These methods aim at transferring knowledge from large amounts of labeled data from several source tasks/domains, enabling more robustness to unlabeled target data distributions shift (e.g., due to different image acquisition conditions). In this work we show that adversarial training can enhance existing self-supervised learning approaches by enforcing consistency on multiple levels across layers of deep models: pixel level image generation, feature space mapping preservation along the network pipeline, and semantic scene understanding alignment. With these three complementary objectives combined into a single framework, we are able to demonstrate stateof-theart results on common benchmark datasets while using fewer annotations than previous works. We further analyze our approach qualitatively and quantitatively, showing improvements for both semantic segmentation and object detection under distribution shifts such as variations in lighting, texture, scale and background clutter. Our method outperforms other state-ofthe-art competitors and sets new records even without explicit supervision beyond some limited label information provided within each single source domain. Additionally, experiments conducted on smaller datasets prove the efficacy of our algorithm in low-data regimes where many realworld problems exist, demonstrating robustness to domain gaps far larger than previously reported in comparisons against alternative techniques. This study highlights the synergy of combining image synthesis with consistency constraints for high accuracy in cross-domain predictions, thus paving the way for future research directions in generative modeling f",1
"Since its inception, the neural estimation of mutual information (MI) has demonstrated the empirical success of modeling expected dependency between high-dimensional random variables. However, MI is an aggregate statistic and cannot be used to measure point-wise dependency between different events. In this work, instead of estimating the expected dependency, we focus on estimating point-wise dependency (PD), which quantitatively measures how likely two outcomes co-occur. We show that we can naturally obtain PD when we are optimizing MI neural variational bounds. However, optimizing these bounds is challenging due to its large variance in practice. To address this issue, we develop two methods (free of optimizing MI variational bounds): Probabilistic Classifier and Density-Ratio Fitting. We demonstrate the effectiveness of our approaches in 1) MI estimation, 2) self-supervised representation learning, and 3) cross-modal retrieval task.",0
"Title: Enhancing point-wise dependency estimation through neural methods  This paper presents novel techniques for improving the accuracy of point-wise dependency estimation using deep learning models. Traditional approaches to estimating dependencies rely on heuristics that can result in suboptimal results. In contrast, our proposed method leverages convolutional neural networks (CNNs) to learn complex patterns from large datasets, resulting in improved precision. Our approach involves training the network using synthetic data generated by randomly shuffling real data, allowing us to estimate dependencies without needing actual data pairs. We validate the effectiveness of our method by comparing its performance against standard baseline algorithms on benchmark datasets, demonstrating significant improvements in accuracy. Overall, this work represents a step forward in point-wise dependency estimation, with broad applications in areas such as natural language processing, computer vision, and time series analysis.",1
"Improving sample efficiency is a key research problem in reinforcement learning (RL), and CURL, which uses contrastive learning to extract high-level features from raw pixels of individual video frames, is an efficient algorithm~\citep{srinivas2020curl}. We observe that consecutive video frames in a game are highly correlated but CURL deals with them independently. To further improve data efficiency, we propose a new algorithm, masked contrastive representation learning for RL, that takes the correlation among consecutive inputs into consideration. In addition to the CNN encoder and the policy network in CURL, our method introduces an auxiliary Transformer module to leverage the correlations among video frames. During training, we randomly mask the features of several frames, and use the CNN encoder and Transformer to reconstruct them based on the context frames. The CNN encoder and Transformer are jointly trained via contrastive learning where the reconstructed features should be similar to the ground-truth ones while dissimilar to others. During inference, the CNN encoder and the policy network are used to take actions, and the Transformer module is discarded. Our method achieves consistent improvements over CURL on $14$ out of $16$ environments from DMControl suite and $21$ out of $26$ environments from Atari 2600 Games. The code is available at https://github.com/teslacool/m-curl.",0
"Artificial intelligence (AI) has revolutionized many industries by automating tasks that were previously thought to require human expertise and judgment. In recent years, one area where AI has made significant progress is reinforcement learning (RL), which involves training agents to make decisions based on feedback from their environment. One popular approach to RL is contrastive representation learning, which aims to learn representations that capture meaningful differences between states encountered by the agent. This paper introduces a new method called ""masked contrastive representation learning"" (MCR) that improves upon existing techniques by explicitly modeling uncertainty and enabling more effective exploration in complex environments. Our results demonstrate the effectiveness of our approach compared to state-of-the-art methods in several challenging domains. We believe that MCR has the potential to improve the efficiency and performance of RL algorithms and lead to better decision making in real-world applications.",1
"An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.",0
"Abstract: This paper presents a novel deep learning model called the ""Implicit Rank-Minimizing Autoencoder (IRMA)"". IRMA is designed to improve over the limitations of traditional autoencoders by incorporating recent advances in deep learning theory that allow us to optimize jointly for both reconstruction loss and ranking functions. Our key insight is that these objectives can be combined into a single optimization problem using the concept of implicit differentiation. We show that our approach leads to improved performance on challenging benchmark datasets compared to state-of-the-art methods. Additionally, we demonstrate how IRMA can effectively regularize other types of models such as generative adversarial networks, improving their stability during training and enhancing generated samples' quality. We believe that the proposed method has potential applications beyond computer vision and natural language processing tasks and could serve as a building block for future research in unsupervised representation learning.",1
"In this paper, we introduce InstantEmbedding, an efficient method for generating single-node representations using local PageRank computations. We theoretically prove that our approach produces globally consistent representations in sublinear time. We demonstrate this empirically by conducting extensive experiments on real-world datasets with over a billion edges. Our experiments confirm that InstantEmbedding requires drastically less computation time (over 9,000 times faster) and less memory (by over 8,000 times) to produce a single node's embedding than traditional methods including DeepWalk, node2vec, VERSE, and FastRP. We also show that our method produces high quality representations, demonstrating results that meet or exceed the state of the art for unsupervised representation learning on tasks like node classification and link prediction.",0
"This paper presents the InstantEmbedding algorithm which creates representations of local node neighborhoods based on their connection patterns in graphs without requiring full graph embedding. Unlike existing techniques that rely on global embedding methods such as deep learning models, our approach can handle large and dense graphs by only considering small subgraphs at each step and can provide interpretability through human readable embeddings. We validate our method using several benchmark datasets from different domains including social network analysis, biological networks, textual data and image classification tasks showing competitive performance compared to state of art approaches while being orders of magnitude faster. Our results demonstrate that InstantEmbedding can efficiently create meaningful local embeddings even for very large graphs enabling new applications where fast and scalable node similarity computations are required.",1
"In this paper, we address self-supervised representation learning from human skeletons for action recognition. Previous methods, which usually learn feature presentations from a single reconstruction task, may come across the overfitting problem, and the features are not generalizable for action recognition. Instead, we propose to integrate multiple tasks to learn more general representations in a self-supervised manner. To realize this goal, we integrate motion prediction, jigsaw puzzle recognition, and contrastive learning to learn skeleton features from different aspects. Skeleton dynamics can be modeled through motion prediction by predicting the future sequence. And temporal patterns, which are critical for action recognition, are learned through solving jigsaw puzzles. We further regularize the feature space by contrastive learning. Besides, we explore different training strategies to utilize the knowledge from self-supervised tasks for action recognition. We evaluate our multi-task self-supervised learning approach with action classifiers trained under different configurations, including unsupervised, semi-supervised and fully-supervised settings. Our experiments on the NW-UCLA, NTU RGB+D, and PKUMMD datasets show remarkable performance for action recognition, demonstrating the superiority of our method in learning more discriminative and general features. Our project website is available at https://langlandslin.github.io/projects/MSL/.",0
"In recent years, self-supervised learning has become an increasingly popular approach in computer vision tasks due to its ability to learn representations from large amounts of unlabeled data. One such task that can benefit greatly from self-supervision is action recognition, which involves identifying actions from skeletal motion capture data. To address the challenges posed by action recognition, we propose a novel multi-task self-supervised learning framework called MS$^2$L. Our proposed method utilizes multiple pretext tasks during training to better leverage the available data and improve performance on the target downstream action classification task. We evaluate our method using several benchmark datasets and demonstrate state-of-the-art results across all metrics. By combining different types of pretext tasks under one framework, we achieve significant improvements over previous methods and showcase the effectiveness of our proposed approach in tackling action recognition problems. With the growing interest in deep learning for human behavior analysis, our work represents a significant step towards improving skeleton-based action recognition models through effective self-supervised learning techniques.",1
"Representation Learning in a heterogeneous space with mixed variables of numerical and categorical types has interesting challenges due to its complex feature manifold. Moreover, feature learning in an unsupervised setup, without class labels and a suitable learning loss function, adds to the problem complexity. Further, the learned representation and subsequent predictions should not reflect discriminatory behavior towards certain sensitive groups or attributes. The proposed feature map should preserve maximum variations present in the data and needs to be fair with respect to the sensitive variables. We propose, in the first phase of our work, an efficient encoder-decoder framework to capture the mixed-domain information. The second phase of our work focuses on de-biasing the mixed space representations by adding relevant fairness constraints. This ensures minimal information loss between the representations before and after the fairness-preserving projections. Both the information content and the fairness aspect of the final representation learned has been validated through several metrics where it shows excellent performance. Our work (FairMixRep) addresses the problem of Mixed Space Fair Representation learning from an unsupervised perspective and learns a Universal representation that is timely, unique, and a novel research contribution.",0
"In today's world where data driven decision making has become more prevalent than ever before, there exists a need to represent different types of heterogenous data sets using robust representation learning techniques. This becomes even more critical when dealing with complex real life datasets that have underlying fairness concerns associated with them. One such example can be seen in societal applications like human capital management systems used by firms, which use algorithms based on educational background as one factor among many in determining hiring decisions. Clearly, these algorithms must take into account education level as well as other factors affecting job performance while ensuring parity in outcome across protected groups. Other examples include healthcare, criminal justice, bank lending etc.  In order to address these challenges, we propose a new framework called ""FairMixRep"" based on self supervised robust representations learning for diverse datasets while maintaining fairness criteria. Our proposed method works by creating virtual training distributions from the original ones that satisfy given statistical parities while preserving the informational content required to train accurate models. We show through extensive experiments on several benchmark datasets, that our approach consistently outperforms traditional unsupervised baselines, as well as state of the art methods for both fair representation learning as well as standard representation learning objectives. Additionally, we demonstrate how incorporating fairness metrics within a deep model improves interpretability for societally beneficial decision making and reduces unwanted disparate impacts arising due to blind usage of machine learning tools.",1
"In this paper, we study network representation learning for tripartite heterogeneous networks which learns node representation features for networks with three types of node entities. We argue that tripartite networks are common in real world applications, and the essential challenge of the representation learning is the heterogeneous relations between various node types and links in the network. To tackle the challenge, we develop a tripartite heterogeneous network embedding called TriNE. The method considers unique user-item-tag tripartite relationships, to build an objective function to model explicit relationships between nodes (observed links), and also capture implicit relationships between tripartite nodes (unobserved links across tripartite node sets). The method organizes metapath guided random walks to create heterogeneous neighborhood for all node types in the network. This information is then utilized to train a heterogeneous skip-gram model based on a joint optimization. Experiments on real-world tripartite networks validate the performance of TriNE for the online user response prediction using embedding node features.",0
"Abstract: This paper presents a novel framework called ""TriNE"" that leverages tripartite networks representation learning (TLR) on heterogeneous graphs. In contrast to traditional representation learning approaches that focus exclusively on unipartite or bipartite relationships, our model can capture both high-order connectivity patterns as well as attribute information across multiple types of nodes and edges. This enables us to simultaneously learn embeddings that preserve structural information from all three aspects of the network - a capability which has proven crucial in applications such as recommender systems, social media analysis, and biological knowledge discovery. Our extensive experimental evaluation demonstrates that TLR significantly outperforms competing methods across four diverse datasets. We believe our work represents a significant contribution towards bridging the gap between relational learning and multi-modal data fusion; advancing the state-of-the-art in Graph Neural Network research. Keywords: tripartite graph representation, embedding learning, node classification",1
"Group re-identification (G-ReID) is an important yet less-studied task. Its challenges not only lie in appearance changes of individuals which have been well-investigated in general person re-identification (ReID), but also derive from group layout and membership changes. So the key task of G-ReID is to learn representations robust to such changes. To address this issue, we propose a Transferred Single and Couple Representation Learning Network (TSCN). Its merits are two aspects: 1) Due to the lack of labelled training samples, existing G-ReID methods mainly rely on unsatisfactory hand-crafted features. To gain the superiority of deep learning models, we treat a group as multiple persons and transfer the domain of a labeled ReID dataset to a G-ReID target dataset style to learn single representations. 2) Taking into account the neighborhood relationship in a group, we further propose learning a novel couple representation between two group members, that achieves more discriminative power in G-ReID tasks. In addition, an unsupervised weight learning method is exploited to adaptively fuse the results of different views together according to result patterns. Extensive experimental results demonstrate the effectiveness of our approach that significantly outperforms state-of-the-art methods by 11.7\% CMC-1 on the Road Group dataset and by 39.0\% CMC-1 on the DukeMCMT dataset.",0
"In recent years, deep learning has been increasingly applied to computer vision tasks such as image recognition, object detection, and face verification. However, one area that remains challenging is group re-identification (GR), which involves identifying individuals within a large dataset across multiple cameras or time periods. Traditional methods often rely on template matching or feature extraction, but these approaches can fail when there are significant changes in appearance or illumination conditions. In our paper, we propose a novel approach called ""DotSCN"" for GR by leveraging domain transferred representation learning. Our method uses two types of representations - single images and couples extracted from the original data - to learn features that are invariant across domains. This helps the model generalize well even under difficult scenarios where there might be occlusions, pose variations, or other complex factors at play. We evaluate our approach using several benchmark datasets and demonstrate state-of-the-art performance compared to existing techniques. Our results showcase the potential of transferring learned features across domains for improving GR accuracy while addressing some of the limitations associated with traditional methods. Overall, our work offers a promising direction for future researchers interested in tackling GR problems.",1
"Tensor ring (TR) decomposition is a powerful tool for exploiting the low-rank nature of multiway data and has demonstrated great potential in a variety of important applications. In this paper, nonnegative tensor ring (NTR) decomposition and graph regularized NTR (GNTR) decomposition are proposed, where the former equips TR decomposition with local feature extraction by imposing nonnegativity on the core tensors and the latter is additionally able to capture manifold geometry information of tensor data, both significantly extend the applications of TR decomposition for nonnegative multiway representation learning. Accelerated proximal gradient based methods are derived for NTR and GNTR. The experimental result demonstrate that the proposed algorithms can extract parts-based basis with rich colors and rich lines from tensor objects that provide more interpretable and meaningful representation, and hence yield better performance than the state-of-the-art tensor based methods in clustering and classification tasks.",0
"This work presents a new method for multiway representation learning called graph regularized nonnegative tensor ring decomposition (GRTD). GRTD extends traditional matrix factorization techniques to higher dimensions by decomposing tensors into sums of rank one tensors that form a ring structure. Our proposed algorithm uses a graph to enforce regularity constraints on the resulting factors, which promotes interpretability and identifiability of the underlying representations. We show how our approach can effectively capture complex relationships across multiple modes of data while preserving nonnegativity and orthogonality properties. Experimental results demonstrate the effectiveness of our method in tasks such as image and video denoising, bioinformatics, and collaborative filtering. Overall, we believe that our contribution provides a valuable tool for exploring high dimensional data sets, uncovering hidden structures, and performing predictive modeling tasks.",1
"Over the past decade, multivariate time series classification (MTSC) has received great attention with the advance of sensing techniques. Current deep learning methods for MTSC are based on convolutional and recurrent neural network, with the assumption that time series variables have the same effect to each other. Thus they cannot model the pairwise dependencies among variables explicitly. What's more, current spatial-temporal modeling methods based on GNNs are inherently flat and lack the capability of aggregating node information in a hierarchical manner. To address this limitation and attain expressive global representation of MTS, we propose a graph pooling based framework MTPool and view MTSC task as graph classification task. With graph structure learning and temporal convolution, MTS slices are converted to graphs and spatial-temporal features are extracted. Then, we propose a novel graph pooling method, which uses an ``encoder-decoder'' mechanism to generate adaptive centroids for cluster assignments. GNNs and graph pooling layers are used for joint graph representation learning and graph coarsening. With multiple graph pooling layers, the input graphs are hierachically coarsened to one node. Finally, differentiable classifier takes this coarsened one-node graph as input to get the final predicted class. Experiments on 10 benchmark datasets demonstrate MTPool outperforms state-of-the-art methods in MTSC tasks.",0
"Abstract. Deep learning has made great strides in solving many computer vision tasks including image classification, object detection, semantic segmentation, and others [2]. However, time series problems have received less attention from deep learning methods which tend to struggle with sequence data due to their limited ability to capture complex temporal dependencies and patterns such as correlations across different features over time [6][7]. Recently proposed approaches exploiting Convolutional Neural Networks (CNN) architectures on top of windowed views of raw time-series signals seem promising but often require substantial hand engineering or domain knowledge [9]. In our paper we aim at introducing a novel architecture capable of capturing hierarchies of non-linear interactions among multiple variables without imposing predefined constraints to the problem formulation while maintaining efficiency on modern GPU hardware [13]. We propose a new end-to-end trainable network called HVGNet that incorporates graph pooling layers built on top of shared variational autoencoders to model high-level abstractions from the raw inputs. The resulting hierarchy of graphs enables efficient yet expressive modelling of higher level relations among multiple time-series variables through message passing and inference techniques from graph neural networks literature. Empirically, we demonstrate state-of-the-art performance on four benchmark datasets popularly used for evaluating multivariate time series forecasting models. Our results show that HVGNet outperforms established time series models by significant margins as well as competitive deep learning baselines adapted from other fields. Further analysis suggests that the learned representations generalise well on unseen domains and yield insights into human interpretable features driving prediction accuracy. Overall, th",1
"Graph Neural Network (GNN) aggregates the neighborhood of each node into the node embedding and shows its powerful capability for graph representation learning. However, most existing GNN variants aggregate the neighborhood information in a fixed non-injective fashion, which may map different graphs or nodes to the same embedding, reducing the model expressiveness. We present a theoretical framework to design a continuous injective set function for neighborhood aggregation in GNN. Using the framework, we propose expressive GNN that aggregates the neighborhood of each node with a continuous injective set function, so that a GNN layer maps similar nodes with similar neighborhoods to similar embeddings, different nodes to different embeddings and the equivalent nodes or isomorphic graphs to the same embeddings. Moreover, the proposed expressive GNN can naturally learn expressive representations for graphs with continuous node attributes. We validate the proposed expressive GNN (ExpGNN) for graph classification on multiple benchmark datasets including simple graphs and attributed graphs. The experimental results demonstrate that our model achieves state-of-the-art performances on most of the benchmarks.",0
"In recent years, graph representation has become increasingly important in computer science due to the growing number of applications that rely on graphs as data structures. However, current methods for representing graphs can lack expressivity and flexibility, leading to difficulties in accurately modeling real-world scenarios. This paper proposes a new approach towards expressive graph representation which addresses these limitations by introducing novel concepts that increase the expressiveness and adaptability of graph models. Through case studies and experiments, we demonstrate how our methodology enables more accurate modeling of complex systems while maintaining computational efficiency. Our results highlight the potential benefits of using our proposed framework across various domains, from social network analysis to knowledge management, paving the way for further advancements in the field of graph representation.",1
"Graph convolutional networks (GCNs) have been widely used for representation learning on graph data, which can capture structural patterns on a graph via specifically designed convolution and readout operations. In many graph classification applications, GCN-based approaches have outperformed traditional methods. However, most of the existing GCNs are inefficient to preserve local information of graphs -- a limitation that is especially problematic for graph classification. In this work, we propose a locality-preserving dense GCN with graph context-aware node representations. Specifically, our proposed model incorporates a local node feature reconstruction module to preserve initial node features into node representations, which is realized via a simple but effective encoder-decoder mechanism. To capture local structural patterns in neighbourhoods representing different ranges of locality, dense connectivity is introduced to connect each convolutional layer and its corresponding readout with all previous convolutional layers. To enhance node representativeness, the output of each convolutional layer is concatenated with the output of the previous layer's readout to form a global context-aware node representation. In addition, a self-attention module is introduced to aggregate layer-wise representations to form the final representation. Experiments on benchmark datasets demonstrate the superiority of the proposed model over state-of-the-art methods in terms of classification accuracy.",0
"This research paper proposes a novel approach to graph convolutional networks (GCN) that preserves locality while improving performance on dense graphs. Our method utilizes two key components: locally consistent neighborhood sampling and graph context-aware node representations. Through our algorithmic design, we can efficiently approximate graph convolutions on large datasets by leveraging precomputed graph contextual information at a fraction of the computational cost. We demonstrate the effectiveness of our technique through extensive experiments across multiple benchmark datasets, outperforming state-of-the-art methods in both accuracy and efficiency. By providing more accurate results at significantly reduced computational costs, our work has important implications for real-world applications such as social network analysis and image processing using GCNs.",1
"Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",0
"In recent years, hypergraphs have emerged as a powerful tool in data mining, computer vision, natural language processing, and other domains due to their ability to model complex relationships among entities. However, designing effective methods for learning hypergraph representations remains a challenging task because traditional graph convolution operations fail to capture high-order interactions that exist in these structures. In this work, we propose two novel models that utilize hypergraph convolution and attention mechanisms to learn expressive and interpretable node embeddings from hypergraph data. Our contributions can be summarized as follows:  We introduce hypergraph convolution architecture capable of capturing rich interactions among nodes in multiple hyperedge components by defining different filter weights for each edge type and adaptively fusing them using neural networks. To address the issue of computational complexity caused by large kernels used in existing approaches, we develop kernel fusion techniques based on matrix factorizations to drastically reduce computational requirements without loss in performance. We present two variants of our approach: (i) HGConv for unweighted hypergraphs and (ii) HGCN for weighted ones. We empirically validate both models on benchmark datasets commonly used in social network analysis and image classification tasks and show that they significantly outperform state-of-the-art baselines across all evaluation metrics. Furthermore, we demonstrate how hypergraph attention modules can effectively refine the learned embedding vectors by selectively emphasizing important edges depending on the application at hand. Finally, through ablation studies and parameter analyses, we provide insights into the design choices made throughout our framework development. Overall, our findings highlight the effectiveness of hypergraph representation learning algorithms and inspire future research directions in t",1
"An unsolved fundamental problem in biology and ecology is to predict observable traits (phenotypes) from a new genetic constitution (genotype) of an organism under environmental perturbations (e.g., drug treatment). The emergence of multiple omics data provides new opportunities but imposes great challenges in the predictive modeling of genotype-phenotype associations. Firstly, the high-dimensionality of genomics data and the lack of labeled data often make the existing supervised learning techniques less successful. Secondly, it is a challenging task to integrate heterogeneous omics data from different resources. Finally, the information transmission from DNA to phenotype involves multiple intermediate levels of RNA, protein, metabolite, etc. The higher-level features (e.g., gene expression) usually have stronger discriminative power than the lower level features (e.g., somatic mutation). To address above issues, we proposed a novel Cross-LEvel Information Transmission network (CLEIT) framework. CLEIT aims to explicitly model the asymmetrical multi-level organization of the biological system. Inspired by domain adaptation, CLEIT first learns the latent representation of high-level domain then uses it as ground-truth embedding to improve the representation learning of the low-level domain in the form of contrastive loss. In addition, we adopt a pre-training-fine-tuning approach to leveraging the unlabeled heterogeneous omics data to improve the generalizability of CLEIT. We demonstrate the effectiveness and performance boost of CLEIT in predicting anti-cancer drug sensitivity from somatic mutations via the assistance of gene expressions when compared with state-of-the-art methods.",0
"In recent years, there has been increasing interest in using genetic data to predict phenotypic outcomes such as disease risk or response to treatment. However, traditional approaches that rely on individual loci or small sets of features have limitations in their ability to capture complex interactions and patterns across multiple levels of biological organization. To address these challenges, we developed a cross-level information transmission network (CLITN) approach that integrates different types of genomic and clinical data to improve predictions of phenotypes. We apply our methodology to cancer precision medicine by building CLITNs that can accurately predict patient survival outcome and drug sensitivity profile based solely on the tumor genome sequence data. Our results demonstrate the effectiveness of our approach in capturing nonlinear relationships between molecular events and clinical outcomes, highlighting the potential value of multi-scale integration in advancing personalized medicine research. Overall, this work provides new insights into understanding how multiscale data fusion can improve predictions of human traits, leading to more accurate diagnosis and treatment of diseases.",1
"Graphs are the most ubiquitous form of structured data representation used in machine learning. They model, however, only pairwise relations between nodes and are not designed for encoding the higher-order relations found in many real-world datasets. To model such complex relations, hypergraphs have proven to be a natural representation. Learning the node representations in a hypergraph is more complex than in a graph as it involves information propagation at two levels: within every hyperedge and across the hyperedges. Most current approaches first transform a hypergraph structure to a graph for use in existing geometric deep learning algorithms. This transformation leads to information loss, and sub-optimal exploitation of the hypergraph's expressive power. We present HyperSAGE, a novel hypergraph learning framework that uses a two-level neural message passing strategy to accurately and efficiently propagate information through hypergraphs. The flexible design of HyperSAGE facilitates different ways of aggregating neighborhood information. Unlike the majority of related work which is transductive, our approach, inspired by the popular GraphSAGE method, is inductive. Thus, it can also be used on previously unseen nodes, facilitating deployment in problems such as evolving or partially observed hypergraphs. Through extensive experimentation, we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets. We also demonstrate that the higher expressive power of HyperSAGE makes it more stable in learning node representations as compared to the alternatives.",0
"Machine learning models can greatly benefit from representing patterns found in large amounts of data using hypergraphs as inputs, since they naturally allow relationships between multiple instances of the same object (or ""nodes"") to be captured in full generality, while ensuring that all necessary local constraints among features are met at once. In practice, however, learning inductive representations of node subsets over these hyperedges requires new methodologies beyond today's approaches based on graph neural networks (GNNs). To address this challenge, we introduce HyperSAGE, which makes key innovations in both representation learning and architectural design for GCNs over hypergraphs.  In particular, HyperSage advances state-of-the-art performance across diverse tasks including edge prediction, benchmark molecule property regression, link prediction, and recommender systems by outperforming competitive baselines. Our approach uses novel permutation invariant message passing combined with self attention mechanisms inspired by transformer architecture design choices to obtain better context processing. Moreover, we integrate several unique designs into our model, such as initializing edge embeddings via node classification so their values directly capture predictive signal regarding their connectedness, thus reducing training time without sacrificing accuracy compared to other methods like pre-training via random walks. Furthermore, we make explicit use of known topological and metric properties about edges through carefully designed prior knowledge injection modules, allowing our framework to learn better from noisy data. Finally, extensive ablation studies rigorously evaluate each component choice throughout the paper.  Overall, HyperSage significantly improves upon current practices for learning meaningful hypergraph structure for machine learning problems; given its improved sample efficiency, increased robustness to noise, ability to capture complex multi-relational dependencies, and high scalability provided by its ef",1
"Meta-learning approaches have shown great success in vision and language domains. However, few studies discuss the practice of meta-learning for large-scale industrial applications. Although e-commerce companies have spent many efforts on learning representations to provide a better user experience, we argue that such efforts cannot be stopped at this step. In addition to learning a strong profile, the challenging question about how to effectively transfer the learned representation is raised simultaneously. This paper introduces the contributions that we made to address these challenges from three aspects. 1) Meta-learning model: In the context of representation learning with e-commerce user behavior data, we propose a meta-learning framework called the Meta-Profile Network, which extends the ideas of matching network and relation network for knowledge transfer and fast adaptation; 2) Encoding strategy: To keep high fidelity of large-scale long-term sequential behavior data, we propose a time-heatmap encoding strategy that allows the model to encode data effectively; 3) Deep network architecture: A multi-modal model combined with multi-task learning architecture is utilized to address the cross-domain knowledge learning and insufficient label problems. Moreover, we argue that an industrial model should not only have good performance in terms of accuracy, but also have better robustness and uncertainty performance under extreme conditions. We evaluate the performance of our model with extensive control experiments in various extreme scenarios, i.e. out-of-distribution detection, data insufficiency and class imbalance scenarios. The Meta-Profile Network shows significant improvement in the model performance when compared to baseline models.",0
"This paper presents a new method for improving few-shot learning performance by constructing user meta-profile networks that capture relationships between users based on their interaction history. By leveraging these relationships, our approach can effectively generate synthetic training data from other similar tasks, allowing for improved few-shot learning performance across a range of domains. Experiments demonstrate significant gains over several state-of-the-art methods for few-shot classification, providing strong evidence of the effectiveness of our proposed approach. Overall, we believe that this work represents an important step towards building more intelligent machine learning systems capable of efficient adaptation to new situations and environments.",1
"Current autonomous driving systems are composed of a perception system and a decision system. Both of them are divided into multiple subsystems built up with lots of human heuristics. An end-to-end approach might clean up the system and avoid huge efforts of human engineering, as well as obtain better performance with increasing data and computation resources. Compared to the decision system, the perception system is more suitable to be designed in an end-to-end framework, since it does not require online driving exploration. In this paper, we propose a novel end-to-end approach for autonomous driving perception. A latent space is introduced to capture all relevant features useful for perception, which is learned through sequential latent representation learning. The learned end-to-end perception model is able to solve the detection, tracking, localization and mapping problems altogether with only minimum human engineering efforts and without storing any maps online. The proposed method is evaluated in a realistic urban driving simulator, with both camera image and lidar point cloud as sensor inputs. The codes and videos of this work are available at our github repo and project website.",0
"Title: ""End-to-end Autonomous Driving Perception with Sequential Latent Representation Learning""  Abstract: This paper presents an end-to-end autonomous driving perception framework that utilizes sequential latent representation learning (SLRL) to generate high-quality pixel-level semantic segmentations of road scenes from raw sensor data. Our approach overcomes several challenges faced by traditional pipeline methods, which rely on handcrafted features and require laborious manual annotation. By combining convolutional neural networks (CNNs) with graphical models, we create a powerful model capable of jointly reasoning about temporal dependencies and spatial structure within scene context. We evaluate our method using two public datasets, KITTI and Cityscapes, achieving state-of-the-art performance while operating at real-time speeds. In summary, our work demonstrates the effectiveness and efficiency of SLRL for enabling fully autonomous vehicles to perceive complex environments with unprecedented accuracy.",1
"Disentangled representation learning has undoubtedly benefited from objective function surgery. However, a delicate balancing act of tuning is still required in order to trade off reconstruction fidelity versus disentanglement. Building on previous successes of penalizing the total correlation in the latent variables, we propose TCWAE (Total Correlation Wasserstein Autoencoder). Working in the WAE paradigm naturally enables the separation of the total-correlation term, thus providing disentanglement control over the learned representation, while offering more flexibility in the choice of reconstruction cost. We propose two variants using different KL estimators and perform extensive quantitative comparisons on data sets with known generative factors, showing competitive results relative to state-of-the-art techniques. We further study the trade off between disentanglement and reconstruction on more-difficult data sets with unknown generative factors, where the flexibility of the WAE paradigm in the reconstruction term improves reconstructions.",0
"Recently, there has been significant interest in developing machine learning models that can learn disentangled representations, which capture meaningful underlying factors of variation in data. One popular approach towards achieving this goal is through the use of autoencoders, which are neural network architectures designed to reconstruct inputs from encoded versions of themselves. In particular, the Wasserstein Autoencoder (WAE) is a variant of the traditional autoencoder that utilizes the Earth Mover's Distance as a reconstruction loss function, encouraging the learned representation to be more interpretable and informative. This paper presents research on using the WAE to learn disentangled representations in various datasets and tasks, including image generation and feature learning. Experimental results demonstrate the effectiveness of the WAE in producing high quality, task relevant outputs while promoting sparsity and interpretability of the learned features. Additionally, we provide insights into the theoretical understanding of why this model works well and how it compares to other state-of-the art methods. Overall, our findings suggest that the WAE is a powerful tool for uncovering latent structure in complex datasets, paving the way for improved performance in downstream applications such as computer vision and natural language processing.",1
"Person re-identification (Re-ID) aims to match a target person across camera views at different locations and times. Existing Re-ID studies focus on the short-term cloth-consistent setting, under which a person re-appears in different camera views with the same outfit. A discriminative feature representation learned by existing deep Re-ID models is thus dominated by the visual appearance of clothing. In this work, we focus on a much more difficult yet practical setting where person matching is conducted over long-duration, e.g., over days and months and therefore inevitably under the new challenge of changing clothes. This problem, termed Long-Term Cloth-Changing (LTCC) Re-ID is much understudied due to the lack of large scale datasets. The first contribution of this work is a new LTCC dataset containing people captured over a long period of time with frequent clothing changes. As a second contribution, we propose a novel Re-ID method specifically designed to address the cloth-changing challenge. Specifically, we consider that under cloth-changes, soft-biometrics such as body shape would be more reliable. We, therefore, introduce a shape embedding module as well as a cloth-elimination shape-distillation module aiming to eliminate the now unreliable clothing appearance features and focus on the body shape information. Extensive experiments show that superior performance is achieved by the proposed model on the new LTCC dataset. The code and dataset will be available at https://naiq.github.io/LTCC_Perosn_ReID.html.",0
"Long-term cloth changing person re-identification (LTCCPR) refers to the task of identifying individuals across multiple cameras over time even if they change their clothes. This can present unique challenges due to changes in appearance that occur as a result of clothing. In order to address these challenges, advanced methods have been developed using deep learning techniques. These approaches rely on the use of Convolutional Neural Networks (CNNs), which learn features from images to create robust representations of individuals. By leveraging both global and local feature descriptors, LTCCPR has seen significant improvements in accuracy compared to previous methods based solely on image similarity metrics such as Eigenfaces and Local Binary Pattern Histograms. However, there remain limitations to existing approaches, including high computational requirements and sensitivity to changes in lighting conditions. Future work should focus on developing more efficient algorithms and exploring new ways to capture additional cues that may improve performance in real-world scenarios where conditions are less controlled. Ultimately, advances in LTCCPR have important applications in areas such as public safety, retail analytics, and traffic management.",1
"In graph neural networks (GNNs), pooling operators compute local summaries of input graphs to capture their global properties, and they are fundamental for building deep GNNs that learn hierarchical representations. In this work, we propose the Node Decimation Pooling (NDP), a pooling operator for GNNs that generates coarser graphs while preserving the overall graph topology. During training, the GNN learns new node representations and fits them to a pyramid of coarsened graphs, which is computed offline in a pre-processing stage. NDP consists of three steps. First, a node decimation procedure selects the nodes belonging to one side of the partition identified by a spectral algorithm that approximates the \maxcut{} solution. Afterwards, the selected nodes are connected with Kron reduction to form the coarsened graph. Finally, since the resulting graph is very dense, we apply a sparsification procedure that prunes the adjacency matrix of the coarsened graph to reduce the computational cost in the GNN. Notably, we show that it is possible to remove many edges without significantly altering the graph structure. Experimental results show that NDP is more efficient compared to state-of-the-art graph pooling operators while reaching, at the same time, competitive performance on a significant variety of graph classification tasks.",0
"Increasingly, graph neural networks (GNNs) have proven effective at tasks involving complex relationships between entities represented as graphs. At the core of GNNs is their ability to process hierarchical representations of data: node features can be transformed to encode higher-level relations and dependencies within the graph. However, existing methods often fail to capture deep structure because they only use local message passing, which limits their expressiveness. To mitigate these limitations, we introduce a new technique called Node Decimation Pooling that improves the scalability and interpretability of large GNN models while preserving their power for modeling hierarchical representations. Our method involves iteratively applying different pooling operations to aggregate nodes into increasingly coarser clusters before aggregating cluster embeddings over time steps during inference. This allows us to learn high-quality initializations that make it possible to train larger GNNs on real datasets without losing accuracy. Through a variety of experiments, we show that our approach significantly outperforms baseline methods across multiple domains including social network analysis, computer vision, and language processing. Additionally, we demonstrate how interpretable clusters learned by our method can provide insightful summaries of complex patterns in large graphs. Overall, our work represents an important step forward towards building more powerful and efficient GNN architectures capable of capturing hierarchical representations from vast amounts of interconnected data. We believe our techniques will inspire future research in graph learning, helping tackle some of society’s most challenging problems, from drug discovery to understanding human social behavior.",1
"We develop a technique for generating smooth and accurate 3D human pose and motion estimates from RGB video sequences. Our method, which we call Motion Estimation via Variational Autoencoder (MEVA), decomposes a temporal sequence of human motion into a smooth motion representation using auto-encoder-based motion compression and a residual representation learned through motion refinement. This two-step encoding of human motion captures human motion in two stages: a general human motion estimation step that captures the coarse overall motion, and a residual estimation that adds back person-specific motion details. Experiments show that our method produces both smooth and accurate 3D human pose and motion estimates.",0
"This work presents a method for estimating human motion using a combination of motion compression and refinement techniques. We begin by processing depth maps from Microsoft Kinect data to estimate joint locations and their corresponding body orientation parameters. Next, we apply our novel motion compression algorithm which reduces the high dimensionality of these pose estimates into a compact representation that can still capture essential aspects of movement quality. Finally, we use a nonrigid pose optimization approach based on an energy function composed of several terms designed to minimize error due to tracking drift and sensor noise. Our experiments show improved accuracy over other state-of-the-art methods in both quantitative and qualitative evaluations. In this paper, we present a new approach for estimating human motion usingMicrosoftKinectdata.Ourmethodprocessesdepthmapsandestimatesjointlocationsandbod",1
"In this work we consider partially observable environments with sparse rewards. We present a self-supervised representation learning method for image-based observations, which arranges embeddings respecting temporal distance of observations. This representation is empirically robust to stochasticity and suitable for novelty detection from the error of a predictive forward model. We consider episodic and life-long uncertainties to guide the exploration. We propose to estimate the missing information about the environment with the world model, which operates in the learned latent space. As a motivation of the method, we analyse the exploration problem in a tabular Partially Observable Labyrinth. We demonstrate the method on image-based hard exploration environments from the Atari benchmark and report significant improvement with respect to prior work. The source code of the method and all the experiments is available at https://github.com/htdt/lwm.",0
"""In recent years, there has been increasing interest in developing artificial intelligence (AI) systems that can learn from their environment through intrinsic motivation rather than relying solely on external reinforcement. One approach to achieve this is by using latent world models (LWMs), which aim to predict future observations based on past experiences. LWMs have shown promising results in enabling agents to engage in exploratory behavior without explicit guidance. This paper presents a comprehensive study of LWMs for intrinsically motivated exploration. We propose a novel framework for training LWMs that captures temporal relationships among observations, as well as a method for selecting informative actions to maximize the acquisition of new knowledge. Our experiments demonstrate the effectiveness of our approach in several domains, including grid navigation and continuous control tasks. Furthermore, we show that the learned behaviors correspond to meaningful concepts in these environments, highlighting the potential applications of LWMs beyond pure exploration.""",1
"Contrastive learning has been adopted as a core method for unsupervised visual representation learning. Without human annotation, the common practice is to perform an instance discrimination task: Given a query image crop, this task labels crops from the same image as positives, and crops from other randomly sampled images as negatives. An important limitation of this label assignment strategy is that it can not reflect the heterogeneous similarity between the query crop and each crop from other images, taking them as equally negative, while some of them may even belong to the same semantic class as the query. To address this issue, inspired by consistency regularization in semi-supervised learning on unlabeled data, we propose Consistent Contrast (CO2), which introduces a consistency regularization term into the current contrastive learning framework. Regarding the similarity of the query crop to each crop from other images as ""unlabeled"", the consistency term takes the corresponding similarity of a positive crop as a pseudo label, and encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC. This shows that CO2 learns better visual representations for these downstream tasks.",0
"New Study Shows Promise for Improved Unsupervised Learning Techniques Using Carbon Dioxide  Researchers have recently developed a groundbreaking new method using carbon dioxide (CO2) for unsupervised visual representation learning. This innovative technique has been shown to provide consistent contrasts that greatly improve upon traditional methods currently used in the field. By utilizing CO2, researchers were able to achieve significant improvements over existing techniques, resulting in more accurate representations and greater overall performance. Further testing and evaluation of this approach shows great promise for continued advancements in unsupervised learning and artificial intelligence. These findings hold significant implications for a wide range of applications including computer vision, image processing, and natural language understanding. Overall, the use of CO2 as a tool for unsupervised visual representation learning represents a major step forward in the field of artificial intelligence and will undoubtedly lead to further developments in the years to come.",1
"Multivariate time series (MTS) data are becoming increasingly ubiquitous in diverse domains, e.g., IoT systems, health informatics, and 5G networks. To obtain an effective representation of MTS data, it is not only essential to consider unpredictable dynamics and highly variable lengths of these data but also important to address the irregularities in the sampling rates of MTS. Existing parametric approaches rely on manual hyperparameter tuning and may cost a huge amount of labor effort. Therefore, it is desirable to learn the representation automatically and efficiently. To this end, we propose an autonomous representation learning approach for multivariate time series (TimeAutoML) with irregular sampling rates and variable lengths. As opposed to previous works, we first present a representation learning pipeline in which the configuration and hyperparameter optimization are fully automatic and can be tailored for various tasks, e.g., anomaly detection, clustering, etc. Next, a negative sample generation approach and an auxiliary classification task are developed and integrated within TimeAutoML to enhance its representation capability. Extensive empirical studies on real-world datasets demonstrate that the proposed TimeAutoML outperforms competing approaches on various tasks by a large margin. In fact, it achieves the best anomaly detection performance among all comparison algorithms on 78 out of all 85 UCR datasets, acquiring up to 20% performance improvement in terms of AUC score.",0
"This paper presents TimeAutoML, a novel framework for autonomously learning representations for multivariate irregularly sampled time series data. We address several challenges faced by current state-of-the-art representation learning methods, including the need for extensive manual engineering, limited ability to adapt to new domains, and suboptimal performance on high-dimensional datasets. Our proposed approach leverages recent advances in neural architecture search (NAS) to automatically identify efficient models tailored to specific tasks and dataset characteristics. We demonstrate significant improvements over benchmark methods across multiple domains, including EEG signals, speech analysis, and financial forecasting. Finally, we provide insights into how TimeAutoML can benefit practitioners by reducing their reliance on manual design expertise and streamlining model development processes. By unlocking the full potential of automated representation learning, our work sets the stage for future advancements in applied machine learning research.",1
"It has been demonstrated that hidden representation learned by a deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve the privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide a formal privacy guarantee. Further, we show that masking words via dropout can further enhance privacy. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on benchmark datasets under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.",0
"Abstract This paper presents new differential privacy mechanisms designed specifically for natural language processing tasks such as sentiment analysis or named entity recognition. Our methods address important deficiencies in current approaches that rely heavily on masking or deletion of tokens from textual inputs. These limitations have been shown to result in significant degradation of model performance which often leads to poor generalization accuracy. We first establish formal guarantees of privacy for our mechanism under standard adversarial settings. Extensive experiments demonstrate the effectiveness of our techniques in preserving utility while maintaining acceptable levels of privacy protection. Moreover we show our method to be robust against different types of attacks including transferability based and query specific attacks demonstrating superior robustness compared to state of art baselines. Finally, we present experimental evaluation using both synthetic data experiments and real user feedback on social media datasets confirming that our methods provide stronger privacy assurances without sacrificing fairness in predictive models. Keywords: Natural Language Processing (NLP), Privacy Protection, Data Utility Preservation, Adversarial Attack Resistance, Social Media Benchmarking",1
"We present MIX'EM, a novel solution for unsupervised image classification. MIX'EM generates representations that by themselves are sufficient to drive a general-purpose clustering algorithm to deliver high-quality classification. This is accomplished by building a mixture of embeddings module into a contrastive visual representation learning framework in order to disentangle representations at the category level. It first generates a set of embedding and mixing coefficients from a given visual representation, and then combines them into a single embedding. We introduce three techniques to successfully train MIX'EM and avoid degenerate solutions; (i) diversify mixture components by maximizing entropy, (ii) minimize instance conditioned component entropy to enforce a clustered embedding space, and (iii) use an associative embedding loss to enforce semantic separability. By applying (i) and (ii), semantic categories emerge through the mixture coefficients, making it possible to apply (iii). Subsequently, we run K-means on the representations to acquire semantic classification. We conduct extensive experiments and analyses on STL10, CIFAR10, and CIFAR100-20 datasets, achieving state-of-the-art classification accuracy of 78\%, 82\%, and 44\%, respectively. To achieve robust and high accuracy, it is essential to use the mixture components to initialize K-means. Finally, we report competitive baselines (70\% on STL10) obtained by applying K-means to the ""normalized"" representations learned using the contrastive loss.",0
"This research presents a novel unsupervised method for image classification called MIX'EM (Mixture Of Embeddings). The algorithm uses a mixture model that combines multiple embeddings from pretrained convolutional neural networks (CNNs) as features for image representation. By leveraging different CNN architectures and training datasets, MIX'EM can capture complementary aspects of images while addressing their individual weaknesses.  The method proceeds by first extracting embeddings from various pretrained models for each input image. These embeddings are then combined into a probability distribution following a Gaussian mixture model assumption. We show how this approach allows us to effectively classify images without explicit supervision.  Experiments on several benchmark datasets demonstrate that our method significantly outperforms strong baselines such as K-means clustering and linear discriminant analysis. Furthermore, we analyze the effectiveness of the mixture components and observe promising results across various settings. Overall, MIX'EM provides a competitive alternative for unsupervised image classification tasks with minimal computational overhead. Our source code has been made publicly available for future comparisons and improvements.",1
"Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical TD algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks.",0
"Abstract: Deep reinforcement learning (RL) algorithms have made significant strides in solving challenging sequential decision tasks, but they struggle with stability issues that make them difficult to deploy in practice. One major source of instability comes from off-policy RL methods such as deep Q-learning (DQL), which require accurate estimates of state values to perform policy improvement updates. In this work, we introduce new representations that enable stable off-policy learning by facilitating the training and evaluation of value functions across states, actions, and policies that differ substantially from those used during training. Our representations are based on a novel combination of a learned intrinsic motivation signal that enhances exploration, a flexible similarity metric that adapts to changing behaviors over time, and a data reuse scheme that minimizes the need for additional interactions. Experimental results demonstrate the effectiveness of our approach across several Atari games and continuous control benchmarks, achieving comparable performance with on-policy methods while requiring significantly fewer samples. These findings open up exciting possibilities for improving the scalability and reliability of off-policy RL applications. Keywords: deep RL, representation learning, off-policy optimization, value function estimation, exploration, action selection, state abstraction.",1
"This work introduces a new unsupervised representation learning technique called Deep Convolutional Transform Learning (DCTL). By stacking convolutional transforms, our approach is able to learn a set of independent kernels at different layers. The features extracted in an unsupervised manner can then be used to perform machine learning tasks, such as classification and clustering. The learning technique relies on a well-sounded alternating proximal minimization scheme with established convergence guarantees. Our experimental results show that the proposed DCTL technique outperforms its shallow version CTL, on several benchmark datasets.",0
"In recent years, deep learning has emerged as one of the most promising techniques for solving complex data analysis problems. At the core of many successful applications of deep learning are convolutional neural networks (CNNs), which have been applied to tasks such as image classification, object detection, and segmentation. However, designing effective CNN architectures remains challenging due to their highly nonlinear nature and parameter sensitivity. This paper presents a novel approach to learn deep convolutional transformations that generalize across multiple scales of data representation, resulting in improved performance on difficult benchmark datasets. Our approach builds upon previous work in transformer models and applies attention mechanisms at different levels of abstraction, allowing our model to efficiently capture local dependencies without resorting to explicit spatial pyramid pooling. We demonstrate the effectiveness of our method through extensive experimental evaluation on several benchmark datasets, including CIFAR-10, CIFAR-100, SVHN, STL-10, and ImageNet. Additionally, we provide detailed comparisons with state-of-the-art methods and ablation studies to highlight the contributions of each component in our proposed architecture. Our findings suggest that learned convolutional transformations can significantly improve the robustness and performance of deep CNNs, paving the way for future advances in computer vision and other domains requiring feature extraction from high-dimensional data.",1
"This paper challenges the common assumption that the weight $\beta$, in $\beta$-VAE, should be larger than $1$ in order to effectively disentangle latent factors. We demonstrate that $\beta$-VAE, with $\beta  1$, can not only attain good disentanglement but also significantly improve reconstruction accuracy via dynamic control. The paper removes the inherent trade-off between reconstruction accuracy and disentanglement for $\beta$-VAE. Existing methods, such as $\beta$-VAE and FactorVAE, assign a large weight to the KL-divergence term in the objective function, leading to high reconstruction errors for the sake of better disentanglement. To mitigate this problem, a ControlVAE has recently been developed that dynamically tunes the KL-divergence weight in an attempt to control the trade-off to more a favorable point. However, ControlVAE fails to eliminate the conflict between the need for a large $\beta$ (for disentanglement) and the need for a small $\beta$. Instead, we propose DynamicVAE that maintains a different $\beta$ at different stages of training, thereby decoupling disentanglement and reconstruction accuracy. In order to evolve the weight, $\beta$, along a trajectory that enables such decoupling, DynamicVAE leverages a modified incremental PI (proportional-integral) controller, and employs a moving average as well as a hybrid annealing method to evolve the value of KL-divergence smoothly in a tightly controlled fashion. We theoretically prove the stability of the proposed approach. Evaluation results on three benchmark datasets demonstrate that DynamicVAE significantly improves the reconstruction accuracy while achieving disentanglement comparable to the best of existing methods. The results verify that our method can separate disentangled representation learning and reconstruction, removing the inherent tension between the two.",0
"Abstract  The paper presents Dynamic VAE (DyVAE), a novel deep learning framework that decouples reconstruction error from disentanglement using dynamic scaling factors. DyVAE learns disentangled representations by maximizing their mutual information with input variables while minimizing reconstruction errors.  Our approach utilizes dynamic scaling factors to scale up the importance of each task loss during training. This ensures that reconstruction errors receive appropriate attention without overwhelming the optimization process. The dynamic scalars adaptively balance the relative weighting of cross-entropy loss for supervised tasks and KL divergence loss for unsupervised representation learning.  We demonstrate the effectiveness of our method on multiple benchmark datasets across different modalities including image generation, transfer learning, and feature learning tasks. Our experiments show consistent improvements over strong baselines such as static scalar VAEs (SVAEs) and standard autoencoders (AEs). In addition, we achieve superior results compared to recently proposed methods like Beta VAE. Finally, our ablation studies further validate the contributions made by dynamic scalars.  This work advances knowledge in disentangled representation learning by offering new insights into balancing reconstruction error and regularization losses. We believe our research opens up promising directions for future research in understanding complex data relationships through learned features. By achieving high-quality results, our DyVAE model could be applied in several real-world applications involving information processing or decision making based on deep learning models with enhanced interpretability.  Keywords: Variational Autoencoder (VAE); Mutual Information; Disentangl",1
"Events in natural videos typically arise from spatio-temporal interactions between actors and objects and involve multiple co-occurring activities and object classes. To capture this rich visual and semantic context, we propose using two graphs: (1) an attributed spatio-temporal visual graph whose nodes correspond to actors and objects and whose edges encode different types of interactions, and (2) a symbolic graph that models semantic relationships. We further propose a graph neural network for refining the representations of actors, objects and their interactions on the resulting hybrid graph. Our model goes beyond current approaches that assume nodes and edges are of the same type, operate on graphs with fixed edge weights and do not use a symbolic graph. In particular, our framework: a) has specialized attention-based message functions for different node and edge types; b) uses visual edge features; c) integrates visual evidence with label relationships; and d) performs global reasoning in the semantic space. Experiments on challenging video understanding tasks, such as temporal action localization on the Charades dataset, show that the proposed method leads to state-of-the-art performance.",0
"Abstract: This paper presents a novel method for video understanding using representation learning on visual-symbolic graphs. In recent years, deep learning techniques have been successfully applied to many computer vision tasks such as object detection and image classification. However, these methods typically focus only on visual representations, ignoring the rich symbolic knowledge available in natural language descriptions or scripts. We propose to use graph convolutional networks (GCN) to jointly learn both visual and symbolic representations, where nodes represent either objects in images or symbols from a script and edges encode their relationships. Our approach enables effective integration of multimodal information to improve video understanding tasks such as activity recognition, event prediction, and action localization. Extensive experiments show that our model significantly outperforms state-of-the-art methods across multiple benchmark datasets.",1
"Automatically finding good and general remote sensing representations allows to perform transfer learning on a wide range of applications - improving the accuracy and reducing the required number of training samples. This paper investigates development of generic remote sensing representations, and explores which characteristics are important for a dataset to be a good source for representation learning. For this analysis, five diverse remote sensing datasets are selected and used for both, disjoint upstream representation learning and downstream model training and evaluation. A common evaluation protocol is used to establish baselines for these datasets that achieve state-of-the-art performance. As the results indicate, especially with a low number of available training samples a significant performance enhancement can be observed when including additionally in-domain data in comparison to training models from scratch or fine-tuning only on ImageNet (up to 11% and 40%, respectively, at 100 training samples). All datasets and pretrained representation models are published online.",0
"This abstract presents a method for training a model capable of generating accurate high resolution satellite imagery by leveraging both remote sensing data and additional geospatial datasets. Our approach utilizes Convolutional Neural Networks (CNN) pretrained on synthetic data from ground truth images and then finetuned on a combination of real world reference images, simulated imagery generated from airborne lidar point clouds, and additional georeferenced features including elevation models, soil type maps, land use labels, meteorological variables, and other relevant contextual information. We evaluate our proposed approach through several experiments which demonstrate that our framework generates more accurate results than those trained solely on high quality synthetic data, and achieve comparable performance to state-of-the-art methods trained on large quantities of expensive real-world data. We believe this research has important implications for improving access to remotely acquired environmental, agricultural, urban planning, and disaster response applications.",1
"Event-based cameras record an asynchronous stream of per-pixel brightness changes. As such, they have numerous advantages over the standard frame-based cameras, including high temporal resolution, high dynamic range, and no motion blur. Due to the asynchronous nature, efficient learning of compact representation for event data is challenging. While it remains not explored the extent to which the spatial and temporal event ""information"" is useful for pattern recognition tasks. In this paper, we focus on single-layer architectures. We analyze the performance of two general problem formulations: the direct and the inverse, for unsupervised feature learning from local event data (local volumes of events described in space-time). We identify and show the main advantages of each approach. Theoretically, we analyze guarantees for an optimal solution, possibility for asynchronous, parallel parameter update, and the computational complexity. We present numerical experiments for object recognition. We evaluate the solution under the direct and the inverse problem and give a comparison with the state-of-the-art methods. Our empirical results highlight the advantages of both approaches for representation learning from event data. We show improvements of up to 9 % in the recognition accuracy compared to the state-of-the-art methods from the same class of methods.",0
"Artificial intelligence (AI) systems rely on features extracted from raw data to learn models that can perform tasks such as classification, prediction, and decision making. However, extracting these features manually requires domain knowledge and laborious engineering efforts. Unsupervised feature learning techniques have emerged as promising solutions by automatically discovering meaningful patterns directly from raw data without labeled examples. Yet, existing approaches mostly focus on generic applications, neglecting the needs of event analysis which calls for tailored methods due to distinct characteristics. This work presents a comparative study between two formulations of unsupervised feature learning for event data. We investigate the direct problem setup that learns representations explicitly optimized for a target task versus the inverse problem formulation that reconstructs input signals under constraints. Our extensive experiments evaluate both formulations against baselines using six diverse datasets across multiple scenarios (e.g., sales forecasting, anomaly detection). Results showcase the strengths of each approach depending on the task requirements, encouraging practitioners to consider the appropriate choice according to their specific application context. Overall, our findings contribute new insights into unsupervised feature learning for event data while providing practical guidelines for selecting the suitable method based on their use cases.",1
"We propose a novel framework, called Markov-Lipschitz deep learning (MLDL), to tackle geometric deterioration caused by collapse, twisting, or crossing in vector-based neural network transformations for manifold-based representation learning and manifold data generation. A prior constraint, called locally isometric smoothness (LIS), is imposed across-layers and encoded into a Markov random field (MRF)-Gibbs distribution. This leads to the best possible solutions for local geometry preservation and robustness as measured by locally geometric distortion and locally bi-Lipschitz continuity. Consequently, the layer-wise vector transformations are enhanced into well-behaved, LIS-constrained metric homeomorphisms. Extensive experiments, comparisons, and ablation study demonstrate significant advantages of MLDL for manifold learning and manifold data generation. MLDL is general enough to enhance any vector transformation-based networks. The code is available at https://github.com/westlake-cairi/Markov-Lipschitz-Deep-Learning.",0
"Abstract: In recent years, deep learning has emerged as one of the most powerful tools for solving complex problems across a variety of domains, including computer vision, natural language processing, speech recognition, and robotics. However, understanding how these models work remains a major challenge due to their large scale, nonlinear nature, and highly optimized architectures. One approach to addressing this challenge is through analyzing the smoothness properties of loss functions used in training, which have recently been shown to provide valuable insights into optimization dynamics and generalization performance. In this paper, we propose a new technique called Markov-Lipschitz deep learning that leverages Lipschitz continuity assumptions on gradients and Markov chain theory to prove tight bounds on loss smoothness and other key metrics related to model robustness and expressiveness. We demonstrate the effectiveness of our method using several case studies, showing improved accuracy, stability, and interpretability compared to existing methods. Overall, our work represents a significant contribution to the field of deep learning by providing novel theoretical foundations and practical algorithms for designing more effective models and accelerating research progress.",1
"The current research focus on Content-Based Video Retrieval requires higher-level video representation describing the long-range semantic dependencies of relevant incidents, events, etc. However, existing methods commonly process the frames of a video as individual images or short clips, making the modeling of long-range semantic dependencies difficult. In this paper, we propose TCA (Temporal Context Aggregation for Video Retrieval), a video representation learning framework that incorporates long-range temporal information between frame-level features using the self-attention mechanism. To train it on video retrieval datasets, we propose a supervised contrastive learning method that performs automatic hard negative mining and utilizes the memory bank mechanism to increase the capacity of negative samples. Extensive experiments are conducted on multiple video retrieval tasks, such as CC_WEB_VIDEO, FIVR-200K, and EVVE. The proposed method shows a significant performance advantage (~17% mAP on FIVR-200K) over state-of-the-art methods with video-level features, and deliver competitive results with 22x faster inference time comparing with frame-level features.",0
"Video retrieval tasks involve finding relevant videos given natural language queries such as keywords and descriptions, which rely on understanding visual contents of the video. One key challenge lies in incorporating temporal context to capture the evolution and interactions among objects across time. In this work, we introduce a novel framework called Temporal Context Aggregation (TCA) to integrate both spatial and temporal features effectively while alleviate redundant computation caused by aggregating over the entire sequence. TCA comprises two stages: Firstly, we use temporal convolutional networks (TCNs) to learn frame-level representations that encode spatio-temporal patterns from clip-level annotations. Then, we apply Self Attention Mechanism (SAM) to model interactions between local regions through their learned multi-head attention weights without considering temporal order. Finally, the proposed method constructs query-specific classifiers via contrastive learning to distinguish positive clips from negative ones generated dynamically at runtime. We extensively evaluate our approach on three popular benchmark datasets: ActivityNet Captioning, Charades and MPII-MD dataset, where our method achieves state-of-the-art performance. Moreover, ablation studies demonstrate the effectiveness of each component in TCA and further analysis reveals some interesting insights into how different components contribute to the improvement in accuracy. Our contributions lie in developing an effective framework leveraging temporal context for video retrieval that yields substantial improvements over existing methods. By establishing a solid foundation based on high quality annotations, future research can focus more on unsupervised pre-training techniques and mining large scale web video corpus rather than relying exclus",1
"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.",0
"This is an example of how the abstract could look like:  An Effective Method for Image Captioning and Visual QA ImageNet-trained convolutional neural networks (CNNs) have recently achieved impressive results on image classification tasks but there are still challenges that need to be addressed to make them effective at other important vision tasks such as image description/caption generation and visual question answering (VQA). In this work we present a new model called Show & Tell which combines two ideas to achieve state of the art performance on both task types. First, rather than predicting just one class label associated with each image, our method produces a set of multiple labels, where each label corresponds to different aspect(s) or attribute(s) of the object in the picture. Secondly, in addition to producing textual output, our system can generate bounding boxes overlaid onto the image indicating what part of the picture should be attended to most closely match the generated output. We show in experiments comparing against previous methods that explicitly providing these additional pieces of information makes a big difference in terms of getting correct answers to questions posed by VQA models. Furthermore, images paired with the corresponding natural language descriptions produced by our system transfer well across all variants of zero-shot image classification tasks even when fine-tuning was done exclusively on only the unpaired dataset split. These results validate the effectiveness of using object detectors and attending selectively regions of the input images instead of solely relying on global max pooling strategies in CNN architectures. We believe our approach provides insights into how powerful feature extraction systems like ImageNet-trained CNNs might be complemented in future research directions along the lines suggested here.  ---",1
"We propose a novel tree-based ensemble method named Selective Cascade of Residual ExtraTrees (SCORE). SCORE draws inspiration from representation learning, incorporates regularized regression with variable selection features, and utilizes boosting to improve prediction and reduce generalization errors. We also develop a variable importance measure to increase the explainability of SCORE. Our computer experiments show that SCORE provides comparable or superior performance in prediction against ExtraTrees, random forest, gradient boosting machine, and neural networks; and the proposed variable importance measure for SCORE is comparable to studied benchmark methods. Finally, the predictive performance of SCORE remains stable across hyper-parameter values, suggesting potential robustness to hyperparameter specification.",0
"This paper presents a new machine learning model called ""Selective Cascade of Residual ExtraTrees"" (SCoRE), which achieves state-of-the-art performance on several benchmark datasets by combining the strengths of ensemble methods, cascading models, and residual networks into one framework. SCoRE works by first training multiple base decision trees independently using random subsets of features, then combining them through an early-exit voting scheme where each subsequent tree votes for either the previous prediction or its own classification. Finally, SCoRE integrates the predictions from all base trees into a final output using a weighted average or a more complex combination function based on attention mechanisms. Experiments show that our method significantly outperforms other strong baselines across various tasks, including image classification, object detection, and segmentation. Furthermore, ablation studies demonstrate the effectiveness of different components in SCoRE and provide insights into how these components interact to improve overall performance. Overall, our work represents a significant contribution to the field of computer vision and highlights the importance of developing powerful yet efficient deep learning algorithms that can operate under strict resource constraints.",1
"The recently occurred representation learning make an attractive performance in NLP and complex network, it is becoming a fundamental technology in machine learning and data mining. How to use representation learning to improve the performance of classifiers is a very significance research direction. We using representation learning technology to map raw data(node of graph) to a low-dimensional feature space. In this space, each raw data obtained a lower dimensional vector representation, we do some simple linear operations for those vectors to produce some virtual data, using those vectors and virtual data to training multi-tag classifier. After that we measured the performance of classifier by F1 score(Macro% F1 and Micro% F1). Our method make Macro F1 rise from 28 % - 450% and make average F1 score rise from 12 % - 224%. By contrast, we trained the classifier directly with the lower dimensional vector, and measured the performance of classifiers. We validate our algorithm on three public data sets, we found that the virtual data helped the classifier greatly improve the F1 score. Therefore, our algorithm is a effective way to improve the performance of classifier. These result suggest that the virtual data generated by simple linear operation, in representation space, still retains the information of the raw data. It's also have great significance to the learning of small sample data sets.",0
"In this work we present EEMC (Embedding Enhanced Multi-tag Classifier), which is based on deep neural networks. EEMC takes advantage of multi-task learning by training multiple instance classifiers simultaneously, each one taking into account several relevant tags such as image content, color distribution, edges detection, corner points and more. These tags provide additional contextual and semantic information that helps the algorithm better discriminate among objects of different categories. Our method is tested over three standard benchmark datasets, including MNIST, CIFAR10 and SVHN. For all these tasks our model achieves state-of-the-art accuracy while running significantly faster than similar methods from the literature. Furthermore we compare the results of using only raw pixels versus adding some prior knowledge via informative tags. This comparison shows clear improvement both qualitatively through visualizations and quantitatively through enhanced classification performance. Finally we discuss potential future research directions related to tag selection and optimal architecture design. Overall, this paper presents a novel approach that combines traditional computer vision features along with recent advancements made possible due to large scale convolutional neural network architectures. We hope that our findings can serve as stepping stone towards creating models capable of achieving even higher accuracies on complex real world problems.",1
"Source code representations are key in applying machine learning techniques for processing and analyzing programs. A popular approach in representing source code is neural source code embeddings that represents programs with high-dimensional vectors computed by training deep neural networks on a large volume of programs. Although successful, there is little known about the contents of these vectors and their characteristics. In this paper, we present our preliminary results towards better understanding the contents of code2vec neural source code embeddings. In particular, in a small case study, we use the code2vec embeddings to create binary SVM classifiers and compare their performance with the handcrafted features. Our results suggest that the handcrafted features can perform very close to the highly-dimensional code2vec embeddings, and the information gains are more evenly distributed in the code2vec embeddings compared to the handcrafted features. We also find that the code2vec embeddings are more resilient to the removal of dimensions with low information gains than the handcrafted features. We hope our results serve a stepping stone toward principled analysis and evaluation of these code representations.",0
"Abstract: This study aims to investigate dimensions of source code embeddings by applying different dimensionality reduction techniques on software repositories. By doing so, we aim to gain insights into how developers write code and which patterns they follow. Our results show that there exist specific communities within the developer ecosystems where similar coding practices are followed. Additionally, our work highlights some common features across programming languages such as naming conventions, functions, loops, conditionals, comments etc., which are crucial while writing maintainable code. Overall, our findings can aid practitioners and researchers in developing better tools and guidelines for software development.",1
"Recently, there is an increasing demand for automatically detecting anatomical landmarks which provide rich structural information to facilitate subsequent medical image analysis. Current methods related to this task often leverage the power of deep neural networks, while a major challenge in fine tuning such models in medical applications arises from insufficient number of labeled samples. To address this, we propose to regularize the knowledge transfer across source and target tasks through cross-task representation learning. The proposed method is demonstrated for extracting facial anatomical landmarks which facilitate the diagnosis of fetal alcohol syndrome. The source and target tasks in this work are face recognition and landmark detection, respectively. The main idea of the proposed method is to retain the feature representations of the source model on the target task data, and to leverage them as an additional source of supervisory signals for regularizing the target model learning, thereby improving its performance under limited training samples. Concretely, we present two approaches for the proposed representation learning by constraining either final or intermediate model features on the target model. Experimental results on a clinical face image dataset demonstrate that the proposed approach works well with few labeled data, and outperforms other compared approaches.",0
"Accurately detecting anatomical landmarks is crucial for many medical image analysis tasks such as automatic diagnosis, surgery planning, and patient monitoring. However, annotating these landmarks can be extremely time-consuming due to their high variability across different patients, imaging modalities, and disease states. As a result, there has been increased interest in developing models that can automatically detect anatomical landmarks from images using machine learning algorithms. To address this challenge, we propose cross-task representation learning (XRL), which enables efficient transfer of knowledge gained by one task to another related but distinct task, even if limited annotations are available. Specifically, XRL leverages pre-trained models on auxiliary tasks to learn discriminative representations shared among multiple tasks. We demonstrate the effectiveness of our method on four challenging anatomical landmark detection benchmarks: PennFUSION, Head/Neck CT, VGG FACE Sysème, and DeepMedic Pelvis, achieving state-of-the-art performance on all datasets with fewer annotations compared to prior methods. Our results showcase the potential of cross-task learning approaches for improving accuracy while reducing annotation effort required for training accurate medical image understanding systems.",1
"We consider the problem of representation learning for temporal interaction graphs where a network of entities with complex interactions over an extended period of time is modeled as a graph with a rich set of node and edge attributes. In particular, an edge between a node-pair within the graph corresponds to a multi-dimensional time-series. To fully capture and model the dynamics of the network, we propose GTEA, a framework of representation learning for temporal interaction graphs with per-edge time-based aggregation. Under GTEA, a Graph Neural Network (GNN) is integrated with a state-of-the-art sequence model, such as LSTM, Transformer and their time-aware variants. The sequence model generates edge embeddings to encode temporal interaction patterns between each pair of nodes, while the GNN-based backbone learns the topological dependencies and relationships among different nodes. GTEA also incorporates a sparsity-inducing self-attention mechanism to distinguish and focus on the more important neighbors of each node during the aggregation process. By capturing temporal interactive dynamics together with multi-dimensional node and edge attributes in a network, GTEA can learn fine-grained representations for a temporal interaction graph to enable or facilitate other downstream data analytic tasks. Experimental results show that GTEA outperforms state-of-the-art schemes including GraphSAGE, APPNP, and TGAT by delivering higher accuracy (100.00%, 98.51%, 98.05% ,79.90%) and macro-F1 score (100.00%, 98.51%, 96.68% ,79.90%) over four large-scale real-world datasets for binary/ multi-class node classification.",0
"This could include some motivations around why you might want to study graphs like these (e.g., they model interactions between objects over time; or networks often have temporal as well as graph structure). Might mention something that makes your work novel/innovative here as well. Could even say what kind of tasks are done using models built on these representations. | What would you like me to write?",1
"Current deep domain adaptation methods used in computer vision have mainly focused on learning discriminative and domain-invariant features across different domains. In this paper, we present a novel ""deep adversarial transition learning"" (DATL) framework that bridges the domain gap by projecting the source and target domains into intermediate, transitional spaces through the employment of adjustable, cross-grafted generative network stacks and effective adversarial learning between transitions. Specifically, we construct variational auto-encoders (VAE) for the two domains, and form bidirectional transitions by cross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial networks (GAN) are employed for domain adaptation, mapping the target domain data to the known label space of the source domain. The overall adaptation process hence consists of three phases: feature representation learning by VAEs, transitions generation, and transitions alignment by GANs. Experimental results demonstrate that our method outperforms the state-of-the art on a number of unsupervised domain adaptation benchmarks.",0
"This research presents a novel approach to adversarial transition learning that utilizes cross-grafted generative stacks. By leveraging recent advances in deep learning, we propose a methodology that allows us to effectively address challenges related to modeling complex interactions among multiple modalities. Our framework achieves state-of-the-art performance by balancing reconstruction losses and adversarial objectives while maintaining computational efficiency. In addition, our experimental results demonstrate superiority over existing methods on several benchmark datasets. These findings have important implications for the field, as they suggest new opportunities for developing more robust models capable of handling real-world data. Overall, our work represents an significant contribution to the broader domain of artificial intelligence.",1
"In the realms of computer vision, it is evident that deep neural networks perform better in a supervised setting with a large amount of labeled data. The representations learned with supervision are not only of high quality but also helps the model in enhancing its accuracy. However, the collection and annotation of a large dataset are costly and time-consuming. To avoid the same, there has been a lot of research going on in the field of unsupervised visual representation learning especially in a self-supervised setting. Amongst the recent advancements in self-supervised methods for visual recognition, in SimCLR Chen et al. shows that good quality representations can indeed be learned without explicit supervision. In SimCLR, the authors maximize the similarity of augmentations of the same image and minimize the similarity of augmentations of different images. A linear classifier trained with the representations learned using this approach yields 76.5% top-1 accuracy on the ImageNet ILSVRC-2012 dataset. In this work, we propose that, with the normalized temperature-scaled cross-entropy (NT-Xent) loss function (as used in SimCLR), it is beneficial to not have images of the same category in the same batch. In an unsupervised setting, the information of images pertaining to the same category is missing. We use the latent space representation of a denoising autoencoder trained on the unlabeled dataset and cluster them with k-means to obtain pseudo labels. With this apriori information we batch images, where no two images from the same category are to be found. We report comparable performance enhancements on the CIFAR10 dataset and a subset of the ImageNet dataset. We refer to our method as G-SimCLR.",0
"Title: Improving self-supervised learning through guided projection and pseudo labeling  Self-supervised learning (SSL) has emerged as a powerful approach to train deep neural networks without relying on large amounts of labeled data. One popular method in SSL is contrastive learning, which learns representations by maximizing agreement between augmented views of the same input and minimizing agreement between different inputs. However, previous methods have faced challenges such as poor convergence rates and limited performance gains over simpler baselines.  This work presents G-SimCLR, a novel self-supervised algorithm that addresses these issues using two key components: guided projection and pseudo labeling. Guided projection applies a learned linear transformation to improve the quality of negative pairs generated during training, resulting in more effective pretext tasks. This improves both the efficiency and stability of training, allowing for better usage of computational resources. Additionally, pseudo labeling generates labels for unlabeled data based on confidence scores produced by the network, allowing for better utilization of available information and improving overall model performance.  Experiments across several benchmark datasets demonstrate significant improvements in accuracy compared to existing state-of-the-art SSL algorithms. These results highlight the effectiveness of our proposed approach in enhancing self-supervised learning through careful design choices. Overall, our contributions provide new insights into how we can push the boundaries of deep learning under real-world constraints where massive annotated datasets may not always be readily available.",1
"Representation learning has been proven to play an important role in the unprecedented success of machine learning models in numerous tasks, such as machine translation, face recognition and recommendation. The majority of existing representation learning approaches often require a large number of consistent and noise-free labels. However, due to various reasons such as budget constraints and privacy concerns, labels are very limited in many real-world scenarios. Directly applying standard representation learning approaches on small labeled data sets will easily run into over-fitting problems and lead to sub-optimal solutions. Even worse, in some domains such as education, the limited labels are usually annotated by multiple workers with diverse expertise, which yields noises and inconsistency in such crowdsourcing settings. In this paper, we propose a novel framework which aims to learn effective representations from limited data with crowdsourced labels. Specifically, we design a grouping based deep neural network to learn embeddings from a limited number of training samples and present a Bayesian confidence estimator to capture the inconsistency among crowdsourced labels. Furthermore, to expedite the training process, we develop a hard example selection procedure to adaptively pick up training examples that are misclassified by the model. Extensive experiments conducted on three real-world data sets demonstrate the superiority of our framework on learning representations from limited data with crowdsourced labels, comparing with various state-of-the-art baselines. In addition, we provide a comprehensive analysis on each of the main components of our proposed framework and also introduce the promising results it achieved in our real production to fully understand the proposed framework.",0
"Abstract: As machine learning models continue to proliferate in our daily lives, there has been growing concern over their potential impact on society. One key area of interest is how these systems can perpetuate harmful biases by amplifying existing data trends. In light of recent events at major tech companies such as Apple and Google, it seems prudent to investigate ways in which we might mitigate these risks without compromising accuracy. Here I propose one possible solution using limited educational datasets supplemented with crowdsource labels. By leveraging collective intelligence, we may reduce bias while increasing labeling efficiency and accessibility for lesser-researched domains. This work presents initial results comparing crowdsourcing strategies across popular models under different evaluation metrics before discussing future directions towards more inclusive AI development.",1
"Representation learning is a fundamental building block for analyzing entities in a database. While the existing embedding learning methods are effective in various data mining problems, their applicability is often limited because these methods have pre-determined assumptions on the type of semantics captured by the learned embeddings, and the assumptions may not well align with specific downstream tasks. In this work, we propose an embedding learning framework that 1) uses an input format that is agnostic to input data type, 2) is flexible in terms of the relationships that can be embedded into the learned representations, and 3) provides an intuitive pathway to incorporate domain knowledge into the embedding learning process. Our proposed framework utilizes a set of entity-relation-matrices as the input, which quantifies the affinities among different entities in the database. Moreover, a sampling mechanism is carefully designed to establish a direct connection between the input and the information captured by the output embeddings. To complete the representation learning toolbox, we also outline a simple yet effective post-processing technique to properly visualize the learned embeddings. Our empirical results demonstrate that the proposed framework, in conjunction with a set of relevant entity-relation-matrices, outperforms the existing state-of-the-art approaches in various data mining tasks.",0
"This paper presents a novel embedding learning framework that provides increased flexibility over traditional methods. The proposed approach uses a modular architecture consisting of multiple layers of embeddings which can be fine-tuned based on specific tasks. By leveraging recent advancements in neural network pruning, our method allows for efficient feature selection while ensuring minimal loss of performance. Experiments performed on several benchmark datasets demonstrate significant improvements in accuracy compared to state-of-the-art models. Our framework achieves these results without increasing model complexity or computational cost. Overall, our work paves the way for more flexible and effective usage of embedded representations in various NLP applications.",1
This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.,0
"In this paper, we propose a method for option discovery in hierarchical reinforcement learning (HRL) through the use of spatio-temporal clustering. Options are discrete behaviors that can be composed into more complex tasks, allowing agents to efficiently learn and execute sequences of actions without needing explicit memorization of each step. However, discovering options from raw sensorimotor inputs remains a challenge, especially in HRL where multiple levels of hierarchy may exist.  To address this, our approach identifies distinct submanifolds within state spaces by grouping similar states based on their temporal coherence. By modeling these submanifolds as clusters, our algorithm promotes spatial diversity while considering the agent's temporal evolution, capturing meaningful differences in transitions between states. This allows us to extract a set of diverse yet informative options that encode different aspects of the task at hand.  Our experiments evaluate the effectiveness of our method across several continuous control domains and demonstrate improvements over prior techniques in both the quality and quantity of discovered options. We additionally provide qualitative analysis on learned options, highlighting how they directly capture interpretable aspects of the underlying environment.  Overall, our work bridges the gap between local exploration in sparse reward settings and global understanding of a high-dimensional state space, paving the way towards better understanding and automating hierarchical reasoning for artificial agents.",1
"Axis-aligned decision forests have long been the leading class of machine learning algorithms for modeling tabular data. In many applications of machine learning such as learning-to-rank, decision forests deliver remarkable performance. They also possess other coveted characteristics such as interpretability. Despite their widespread use and rich history, decision forests to date fail to consume raw structured data such as text, or learn effective representations for them, a factor behind the success of deep neural networks in recent years. While there exist methods that construct smoothed decision forests to achieve representation learning, the resulting models are decision forests in name only: They are no longer axis-aligned, use stochastic decisions, or are not interpretable. Furthermore, none of the existing methods are appropriate for problems that require a Transfer Learning treatment. In this work, we present a novel but intuitive proposal to achieve representation learning for decision forests without imposing new restrictions or necessitating structural changes. Our model is simply a decision forest, possibly trained using any forest learning algorithm, atop a deep neural network. By approximating the gradients of the decision forest through input perturbation, a purely analytical procedure, the decision forest directs the neural network to learn or fine-tune representations. Our framework has the advantage that it is applicable to any arbitrary decision forest and that it allows the use of arbitrary deep neural networks for representation learning. We demonstrate the feasibility and effectiveness of our proposal through experiments on synthetic and benchmark classification datasets.",0
"In recent years, decision forests have emerged as powerful tools for solving complex machine learning problems due to their ability to model nonlinear interactions and handle high-dimensional data efficiently. However, the performance of decision forest models can often depend on the quality of the features used for training. One approach that has been shown to improve feature representations in decision trees and random forests is input perturbation, where small changes are made to the input data during tree construction. By iteratively applying these changes to a base set of features, more informative feature subsets can be generated and combined into ensemble methods such as axis-aligned decision forests (AADF). This study presents a methodology for learning representation of AADFs using input perturbation by utilizing a set of existing heuristics designed specifically to improve feature selection in decision forests. We evaluate our method on several benchmark datasets across multiple domains, demonstrating significant improvements in accuracy compared to standard ADF approaches without input preprocessing techniques. Our findings suggest that learned feature representations can lead to better decision forests, highlighting the importance of effective feature engineering strategies in machine learning applications.",1
"Recently, the interest of graph representation learning has been rapidly increasing in recommender systems. However, most existing studies have focused on improving accuracy, but in real-world systems, the recommendation diversity should be considered as well to improve user experiences. In this paper, we propose the diversity-emphasized node embedding div2vec, which is a random walk-based unsupervised learning method like DeepWalk and node2vec. When generating random walks, DeepWalk and node2vec sample nodes of higher degree more and nodes of lower degree less. On the other hand, div2vec samples nodes with the probability inversely proportional to its degree so that every node can evenly belong to the collection of random walks. This strategy improves the diversity of recommendation models. Offline experiments on the MovieLens dataset showed that our new method improves the recommendation performance in terms of both accuracy and diversity. Moreover, we evaluated the proposed model on two real-world services, WATCHA and LINE Wallet Coupon, and observed the div2vec improves the recommendation quality by diversifying the system.",0
"Div2Vec is a novel approach to node embedding that emphasizes diversity across different semantic groups within the data set. By incorporating linguistic diversity into vector representations of documents, we can capture more nuanced and accurate relationships between them. We propose two methods for promoting diversity within embeddings - hard contrastive learning (HCL), which encourages nodes from distinct groups to form clusters while pushing apart duplicates; and soft contrastive learning (SCL), which minimizes intra-cluster similarity while maximizing inter-cluster differences. Experimental results on several benchmark datasets demonstrate that both HCL and SCL significantly improve over baseline models across various downstream tasks such as sentiment analysis and question answering, achieving new state-of-the-art performance in some cases. These findings highlight the importance of considering diversity during document representation learning for NLP applications, and suggest that our proposed techniques may provide a powerful tool towards improving their accuracy and robustness.",1
"We present a novel generalized zero-shot algorithm to recognize perceived emotions from gestures. Our task is to map gestures to novel emotion categories not encountered in training. We introduce an adversarial, autoencoder-based representation learning that correlates 3D motion-captured gesture sequence with the vectorized representation of the natural-language perceived emotion terms using word2vec embeddings. The language-semantic embedding provides a representation of the emotion label space, and we leverage this underlying distribution to map the gesture-sequences to the appropriate categorical emotion labels. We train our method using a combination of gestures annotated with known emotion terms and gestures not annotated with any emotions. We evaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and obtain an accuracy of $58.43\%$. This improves the performance of current state-of-the-art algorithms for generalized zero-shot learning by $25$--$27\%$ on the absolute.",0
"This paper presents a novel framework that can recognize emotions based on gestures observed through video footage without having seen any examples of those emotions during training. By leveraging the power of deep learning algorithms, we can condition perceptual decisions on semantic labels while using adversarial autoencoders to ensure robustness and generalization to unseen data distributions. We demonstrate our approach by evaluating it on two different datasets involving multiple human subjects performing spontaneous actions and comparing against state-of-the-art methods. Our results show significant improvements over existing techniques and highlight the importance of combining rich semantic representations with discriminative perception models. Overall, this work has important implications for automated emotion recognition systems, particularly in settings where large labeled datasets may be scarce or difficult to obtain.  This paper proposes a new method for recognizing emotions expressed through gestures, even if they haven’t been explicitly shown during training. Using advanced machine learning techniques such as zero-shot learning and semantically-conditioned perception, the authors achieved significantly higher accuracy than traditional approaches. Two separate experiments were conducted, each using diverse groups of participants behaving naturally and freely expressing their emotions. Compared to other current technologies, these outcomes validate the effectiveness of integrating rich meaning (semantics) into the process of identifying and comprehending emotional expressions. With potential applications ranging from mental health monitoring to customer engagement analysis, this research offers valuable insights into enhancing automated emotion detection overall.",1
"Video representation learning has recently attracted attention in computer vision due to its applications for activity and scene forecasting or vision-based planning and control. Video prediction models often learn a latent representation of video which is encoded from input frames and decoded back into images. Even when conditioned on actions, purely deep learning based architectures typically lack a physically interpretable latent space. In this study, we use a differentiable physics engine within an action-conditional video representation network to learn a physical latent representation. We propose supervised and self-supervised learning methods to train our network and identify physical properties. The latter uses spatial transformers to decode physical states back into images. The simulation scenarios in our experiments comprise pushing, sliding and colliding objects, for which we also analyze the observability of the physical properties. In experiments we demonstrate that our network can learn to encode images and identify physical properties like mass and friction from videos and action sequences in the simulated scenarios. We evaluate the accuracy of our supervised and self-supervised methods and compare it with a system identification baseline which directly learns from state trajectories. We also demonstrate the ability of our method to predict future video frames from input images and actions.",0
"This paper presents a new method called ""differentiable physics"" which can learn physical parameters from video by using real-time simulation based on first principles of mechanics. Our approach takes advantage of recent advances in differentiable rendering and deep learning to train generative models that predict physical quantities such as masses, damping coefficients, and friction values directly from raw video inputs. We evaluate our system across a range of challenging scenarios, demonstrating state-of-the-art performance in terms of accuracy and robustness against noise and sensor errors. Additionally, we showcase two exciting applications that highlight the potential impact of our technology: real-time robot control and augmented reality experiences. Overall, this work represents a significant step towards enabling machines to reason physically, which could open up many opportunities in fields ranging from manufacturing to entertainment.",1
"The heterogeneous network is a robust data abstraction that can model entities of different types interacting in various ways. Such heterogeneity brings rich semantic information but presents nontrivial challenges in aggregating the heterogeneous relationships between objects - especially those of higher-order indirect relations. Recent graph neural network approaches for representation learning on heterogeneous networks typically employ the attention mechanism, which is often only optimized for predictions based on direct links. Furthermore, even though most deep learning methods can aggregate higher-order information by building deeper models, such a scheme can diminish the degree of interpretability. To overcome these challenges, we explore an architecture - Layer-stacked ATTention Embedding (LATTE) - that automatically decomposes higher-order meta relations at each layer to extract the relevant heterogeneous neighborhood structures for each node. Additionally, by successively stacking layer representations, the learned node embedding offers a more interpretable aggregation scheme for nodes of different types at different neighborhood ranges. We conducted experiments on several benchmark heterogeneous network datasets. In both transductive and inductive node classification tasks, LATTE can achieve state-of-the-art performance compared to existing approaches, all while offering a lightweight model. With extensive experimental analyses and visualizations, the framework can demonstrate the ability to extract informative insights on heterogeneous networks.",0
"Title: ""Embedded Learning in Graph Neural Networks""  Abstract: This paper presents a novel approach for embedding learning in graph neural networks (GNN) through layer-stacking attention mechanisms that integrate heterogenous layers. GNNs have become increasingly popular due to their ability to capture complex relationships within graph data structures such as social networks and molecular graphs. However, current GNN architectures suffer from limitations regarding scalability and capacity, particularly with respect to deep network embeddings. In our proposed method, we leverage multiple attention heads in each layer that adaptively focus on specific subgraph patterns, allowing for improved scalability and representational power. Our contributions can be summarized as follows: We introduce a new architecture called Layer-Stacked Attention (LSA), which integrates both homogeneous and heterogeneous layers using attention gates and dynamically adjusts importance based on local node features. Second, we demonstrate experimentally that LSA significantly outperforms state-of-the-art approaches across diverse tasks ranging from link prediction to node classification. Lastly, we provide ablation studies and analyses of model behaviors that highlight key aspects of the LSA mechanism, including visualizations that clarify how attention weights evolve during message passing. Collectively, these results substantiate the effectiveness and versatility of our framework in addressing challenging problems involving graph embeddings.",1
"We propose a webly-supervised representation learning method that does not suffer from the annotation unscalability of supervised learning, nor the computation unscalability of self-supervised learning. Most existing works on webly-supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art performance on WebVision, a weakly-labeled noisy dataset. MoPro also shows superior performance when the pretrained model is transferred to down-stream image classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.3 when finetuned on 1\% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shifts. Code and pretrained models are available at https://github.com/salesforce/MoPro.",0
"This paper presents MoPro, a new method for webly supervised learning that utilizes momentum prototypes to effectively leverage weak labels found on the internet. Weak supervision has emerged as a promising approach to overcome the limited availability of strong annotations, but current methods suffer from several limitations. In particular, they often require manual feature engineering and may fail to exploit complex dependencies among different modalities. To address these issues, we introduce a simple yet effective model that learns multi-modal representations by matching patterns encoded into momentum prototypes using attention mechanisms. Our experiments demonstrate significant improvements over state-of-the-art methods across a range of tasks including image classification, video recognition, and natural language processing. Moreover, our analysis shows that the learned prototypes indeed capture meaningful patterns in data that can facilitate learning under weak supervision. Overall, MoPro offers a powerful toolkit for researchers working with large amounts of weakly annotated data to achieve better results while greatly reducing annotation costs.",1
"The diagnosis process of colorectal cancer mainly focuses on the localization and characterization of abnormal growths in the colon tissue known as polyps. Despite recent advances in deep object localization, the localization of polyps remains challenging due to the similarities between tissues, and the high level of artifacts. Recent studies have shown the negative impact of the presence of artifacts in the polyp detection task, and have started to take them into account within the training process. However, the use of prior knowledge related to the spatial interaction of polyps and artifacts has not yet been considered. In this work, we incorporate artifact knowledge in a post-processing step. Our method models this task as an inductive graph representation learning problem, and is composed of training and inference steps. Detected bounding boxes around polyps and artifacts are considered as nodes connected by a defined criterion. The training step generates a node classifier with ground truth bounding boxes. In inference, we use this classifier to analyze a second graph, generated from artifact and polyp predictions given by region proposal networks. We evaluate how the choices in the connectivity and artifacts affect the performance of our method and show that it has the potential to reduce the false positives in the results of a region proposal network.",0
"Artifact analysis involves understanding relationships within sets of objects in order to gain insights into their origins, uses, and meanings. In many cases, artifacts are found alongside other objects such as remains of structures or natural features that provide contextual information. Identifying patterns in these relationships can reveal new perspectives on both individual artifacts and broader cultural phenomena.  Polyps, which refer to groups of human settlements, have been used extensively by archaeologists studying ancient civilizations across different regions and time periods. These clusters of sites are often located near natural resources, trade routes, or strategic points. Understanding polyp-artifact relationships provides valuable insights into factors driving site location choices, distribution of material culture, social organization, and economic systems. However, analyzing these complex connections has remained challenging due to limitations in data accessibility and quality.  This study develops a novel approach utilizing graph indu",1
"Unsupervised (or self-supervised) graph representation learning is essential to facilitate various graph data mining tasks when external supervision is unavailable. The challenge is to encode the information about the graph structure and the attributes associated with the nodes and edges into a low dimensional space. Most existing unsupervised methods promote similar representations across nodes that are topologically close. Recently, it was shown that leveraging additional graph-level information, e.g., information that is shared among all nodes, encourages the representations to be mindful of the global properties of the graph, which greatly improves their quality. However, in most graphs, there is significantly more structure that can be captured, e.g., nodes tend to belong to (multiple) clusters that represent structurally similar nodes. Motivated by this observation, we propose a graph representation learning method called Graph InfoClust (GIC), that seeks to additionally capture cluster-level information content. These clusters are computed by a differentiable K-means method and are jointly optimized by maximizing the mutual information between nodes of the same clusters. This optimization leads the node representations to capture richer information and nodal interactions, which improves their quality. Experiments show that GIC outperforms state-of-art methods in various downstream tasks (node classification, link prediction, and node clustering) with a 0.9% to 6.1% gain over the best competing approach, on average.",0
"Unsure if you can meet that request? Please review my previous message again. Thank you. Can you provide an example of such an abstract which follows those rules and has less than 200 words? An ideal sample would be more helpful, but I would accept one with up to 4 sentences that don’t follow these guidelines as it’s easier to spot what went wrong. Just make clear that you know they aren’t following all your instructions so we can discuss them later on. You could either post it here or via PM. Thanks.",1
"The random subspace method, known as the pillar of random forests, is good at making precise and robust predictions. However, there is not a straightforward way yet to combine it with deep learning. In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than previous higher-order pooling methods, producing good results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS achieves superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with minor extra cost. Code is available at https://github.com/CupidJay/NRS_pytorch.",0
"This research proposes a novel method for neural network design that improves upon traditional techniques by incorporating randomness into subspaces, resulting in better performance on complex tasks. We demonstrate how our approach can achieve state-of-the-art results on several benchmark datasets across different domains including computer vision and natural language processing. Our contributions lie in demonstrating how randomly selected subsets of neurons can provide significant improvements over established methods, as well as showing the effectiveness of randomly sampling from different dimensions such as input features and model architecture. Overall, our work shows promise in advancing the field of deep learning through the use of randomization techniques, and we hope to encourage further exploration in this area.",1
"In this work, our objective is to address the problems of generalization and flexibility for text recognition in documents. We introduce a new model that exploits the repetitive nature of characters in languages, and decouples the visual representation learning and linguistic modelling stages. By doing this, we turn text recognition into a shape matching problem, and thereby achieve generalization in appearance and flexibility in classes. We evaluate the new model on both synthetic and real datasets across different alphabets and show that it can handle challenges that traditional architectures are not able to solve without expensive retraining, including: (i) it can generalize to unseen fonts without new exemplars from them; (ii) it can flexibly change the number of classes, simply by changing the exemplars provided; and (iii) it can generalize to new languages and new characters that it has not been trained for by providing a new glyph set. We show significant improvements over state-of-the-art models for all these cases.",0
"Here is one possible abstract that would meet your requirements:  ""This paper presents a novel approach to text recognition using visual matching techniques. Traditional approaches rely on machine learning algorithms trained on large datasets of labeled images, which can be time-consuming and costly to create. Our method instead uses a convolutional neural network (CNN) architecture that has been pre-trained on ImageNet to extract features from input images, followed by a regression layer to predict the character sequence. We demonstrate the effectiveness of our approach on several benchmark data sets, achieving state-of-the-art results in many cases.""  ---",1
"The contemporary process-aware information systems possess the capabilities to record the activities generated during the process execution. To leverage these process specific fine-granular data, process mining has recently emerged as a promising research discipline. As an important branch of process mining, predictive business process management, pursues the objective to generate forward-looking, predictive insights to shape business processes. In this study, we propose a conceptual framework sought to establish and promote understanding of decision-making environment, underlying business processes and nature of the user characteristics for developing explainable business process prediction solutions. Consequently, with regard to the theoretical and practical implications of the framework, this study proposes a novel local post-hoc explanation approach for a deep learning classifier that is expected to facilitate the domain experts in justifying the model decisions. In contrary to alternative popular perturbation-based local explanation approaches, this study defines the local regions from the validation dataset by using the intermediate latent space representations learned by the deep neural networks. To validate the applicability of the proposed explanation method, the real-life process log data delivered by the Volvo IT Belgium's incident management system are used.The adopted deep learning classifier achieves a good performance with the Area Under the ROC Curve of 0.94. The generated local explanations are also visualized and presented with relevant evaluation measures that are expected to increase the users' trust in the black-box-model.",0
"This article presents an overview of explainable artificial intelligence (XAI) as applied to process mining, which involves analyzing data from business processes to identify opportunities for improvement. In particular, we focus on predictive monitoring using local explanations, a novel approach that can provide insights into how specific events or decisions impact the overall performance of a process. We discuss the challenges and limitations of traditional XAI methods and highlight the benefits of our proposed approach, including improved transparency, interpretability, and decision support capabilities. Our results demonstrate the effectiveness of this methodology in real-world applications across diverse industries, indicating significant potential for enhancing process efficiency while ensuring human understanding and control. By advancing the state of art in process mining through explainable modeling techniques, our research contributes to better management practices and informed organizational transformation.",1
"In this paper, we investigate the suitability of state-of-the-art representation learning methods to the analysis of behavioral similarity of moving individuals, based on CDR trajectories. The core of the contribution is a novel methodological framework, mob2vec, centered on the combined use of a recent symbolic trajectory segmentation method for the removal of noise, a novel trajectory generalization method incorporating behavioral information, and an unsupervised technique for the learning of vector representations from sequential data. Mob2vec is the result of an empirical study conducted on real CDR data through an extensive experimentation. As a result, it is shown that mob2vec generates vector representations of CDR trajectories in low dimensional spaces which preserve the similarity of the mobility behavior of individuals.",0
"This work focuses on learning behavioral representations of human mobility patterns using location data from smartphones. We aim to develop models that can accurately predict future movement patterns based on past trajectories while minimizing error through fine tuning hyperparameters. To achieve this goal, we first gather and preprocess a large dataset consisting of user locations at different times over multiple days. After exploring several state-of-the-art deep learning techniques such as Recurrent Neural Networks (RNN), Long Short Term Memory networks (LSTM) and Transformer models, we demonstrate that transformer architectures are particularly well suited for capturing complex spatio temporal relationships among time-series data points. We evaluate our approach by comparing predicted versus actual user movements and showcase high accuracy across all metrics. Our results pave the way towards creating accurate behavioral representations for applications such as personalized traffic forecasting, emergency response systems and urban planning. Overall, this research lays down important foundational components required for advanced human activity recognition solutions.",1
"Although supervised deep representation learning has attracted enormous attentions across areas of pattern recognition and computer vision, little progress has been made towards unsupervised deep representation learning for image clustering. In this paper, we propose a deep spectral analysis network for unsupervised representation learning and image clustering. While spectral analysis is established with solid theoretical foundations and has been widely applied to unsupervised data mining, its essential weakness lies in the fact that it is difficult to construct a proper affinity matrix and determine the involving Laplacian matrix for a given dataset. In this paper, we propose a SA-Net to overcome these weaknesses and achieve improved image clustering by extending the spectral analysis procedure into a deep learning framework with multiple layers. The SA-Net has the capability to learn deep representations and reveal deep correlations among data samples. Compared with the existing spectral analysis, the SA-Net achieves two advantages: (i) Given the fact that one spectral analysis procedure can only deal with one subset of the given dataset, our proposed SA-Net elegantly integrates multiple parallel and consecutive spectral analysis procedures together to enable interactive learning across different units towards a coordinated clustering model; (ii) Our SA-Net can identify the local similarities among different images at patch level and hence achieves a higher level of robustness against occlusions. Extensive experiments on a number of popular datasets support that our proposed SA-Net outperforms 11 benchmarks across a number of image clustering applications.",0
"In recent years, advances in deep learning have shown great promise in achieving state-of-the-art results in tasks such as image classification, object detection, and segmentation. However, these methods often rely on large amounts of annotated data, which can be time-consuming and expensive to obtain. In this paper, we propose SA-Net, a novel deep neural network architecture that utilizes spectral analysis techniques for unsupervised image clustering. Our method leverages the discriminative power of convolutional neural networks while addressing some of their limitations, resulting in improved performance compared to prior methods. We demonstrate the effectiveness of our approach through comprehensive experiments on several benchmark datasets, including MNIST, CIFAR-10, and SVHN. Our findings suggest that SA-Net has the potential to significantly impact the field of computer vision by enabling scalable, high-quality clustering without requiring extensive annotations.",1
"Deep representation learning is a crucial procedure in multimedia analysis and attracts increasing attention. Most of the popular techniques rely on convolutional neural network and require a large amount of labeled data in the training procedure. However, it is time consuming or even impossible to obtain the label information in some tasks due to cost limitation. Thus, it is necessary to develop unsupervised deep representation learning techniques. This paper proposes a new network structure for unsupervised deep representation learning based on spectral analysis, which is a popular technique with solid theory foundations. Compared with the existing spectral analysis methods, the proposed network structure has at least three advantages. Firstly, it can identify the local similarities among images in patch level and thus more robust against occlusion. Secondly, through multiple consecutive spectral analysis procedures, the proposed network can learn more clustering-friendly representations and is capable to reveal the deep correlations among data samples. Thirdly, it can elegantly integrate different spectral analysis procedures, so that each spectral analysis procedure can have their individual strengths in dealing with different data sample distributions. Extensive experimental results show the effectiveness of the proposed methods on various image clustering tasks.",0
"This paper presents a novel deep learning model called Spectral Analysis Network (SAN) that integrates spectral analysis into convolutional neural networks to learn robust image representations. SAN introduces a new building block called the Spectral Attention Module (SAM), which leverages the power spectrum of images as a guidance to capture high-frequency details. Through extensive experiments on various benchmarks including CIFAR-10, CIFAR-100, STL-10, and ImageNet, we demonstrate that SAN achieves superior performance over state-of-the-art methods, setting new records on several datasets while maintaining computational efficiency and interpretability. Our approach provides new insights into representation learning and can be applied to diverse applications such as image clustering and classification. By bridging the gap between traditional handcrafted features and data-driven methods, our work opens up promising research directions towards unifying these complementary approaches within a single framework.",1
"While supervised deep learning has achieved great success in a range of applications, relatively little work has studied the discovery of knowledge from unlabeled data. In this paper, we propose an unsupervised deep learning framework to provide a potential solution for the problem that existing deep learning techniques require large labeled data sets for completing the training process. Our proposed introduces a new principle of joint learning on both deep representations and GMM (Gaussian Mixture Model)-based deep modeling, and thus an integrated objective function is proposed to facilitate the principle. In comparison with the existing work in similar areas, our objective function has two learning targets, which are created to be jointly optimized to achieve the best possible unsupervised learning and knowledge discovery from unlabeled data sets. While maximizing the first target enables the GMM to achieve the best possible modeling of the data representations and each Gaussian component corresponds to a compact cluster, maximizing the second term will enhance the separability of the Gaussian components and hence the inter-cluster distances. As a result, the compactness of clusters is significantly enhanced by reducing the intra-cluster distances, and the separability is improved by increasing the inter-cluster distances. Extensive experimental results show that the propose method can improve the clustering performance compared with benchmark methods.",0
"Title: Unsupervised Deep Learning Framework through Integrated Optimization of Representation Learning and Gaussian Mixture Modeling Abstract This paper presents a novel approach to unsupervised learning using a deep neural network that integrates both representation learning and Gaussian mixture modeling (GMM). By optimizing these two components simultaneously, we show that better results can be achieved compared to traditional methods that only use one method alone. Our proposed framework involves training the deep neural network on large amounts of data in order to learn representations of the underlying patterns present within the data. These learned representations serve as inputs to our GMM, which then models the probability distributions associated with each latent class. We optimize the parameters of both the neural network and the GMM together by minimizing a joint loss function that ensures that the learned representations lead to optimal clustering performance. Experimental results demonstrate the effectiveness of our proposed framework across a variety of tasks including image segmentation and anomaly detection, outperforming state-of-the-art unsupervised approaches. Overall, our work shows promise in enabling unsupervised machine learning that requires no human annotations or labels, allowing for more flexible and scalable solutions for real-world applications.",1
"Learning to infer graph representations and performing spatial reasoning in a complex surgical environment can play a vital role in surgical scene understanding in robotic surgery. For this purpose, we develop an approach to generate the scene graph and predict surgical interactions between instruments and surgical region of interest (ROI) during robot-assisted surgery. We design an attention link function and integrate with a graph parsing network to recognize the surgical interactions. To embed each node with corresponding neighbouring node features, we further incorporate SageConv into the network. The scene graph generation and active edge classification mostly depend on the embedding or feature extraction of node and edge features from complex image representation. Here, we empirically demonstrate the feature extraction methods by employing label smoothing weighted loss. Smoothing the hard label can avoid the over-confident prediction of the model and enhances the feature representation learned by the penultimate layer. To obtain the graph scene label, we annotate the bounding box and the instrument-ROI interactions on the robotic scene segmentation challenge 2018 dataset with an experienced clinical expert in robotic surgery and employ it to evaluate our propositions.",0
"Learning and reasoning play crucial roles in robotic surgery (RS), where the accuracy and effectiveness of surgeries rely heavily on the ability to make decisions based on limited sensory feedback from cameras and touch/force sensors. Graphs provide natural representations that can encode structured relationships among objects of interest, offering benefits such as better scalability and expressivity compared to traditional feature vectors. We develop a graph structure representation approach for RS procedures using graphs whose nodes represent key features observed during operations and edges model logical or causal dependencies. By leveraging graph convolution networks (GCNs) and probabilistic inference models like Hidden Markov Models (HMM), we enable automatic learning of relationships between different states within surgical workflows, so as to improve decision making at runtime, enabling greater autonomy for robots conducting RS. Our experimental results demonstrate improvements over baseline methods, indicating the potential utility of our proposed framework for enhancing efficiency and precision in real-world minimally invasive surgery settings.",1
"We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",0
"New approaches to self supervised learning have been developed that allow individuals to learn without the use of large datasets. These methods typically involve pretraining models using weakly labeled data, before fine tuning them on small amounts of task specific labels. While these approaches show promising results there are still limitations such as difficulty in obtaining high quality pseudo-labels. To overcome some of these difficulties we introduce ""Bootstrap Your Own Latent,"" a novel method that allows users to generate task relevant latent spaces from scratch using only unlabeled images. We demonstrate through experiments that our proposed approach outperforms baseline methods across several tasks including object detection, image classification and semantic segmentation while requiring fewer labeled examples. Our method opens up new possibilities in areas where collecting massive amounts of labeled data can be difficult, expensive, or impractical. -----",1
"Transportation systems often rely on understanding the flow of vehicles or pedestrian. From traffic monitoring at the city scale, to commuters in train terminals, recent progress in sensing technology make it possible to use cameras to better understand the demand, i.e., better track moving agents (e.g., vehicles and pedestrians). Whether the cameras are mounted on drones, vehicles, or fixed in the built environments, they inevitably remain scatter. We need to develop the technology to re-identify the same agents across images captured from non-overlapping field-of-views, referred to as the visual re-identification task. State-of-the-art methods learn a neural network based representation trained with the cross-entropy loss function. We argue that such loss function is not suited for the visual re-identification task hence propose to model confidence in the representation learning framework. We show the impact of our confidence-based learning framework with three methods: label smoothing, confidence penalty, and deep variational information bottleneck. They all show a boost in performance validating our claim. Our contribution is generic to any agent of interest, i.e., vehicles or pedestrians, and outperform highly specialized state-of-the-art methods across 5 datasets. The source code and models are shared towards an open science mission.",0
This sounds like a fascinating study! Can you please tell me more? I can then create an abstract that accurately summarizes your research findings.,1
"In deep representational learning, it is often desired to isolate a particular factor (termed {\em content}) from other factors (referred to as {\em style}). What constitutes the content is typically specified by users through explicit labels in the data, while all unlabeled/unknown factors are regarded as style. Recently, it has been shown that such content-labeled data can be effectively exploited by modifying the deep latent factor models (e.g., VAE) such that the style and content are well separated in the latent representations. However, the approach assumes that the content factor is categorical-valued (e.g., subject ID in face image data, or digit class in the MNIST dataset). In certain situations, the content is ordinal-valued, that is, the values the content factor takes are {\em ordered} rather than categorical, making content-labeled VAEs, including the latent space they infer, suboptimal. In this paper, we propose a novel extension of VAE that imposes a partially ordered set (poset) structure in the content latent space, while simultaneously making it aligned with the ordinal content values. To this end, instead of the iid Gaussian latent prior adopted in prior approaches, we introduce a conditional Gaussian spacing prior model. This model admits a tractable joint Gaussian prior, but also effectively places negligible density values on the content latent configurations that violate the poset constraint. To evaluate this model, we consider two specific ordinal structured problems: estimating a subject's age in a face image and elucidating the calorie amount in a food meal image. We demonstrate significant improvements in content-style separation over previous non-ordinal approaches.",0
"Title: Abstract Paper Title: Ordinal-Content VAE: Isolating Ordinal-Valued Content Factors in Deep Latent Variable Models  This paper proposes a new variant of Variational Autoencoders (VAEs), called Ordinal-Content VAEs (OCVAEs) that can learn ordinal-valued content factors from deep latent variable models. OCVAEs extend traditional VAEs by replacing their continuous Gaussian prior distributions over latent variables with discrete ones that are constrained to only take on integer values. This allows OCVAEs to model data modalities such as text, speech, music, images, etc., which often exhibit ordinal structure across multiple dimensions. We present results demonstrating that our method outperforms standard VAEs at tasks such as image generation, feature learning, and downstream task transfer, while also providing interpretability benefits by enabling a user to control the specific aspects of generated outputs through inputting desired ordinal values. Overall, our approach represents a promising direction towards building more interpretable generative models capable of handling complex, structured data types.",1
"Deep neural networks have shown exceptional learning capability and generalizability in the source domain when massive labeled data is provided. However, the well-trained models often fail in the target domain due to the domain shift. Unsupervised domain adaptation aims to improve network performance when applying robust models trained on medical images from source domains to a new target domain. In this work, we present an approach based on the Wasserstein distance guided disentangled representation to achieve 3D multi-domain liver segmentation. Concretely, we embed images onto a shared content space capturing shared feature-level information across domains and domain-specific appearance spaces. The existing mutual information-based representation learning approaches often fail to capture complete representations in multi-domain medical imaging tasks. To mitigate these issues, we utilize Wasserstein distance to learn more complete representation, and introduces a content discriminator to further facilitate the representation disentanglement. Experiments demonstrate that our method outperforms the state-of-the-art on the multi-modality liver segmentation task.",0
"Incorporate details such as contributions/innovations of your study, relevant background info (e.g., challenges), and possible applications: In this paper we propose an unsupervised domain adaptation method using the Earth Mover's distance to improve segmentation results on multiple domains of medical images where annotations aren't available. We show how our approach can effectively learn from limited labelled data in order to produce accurate segmentations across varying imaging modalities. Our work addresses current limitations in domain adaptation methods that rely solely on supervision, providing a valuable contribution to the field of computer vision. Potential clinical applications of 3D multi-domain liver segmentation could include improved radiotherapy planning and surgery preplanning by improving accuracy and efficiency of automatic organ segmentation. Overall, our proposed method shows promising results and paves the way towards more robust cross-modality image analysis solutions in medicine. (if you have any questions regarding my research please don't hesitate to ask)",1
"Graph neural networks (GNN), as a popular methodology for node representation learning on graphs, currently mainly focus on preserving the smoothness and identifiability of node representations. A robust node representation on graphs should further hold the stability property which means a node representation is resistant to slight perturbations on the input. In this paper, we introduce the stability of node representations in addition to the smoothness and identifiability, and develop a novel method called contrastive graph neural networks (CGNN) that learns robust node representations in an unsupervised manner. Specifically, CGNN maintains the stability and identifiability by a contrastive learning objective, while preserving the smoothness with existing GNN models. Furthermore, the proposed method is a generic framework that can be equipped with many other backbone models (e.g. GCN, GraphSage and GAT). Extensive experiments on four benchmarks under both transductive and inductive learning setups demonstrate the effectiveness of our method in comparison with recent supervised and unsupervised models.",0
"In recent years there has been a growing interest in graph neural networks (GNNs) due to their ability to model complex relationships between nodes in graphs. However, these models can be prone to overfitting and instability, particularly on large datasets with noisy labels. To address these issues, we propose a novel approach called GraphRank that learns robust node representations by combining principles from random walk theory with deep learning techniques. Our method iteratively constructs a set of graph signals at each layer, which encode rich structural information about the graph. These signals are then processed using graph convolutional layers and aggregation functions to learn spatially coherent embeddings. Finally, we use a denoising autoencoder architecture to regularize our learned representations and improve generalization performance. We evaluate our proposed method on several benchmark datasets including citation networks, social network graphs, and biological protein interaction graphs, demonstrating improved accuracy compared to state-of-the-art GNN algorithms. Overall, GraphRank offers a new perspective on graph representation learning that balances the extraction of dense structure with sparsity constraints induced through random walks. This enables us to obtain more accurate predictions while mitigating the effects of noise in large datasets. Keywords: Graph Neural Networks, Random Walks, Autoencoders, Regularization, Noise Tolerance.",1
"Graph neural networks (GNNs) are emerging machine learning models on graphs. One key property behind the expressiveness of existing GNNs is that the learned node representations are permutation-equivariant. Though being a desirable property for certain tasks, however, permutation-equivariance prevents GNNs from being proximity-aware, i.e., preserving the walk-based proximities between pairs of nodes, which is another critical property for graph analytical tasks. On the other hand, some variants of GNNs are proposed to preserve node proximities, but they fail to maintain permutation-equivariance. How to empower GNNs to be proximity-aware while maintaining permutation-equivariance remains an open problem. In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to maintain both proximity-awareness and permutation-equivariance properties. Specifically, we augment the existing GNNs with stochastic node representations learned to preserve node proximities. Though seemingly simple, we prove that such a mechanism can enable GNNs to preserve node proximities in theory while maintaining permutation-equivariance with certain parametrization. Extensive experimental results demonstrate the effectiveness and efficiency of SMP for tasks including node classification and link prediction.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for learning on graphs, with applications ranging from computer vision and natural language processing to recommendation systems and game playing. However, many existing GNN architectures suffer from limitations such as high computational complexity, limited scalability, and sensitivity to hyperparameter settings.  This paper proposes a simple and general framework for training GNNs that addresses these issues by using stochastic message passing. This approach allows us to train larger models with more complex interactions between nodes while significantly reducing computation time compared to batch methods. Our method also eliminates the need for explicit feature engineering, which can improve performance and reduce model size.  We evaluate our framework on several benchmark datasets across multiple domains and demonstrate significant improvements over baseline models in terms of accuracy, efficiency, and robustness. Overall, we believe our work represents a step forward towards building more effective and efficient graph learning models that can handle real-world problems at scale.",1
"Finding general evaluation metrics for unsupervised representation learning techniques is a challenging open research question, which recently has become more and more necessary due to the increasing interest in unsupervised methods. Even though these methods promise beneficial representation characteristics, most approaches currently suffer from the objective function mismatch. This mismatch states that the performance on a desired target task can decrease when the unsupervised pretext task is learned too long - especially when both tasks are ill-posed. In this work, we build upon the widely used linear evaluation protocol and define new general evaluation metrics to quantitatively capture the objective function mismatch and the more generic metrics mismatch. We discuss the usability and stability of our protocols on a variety of pretext and target tasks and study mismatches in a wide range of experiments. Thereby we disclose dependencies of the objective function mismatch across several pretext and target tasks with respect to the pretext model's representation size, target model complexity, pretext and target augmentations as well as pretext and target task types.",0
"Despite recent progress in unsupervised representation learning, one key challenge remains: how can we evaluate whether these representations capture meaningful structure in the data? In our work, we argue that previous evaluation metrics have missed a crucial aspect: the objective function mismatch between training and test time. This mismatch arises due to the use of different objectives during pretraining (e.g., reconstruction loss) compared to fine-tuning (e.g., classification or regression). We show through both theoretical analysis and experiments on benchmark datasets that previous approaches suffer from poor alignment between the pretrain and finetune stages, resulting in suboptimal performance. By proposing new methods grounded in Bayesian inference theory, we successfully address the objective function mismatch issue and improve downstream task results across several domains including computer vision and natural language processing. Our insights highlight important considerations for designing effective unsupervised representation learning algorithms.",1
"In this paper, we introduce the MLM (Multiple Languages and Modalities) dataset - a new resource to train and evaluate multitask systems on samples in multiple modalities and three languages. The generation process and inclusion of semantic data provide a resource that further tests the ability for multitask systems to learn relationships between entities. The dataset is designed for researchers and developers who build applications that perform multiple tasks on data encountered on the web and in digital archives. A second version of MLM provides a geo-representative subset of the data with weighted samples for countries of the European Union. We demonstrate the value of the resource in developing novel applications in the digital humanities with a motivating use case and specify a benchmark set of tasks to retrieve modalities and locate entities in the dataset. Evaluation of baseline multitask and single task systems on the full and geo-representative versions of MLM demonstrate the challenges of generalising on diverse data. In addition to the digital humanities, we expect the resource to contribute to research in multimodal representation learning, location estimation, and scene understanding.",0
"Developing machine learning models that can handle multiple tasks across different languages and modalities remains a challenging problem. One reason for this difficulty is the lack of datasets available for training and evaluation. To address this need, we present MLM: a benchmark dataset designed for multitask learning with multiple languages and modalities. We introduce new tasks involving natural language understanding, generation, and question answering in four languages (English, Spanish, German, and French). Additionally, we provide datasets in two distinct modalities: textual data (e.g., Wikipedia articles) and image data (i.e., images from Flickr with corresponding descriptions). Our dataset includes over 7 million tokens across 48k examples, making it one of the largest multilingual datasets to date. In addition, we create baseline models using popular architectures such as BERT, GPT, RoBERTa, and demonstrate the efficacy of our dataset by comparing these baselines on a suite of downstream evaluation tasks. Our hope is that by providing this extensive resource, we inspire researchers to innovate methods which better enable artificial intelligence systems to work effectively within diverse communication scenarios.",1
"This paper addresses the problem of self-supervised video representation learning from a new perspective -- by video pace prediction. It stems from the observation that human visual system is sensitive to video pace, e.g., slow motion, a widely used technique in film making. Specifically, given a video played in natural pace, we randomly sample training clips in different paces and ask a neural network to identify the pace for each video clip. The assumption here is that the network can only succeed in such a pace reasoning task when it understands the underlying video content and learns representative spatio-temporal features. In addition, we further introduce contrastive learning to push the model towards discriminating different paces by maximizing the agreement on similar video content. To validate the effectiveness of the proposed method, we conduct extensive experiments on action recognition and video retrieval tasks with several alternative network architectures. Experimental evaluations show that our approach achieves state-of-the-art performance for self-supervised video representation learning across different network architectures and different benchmarks. The code and pre-trained models are available at https://github.com/laura-wang/video-pace.",0
"Recent advances in deep learning have enabled significant improvements in computer vision tasks such as object detection, image classification, and semantic segmentation. However, training models on large amounts of annotated data can be expensive and time consuming. In this work, we propose a self-supervised approach that leverages video data to learn representations without explicit supervision. Our method uses the idea of pace prediction, which involves predicting future frame orders within a given video clip. By doing so, our model learns to encode meaningful features that capture motion patterns and spatiotemporal relationships between frames. We show that these learned representations outperform state-of-the-art methods on standard benchmarks for action recognition and video retrieval. Furthermore, our framework can be easily adapted to different architectures and datasets, making it a versatile tool for researchers working on video analysis problems. Overall, our work demonstrates the effectiveness of using unlabeled videos for representation learning and highlights the potential of self-supervised approaches for building robust visual systems.",1
"In discrete choice modeling (DCM), model misspecifications may lead to limited predictability and biased parameter estimates. In this paper, we propose a new approach for estimating choice models in which we divide the systematic part of the utility specification into (i) a knowledge-driven part, and (ii) a data-driven one, which learns a new representation from available explanatory variables. Our formulation increases the predictive power of standard DCM without sacrificing their interpretability. We show the effectiveness of our formulation by augmenting the utility specification of the Multinomial Logit (MNL) and the Nested Logit (NL) models with a new non-linear representation arising from a Neural Network (NN), leading to new choice models referred to as the Learning Multinomial Logit (L-MNL) and Learning Nested Logit (L-NL) models. Using multiple publicly available datasets based on revealed and stated preferences, we show that our models outperform the traditional ones, both in terms of predictive performance and accuracy in parameter estimation. All source code of the models are shared to promote open science.",0
"Abstract: Discrete choice models (DCMs) have been widely used in economics and marketing research to model consumer decision making behavior. However, traditional DCM specifications often fail to capture important aspects of consumers' preferences such as nonlinear effects, interactions among features, and heterogeneity across groups. In recent years, representation learning techniques such as deep neural networks (DNNs) have emerged as powerful tools that can improve our understanding of complex data structures by capturing high-level representations of the input data. This study proposes a novel framework that combines DCMs with DNNs to enhance the explanatory power of standard DCM specifications. Our approach involves training a neural network to predict the probability distribution over alternatives given observable characteristics and latent factors that capture unobserved heterogeneity. By integrating these two types of models, we aim to provide better predictions of individual choices while accounting for preference uncertainty and distributional assumptions. Using simulated and real-world datasets, we show that our method significantly outperforms conventional DCMs on multiple performance metrics including accuracy, coverage rates, and likelihood values. Overall, our results suggest that representation learning approaches can effectively enhance discrete choice models, providing valuable insights into customer decision making processes.",1
"Representation learning on networks offers a powerful alternative to the oft painstaking process of manual feature engineering, and as a result, has enjoyed considerable success in recent years. However, all the existing representation learning methods are based on the first-order network (FON), that is, the network that only captures the pairwise interactions between the nodes. As a result, these methods may fail to incorporate non-Markovian higher-order dependencies in the network. Thus, the embeddings that are generated may not accurately represent of the underlying phenomena in a network, resulting in inferior performance in different inductive or transductive learning tasks. To address this challenge, this paper presents HONEM, a higher-order network embedding method that captures the non-Markovian higher-order dependencies in a network. HONEM is specifically designed for the higher-order network structure (HON) and outperforms other state-of-the-art methods in node classification, network re-construction, link prediction, and visualization for networks that contain non-Markovian higher-order dependencies.",0
"Abstract--- Higher order networks (HON) have received increasing attention from researchers due to their ability to model more complex relationships between entities compared to traditional first-order logic. In recent years, several approaches have been proposed for learning HON embeddings which can represent higher order relationships effectively in low-dimensional vector spaces while preserving structural properties. However, these methods often struggle to capture semantic nuances in text data, particularly for rare relationships. To address these limitations, we propose a novel method called ""HOMER"" that utilizes multiple types of semantic embeddings along with graph convolutional layers to learn effective HON embeddings. Our approach significantly outperforms state-of-the-art baselines across a variety of benchmark datasets by achieving superior performance on metrics such as accuracy, F1 score, precision, recall, and macro F1. Additionally, our ablation studies demonstrate the effectiveness of each component used in the proposed method. Overall, our work shows promise towards enabling downstream NLP applications to leverage higher-level reasoning capabilities provided by HON representations.",1
"Unsupervised graph representation learning aims to learn low-dimensional node embeddings without supervision while preserving graph topological structures and node attributive features. Previous graph neural networks (GNN) require a large number of labeled nodes, which may not be accessible in real-world graph data. In this paper, we present a novel cluster-aware graph neural network (CAGNN) model for unsupervised graph representation learning using self-supervised techniques. In CAGNN, we perform clustering on the node embeddings and update the model parameters by predicting the cluster assignments. Moreover, we observe that graphs often contain inter-class edges, which mislead the GNN model to aggregate noisy information from neighborhood nodes. We further refine the graph topology by strengthening intra-class edges and reducing node connections between different classes based on cluster labels, which better preserves cluster structures in the embedding space. We conduct comprehensive experiments on two benchmark tasks using real-world datasets. The results demonstrate the superior performance of the proposed model over existing baseline methods. Notably, our model gains over 7% improvements in terms of accuracy on node clustering over state-of-the-arts.",0
"In this research work, we propose a novel approach called ""CAGNN"" (Cluster-Aware Graph Neural Network) to uncover meaningful representations of graphs from complex data sets using deep learning techniques. Our method leverages graph neural networks (GNNs) to learn high-dimensional embeddings that capture both local and global features while preserving neighborhood relationships. We introduce cluster formation during training which results in more distinctive, structured clusters. This provides interpretable insights into the underlying structure of the graphs, facilitating exploration by domain experts. Evaluations on multiple benchmark datasets demonstrate significant improvements over existing methods across tasks such as node classification and link prediction. These findings highlight the effectiveness of our proposed framework and have promising implications for real-world applications where understanding complex relationships among entities is essential, including social network analysis, bioinformatics, and recommendation systems.",1
"Joint image-text embedding extracted from medical images and associated contextual reports is the bedrock for most biomedical vision-and-language (V+L) tasks, including medical visual question answering, clinical image-text retrieval, clinical report auto-generation. In this study, we adopt four pre-trained V+L models: LXMERT, VisualBERT, UNIER and PixelBERT to learn multimodal representation from MIMIC-CXR radiographs and associated reports. The extrinsic evaluation on OpenI dataset shows that in comparison to the pioneering CNN-RNN model, the joint embedding learned by pre-trained V+L models demonstrate performance improvement in the thoracic findings classification task. We conduct an ablation study to analyze the contribution of certain model components and validate the advantage of joint embedding over text-only embedding. We also visualize attention maps to illustrate the attention mechanism of V+L models.",0
"Artificial intelligence (AI) systems have greatly improved over time as advancements in natural language processing techniques allow models to extract high quality representation from textual data. Additionally, deep learning approaches such as pre-trained transformers, specifically, vision and language architectures like CLIP and LXMERT, have been particularly effective in multimodality tasks where both image and text modalities are present. In this work we aim to compare these two state-of-the-art multimodal modeling architectures by using them on diverse medical datasets which consist of paired imaging studies and their respective reports. Our research focuses on understanding how well each architecture generalizes across different domains within radiology and pathology. Furthermore, we examine whether fine-tuning pre-trained language or vision components separately yields any significant performance improvements for downstream classification or regression problems. To evaluate our results, we construct experiments that measure accuracy in binary classification, multi-label prediction, and ablation analyses. Ultimately, through extensive experimentation and analysis, we provide insight into the strengths and weaknesses of both architectures, shedding light on best practices for applying multimodal models towards real world applications in healthcare settings.",1
"Unsupervised learning methods for feature extraction are becoming more and more popular. We combine the popular contrastive learning method (prototypical contrastive learning) and the classic representation learning method (autoencoder) to design an unsupervised feature learning network for hyperspectral classification. Experiments have proved that our two proposed autoencoder networks have good feature learning capabilities by themselves, and the contrastive learning network we designed can better combine the features of the two to learn more representative features. As a result, our method surpasses other comparison methods in the hyperspectral classification experiments, including some supervised methods. Moreover, our method maintains a fast feature extraction speed than baseline methods. In addition, our method reduces the requirements for huge computing resources, separates feature extraction and contrastive learning, and allows more researchers to conduct research and experiments on unsupervised contrastive learning.",0
"Unsupervised feature learning methods provide an effective means to learn data representations that can improve classification performance without relying on labelled data. In this work, we explore two unsupervised pretraining strategies – autoencoder training followed by prototypical contrastive learning (AutoPC) and self-supervised representation learning using a denoising autoencoder (DAE). We evaluate these approaches using the widely used ROSIS dataset, which contains both visible and near infrared hyperspectral imagery over urban environments. Our experimental results show that the proposed AutoPC approach achieves comparable accuracies relative to supervised baselines while requiring fewer labeled samples during fine-tuning. Additionally, our findings suggest that leveraging deep generative models as a component of SSL may offer improved generalization capabilities compared to traditional DAEs alone. Overall, this work highlights promising opportunities for advancing SSL research within remote sensing applications where large amounts of annotated training data remain challenging to obtain.",1
"Due to the hierarchical structure of many machine learning problems, bilevel programming is becoming more and more important recently, however, the complicated correlation between the inner and outer problem makes it extremely challenging to solve. Although several intuitive algorithms based on the automatic differentiation have been proposed and obtained success in some applications, not much attention has been paid to finding the optimal formulation of the bilevel model. Whether there exists a better formulation is still an open problem. In this paper, we propose an improved bilevel model which converges faster and better compared to the current formulation. We provide theoretical guarantee and evaluation results over two tasks: Data Hyper-Cleaning and Hyper Representation Learning. The empirical results show that our model outperforms the current bilevel model with a great margin. \emph{This is a concurrent work with \citet{liu2020generic} and we submitted to ICML 2020. Now we put it on the arxiv for record.}",0
"An improved bilevel model has been developed that offers faster computations and theoretical guarantees, which significantly enhances its effectiveness compared to traditional methods. Our new algorithm exploits recent advances in optimization techniques to drastically reduce computation time while retaining the accuracy of the solutions obtained. We demonstrate through numerical experiments on real-world problems that our method achieves optimal solutions at least as fast as state-of-the-art commercial software packages and can solve large-scale problems impracticable by these solvers. Further analysis shows that the proposed approach outperforms existing algorithms across several problem classes. The key contributions of the study include introducing the optimized algorithm for solving bi-level programs, offering comprehensive experimental results on benchmark instances of varying sizes and complexities, and establishing strong convergence properties. Overall, we provide significant improvements over current techniques, making our solution highly attractive for industrial applications requiring efficient decision support tools based on mathematical programming.",1
"Representation learning over graph structure data has been widely studied due to its wide application prospects. However, previous methods mainly focus on static graphs while many real-world graphs evolve over time. Modeling such evolution is important for predicting properties of unseen networks. To resolve this challenge, we propose SGRNN, a novel neural architecture that applies stochastic latent variables to simultaneously capture the evolution in node attributes and topology. Specifically, deterministic states are separated from stochastic states in the iterative process to suppress mutual interference. With semi-implicit variational inference integrated to SGRNN, a non-Gaussian variational distribution is proposed to help further improve the performance. In addition, to alleviate KL-vanishing problem in SGRNN, a simple and interpretable structure is proposed based on the lower bound of KL-divergence. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed model. Code is available at https://github.com/StochasticGRNN/SGRNN.",0
"Title: ""Stochastic Graph Recurrent Neural Networks""Authors: [Your name here]Publication year: (Year)Abstract:In recent years, deep learning has become increasingly important as a methodology for artificial intelligence research. This trend is reflected by the growth of stochastic graph recurrent neural networks (SGNN). SGNNs have been used successfully on many tasks, including image recognition, natural language processing, and playing games.The focus of this paper is on understanding how well they perform compared with traditional architectures like convolutional neural networks (CNNs), feedforward neural networks (FNNs), LSTMs, etc., as well as other generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). Our experiments show that SGNNs often achieve state-of-the art performance in terms of accuracy and speed.Our contributions in this work can be summarized as follows:We introduce a novel architecture based on the idea of using Gumbel-softmax distributions instead of deterministic softmax functions for sampling from the probability simplex. We discuss two ways to train our model - maximum likelihood estimation and contrastive divergence methods - and compare their results. We study whether our model achieves better performance than previous methods by conducting extensive evaluations on several benchmark datasets.Our findings indicate that SGNNs hold great promise for future work in computer vision, natural language processing, and game-playing domains. By providing strong empirical evidence of their effectiveness, we hope to encourage further exploration into these promising new architectures.----------------------------------------------This paper investigates the use of Stochastic Graph Recurrent Neural Networks (SGNNs) for performing tasks across multiple domains. Unlike traditional deep learning frameworks, SGNNs utilize the power of randomness during training and inference. Using pretrained SGNNs trained on ImageNet data as initialization, experim",1
"Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set, or sequence. Previously, the research community tries to tackle it by aggregating the elements in a group based on an indicator either defined by humans such as the quality and saliency, or generated by a black box such as the attention score. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from one of its element is not the quality or an inexplicable score, but the discriminability w.r.t. the model. We explicitly design the discrimiability using embedded class centroids on a proxy set. We show the discrimiability knowledge has good properties that can be distilled by a light-weight distillation network and can be generalized on the unseen target set. The whole procedure is denoted as discriminability distillation learning (DDL). The proposed DDL can be flexibly plugged into many group-based recognition tasks without influencing the original training procedures. Comprehensive experiments on various tasks have proven the effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes forward the state-of-the-art results on these tasks by an impressive margin.",0
"In recent years, group representation learning (GRL) has emerged as a powerful technique for solving complex machine learning problems by leveraging group structures. However, GRL models often suffer from poor discriminability performance due to their reliance on shallow representations that lack high-level abstractions. This paper presents a new approach called ""Discriminability Distillation"" (DiD) which addresses these shortcomings and improves the accuracy and robustness of GRL models.  The DiD method uses knowledge distillation techniques commonly employed in deep neural networks to transfer information from pretrained student models to a weaker teacher model. By doing so, our framework extracts knowledge from these students and incorporates them into a weak teacher network trained via self-supervision only, thus enabling richer, more informative features to be learned. Additionally, we propose a novel variant of the popular N-pair loss function that significantly boosts the effectiveness of the discriminative constraints imposed upon each layer of the student model during training. These modifications yield substantial improvements over state-of-the-art methods in multiple challenging computer vision tasks including face verification, fine-grained image classification, object detection, and human pose estimation.  In summary, our work presents a novel framework for advancing GRL techniques using insights derived from deep neural network training. Our experimental results demonstrate the effectiveness of our approach in significantly outperforming prior arts across a wide range of applications, validating the applicability of our methodology to real-world scenarios where accurate, scalable, and interpretable solutions are required.",1
"Person re-identification (re-id) aims to match the same person from images taken across multiple cameras. Most existing person re-id methods generally require a large amount of identity labeled data to act as discriminative guideline for representation learning. Difficulty in manually collecting identity labeled data leads to poor adaptability in practical scenarios. To overcome this problem, we propose an unsupervised center-based clustering approach capable of progressively learning and exploiting the underlying re-id discriminative information from temporal continuity within a camera. We call our framework Temporal Continuity based Unsupervised Learning (TCUL). Specifically, TCUL simultaneously does center based clustering of unlabeled (target) dataset and fine-tunes a convolutional neural network (CNN) pre-trained on irrelevant labeled (source) dataset to enhance discriminative capability of the CNN for the target dataset. Furthermore, it exploits temporally continuous nature of images within-camera jointly with spatial similarity of feature maps across-cameras to generate reliable pseudo-labels for training a re-identification model. As the training progresses, number of reliable samples keep on growing adaptively which in turn boosts representation ability of the CNN. Extensive experiments on three large-scale person re-id benchmark datasets are conducted to compare our framework with state-of-the-art techniques, which demonstrate superiority of TCUL over existing methods.",0
"This paper presents a novel approach for unsupervised learning in person re-identification using temporal continuity as a prior. Our method leverages the assumption that there exists inherent consistency in human motion patterns across time and space, which can provide valuable cues for learning representations without explicit labels. To achieve this goal, we introduce a new framework called Temporal Continuity based Unsupervised Learning (TCUL), which learns spatio-temporal embeddings by modeling the similarity between nearby video frames via self-supervision. We validate our approach on several benchmark datasets and demonstrate state-of-the-art performance while maintaining high efficiency and low computational overhead compared to existing supervised methods. Our results showcase the effectiveness of utilizing temporal continuity as a powerful regularizer for unsupervised person re-identification, paving the way towards robust and efficient solutions for various computer vision tasks.",1
"In this paper, we show that the performance of a learnt generative model is closely related to the model's ability to accurately represent the inferred \textbf{latent data distribution}, i.e. its topology and structural properties. We propose LaDDer to achieve accurate modelling of the latent data distribution in a variational autoencoder framework and to facilitate better representation learning. The central idea of LaDDer is a meta-embedding concept, which uses multiple VAE models to learn an embedding of the embeddings, forming a ladder of encodings. We use a non-parametric mixture as the hyper prior for the innermost VAE and learn all the parameters in a unified variational framework. From extensive experiments, we show that our LaDDer model is able to accurately estimate complex latent distribution and results in improvement in the representation quality. We also propose a novel latent space interpolation method that utilises the derived data distribution.",0
"Infertility affects millions of couples worldwide, causing distress and anxiety. Assisted reproductive technologies (ART) have significantly improved the chances of fertility but remain expensive and invasive procedures that involve multiple clinical interventions over several months. This study aimed to develop a predictive model using machine learning techniques such as Artificial Neural Network (ANN), Random Forest Regression (RFR) and XGBoost to identify the most relevant factors influencing ART success rates. We collected data from women undergoing their first cycle of IVF treatment at two Australian hospitals between January 2016 and December 2017, resulting in a dataset containing clinical details, hormonal measurements and sonographic imaging findings from more than 800 cycles. Our results indicate that patient age, male factor parameters and endometrial thickness on the day of oocyte retrieval were significant independent predictors of ART success rate. Furthermore, we found that a combination of these three variables allowed for better prediction compared to any single variable alone. By creating a simple yet informative tool that can accurately estimate a woman’s chance of achieving pregnancy following an initial ART attempt based on readily available characteristics, healthcare providers could counsel patients regarding realistic expectations while reducing both psychological stress and costly repetitive treatments. Overall, our approach has the potential to improve patient outcomes without increasing costs through personalised decision making regarding appropriate timing and management of subsequent ART attempts.",1
"Apart from discriminative models for classification and object detection tasks, the application of deep convolutional neural networks to basic research utilizing natural imaging data has been somewhat limited; particularly in cases where a set of interpretable features for downstream analysis is needed, a key requirement for many scientific investigations. We present an algorithm and training paradigm designed specifically to address this: decontextualized hierarchical representation learning (DHRL). By combining a generative model chaining procedure with a ladder network architecture and latent space regularization for inference, DHRL address the limitations of small datasets and encourages a disentangled set of hierarchically organized features. In addition to providing a tractable path for analyzing complex hierarchal patterns using variation inference, this approach is generative and can be directly combined with empirical and theoretical approaches. To highlight the extensibility and usefulness of DHRL, we demonstrate this method in application to a question from evolutionary biology.",0
"Learning to represent visual data using hierarchies has been shown to improve interpretability, efficiency, and accuracy, especially on complex tasks such as object recognition. This research proposes a novel approach called decontextualized learning (DL) that creates interpretable hierarchical representations by gradually removing context from training images. DL uses two deep neural networks: a generator network that encodes input images into latent space, and a discriminator network that evaluates the realism of the generated features. The generator network iteratively removes the lowest resolution layers until only high-resolution features remain, forcing the model to learn more abstract and meaningful representations at each level. Experimental results demonstrate the effectiveness of our method, achieving state-of-the art performance on several benchmark datasets while producing easily interpretable hierarchies through feature visualization and ablation studies. Our findings have important implications for understanding how machines can generate human-like representations, opening up new opportunities for designing intelligent systems that interact effectively with humans.",1
"Recently, the surge in popularity of Internet of Things (IoT), mobile devices, social media, etc. has opened up a large source for graph data. Graph embedding has been proved extremely useful to learn low-dimensional feature representations from graph structured data. These feature representations can be used for a variety of prediction tasks from node classification to link prediction. However, existing graph embedding methods do not consider users' privacy to prevent inference attacks. That is, adversaries can infer users' sensitive information by analyzing node representations learned from graph embedding algorithms. In this paper, we propose Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that integrates the disentangling and purging mechanisms to remove users' private information from learned node representations. The proposed method preserves the structural information and utility attributes of a graph while concealing users' private attributes from inference attacks. Extensive experiments on real-world graph datasets demonstrate the superior performance of APGE compared to the state-of-the-arts. Our source code can be found at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.",0
"Title: Anonymous Graph Representation via Learned Subspace Clustering Privacy concerns have become increasingly important as personal data becomes widely available online. To protect users from potential privacy breaches, graph embedding techniques that provide efficient representations of graphs while preserving user privacy are crucial. Existing methods focus on the application of random masks or noise vectors to original data. These approaches suffer from limitations such as high computational cost, impractical assumptions regarding the availability of training data, and lack of robustness against inference attacks. This work proposes a novel methodology based on subspace clustering to construct embeddings while guaranteeing strong protection against inference attack. Our method employs a learned mapping matrix to generate low-dimensional representations by minimizing reconstruction error subjected to constraints. We evaluate our proposed approach using four benchmark datasets comprising both real and synthetic graphs and demonstrate improvement over current state-of-the-art techniques.",1
"Large scale analysis of source code, and in particular scientific source code, holds the promise of better understanding the data science process, identifying analytical best practices, and providing insights to the builders of scientific toolkits. However, large corpora have remained unanalyzed in depth, as descriptive labels are absent and require expert domain knowledge to generate. We propose a novel weakly supervised transformer-based architecture for computing joint representations of code from both abstract syntax trees and surrounding natural language comments. We then evaluate the model on a new classification task for labeling computational notebook cells as stages in the data analysis process from data import to wrangling, exploration, modeling, and evaluation. We show that our model, leveraging only easily-available weak supervision, achieves a 38% increase in accuracy over expert-supplied heuristics and outperforms a suite of baselines. Our model enables us to examine a set of 118,000 Jupyter Notebooks to uncover common data analysis patterns. Focusing on notebooks with relationships to academic articles, we conduct the largest ever study of scientific code and find that notebook composition correlates with the citation count of corresponding papers.",0
"This research project proposes an innovative framework called Coral that combines code representation learning with weakly supervised transformer models for analyzing data analysis tasks. By leveraging static program representations and weak annotations generated from natural language queries, the proposed approach can learn effective representations that capture important structural features in code graphs while mitigating the need for manual feature engineering. An extensive experimental evaluation on four benchmark datasets shows that our method outperforms previous state-of-the-art approaches across a range of metrics including recall@k, F1 score, and mean reciprocal rank (MRR). Furthermore, we conduct a comprehensive ablation study to demonstrate the effectiveness of each component in our system. Our results suggest that Coral has significant potential as a powerful tool for solving challenging problems in software engineering, programming languages, and other related fields. Overall, this work represents a step forward towards building more reliable and efficient systems for automated source code analysis.",1
"Visual reasoning tasks such as visual question answering (VQA) require an interplay of visual perception with reasoning about the question semantics grounded in perception. However, recent advances in this area are still primarily driven by perception improvements (e.g. scene graph generation) rather than reasoning. Neuro-symbolic models such as Neural Module Networks bring the benefits of compositional reasoning to VQA, but they are still entangled with visual representation learning, and thus neural reasoning is hard to improve and assess on its own. To address this, we propose (1) a framework to isolate and evaluate the reasoning aspect of VQA separately from its perception, and (2) a novel top-down calibration technique that allows the model to answer reasoning questions even with imperfect perception. To this end, we introduce a differentiable first-order logic formalism for VQA that explicitly decouples question answering from visual perception. On the challenging GQA dataset, this framework is used to perform in-depth, disentangled comparisons between well-known VQA models leading to informative insights regarding the participating models as well as the task.",0
"Title: Unraveling Neural Symbolism in Visual Perception  A major challenge in neuroscience is understanding how we process complex visual scenes, particularly how our brains integrate sensory inputs into coherent representations that drive behavior. One promising approach is neuro-symbolic artificial intelligence (AI), which combines insights from neural computation with symbolic reasoning techniques to model high-level cognitive processes like perception and decision making. However, despite recent advances in both domains, many questions remain regarding the role of symbols in neural processing and vice versa. In particular, existing models struggle to disentangle so-called ""visual"" features (e.g., edges, corners) from more abstract ""reasoning"" concepts (e.g., object identity, spatial relationships). As such, it remains unclear whether these different aspects truly exist as separate entities within the brain or instead emerge through interactions among lower-level computations. This study addresses these issues by investigating human performance on challenging visual tasks and comparing those results against state-of-the-art deep learning and logic-based systems. Our analyses demonstrate several surprising links between symbol use, task difficulty, and overall performance across modalities - findings that have important implications for both theoretical and applied research in vision science. By better characterizing the interplay between perceptual and conceptual operations, this work paves the way for more sophisticated models of human cognition that can capture increasingly complex behaviors while respecting biological constraints.",1
"High-dimensional latent representations learned by neural network classifiers are notoriously hard to interpret. Especially in medical applications, model developers and domain experts desire a better understanding of how these latent representations relate to the resulting classification performance. We present Projective Latent Interventions (PLIs), a technique for retraining classifiers by back-propagating manual changes made to low-dimensional embeddings of the latent space. The back-propagation is based on parametric approximations of t-distributed stochastic neighbourhood embeddings. PLIs allow domain experts to control the latent decision space in an intuitive way in order to better match their expectations. For instance, the performance for specific pairs of classes can be enhanced by manually separating the class clusters in the embedding. We evaluate our technique on a real-world scenario in fetal ultrasound imaging.",0
"Abstract: For decades, machine learning has been used in fields as diverse as finance, healthcare, education, and marketing to create accurate predictive models that can make data-driven predictions or decisions. Despite their widespread adoption, these classifiers often suffer from two fundamental limitations: (i) they learn only on the available training examples; thus, any new unseen concepts or situations may cause drastic performance degradation, (ii) fine-grained control over their behavior is typically difficult due to the black box nature of most learned representations. In this work we address both problems via projective latent interventions. Informed by human feedback, our method adjusts the input encoding process to modify how the model processes certain aspects relevant to the task, effectively steering the system towards more desirable regions of its decision space while preserving original accuracy where possible. We demonstrate the applicability of our approach across different benchmark datasets and domains, including image classification, sentiment analysis, and speech recognition, showing improved generalization and interpretability compared to strong baselines like adversarial training or concept activation mapping.",1
"Federated learning allows many parties to collaboratively build a model without exposing data. Particularly, vertical federated learning (VFL) enables parties to build a robust shared machine learning model based upon distributed features about the same samples. However, VFL requires all parties to share a sufficient amount of overlapping samples. In reality, the set of overlapping samples may be small, leaving the majority of the non-overlapping data unutilized. In this paper, we propose Federated Multi-View Training (FedMVT), a semi-supervised learning approach that improves the performance of VFL with limited overlapping samples. FedMVT estimates representations for missing features and predicts pseudo-labels for unlabeled samples to expand training set, and trains three classifiers jointly based upon different views of the input to improve model's representation learning. FedMVT does not require parties to share their original data and model parameters, thus preserving data privacy. We conduct experiments on the NUS-WIDE and the CIFAR10. The experimental results demonstrate that FedMVT significantly outperforms vanilla VFL that only utilizes overlapping samples, and improves the performance of the local model in the party that owns labels.",0
"In order to increase robustness and scalability of vertical federated learning (vfl) methods that use supervision from a central server, we propose fedmvnt: semi-supervised vfl using multi-view training. Our method uses local unlabeled data in addition to labeled data provided by the server, allowing clients to better adapt their models to local data distributions and increasing both performance and privacy. We show through experiments on three real world datasets that our method outperforms previous state of art approaches for vfl under both iid and non-iid settings across several metrics such as accuracy and f1 score.",1
"With recent developments in smart technologies, there has been a growing focus on the use of artificial intelligence and machine learning for affective computing to further enhance the user experience through emotion recognition. Typically, machine learning models used for affective computing are trained using manually extracted features from biological signals. Such features may not generalize well for large datasets and may be sub-optimal in capturing the information from the raw input data. One approach to address this issue is to use fully supervised deep learning methods to learn latent representations of the biosignals. However, this method requires human supervision to label the data, which may be unavailable or difficult to obtain. In this work we propose an unsupervised framework reduce the reliance on human supervision. The proposed framework utilizes two stacked convolutional autoencoders to learn latent representations from wearable electrocardiogram (ECG) and electrodermal activity (EDA) signals. These representations are utilized within a random forest model for binary arousal classification. This approach reduces human supervision and enables the aggregation of datasets allowing for higher generalizability. To validate this framework, an aggregated dataset comprised of the AMIGOS, ASCERTAIN, CLEAS, and MAHNOB-HCI datasets is created. The results of our proposed method are compared with using convolutional neural networks, as well as methods that employ manual extraction of hand-crafted features. The methodology used for fusing the two modalities is also investigated. Lastly, we show that our method outperforms current state-of-the-art results that have performed arousal detection on the same datasets using ECG and EDA biosignals. The results show the wide-spread applicability for stacked convolutional autoencoders to be used with machine learning for affective computing.",0
"In recent years, there has been growing interest in using wearable sensors to measure physiological signals that can provide insight into individuals’ emotional states. These multi-modal data streams offer rich opportunities for affective computing researchers to investigate new methods for analyzing and modeling human behavior. However, developing effective models that capture the complex relationships between these different modalities remains a challenging task. This paper presents a novel approach to unsupervised representation learning from heterogeneous time-series wearable sensor data with multiple modalities (e.g., ECG, skin conductance, body movement). We propose a two-stage methodology: first, we use autoencoders to learn low-dimensional representations for each modality separately; then, we use a graph convolutional network (GCN) to fuse these learned features across all modalities into one shared space while capturing inter-modality correlations. Our proposed framework achieves state-of-the-art performance on several benchmark datasets commonly used in affective computing research and demonstrates improved generalization ability compared to other unimodal baselines. Our contributions enable more accurate and nuanced understanding of emotions from wearable sensors, enabling potential applications in healthcare, mental wellbeing, and personalized entertainment systems. The code and pretrained models have been made publicly available at <https://github.com/jianxinwang89127/> for further research advancements.",1
"The drug discovery stage is a vital aspect of the drug development process and forms part of the initial stages of the development pipeline. In recent times, machine learning-based methods are actively being used to model drug-target interactions for rational drug discovery due to the successful application of these methods in other domains. In machine learning approaches, the numerical representation of molecules is critical to the performance of the model. While significant progress has been made in molecular representation engineering, this has resulted in several descriptors for both targets and compounds. Also, the interpretability of model predictions is a vital feature that could have several pharmacological applications. In this study, we propose a self-attention-based multi-view representation learning approach for modeling drug-target interactions. We evaluated our approach using three benchmark kinase datasets and compared the proposed method to some baseline models. Our experimental results demonstrate the ability of our method to achieve competitive prediction performance and offer biologically plausible drug-target interaction interpretations.",0
"Title: ""Predicting drug-target interactions using multi-view self-attention""  Drug target interaction prediction plays a crucial role in understanding how drugs interact with biological systems at the molecular level. Accurate predictions can facilitate drug development by identifying potential drug targets, predicting adverse effects, and optimizing dosages. Traditional approaches have focused on either sequence-based features or chemical structure-based features, but neglect the importance of integrating multiple views of data. In our work, we present a novel approach that utilizes multi-view self-attention to integrate sequence, structure, and ligand-based features for improved drug-target interaction prediction. Our method achieves state-of-the-art performance across several benchmark datasets, demonstrating the effectiveness of incorporating multiple feature types through attention mechanisms. Furthermore, our model produces interpretable outputs, allowing researchers to gain insights into which aspects of a given view contribute most strongly to the predicted outcome. Overall, our work presents a significant advancement in the field of computational pharmacology, providing a powerful tool for drug discovery and design.",1
"We propose and demonstrate a novel machine learning algorithm that assesses pulmonary edema severity from chest radiographs. While large publicly available datasets of chest radiographs and free-text radiology reports exist, only limited numerical edema severity labels can be extracted from radiology reports. This is a significant challenge in learning such models for image classification. To take advantage of the rich information present in the radiology reports, we develop a neural network model that is trained on both images and free-text to assess pulmonary edema severity from chest radiographs at inference time. Our experimental results suggest that the joint image-text representation learning improves the performance of pulmonary edema assessment compared to a supervised model trained on images only. We also show the use of the text for explaining the image classification by the joint model. To the best of our knowledge, our approach is the first to leverage free-text radiology reports for improving the image model performance in this application. Our code is available at https://github.com/RayRuizhiLiao/joint_chestxray.",0
"Accurately assessing pulmonary edema on chest radiography (CXR) can be challenging due to the subjectivity involved in interpreting these images, as well as the variability in medical expertise among physicians. To address this issue, we propose a novel approach that involves joint modeling of CXR images and associated radiology reports using natural language processing (NLP). By leveraging both visual information from the CXR image itself and textual information from the accompanying report, our method has demonstrated improved accuracy compared to traditional approaches relying solely on image interpretation. Furthermore, our proposed system achieves state-of-the-art performance compared to previous NLP-based methods in classifying images into normal and abnormal categories based on severity levels identified by human experts. Our findings indicate that this combined approach represents a promising direction towards improving computer-aided diagnostics for lung diseases, potentially reducing errors caused by operator inconsistency and enabling more efficient workflow management within busy hospital environments.  Keywords: Natural Language Processing (NLP), Computer Vision, Medical Imagery Analysis, Lung Disease Diagnosis, Image Classification, Deep Learning  Word Count: 296 (excluding title) ---------------------------------------------------------------...",1
"Learning about many things can provide numerous benefits to a reinforcement learning system. For example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. The question we tackle in this paper is how to sculpt the stream of experience---how to adapt the learning system's behavior---to optimize the learning of a collection of value functions. A simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. Unfortunately, implementing this simple idea has proven difficult, and thus has been the focus of decades of study. It remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. In this paper, we investigate and compare different intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. We discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. We provide a comprehensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement learning and active learning. Our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behavior, if each individual learner is introspective.",0
"This article surveys relevant research into adaptive behavior through intrinsic reward mechanisms and presents empirical findings from our own studies on the subject. We found that intrinsic rewards can indeed drive adaptation, even without extrinsic motivation. However, we observed important differences across individuals, suggesting a need for personalized strategies. Our work contributes to current debates by refining existing models and introducing novel hypotheses that should guide future research. Ultimately, insights gained here have implications far beyond the laboratory and could inspire new approaches to promoting beneficial behaviors in diverse contexts. We believe that our study advances knowledge on human decision making, cognition, and learning—core components of artificial intelligence development and applications.",1
"In recent years, deep learning-based feature representation methods have shown a promising impact in electroencephalography (EEG)-based brain-computer interface (BCI). Nonetheless, owing to high intra- and inter-subject variabilities, many studies on decoding EEG were designed in a subject-specific manner by using calibration samples, with no concern of its practical use, hampered by time-consuming steps and a large data requirement. To this end, recent studies adopted a transfer learning strategy, especially domain adaptation techniques. Among those, to our knowledge, an adversarial learning has shown its potential in BCIs. In the meantime, it is known that adversarial learning-based domain adaptation methods are prone to negative transfer that disrupts learning generalized feature representations, applicable to diverse domains, e.g., subjects or sessions in BCIs. In this paper, we propose a novel framework that learns class-relevant and subject-invariant feature representations in an information-theoretic manner, without using adversarial learning. To be specific, we devise two operational components in a deep network that explicitly estimate mutual information between feature representations; (1) to decompose features in an intermediate layer into class-relevant and class-irrelevant ones, (2) to enrich class-discriminative feature representation. On two large EEG datasets, we validated the effectiveness of our proposed framework by comparing with several comparative methods in performance. Further, we conducted rigorous analyses by performing an ablation study in regard to the components in our network, explaining our model's decision on input EEG signals via layer-wise relevance propagation, and visualizing the distribution of learned features via t-SNE.",0
"Recent advances in Brain Computer Interface (BCI) technology have shown promise in enabling humans to control external devices using their brain signals. However, one major challenge faced by BCI systems is the lack of class-specific features that can accurately represent different mental states or tasks. In order to address this issue, we propose a novel deep learning framework called Mutual Information-driven Subject-Invariant and Class-Relevant Deep Representation Learning (MISIDRDL).  Our approach utilizes mutual information estimation as a regularization term during training to encourage our model to learn subject-invariant yet class-relevant representations. We evaluated our proposed method on two publicly available datasets, P300 Speller and LASER EEG dataset, and compared its performance against state-of-the-art methods. Experimental results showed that MISIDRDL significantly outperformed other algorithms, achieving higher accuracy rates and robustness across subjects.  Overall, our work demonstrates that utilizing mutual information as a regularization term enables BCI models to generate more accurate feature representations. Our findings could potentially lead to improved performance in BCI applications, such as assistive technologies for individuals with disabilities or advanced interfaces for gaming and entertainment.  Keywords: Brain Computer Interface (BCI), Mutual Information, Subject-invariance, Class-relevance, Deep Representation Learning",1
"Recommender systems play a fundamental role in web applications in filtering massive information and matching user interests. While many efforts have been devoted to developing more effective models in various scenarios, the exploration on the explainability of recommender systems is running behind. Explanations could help improve user experience and discover system defects. In this paper, after formally introducing the elements that are related to model explainability, we propose a novel explainable recommendation model through improving the transparency of the representation learning process. Specifically, to overcome the representation entangling problem in traditional models, we revise traditional graph convolution to discriminate information from different layers. Also, each representation vector is factorized into several segments, where each segment relates to one semantic aspect in data. Different from previous work, in our model, factor discovery and representation learning are simultaneously conducted, and we are able to handle extra attribute information and knowledge. In this way, the proposed model can learn interpretable and meaningful representations for users and items. Unlike traditional methods that need to make a trade-off between explainability and effectiveness, the performance of our proposed explainable model is not negatively affected after considering explainability. Finally, comprehensive experiments are conducted to validate the performance of our model as well as explanation faithfulness.",0
"Title: ""Explaining the Mystery Behind Recommendation Engines""  Recommendation systems have become ubiquitous across various platforms and industries, from streaming services suggesting movies or music to e-commerce websites recommending products. Despite their widespread adoption and impact on user engagement and satisfaction, these recommendation engines often operate as ""black boxes,"" lacking transparency into how they generate recommendations. This study proposes a novel approach to enhance explainability in recommendation systems through resolving learning representations (RLR). By leveraging RLR, we aim to unravel the complex decision-making process behind recommendation algorithms and provide insights into how individual user preferences and behaviors influence generated suggestions. Our methodology involves training recommendation models using deep learning techniques, applying regularization methods to reduce overfitting and improve interpretability, and utilizing dimensionality reduction techniques to visualize latent feature spaces. We evaluate our approach using multiple datasets and compare performance against traditional recommendation algorithms. Results indicate that our proposed solution achieves competitive prediction accuracy while offering greater transparency into the underlying reasoning processes. Overall, this research advances the understanding of recommendation mechanisms by demystifying machine learning-based approaches, promoting trustworthiness in personalized content delivery, and paving the way towards more human-like explanations of automated decisions.",1
"Deep clustering against self-supervised learning is a very important and promising direction for unsupervised visual representation learning since it requires little domain knowledge to design pretext tasks. However, the key component, embedding clustering, limits its extension to the extremely large-scale dataset due to its prerequisite to save the global latent embedding of the entire dataset. In this work, we aim to make this framework more simple and elegant without performance decline. We propose an unsupervised image classification framework without using embedding clustering, which is very similar to standard supervised training manner. For detailed interpretation, we further analyze its relation with deep clustering and contrastive learning. Extensive experiments on ImageNet dataset have been conducted to prove the effectiveness of our method. Furthermore, the experiments on transfer learning benchmarks have verified its generalization to other downstream tasks, including multi-label image classification, object detection, semantic segmentation and few-shot image classification.",0
"Title: ""Unsupervised Image Classification for Deep Representation Learning""  In recent years, deep learning has emerged as one of the most powerful tools in artificial intelligence, particularly in image classification tasks. However, training deep neural networks requires large amounts of labeled data, which can be time-consuming and expensive to collect. In many real-world scenarios, only small amounts of annotated images may be available, leading to poor performance on unseen images. To overcome this challenge, we propose a new approach that leverages unlabeled datasets for deep representation learning. We demonstrate that our method significantly improves accuracy compared to traditional supervised approaches while requiring fewer annotations. Our results show that pretraining on unlabeled data using self-supervision achieves state-of-the-art results across several benchmarks, outperforming other popular methods by significant margins. These findings provide important insights into how unsupervised learning techniques can enhance deep representations for image classification, making it more accessible to practitioners who have limited access to labeled data. Overall, these advances hold great promise for a wide range of applications, from medical imaging to computer vision, where accurate object recognition is critical.",1
"Graph representation learning is gaining popularity in a wide range of applications, such as social networks analysis, computational biology, and recommender systems. However, different with positive results from many academic studies, applying graph neural networks (GNNs) in a real-world application is still challenging due to non-stationary environments. The underlying distribution of streaming data changes unexpectedly, resulting in different graph structures (a.k.a., concept drift). Therefore, it is essential to devise a robust graph learning technique so that the model does not overfit to the training graphs. In this work, we present Hop Sampling, a straightforward regularization method that can effectively prevent GNNs from overfishing. The hop sampling randomly selects the number of propagation steps rather than fixing it, and by doing so, it encourages the model to learn meaningful node representation for all intermediate propagation layers and to experience a variety of plausible graphs that are not in the training set. Particularly, we describe the use case of our method in recommender systems, a representative example of the real-world non-stationary case. We evaluated hop sampling on a large-scale real-world LINE dataset and conducted an online A/B/n test in LINE Coupon recommender systems of LINE Wallet Tab. Experimental results demonstrate that the proposed scheme improves the prediction accuracy of GNNs. We observed hop sampling provides 7.97% and 16.93% improvements for NDCG and MAP compared to non-regularized GNN models in our online service. Furthermore, models using hop sampling alleviate the oversmoothing issue in GNNs enabling a deeper model as well as more diversified representation.",0
"In many real world applications, non-stationarity exists due to dynamic environments such as changing distributions and drifting rewards during agent training, making model learning difficult. To address this issue, we propose Hop Sampling, a simple regularization approach to graph neural network architectures that enables generalizable and adaptive behavior under partial observability. We demonstrate through both synthetic experiments and benchmark comparison on two challenging MuJoCo tasks - Ant and Humanoid-v2 - how our method achieves comparable results to previous state-of-the-art methods while using less computational resources, suggesting its efficiency and effectiveness in tackling the problem of non-stationarity. Our work provides insights into understanding the tradeoff between exploration and exploitation, highlighting the utility of incorporating uncertainty modeling in RL algorithms. Overall, our contributions offer a new perspective on graph learning from partially observable systems, promising further research into the combination of regularization techniques with RL models.",1
"This paper presents the novel Riemannian Fusion Network (RFNet), a deep neural architecture for learning spatial and temporal information from Electroencephalogram (EEG) for a number of different EEG-based Brain Computer Interface (BCI) tasks and applications. The spatial information relies on Spatial Covariance Matrices (SCM) of multi-channel EEG, whose space form a Riemannian Manifold due to the Symmetric and Positive Definite structure. We exploit a Riemannian approach to map spatial information onto feature vectors in Euclidean space. The temporal information characterized by features based on differential entropy and logarithm power spectrum density is extracted from different windows through time. Our network then learns the temporal information by employing a deep long short-term memory network with a soft attention mechanism. The output of the attention mechanism is used as the temporal feature vector. To effectively fuse spatial and temporal information, we use an effective fusion strategy, which learns attention weights applied to embedding-specific features for decision making. We evaluate our proposed framework on four public datasets from three popular fields of BCI, notably emotion recognition, vigilance estimation, and motor imagery classification, containing various types of tasks such as binary classification, multi-class classification, and regression. RFNet approaches the state-of-the-art on one dataset (SEED) and outperforms other methods on the other three datasets (SEED-VIG, BCI-IV 2A, and BCI-IV 2B), setting new state-of-the-art values and showing the robustness of our framework in EEG representation learning.",0
"In recent years, brain-computer interfaces (BCIs) have become increasingly popular as they offer a potential means of communication and control for individuals suffering from neurological disorders such as paralysis and locked-in syndrome. One common method for creating BCIs involves using electroencephalography (EEG), which records electrical activity on the scalp to infer neural activity in the brain. However, developing reliable BCI systems can be challenging due to the complexity and variability of human brain signals. To address these limitations, we propose the use of a novel deep learning architecture called the Riemannian Fusion Network (RFNet). This architecture combines multiple modalities of EEG data into one model that learns rich representations of brain states. Our experiments demonstrate that RFNets significantly outperform traditional linear models and other state-of-the-art deep learning methods across three different BCI tasks, including imagined hand movement classification, motor execution prediction, and online robot arm control. These results showcase the great promise of our approach for advancing EEG-based BCIs towards real-world applications. We believe that RFNets could provide valuable tools for researchers studying the brain as well as clinicians seeking non-invasive ways of assisting their patients with limited mobility. While more work needs to be done in terms of validating performance over larger datasets and exploring additional use cases, our findings highlight the tremendous opportunity offered by deep learning techniques in the field of neuroscience research and neuroprosthetics development.",1
"Face anti-spoofing is crucial to security of face recognition systems. Previous approaches focus on developing discriminative models based on the features extracted from images, which may be still entangled between spoof patterns and real persons. In this paper, motivated by the disentangled representation learning, we propose a novel perspective of face anti-spoofing that disentangles the liveness features and content features from images, and the liveness features is further used for classification. We also put forward a Convolutional Neural Network (CNN) architecture with the process of disentanglement and combination of low-level and high-level supervision to improve the generalization capabilities. We evaluate our method on public benchmark datasets and extensive experimental results demonstrate the effectiveness of our method against the state-of-the-art competitors. Finally, we further visualize some results to help understand the effect and advantage of disentanglement.",0
"This work presents a novel approach to face anti-spoofing using disentangled representation learning. In recent years, there has been significant interest in developing methods that can effectively distinguish between genuine faces and spoof attacks such as photographs or videos presented to fool the system. However, most existing approaches rely on handcrafted features which may not capture all relevant information from complex images. Our method addresses this issue by leveraging unsupervised disentanglement techniques to learn more meaningful representations of facial identities. These representations encode both identity-specific features and general image patterns that are irrelevant for authentication purposes. By doing so, we enhance the robustness of our model against various types of spoofing attacks while achieving state-of-the-art performance. We evaluate our method on several publicly available datasets and demonstrate its effectiveness through extensive experiments. Overall, our work provides valuable insights into how to use disentangled representations for improved face anti-spoofing and offers promising directions for future research in the field.",1
"Knowledge Distillation (KD) based methods adopt the one-way Knowledge Transfer (KT) scheme in which training a lower-capacity student network is guided by a pre-trained high-capacity teacher network. Recently, Deep Mutual Learning (DML) presented a two-way KT strategy, showing that the student network can be also helpful to improve the teacher network. In this paper, we propose Dense Cross-layer Mutual-distillation (DCM), an improved two-way KT method in which the teacher and student networks are trained collaboratively from scratch. To augment knowledge representation learning, well-designed auxiliary classifiers are added to certain hidden layers of both teacher and student networks. To boost KT performance, we introduce dense bidirectional KD operations between the layers appended with classifiers. After training, all auxiliary classifiers are discarded, and thus there are no extra parameters introduced to final models. We test our method on a variety of KT tasks, showing its superiorities over related methods. Code is available at https://github.com/sundw2014/DCM",0
"In recent years, the use of deep learning techniques has revolutionized many fields including computer vision, natural language processing, and speech recognition. However, one major challenge that remains is how to effectively transfer knowledge gained from large pre-trained models to smaller models. This paper proposes a new method called dense cross-layer mutual distillation (DCMD) which efficiently transfers knowledge across layers within the same model as well as between different models. Our approach involves training two parallel deep neural networks simultaneously while iteratively exchanging their knowledge by selectively keeping and sharing representations at each layer. We demonstrate through extensive experiments on several benchmark datasets that DCMD achieves state-of-the-art performance compared to other methods for knowledge transfer. Additionally, we show that our method can significantly reduce the computational cost required for fine-tuning without sacrificing accuracy. Finally, we provide insights into why DCMD works so well and discuss potential future research directions. Overall, our results highlight the effectiveness of using cross-layer distillation for improving the efficiency and performance of deep learning systems.",1
"Self-supervised representation learning has achieved impressive results in recent years, with experiments primarily coming on ImageNet or other similarly large internet imagery datasets. There has been little to no work with these methods on other smaller domains, such as satellite, textural, or biological imagery. We experiment with several popular methods on an unprecedented variety of domains. We discover, among other findings, that Rotation is by far the most semantically meaningful task, with much of the performance of Jigsaw and Instance Discrimination being attributable to the nature of their induced distribution rather than semantic understanding. Additionally, there are several areas, such as fine-grain classification, where all tasks underperform. We quantitatively and qualitatively diagnose the reasons for these failures and successes via novel experiments studying pretext generalization, random labelings, and implicit dimensionality. Code and models are available at https://github.com/BramSW/Extending_SSRL_Across_Domains/.",0
"Artificial intelligence has been rapidly growing over the past decade as more data becomes available. As a result, self supervision learning (SSL) has emerged as a subfield that trains deep neural networks without any labels on the training data by using pretext tasks or predictive models which generate pseudo annotations for self training. In order to apply SSL effectively across domains we must first extend SSL techniques to new datasets. Furthermore, SSL should be analyzed and compared against fully supervised techniques to determine if SSL can achieve comparable performance given fewer labeled examples. To evaluate the generalization performance, SSL should also be applied to multiple datasets from different tasks such as natural language processing(NLP), computer vision(CV), audio and speech processing(ASP). This would allow us to see how well SSL performs cross domain transfer. Ultimately, our goal is to provide guidance on where and how SSL works best to benefit both academic researchers and industry practitioners.",1
We propose a novel algorithm for unsupervised graph representation learning with attributed graphs. It combines three advantages addressing some current limitations of the literature: i) The model is inductive: it can embed new graphs without re-training in the presence of new data; ii) The method takes into account both micro-structures and macro-structures by looking at the attributed graphs at different scales; iii) The model is end-to-end differentiable: it is a building block that can be plugged into deep learning pipelines and allows for back-propagation. We show that combining a coarsening method having strong theoretical guarantees with mutual information maximization suffices to produce high quality embeddings. We evaluate them on classification tasks with common benchmarks of the literature. We show that our algorithm is competitive with state of the art among unsupervised graph representation learning methods.,0
"This paper presents a new method for hierarchical graph representation learning called Loukas's coarsening (LK). LK uses unsupervised techniques to create a hierarchy of graphs that capture different levels of detail from an input dataset. Our results show that LK can effectively learn meaningful representations of graphs while preserving their structural properties such as connectivity, distances, and cluster structure. We evaluate our approach on several benchmark datasets and compare it against other state-of-the-art methods, demonstrating its competitiveness across multiple metrics. Overall, we believe that LK is a promising technique for solving problems in areas such as computer vision, natural language processing, and computational social science where graphs play a central role.",1
"The advisor-advisee relationship represents direct knowledge heritage, and such relationship may not be readily available from academic libraries and search engines. This work aims to discover advisor-advisee relationships hidden behind scientific collaboration networks. For this purpose, we propose a novel model based on Network Representation Learning (NRL), namely Shifu2, which takes the collaboration network as input and the identified advisor-advisee relationship as output. In contrast to existing NRL models, Shifu2 considers not only the network structure but also the semantic information of nodes and edges. Shifu2 encodes nodes and edges into low-dimensional vectors respectively, both of which are then utilized to identify advisor-advisee relationships. Experimental results illustrate improved stability and effectiveness of the proposed model over state-of-the-art methods. In addition, we generate a large-scale academic genealogy dataset by taking advantage of Shifu2.",0
"Title: Uncovering Advisor-Advisee Interactions using Network Representation Learning Techniques  This study presents a novel approach to extract relationships among advisors and advisees within academic institutions based on publicly available sources such as research papers, grants, co-authorships, and shared affiliations. We introduce Shifu2 - a network representation learning model that mines data from these sources to infer advisor-advisee connections by leveraging techniques like node embeddings, graph convolutional networks, and transfer learning. This enables us to capture nonlinear patterns in these complex relationships and obtain more accurate results than traditional methods.  Our experiments demonstrate the effectiveness of our method, achieving high accuracy on both synthetic datasets and real-world examples drawn from computer science and biomedical communities. The evaluation reveals significant improvements over state-of-the-art baseline models, emphasizing the potential benefits of Shifu2 for unearthing informative knowledge about mentorship ties across different domains.  In summary, we have developed a powerful tool capable of identifying hidden mentoring interactions that can aid recruitment efforts, improve collaboration practices, and better align students with appropriate advisors to advance their careers. Our work highlights the importance of mining these types of connections and opens new possibilities for future exploration into the dynamics underlying successful mentoring relationships.",1
"With the rise of deep learning methods, person Re-Identification (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons' appearance rarely changes. In real-world applications such as in a shopping mall, the same person's clothing may change, and different persons may wearing similar clothes. All these cases can result in an inconsistent ReID performance, revealing a critical problem that current ReID models heavily rely on person's apparels. Therefore, it is critical to learn an apparel-invariant person representation under cases like cloth changing or several persons wearing similar clothes. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN) to synthesize cloth changing images according to the target cloth embedding. It's worth noting that the images used in ReID tasks were cropped from real-world low-quality CCTV videos, making it more challenging to synthesize cloth changing images. We conduct extensive experiments on several datasets comparing with several baselines. Experimental results demonstrate that our proposal can improve the ReID performance of the baseline models.",0
"Apparel-invariant feature learning for person re-identification is essential since clothes may change drastically over time but human features remain unaltered. Most existing methods address this problem by either utilizing high-level clothing parsing techniques or employing image-specific deep models. In our work we take an intermediate approach that extracts appearance-based representations from raw images at different levels using pre-trained deep convolutional neural networks (CNN) and then applies multi-task metric learning for apparel-independent comparisons. To achieve better robustness across changes in illumination, viewpoint as well as background, we collect datasets of images labeled with annotations on identity and bounding box across camera views. We extensively analyze two state-of-the-art baseline methods and show that our proposed method outperforms these methods on four benchmark dataset splits consistently. Our evaluation also reveals some interesting insights into the limitations of current approaches and suggests several directions for future research. Overall our results demonstrate significant progress towards reliable person identification across varied environments and circumstances.",1
"In medical imaging, manual annotations can be expensive to acquire and sometimes infeasible to access, making conventional deep learning-based models difficult to scale. As a result, it would be beneficial if useful representations could be derived from raw data without the need for manual annotations. In this paper, we propose to address the problem of self-supervised representation learning with multi-modal ultrasound video-speech raw data. For this case, we assume that there is a high correlation between the ultrasound video and the corresponding narrative speech audio of the sonographer. In order to learn meaningful representations, the model needs to identify such correlation and at the same time understand the underlying anatomical features. We designed a framework to model the correspondence between video and audio without any kind of human annotations. Within this framework, we introduce cross-modal contrastive learning and an affinity-aware self-paced learning scheme to enhance correlation modelling. Experimental evaluations on multi-modal fetal ultrasound video and audio show that the proposed approach is able to learn strong representations and transfers well to downstream tasks of standard plane detection and eye-gaze prediction.",0
This study proposes a self-supervised learning method that combines contrastive learning with speech recognition to develop video representation models from ultrasound data. The method uses pretext tasks such as jigsaw puzzles and temporal order prediction to learn the underlying representations without any labeled examples. Experiments show significant improvements over unsupervised baselines in downstream applications like speaker verification and automatic speech recognition (ASR). The results suggest that our approach can effectively capture discriminative features from the unlabeled videos by mining the intrinsic structure within them. Our work demonstrates the effectiveness of incorporating self-supervision into visual feature learning pipelines for challenging medical image analysis tasks.,1
"We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.",0
"Introducing LEEP (Learning Efficiency Evaluation Proxy), a new methodology designed to evaluate the transferability of learned representations across different deep learning tasks. LEEP combines multiple measures into one coherent framework to provide better evaluation of learned representations than existing methods such as human accuracy metric, average precision score and model robustness test. Furthermore, our proposed methodology enables automatic analysis via the integration of metrics computation algorithms, allowing greater scalability over large datasets. We demonstrate how LEEP outperforms traditional benchmarks through experiments on popular image classification datasets such as ImageNet and CIFAR-10. Overall, LEEP offers significant advantages for evaluating representation quality in machine learning research, making it well suited for future research applications.",1
"Deep generative models have made tremendous advances in image and signal representation learning and generation. These models employ the full Euclidean space or a bounded subset as the latent space, whose flat geometry, however, is often too simplistic to meaningfully reflect the manifold structure of the data. In this work, we advocate the use of a multi-chart latent space for better data representation. Inspired by differential geometry, we propose a \textbf{Chart Auto-Encoder (CAE)} and prove a universal approximation theorem on its representation capability. We show that the training data size and the network size scale exponentially in approximation error with an exponent depending on the intrinsic dimension of the data manifold. CAE admits desirable manifold properties that auto-encoders with a flat latent space fail to obey, predominantly proximity of data. We conduct extensive experimentation with synthetic and real-life examples to demonstrate that CAE provides reconstruction with high fidelity, preserves proximity in the latent space, and generates new data remaining near the manifold. These experiments show that CAE is advantageous over existing auto-encoders and variants by preserving the topology of the data manifold as well as its geometry.",0
"This paper presents a novel method for encoding structured data using autoencoders. The proposed approach uses chart autoencoders (CAEs), which encode graphs into continuous representations that capture both the topological structure of the graph and the attributes of each node. CAEs can handle complex relationships between different entities in structured data, making them well suited for applications such as recommendation systems, fraud detection, and anomaly detection. Our experimental results demonstrate the effectiveness of our approach on several benchmark datasets. We compare our model against state-of-the-art methods and show significant improvements across all metrics. Overall, this work shows promise for the use of CAEs in unlocking new opportunities for analyzing and leveraging structured data.",1
"In this paper, we propose an easily trained yet powerful representation learning approach with performance highly competitive to deep neural networks in a digital pathology image segmentation task. The method, called sparse coding driven deep decision tree ensembles that we abbreviate as ScD2TE, provides a new perspective on representation learning. We explore the possibility of stacking several layers based on non-differentiable pairwise modules and generate a densely concatenated architecture holding the characteristics of feature map reuse and end-to-end dense learning. Under this architecture, fast convolutional sparse coding is used to extract multi-level features from the output of each layer. In this way, rich image appearance models together with more contextual information are integrated by learning a series of decision tree ensembles. The appearance and the high-level context features of all the previous layers are seamlessly combined by concatenating them to feed-forward as input, which in turn makes the outputs of subsequent layers more accurate and the whole model efficient to train. Compared with deep neural networks, our proposed ScD2TE does not require back-propagation computation and depends on less hyper-parameters. ScD2TE is able to achieve a fast end-to-end pixel-wise training in a layer-wise manner. We demonstrated the superiority of our segmentation technique by evaluating it on the multi-disease state and multi-organ dataset where consistently higher performances were obtained for comparison against several state-of-the-art deep learning methods such as convolutional neural networks (CNN), fully convolutional networks (FCN), etc.",0
"In this paper we present a new method based on deep decision trees that can accurately segment nuclear regions from digital pathology images using sparse coding techniques. We show how the use of decision tree ensembles can improve accuracy by leveraging multiple base classifiers trained on different subregions of the image. Our approach combines traditional computer vision methods such as feature extraction, selection and dimensionality reduction, with modern machine learning techniques such as decision trees and ensemble methods. To evaluate our proposed method we compare it against two state-of-the art approaches (N4ITK and Faster R-CNN) on a publicly available dataset containing breast histopathological images. Experimental results demonstrate significantly better performance compared to both baseline approaches. This study contributes a novel architecture capable of efficient, accurate and robust automatic nuclear segmentation in complex histologic images.",1
"We propose a self-supervised method to learn feature representations from videos. A standard approach in traditional self-supervised methods uses positive-negative data pairs to train with contrastive learning strategy. In such a case, different modalities of the same video are treated as positives and video clips from a different video are treated as negatives. Because the spatio-temporal information is important for video representation, we extend the negative samples by introducing intra-negative samples, which are transformed from the same anchor video by breaking temporal relations in video clips. With the proposed Inter-Intra Contrastive (IIC) framework, we can train spatio-temporal convolutional networks to learn video representations. There are many flexible options in our IIC framework and we conduct experiments by using several different configurations. Evaluations are conducted on video retrieval and video recognition tasks using the learned video representation. Our proposed IIC outperforms current state-of-the-art results by a large margin, such as 16.7% and 9.5% points improvements in top-1 accuracy on UCF101 and HMDB51 datasets for video retrieval, respectively. For video recognition, improvements can also be obtained on these two benchmark datasets. Code is available at https://github.com/BestJuly/Inter-intra-video-contrastive-learning.",0
"Here is an example:  A core challenge in video understanding lies in extracting and utilizing meaningful representations that capture both short-term dynamics within clips (intra-contrasts) and relationships across clips (inter-contrasts). Recent advancements have relied on contrastive learning by leveraging large amounts of labeled data, which is often impractical due to difficulties associated with annotation. Thus, self-supervised representation learning has gained attention as an attractive alternative that exploits temporal structure to learn powerful visual features from unlabeled videos. In this paper, we introduce ICCV, an efficient inter-intra contrastive framework enabling large-scale pretraining using only unpaired images without any form of annotations. We demonstrate significant improvements over existing methods in terms of generalization ability and robustness under domain shifts. Moreover, our approach achieves state-of-the art performance among self-supervised representation learning methods on action recognition benchmarks UCF-101/HMDB and Kinetics-400. Our method provides high efficiency and simplicity compared to competitors while establishing new quality bars in accuracy and feature transferability. We believe that our work paves the pathway towards more scalable paradigms in unlocking the potential of deep neural networks for challenging vision tasks.  This research explores an essential problem in video comprehension where effective extraction and application of meaningful representations constitutes a major obstacle. Existing approaches require substantial annotated datasets; however, obtaining these labels is typically difficult and costly. Therefore, numerous studies have turned to",1
"We propose a unified representation learning framework to address the Cross Model Compatibility (CMC) problem in the context of visual search applications. Cross compatibility between different embedding models enables the visual search systems to correctly recognize and retrieve identities without re-encoding user images, which are usually not available due to privacy concerns. While there are existing approaches to address CMC in face identification, they fail to work in a more challenging setting where the distributions of embedding models shift drastically. The proposed solution improves CMC performance by introducing a light-weight Residual Bottleneck Transformation (RBT) module and a new training scheme to optimize the embedding spaces. Extensive experiments demonstrate that our proposed solution outperforms previous approaches by a large margin for various challenging visual search scenarios of face recognition and person re-identification.",0
"In recent years, deep learning has achieved great success across many domains including computer vision, natural language processing (NLP), robotics, among others. However, due to differences in model architectures and training objectives, models developed for these different tasks often struggle to achieve interoperability. In this work, we propose a unified representation learning framework that enables cross-model compatibility by utilizing multi-task and transfer learning techniques. Our method takes advantage of diverse datasets from multiple domains to learn representations which can be applied across all the domains uniformly. By doing so, our approach allows models trained on one task to be used directly as feature extractors for another related task, leading to superior performance compared to traditional single-task methods. We demonstrate the effectiveness of our proposed approach through extensive experiments on four different benchmarks spanning three domains - image classification, sentiment analysis, question answering and reinforcement learning. Results show clear improvements over strong baseline methods across all the evaluations, thereby validating the robustness and versatility of our system.",1
"We propose PiNet, a generalised differentiable attention-based pooling mechanism for utilising graph convolution operations for graph level classification. We demonstrate high sample efficiency and superior performance over other graph neural networks in distinguishing isomorphic graph classes, as well as competitive results with state of the art methods on standard chemo-informatics datasets.",0
"This paper presents PiNet, a novel approach for graph classification using attention pooling. We introduce a new type of pooling layer that takes into account both the connectivity patterns of nodes in graphs as well as their individual features. Our method uses self-attention mechanisms to learn which node features should be aggregated for each target node, providing better localized context than traditional methods. Experimental results on five benchmark datasets show that our proposed method outperforms state-of-the-art baselines. In addition, we demonstrate the effectiveness of attention pooling by ablation studies and visualizations of learned attention weights. The code for PiNet has been made publicly available to encourage further research in this area. Overall, our work represents a significant advance in graph representation learning and opens up promising opportunities for applying machine learning techniques to complex networks beyond just social media analysis.",1
"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at https://github.com/bfshi/InfoDrop.",0
"Here is our new approach to robust representation learning based on informative dropout and shape bias:  Robustness is important in deep neural networks, as they can become overly sensitive to small variations in inputs that should be ignored. We show how to use informative dropout to add noise to the network during training, which encourages the model to learn more general features instead of just memorizing patterns in the data. This helps improve performance under real-world conditions where there may be additional background objects or occlusions. But this method faces challenges due to the high computational cost, which slows down training even at low temperature values. Our solution addresses these issues by proposing a novel ""shape-biased"" framework based on a latent space shaping loss function that regularizes feature maps according to their geometry. By aligning feature shapes across all augmentations, we encourage better representations that focus only on meaningful patterns while ignoring irrelevant details. We evaluate our method using four benchmark datasets (CIFAR-10, CelebA, ImageNet, and COCO) for image classification, object detection, instance segmentation, and semantic segmentation tasks. Experiments demonstrate that our proposed approach achieves significant improvements compared with other state-of-the-art techniques. This work provides insights into understanding the relationship between informative dropout and shape bias and suggests exciting opportunities for future research.  Keywords: Deep neural networks, robustness, informative dropout, shape bias, latent space shaping loss function, computer vision applications  (Note: If you need me to make any changes or adjustments, please let me know.)",1
"Deep neural networks often degrade significantly when training data suffer from class imbalance problems. Existing approaches, e.g., re-sampling and re-weighting, commonly address this issue by rearranging the label distribution of training data to train the networks fitting well to the implicit balanced label distribution. However, most of them hinder the representative ability of learned features due to insufficient use of intra/inter-sample information of training data. To address this issue, we propose meta feature modulator (MFM), a meta-learning framework to model the difference between the long-tailed training data and the balanced meta data from the perspective of representation learning. Concretely, we employ learnable hyper-parameters (dubbed modulation parameters) to adaptively scale and shift the intermediate features of classification networks, and the modulation parameters are optimized together with the classification network parameters guided by a small amount of balanced meta data. We further design a modulator network to guide the generation of the modulation parameters, and such a meta-learner can be readily adapted to train the classification network on other long-tailed datasets. Extensive experiments on benchmark vision datasets substantiate the superiority of our approach on long-tailed recognition tasks beyond other state-of-the-art methods.",0
"In recent years, deep learning has emerged as a powerful tool for solving complex problems in computer vision. However, traditional convolutional neural networks (CNNs) often struggle with long-tailed recognition tasks, where there are many examples of certain classes and only few examples of others. This imbalance can cause CNNs to overfit to the majority class and perform poorly on minority classes. To address this issue, we propose a meta feature modulator (MFM) that adaptively focuses attention on informative features during training. Our MFM module learns to predict how important each feature map is for different classes at each layer of the network. By using these predictions to scale up less important maps and downscale more important ones, our method effectively recalibrates the model’s attention towards the most relevant features. We evaluate our approach on several benchmark datasets and demonstrate state-of-the-art results across all metrics, including mAP and top-N recall. Overall, our work shows that by incorporating adaptive attentional mechanisms into deep learning models, we can significantly improve their performance on challenging long-tailed recognition tasks.",1
"An advanced conceptual validation framework for multimodal multivariate time series defines a multi-level contextual anomaly detection ranging from an univariate context definition, to a multimodal abstract context representation learnt by an Autoencoder from heterogeneous data (images, time series, sounds, etc.) associated to an industrial process. Each level of the framework is either applicable to historical data and/or live data. The ultimate level is based on causal discovery to identify causal relations in observational data in order to exclude biased data to train machine learning models and provide means to the domain expert to discover unknown causal relations in the underlying process represented by the data sample. A Long Short-Term Memory Autoencoder is successfully evaluated on multivariate time series to validate the learnt representation of abstract contexts associated to multiple assets of a blast furnace. A research roadmap is identified to combine causal discovery and representation learning as an enabler for unsupervised Root Cause Analysis applied to the process industry.",0
"In recent years, time series validation has become increasingly important due to advancements in sensor technology and the proliferation of IoT devices. However, traditional methods rely on subjective assumptions and manual expert analysis, leading to high computational cost and low accuracy. To address these challenges, we propose a novel framework that uses unsupervised deep learning techniques to enhance multivariance analysis and improve causality detection in multimodal datasets. Our approach combines both feature extraction and selection steps, allowing us to identify patterns in large and complex data sets from multiple modalities. We then apply statistical tests based on Granger Causality to determine the direction and strength of relationships among variables in each mode. By combining these two stages into one coherent process, our method can provide more accurate results than existing approaches while reducing computational complexity. Furthermore, through case studies on real-world industrial applications, we demonstrate how our proposed framework enables efficient validation of multimodal time series data, thereby facilitating the adoption of Industry 4.0 principles. Overall, our work represents a significant step towards achieving accurate, automated, and scalable multivariate time series validation.",1
"Temporal cues in videos provide important information for recognizing actions accurately. However, temporal-discriminative features can hardly be extracted without using an annotated large-scale video action dataset for training. This paper proposes a novel Video-based Temporal-Discriminative Learning (VTDL) framework in self-supervised manner. Without labelled data for network pretraining, temporal triplet is generated for each anchor video by using segment of the same or different time interval so as to enhance the capacity for temporal feature representation. Measuring temporal information by time derivative, Temporal Consistent Augmentation (TCA) is designed to ensure that the time derivative (in any order) of the augmented positive is invariant except for a scaling constant. Finally, temporal-discriminative features are learnt by minimizing the distance between each anchor and its augmented positive, while the distance between each anchor and its augmented negative as well as other videos saved in the memory bank is maximized to enrich the representation diversity. In the downstream action recognition task, the proposed method significantly outperforms existing related works. Surprisingly, the proposed self-supervised approach is better than fully-supervised methods on UCF101 and HMDB51 when a small-scale video dataset (with only thousands of videos) is used for pre-training. The code has been made publicly available on https://github.com/FingerRec/Self-Supervised-Temporal-Discriminative-Representation-Learning-for-Video-Action-Recognition.",0
"This research proposes a novel approach for video representation learning using self-supervised temporal discriminative learning. The proposed method leverages unlabeled videos and learns representations that capture both spatial and temporal features in a discriminative manner. In contrast to existing methods that rely on handcrafted spatio-temporal features or pretraining on large amounts of labeled data, our approach is able to learn meaningful video representations with minimal supervision. The key contribution of our work lies in two components: 1) a temporal shuffling algorithm that ensures temporal order is preserved but randomized within video clips; and 2) a temporally modulated discriminator network that learns to distinguish original from randomized versions of input videos. Experimental results demonstrate the effectiveness of our method by achieving state-of-the-art performance on several benchmark datasets. Our findings have important implications for video analysis tasks such as action recognition, video classification, and anomaly detection. Overall, we believe that this work serves as a step forward towards enabling machines to effectively process video data at scale without requiring vast amounts of annotated examples.",1
"The problems of shape classification and part segmentation from 3D point clouds have garnered increasing attention in the last few years. Both of these problems, however, suffer from relatively small training sets, creating the need for statistically efficient methods to learn 3D shape representations. In this paper, we investigate the use of Approximate Convex Decompositions (ACD) as a self-supervisory signal for label-efficient learning of point cloud representations. We show that using ACD to approximate ground truth segmentation provides excellent self-supervision for learning 3D point cloud representations that are highly effective on downstream tasks. We report improvements over the state-of-the-art for unsupervised representation learning on the ModelNet40 shape classification dataset and significant gains in few-shot part segmentation on the ShapeNetPart dataset.Code available at https://github.com/matheusgadelha/PointCloudLearningACD",0
"This paper presents a new method for learning from point cloud data that utilizes approximate convex decompositions for efficient labeling. By dividing complex shapes into simpler, more manageable pieces, our approach can accurately identify objects even in cluttered scenes. Our experiments demonstrate that our method outperforms state-of-the-art techniques across multiple benchmark datasets, achieving higher accuracy while requiring fewer labels. With its efficiency and effectiveness, our approach has significant implications for applications such as robotic manipulation, autonomous driving, and augmented reality.",1
"Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce {\em image-conditioned masked language modeling} (ICMLM) -- a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: https://europe.naverlabs.com/icmlm.",0
"This should summarize your methodology and results. Here's an example of a well-written abstract that I can use as a reference: https://arxiv.org/abs/2112.07149 ------ Based on my analysis of learning visual representations using caption annotations (LVRCA), I have discovered several significant insights into how these methods perform and their potential applications. My research approach focused on examining current state-of-the-art techniques used for LVRCA and conducting extensive experiments to evaluate their effectiveness. Through rigorous testing, we were able to identify key trends and areas requiring improvement within existing models. Our findings demonstrate the importance of considering both image quality and caption context when optimizing LVRCA performance. Additionally, our work highlighted new directions for future exploration, including expanding beyond traditional textual annotations to incorporate other modalities such as video demonstrations. Overall, our study provides valuable contributions towards advancing this rapidly developing field of artificial intelligence (AI).",1
"In this paper, we present a new algorithm for semi-supervised representation learning. In this algorithm, we first find a vector representation for the labels of the data points based on their local positions in the space. Then, we map the data to lower-dimensional space using a linear transformation such that the dependency between the transformed data and the assigned labels is maximized. In fact, we try to find a mapping that is as discriminative as possible. The approach will use Hilber-Schmidt Independence Criterion (HSIC) as the dependence measure. We also present a kernelized version of the algorithm, which allows non-linear transformations and provides more flexibility in finding the appropriate mapping. Use of unlabeled data for learning new representation is not always beneficial and there is no algorithm that can deterministically guarantee the improvement of the performance by exploiting unlabeled data. Therefore, we also propose a bound on the performance of the algorithm, which can be used to determine the effectiveness of using the unlabeled data in the algorithm. We demonstrate the ability of the algorithm in finding the transformation using both toy examples and real-world datasets.",0
"One key challenge facing representation learning techniques is how to make use of large amounts of unlabeled data to improve performance. In many cases, some labeled data is available, but generating new labels can be expensive or time consuming. We present a method for semi-supervised representation learning that makes use of probabilistically generated labels to train models with both labeled and unlabeled data. This approach allows us to effectively utilize the vast amount of unlabeled data while still leveraging the valuable supervision provided by the labeled examples. Our results demonstrate significant improvements over fully unsupervised methods as well as supervised approaches using only the limited labeled data. Additionally, we show through analysis that our model can accurately capture features from unlabeled images which are relevant for downstream tasks.",1
"We introduce BSD-GAN, a novel multi-branch and scale-disentangled training method which enables unconditional Generative Adversarial Networks (GANs) to learn image representations at multiple scales, benefiting a wide range of generation and editing tasks. The key feature of BSD-GAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network of BSD-GAN, is deliberately split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively ""de-freeze"" the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are added. A consequence of such an explicit sub-vector designation is that we can directly manipulate and even combine latent (sub-vector) codes which model different feature scales.Extensive experiments demonstrate the effectiveness of our training method in scale-disentangled learning of image representations and synthesis of novel image contents, without any extra labels and without compromising quality of the synthesized high-resolution images. We further demonstrate several image generation and manipulation applications enabled or improved by BSD-GAN. Source codes are available at https://github.com/duxingren14/BSD-GAN.",0
"Here is an example of how you could write an abstract based on that prompt:  This research introduces BSD-GAN (Branched Scale Discretization GAN), a new architecture for generative adversarial networks that enables disentanglement of scale from other attributes within generated images. Using branching and scale discretization, our approach learns a compact representation for image synthesis. Experiments demonstrate compelling results across unconditional generation, conditional generation, and cross domain generation tasks across datasets such as CelebA, AFHQ, LSUN, and more. Code and models will be made publicly available upon acceptance.  The full version of this paper can be found at arXiv:[arXiv link].  ---",1
"The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",0
"This research presents a novel approach to video representation learning that leverages memory augmentation in a dense predictive coding framework. By incorporating memories into the model architecture, we aim to capture more complex spatiotemporal patterns present in videos. Our method integrates recent advances in deep neural networks and offers a promising alternative to traditional approaches. We evaluate our model on two challenging benchmark datasets and demonstrate consistent improvement over strong baseline methods. Additionally, we provide an extensive analysis of the learned representations by examining their characteristics and interpretability, which highlights the strengths of our proposed approach. Overall, this work contributes to the growing field of video representation learning and paves the way for future developments in this area.",1
"Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks), that do not require labeled data, to learn semantic representations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words, yet predicting this $known\ $information helps in learning representations effective for downstream prediction tasks. This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks. Formally, we quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task with drastically reduced sample complexity by just training a linear layer on top of the learned representation.",0
"This paper explores the concept of self-supervised learning and presents a new method called ""provable self-supervised learning"" that leverages human knowledge to improve performance on complex tasks. The proposed approach builds upon existing work by incorporating a priori knowledge into the learning process, allowing machines to use their prior understanding of a problem to guide their decision making. Experiments conducted using popular benchmark datasets demonstrate that the proposed algorithm significantly outperforms traditional methods in several domains, including computer vision and natural language processing. Furthermore, the authors provide theoretical guarantees on the improvement achieved through provable self-supervision, establishing a solid foundation for future research in the field. Overall, this study contributes valuable insights towards enabling more effective self-supervised learning models that can learn from small amounts of data while drawing on existing knowledge to achieve better results.",1
"Graph representation learning aims to encode all nodes of a graph into low-dimensional vectors that will serve as input of many compute vision tasks. However, most existing algorithms ignore the existence of inherent data distribution and even noises. This may significantly increase the phenomenon of over-fitting and deteriorate the testing accuracy. In this paper, we propose a Distribution-induced Bidirectional Generative Adversarial Network (named DBGAN) for graph representation learning. Instead of the widely used normal distribution assumption, the prior distribution of latent representation in our DBGAN is estimated in a structure-aware way, which implicitly bridges the graph and feature spaces by prototype learning. Thus discriminative and robust representations are generated for all nodes. Furthermore, to improve their generalization ability while preserving representation ability, the sample-level and distribution-level consistency is well balanced via a bidirectional adversarial learning framework. An extensive group of experiments are then carefully designed and presented, demonstrating that our DBGAN obtains remarkably more favorable trade-off between representation and robustness, and meanwhile is dimension-efficient, over currently available alternatives in various tasks.",0
"In recent years, graph representation learning has gained significant attention due to its applications in various fields such as social network analysis, recommendation systems, and bioinformatics. One popular approach to learn meaningful representations from graphs is by using deep neural networks (DNNs). However, most DNN architectures rely on the i.i.d. assumption which may not hold true for many real-world graph data. This work proposes a novel distribution-induced bidirectional generative adversarial network (DBGAN) model that captures non-stationary patterns present in real-world graph datasets. DBGAN consists of two components: a generator and a discriminator. Both these components use temporal convolutions which allow them to capture complex spatial dependencies within each time step and across different time steps. Additionally, DBGAN incorporates a denoising autoencoder architecture into the generator component, allowing it to effectively encode noisy input graphs into low-dimensional latent spaces. Experiments on several benchmark datasets demonstrate that DBGAN outperforms state-of-the-art approaches in terms of node classification accuracy and visualization quality. Overall, our proposed model offers a powerful framework for learning high-quality graph representations that can effectively handle challenging distributions present in real-world datasets.",1
"Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough (approx. 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised disentanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.",0
"Title: ""dMelodies: A Large Scale Dataset for Disentaglement Learning"" Authors: Jake Baskin*, Alexander Mordvintsev**, Chris Olah***, Lukasz Kulikowski****, Slawek Michalski***** (* Researcher @ Google Brain ** Senior AI Designer @ Google *** Director at Microsoft Research **** Associate Professor UC Santa Barbara ***** Professor AGH University) Abstract:  In recent years, deep learning has achieved remarkable successes in various domains including computer vision, natural language processing (NLP), speech recognition, and music generation. However, these models often struggle when it comes to explaining their decisions due to the black box nature of many techniques commonly used in deep learning. To address this challenge, there has been growing interest in developing methods that enable more interpretability and explainability of deep neural networks. One such method that has gained attention recently is disentanglement learning which seeks to identify underlying factors or causes from complex data distributions by learning latent representations with distinct semantic meanings. In this work we present a new large scale dataset consisting of over one million audio clips which can be used as input into such systems. Our aim is to provide researchers and practitioners alike access to well annotated music datasets which may serve as training sets for machine learning tasks revolving around audio signal representation analysis as well as generative modelling allowing the development of systems capable of performing a wide range of musical features extraction as well as synthesis based on state of the art generative models. This should allow users to create high quality audio content quickly, easily and without requiring extensive domain knowledge. We introduce two versions of our dataset; a small subset containing only samples from the most popular genres and another full version covering 89 different categories ranging fr",1
"Self-supervised representation learning approaches have recently surpassed their supervised learning counterparts on downstream tasks like object detection and image classification. Somewhat mysteriously the recent gains in performance come from training instance classification models, treating each image and it's augmented versions as samples of a single class. In this work, we first present quantitative experiments to demystify these gains. We demonstrate that approaches like MOCO and PIRL learn occlusion-invariant representations. However, they fail to capture viewpoint and category instance invariance which are crucial components for object recognition. Second, we demonstrate that these approaches obtain further gains from access to a clean object-centric training dataset like Imagenet. Finally, we propose an approach to leverage unstructured videos to learn representations that possess higher viewpoint invariance. Our results show that the learned representations outperform MOCOv2 trained on the same data in terms of invariances encoded and the performance on downstream image classification and semantic segmentation tasks.",0
"This paper explores the key aspects that drive the success of contrastive self-supervised learning (ConvSup), one of the most popular approaches in artificial intelligence today. We demystify the three pillars behind ConvSup – invariance, augmentation, and dataset biases – and present a theoretical framework that explains how they work together to achieve state-of-the-art results across multiple domains. Our analysis sheds light on why some methods succeed while others fail, providing valuable insights into future research directions and methodologies. By clarifying these fundamental concepts, we aim to advance our understanding of unsupervised learning in general and ConvSup specifically. Overall, our study offers new perspectives for practitioners and researchers alike looking to push the boundaries of AI even further.",1
"Graph representation learning based on graph neural networks (GNNs) can greatly improve the performance of downstream tasks, such as node and graph classification. However, the general GNN models do not aggregate node information in a hierarchical manner, and can miss key higher-order structural features of many graphs. The hierarchical aggregation also enables the graph representations to be explainable. In addition, supervised graph representation learning requires labeled data, which is expensive and error-prone. To address these issues, we present an unsupervised graph representation learning method, Unsupervised Hierarchical Graph Representation (UHGR), which can generate hierarchical representations of graphs. Our method focuses on maximizing mutual information between ""local"" and high-level ""global"" representations, which enables us to learn the node embeddings and graph embeddings without any labeled data. To demonstrate the effectiveness of the proposed method, we perform the node and graph classification using the learned node and graph embeddings. The results show that the proposed method achieves comparable results to state-of-the-art supervised methods on several benchmarks. In addition, our visualization of hierarchical representations indicates that our method can capture meaningful and interpretable clusters.",0
"In recent years, graph representation learning has emerged as a powerful tool for capturing complex relationships among data points in graphs. However, most existing methods rely on supervised learning, which requires labeled training data that can be expensive and time-consuming to obtain. To address this limitation, we propose a novel unsupervised hierarchical graph representation learning method based on mutual information maximization. Our approach works by iteratively grouping nodes into clusters and optimizing their representations to capture as much information as possible from their neighborhoods. We show that our method outperforms state-of-the-art baselines across several benchmark datasets and tasks, demonstrating its effectiveness in learning meaningful and interpretable node representations without relying on annotated data. Our work provides a new direction for unsupervised graph representation learning research, opening up exciting opportunities for applications such as recommendations systems, fraud detection, and anomaly detection.",1
"Self-supervised learning has made unsupervised pretraining relevant again for difficult computer vision tasks. The most effective self-supervised methods involve prediction tasks based on features extracted from diverse views of the data. DeepInfoMax (DIM) is a self-supervised method which leverages the internal structure of deep networks to construct such views, forming prediction tasks between local features which depend on small patches in an image and global features which depend on the whole image. In this paper, we extend DIM to the video domain by leveraging similar structure in spatio-temporal networks, producing a method we call Video Deep InfoMax(VDIM). We find that drawing views from both natural-rate sequences and temporally-downsampled sequences yields results on Kinetics-pretrained action recognition tasks which match or outperform prior state-of-the-art methods that use more costly large-time-scale transformer models. We also examine the effects of data augmentation and fine-tuning methods, accomplishingSoTA by a large margin when training only on the UCF-101 dataset.",0
"Abstract: We present an algorithm for representation learning from video data using deepInfoMax, a generalization of MINE (maximum entropy model) based on predictive coding principles. This method learns to extract meaningful representations by maximizing mutual information between its own predictions and the ground truth features while minimizing the reconstruction error of input frames. Our experimental results show that our approach achieves state-of-the-art performance across multiple benchmark datasets for action recognition, object tracking, and unsupervised pretraining, outperforming competitive baselines such as DALL-E, AVA, I2C, MoCo, RSPNet, and LSRA.",1
"Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns -- without supervision -- to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.",0
"Recent advances have made it possible to model high-dimensional complex data distributions using deep neural networks (DNNs). However, directly inferring from raw data can result in overfitting due to the curse of dimensionality, while strong regularization methods such as dropout may reduce accuracy. To overcome these limitations, we propose multi-object representation learning with iterative variational inference (MORL) by extending maximum likelihood estimation to handle multiple objectives. This framework allows us to balance reconstruction fidelity against robustness and interpretability constraints, without sacrificing fit quality. Our experiments show that MORL achieves better generalization performance compared to existing DNNs trained under similar conditions, especially on challenging benchmark datasets. Furthermore, our method produces diverse valid solutions that meet different requirements, providing more value than conventional approaches that aim at one global optimum. Overall, MORL demonstrates great potential for enhancing DNN applications with more meaningful representations. Further research remains essential to unlock additional benefits offered by variational Bayesian models.",1
"Noisy labels are an unavoidable consequence of labeling processes and detecting them is an important step towards preventing performance degradations in Convolutional Neural Networks. Discarding noisy labels avoids a harmful memorization, while the associated image content can still be exploited in a semi-supervised learning (SSL) setup. Clean samples are usually identified using the small loss trick, i.e. they exhibit a low loss. However, we show that different noise distributions make the application of this trick less straightforward and propose to continuously relabel all images to reveal a discriminative loss against multiple distributions. SSL is then applied twice, once to improve the clean-noisy detection and again for training the final model. We design an experimental setup based on ImageNet32/64 for better understanding the consequences of representation learning with differing label noise distributions and find that non-uniform out-of-distribution noise better resembles real-world noise and that in most cases intermediate features are not affected by label noise corruption. Experiments in CIFAR-10/100, ImageNet32/64 and WebVision (real-world noise) demonstrate that the proposed label noise Distribution Robust Pseudo-Labeling (DRPL) approach gives substantial improvements over recent state-of-the-art. Code is available at https://git.io/JJ0PV.",0
"This paper presents a study on how different label noise distributions affect the performance of machine learning algorithms. In real-world datasets, labels can often contain errors, which can lead to decreased accuracy in predictions made by these models. By simulating varying levels of error in labels, we aimed to identify patterns in model behavior that could provide insights into improving robustness under label noise conditions. Our results show that traditional methods such as mean squared error tend to perform poorly under high levels of label noise, while alternative loss functions like quadratic loss and symmetric cross entropy better handle imprecise labels. We discuss our findings and their potential impact on advancing research in learning from noisy data.",1
"Graph neural networks (GNNs) achieve remarkable success in graph-based semi-supervised node classification, leveraging the information from neighboring nodes to improve the representation learning of target node. The success of GNNs at node classification depends on the assumption that connected nodes tend to have the same label. However, such an assumption does not always work, limiting the performance of GNNs at node classification. In this paper, we propose label-consistency based graph neural network(LC-GNN), leveraging node pairs unconnected but with the same labels to enlarge the receptive field of nodes in GNNs. Experiments on benchmark datasets demonstrate the proposed LC-GNN outperforms traditional GNNs in graph-based semi-supervised node classification.We further show the superiority of LC-GNN in sparse scenarios with only a handful of labeled nodes.",0
"Abstract: In recent years, graph neural networks have emerged as powerful tools for semi-supervised node classification tasks due to their ability to capture complex relationships within graphs. However, most existing approaches suffer from the problem of label inconsistency where nodes with the same label may have different representations. This can lead to poor performance on downstream tasks as well as limit the interpretability of the learned models. To address this issue, we propose a novel framework called LC-GNN which incorporates label consistency into the GNN training process. Our approach encourages nodes with similar labels to have consistent representation by explicitly regularizing them to align their feature representations during learning. Extensive experiments across multiple real-world datasets demonstrate that our method outperforms state-of-the art algorithms both quantitatively and qualitatively while maintaining better label consistency. Moreover, we provide detailed analysis on the effectiveness of each component in our model as well as showcasing applications such as anomaly detection and network visualization. Overall, our work highlights the importance of label consistency in graph neural networks for accurate and interpretable node classification tasks.",1
"Several multi-modality representation learning approaches such as LXMERT and ViLBERT have been proposed recently. Such approaches can achieve superior performance due to the high-level semantic information captured during large-scale multimodal pretraining. However, as ViLBERT and LXMERT adopt visual region regression and classification loss, they often suffer from domain gap and noisy label problems, based on the visual features having been pretrained on the Visual Genome dataset. To overcome these issues, we propose unbiased Contrastive Visual-Linguistic Pretraining (CVLP), which constructs a visual self-supervised loss built upon contrastive learning. We evaluate CVLP on several down-stream tasks, including VQA, GQA and NLVR2 to validate the superiority of contrastive learning on multi-modality representation learning. Our code is available at: https://github.com/ArcherYunDong/CVLP-.",0
"Here we present ""Contrastive Visual-Linguistic Pretraining,"" a novel approach that combines visual representation learning with language pretraining through contrastive learning objectives. Our model learns representations by predicting the masked tokens of both textual descriptions and images using shared learned parameters, effectively aligning their embedding spaces without explicit supervision. We evaluate our method on four diverse tasks including image classification, retrieval, question answering, and cross-modal similarity detection. Results show that incorporating vision improves over strong baselines across all benchmarks, achieving new state-of-the-art performance in three out of four settings. Further analysis demonstrates that our learned models can capture meaningful correspondences between text and images. This work paves the way towards bridging the gap between high-level symbolic reasoning and raw sensory input in artificial intelligence, opening up possibilities in areas like zero-shot problem solving, few-shot adaptation, and intelligent user interfaces.",1
"Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth of data that cannot be accumulated in a centralized repository for learning supervised models due to privacy, bandwidth limitations, and the prohibitive cost of annotations. Federated learning provides a compelling framework for learning models from decentralized data, but conventionally, it assumes the availability of labeled samples, whereas on-device data are generally either unlabeled or cannot be annotated readily through user interaction. To address these issues, we propose a self-supervised approach termed \textit{scalogram-signal correspondence learning} based on wavelet transform to learn useful representations from unlabeled sensor inputs, such as electroencephalography, blood volume pulse, accelerometer, and WiFi channel state information. Our auxiliary task requires a deep temporal neural network to determine if a given pair of a signal and its complementary viewpoint (i.e., a scalogram generated with a wavelet transform) align with each other or not through optimizing a contrastive objective. We extensively assess the quality of learned features with our multi-view strategy on diverse public datasets, achieving strong performance in all domains. We demonstrate the effectiveness of representations learned from an unlabeled input collection on downstream tasks with training a linear classifier over pretrained network, usefulness in low-data regime, transfer learning, and cross-validation. Our methodology achieves competitive performance with fully-supervised networks, and it outperforms pre-training with autoencoders in both central and federated contexts. Notably, it improves the generalization in a semi-supervised setting as it reduces the volume of labeled data required through leveraging self-supervised learning.",0
"Federated learning has proven effective at leveraging distributed data sets and devices to improve models. We extend these concepts by introducing a federation of sensors that collectively learn multi-sensor representations using self-supervision. Our system includes both vision and audio sensors, which we use as examples since they present compelling applications but generalize to other modalities like LiDAR and more exotic sensor types such as hyperspectral imaging. Using our framework we show that: (i) the learned sensor representations can enhance existing classification pipelines on benchmark datasets; (ii) we can enable new capabilities such as zero shot cross dataset transfer; (iii) the learned representation improves performance over time even without retraining; and (iv) user studies demonstrate improved task performance via augmented displays utilizing learned sensor fusion. To date there have been no deep studies into sensor federated learning, but we believe many future embedded intelligence tasks could benefit from similar techniques given their abundant availability in mobile and IoT platforms. Thus, we release our code and model checkpoint to facilitate further study.",1
"Invariance (defined in a general sense) has been one of the most effective priors for representation learning. Direct factorization of parametric models is feasible only for a small range of invariances, while regularization approaches, despite improved generality, lead to nonconvex optimization. In this work, we develop a convex representation learning algorithm for a variety of generalized invariances that can be modeled as semi-norms. Novel Euclidean embeddings are introduced for kernel representers in a semi-inner-product space, and approximation bounds are established. This allows invariant representations to be learned efficiently and effectively as confirmed in our experiments, along with accurate predictions.",0
"Abstract: Deep learning models have made tremendous strides in recent years, particularly in computer vision tasks where they outperform traditional methods by large margins. One fundamental reason behind these successes lies in deep neural networks' capacity to model complex nonlinear relationships via the hierarchical arrangement of multiple processing layers. Another important factor comes from their ability to learn data representations that capture discriminative patterns using convolutional filters and max pooling operations. Motivated by such achievements, we study the problem of representation learning under weak geometric assumptions on inputs, which generalizes previous approaches that rely heavily on strong invariance properties based on linear transformations (e.g., translation/scaling) or inner product structures. In particular, our work focuses on developing convex optimization algorithms for learning generalized invariant representations in semi-inner-product spaces. These mathematical objects naturally unify popular types of vector fields over Euclidean and Grassmannian manifolds while providing necessary flexibility to tackle more generic problems. Our method can handle diverse symmetries without restrictive prior knowledge, leading to efficient estimation procedures for various applications including image classification, object detection, and segmentation. Experiments demonstrate consistent gains against several state-of-the-art baselines across different datasets, validating the effectiveness of our framework.",1
"Protein function prediction may be framed as predicting subgraphs (with certain closure properties) of a directed acyclic graph describing the hierarchy of protein functions. Graph neural networks (GNNs), with their built-in inductive bias for relational data, are hence naturally suited for this task. However, in contrast with most GNN applications, the graph is not related to the input, but to the label space. Accordingly, we propose Tail-GNNs, neural networks which naturally compose with the output space of any neural network for multi-task prediction, to provide relationally-reinforced labels. For protein function prediction, we combine a Tail-GNN with a dilated convolutional network which learns representations of the protein sequence, making significant improvement in F_1 score and demonstrating the ability of Tail-GNNs to learn useful representations of labels and exploit them in real-world problem solving.",0
"Here is your abstract:  Hierarchical Protein Function Prediction with Tail-GNNs is a powerful new method that uses graph neural networks (GNNs) to predict protein function at multiple levels of granularity. By taking advantage of hierarchical relationships between proteins, our approach can identify functional subunits within large multiprotein complexes as well as individual residue roles that impact binding specificity or catalytic activity. Our results show consistent improvement over baseline methods across three different benchmark datasets, demonstrating both computational efficiency and biological accuracy. This work represents an important step forward towards fully automatic annotation of protein function from sequence alone, with implications for human health research.",1
"With promising results of machine learning based models in computer vision, applications on medical imaging data have been increasing exponentially. However, generalizations to complex real-world clinical data is a persistent problem. Deep learning models perform well when trained on standardized datasets from artificial settings, such as clinical trials. However, real-world data is different and translations are yielding varying results. The complexity of real-world applications in healthcare could emanate from a mixture of different data distributions across multiple device domains alongside the inevitable noise sourced from varying image resolutions, human errors, and the lack of manual gradings. In addition, healthcare applications not only suffer from the scarcity of labeled data, but also face limited access to unlabeled data due to HIPAA regulations, patient privacy, ambiguity in data ownership, and challenges in collecting data from different sources. These limitations pose additional challenges to applying deep learning algorithms in healthcare and clinical translations. In this paper, we utilize self-supervised representation learning methods, formulated effectively in transfer learning settings, to address limited data availability. Our experiments verify the importance of diverse real-world data for generalization to clinical settings. We show that by employing a self-supervised approach with transfer learning on a multi-domain real-world dataset, we can achieve 16% relative improvement on a standardized dataset over supervised baselines.",0
"In recent years, there has been growing interest in using large amounts of data from diverse sources to improve healthcare outcomes through advanced machine learning algorithms. One particular approach that has shown promise is multi-domain data applications, which involve integrating datasets from multiple domains into one model for improved generalization performance. This research paper presents several real-world case studies of multi-domain data applications used in clinical settings, focusing on their methodology and evaluation. Results demonstrate significant improvements over traditional single domain approaches in terms of accuracy and robustness across different diseases and populations. These findings have important implications for the development and deployment of machine learning models in healthcare, as they showcase the potential benefits of incorporating multiple types of data in order to enhance patient care. By highlighting these successful implementations, we aim to provide valuable insights and guidance for future work in this area.",1
"Deep generative models seek to recover the process with which the observed data was generated. They may be used to synthesize new samples or to subsequently extract representations. Successful approaches in the domain of images are driven by several core inductive biases. However, a bias to account for the compositional way in which humans structure a visual scene in terms of objects has frequently been overlooked. In this work, we investigate object compositionality as an inductive bias for Generative Adversarial Networks (GANs). We present a minimal modification of a standard generator to incorporate this inductive bias and find that it reliably learns to generate images as compositions of objects. Using this general design as a backbone, we then propose two useful extensions to incorporate dependencies among objects and background. We extensively evaluate our approach on several multi-object image datasets and highlight the merits of incorporating structure for representation learning purposes. In particular, we find that our structured GANs are better at generating multi-object images that are more faithful to the reference distribution. More so, we demonstrate how, by leveraging the structure of the learned generative process, one can `invert' the learned generative model to perform unsupervised instance segmentation. On the challenging CLEVR dataset, it is shown how our approach is able to improve over other recent purely unsupervised object-centric approaches to image generation.",0
"Investigating Object Compositionality in GANs  Generative Adversarial Networks (GANs) have revolutionized computer graphics by generating high-quality images and videos that closely resemble real objects. However, these models often struggle with understanding the underlying structure of complex scenes. For example, if you show a GAN an image of two apples next to each other, it may generate another image containing multiple apples scattered across a kitchen scene instead of combining them into one larger apple or some kind of composite object. This problem is known as the compositional generalization challenge. In this work, we aim to investigate how well GANs can learn the compositionality of common objects within their training data. To achieve this, we design a new evaluation metric called CompositiveFool, which measures the model’s ability to recognize and create compositions of different objects while considering lighting conditions, backgrounds, textures, and shapes. Using this metric on existing state-of-the-art GAN architectures such as BigGAN and Stable Diffusion, our results show significant room for improvement and highlight the importance of incorporating compositionality in future research. We hope this study inspires further exploration into developing more sophisticated generative models capable of better capturing relationships among familiar objects in the world.",1
"This paper studies semi-supervised object classification in relational data, which is a fundamental problem in relational data modeling. The problem has been extensively studied in the literature of both statistical relational learning (e.g. relational Markov networks) and graph neural networks (e.g. graph convolutional networks). Statistical relational learning methods can effectively model the dependency of object labels through conditional random fields for collective classification, whereas graph neural networks learn effective object representations for classification through end-to-end training. In this paper, we propose the Graph Markov Neural Network (GMNN) that combines the advantages of both worlds. A GMNN models the joint distribution of object labels with a conditional random field, which can be effectively trained with the variational EM algorithm. In the E-step, one graph neural network learns effective object representations for approximating the posterior distributions of object labels. In the M-step, another graph neural network is used to model the local label dependency. Experiments on object classification, link classification, and unsupervised node representation learning show that GMNN achieves state-of-the-art results.",0
"This paper introduces Graph Markov Neural Networks (GMNN), a novel deep learning architecture designed for processing sequential data on irregular graphs. Inspired by classical Markov chain theory, we extend traditional recurrent neural networks to handle graph structured inputs and outputs while preserving their ability to capture complex temporal dependencies. We achieve this through designing a new type of building block called Graph Markov Units (MU) that operates on local neighborhoods rather than individual nodes, allowing for efficient inference during training and testing. Our experimental results demonstrate significant improvement over state-of-the-art methods across three benchmark datasets, including social network analysis, natural language processing, and bioinformatics applications. These findings underscore the potential effectiveness of our approach in modeling spatio-temporal patterns on large-scale graph structures. Overall, GMNN represents a promising direction for future research in deep learning on irregularly structured data.",1
"Many learning tasks involve multi-modal data streams, where continuous data from different modes convey a comprehensive description about objects. A major challenge in this context is how to efficiently interpret multi-modal information in complex environments. This has motivated numerous studies on learning unsupervised representations from multi-modal data streams. These studies aim to understand higher-level contextual information (e.g., a Twitter message) by jointly learning embeddings for the lower-level semantic units in different modalities (e.g., text, user, and location of a Twitter message). However, these methods directly associate each low-level semantic unit with a continuous embedding vector, which results in high memory requirements. Hence, deploying and continuously learning such models in low-memory devices (e.g., mobile devices) becomes a problem. To address this problem, we present METEOR, a novel MEmory and Time Efficient Online Representation learning technique, which: (1) learns compact representations for multi-modal data by sharing parameters within semantically meaningful groups and preserves the domain-agnostic semantics; (2) can be accelerated using parallel processes to accommodate different stream rates while capturing the temporal changes of the units; and (3) can be easily extended to capture implicit/explicit external knowledge related to multi-modal data streams. We evaluate METEOR using two types of multi-modal data streams (i.e., social media streams and shopping transaction streams) to demonstrate its ability to adapt to different domains. Our results show that METEOR preserves the quality of the representations while reducing memory usage by around 80% compared to the conventional memory-intensive embeddings.",0
This paper presents METEOR (Learning Meteor and Time efficient representations from multi modal data streams) which uses meteorological information along side other sensor readings collected by mobile devices to learn memory based models representing the relationship between weather and user behavior. The model can then use these learned relations to make better recommendations on how to change ones routine given changing weather conditions in real time without sacrificing energy efficiency. The system performs both space wise operations in parallel using CUDA allowing low latency responses. Additionally we perform temporal shaping to ensure more recent interactions hold higher weights making our systems decisions less dependent on older possibly irrelevant data points. We test this approach through multiple experiments which demonstrate improvements over state of art approaches.,1
"Electronic medical record (EMR) data contains historical sequences of visits of patients, and each visit contains rich information, such as patient demographics, hospital utilisation and medical codes, including diagnosis, procedure and medication codes. Most existing EMR embedding methods capture visit-code associations by constructing input visit representations as binary vectors with a static vocabulary of medical codes. With this limited representation, they fail in encapsulating rich attribute information of visits (demographics and utilisation information) and/or codes (e.g., medical code descriptions). Furthermore, current work considers visits of the same patient as discrete-time events and ignores time gaps between them. However, the time gaps between visits depict dynamics of the patient's medical history inducing varying influences on future visits. To address these limitations, we present $\mathtt{MedGraph}$, a supervised EMR embedding method that captures two types of information: (1) the visit-code associations in an attributed bipartite graph, and (2) the temporal sequencing of visits through a point process. $\mathtt{MedGraph}$ produces Gaussian embeddings for visits and codes to model the uncertainty. We evaluate the performance of $\mathtt{MedGraph}$ through an extensive experimental study and show that $\mathtt{MedGraph}$ outperforms state-of-the-art EMR embedding methods in several medical risk prediction tasks.",0
"Med Graph is a system that represents EMRs as graphs (nodes = named entities; edges = relationships), clustering them by patient visit and representing substructures like med lists, procedures etc. This method allows EMR data to be queried more efficiently than SQL-like methods at orders of magnitude faster performance due to graph indices allowing linear time queries on many types of paths through the graph (e.g., find all conditions associated with given medications). Importantly, Med Graph can take any raw text source without any special tagging, extracting Named Entities using MetaMap (a standard tool from NLM) but learning relationships from examples provided by users querying the database, with automatic error correction so that incorrect relationships are corrected after some number of iterations of use. Relationships are stored explicitly as well as learned implicitly via inference over the graph structure (e.g., automatically discovering indirect relations such as ""is contraindicated for"" based on other relationships present in the graph). We have implemented this work within the MIMIC III database providing fast query support across a wide range of clinical questions including complex temporal sequences (""what was the date of the first order entered by Dr. Smith during his fellowship"").  We evaluate our approach on two tasks. First, we measure graph quality via precision/recall against gold standards constructed either manually or from existing databases. In addition to overall structural recall, we provide detailed breakdowns into different types of errors made during extraction (e.g., missing vs adding extra nodes vs incorrect relation labels). Secondly, we evaluate the efficiency of the system for search and retrieval of patient records. Our results demonstrate high precision/recall over multiple domains (problem list, med lists, labs, procedures, vitals) with excellent scaling properties across different sizes of data sets, outperforming previous state of the art systems by factors of up to three orders of magnitude for both index construction t",1
"With the wide adoption of mobile devices, today's location tracking systems such as satellites, cellular base stations and wireless access points are continuously producing tremendous amounts of location data of moving objects. The ability to discover moving objects that travel together, i.e., traveling companions, from their trajectories is desired by many applications such as intelligent transportation systems and location-based services. Existing algorithms are either based on pattern mining methods that define a particular pattern of traveling companions or based on representation learning methods that learn similar representations for similar trajectories. The former methods suffer from the pairwise point-matching problem and the latter often ignore the temporal proximity between trajectories. In this work, we propose a generic deep representation learning model using autoencoders, namely, ATTN-MEAN, for the discovery of traveling companions. ATTN-MEAN collectively injects spatial and temporal information into its input embeddings using skip-gram, positional encoding techniques, respectively. Besides, our model further encourages trajectories to learn from their neighbours by leveraging the Sort-Tile-Recursive algorithm, mean operation and global attention mechanism. After obtaining the representations from the encoders, we run DBSCAN to cluster the representations to find travelling companion. The corresponding trajectories in the same cluster are considered as traveling companions. Experimental results suggest that ATTN-MEAN performs better than the state-of-the-art algorithms on finding traveling companions.",0
In summary: An autoencoder can take raw data from social media profiles and convert them into a feature vector space that encodes traveler attributes; these vectors can then be used to find similarities between traveling companions and match individuals looking for someone to explore new destinations with based on compatibility measures. An analysis of user feedback highlights the importance of trustworthiness over similarity as measured by feature vector distance in determining compatibility. These insights may provide guidance to future work on designing travel companion discovery systems.,1
"In order to deal with the curse of dimensionality in reinforcement learning (RL), it is common practice to make parametric assumptions where values or policies are functions of some low dimensional feature space. This work focuses on the representation learning question: how can we learn such features? Under the assumption that the underlying (unknown) dynamics correspond to a low rank transition matrix, we show how the representation learning question is related to a particular non-linear matrix decomposition problem. Structurally, we make precise connections between these low rank MDPs and latent variable models, showing how they significantly generalize prior formulations for representation learning in RL. Algorithmically, we develop FLAMBE, which engages in exploration and representation learning for provably efficient RL in low rank transition models.",0
"This paper presents a new method called structured low rank Markov decision processes (MDP) which allows the representation of complex environments by learning over very few parameters. Using techniques from control theory we can learn and sample efficiently from these low complexity models while still achieving high performance. Additionally, we describe a novel graph structure called the flambe structure that makes efficient use of memory while retaining important connections and dependencies in order to represent highly connected systems. We evaluate our methods on challenging problems including robot arm manipulation tasks as well as gridworld examples. Our results show that structured low rank MDPs achieve comparable performance to other state-of-the art models but require significantly fewer parameters making them more scalable and easier to implement. Finally, our approach is able to capture complex environmental structures in real-time using only basic sensory inputs, demonstrating the potential applications in robotics and automated systems.",1
"The advancement of visual tracking has continuously been brought by deep learning models. Typically, supervised learning is employed to train these models with expensive labeled data. In order to reduce the workload of manual annotations and learn to track arbitrary objects, we propose an unsupervised learning method for visual tracking. The motivation of our unsupervised learning is that a robust tracker should be effective in bidirectional tracking. Specifically, the tracker is able to forward localize a target object in successive frames and backtrace to its initial position in the first frame. Based on such a motivation, in the training process, we measure the consistency between forward and backward trajectories to learn a robust tracker from scratch merely using unlabeled videos. We build our framework on a Siamese correlation filter network, and propose a multi-frame validation scheme and a cost-sensitive loss to facilitate unsupervised learning. Without bells and whistles, the proposed unsupervised tracker achieves the baseline accuracy as classic fully supervised trackers while achieving a real-time speed. Furthermore, our unsupervised framework exhibits a potential in leveraging more unlabeled or weakly labeled data to further improve the tracking accuracy.",0
"Title: ""Unsupervised Deep Representation Learning for Real-time Object Tracking""  Object tracking is a fundamental task in computer vision that involves estimating the position, scale, rotation, and appearance of an object over time. Traditional approaches rely on handcrafted features such as color histograms and HoG descriptors combined with online learning algorithms. Recent advances have shown that deep neural networks can improve performance by using end-to-end feature extraction and learning representations directly from raw pixel data. However, these methods often require large amounts of labeled training data, which is expensive and laborious to collect. In this work, we propose an unsupervised approach for real-time object tracking that uses deep convolutional neural networks (CNNs) pretrained on image classification tasks to extract visual representations without any explicit supervision for tracking. We show that our method achieves state-of-the art accuracy while running at near real-time speeds on modern hardware. Our main contributions are as follows:  * We introduce an efficient algorithm for unsupervised representation learning based on randomized backpropagation through convolutional layers. By applying dropout during testing, we show how to remove the impact of hard-to-optimize batch normalization operations commonly used in CNN models. * Our proposed tracker learns an embedding function based on randomly augmented images generated during runtime. This embedding maps high dimensional pixels to lower dimensional space where a similarity measure quantifies correspondence between target objects and search region proposals. * We validate our approach on multiple challenging benchmark datasets and demonstrate superior results compared to several recent trackers relying either on fully supervised or semi-supervised fine-tuning. Notably, our tracker even outperforms SOTA methods trained under full supervision.  In summary, we present a novel unsupervised framework for real-ti",1
"Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their efficiency in face parsing, which however overlook the correlation among different face regions. The correlation is a critical clue about the facial appearance, pose, expression etc., and should be taken into account for face parsing. To this end, we propose to model and reason the region-wise relations by learning graph representations, and leverage the edge information between regions for optimized abstraction. Specifically, we encode a facial image onto a global graph representation where a collection of pixels (""regions"") with similar features are projected to each vertex. Our model learns and reasons over relations between the regions by propagating information across vertices on the graph. Furthermore, we incorporate the edge information to aggregate the pixel-wise features onto vertices, which emphasizes on the features around edges for fine segmentation along edges. The finally learned graph representation is projected back to pixel grids for parsing. Experiments demonstrate that our model outperforms state-of-the-art methods on the widely used Helen dataset, and also exhibits the superior performance on the large-scale CelebAMask-HQ and LaPa dataset. The code is available at https://github.com/tegusi/EAGRNet.",0
"In the paper ""Edge-aware graph representation learning and reasoning for face parsing,"" we present a novel approach to face parsing using graphical models that incorporate edge awareness into their design. By doing so, our method is able to capture more subtle relationships between facial features than previous methods, resulting in improved accuracy and robustness across a variety of challenging conditions. Our key contribution lies in leveraging recent advances in deep convolutional neural networks (CNNs) and developing a probabilistic model that can reason jointly over shape and appearance information. This enables us to effectively integrate data from multiple modalities and learn representations that generalize well to unseen scenarios. We evaluate our system on several benchmark datasets and demonstrate competitive performance compared to state-of-the-art techniques. Overall, our work represents a significant step forward in automatic face parsing, paving the way for further progress in computer vision applications such as affective computing, biometrics, and human-computer interaction.",1
"This paper proposes a knowledge distillation method for foreground object search (FoS). Given a background and a rectangle specifying the foreground location and scale, FoS retrieves compatible foregrounds in a certain category for later image composition. Foregrounds within the same category can be grouped into a small number of patterns. Instances within each pattern are compatible with any query input interchangeably. These instances are referred to as interchangeable foregrounds. We first present a pipeline to build pattern-level FoS dataset containing labels of interchangeable foregrounds. We then establish a benchmark dataset for further training and testing following the pipeline. As for the proposed method, we first train a foreground encoder to learn representations of interchangeable foregrounds. We then train a query encoder to learn query-foreground compatibility following a knowledge distillation framework. It aims to transfer knowledge from interchangeable foregrounds to supervise representation learning of compatibility. The query feature representation is projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds. The proposed method outperforms the previous state-of-the-art by 10.42% in absolute difference and 24.06% in relative improvement evaluated by mean average precision (mAP). Extensive experimental results also demonstrate its efficacy from various aspects. The benchmark dataset and code will be release shortly.",0
"In recent years there has been growing interest in the development of algorithms capable of accurately identifying objects within images by leveraging machine learning techniques such as convolutional neural networks (CNNs). However, while these models have demonstrated impressive performance on various object detection benchmarks, they often suffer from two significant drawbacks: interpretability and computational complexity. On one hand, interpreting the inner workings of CNNs can be challenging due to their deep architectures and millions of parameters. This lack of transparency hinders our understanding of how these systems make decisions and can lead to concerns regarding safety-critical applications such as self-driving cars or medical diagnosis assistance. On the other hand, current state-of-the-art methods require powerful GPUs during both training and inference which makes deployment more difficult. There exists the need to develop methods that effectively balance accuracy with explainability and efficiency, enabling accurate object detection without sacrificing transparency or hardware requirements. This paper proposes a novel approach to tackling the problem of foreground object search known as Interpretable Foreground Object Search as Knowledge Distillation (IFOS-KD). Our method builds upon existing object detection frameworks based on deep learning, but adds an additional layer of knowledge distillation designed specifically to provide insight into model decision making processes. By using attention mechanisms and feature visualization techniques, we create a model that captures important features used by the detector to determine if an image contains a specific object class. Furthermore, IFOS-KD uses transfer learning strategies to significantly reduce parameter counts compared to standard object detectors, leading to increased efficiency without sacrificing accura",1
"Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at https://github.com/xiaobaishu0097/ECCV-VN.git.",0
"This paper presents a method for learning object relation graphs (ORGs) from image data using deep learning techniques. ORGs provide a compact representation of scene structure that can be used as input to navigation algorithms. Our approach first learns to predict ORGs directly from raw pixel inputs by training on large amounts of image data, and then uses those predictions to learn tentative policies for visual navigation tasks such as following paths and reaching goals. Experimental results demonstrate that our method outperforms prior work in terms of both prediction accuracy and policy performance, suggesting that ORGs may serve as an effective building block for complex vision-based robotic systems.",1
Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at \url{https://github.com/facebookresearch/Adversarial-Continual-Learning}.,0
"In recent years, there has been growing interest in developing artificial intelligence systems that can learn from experience and adapt to new situations. One approach to achieving this goal is through continual learning, which involves training machine learning models on data streams that arrive over time. However, as these models accumulate more knowledge, they become increasingly susceptible to adversarial attacks, where small perturbations to the input data can cause significant changes in the model’s behavior. This paper presents a novel framework for adversarial continual learning, which addresses these vulnerabilities by incorporating robustness against adversarial examples into the continual learning process itself. We demonstrate the effectiveness of our approach using experiments on benchmark datasets and discuss potential applications in real-world settings. Our results show that our method leads to improved performance and increased resilience to attack compared to state-of-the-art methods, opening up new possibilities for reliable deployment of artificial intelligence systems under adversarial conditions.",1
"We introduce a novel self-supervised learning approach to learn representations of videos that are responsive to changes in the motion dynamics. Our representations can be learned from data without human annotation and provide a substantial boost to the training of neural networks on small labeled data sets for tasks such as action recognition, which require to accurately distinguish the motion of objects. We promote an accurate learning of motion without human annotation by training a neural network to discriminate a video sequence from its temporally transformed versions. To learn to distinguish non-trivial motions, the design of the transformations is based on two principles: 1) To define clusters of motions based on time warps of different magnitude; 2) To ensure that the discrimination is feasible only by observing and analyzing as many image frames as possible. Thus, we introduce the following transformations: forward-backward playback, random frame skipping, and uniform frame skipping. Our experiments show that networks trained with the proposed method yield representations with improved transfer performance for action recognition on UCF101 and HMDB51.",0
"""Abstract: Recent advances in deep learning have enabled significant progress in image recognition tasks such as object detection, segmentation, and classification. However, videos present distinct challenges due to their complex temporal dynamics and high dimensionality, which has resulted in limited progress in video understanding compared to static images. In our work, we address these issues by developing a novel framework for video representation learning through recognizing temporal transformations.""",1
"Complex categorical data is often hierarchically coupled with heterogeneous relationships between attributes and attribute values and the couplings between objects. Such value-to-object couplings are heterogeneous with complementary and inconsistent interactions and distributions. Limited research exists on unlabeled categorical data representations, ignores the heterogeneous and hierarchical couplings, underestimates data characteristics and complexities, and overuses redundant information, etc. The deep representation learning of unlabeled categorical data is challenging, overseeing such value-to-object couplings, complementarity and inconsistency, and requiring large data, disentanglement, and high computational power. This work introduces a shallow but powerful UNsupervised heTerogeneous couplIng lEarning (UNTIE) approach for representing coupled categorical data by untying the interactions between couplings and revealing heterogeneous distributions embedded in each type of couplings. UNTIE is efficiently optimized w.r.t. a kernel k-means objective function for unsupervised representation learning of heterogeneous and hierarchical value-to-object couplings. Theoretical analysis shows that UNTIE can represent categorical data with maximal separability while effectively represent heterogeneous couplings and disclose their roles in categorical data. The UNTIE-learned representations make significant performance improvement against the state-of-the-art categorical representations and deep representation models on 25 categorical data sets with diversified characteristics.",0
"This paper presents a method for learning a categorical representation that captures relationships between pairs of objects from different domains based on their semantic features alone. We call our approach Unsupervised Heterogeneous Coupling Learning (UHCL) because we learn these couplings unsupervisedly without any explicit correspondences between instances or labels, using only the raw data from each source domain as input. By representing each object as a joint distribution over all possible categories, we can capture fine-grained similarity between them while still preserving structure within each domain. To evaluate UHCL on real-world datasets, we apply our model to the task of cross-domain sentiment analysis by coupling text reviews with product ratings. Our results demonstrate that UHCL significantly outperforms traditional feature fusion approaches across several metrics, providing evidence that our framework effectively captures inter-domain dependencies in high-dimensional spaces. We plan to extend UHCL beyond sentiment analysis applications towards more general tasks such as zero-shot transfer learning, where knowledge learned from one set of sources can improve performance on new problems without further supervision. Overall, our work provides insights into effective strategies for incorporating semantics in coupled models for heterogeneous data.",1
"Graph neural networks have achieved great success in learning node representations for graph tasks such as node classification and link prediction. Graph representation learning requires graph pooling to obtain graph representations from node representations. It is challenging to develop graph pooling methods due to the variable sizes and isomorphic structures of graphs. In this work, we propose to use second-order pooling as graph pooling, which naturally solves the above challenges. In addition, compared to existing graph pooling methods, second-order pooling is able to use information from all nodes and collect second-order statistics, making it more powerful. We show that direct use of second-order pooling with graph neural networks leads to practical problems. To overcome these problems, we propose two novel global graph pooling methods based on second-order pooling; namely, bilinear mapping and attentional second-order pooling. In addition, we extend attentional second-order pooling to hierarchical graph pooling for more flexible use in GNNs. We perform thorough experiments on graph classification tasks to demonstrate the effectiveness and superiority of our proposed methods. Experimental results show that our methods improve the performance significantly and consistently.",0
"Title: ""Second-Order Pooling for Graph Neural Networks""  Abstract: This work proposes a new graph pooling method that utilizes second-order statistical information from graph signals. Traditional graph neural networks (GNN) use first-order statistics to aggregate local neighborhood information into node representations, but often overlook important higher-order relationships between nodes. Our approach introduces a second-order pooling mechanism based on the Hessian matrix of the signal at each node. By combining both first and second order information we obtain more expressive node embeddings which can capture richer patterns in the underlying graph structure. We evaluate our proposed method against several baseline GNN architectures on benchmark datasets across diverse tasks including semi-supervised learning, node classification, and clustering. Results demonstrate improved performance on all three tasks, highlighting the effectiveness of incorporating high-order information in GNN models. This work paves the way towards advanced pooling techniques for more powerful GNN architectures.  Keywords: Graph Neural Networks; Higher-Order Statistics; Pooling Methods; Semi-Supervised Learning; Node Classification; Clustering",1
"The problem of catastrophic forgetting occurs in deep learning models trained on multiple databases in a sequential manner. Recently, generative replay mechanisms (GRM), have been proposed to reproduce previously learned knowledge aiming to reduce the forgetting. However, such approaches lack an appropriate inference model and therefore can not provide latent representations of data. In this paper, we propose a novel lifelong learning approach, namely the Lifelong VAEGAN (L-VAEGAN), which not only induces a powerful generative replay network but also learns meaningful latent representations, benefiting representation learning. L-VAEGAN can allow to automatically embed the information associated with different domains into several clusters in the latent space, while also capturing semantically meaningful shared latent variables, across different data domains. The proposed model supports many downstream tasks that traditional generative replay methods can not, including interpolation and inference across different data domains.",0
"In recent years, there has been increasing interest in developing machine learning algorithms that can learn from and adapt to new tasks quickly and effectively. One promising approach to achieving this goal is through the use of lifelong learning methods, which allow models to accumulate knowledge over time and apply it to novel situations.  In this paper, we propose a novel algorithm called Lifelong Vanilla Autoencoder Guided Networks (Lifelong VAEGAN) for learning latent representations across multiple datasets. Our method combines the strengths of variational autoencoders (VAEs), adversarial training, and continual learning techniques to enable efficient adaptation to new tasks. We evaluate our model on several benchmark datasets and show that it outperforms state-of-the-art baselines in terms of reconstruction accuracy and transfer learning performance.  Our results demonstrate the effectiveness of using a simple but powerful architecture combined with effective regularization techniques to achieve strong generalization ability across diverse task distributions. By leveraging both generative and discriminative objectives, Lifelong VAEGAN learns compact and meaningful latent representations that can be applied to downstream tasks without fine-tuning. Overall, this work represents an important step forward in the field of lifelong learning and has significant implications for real-world applications where adaptability and versatility are crucial.",1
"For person re-identification, existing deep networks often focus on representation learning. However, without transfer learning, the learned model is fixed as is, which is not adaptable for handling various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate person image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct query-adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and results are interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of this architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. Through direct cross-dataset evaluation, the proposed Query-Adaptive Convolution (QAConv) method gains large improvements over popular learning methods (about 10%+ mAP), and achieves comparable results to many transfer learning methods. Besides, a model-free temporal cooccurrence based score weighting method called TLift is proposed, which improves the performance to a further extent, achieving state-of-the-art results in cross-dataset person re-identification. Code is available at https://github.com/ShengcaiLiao/QAConv.",0
"This paper presents a novel approach to person re-identification using query-adaptive convolution and temporal lifting techniques. We address two key challenges faced by current methods: interpretability and generalizability. Our method utilizes query-adaptive convolutions to enhance local features at multiple scales, allowing us to better focus on specific regions of interest in each image. Additionally, we introduce temporal lifting operations that capture both short-term and long-term patterns in video sequences, improving our model’s ability to recognize individuals over time. The results demonstrate the effectiveness of these techniques, achieving state-of-the-art performance on several benchmark datasets while providing clear explanations for why certain decisions were made. Overall, our work provides insights into how interpretable models can achieve high accuracy without sacrificing generalization capabilities.",1
"Applying network science approaches to investigate the functions and anatomy of the human brain is prevalent in modern medical imaging analysis. Due to the complex network topology, for an individual brain, mining a discriminative network representation from the multimodal brain networks is non-trivial. The recent success of deep learning techniques on graph-structured data suggests a new way to model the non-linear cross-modality relationship. However, current deep brain network methods either ignore the intrinsic graph topology or require a network basis shared within a group. To address these challenges, we propose a novel end-to-end deep graph representation learning (Deep Multimodal Brain Networks - DMBN) to fuse multimodal brain networks. Specifically, we decipher the cross-modality relationship through a graph encoding and decoding process. The higher-order network mappings from brain structural networks to functional networks are learned in the node domain. The learned network representation is a set of node features that are informative to induce brain saliency maps in a supervised manner. We test our framework in both synthetic and real image data. The experimental results show the superiority of the proposed method over some other state-of-the-art deep brain network models.",0
"Here's an example: Abstract. Recent years have witnessed significant advances in deep representation learning techniques that aim at capturing latent representations by minimizing reconstruction errors on high dimensional data such as images and videos. Inspired by these successes, we propose here a novel framework called MUlti-Modal BRAin Networks (MUBRAIN) which extends existing methods to handle multimodal brain network datasets containing multiple neuroimaging modalities like fMRI, EEG/MEG, DTI etc. By taking advantage of recent developments in variational autoencoder architectures, our approach learns compact low-dimensional representations for each modality, while jointly exploring their correspondences across different imaging sources. An extensive set of experiments demonstrate significant improvements over several state-of-the-art baseline models on both unsupervised benchmark tasks and transference learning scenarios where features from one dataset generalize to others. Our findings confirm the potential benefits of developing such generic representations learned through joint multi-modal integration, opening perspectives for future work on heterogeneous population studies and enhanced clinical decision support tools. Keywords. Multi-modal; Deep; Neural networks; Autoencoder; Reconstruction error; Transference learning; fMRI; EEG/MEG; DTI.",1
"Deep neural networks have gained tremendous success in a broad range of machine learning tasks due to its remarkable capability to learn semantic-rich features from high-dimensional data. However, they often require large-scale labelled data to successfully learn such features, which significantly hinders their adaption into unsupervised learning tasks, such as anomaly detection and clustering, and limits their applications into critical domains where obtaining massive labelled data is prohibitively expensive. To enable unsupervised learning on those domains, in this work we propose to learn features without using any labelled data by training neural networks to predict data distances in a randomly projected space. Random mapping is a theoretically proven approach to obtain approximately preserved distances. To well predict these random distances, the representation learner is optimised to learn genuine class structures that are implicitly embedded in the randomly projected space. Empirical results on 19 real-world datasets show that our learned representations substantially outperform a few state-of-the-art competing methods in both anomaly detection and clustering tasks. Code is available at https://git.io/RDP",0
"Recent work has shown that unsupervised representation learning can effectively learn representations from raw data by predicting random distances. By training models on randomly generated distance metrics, they can learn meaningful representations without any explicit supervision. In this paper, we propose a new method for unsupervised representation learning using random distances. Our approach builds upon previous methods but uses a different algorithm to train models. We evaluate our model against several state-of-the-art approaches and show that our method achieves better performance across multiple datasets and tasks. Additionally, we provide an analysis of our learned representations and demonstrate their effectiveness in downstream tasks such as image classification and object detection. Overall, our results suggest that our proposed method represents a significant advancement in unsupervised representation learning and provides insight into how these systems work.",1
"Graph neural networks get significant attention for graph representation and classification in machine learning community. Attention mechanism applied on the neighborhood of a node improves the performance of graph neural networks. Typically, it helps to identify a neighbor node which plays more important role to determine the label of the node under consideration. But in real world scenarios, a particular subset of nodes together, but not the individual pairs in the subset, may be important to determine the label of the graph. To address this problem, we introduce the concept of subgraph attention for graphs. On the other hand, hierarchical graph pooling has been shown to be promising in recent literature. But due to noisy hierarchical structure of real world graphs, not all the hierarchies of a graph play equal role for graph classification. Towards this end, we propose a graph classification algorithm called SubGattPool which jointly learns the subgraph attention and employs two different types of hierarchical attention mechanisms to find the important nodes in a hierarchy and the importance of individual hierarchies in a graph. Experimental evaluation with different types of graph classification algorithms shows that SubGattPool is able to improve the state-of-the-art or remains competitive on multiple publicly available graph classification datasets. We conduct further experiments on both synthetic and real world graph datasets to justify the usefulness of different components of SubGattPool and to show its consistent performance on other downstream tasks.",0
"Graph classification is a fundamental problem in machine learning that has gained significant attention in recent years due to its wide range of applications in areas such as social network analysis, computer vision, bioinformatics, and more. In many real-world scenarios, graphs can have complex structures and interactions between nodes, making it challenging to design effective graph representation methods and classifiers. To address these limitations, we propose a novel subgraph attention framework for robust hierarchical graph classification, which can capture important patterns from large graphs efficiently. Our approach involves generating subgraphs at different scales using a pool of operators and then utilizing attention mechanisms to learn their importance for each node in relation to the target label. Furthermore, we introduce two variants: one based on GCNs and another that incorporates subgraph clustering for better scalability. Extensive experiments demonstrate that our methods significantly outperform state-of-the-art approaches across multiple benchmark datasets while providing robustness to noise and edge deletions. This work contributes new insights into hierarchical graph representations and highlights promising directions for future research in graph classification.",1
"Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.",0
"Abstract: In recent years, graph neural networks (GNNs) have emerged as powerful models for representing complex relationships between data points on graphs. However, existing GNN architectures often suffer from two limitations: they are limited in their ability to model high-order dependencies, and they struggle to scale up to large datasets due to computational complexity issues. In this work, we propose a new approach towards deeper GNNs that addresses these challenges by introducing novel building blocks capable of capturing richer representations, along with efficient training algorithms designed to handle larger datasets. Our results show significant improvement over state-of-the art methods across several benchmarks, demonstrating the potential of our proposed architecture for advancing the field of GNNs. We believe this work serves as a step forward towards building more effective deep learning models capable of handling complex graph-structured data.",1
"Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into GCNs. We first elaborate three mechanisms to incorporate self-supervision into GCNs, analyze the limitations of pretraining & finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate three novel self-supervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. Our codes are available at https://github.com/Shen-Lab/SS-GCNs.",0
"This paper investigates the impact of self-supervision on graph convolutional networks (GCN). GCN have recently gained popularity due to their ability to effectively handle graphs in various applications such as node classification, link prediction, and community detection. However, training these models can be challenging because obtaining large amounts of labeled data is often impractical or expensive. In recent years, there has been increasing interest in using self-supervised learning (SSL) techniques to improve the performance of deep neural networks. These methods rely on unlabeled data to train models that can generalize better to new datasets. However, the effectiveness of SSL on GCNs remains largely unknown. To address this gap, we conduct extensive experiments evaluating several state-of-the-art SSL algorithms applied to different architectures on three benchmark datasets commonly used in graph mining research. Our results show that while some SSL approaches consistently boost GCNs accuracy across different settings, others may lead to poorer performance than using conventional supervised learning alone. We provide insights into why certain strategies perform well, whereas others fail to yield positive gains, discussing the tradeoffs involved in designing effective self-supervised GCN systems. Overall, our study provides valuable guidance to practitioners wishing to apply SSL within GCNs, helping them make informed choices based on their specific needs. By shedding light on this understudied area, we hope to advance future work seeking to enhance the capabilities of GCNs through self-supervision.",1
"This paper studies the fundamental problem of learning deep generative models that consist of multiple layers of latent variables organized in top-down architectures. Such models have high expressivity and allow for learning hierarchical representations. Learning such a generative model requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference typically requires Markov chain Monte Caro (MCMC) that can be time consuming. In this paper, we propose to use noise initialized non-persistent short run MCMC, such as finite step Langevin dynamics initialized from the prior distribution of the latent variables, as an approximate inference engine, where the step size of the Langevin dynamics is variationally optimized by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Our experiments show that the proposed method outperforms variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. The advantage of the proposed method is that it is simple and automatic without the need to design an inference model.",0
"This paper presents a methodology called learning multi-level latent variable models (LVMM) using variational optimization methods combined with short run Markov Chain Monte Carlo (MCMC). The proposed framework overcomes limitations found in existing LVMMs by enabling efficient approximate inference that balances flexibility against parsimony. Our approach relies on variational approximations that are optimized using gradient ascent based on stochastic gradient estimates obtained from short runs of MCMC chains, which makes our method scalable to large datasets while preserving accuracy. Experiments demonstrate that the new model effectively captures dependencies among variables at different levels of abstraction, leading to state-of-the-art predictive performance and interpretability.",1
"Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR$^2$. Code is available at https://github.com/ChenRocks/UNITER.",0
"This paper presents an image representation learning method that can handle text data as well as images by leveraging their common structure - the semantic graph. We first introduce a Universal Semantic Graph (USG) that models both images and texts, then propose a neural network architecture that maps each input modality into the USG space. Our experiments show substantial improvements over current state-of-the art methods on several benchmark datasets, including SVHN, CIFAR-10, NORB, SUN RGB-D, LSUN, and COCO. Furthermore, we demonstrate that our model can generate high quality images using the generated features from the decoder network based on the USG embeddings. In conclusion, UNITER achieves promising results across different tasks, suggesting great potential for future research into image-text integration.",1
"Building in silico models to predict chemical properties and activities is a crucial step in drug discovery. However, limited labeled data often hinders the application of deep learning in this setting. Meanwhile advances in meta-learning have enabled state-of-the-art performances in few-shot learning benchmarks, naturally prompting the question: Can meta-learning improve deep learning performance in low-resource drug discovery projects? In this work, we assess the transferability of graph neural networks initializations learned by the Model-Agnostic Meta-Learning (MAML) algorithm - and its variants FO-MAML and ANIL - for chemical properties and activities tasks. Using the ChEMBL20 dataset to emulate low-resource settings, our benchmark shows that meta-initializations perform comparably to or outperform multi-task pre-training baselines on 16 out of 20 in-distribution tasks and on all out-of-distribution tasks, providing an average improvement in AUPRC of 11.2% and 26.9% respectively. Finally, we observe that meta-initializations consistently result in the best performing models across fine-tuning sets with $k \in \{16, 32, 64, 128, 256\}$ instances.",0
"This paper presents an approach to meta-learning graph neural network (GNN) initializations for low-resource molecular property prediction tasks. In situations where there may be limited data available for training a GNN model on specific target properties of interest, traditional methods of initializing GNNs from scratch can lead to suboptimal performance. To address this challenge, we propose using previously trained models as initialization points for meta-training. By utilizing pretrained models initialized with random weights, our method allows the model to adapt to new datasets more efficiently and accurately than starting from scratch. Through extensive experiments across multiple benchmark datasets, we show that our approach significantly improves generalization performance over competing baselines. Our results suggest that meta-learning GNN initializations have great potential for enabling powerful and efficient learning under data scarcity constraints.",1
"Several animal species (e.g., bats, dolphins, and whales) and even visually impaired humans have the remarkable ability to perform echolocation: a biological sonar used to perceive spatial layout and locate objects in the world. We explore the spatial cues contained in echoes and how they can benefit vision tasks that require spatial reasoning. First we capture echo responses in photo-realistic 3D indoor scene environments. Then we propose a novel interaction-based representation learning framework that learns useful visual features via echolocation. We show that the learned image features are useful for multiple downstream vision tasks requiring spatial reasoning---monocular depth estimation, surface normal estimation, and visual navigation---with results comparable or even better than heavily supervised pre-training. Our work opens a new path for representation learning for embodied agents, where supervision comes from interacting with the physical world.",0
"Echolocation has been used by animals such as bats and dolphins for millions of years to navigate their environments by producing sounds and interpreting the echoes they receive. Inspired by these natural abilities, we propose a novel approach to visual representation learning called VisualEchoes. Our method utilizes a virtual acoustic environment similar to that found in nature, where an agent learns to ""see"" through echolocation by generating sound waves and processing the resulting echoes. By doing so, our agent can learn representations of objects without any visual input, relying solely on auditory cues. We evaluate our approach using several benchmark datasets and demonstrate its effectiveness compared to other methods of unsupervised learning. Furthermore, we explore the emergence of spatial relationships within the learned representations, highlighting how echolocation provides an intuitive mechanism for understanding 2D and 3D space. Overall, VisualEchoes represents a significant advancement in the field of representation learning and offers new insights into human-AI collaboration.",1
"Contrastive unsupervised representation learning (CURL) is the state-of-the-art technique to learn representations (as a set of features) from unlabelled data. While CURL has collected several empirical successes recently, theoretical understanding of its performance was still missing. In a recent work, Arora et al. (2019) provide the first generalisation bounds for CURL, relying on a Rademacher complexity. We extend their framework to the flexible PAC-Bayes setting, allowing us to deal with the non-iid setting. We present PAC-Bayesian generalisation bounds for CURL, which are then used to derive a new representation learning algorithm. Numerical experiments on real-life datasets illustrate that our algorithm achieves competitive accuracy, and yields non-vacuous generalisation bounds.",0
"In recent years, unsupervised representation learning has emerged as a promising approach to learning representations without explicit supervision. One popular method within this area is contrastive learning, which involves maximizing the similarity between positive pairs (e.g., two augmentations of the same image) while minimizing it for negative pairs (e.g., two different images). However, traditional contrastive learning methods rely on handcrafted positives/negatives pairs or predefined anchors, which may limit their performance.  To address these limitations, we propose PAC-Bayesian Contrastive Unsupervised Representation Learning (PBCURL), a framework that leverages probability analysis for controlling the trade-off between sample complexity and model capacity in representation learning. Our approach combines state-of-the-art Bayesian optimization techniques with the powerful contrastive learning framework based on Probabilistic Approximate Counting (PAC). This allows us to learn high-quality representations using a wide range of data augmentation strategies, including both randomized augmentation policies and human-designed ones.  In our experiments, we demonstrate that PBCURL achieves superior results compared to several competitive baselines across multiple benchmark datasets and tasks, such as image classification, object detection, and generative image synthesis. Furthermore, we provide comprehensive analyses of key components and hyperparameters of our framework to shed light on its behavior and generality. Overall, our work suggests that PAC-based approaches hold promise for efficient, scalable, and effective self-supervised learning algorithms beyond just contrastive learning.",1
"In architecture and computer-aided design, wireframes (i.e., line-based models) are widely used as basic 3D models for design evaluation and fast design iterations. However, unlike a full design file, a wireframe model lacks critical information, such as detailed shape, texture, and materials, needed by a conventional renderer to produce 2D renderings of the objects or scenes. In this paper, we bridge the information gap by generating photo-realistic rendering of indoor scenes from wireframe models in an image translation framework. While existing image synthesis methods can generate visually pleasing images for common objects such as faces and birds, these methods do not explicitly model and preserve essential structural constraints in a wireframe model, such as junctions, parallel lines, and planar surfaces. To this end, we propose a novel model based on a structure-appearance joint representation learned from both images and wireframes. In our model, structural constraints are explicitly enforced by learning a joint representation in a shared encoder network that must support the generation of both images and wireframes. Experiments on a wireframe-scene dataset show that our wireframe-to-image translation model significantly outperforms the state-of-the-art methods in both visual quality and structural integrity of generated images.",0
"In recent years, there has been significant interest in using deep learning techniques to generate realistic images from textual descriptions (text-to-image synthesis) or other high-level inputs such as natural language commands (NLG). However, most previous work on image generation has focused on producing full color images with fine detail rather than wireframes which represent an object or scene at a higher level but can still convey important details and concepts. This paper presents a new neural architecture that we call the ""neural wireframe renderer"" which takes input in the form of a set of 2D bounding boxes representing a 3D object and produces a grayscale wireframe image of the object as output. Our network uses a combination of convolutional and recurrent layers to learn complex translation functions between the input and output representations. We demonstrate state-of-the-art performance across multiple datasets including ShapeNetCoreV2 and our own large-scale 3dWarehouse dataset while outperforming existing methods in accuracy and speed. Additionally, we evaluate the quality of generated wireframes through user studies, demonstrating their effectiveness in conveying key features of objects to human observers. We believe our approach represents a promising direction for future research in NLG tasks where simplicity and readability are more desirable over photo-realism. While many potential applications exist outside of computer vision for wireframe rendering including architectural design and web development, further work remains in bridging the gap between these different domains by providing intuitive tools for end users to create wireframe sketches and translating them into interactive 3D models. Overall, we believe this work serves as",1
"Recently, there has been an increasing interest in (supervised) learning with graph data, especially using graph neural networks. However, the development of meaningful benchmark datasets and standardized evaluation procedures is lagging, consequently hindering advancements in this area. To address this, we introduce the TUDataset for graph classification and regression. The collection consists of over 120 datasets of varying sizes from a wide range of applications. We provide Python-based data loaders, kernel and graph neural network baseline implementations, and evaluation tools. Here, we give an overview of the datasets, standardized evaluation procedures, and provide baseline experiments. All datasets are available at www.graphlearning.io. The experiments are fully reproducible from the code available at www.github.com/chrsmrrs/tudataset.",0
"Graphs have become essential data structures used by machine learning models in many domains such as natural language processing (NLP), computer vision, chemistry, biology, neuroscience, recommender systems, among others. With their widespread use comes the need for large and diverse datasets that can provide challenging evaluation tasks to train, validate and compare graph-based model performance. In this work we introduce TUDataset - a set of over 20 benchmark datasets ranging from molecular property prediction to social network analysis to semantic image segmentation on graphs. We analyze these datasets regarding their size, density of edges, clustering coefficient, degree distribution and other properties. All our datasets are open source and can easily be obtained online through popular archives like GitHub repositories. We showcase examples of how state-of-the-art models from different fields benefit from using our proposed dataset collections for improved generalization ability across multiple domains. Our findings suggest that there exists no single ""best"" dataset but rather a small selection of them provides better results than most individual datasets alone.",1
"When learning to sketch, beginners start with simple and flexible shapes, and then gradually strive for more complex and accurate ones in the subsequent training sessions. In this paper, we design a ""shape curriculum"" for learning continuous Signed Distance Function (SDF) on shapes, namely Curriculum DeepSDF. Inspired by how humans learn, Curriculum DeepSDF organizes the learning task in ascending order of difficulty according to the following two criteria: surface accuracy and sample difficulty. The former considers stringency in supervising with ground truth, while the latter regards the weights of hard training samples near complex geometry and fine structure. More specifically, Curriculum DeepSDF learns to reconstruct coarse shapes at first, and then gradually increases the accuracy and focuses more on complex local details. Experimental results show that a carefully-designed curriculum leads to significantly better shape reconstructions with the same training data, training epochs and network architecture as DeepSDF. We believe that the application of shape curricula can benefit the training process of a wide variety of 3D shape representation learning methods.",0
"Recent years have seen tremendous advances in computer vision techniques, which allow machines to interpret complex visual data with high accuracy. Among these methods, dense voxel representation has emerged as one of the most promising approaches due to its ability to represent scenes at scale and achieve state-of-the-art performance on multiple benchmarks. However, despite their success, existing deep learning models often struggle to generalize beyond their training data, leading to poor performance when faced with novel scenarios. This shortcoming limits the applicability of current systems and motivates the need for improved domain adaptation techniques that can better adapt to new domains while maintaining good performance across all tasks. In response to this challenge, we introduce ""Curriculum DeepSDF,"" a method that utilizes task curricula to gradually improve model robustness by exposing it to increasingly difficult examples from diverse environments during training. We demonstrate through extensive experiments that our proposed approach leads to significant improvements over prior work in terms of both numerical metrics and qualitative analysis. These results highlight the effectiveness of task-specific curricula in enhancing scene understanding and generalization capabilities, paving the way for more versatile machine perception solutions.",1
"In this work, we propose a new unsupervised image segmentation approach based on mutual information maximization between different constructed views of the inputs. Taking inspiration from autoregressive generative models that predict the current pixel from past pixels in a raster-scan ordering created with masked convolutions, we propose to use different orderings over the inputs using various forms of masked convolutions to construct different views of the data. For a given input, the model produces a pair of predictions with two valid orderings, and is then trained to maximize the mutual information between the two outputs. These outputs can either be low-dimensional features for representation learning or output clusters corresponding to semantic labels for clustering. While masked convolutions are used during training, in inference, no masking is applied and we fall back to the standard convolution where the model has access to the full input. The proposed method outperforms current state-of-the-art on unsupervised image segmentation. It is simple and easy to implement, and can be extended to other visual tasks and integrated seamlessly into existing unsupervised learning methods requiring different views of the data.",0
"This paper presents a novel method for unsupervised image segmentation using an autoregressive model. Our approach leverages recent advances in density estimation and deep learning to efficiently predict dense output masks without any ground truth labels. We demonstrate that our method outperforms existing state-of-the-art techniques on several benchmark datasets while requiring significantly less computational resources. In addition, we provide a detailed analysis of the factors influencing performance and showcase applications in challenging real-world scenarios. Our work represents a significant step towards fully automated image segmentation, paving the way for new AI-driven solutions across a range of fields including computer vision, robotics, and medical imaging.",1
"Determining the traffic scenario space is a major challenge for the homologation and coverage assessment of automated driving functions. In contrast to current approaches that are mainly scenario-based and rely on expert knowledge, we introduce two data driven autoencoding models that learn a latent representation of traffic scenes. First is a CNN based spatio-temporal model that autoencodes a grid of traffic participants' positions. Secondly, we develop a pure temporal RNN based model that auto-encodes a sequence of sets. To handle the unordered set data, we had to incorporate the permutation invariance property. Finally, we show how the latent scenario embeddings can be used for clustering traffic scenarios and similarity retrieval.",0
"This paper presents a novel approach to traffic scene representation and clustering using deep learning techniques. We first use convolutional neural networks (CNNs) to extract features from raw image data, capturing important patterns and characteristics of different traffic scenarios such as pedestrians, vehicles, road signs, and intersections. Our method involves training multiple CNN models on small sub-regions within the image, enabling more detailed representations compared to traditional single-shot methods. Next, we employ unsupervised machine learning algorithms like k-means clustering to group similar scenes together based on their learned feature representations. Experimental results show that our proposed method outperforms state-of-the-art approaches in terms of accuracy and computational efficiency. Additionally, we visualize the learned features through t-SNE dimensionality reduction and observe clear separation between clusters corresponding to distinct traffic situations. Overall, our work advances the field of computer vision by demonstrating the effectiveness of deep representation learning and clustering for analyzing complex urban environments.",1
"Recently there was an increasing interest in applications of graph neural networks in non-Euclidean geometry; however, are non-Euclidean representations always useful for graph learning tasks? For different problems such as node classification and link prediction we compute hyperbolic embeddings and conclude that for tasks that require global prediction consistency it might be useful to use non-Euclidean embeddings, while for other tasks Euclidean models are superior. To do so we first fix an issue of the existing models associated with the optimization process at zero curvature. Current hyperbolic models deal with gradients at the origin in ad-hoc manner, which is inefficient and can lead to numerical instabilities. We solve the instabilities of kappa-Stereographic model at zero curvature cases and evaluate the approach of embedding graphs into the manifold in several graph representation learning tasks.",0
"In many fields, graphs are essential tools for understanding complex relationships among entities. However, finding meaningful patterns can be challenging due to large amounts of data. One solution is to use hyperbolic embeddings of graphs that preserve certain properties, but these methods may introduce distortions. This work examines whether different hyperbolic representations lead to similar insights by comparing two popular embedding techniques: Poincaré ball and Klein disk models. We evaluate their performance on several graph datasets using established metrics such as precision, recall, F1 score, and KL divergence. Our results show inconsistent differences between the embeddings across datasets and measures. Therefore, we conclude that there exists no clear winner among hyperbolic representations; choice of model should depend on specific research questions and goals. Future research directions could explore ways to optimize each method for particular applications or develop hybrid approaches. By acknowledging the diversity of hyperbolic representations, practitioners can make informed decisions while leveraging the advantages of non-Euclidean geometry for visualizing networks.",1
"Dictionary learning is a classic representation learning method that has been widely applied in signal processing and data analytics. In this paper, we investigate a family of $\ell_p$-norm ($p2,p \in \mathbb{N}$) maximization approaches for the complete dictionary learning problem from theoretical and algorithmic aspects. Specifically, we prove that the global maximizers of these formulations are very close to the true dictionary with high probability, even when Gaussian noise is present. Based on the generalized power method (GPM), an efficient algorithm is then developed for the $\ell_p$-based formulations. We further show the efficacy of the developed algorithm: for the population GPM algorithm over the sphere constraint, it first quickly enters the neighborhood of a global maximizer, and then converges linearly in this region. Extensive experiments will demonstrate that the $\ell_p$-based approaches enjoy a higher computational efficiency and better robustness than conventional approaches and $p=3$ performs the best.",0
"Abstract: This paper presents a novel method for complete dictionary learning using the $\ell_p$-norm maximization technique. In many applications, it is important to obtain a sparse representation of signals using a known basis or dictionary. However, traditional methods of dictionary learning often result in suboptimal solutions that are computationally expensive and require careful tuning of parameters. By contrast, our approach uses a simple yet powerful optimization procedure that leads to exact recovery of the original signals from their linear combinations under mild conditions on the sparsity level and noise content. Our numerical experiments demonstrate the effectiveness of our approach on real and synthetic data sets, outperforming state-of-the-art alternatives in terms of accuracy and speed. This work has important implications for applications such as image compression, feature extraction, and signal denoising.",1
"Motivated by the previous success of Two-Dimensional Convolutional Neural Network (2D CNN) on image recognition, researchers endeavor to leverage it to characterize videos. However, one limitation of applying 2D CNN to analyze videos is that different frames of a video share the same 2D CNN kernels, which may result in repeated and redundant information utilization, especially in the spatial semantics extraction process, hence neglecting the critical variations among frames. In this paper, we attempt to tackle this issue through two ways. 1) Design a sequential channel filtering mechanism, i.e., Progressive Enhancement Module (PEM), to excite the discriminative channels of features from different frames step by step, and thus avoid repeated information extraction. 2) Create a Temporal Diversity Loss (TD Loss) to force the kernels to concentrate on and capture the variations among frames rather than the image regions with similar appearance. Our method is evaluated on benchmark temporal reasoning datasets Something-Something V1 and V2, and it achieves visible improvements over the best competitor by 2.4% and 1.3%, respectively. Besides, performance improvements over the 2D-CNN-based state-of-the-arts on the large-scale dataset Kinetics are also witnessed.",0
"""Temporal distinct representation learning (TDL) is a new method that improves action recognition by representing temporally distant features separately from adjacent ones during neural network training. In our study, we aimed to investigate the effectiveness of TDL on different types of convolutional architectures in terms of accuracy, efficiency and robustness. Our results showed that using TDL improved overall performance across all models tested, including significant gains in accuracy and computational time.""",1
"Medical images are naturally associated with rich semantics about the human anatomy, reflected in an abundance of recurring anatomical patterns, offering unique potential to foster deep semantic representation learning and yield semantically more powerful models for different medical applications. But how exactly such strong yet free semantics embedded in medical images can be harnessed for self-supervised learning remains largely unexplored. To this end, we train deep models to learn semantically enriched visual representation by self-discovery, self-classification, and self-restoration of the anatomy underneath medical images, resulting in a semantics-enriched, general-purpose, pre-trained 3D model, named Semantic Genesis. We examine our Semantic Genesis with all the publicly-available pre-trained models, by either self-supervision or fully supervision, on the six distinct target tasks, covering both classification and segmentation in various medical modalities (i.e.,CT, MRI, and X-ray). Our extensive experiments demonstrate that Semantic Genesis significantly exceeds all of its 3D counterparts as well as the de facto ImageNet-based transfer learning in 2D. This performance is attributed to our novel self-supervised learning framework, encouraging deep models to learn compelling semantic representation from abundant anatomical patterns resulting from consistent anatomies embedded in medical images. Code and pre-trained Semantic Genesis are available at https://github.com/JLiangLab/SemanticGenesis .",0
"In natural language processing (NLP), semantic representations play a crucial role in understanding text and enabling advanced applications like machine translation, question answering, and sentiment analysis. Traditional approaches to building semantic representations have relied on manual annotation and feature engineering, which can be time-consuming, expensive, and prone to error. This paper presents a new approach that leverages self-discovery, self-classification, and self-restoration techniques to automatically learn semantically rich and coherent sentence representations. These methods allow the model to identify contextual relationships among sentences and capture meaningful patterns in data, resulting in improved performance across NLP tasks. Our experimental evaluation shows that the proposed approach outperforms several state-of-the-art baselines and achieves promising results in multiple domains, demonstrating the potential benefits of using unsupervised learning techniques for representation learning in NLP. Overall, our work offers a valuable contribution to the field by providing a scalable and effective solution for creating high-quality semantic representations from raw text without requiring extensive human supervision.",1
"Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.",0
"""Deep learning methods have been proven to be successful in image classification tasks due to their ability to learn features that capture complex patterns from large amounts of data. However, deep neural networks often struggle to generalize well on small datasets or out-of-distribution examples. In order to address these issues, we propose a novel method called Deep Graph Contrastive Representation Learning (DGCRL).  Our approach uses contrastive learning, which involves training two neural network branches, one that generates positive pairs and another that produces negative ones. These branches are trained to maximize agreement between positive pairs while minimizing similarity between negatives. We use graph convolutional layers as our backbone architecture since they can capture meaningful representations even for irregularly sampled signals like graphs. Our contributions are threefold: First, we introduce a new loss function called the Graph Similarity Loss (GSL) that captures both local and global structural information of graphs by comparing them based on their node attributes. Second, we show how self-supervised representation learning using DGCRL leads to state-of-the-art performance on several benchmark graph dataset splits. Finally, we demonstrate the robustness of learned representations through extensive experiments where models trained only on limited labeled samples achieve superior accuracy compared to fully supervised baselines.""",1
"Time series prediction is an important problem in machine learning. Previous methods for time series prediction did not involve additional information. With a lot of dynamic knowledge graphs available, we can use this additional information to predict the time series better. Recently, there has been a focus on the application of deep representation learning on dynamic graphs. These methods predict the structure of the graph by reasoning over the interactions in the graph at previous time steps. In this paper, we propose a new framework to incorporate the information from dynamic knowledge graphs for time series prediction. We show that if the information contained in the graph and the time series data are closely related, then this inter-dependence can be used to predict the time series with improved accuracy. Our framework, DArtNet, learns a static embedding for every node in the graph as well as a dynamic embedding which is dependent on the dynamic attribute value (time-series). Then it captures the information from the neighborhood by taking a relation specific mean and encodes the history information using RNN. We jointly train the model link prediction and attribute prediction. We evaluate our method on five specially curated datasets for this problem and show a consistent improvement in time series prediction results. We release the data and code of model DArtNet for future research at https://github.com/INK-USC/DArtNet .",0
"This paper presents a novel approach to predicting temporal attributes using joint modeling of multi-relational structure evolution. We develop a framework that models the dynamics of interdependent relations over time by integrating temporal link prediction into existing relational representation learning methods. Our method leverages recent advances in machine learning and natural language processing techniques to capture complex spatio-temporal dependencies across multiple relations.  In order to evaluate the effectiveness of our approach, we conduct experiments on two real-world datasets covering different domains, including human relationships and semantic text data. Results show that our proposed algorithm outperforms state-of-the-art baselines in terms of accuracy and efficiency, demonstrating its ability to effectively model evolving multi-relations and accurately predict future attribute values. Furthermore, sensitivity analysis confirms the importance of incorporating both current and historical information from all involved relations in making accurate predictions. Overall, our work represents a significant contribution towards enhancing temporal reasoning capabilities in artificial intelligence systems.",1
"Despite their strong modeling capacities, Convolutional Neural Networks (CNNs) are often scale-sensitive. For enhancing the robustness of CNNs to scale variance, multi-scale feature fusion from different layers or filters attracts great attention among existing solutions, while the more granular kernel space is overlooked. We bridge this regret by exploiting multi-scale features in a finer granularity. The proposed convolution operation, named Poly-Scale Convolution (PSConv), mixes up a spectrum of dilation rates and tactfully allocate them in the individual convolutional kernels of each filter regarding a single convolutional layer. Specifically, dilation rates vary cyclically along the axes of input and output channels of the filters, aggregating features over a wide range of scales in a neat style. PSConv could be a drop-in replacement of the vanilla convolution in many prevailing CNN backbones, allowing better representation learning without introducing additional parameters and computational complexities. Comprehensive experiments on the ImageNet and MS COCO benchmarks validate the superior performance of PSConv. Code and models are available at https://github.com/d-li14/PSConv.",0
"PSConv proposes a new design principle for neural network architectures that can improve model efficiency while minimizing performance loss. This method involves transforming traditional feature pyramids into one compact poly-scale convolutional layer by fusing multiple feature scales together using depthwise separable convolutions. The proposed PSConv approach eliminates redundant computation across different resolutions, allowing for faster inference speeds without compromising accuracy. Experimental results show consistent improvement on image classification tasks over previous state-of-the-art methods. Overall, this research presents a promising technique for enhancing deep learning models, particularly for deployment in resource-constrained environments such as mobile devices.",1
"We present a multi-relational temporal Knowledge Graph based on the daily interactions between artifacts in GitHub, one of the largest social coding platforms. Such representation enables posing many user-activity and project management questions as link prediction and time queries over the knowledge graph. In particular, we introduce two new datasets for i) interpolated time-conditioned link prediction and ii) extrapolated time-conditioned link/time prediction queries, each with distinguished properties. Our experiments on these datasets highlight the potential of adapting knowledge graphs to answer broad software engineering questions. Meanwhile, it also reveals the unsatisfactory performance of existing temporal models on extrapolated queries and time prediction queries in general. To overcome these shortcomings, we introduce an extension to current temporal models using relative temporal information with regards to past events.",0
"In this paper we propose a novel approach for modeling software engineering events within temporal knowledge graphs (TKG). Our proposed method leverages relative time as a key component in identifying and connecting related events that occur throughout the lifecycle of a software project. We argue that traditional approaches which rely on absolute timestamps can lead to incomplete and inconsistent event representations due to differences in clock skew and ambiguity around event boundaries. By incorporating relative time into our TKG model, we provide a more accurate representation of software engineering events allowing for better reasoning and inference capabilities. Our work provides empirical validation of our approach through case studies and experiments, demonstrating significant improvements over existing methods. This has important implications for applications such as anomaly detection and prediction, automated documentation generation, and intelligent decision support systems in software development processes. Overall, our contributions highlight the importance of considering contextual information such as time relativity for effective modeling of complex real world phenomena like software engineering.",1
"Multi-label classification is the challenging task of predicting the presence and absence of multiple targets, involving representation learning and label correlation modeling. We propose a novel framework for multi-label classification, Multivariate Probit Variational AutoEncoder (MPVAE), that effectively learns latent embedding spaces as well as label correlations. MPVAE learns and aligns two probabilistic embedding spaces for labels and features respectively. The decoder of MPVAE takes in the samples from the embedding spaces and models the joint distribution of output targets under a Multivariate Probit model by learning a shared covariance matrix. We show that MPVAE outperforms the existing state-of-the-art methods on a variety of application domains, using public real-world datasets. MPVAE is further shown to remain robust under noisy settings. Lastly, we demonstrate the interpretability of the learned covariance by a case study on a bird observation dataset.",0
"Title: Unsupervised Disentanglement of Generative Models for Improved Multi-label Classifiction --------------------------------------------------  Unsupervised disentanglement has recently emerged as an effective approach for generative models to capture interpretable representations that encode semantic concepts and their interactions. In particular, variational autoencoders (VAEs) have shown promise in generating meaningful latent spaces by incorporating regularization constraints such as $\beta$-VAE and InfoGAN. However, multi-label classification using these disentangled representations remains understudied, mainly due to challenges arising from the complex relationships among multiple labels and their associated feature representations. This work addresses these difficulties through two key contributions. Firstly, we propose a novel VAE architecture that learns covariance-aware multivariate probit models, enabling more robust learning of nonlinear label dependencies. Secondly, our method enhances standard VAE training objectives with adversarial losses to achieve better disentanglement quality while encouraging generalizable feature learning across different architectures. Our experiments on several benchmark datasets demonstrate improved performance over existing methods, providing evidence of the potential benefits gained from exploiting unsupervised disentanglement principles for multi-label classification tasks.",1
"We consider the problem of semi-supervised 3D action recognition which has been rarely explored before. Its major challenge lies in how to effectively learn motion representations from unlabeled data. Self-supervised learning (SSL) has been proved very effective at learning representations from unlabeled data in the image domain. However, few effective self-supervised approaches exist for 3D action recognition, and directly applying SSL for semi-supervised learning suffers from misalignment of representations learned from SSL and supervised learning tasks. To address these issues, we present Adversarial Self-Supervised Learning (ASSL), a novel framework that tightly couples SSL and the semi-supervised scheme via neighbor relation exploration and adversarial learning. Specifically, we design an effective SSL scheme to improve the discrimination capability of learned representations for 3D action recognition, through exploring the data relations within a neighborhood. We further propose an adversarial regularization to align the feature distributions of labeled and unlabeled samples. To demonstrate effectiveness of the proposed ASSL in semi-supervised 3D action recognition, we conduct extensive experiments on NTU and N-UCLA datasets. The results confirm its advantageous performance over state-of-the-art semi-supervised methods in the few label regime for 3D action recognition.",0
"This abstract describes a study that utilizes adversarial self-supervised learning (AdvSSL) techniques for semi-supervised action recognition on 3D skeleton data. This approach involves creating two neural networks: one encoder network and one discriminator network. The encoder takes raw 3D pose data as input and extracts features which are then passed to the discriminator, which attempts to determine whether the extracted feature vectors were generated from labeled or unlabeled data. The goal of this approach is to improve the performance of semi-supervised models by leveraging large amounts of unlabeled training data through self-supervision. Experimental results demonstrate significant improvements over traditional approaches, with AdvSSL outperforming several state-of-the-art methods across multiple metrics. These findings have important implications for the development of more efficient and effective deep learning algorithms for action recognition tasks. Overall, this work represents an important contribution to the field of computer vision and human motion analysis.",1
"Anomaly detection-based spoof attack detection is a recent development in face Presentation Attack Detection (fPAD), where a spoof detector is learned using only non-attacked images of users. These detectors are of practical importance as they are shown to generalize well to new attack types. In this paper, we present a deep-learning solution for anomaly detection-based spoof attack detection where both classifier and feature representations are learned together end-to-end. First, we introduce a pseudo-negative class during training in the absence of attacked images. The pseudo-negative class is modeled using a Gaussian distribution whose mean is calculated by a weighted running mean. Secondly, we use pairwise confusion loss to further regularize the training process. The proposed approach benefits from the representation learning power of the CNNs and learns better features for fPAD task as shown in our ablation study. We perform extensive experiments on four publicly available datasets: Replay-Attack, Rose-Youtu, OULU-NPU and Spoof in Wild to show the effectiveness of the proposed approach over the previous methods. Code is available at: \url{https://github.com/yashasvi97/IJCB2020_anomaly}",0
"This paper presents a novel approach to detecting presentation attacks on face recognition systems using anomaly detection techniques. Presentation attacks, such as using printed photos or replayed video footage, can fool biometric authentication systems by presenting fake credentials that appear authentic to the system. Existing methods for detecting these attacks rely on specialized hardware sensors that detect changes in ambient light or temperature, which can be expensive and difficult to integrate into existing systems. Our proposed method uses machine learning algorithms to identify patterns of behavior that deviate from normal usage, indicating the presence of a potential attack. We evaluate our method using a large dataset of real-world presentation attacks and demonstrate that it outperforms previous approaches while requiring less expensive hardware. By providing a more accurate and cost-effective solution for detecting presentation attacks, we hope to improve the security and reliability of face recognition systems.",1
"In this paper, we study the problem of learning compact (low-dimensional) representations for sequential data that captures its implicit spatio-temporal cues. To maximize extraction of such informative cues from the data, we set the problem within the context of contrastive representation learning and to that end propose a novel objective via optimal transport. Specifically, our formulation seeks a low-dimensional subspace representation of the data that jointly (i) maximizes the distance of the data (embedded in this subspace) from an adversarial data distribution under the optimal transport, a.k.a. the Wasserstein distance, (ii) captures the temporal order, and (iii) minimizes the data distortion. To generate the adversarial distribution, we propose a novel framework connecting Wasserstein GANs with a classifier, allowing a principled mechanism for producing good negative distributions for contrastive learning, which is currently a challenging problem. Our full objective is cast as a subspace learning problem on the Grassmann manifold and solved via Riemannian optimization. To empirically study our formulation, we provide experiments on the task of human action recognition in video sequences. Our results demonstrate competitive performance against challenging baselines.",0
"Title: ""Representation Learning via Adversarially-Contrastive Optimal Transport"" ---------------------------------------------------------------  This work presents a novel framework for representation learning using adversarially-contrastive optimal transport (ACTOT). We introduce the concept of contrastive optimal transport, where we use both discriminators and generators within the objective function to drive meaningful representations that can generalize well on downstream tasks. Our method integrates adversarial training into the OT formulation, allowing us to leverage adversarial examples as regularization terms during optimization. Additionally, our approach uses data augmentations in a manner similar to Generative Adversarial Networks, further improving the stability and robustness of learned representations. Experimental results demonstrate the effectiveness of our framework compared to state-of-the-art approaches across multiple benchmark datasets. These findings highlight the potential utility of ACTOT in a variety of real-world applications ranging from image generation to domain adaptation. Overall, our work represents a significant step towards understanding how we can utilize contrastive mechanisms within the context of deep neural networks to better align these models with their goals.  ---- --Write a thesis statement in one sentence --  Our research introduces a new framework for representation learning through adversarially-contrastive optimal transport (ACTOT), achieving superior performance over traditional methods by integrating adversarial examples as regularization terms and incorporating data augmentations for improved stability.",1
"Deep auto-encoders (DAEs) have achieved great success in learning data representations via the powerful representability of neural networks. But most DAEs only focus on the most dominant structures which are able to reconstruct the data from a latent space and neglect rich latent structural information. In this work, we propose a new representation learning method that explicitly models and leverages sample relations, which in turn is used as supervision to guide the representation learning. Different from previous work, our framework well preserves the relations between samples. Since the prediction of pairwise relations themselves is a fundamental problem, our model adaptively learns them from data. This provides much flexibility to encode real data manifold. The important role of relation and representation learning is evaluated on the clustering task. Extensive experiments on benchmark data sets demonstrate the superiority of our approach. By seeking to embed samples into subspace, we further show that our method can address the large-scale and out-of-sample problem.",0
"In recent years, deep learning has achieved remarkable success across a wide range of tasks, from image classification to natural language processing. However, one challenge that remains is understanding how these models make predictions and why they are effective at certain tasks but not others. To address this issue, we propose relation-guided representation learning (RGRL), a method that exploits prior knowledge about relationships among concepts to improve the interpretability and performance of neural networks. Our approach incorporates domain knowledge into the training process by defining relations between different classes and using them as supervision signals to guide the model towards meaningful representations. We demonstrate the effectiveness of RGRL on several benchmark datasets and show that our approach improves both accuracy and visualizability compared to state-of-the-art methods. Overall, our work advances the field of explainable artificial intelligence and provides insights into the nature of human-like intelligence.",1
"The last decade has seen a flurry of research on all-pairs-similarity-search (or, self-join) for text, DNA, and a handful of other datatypes, and these systems have been applied to many diverse data mining problems. Surprisingly, however, little progress has been made on addressing this problem for time series subsequences. In this thesis, we have introduced a near universal time series data mining tool called matrix profile which solves the all-pairs-similarity-search problem and caches the output in an easy-to-access fashion. The proposed algorithm is not only parameter-free, exact and scalable, but also applicable for both single and multidimensional time series. By building time series data mining methods on top of matrix profile, many time series data mining tasks (e.g., motif discovery, discord discovery, shapelet discovery, semantic segmentation, and clustering) can be efficiently solved. Because the same matrix profile can be shared by a diverse set of time series data mining methods, matrix profile is versatile and computed-once-use-many-times data structure. We demonstrate the utility of matrix profile for many time series data mining problems, including motif discovery, discord discovery, weakly labeled time series classification, and representation learning on domains as diverse as seismology, entomology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring, and medicine. We hope the matrix profile is not the end but the beginning of many more time series data mining projects.",0
"Incorporating temporal dependencies into data mining models has proven challenging due to the high complexity of time series analysis and difficulties associated with handling large amounts of time series data. Recent research suggests that matrix profiles could serve as powerful tools for understanding complex relationships within multivariate datasets and can potentially overcome these limitations by allowing for easy extraction and visualization of key features from large time series databases. We propose the development of a novel framework called the ""Matrix Profile"" for time series data mining that combines classical techniques such as clustering, classification, regression, feature selection, anomaly detection, association rule mining, sequence mining, and motif discovery under one umbrella with the ability to incorporate both static and dynamic temporal aspects of time series data. Our results show that our approach outperforms existing methods on several real world benchmarks while maintaining computational efficiency. Furthermore, we provide evidence to suggest that our approach has applications across many fields where time series data is prevalent including finance, healthcare, transportation, and environmental monitoring. Overall, our work demonstrates significant potential towards developing a universally applicable tool for uncovering patterns and knowledge hidden within time series data.",1
"The graph structure of biomedical data differs from those in typical knowledge graph benchmark tasks. A particular property of biomedical data is the presence of long-range dependencies, which can be captured by patterns described as logical rules. We propose a novel method that combines these rules with a neural multi-hop reasoning approach that uses reinforcement learning. We conduct an empirical study based on the real-world task of drug repurposing by formulating this task as a link prediction problem. We apply our method to the biomedical knowledge graph Hetionet and show that our approach outperforms several baseline methods.",0
"In this paper, we aim to integrate logical rules into neural multi-hop reasoning for drug repurposing by leveraging existing knowledge bases such as medical textbooks, articles, FDA approvals etc. We propose LRA-MHR (Logic Rule Augmented Multi-Hop Reasoning), which incorporates first-order logic into convolutional neural networks via attention mechanisms. Our model takes advantage of large amounts of prior biomedical domain expertise encoded in rule sets like KG+rulesets available at resources like BiKAZAN. By doing so, our system can make accurate predictions on new applications for already approved drugs while taking into account the known safety and efficacy profiles. In summary, our approach achieves state-of-the-art results outperforming baseline models that only utilize either deep learning techniques or symbolic inference engines alone, demonstrating the effectiveness of integrating both approaches together into the same model.",1
"Recently deep neural networks demonstrate competitive performances in classification and regression tasks for many temporal or sequential data. However, it is still hard to understand the classification mechanisms of temporal deep neural networks. In this paper, we propose two new frameworks to visualize temporal representations learned from deep neural networks. Given input data and output, our algorithm interprets the decision of temporal neural network by extracting highly activated periods and visualizes a sub-sequence of input data which contributes to activate the units. Furthermore, we characterize such sub-sequences with clustering and calculate the uncertainty of the suggested type and actual data. We also suggest Layer-wise Relevance from the output of a unit, not from the final output, with backward Monte-Carlo dropout to show the relevance scores of each input point to activate units with providing a visual representation of the uncertainty about this impact.",0
"In this paper, we propose a novel method for interpreting deep temporal representations using selective visualization of internally activated nodes. We demonstrate that our approach can effectively identify salient features in deep neural network architectures without relying on external annotations. Our approach uses a guided backpropagation algorithm to trace the flow of activation through the network and visualize only the most relevant nodes for each time step. This allows us to focus attention on key regions of interest while minimizing distractions from less important areas. We evaluate our approach on several benchmark datasets and show that it outperforms state-of-the-art methods in terms of accuracy and interpretability. Overall, our work represents an important step towards understanding how deep networks make predictions and opens up new possibilities for improving model transparency and explainability.",1
"State-of-the-art single depth image-based 3D hand pose estimation methods are based on dense predictions, including voxel-to-voxel predictions, point-to-point regression, and pixel-wise estimations. Despite the good performance, those methods have a few issues in nature, such as the poor trade-off between accuracy and efficiency, and plain feature representation learning with local convolutions. In this paper, a novel pixel-wise prediction-based method is proposed to address the above issues. The key ideas are two-fold: a) explicitly modeling the dependencies among joints and the relations between the pixels and the joints for better local feature representation learning; b) unifying the dense pixel-wise offset predictions and direct joint regression for end-to-end training. Specifically, we first propose a graph convolutional network (GCN) based joint graph reasoning module to model the complex dependencies among joints and augment the representation capability of each pixel. Then we densely estimate all pixels' offsets to joints in both image plane and depth space and calculate the joints' positions by a weighted average over all pixels' predictions, totally discarding the complex postprocessing operations. The proposed model is implemented with an efficient 2D fully convolutional network (FCN) backbone and has only about 1.4M parameters. Extensive experiments on multiple 3D hand pose estimation benchmarks demonstrate that the proposed method achieves new state-of-the-art accuracy while running very efficiently with around a speed of 110fps on a single NVIDIA 1080Ti GPU.",0
"This paper presents a novel approach to predict hand pose from a single depth image using joint graph reasoning and pixel-to-offset prediction networks (JGR-P2O). We propose a method that uses graph representation learning to encode relationships among keypoints on both hands and fingertips, as well as spatial contextual dependencies. Our pixel-to-offset prediction network then estimates offsets between each corresponding point pair within the grasped objects. We validate our approach through comprehensive experiments that demonstrate significant improvement over state-of-the-art methods in accuracy and generalization ability across challenging scenarios. The proposed approach shows great potential towards robust 3D hand pose estimation under complex environments and real-world applications.",1
"Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.",0
"""Model-free reinforcement learning (RL) has emerged as a powerful approach for training agents to perform complex tasks by interacting with their environment and receiving rewards based on their actions. Recent work has shown that RL can also be performed directly from high-dimensional sensory inputs such as images, making it possible to apply these algorithms to real-world problems involving vision. However, one major challenge faced by model-free RL methods is sample efficiency: the amount of experience required to achieve good performance often scales exponentially with problem difficulty. In this paper, we propose a new method for improving sample efficiency in model-free RL from images by leveraging recent advances in computer graphics and deep neural networks. Our approach uses a novel image generator trained to predict the next state given the current state and action, enabling efficient exploration without requiring actual interaction with the environment. We evaluate our method across a range of challenging continuous control tasks using the DeepMind Control Suite, demonstrating significantly improved sample efficiency compared to existing methods.""",1
"We present a systematic investigation using graph neural networks (GNNs) to model organic chemical reactions. To do so, we prepared a dataset collection of four ubiquitous reactions from the organic chemistry literature. We evaluate seven different GNN architectures for classification tasks pertaining to the identification of experimental reagents and conditions. We find that models are able to identify specific graph features that affect reaction conditions and lead to accurate predictions. The results herein show great promise in advancing molecular machine learning.",0
"In recent years there has been significant interest in developing machine learning models capable of predicting substrates and reagents suitable for specific chemical transformations as well as the conditions under which these transformations take place. Despite the large number of publications that have explored the application of deep neural networks for reaction prediction tasks, few studies have addressed the use of graph neural network architectures specifically designed to capture information about the structural features of molecules involved in organic synthesis. This paper presents an original approach based on graph convolutional networks (GCN) aimed at solving both problems simultaneously. Our model accurately identifies the optimal reactants (substrates), products, solvents, temperature range, and other relevant experimental factors required for organic reactions given only partial knowledge. We demonstrate the effectiveness of our method by performing a comprehensive evaluation using two datasets containing over one million experimental results from different sources. Additionally, we compare our predictions against those obtained using standard feedforward neural network architectures previously employed in similar applications as well as expert human analysis. Overall, our results indicate superior performance compared to existing methods and highlight the potential of GCNs as powerful tools for the accurate prediction of substrate-specific organic reaction conditions, thus providing new opportunities to accelerate drug discovery pipelines and improve process efficiency in chemistry research.",1
"We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting [2, 10]. However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don't fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. Standard training procedures bias neural networks towards learning ""simple"" classification boundaries, which may be less robust than more complex ones. We observe that adversarial training does produce more complex decision boundaries. We conjecture that in part the need for complex decision boundaries arises from sub-optimal representation learning. By means of simple toy examples, we show theoretically how the choice of representation can drastically affect adversarial robustness.",0
"""Overfitting"" is a term used to describe a problem that occurs during machine learning when a model starts performing better on training data than on new, unseen data. This can happen if a model has too many parameters relative to the number of observations it is being trained on, or if it simply converges on random noise instead of meaningful patterns in the dataset. Overfitting leads to poor generalization performance, which means that predictions made by the model are likely to be worse than those from simpler models or even pure chance. However, there exists a concept called ""benign overfitting,"" where despite having high training error, the test error remains low. This may seem counterintuitive but recent work has suggested that under certain conditions, benign overfitting can occur due to different reasons such as noise and randomness present in the data. In our study, we explore these scenarios further and provide insights into understanding how benign overfitting behaves and whether it truly leads to good generalization performance. We use multiple experiments to demonstrate the existence of benign overfitting, analyze factors affecting it, and investigate whether it truly results in robust models capable of making accurate predictions on unseen data. Our findings provide guidance on designing effective learning algorithms, choosing appropriate evaluation metrics, and interpreting experimental results in deep neural networks and other modern models.""",1
"This paper addresses a fundamental challenge in 3D medical image processing: how to deal with imaging thickness. For anisotropic medical volumes, there is a significant performance gap between thin-slice (mostly 1mm) and thick-slice (mostly 5mm) volumes. Prior arts tend to use 3D approaches for the thin-slice and 2D approaches for the thick-slice, respectively. We aim at a unified approach for both thin- and thick-slice medical volumes. Inspired by recent advances in video analysis, we propose AlignShift, a novel parameter-free operator to convert theoretically any 2D pretrained network into thickness-aware 3D network. Remarkably, the converted networks behave like 3D for the thin-slice, nevertheless degenerate to 2D for the thick-slice adaptively. The unified thickness-aware representation learning is achieved by shifting and fusing aligned ""virtual slices"" as per the input imaging thickness. Extensive experiments on public large-scale DeepLesion benchmark, consisting of 32K lesions for universal lesion detection, validate the effectiveness of our method, which outperforms previous state of the art by considerable margins without whistles and bells. More importantly, to our knowledge, this is the first method that bridges the performance gap between thin- and thick-slice volumes by a unified framework. To improve research reproducibility, our code in PyTorch is open source at https://github.com/M3DV/AlignShift.",0
"In computer graphics, geometric models often need to be created from large volumetric data sets that contain detailed information on tissue properties such as density, elasticity, contrast agent distribution or fluorescence lifetime information. However, current methods only provide limited control over local imaging thickness leading to loss of fine detail caused by interpolation artifacts or staircase effects in the output. We propose AlignShift, a method that preserves sharp features while aligning imaging planes along the object’s surface normals. Our approach works in two steps. First, we compute normal vectors at each point inside the volume using curvature analysis which can cope well even with large noise levels in the input geometry. Secondly, we project points onto their corresponding normal direction up to a signed distance threshold ensuring continuity across object boundaries. Since our method operates solely on shape information without any additional color information it generalizes to many different modalities. Extensive evaluations prove that AlignShift significantly reduces staircasing compared to state-of-the art approaches yielding more accurate geometric representations particularly in high resolution cases while still providing similar visual fidelity. For a given budget of computational resources, our novel technique produces higher quality results than other methods since it better exploits available memory bandwidth due to reduced interpolation needs. As future work, we believe that our framework could be extended further to perform anisotropic adaptive sampling strategies guided by the computed surface normals. Overall, our contributions include closing the gap between image thickness and high-resolution data resulting in vastly improved outputs for multiple application domains requiring high-fidelity meshes fr",1
"Audio representation learning based on deep neural networks (DNNs) emerged as an alternative approach to hand-crafted features. For achieving high performance, DNNs often need a large amount of annotated data which can be difficult and costly to obtain. In this paper, we propose a method for learning audio representations, aligning the learned latent representations of audio and associated tags. Aligning is done by maximizing the agreement of the latent representations of audio and tags, using a contrastive loss. The result is an audio embedding model which reflects acoustic and semantic characteristics of sounds. We evaluate the quality of our embedding model, measuring its performance as a feature extractor on three different tasks (namely, sound event recognition, and music genre and musical instrument classification), and investigate what type of characteristics the model captures. Our results are promising, sometimes in par with the state-of-the-art in the considered tasks and the embeddings produced with our method are well correlated with some acoustic descriptors.",0
"This paper presents a new approach to learning semantically enriched audio representations using co-aligned autoencoders (COALAs). In recent years, deep neural networks have achieved state-of-the-art performance on many tasks involving audio data, but these models often lack interpretability and fail to capture meaningful semantic relationships within the input signals. To address this problem, we introduce COALAs, which use adversarial training to learn more meaningful latent representations that preserve important semantic structures present in the input data. Our experiments show that COALAs outperform traditional autoencoders in terms of reconstruction accuracy and generate more interpretable and discriminative features for downstream applications such as music tagging, genre classification, and retrieval tasks. Overall, our work demonstrates the effectiveness of incorporating semantics into unsupervised representation learning pipelines to improve their quality and usefulness in real-world scenarios.",1
"Messenger advertisements (ads) give direct and personal user experience yielding high conversion rates and sales. However, people are skeptical about ads and sometimes perceive them as spam, which eventually leads to a decrease in user satisfaction. Targeted advertising, which serves ads to individuals who may exhibit interest in a particular advertising message, is strongly required. The key to the success of precise user targeting lies in learning the accurate user and ad representation in the embedding space. Most of the previous studies have limited the representation learning in the Euclidean space, but recent studies have suggested hyperbolic manifold learning for the distinct projection of complex network properties emerging from real-world datasets such as social networks, recommender systems, and advertising. We propose a framework that can effectively learn the hierarchical structure in users and ads on the hyperbolic space, and extend to the Multi-Manifold Learning. Our method constructs multiple hyperbolic manifolds with learnable curvatures and maps the representation of user and ad to each manifold. The origin of each manifold is set as the centroid of each user cluster. The user preference for each ad is estimated using the distance between two entities in the hyperbolic space, and the final prediction is determined by aggregating the values calculated from the learned multiple manifolds. We evaluate our method on public benchmark datasets and a large-scale commercial messenger system LINE, and demonstrate its effectiveness through improved performance.",0
"Title: ""Multi-manifold Learning for Large-Scale Targeted Advertising Systems""  This paper presents a novel approach to large-scale targeted advertising systems using multi-manifold learning techniques. In recent years, there has been an increasing demand for more effective ways to reach potential customers through digital marketing platforms such as social media, search engines, and websites. However, current methods often rely on simplistic user profiling, limited data analysis, or lack sufficient personalization. This leads to poor ad performance and low return on investment (ROI). To address these issues, our proposed method integrates multiple data sources and machine learning models, resulting in enhanced user understanding, improved ROIs, and increased customer satisfaction.  Our approach involves collecting and aggregating diverse datasets, including online behaviors, demographics, interests, purchase histories, clickstreams, and website interactions. We then utilize advanced feature engineering techniques to transform raw data into meaningful features that capture valuable insights about users' preferences. Next, we develop a hybrid model combining several state-of-the-art machine learning algorithms optimized for each specific prediction task. These tasks include audience segmentation, interest clustering, propensity scoring, clickthrough probability estimation, and conversion rate forecasting. Each component uses domain knowledge, statistical analysis, deep neural networks, natural language processing, graph embedding, reinforcement learning, or Bayesian inference to achieve high accuracy and interpretability.  To evaluate the effectiveness of our system, we conduct experiments on real-world datasets from various industries, covering millions of users and billions of impressions across different regions. Our results show significant improvements compared to industry benchmarks, achieving up to +29% increase in CTR, -46% decrease in CPC, and +87% lift in CVR by balancing bid optimization, creative rendering, and KPI maximization. Additionally, we provide qualitative analyses demonstrating how our ma",1
"Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions typically lack identifiability in parameter space, because they are overparameterized by design. In this paper, building on recent advances in nonlinear ICA, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.",0
"This is quite challenging as I would need more context on the specifics of your research project and/or publication in order to draft a compelling abstract. Abstract writing requires tailoring language specifically to the intended audience in mind so that they may quickly grasp the focus, novelty and implications of the work. Can you provide additional details? What field of study are we discussing (e.g., Artificial Intelligence), who is likely reading this abstract, how technical should the jargon become etc.. Are there any unique results in particular we can highlight without revealing the main findings first since those come after the abstract normally:  Here are some general guidelines regarding the content of the abstract once these other factors have been established. * Begin by asking yourself if your work is scientific in nature versus engineering; their abstract needs differ. For example: scientific papers typically open describing the problem statement before divulging results whereas most engineering publications introduce significant contributions at minimum. You should address either one of these depending on which applies for your subject matter most closely. If unsure ask others in your same area whether they think the focus leans toward one side over another. There is no hard rule of thumb. Once you know which side of this divide you fall under then determine how many sentences each portion takes up respectively. Most scientific reports average 8 out of 9 sentences covering a ""problem"" statement while 2 out of nine might mention the conclusion reached later on when presenting results. Engineering papers tend to devote far fewer sentences (perhaps only two or three) to motivation and instead go straight into listing their chief takeaways and benefits. When making your determination consider such points and also consult colleagues experienced within the same domain. * Be sure to emphasize the core methodology employed early enough that reviewers could roughly estimate the quality o",1
"Fair representation learning aims to encode invariant representation with respect to the protected attribute, such as gender or age. In this paper, we design Fairness-aware Disentangling Variational AutoEncoder (FD-VAE) for fair representation learning. This network disentangles latent space into three subspaces with a decorrelation loss that encourages each subspace to contain independent information: 1) target attribute information, 2) protected attribute information, 3) mutual attribute information. After the representation learning, this disentangled representation is leveraged for fairer downstream classification by excluding the subspace with the protected attribute information. We demonstrate the effectiveness of our model through extensive experiments on CelebA and UTK Face datasets. Our method outperforms the previous state-of-the-art method by large margins in terms of equal opportunity and equalized odds.",0
"A new method has been developed for representation learning that prioritizes both fairness and disentanglement. This approach tackles the problem of identifying and addressing unfair biases present in data sets while simultaneously ensuring that learned representations are interpretable and meaningful. By utilizing a novel regularization technique called Fairness-aware Disentangled Representation Learning (FDCRL), the proposed method enhances the ability of machine learning algorithms to produce accurate predictions while promoting equality and equity across different subpopulations. Experimental results demonstrate the effectiveness of FDCRL in improving model performance on benchmark datasets, particularly in settings where traditional methods struggle due to confounding factors caused by unfair bias. Overall, the work presented here represents a significant advancement towards building more inclusive artificial intelligence systems capable of serving diverse communities.",1
"We present Wiki-CS, a novel dataset derived from Wikipedia for benchmarking Graph Neural Networks. The dataset consists of nodes corresponding to Computer Science articles, with edges based on hyperlinks and 10 classes representing different branches of the field. We use the dataset to evaluate semi-supervised node classification and single-relation link prediction models. Our experiments show that these methods perform well on a new domain, with structural properties different from earlier benchmarks. The dataset is publicly available, along with the implementation of the data pipeline and the benchmark experiments, at https://github.com/pmernyei/wiki-cs-dataset .",0
"In recent years, there has been increasing interest in using graph neural networks (GNNs) for tasks such as node classification, link prediction, and graph generation. However, evaluating the performance of GNN models on real-world datasets can be challenging due to the lack of standard benchmark datasets and metrics that capture their unique properties. To address this problem, we propose Wiki-CS, a new benchmark dataset based on the Wikipedia network dataset. Our dataset includes multiple graphs representing different domains within the English Wikipedia knowledge base. We provide detailed annotations and baseline models to facilitate research and comparison of state-of-the-art GNN techniques. Our experiments demonstrate that Wiki-CS effectively captures important characteristics found in large knowledge bases while allowing researchers to evaluate the accuracy and stability of GNN methods under diverse settings. This work provides a valuable resource for the machine learning community working on graph neural networks, paving the way towards better understanding, evaluation, and development of these powerful algorithms.",1
"Multi-view learning is a learning task in which data is described by several concurrent representations. Its main challenge is most often to exploit the complementarities between these representations to help solve a classification/regression task. This is a challenge that can be met nowadays if there is a large amount of data available for learning. However, this is not necessarily true for all real-world problems, where data are sometimes scarce (e.g. problems related to the medical environment). In these situations, an effective strategy is to use intermediate representations based on the dissimilarities between instances. This work presents new ways of constructing these dissimilarity representations, learning them from data with Random Forest classifiers. More precisely, two methods are proposed, which modify the Random Forest proximity measure, to adapt it to the context of High Dimension Low Sample Size (HDLSS) multi-view classification problems. The second method, based on an Instance Hardness measurement, is significantly more accurate than other state-of-the-art measurements including the original RF Proximity measurement and the Large Margin Nearest Neighbor (LMNN) metric learning measurement.",0
"Title: A New Approach for Measuring Dissimilarity in Multi-View Learning using Random Forests  The problem of measuring dissimilarity between samples is crucial in many fields such as pattern recognition, machine learning, computer vision, bioinformatics, and data mining. In multi-view learning, where multiple views of the same set of data are available, traditional dissimilarity measures may no longer be appropriate due to their limited ability to capture complex relationships among different views. To address these limitations, we propose a novel random forest dissimi",1
"Learning discriminative powerful representations is a crucial step for machine learning systems. Introducing invariance against arbitrary nuisance or sensitive attributes while performing well on specific tasks is an important problem in representation learning. This is mostly approached by purging the sensitive information from learned representations. In this paper, we propose a novel disentanglement approach to invariant representation problem. We disentangle the meaningful and sensitive representations by enforcing orthogonality constraints as a proxy for independence. We explicitly enforce the meaningful representation to be agnostic to sensitive information by entropy maximization. The proposed approach is evaluated on five publicly available datasets and compared with state of the art methods for learning fairness and invariance achieving the state of the art performance on three datasets and comparable performance on the rest. Further, we perform an ablative study to evaluate the effect of each component.",0
"This paper presents a new approach for achieving fairness in deep learning models through orthogonal disentangling. By training neural networks to learn representations that are orthogonal and can be easily factored into separate components, we demonstrate how to improve interpretability and mitigate unwanted biases in data sets. Our results show significantly improved fairness metrics on several benchmark tasks and datasets.",1
"Studies on acquiring appropriate continuous representations of discrete objects, such as graphs and knowledge base data, have been conducted by many researchers in the field of machine learning. In this study, we introduce Nested SubSpace (NSS) arrangement, a comprehensive framework for representation learning. We show that existing embedding techniques can be regarded as special cases of the NSS arrangement. Based on the concept of the NSS arrangement, we implement a Disk-ANChor ARrangement (DANCAR), a representation learning method specialized to reproducing general graphs. Numerical experiments have shown that DANCAR has successfully embedded WordNet in ${\mathbb R}^{20}$ with an F1 score of 0.993 in the reconstruction task. DANCAR is also suitable for visualization in understanding the characteristics of graphs.",0
"In this work, we present a nested subspace arrangement framework to address the high dimensionality issues in data representation problems. We leverage recent advances from deep learning and compressive sensing literature that exploit efficient random projections via matrix factorization for subspace preservation. By combining these methods into a hierarchical scheme, our approach achieves state-of-the-art results on several real world datasets as well as extensive numerical experiments conducted in this study. Our proposed methodology can be used effectively across application domains including image recognition, natural language processing and bioinformatics. The codebase developed under this project has been made available publicly for use by other researchers who wish to utilize this framework on their datasets (<https://github.com/NSTARGroup/NSA-Rep>). Finally, there are many exciting open questions and potential future directions arising from this work which could lead to further improvements in data analysis performance.",1
"The problem of linking functional connectomics to behavior is extremely challenging due to the complex interactions between the two distinct, but related, data domains. We propose a coupled manifold optimization framework which projects fMRI data onto a low dimensional matrix manifold common to the cohort. The patient specific loadings simultaneously map onto a behavioral measure of interest via a second, non-linear, manifold. By leveraging the kernel trick, we can optimize over a potentially infinite dimensional space without explicitly computing the embeddings. As opposed to conventional manifold learning, which assumes a fixed input representation, our framework directly optimizes for embedding directions that predict behavior. Our optimization algorithm combines proximal gradient descent with the trust region method, which has good convergence guarantees. We validate our framework on resting state fMRI from fifty-eight patients with Autism Spectrum Disorder using three distinct measures of clinical severity. Our method outperforms traditional representation learning techniques in a cross validated setting, thus demonstrating the predictive power of our coupled objective.",0
"Title: Advancing Neuroimaging Methods by Leveraging Multi-Modal Information Synthesis  Abstract: Understanding brain function at both local and global levels has become increasingly important over recent years as advances in neuroimaging technologies have allowed researchers to collect vast amounts of data on structural and functional aspects of the human brain. However, one major challenge faced by investigators studying neural systems is how to integrate multi-modal datasets that capture different aspects of neural activity, such as fMRI data that maps neural activation patterns during task performance and DTI data which provides insights into white matter tract integrity. To address these limitations, we propose a novel methodological framework termed 'manifold optimization' which leverages existing knowledge from related domains to improve model estimation quality while reducing computational complexity in joint modeling efforts. Herein, we demonstrate our proposed approach using simulations to test its effectiveness in accurately estimating connectivity strength across multiple modalities. Our findings suggest that manifold optimization offers a promising solution for integrating complex neuroimaging data types towards improving understanding of brain structure-function relationships underlying cognitive processes. Overall, our work aims to provide innovative approaches towards advancing neuroimaging methods and bridging gaps between basic science discoveries and clinical practice to better inform diagnosis and treatment strategies for nervous system disorders.",1
"Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6% on CIFAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any ground-truth annotations. The code is made publicly available at https://github.com/wvangansbeke/Unsupervised-Classification.",0
"This paper introduces a new deep learning method called Self-Supervised Contrastive Adaptation Network (SCAN) which can classify images without any explicit labels. Unlike traditional supervised methods that require large amounts of labeled data, SCAN utilizes unlabeled image datasets along with a pretext task to learn features that discriminate between different classes. Through extensive experiments on various benchmark datasets, we show that our approach outperforms previous state-of-the-art models trained on similar scale of unlabeled data while using less computational resources. We believe that SCAN has the potential to enable breakthroughs in computer vision by significantly reducing the reliance on expensive and often impractical manual labeling efforts.",1
"Adversarial Training (AT) is proposed to alleviate the adversarial vulnerability of machine learning models by extracting only robust features from the input, which, however, inevitably leads to severe accuracy reduction as it discards the non-robust yet useful features. This motivates us to preserve both robust and non-robust features and separate them with disentangled representation learning. Our proposed Adversarial Asymmetric Training (AAT) algorithm can reliably disentangle robust and non-robust representations without additional supervision on robustness. Empirical results show our method does not only successfully preserve accuracy by combining two representations, but also achieve much better disentanglement than previous work.",0
"Abstract: Robust disentanglement has become increasingly important as deep generative models such as VAEs have been used to generate complex data like images and audio. However, many methods that achieve robust disentanglement require additional supervision beyond just the unsupervised reconstruction loss commonly seen in VAEs. In our work, we present a decoder free method for achieving robust disentanglement without any explicit regularization terms. Our method relies on optimizing a new set of variational lower bounds to encourage latent code factoriality at training time. We show through both qualitative and quantitative evaluation that this approach leads to improvements over previous state-of-the art methods on standard benchmark datasets while requiring less computational overhead during inference. This work represents an important step forward towards developing truly unsupervised approaches capable of generating high fidelity samples from highly entwined domains like human faces or musical notes.",1
"Visual question answering is concerned with answering free-form questions about an image. Since it requires a deep linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires techniques from both computer vision and natural language processing. We propose a novel method that approaches the task by performing context-driven, sequential reasoning based on the objects and their semantic and spatial relationships present in the scene. As a first step, we derive a scene graph which describes the objects in the image, as well as their attributes and their mutual relationships. A reinforcement agent then learns to autonomously navigate over the extracted scene graph to generate paths, which are then the basis for deriving answers. We conduct a first experimental study on the challenging GQA dataset with manually curated scene graphs, where our method almost reaches the level of human performance.",0
"This paper presents a novel approach to visual question answering (VQA) that leverages scene graph representation and reasoning techniques to improve performance on challenging VQA tasks. We first extract a scene graph from an image using state-of-the-art computer vision algorithms, which represents objects and relationships within the image as nodes and edges respectively. Our model then uses this scene graph to reason about potential answers to questions posed over images. In particular, we propose two approaches: one based on rule-based inference and another based on probabilistic graphical models. We evaluate our methods on several benchmark datasets and demonstrate significant improvement over existing VQA approaches across multiple metrics. We showcase the utility of our method by applying it to real-world applications such as robotics and education, where accurate VQA can have high impact. Overall, our work shows promise towards enabling more advanced forms of artificial intelligence to interact naturally with humans via language understanding and generation.",1
"Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) -- a self-supervised graph neural network pre-training framework -- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.",0
"This article proposes Graph Contrastive Coding (GCC), a novel method for pre-training graph neural networks on large amounts of unlabeled data. Current approaches to pre-training these models rely heavily on random walks, node attributes, or simple graph operations. In contrast, GCC uses a new type of encoding that captures both local and global structure by projecting each vertex onto multiple scales while preserving information about their neighborhoods. We evaluate our method using several benchmark datasets and show that it outperforms competitive baselines significantly, achieving state-of-the-art results in most cases. Our approach is flexible, efficient, scalable, and has strong theoretical guarantees. Overall, we believe that GCC paves the way towards more effective ways of training graph neural networks on complex real-world graphs.",1
"Financial transactions constitute connections between entities and through these connections a large scale heterogeneous weighted graph is formulated. In this labyrinth of interactions that are continuously updated, there exists a variety of similarity-based patterns that can provide insights into the dynamics of the financial system. With the current work, we propose the application of Graph Representation Learning in a scalable dynamic setting as a means of capturing these patterns in a meaningful and robust way. We proceed to perform a rigorous qualitative analysis of the latent trajectories to extract real world insights from the proposed representations and their evolution over time that is to our knowledge the first of its kind in the financial sector. Shifts in the latent space are associated with known economic events and in particular the impact of the recent Covid-19 pandemic to consumer patterns. Capturing such patterns indicates the value added to financial modeling through the incorporation of latent graph representations.",0
"This paper investigates how financial embeddings evolve over time and explores their potential impact on downstream applications such as risk management and portfolio optimization. We conduct empirical analysis using a large dataset of financial instruments and show that embeddings exhibit dynamic behavior, characterized by gradual changes in vector directions and magnitudes across time. Our findings highlight the importance of considering temporal aspects when employing embeddings in real-world scenarios, where static representations may fall short. Furthermore, we demonstrate the utility of dynamically updating embeddings through simulations, which suggest promising gains in forecast accuracy and risk exposure detection compared to fixed embeddings. Overall, our work contributes to a better understanding of the dynamics behind financial embeddings and paves the way for future research in exploiting their full potential.",1
"Most existing works on disentangled representation learning are solely built upon an marginal independence assumption: all factors in disentangled representations should be statistically independent. This assumption is necessary but definitely not sufficient for the disentangled representations without additional inductive biases in the modeling process, which is shown theoretically in recent studies. We argue in this work that disentangled representations should be characterized by their relation with observable data. In particular, we formulate such a relation through the concept of mutual information: the mutual information between each factor of the disentangled representations and data should be invariant conditioned on values of the other factors. Together with the widely accepted independence assumption, we further bridge it with the conditional independence of factors in representations conditioned on data. Moreover, we note that conditional independence of latent variables has been imposed on most VAE-type models and InfoGAN due to the artificial choice of factorized approximate posterior $q(\rvz|\rvx)$ in the encoders. Such an arrangement of encoders introduces a crucial inductive bias for disentangled representations. To demonstrate the importance of our proposed assumption and the related inductive bias, we show in experiments that violating the assumption leads to decline of disentanglement among factors in the learned representations.",0
"In recent years, disentangling representations has become increasingly important in deep learning research as it allows models to capture more interpretable knowledge about data distributions. One challenge in understanding disentanglement arises from the fact that, despite their importance, there exists no clear framework for analyzing these representation spaces. This paper seeks to address this gap by proposing mutual information (MI) as a tool for studying how well learned representations match our intuitive notion of ""disentangling."" We provide theoretical analyses showing that MI captures properties such as factoriality and uniqueness, which characterize successful disentanglement. Further, we show how insights gleaned using MI can inform model selection and training dynamics in both quantitative and qualitative manners. Empirically, we evaluate several current state-of-the-art generative methods trained on diverse datasets ranging from static images, time-series, to graph structured inputs. Our results demonstrate that MI provides new insights into the strengths and weaknesses of different approaches. Throughout, we aim to demystify some common misconceptions surrounding interpretation of MI values, and discuss limitations of existing frameworks like total variance/information. By providing a unified viewpoint that links theoretical developments, interpretability tools, and experiments, we believe that this work offers valuable lessons towards improving future research in the space of disentangled representations. Ultimately, advancing our comprehension of effective disentangling is crucial for developing artificial intelligence systems that generalize across domains while producing intelligible explanations.",1
"Graph Neural Networks (GNNs) are versatile, powerful machine learning methods that enable graph structure and feature representation learning, and have applications across many domains. For applications critically requiring interpretation, attention-based GNNs have been leveraged. However, these approaches either rely on specific model architectures or lack a joint consideration of graph structure and node features in their interpretation. Here we present a model-agnostic framework for interpreting important graph structure and node features, Graph neural networks Including SparSe inTerpretability (GISST). With any GNN model, GISST combines an attention mechanism and sparsity regularization to yield an important subgraph and node feature subset related to any graph-based task. Through a single self-attention layer, a GISST model learns an importance probability for each node feature and edge in the input graph. By including these importance probabilities in the model loss function, the probabilities are optimized end-to-end and tied to the task-specific performance. Furthermore, GISST sparsifies these importance probabilities with entropy and L1 regularization to reduce noise in the input graph topology and node features. Our GISST models achieve superior node feature and edge explanation precision in synthetic datasets, as compared to alternative interpretation approaches. Moreover, our GISST models are able to identify important graph structure in real-world datasets. We demonstrate in theory that edge feature importance and multiple edge types can be considered by incorporating them into the GISST edge probability computation. By jointly accounting for topology, node features, and edge features, GISST inherently provides simple and relevant interpretations for any GNN models and tasks.",0
"As machine learning models become more complex, understanding their behavior becomes increasingly important for ensuring that they perform well on real-world tasks and produce interpretable results. One family of such models are graph neural networks (GNN), which have demonstrated promising performance across several domains including computer vision, natural language processing, and knowledge graphs. However, these models can often suffer from overparameterization, resulting in poor generalization performance and difficult interpretation. Addressing these issues requires a careful balance between model complexity and interpretability, while still achieving state-of-the-art performance. This paper presents our approach to improving GNN interpretability by incorporating structured sparsity into both architecture design and training objectives. We demonstrate significant improvements in accuracy, robustness, and transparency compared to existing methods. Our work highlights the importance of interpretability in machine learning research and practice and provides a solid foundation for further advancements in the field.",1
"In self-supervised visual representation learning, a feature extractor is trained on a ""pretext task"" for which labels can be generated cheaply, without human annotation. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such ""shortcut"" features and hand-designing schemes to reduce their effect. Here, we propose a general framework for mitigating the effect shortcut features. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a ""lens"" network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.",0
"In recent years, representation learning has become an essential component of many natural language processing (NLP) tasks such as sentiment analysis, machine translation, and question answering. Self-supervised representation learning, which involves training models on large amounts of unlabeled data using pretext tasks, has emerged as a powerful alternative to supervised learning. However, self-supervised methods still face challenges such as shortcut solutions that allow models to solve pretext tasks without learning meaningful representations. This paper proposes a method called automatic shortcut removal for self-supervised representation learning. Our approach uses multiple pretext tasks and dynamically updates their difficulty levels during training to encourage the model to learn better representations. We evaluate our method on several benchmark datasets across different NLP domains and demonstrate improved performance compared to previous state-of-the-art methods. Overall, this work shows promising results towards solving the challenge of shortcuts in self-supervised representation learning.",1
"Deep clustering has increasingly been demonstrating superiority over conventional shallow clustering algorithms. Deep clustering algorithms usually combine representation learning with deep neural networks to achieve this performance, typically optimizing a clustering and non-clustering loss. In such cases, an autoencoder is typically connected with a clustering network, and the final clustering is jointly learned by both the autoencoder and clustering network. Instead, we propose to learn an autoencoded embedding and then search this further for the underlying manifold. For simplicity, we then cluster this with a shallow clustering algorithm, rather than a deeper network. We study a number of local and global manifold learning methods on both the raw data and autoencoded embedding, concluding that UMAP in our framework is best able to find the most clusterable manifold in the embedding, suggesting local manifold learning on an autoencoded embedding is effective for discovering higher quality discovering clusters. We quantitatively show across a range of image and time-series datasets that our method has competitive performance against the latest deep clustering algorithms, including out-performing current state-of-the-art on several. We postulate that these results show a promising research direction for deep clustering. The code can be found at https://github.com/rymc/n2d",0
"N2D (Not Too) Deep Clustering is a method that uses clustering analysis on the local manifold of an autoencoded embedding to group data points into meaningful clusters without overfitting. By doing so, N2D provides an efficient and scalable solution for large scale unsupervised learning tasks such as image annotation, anomaly detection, and feature selection. This approach leverages the power of deep neural networks to learn complex representations while controlling their complexity by constraining the depth of clusters. Our experiments demonstrate that N2D outperforms traditional clustering methods like k-means and hierarchical clustering in terms of accuracy, speed and robustness, making it suitable for real world applications involving high dimensional datasets. Overall, N2D represents an important step forward towards bridging the gap between unsupervised learning and deep representation models.",1
"Graph neural networks are promising architecture for learning and inference with graph-structured data. Yet difficulties in modelling the ``parts'' and their ``interactions'' still persist in terms of graph classification, where graph-level representations are usually obtained by squeezing the whole graph into a single vector through graph pooling. From complex systems point of view, mixing all the parts of a system together can affect both model interpretability and predictive performance, because properties of a complex system arise largely from the interaction among its components. We analyze the intrinsic difficulty in graph classification under the unified concept of ``resolution dilemmas'' with learning theoretic recovery guarantees, and propose ``SLIM'', an inductive neural network model for Structural Landmarking and Interaction Modelling. It turns out, that by solving the resolution dilemmas, and leveraging explicit interacting relation between component parts of a graph to explain its complexity, SLIM is more interpretable, accurate, and offers new insight in graph representation learning.",0
"This paper presents structural landmarking as a technique for graph classification that addresses resolution dilemmas that arise from different levels of abstraction when modeling graphs. We propose a novel framework that utilizes local structure descriptors computed at strategic locations along the curve skeleton of planar graphs. By capturing significant features while preserving global geometric structures, our method achieves robustness against noise and distortions. Experimental results demonstrate improved accuracy and efficiency compared to existing methods. Our approach has promising applications across domains such as computer vision, bioinformatics, and cybersecurity.",1
"This paper addresses the task of unsupervised learning of representations for action recognition in videos. Previous works proposed to utilize future prediction, or other domain-specific objectives to train a network, but achieved only limited success. In contrast, in the relevant field of image representation learning, simpler, discrimination-based methods have recently bridged the gap to fully-supervised performance. We first propose to adapt two top performing objectives in this class - instance recognition and local aggregation, to the video domain. In particular, the latter approach iterates between clustering the videos in the feature space of a network and updating it to respect the cluster with a non-parametric classification loss. We observe promising performance, but qualitative analysis shows that the learned representations fail to capture motion patterns, grouping the videos based on appearance. To mitigate this issue, we turn to the heuristic-based IDT descriptors, that were manually designed to encode motion patterns in videos. We form the clusters in the IDT space, using these descriptors as a an unsupervised prior in the iterative local aggregation algorithm. Our experiments demonstrates that this approach outperform prior work on UCF101 and HMDB51 action recognition benchmarks. We also qualitatively analyze the learned representations and show that they successfully capture video dynamics.",0
"In recent years, unsupervised learning has emerged as a powerful technique for extracting meaningful representations from large datasets. In particular, deep neural networks have been used to learn representations that capture important features of images, audio signals, and text data. However, there remains a significant challenge in using unsupervised methods to learn video representations that can effectively capture the complex temporal dynamics present in many videos.  This work addresses this challenge by proposing a novel method based on dense trajectory clustering (DTC). In DTC, the video frames are embedded into a high-dimensional feature space and clustered using k-means. We show that applying k-means at different scales results in clusters containing coherent motion patterns across varying spatial and temporal scales. By aggregating these clusters and their corresponding trajectories, we obtain a set of representative visual elements which can be assembled into a more complete representation of the original video sequence.  Our approach outperforms previous state-of-the art techniques on several benchmark datasets and shows promising results in several downstream tasks such as action recognition and video retrieval. Furthermore, our method requires only weak supervision, making it scalable to larger datasets. Our contributions provide new insights into the effectiveness of trajectory clustering for learning video representations, paving the way for further research in this area.",1
"With the explosion of digital data in recent years, continuously learning new tasks from a stream of data without forgetting previously acquired knowledge has become increasingly important. In this paper, we propose a new continual learning (CL) setting, namely ``continual representation learning'', which focuses on learning better representation in a continuous way. We also provide two large-scale multi-step benchmarks for biometric identification, where the visual appearance of different classes are highly relevant. In contrast to requiring the model to recognize more learned classes, we aim to learn feature representation that can be better generalized to not only previously unseen images but also unseen classes/identities. For the new setting, we propose a novel approach that performs the knowledge distillation over a large number of identities by applying the neighbourhood selection and consistency relaxation strategies to improve scalability and flexibility of the continual learning model. We demonstrate that existing CL methods can improve the representation in the new setting, and our method achieves better results than the competitors.",0
"This is an interesting paper that addresses biometric identification using continual representation learning. The authors present a method based on deep neural networks that can learn representations from multiple tasks continuously over time. These representations are then used as input features for a task such as face verification or fingerprint matching. Experimental results show that their approach outperforms traditional methods of feature extraction, while also requiring less computational resources. Furthermore, they demonstrate how their algorithm can adapt to new data streams by incremental learning, making it suitable for real-world scenarios where the data distribution may change over time. Overall, this work represents a significant advancement in the field of biometric recognition and has potential applications in fields such as security and access control.",1
"Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses (footnote: https://github.com/DSE-MSU/DeepRobust). The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.",0
"Graph neural networks (GNNs) have recently emerged as powerful models for learning representations on graphs. GNNs utilize graph structure to encode relationships among data points, enabling them to capture complex interactions that cannot be fully captured by conventional methods such as deep neural nets on fixed Euclidean features. However, robustness is still a significant challenge for many GNN architectures due to their vulnerability to adversarial attacks. This paper investigates the relationship between graph structure learning and robustness for GNNs. We show that incorporating structured sparsity into the graph structure can improve both model accuracy and resilience against attack. In particular, we propose a method based on solving linear equations to estimate the adjacency matrix from unstructured random noise, which allows us to create graph structures with targeted node degrees. Our experiments demonstrate consistent improvements over baseline methods across multiple datasets and architectures, validating our approach. These findings suggest new directions for developing more reliable graph-based machine learning systems.",1
"Unsupervised multi-object scene decomposition is a fast-emerging problem in representation learning. Despite significant progress in static scenes, such models are unable to leverage important dynamic cues present in video. We propose a novel spatio-temporal iterative inference framework that is powerful enough to jointly model complex multi-object representations and explicit temporal dependencies between latent variables across frames. This is achieved by leveraging 2D-LSTM, temporally conditioned inference and generation within the iterative amortized inference for posterior refinement. Our method improves the overall quality of decompositions, encodes information about the objects' dynamics, and can be used to predict trajectories of each object separately. Additionally, we show that our model has a high accuracy even without color information. We demonstrate the decomposition, segmentation, and prediction capabilities of our model and show that it outperforms the state-of-the-art on several benchmark datasets, one of which was curated for this work and will be made publicly available.",0
"This research presents a novel method for unsupervised video decomposition, which utilizes spatio-temporal iterative inference to separate moving objects from the background scene. The proposed approach overcomes several limitations found in existing techniques by leveraging both spatial and temporal contextual cues, as well as incorporating prior knowledge about object motion patterns. Experimental results on publicly available datasets demonstrate that our method outperforms state-of-the-art methods in terms of accuracy and computational efficiency. Additionally, we showcase applications of our method in tasks such as object tracking, action recognition, and activity analysis, highlighting its versatility and potential impact across diverse fields.",1
"Autoencoders have been widely used for dimensional reduction and feature extraction. Various types of autoencoders have been proposed by introducing regularization terms. Most of these regularizations improve representation learning by constraining the weights in the encoder part, which maps input into hidden nodes and affects the generation of features. In this study, we show that a constraint to the decoder can also significantly improve its performance because the decoder determines how the latent variables contribute to the reconstruction of input. Inspired by the structural modal analysis method in mechanical engineering, a new modal autoencoder (MAE) is proposed by othogonalising the columns of the readout weight matrix. The new regularization helps to disentangle explanatory factors of variation and forces the MAE to extract fundamental modes in data. The learned representations are functionally independent in the reconstruction of input and perform better in consecutive classification tasks. The results were validated on the MNIST variations and USPS classification benchmark suite. Comparative experiments clearly show that the new algorithm has a surprising advantage. The new MAE introduces a very simple training principle for autoencoders and could be promising for the pre-training of deep neural networks.",0
"This research presents a new approach to autoencoding using modal encoding techniques. We introduce a novel architecture that can learn functionally independent features from raw input data. Our method is based on the idea of learning representations by reconstructing inputs. By optimizing for reconstruction error alone, autoencoders have been shown to capture underlying structure present in the data. However, these learned representations may still exhibit some functional dependencies. To mitigate this issue, we use a modal encoder that maps each datum into a high dimensional space where each dimension corresponds to a specific feature type. These types could represent texture, shape, color etc. The decoder then learns to project any one of these types back onto the original input. In our experiments, we show that our method outperforms traditional autoencoder architectures in terms of both quantitative metrics such as PSNR and SSIM, but also qualitative assessment including human evaluation. Additionally, we provide several ablation studies showing the importance of different components in our model. Lastly, we demonstrate the effectiveness of our learned representations by fine tuning pre-trained models on downstream tasks. All code and models used in this study will be made publicly available upon acceptance.",1
"Representation learning models for graphs are a successful family of techniques that project nodes into feature spaces that can be exploited by other machine learning algorithms. Since many real-world networks are inherently dynamic, with interactions among nodes changing over time, these techniques can be defined both for static and for time-varying graphs. Here, we build upon the fact that the skip-gram embedding approach implicitly performs a matrix factorization, and we extend it to perform implicit tensor factorization on different tensor representations of time-varying graphs. We show that higher-order skip-gram with negative sampling (HOSGNS) is able to disentangle the role of nodes and time, with a small fraction of the number of parameters needed by other approaches. We empirically evaluate our approach using time-resolved face-to-face proximity data, showing that the learned time-varying graph representations outperform state-of-the-art methods when used to solve downstream tasks such as network reconstruction, and to predict the outcome of dynamical processes such as disease spreading. The source code and data are publicly available at https://github.com/simonepiaggesi/hosgns.",0
"Graphs can provide rich structures on top of which machine learning algorithms can act upon; yet their direct use as input data faces challenges related to scalability due to the sparsity of graph representation that prohibits downstream applications from effectively capturing features crucial for meaningful predictions. Inspired by recent success in natural language processing, we propose an alternative formulation to represent graphs: time-varying graphs. This approach augments static snapshots of graphs over time to encode temporal dynamics within a single graph object. To capture such dynamics, we leverage higher-order skipgram models with negative sampling techniques to embed nodes in the context of neighboring nodes at varying degrees. Experiments across four diverse datasets demonstrate the effectiveness of our proposed framework by consistently outperforming multiple baseline methods while revealing insights into the nature of node interactions and network evolution. We believe this work marks an important step toward bridging the gap between the high-dimensionality of raw graphs and effective feature extraction for improved model accuracy on complex prediction tasks. Additionally, beyond predictive modeling, embedding learned through our method may serve other analytical purposes, including community detection and anomaly detection.",1
"Neural networks used for multi-interaction trajectory reconstruction lack the ability to estimate the uncertainty in their outputs, which would be useful to better analyse and understand the systems they model. In this paper we extend the Factorised Neural Relational Inference model to output both a mean and a standard deviation for each component of the phase space vector, which together with an appropriate loss function, can account for uncertainty. A variety of loss functions are investigated including ideas from convexification and a Bayesian treatment of the problem. We show that the physical meaning of the variables is important when considering the uncertainty and demonstrate the existence of pathological local minima that are difficult to avoid during training.",0
"Title: A New Perspective on Neural Networks: Uncertainty Quantification in Relational Inference via Gradient Descent Optimization Authors: John Doe (Department of Computer Science, University X), Jane Smith (Department of Mathematics, University Y) Abstract: This work presents a novel approach to uncertainty quantification in neural relational inference trajectory reconstruction, which involves training neural networks to solve problems that can be represented as relations among entities. By leveraging recent advances in deep learning and optimization techniques, we propose a methodology based on gradient descent that allows us to estimate aleatoric and epistemic uncertainties during the forward pass of the network. Our experiments showcase the effectiveness of our framework across several benchmark datasets commonly used in the field. We discuss potential applications of this technique beyond traditional domains where such models have been employed, including natural language processing, computer vision, robotics, and bioinformatics. Keywords: Uncertainty estimation, neural networks, relational inference, gradient descent optimiz",1
"Graph representation learning has been extensively studied in recent years. Despite its potential in generating continuous embeddings for various networks, both the effectiveness and efficiency to infer high-quality representations toward large corpus of nodes are still challenging. Sampling is a critical point to achieve the performance goals. Prior arts usually focus on sampling positive node pairs, while the strategy for negative sampling is left insufficiently explored. To bridge the gap, we systematically analyze the role of negative sampling from the perspectives of both objective and risk, theoretically demonstrating that negative sampling is as important as positive sampling in determining the optimization objective and the resulted variance. To the best of our knowledge, we are the first to derive the theory and quantify that the negative sampling distribution should be positively but sub-linearly correlated to their positive sampling distribution. With the guidance of the theory, we propose MCNS, approximating the positive distribution with self-contrast approximation and accelerating negative sampling by Metropolis-Hastings. We evaluate our method on 5 datasets that cover extensive downstream graph learning tasks, including link prediction, node classification and personalized recommendation, on a total of 19 experimental settings. These relatively comprehensive experimental results demonstrate its robustness and superiorities.",0
"Graph representation learning has emerged as a powerful technique for analyzing graphs in fields ranging from computer science to sociology, physics, and biology. At the heart of graph representation learning lies the process of negative sampling, which allows the model to distinguish between positive and negative examples in order to learn meaningful representations of nodes or edges in the graph. However, understanding how negative sampling works and how it can affect the performance of graph representation learning models remains an open challenge. In this study, we aim to shed light on this issue by investigating the impact of negative sampling on several popular graph representation learning algorithms across different datasets. Our results show that choosing appropriate hyperparameters for negative samplin",1
"In this article we introduce theory and algorithms for learning discrete representations that take on a lattice that is embedded in an Euclidean space. Lattice representations possess an interesting combination of properties: a) they can be computed explicitly using lattice quantization, yet they can be learned efficiently using the ideas we introduce in this paper, b) they are highly related to Gaussian Variational Autoencoders, allowing designers familiar with the latter to easily produce discrete representations from their models and c) since lattices satisfy the axioms of a group, their adoption can lead into a way of learning simple algebras for modeling binary operations between objects through symbolic formalisms, yet learn these structures also formally using differentiation techniques. This article will focus on laying the groundwork for exploring and exploiting the first two properties, including a new mathematical result linking expressions used during training and inference time and experimental validation on two popular datasets.",0
"In recent years, lattice representation learning has emerged as a powerful tool in machine learning research, allowing for more efficient and effective data analysis by organizing high-dimensional input spaces into lower-dimensional lattices. This approach has proven particularly useful in applications where traditional dimensionality reduction techniques have struggled to capture important relationships between features. In this paper, we provide a comprehensive review of recent advances in lattice representation learning, highlighting key challenges faced in applying these methods and presenting promising solutions towards addressing them. We conclude by discussing potential future directions for research in this area. While our focus is on lattice representations, many of the ideas could apply equally well if we were considering any fixed set discrete choice vectors from Z^n.",1
"Continual learning aims to learn tasks sequentially, with (often severe) constraints on the storage of old learning samples, without suffering from catastrophic forgetting. In this work, we propose prescient continual learning, a novel experimental setting, to incorporate existing information about the classes, prior to any training data. Usually, each task in a traditional continual learning setting evaluates the model on present and past classes, the latter with a limited number of training samples. Our setting adds future classes, with no training samples at all. We introduce Ghost Model, a representation-learning-based model for continual learning using ideas from zero-shot learning. A generative model of the representation space in concert with a careful adjustment of the losses allows us to exploit insights from future classes to constraint the spatial arrangement of the past and current classes. Quantitative results on the AwA2 and aP\&Y datasets and detailed visualizations showcase the interest of this new setting and the method we propose to address it.",0
"This abstract presents insights on future developments that can provide guidance for continual learning, enabling individuals to remain competitive and adaptive in fast-changing environments. By analyzing emerging trends, we identify key areas where new knowledge and skills will become essential and highlight approaches for acquiring them. Our findings emphasize the importance of continuous self-evaluation, active seeking out of novel experiences, and leveraging technology to accelerate skill acquisition. We also discuss the need for resilience and openness to change, as well as the role of social networks in supporting lifelong learning efforts. Overall, our work provides valuable perspectives on how to navigate tomorrow’s world of unprecedented opportunities and challenges through proactive development.",1
"Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the outputs become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learning. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explanation for the inferior performance of infinite networks observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA architectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottlenecks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning.",0
"Neural Networks (NN) play a fundamental role as the building blocks enabling Artificial Intelligence (AI). As we observe NN applications that rely on these components, their capacity often increases over time through architectures becoming larger and more complex, leading researchers to question whether there exist limitations imposed by nature itself that could prevent such growth, ultimately asking if bigger truly becomes better? In our work entitled ""Why Bigger Is Not Always Better"" (WBIB), we explore questions surrounding what could constrain growth. Here, we examine Finite Versus Infinite systems using methods based upon computational complexity theory that provide insight into how NN size impacts performance and vice versa. By conducting experiments using benchmark datasets within Machine Learning scenarios, WBIB analyzes both shallow versus deep architectures, and evaluates tradeoffs between running time required for training against test accuracy gains. Our results reveal insights that establish thresholds where NN growth plateaus at specific sizes, showing diminishing returns above these points. With our exploration, researchers will discover important considerations towards network design decisions. Ultimately, these findings can guide practitioners optimizing models efficiently. This paper investigates the relationship between the size of neural networks (NNs) and their effectiveness in artificial intelligence (AI) applications. While increasingly large and complex NN architectures have been used in recent years, some research has suggested that there may be natural limitations to NN growth. This study examines the issue from the perspective of computat",1
"Existing approaches for graph neural networks commonly suffer from the oversmoothing issue, regardless of how neighborhoods are aggregated. Most methods also focus on transductive scenarios for fixed graphs, leading to poor generalization for unseen graphs. To address these issues, we propose a new graph neural network that considers both edge-based neighborhood relationships and node-based entity features, i.e. Graph Entities with Step Mixture via random walk (GESM). GESM employs a mixture of various steps through random walk to alleviate the oversmoothing problem, attention to dynamically reflect interrelations depending on node information, and structure-based regularization to enhance embedding representation. With intensive experiments, we show that the proposed GESM achieves state-of-the-art or comparable performances on eight benchmark graph datasets comprising transductive and inductive learning tasks. Furthermore, we empirically demonstrate the significance of considering global information.",0
"This paper presents a novel approach to natural language processing that leverages the power of graphs and entities. By representing sentences as weighted bipartite graphs where nodes correspond to words (or subwords) and edges capture their relationships, we can capture complex semantic phenomena such as synonymy, hypernymy, etc., and make use of recent advances in graph learning. We then introduce entities, which provide contextual information at multiple levels, allowing us to disambiguate homographs like ""bank"" and resolve referential ambiguity with recurrent reasoning steps. Our model achieves new state-of-the-art results on several benchmark datasets, improving upon both strong baselines and previously published models built with more expressive components. Moreover, ablation studies show our core ideas greatly improve over simpler alternatives, thus validating our design decisions. Finally, while similar ideas have been explored before, our focus on graphs and entities provides a unifying framework, enabling us to study these topics more deeply and achieve significant performance gains, making this work broadly applicable across NLP tasks.",1
"A major challenge in modern reinforcement learning (RL) is efficient control of dynamical systems from high-dimensional sensory observations. Learning controllable embedding (LCE) is a promising approach that addresses this challenge by embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it to perform control in the latent space. Two important questions in this area are how to learn a representation that is amenable to the control problem at hand, and how to achieve an end-to-end framework for representation learning and control. In this paper, we take a few steps towards addressing these questions. We first formulate a LCE model to learn representations that are suitable to be used by a policy iteration style algorithm in the latent space. We call this model control-aware representation learning (CARL). We derive a loss function for CARL that has close connection to the prediction, consistency, and curvature (PCC) principle for representation learning. We derive three implementations of CARL. In the offline implementation, we replace the locally-linear control algorithm (e.g.,~iLQR) used by the existing LCE methods with a RL algorithm, namely model-based soft actor-critic, and show that it results in significant improvement. In online CARL, we interleave representation learning and control, and demonstrate further gain in performance. Finally, we propose value-guided CARL, a variation in which we optimize a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms by extensive experiments on benchmark tasks and compare them with several LCE baselines.",0
"In recent years, model-free deep reinforcement learning (RL) has achieved great success in solving complex tasks. However, these methods often require large amounts of data and computing resources to learn effective policies. On the other hand, model-based RL uses learned dynamics models to plan ahead and make better decisions, but suffers from high computational requirements due to the need to evaluate the entire model and optimize over actions at each time step.  To address these limitations, we propose control-aware representations, which are learned state spaces that capture the structure of the true system while reducing the complexity of planning. We show how these representations can be incorporated into model-predictive control and trajectory optimization frameworks, resulting in more efficient planning and improved policy performance compared to traditional approaches. Our method achieves significant speedups without sacrificing accuracy on challenging benchmark problems such as mountain car, cartpole swingup, and Lunar Lander, even with limited training data.  Overall, our work demonstrates the potential of using learned representations in combination with model-based RL algorithms to achieve competitive performance while significantly reducing computation cost. By exploiting the structure of the problem domain to guide representation learning, we enable both efficient and effective decision making under uncertainty.",1
"Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \cite{oono2019graph} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure ""expressiveness"" of embedding is conceptually clean; it leads to simpler proofs than \cite{oono2019graph} and can handle more non-linearities.",0
"""Over-smoothing"" occurs when graph neural networks produce blurry outputs that lack significant features. This issue can arise due to several factors such as node degree normalization, neighborhood aggregation functions like mean/max pooling, and downsampling operations like global average pooling. In this work, we investigate strategies for mitigating over-smoothing effects while preserving desirable aspects of these techniques. We evaluate our findings through comprehensive experiments conducted across multiple benchmark datasets from diverse domains. Our results show that by judicious tuning of hyperparameters, careful selection of activation functions, and appropriate weight initialization schemes, graph convolutional models can achieve improved performance compared to standard implementations without sacrificing smoothness. These insights provide important guidance for practitioners wishing to build powerful GNN architectures capable of capturing intricate structures present within complex graphs.",1
"Massive open online courses are becoming a modish way for education, which provides a large-scale and open-access learning opportunity for students to grasp the knowledge. To attract students' interest, the recommendation system is applied by MOOCs providers to recommend courses to students. However, as a course usually consists of a number of video lectures, with each one covering some specific knowledge concepts, directly recommending courses overlook students'interest to some specific knowledge concepts. To fill this gap, in this paper, we study the problem of knowledge concept recommendation. We propose an end-to-end graph neural network-based approach calledAttentionalHeterogeneous Graph Convolutional Deep Knowledge Recommender(ACKRec) for knowledge concept recommendation in MOOCs. Like other recommendation problems, it suffers from sparsity issues. To address this issue, we leverage both content information and context information to learn the representation of entities via graph convolution network. In addition to students and knowledge concepts, we consider other types of entities (e.g., courses, videos, teachers) and construct a heterogeneous information network to capture the corresponding fruitful semantic relationships among different types of entities and incorporate them into the representation learning process. Specifically, we use meta-path on the HIN to guide the propagation of students' preferences. With the help of these meta-paths, the students' preference distribution with respect to a candidate knowledge concept can be captured. Furthermore, we propose an attention mechanism to adaptively fuse the context information from different meta-paths, in order to capture the different interests of different students. The promising experiment results show that the proposedACKRecis able to effectively recommend knowledge concepts to students pursuing online learning in MOOCs.",0
"This research proposes a novel approach called Attentional Graph Convolutional Networks (AGCN) for knowledge concept recommendation in Massive Open Online Courses (MOOCs) based on heterogeneous view data. AGCN leverages both graph convolution networks and attentional mechanisms to extract fine-grained features from multiple types of data such as academic concepts, learning behavior and user profiles. An attention mechanism is introduced to learn different weights assigned to each type of feature for better capturing their relevance to learning needs. Furthermore, we design a two-stage aggregator architecture which integrates global context and local semantic correlations for more accurate knowledge concept recommendations. Experimental results show that our proposed method outperforms several state-of-the-art methods by achieving high accuracy and effectiveness in recommending relevant knowledge concepts to students enrolled in online courses. Our work contributes to the development of personalized educational technologies for promoting effective and efficient learning experiences.",1
"Image segmentation is a fundamental and challenging problem in computer vision with applications spanning multiple areas, such as medical imaging, remote sensing, and autonomous vehicles. Recently, convolutional neural networks (CNNs) have gained traction in the design of automated segmentation pipelines. Although CNN-based models are adept at learning abstract features from raw image data, their performance is dependent on the availability and size of suitable training datasets. Additionally, these models are often unable to capture the details of object boundaries and generalize poorly to unseen classes. In this thesis, we devise novel methodologies that address these issues and establish robust representation learning frameworks for fully-automatic semantic segmentation in medical imaging and mainstream computer vision. In particular, our contributions include (1) state-of-the-art 2D and 3D image segmentation networks for computer vision and medical image analysis, (2) an end-to-end trainable image segmentation framework that unifies CNNs and active contour models with learnable parameters for fast and robust object delineation, (3) a novel approach for disentangling edge and texture processing in segmentation networks, and (4) a novel few-shot learning model in both supervised settings and semi-supervised settings where synergies between latent and image spaces are leveraged to learn to segment images given limited training data.",0
"This paper presents a deep learning approach for automated image segmentation that utilizes unified region, edge, and contour models (REC) to accurately separate objects from their backgrounds. The proposed method integrates multiple cues such as color, texture, edge information, and shape priors into a single model to improve segmentation accuracy. An encoder-decoder network architecture is used to learn feature representations and produce pixel-level segmentations. Experiments on diverse datasets demonstrate the effectiveness of the REC model compared to state-of-the-art methods, achieving bettersegmentation results while maintaining efficiency. The REC model can be applied to various applications requiring accurate image segmentation, making it a valuable tool in computer vision research. Overall, this work advances the field by introducing a novel deep learning framework for robust and efficient image segmentation.",1
"Learning interpretable and disentangled representations is a crucial yet challenging task in representation learning. In this work, we focus on semi-supervised disentanglement learning and extend work by Locatello et al. (2019) by introducing another source of supervision that we denote as label replacement. Specifically, during training, we replace the inferred representation associated with a data point with its ground-truth representation whenever it is available. Our extension is theoretically inspired by our proposed general framework of semi-supervised disentanglement learning in the context of VAEs which naturally motivates the supervised terms commonly used in existing semi-supervised VAEs (but not for disentanglement learning). Extensive experiments on synthetic and real datasets demonstrate both quantitatively and qualitatively the ability of our extension to significantly and consistently improve disentanglement with very limited supervision.",0
"In this work, we propose an improved semi-supervised Variational Autoencoder (VAE) model that can learn disentangled representations from both labeled and unlabeled data. Our approach builds upon previous semi-supervised VAEs by incorporating more advanced regularization techniques, such as adversarial training and mutual information maximization, which encourage the encoder to capture meaningful relationships between inputs and outputs. We evaluate our method on several benchmark datasets and demonstrate state-of-the-art performance across multiple metrics, including reconstruction accuracy, entropy minimization, and visual fidelity. Additionally, we provide qualitative analysis of learned latent spaces, showing that our method indeed captures interpretable and disentangled features. Overall, our findings contribute to the growing field of semi-supervised learning and suggest that our proposed method holds significant potential for real-world applications.",1
"The objective of this paper is self-supervised representation learning, with the goal of solving semi-supervised video object segmentation (a.k.a. dense tracking). We make the following contributions: (i) we propose to improve the existing self-supervised approach, with a simple, yet more effective memory mechanism for long-term correspondence matching, which resolves the challenge caused by the dis-appearance and reappearance of objects; (ii) by augmenting the self-supervised approach with an online adaptation module, our method successfully alleviates tracker drifts caused by spatial-temporal discontinuity, e.g. occlusions or dis-occlusions, fast motions; (iii) we explore the efficiency of self-supervised representation learning for dense tracking, surprisingly, we show that a powerful tracking model can be trained with as few as 100 raw video clips (equivalent to a duration of 11mins), indicating that low-level statistics have already been effective for tracking tasks; (iv) we demonstrate state-of-the-art results among the self-supervised approaches on DAVIS-2017 and YouTube-VOS, as well as surpassing most of methods trained with millions of manual segmentation annotations, further bridging the gap between self-supervised and supervised learning. Codes are released to foster any further research (https://github.com/fangruizhu/self_sup_semiVOS).",0
"In recent years, video object segmentation has become increasingly important due to its numerous applications such as video editing, virtual reality, autonomous driving, and surveillance systems. One challenge that arises when working on video object segmentation is the requirement for large amounts of annotated data. However, obtaining annotations can be time-consuming and expensive. As a result, self-supervised learning techniques have gained interest, which use unlabelled videos to train models without human annotation. This study presents an approach called ""Self-Supervised Video Object Segmentation"" (SVOS) that exploits temporal coherency present in consecutive frames of a video. By using temporal consistency loss and cyclic motion prior loss, the model is trained to predict pixel-accurate masks without any manually annotated bounding boxes. Experimental results show that our method outperforms other state-of-the-art self-supervised methods by significant margins. Our SVOS system achieves competitive performance compared to fully supervised approaches while only utilizing cheap self-labels obtained from pretext tasks. In conclusion, the proposed approach represents an efficient solution to tackle the challenges associated with acquiring high-quality annotations, making self-supervision feasible for real-world application scenarios.",1
"Hypergraphs provide a natural representation for many real world datasets. We propose a novel framework, HNHN, for hypergraph representation learning. HNHN is a hypergraph convolution network with nonlinear activation functions applied to both hypernodes and hyperedges, combined with a normalization scheme that can flexibly adjust the importance of high-cardinality hyperedges and high-degree vertices depending on the dataset. We demonstrate improved performance of HNHN in both classification accuracy and speed on real world datasets when compared to state of the art methods.",0
"In HNNs (Hypernetworked Neural Networks), neurons from one neural network can have projections onto another network to exchange their states. They require that each projection originates from either some hidden unit or all output units of the projecting networks. This limits generalization power since hidden layers might not always capture relevant features of input data. We therefore introduce hyperedge neurons which allow connections from any part of one hypergraph partition to connect to multiple parts in other partitions. By allowing connections to bypass conventional hidden layers, we can better approximate arbitrary functions. We test our architecture on benchmark datasets and find improvements over baseline methods.",1
"We present a hierarchical neural message passing architecture for learning on molecular graphs. Our model takes in two complementary graph representations: the raw molecular graph representation and its associated junction tree, where nodes represent meaningful clusters in the original graph, e.g., rings or bridged compounds. We then proceed to learn a molecule's representation by passing messages inside each graph, and exchange messages between the two representations using a coarse-to-fine and fine-to-coarse information flow. Our method is able to overcome some of the restrictions known from classical GNNs, like detecting cycles, while still being very efficient to train. We validate its performance on the ZINC dataset and datasets stemming from the MoleculeNet benchmark collection.",0
"This paper presents a new approach to learning on molecular graphs using hierarchical inter-message passing (HMP). Our method allows us to model complex chemical structures and their interactions while leveraging recent advances in machine learning techniques. We demonstrate that our HMP algorithm outperforms existing methods on multiple benchmark datasets by achieving higher accuracy and faster training times. Furthermore, we provide insights into the working mechanism of HMP and its ability to learn from graph representations. Overall, our work offers a powerful toolkit for chemists and data scientists alike, allowing them to leverage large scale computational resources and make new discoveries across the fields of biochemistry, materials science, and synthetic organic chemistry.",1
"Variational Autoencoders (VAE) and their variants have been widely used in a variety of applications, such as dialog generation, image generation and disentangled representation learning. However, the existing VAE models have some limitations in different applications. For example, a VAE easily suffers from KL vanishing in language modeling and low reconstruction quality for disentangling. To address these issues, we propose a novel controllable variational autoencoder framework, ControlVAE, that combines a controller, inspired by automatic control theory, with the basic VAE to improve the performance of resulting generative models. Specifically, we design a new non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to automatically tune the hyperparameter (weight) added in the VAE objective using the output KL-divergence as feedback during model training. The framework is evaluated using three applications; namely, language modeling, disentangled representation learning, and image generation. The results show that ControlVAE can achieve better disentangling and reconstruction quality than the existing methods. For language modelling, it not only averts the KL-vanishing, but also improves the diversity of generated text. Finally, we also demonstrate that ControlVAE improves the reconstruction quality of generated images compared to the original VAE.",0
"In recent years, deep learning has revolutionized many fields by enabling computers to learn complex patterns from vast amounts of data. One such technique that has gained popularity is variational autoencoders (VAEs), which enable efficient, generative modeling of complex distributions while requiring relatively little labeled training data. However, there remains a need for greater control over VAEs and their outputs, particularly for applications where interpretability and controlling generation parameters is crucial.  In response to these challenges, we propose ControlVAE, a novel framework that extends traditional VAEs to allow users to specify desired properties of generated samples through direct manipulation of latent codes. By combining a simple prior distribution on these codes with a more flexible amortized posterior approximation using neural networks, ControlVAE enables efficient, controllable sampling across a wide range of tasks. Our method achieves competitive performance compared to state-of-the-art approaches while providing explicit control over key aspects of generated distributions, making it well suited for settings demanding transparency, human collaboration, or domain expertise.  We evaluate our approach across diverse domains including images, text, music, and control policies, demonstrating its effectiveness at generating coherent, interpretable output satisfying user constraints. We further showcase its utility in interactive design scenarios ranging from fashion advice to game content creation. Overall, ControlVAE represents a significant step towards democratizing powerful machine learning methods and broadening their use cases beyond purely passive model inference.",1
"We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate per-view 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance.",0
In this paper we present our work on lightweight multi-view 3D pose estimation using camera-disentangled representation. Our method uses a convolutional neural network to learn a latent representation that encodes both viewpoint-invariant 3D human shape characteristics and camera-specific information such as pose and projection parameters. This allows us to infer accurate 3D poses from multiple views without the need for explicit correspondence estimation. We demonstrate the effectiveness of our approach by comparing it against state-of-the-art methods on challenging benchmark datasets. Our experiments show that our method achieves competitive performance while requiring significantly fewer model parameters and computational resources. We conclude that our camera-disentangled representation can serve as an effective foundation for lightweight 3D pose estimation systems that run on consumer hardware devices.,1
"In self-supervised spatio-temporal representation learning, the temporal resolution and long-short term characteristics are not yet fully explored, which limits representation capabilities of learned models. In this paper, we propose a novel self-supervised method, referred to as video Playback Rate Perception (PRP), to learn spatio-temporal representation in a simple-yet-effective way. PRP roots in a dilated sampling strategy, which produces self-supervision signals about video playback rates for representation model learning. PRP is implemented with a feature encoder, a classification module, and a reconstructing decoder, to achieve spatio-temporal semantic retention in a collaborative discrimination-generation manner. The discriminative perception model follows a feature encoder to prefer perceiving low temporal resolution and long-term representation by classifying fast-forward rates. The generative perception model acts as a feature decoder to focus on comprehending high temporal resolution and short-term representation by introducing a motion-attention mechanism. PRP is applied on typical video target tasks including action recognition and video retrieval. Experiments show that PRP outperforms state-of-the-art self-supervised models with significant margins. Code is available at github.com/yuanyao366/PRP",0
"This paper presents video playback rate perception for self-supervised spatio-temporal representation learning as a means of creating an artificial intelligence that can process visual data at high speeds. By training the model on a large dataset of images and videos, we were able to achieve state-of-the-art performance in action recognition tasks using this approach. Furthermore, our method outperforms previous methods by a significant margin, demonstrating the effectiveness of our proposed solution. In conclusion, this research shows great promise for future applications of artificial intelligence in computer vision and multimedia processing.",1
"Graph neural networks have attracted wide attentions to enable representation learning of graph data in recent works. In complement to graph convolution operators, graph pooling is crucial for extracting hierarchical representation of graph data. However, most recent graph pooling methods still fail to efficiently exploit the geometry of graph data. In this paper, we propose a novel graph pooling strategy that leverages node proximity to improve the hierarchical representation learning of graph data with their multi-hop topology. Node proximity is obtained by harmonizing the kernel representation of topology information and node features. Implicit structure-aware kernel representation of topology information allows efficient graph pooling without explicit eigendecomposition of the graph Laplacian. Similarities of node signals are adaptively evaluated with the combination of the affine transformation and kernel trick using the Gaussian RBF function. Experimental results demonstrate that the proposed graph pooling strategy is able to achieve state-of-the-art performance on a collection of public graph classification benchmark datasets.",0
"Our proposed model uses graph pooling with node proximity as a means of learning hierarchical representations. We find that using both features together improves performance over traditional pooling methods on several benchmark datasets. Furthermore, we show how our method can generate meaningful hierarchies in real world networks by running experiments on protein interaction graphs.",1
"Joint clustering and feature learning methods have shown remarkable performance in unsupervised representation learning. However, the training schedule alternating between feature clustering and network parameters update leads to unstable learning of visual representations. To overcome this challenge, we propose Online Deep Clustering (ODC) that performs clustering and network update simultaneously rather than alternatingly. Our key insight is that the cluster centroids should evolve steadily in keeping the classifier stably updated. Specifically, we design and maintain two dynamic memory modules, i.e., samples memory to store samples labels and features, and centroids memory for centroids evolution. We break down the abrupt global clustering into steady memory update and batch-wise label re-assignment. The process is integrated into network update iterations. In this way, labels and the network evolve shoulder-to-shoulder rather than alternatingly. Extensive experiments demonstrate that ODC stabilizes the training process and boosts the performance effectively. Code: https://github.com/open-mmlab/OpenSelfSup.",0
"In recent years, deep clustering has emerged as a powerful tool for unsupervised representation learning. This approach leverages deep neural networks (DNNs) to simultaneously learn feature representations and cluster assignments from large amounts of raw data such as images or text. With the explosion of online data due to social media platforms like Instagram, Youtube, etc., developing algorithms that can effectively leverage online data and scale well with increasing dataset size is critical. In this paper we present a novel algorithm called ODCLR(Online DeepClustering by Large Recurrent Neural Networks). We evaluate our algorithm on several benchmark datasets against state-of-the-art baselines and demonstrate significant performance improvements across all metrics. Our method also shows strong robustness to changes in hyperparameters and random initialization making it ideal for real-world applications where fine-tuning may not be feasible. Finally, we provide insights into why ODCLR outperforms other methods through qualitative analysis and ablation studies. Overall, our results suggest that online deep clustering techniques have great potential for unlocking new capabilities in representation learning tasks and ODCLR serves as a promising step towards this goal.",1
"In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting is NP-complete and requires more global inference to oversee the whole graph. To make it scalable for large-scale graphs and patterns, we propose a learning framework which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize subgraph isomorphisms for the global counting. We develop both small graphs (= 1,024 subgraph isomorphisms in each) and large graphs (= 4,096 subgraph isomorphisms in each) sets to evaluate different models. A mutagenic compound dataset, MUTAG, is also used to evaluate neural models and demonstrate the success of transfer learning. While the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in linear time compared to the exponential time of the original NP-complete problem. Experimental results show that learning based subgraph isomorphism counting can speed up the traditional algorithm, VF2, 10-1,000 times with acceptable errors. Domain adaptation based on fine-tuning also shows the usefulness of our approach in real-world applications.",0
"Graph neural networks (GNNs) have emerged as powerful tools for predictive modeling tasks on graph data. Unfortunately, their application has been hindered by computational limitations: GNN inference algorithms typically require cubic time with respect to the size of the input graph, limiting their use cases. In this work we present new techniques that leverage recent advances in graph embedding spaces to dramatically reduce the cost of computing kernel matrices -- a key component in GNN predictions -- allowing linear scaling under mild assumptions. We validate our ideas on real datasets using popular GNN models; on all benchmark datasets we achieve state-of-the-art accuracy while reducing training times from hours to minutes! Our results open up a path towards large-scale deployment of GNNs in production settings.",1
"Graph Representation Learning (GRL) has experienced significant progress as a means to extract structural information in a meaningful way for subsequent learning tasks. Current approaches including shallow embeddings and Graph Neural Networks have mostly been tested with node classification and link prediction tasks. In this work, we provide an application oriented perspective to a set of popular embedding approaches and evaluate their representational power with respect to real-world graph properties. We implement an extensive empirical data-driven framework to challenge existing norms regarding the expressive power of embedding approaches in graphs with varying patterns along with a theoretical analysis of the limitations we discovered in this process. Our results suggest that ""one-to-fit-all"" GRL approaches are hard to define in real-world scenarios and as new methods are being introduced they should be explicit about their ability to capture graph properties and their applicability in datasets with non-trivial structural differences.",0
Graph representation learning (GRL) has been shown to produce high quality solutions on many real world problems such as image classification and natural language processing. However despite these successes there remain some challenges in applying GRL methods in practice. This paper seeks to provide quantitative insights into these challenges by introducing a number of new evaluation metrics that allow us to better assess how well different approaches perform under specific conditions. To support our analysis we conduct experiments across several domains to demonstrate both the effectiveness of current state of the art models and areas where further improvement is required. We believe that providing greater insight into the performance of graph representation techniques will enable researchers to develop more robust algorithms better suited to their applications. Our work serves as a starting point towards achieving this goal.,1
"Robust representation learning of temporal dynamic interactions is an important problem in robotic learning in general and automated unsupervised learning in particular. Temporal dynamic interactions can be described by (multiple) geometric trajectories in a suitable space over which unsupervised learning techniques may be applied to extract useful features from raw and high-dimensional data measurements. Taking a geometric approach to robust representation learning for temporal dynamic interactions, it is necessary to develop suitable metrics and a systematic methodology for comparison and for assessing the stability of an unsupervised learning method with respect to its tuning parameters. Such metrics must account for the (geometric) constraints in the physical world as well as the uncertainty associated with the learned patterns. In this paper we introduce a model-free metric based on the Procrustes distance for robust representation learning of interactions, and an optimal transport based distance metric for comparing between distributions of interaction primitives. These distance metrics can serve as an objective for assessing the stability of an interaction learning algorithm. They are also used for comparing the outcomes produced by different algorithms. Moreover, they may also be adopted as an objective function to obtain clusters and representative interaction primitives. These concepts and techniques will be introduced, along with mathematical properties, while their usefulness will be demonstrated in unsupervised learning of vehicle-to-vechicle interactions extracted from the Safety Pilot database, the world's largest database for connected vehicles.",0
"In recent years, unsupervised learning has become increasingly popular as researchers seek new ways to train artificial intelligence (AI) models without labeled data sets. One key challenge in this area is how to represent complex dynamic systems using unlabeled time-series data. This paper presents a novel approach called ""Robust Unsupervised Learning of Temporal Dynamic Interactions"" that addresses this problem by leveraging temporal correlations and latent representations to learn meaningful representations of dynamics.  We first introduce a method that identifies patterns in unlabelled time-series data through iterative clustering on top of embeddings derived from autoencoders trained on windows of those observations. The resulting clusters are interpreted as metastates, i.e., long-lasting configurations characterising multiple consecutive observations. We then develop methods to model transitions among these states.  Our contributions consist of two parts: first, we describe a novel technique based on robust statistics for extracting informative features capturing relevant aspects of the original signal, leading to better quality metastate detection; second, we propose a nonparametric Bayesian framework allowing to infer statistical relationships between observed variables directly from the raw signals under minimal assumptions while providing uncertainty estimates via Markov Chain Monte Carlo sampling. Our case studies demonstrate the effectiveness of our methodology across diverse domains ranging from financial trading signals to speech recognition and wildlife tracking. They reveal interesting insights into each application domain, e.g., stock market predictability and human voice disguise.",1
"Over the last few years, graph autoencoders (AE) and variational autoencoders (VAE) emerged as powerful node embedding methods, with promising performances on challenging tasks such as link prediction and node clustering. Graph AE, VAE and most of their extensions rely on multi-layer graph convolutional networks (GCN) encoders to learn vector space representations of nodes. In this paper, we show that GCN encoders are actually unnecessarily complex for many applications. We propose to replace them by significantly simpler and more interpretable linear models w.r.t. the direct neighborhood (one-hop) adjacency matrix of the graph, involving fewer operations, fewer parameters and no activation function. For the two aforementioned tasks, we show that this simpler approach consistently reaches competitive performances w.r.t. GCN-based graph AE and VAE for numerous real-world graphs, including all benchmark datasets commonly used to evaluate graph AE and VAE. Based on these results, we also question the relevance of repeatedly using these datasets to compare complex graph AE and VAE.",0
"In recent years, graph autoencoders have emerged as powerful tools for data reconstruction and feature extraction on graphs. However, designing effective graph autoencoders can be challenging due to their computational complexity and model capacity requirements. To address these limitations, we propose a simple yet effective approach that utilizes one-hop linear models in conjunction with efficient encoding schemes. Our method relies on sparse neighborhood sampling, which captures only a small subset of neighboring nodes per vertex in the graph. This reduces computational overhead while preserving important topological features. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets across different domains such as social networks, biological systems, and citation networks. Results show that our proposed graph autoencoder significantly outperforms existing state-of-the-art methods, achieving superior performance in terms of reconstruction accuracy and downstream task performance (e.g., node classification). Overall, our work provides a valuable contribution towards understanding the tradeoffs between simplicity and effectiveness in graph autoencoder design, paving the way for further advancements in graph representation learning.",1
"Self-supervised learning allows for better utilization of unlabelled data. The feature representation obtained by self-supervision can be used in downstream tasks such as classification, object detection, segmentation, and anomaly detection. While classification, object detection, and segmentation have been investigated with self-supervised learning, anomaly detection needs more attention. We consider the problem of anomaly detection in images and videos, and present a new visual anomaly detection technique for videos. Numerous seminal and state-of-the-art self-supervised methods are evaluated for anomaly detection on a variety of image datasets. The best performing image-based self-supervised representation learning method is then used for video anomaly detection to see the importance of spatial features in visual anomaly detection in videos. We also propose a simple self-supervision approach for learning temporal coherence across video frames without the use of any optical flow information. At its core, our method identifies the frame indices of a jumbled video sequence allowing it to learn the spatiotemporal features of the video. This intuitive approach shows superior performance of visual anomaly detection compared to numerous methods for images and videos on UCF101 and ILSVRC2015 video datasets.",0
"In recent years, representation learning has emerged as a powerful approach for detecting anomalies in images. Traditional supervised methods require large amounts of labeled data, which can be expensive and time consuming to collect and annotate. To address this limitation, self-supervised representation learning techniques have been proposed that learn representations without any labeled data. These approaches use pretext tasks such as image rotation prediction, colorization, jigsaw puzzles, etc., to train deep neural networks. While these pretext tasks have proven effective in many applications, they may not always capture the unique characteristics of anomaly detection problems. For example, some tasks may require local information, while others need global context. Additionally, existing self-supervised anomaly detection methods often assume static environments, meaning that anomalous objects remain stationary within the scene. However, real-world scenarios may involve dynamic backgrounds, making traditional pretext tasks insufficient for capturing temporal changes and variations. This paper proposes a novel framework for self-supervised representation learning tailored specifically for visual anomaly detection, which can handle complex scenarios involving both static and dynamic scenes. Our method uses multiple pretext tasks tailored towards different aspects of anomaly detection such as object interactions, motion analysis, and spatial inconsistencies. Experimental results on various benchmark datasets demonstrate significant improvements over state-of-the-art methods for both static and dynamic environments, highlighting the effectiveness of our approach for self-supervised anomaly detection.",1
"Organizations that own data face increasing legal liability for its discriminatory use against protected demographic groups, extending to contractual transactions involving third parties access and use of the data. This is problematic, since the original data owner cannot ex-ante anticipate all its future uses by downstream users. This paper explores the upstream ability to preemptively remove the correlations between features and sensitive attributes by mapping features to a fair representation space. Our main result shows that the fairness measured by the demographic parity of the representation distribution can be certified from a finite sample if and only if the chi-squared mutual information between features and representations is finite. Empirically, we find that smoothing the representation distribution provides generalization guarantees of fairness certificates, which improves upon existing fair representation learning approaches. Moreover, we do not observe that smoothing the representation distribution degrades the accuracy of downstream tasks compared to state-of-the-art methods in fair representation learning.",0
"This paper describes a method for learning smooth, fair representations that capture complex relationships between variables while preserving interpretability and stability. We present two algorithms: one based on gradient descent and another based on coordinate ascent. Our approach takes advantage of randomization to reduce noise and improve performance, and includes regularization terms to enforce desired properties such as nonnegativity and sparsity. We demonstrate the effectiveness of our methods through experiments on several benchmark datasets, showing improvement over state-of-the-art techniques in terms of accuracy, robustness, and interpretability.",1
"To learn intrinsic low-dimensional structures from high-dimensional data that most discriminate between classes, we propose the principle of Maximal Coding Rate Reduction ($\text{MCR}^2$), an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class. We clarify its relationships with most existing frameworks such as cross-entropy, information bottleneck, information gain, contractive and contrastive learning, and provide theoretical guarantees for learning diverse and discriminative features. The coding rate can be accurately computed from finite samples of degenerate subspace-like distributions and can learn intrinsic representations in supervised, self-supervised, and unsupervised settings in a unified manner. Empirically, the representations learned using this principle alone are significantly more robust to label corruptions in classification than those using cross-entropy, and can lead to state-of-the-art results in clustering mixed data from self-learned invariant features.",0
"This abstract describes a novel method for learning diverse and discriminative representations, which we call Maximally Contrastive Learning (MCL). MCL is based on minimizing a bound on the mutual information between latent features and data by maximizing the coding rate reduction achieved through contrastive feature training. By using adversarial loss to enforce explicit control over the latent space, MCL effectively learns both distinctive and informative features without additional supervision. Extensive experiments across multiple datasets demonstrate that our approach significantly improves generalization performance while maintaining low annotation costs relative to other methods. Overall, MCL provides a principled framework for generating high quality representations from large scale datasets that achieve stateof-theart results at far lower computational cost than traditional methods.",1
"Many tasks in computer vision are often calibrated and evaluated relative to human perception. In this paper, we propose to directly approximate the perceptual function performed by human observers completing a visual detection task. Specifically, we present a novel methodology for learning to detect image transformations visible to human observers through approximating perceptual thresholds. To do this, we carry out a subjective two-alternative forced-choice study to estimate perceptual thresholds of human observers detecting local exposure shifts in images. We then leverage transformation equivariant representation learning to overcome issues of limited perceptual data. This representation is then used to train a dense convolutional classifier capable of detecting local suprathreshold exposure shifts - a distortion common to image composites. In this context, our model can approximate perceptual thresholds with an average error of 0.1148 exposure stops between empirical and predicted thresholds. It can also be trained to detect a range of different local transformations.",0
"This could lead to confusion if someone should copy parts of your text into their own. Try using a descriptive first sentence instead. Also make the body paragraphs of at least two sentences each.  Abstract  A method has been developed to estimate human perceptual thresholds for detecting suprathreshold transformations applied to natural images. By comparing detection performance on a psychophysics task across multiple observers, we can reliably predict how difficult different transformation categories would be for humans to perceive at arbitrary image quality levels. We demonstrate that our model outperforms several existing methods from computer vision literature for estimating thresholds, including mean opinion scores (MOS), structural similarity index (SSIM) values, feature matching errors, and frequency domain measures such as peak signal-to-noise ratio (PSNR). Our findings suggest that observer differences may play a crucial role in explaining variability across existing threshold estimation models, even after accounting for algorithmic choice and calibration procedures. These results have important implications for understanding mechanisms underlying perception of real-world scenes and improving metrics used in machine learning tasks that require accurate prediction of human perception.",1
"With the dramatic increase of dimensions in the data representation, extracting latent low-dimensional features becomes of the utmost importance for efficient classification. Aiming at the problems of unclear margin representation and difficulty in revealing the data manifold structure in most of the existing linear discriminant methods, we propose a new discriminant feature extraction framework, namely Robust Locality-Aware Regression (RLAR). In our model, we introduce a retargeted regression to perform the marginal representation learning adaptively instead of using the general average inter-class margin. Besides, we formulate a new strategy for enhancing the local intra-class compactness of the data manifold, which can achieve the joint learning of locality-aware graph structure and desirable projection matrix. To alleviate the disturbance of outliers and prevent overfitting, we measure the regression term and locality-aware term together with the regularization term by the L2,1 norm. Further, forcing the row sparsity on the projection matrix through the L2,1 norm achieves the cooperation of feature selection and feature extraction. Then, we derive an effective iterative algorithm for solving the proposed model. The experimental results over a range of UCI data sets and other benchmark databases demonstrate that the proposed RLAR outperforms some state-of-the-art approaches.",0
"Local regression algorithms are used to model complex data dependencies by estimating smooth functions from training points scattered throughout the input space. These models can then be used as surrogates of expensive black box function evaluations for optimization tasks such as calibration curves fitting, classification, sensor placement, etc. Although these methods have proven successful in many applications, they suffer from several limitations: (i) their sensitivity to outliers; (ii) their inability to capture discontinuities; (iii) their tendency to overfit high density regions while underfitting low density areas; and (iv) difficulty recovering multimodal relationships from scarce labeled training samples due to lack of robustness to label noise. This paper proposes a new method called Locally Weighted Smoothing Kernel Machines (LWSKM), which addresses these issues by casting local regression into a kernel framework where multiple weight vectors reflect the varying influence that each neighborhood has on different parts of the predictor hyperspace. In addition, we develop a novel regularizer based on geometric measures to promote piecewise linearity of the learned model without explicitly imposing constraints on it. Our approach provides competitive performance across diverse benchmark datasets against state-of-the-art alternatives including locally weighted projection, support vector machines, random kernels matrices, etc., thereby validating our theoretical claims made earlier regarding the impacts of distinct challenges faced by local regression techniques on prediction quality. Our study underscores the importance of considering distribution geometry during learning procedures and suggests that explicit manipulation of local influence can significantly enhance generalization capacity of machine lea",1
"The adaptive processing of graph data is a long-standing research topic which has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is designed as a tutorial introduction to the field of deep learning for graphs. It favours a consistent and progressive introduction of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view to the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. It introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. The methodological exposition is complemented by a discussion of interesting research challenges and applications in the field.",0
"Here's an example abstract:  ""This study introduces deep learning techniques for graph data structures, which have become increasingly important as networks form the backbone of many natural language processing applications. We focus on developing methods that can learn complex feature representations from graphs, allowing them to capture nonlinear relationships between nodes without relying on hand-engineered features. Our approach uses autoencoders to iteratively extract high-dimensional node embeddings that encode structural properties of the graph while preserving latent semantics. Experiments using several benchmark datasets demonstrate the effectiveness of our method compared against other state-of-the-art approaches.""",1
"In recent years, we have witnessed a surge of interest in multi-view representation learning, which is concerned with the problem of learning representations of multi-view data. When facing multiple views that are highly related but sightly different from each other, most of existing multi-view methods might fail to fully integrate multi-view information. Besides, correlations between features from multiple views always vary seriously, which makes multi-view representation challenging. Therefore, how to learn appropriate embedding from multi-view information is still an open problem but challenging. To handle this issue, this paper proposes a novel multi-view learning method, named Multi-view Low-rank Preserving Embedding (MvLPE). It integrates different views into one centroid view by minimizing the disagreement term, based on distance or similarity matrix among instances, between the centroid view and each view meanwhile maintaining low-rank reconstruction relations among samples for each view, which could make more full use of compatible and complementary information from multi-view features. Unlike existing methods with additive parameters, the proposed method could automatically allocate a suitable weight for each view in multi-view information fusion. However, MvLPE couldn't be directly solved, which makes the proposed MvLPE difficult to obtain an analytic solution. To this end, we approximate this solution based on stationary hypothesis and normalization post-processing to efficiently obtain the optimal solution. Furthermore, an iterative alternating strategy is provided to solve this multi-view representation problem. The experiments on six benchmark datasets demonstrate that the proposed method outperforms its counterparts while achieving very competitive performance.",0
"In recent years there has been considerable interest in multi-view representation due to its ability to provide a richer understanding of complex data sets by capturing complementary information across multiple views. However, traditional methods used for multi-view embedding have several limitations such as high computation cost, low rank preservation, and limited scalability. To address these challenges, we propose a novel method called Multi-view Low-rank Preserving Embedding (MLPE). Our approach adopts a compact and efficient framework that effectively represents each view through low-dimensional embeddings while ensuring low rank preservation. MLPE employs a set of simple linear models in conjunction with gradient descent optimization techniques to achieve fast convergence. Experimental results on a range of benchmark datasets demonstrate the effectiveness of our proposed method compared to state-of-the-art approaches in terms of accuracy and efficiency. Our work contributes towards bridging the gap between performance and computational complexity required for large scale multi-view applications.",1
"Graph Convolutional Networks (GCNs) have been drawing significant attention with the power of representation learning on graphs. Unlike Convolutional Neural Networks (CNNs), which are able to take advantage of stacking very deep layers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting issues when going deeper. These challenges limit the representation power of GCNs on large-scale graphs. This paper proposes DeeperGCN that is capable of successfully and reliably training very deep GCNs. We define differentiable generalized aggregation functions to unify different message aggregation operations (e.g. mean, max). We also propose a novel normalization layer namely MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) show DeeperGCN significantly boosts performance over the state-of-the-art on the large scale graph learning tasks of node property prediction and graph property prediction. Please visit https://www.deepgcns.org for more information.",0
"Here is a draft. Let me know if you need changes:  Recent advancements in graph convolutional networks (GCNs) have demonstrated their effectiveness in addressing problems such as node classification, link prediction, and anomaly detection on graphs. However, training deeper GCN models often leads to overfitting due to high variances in gradients caused by skip connections between every two layers. To tackle this challenge, we propose a novel technique called DeeperGCN that enables efficient training of deep GCN architectures without sacrificing model accuracy. Our proposed method employs noise injection during backpropagation to stabilize gradient flow, leading to improved generalization performance across datasets. We showcase our approach through extensive experiments on six benchmark graphs from various domains and demonstrate its superiority compared to state-of-the-art baseline methods. Additionally, we provide insights into how different hyperparameters affect the stability and performance of deep GCN models using ablation studies and visualizations. With our contributions, researchers can build more powerful GCN models capable of handling complex real-world applications with ease.",1
"As a crucial component in intelligent transportation systems, traffic flow prediction has recently attracted widespread research interest in the field of artificial intelligence (AI) with the increasing availability of massive traffic mobility data. Its key challenge lies in how to integrate diverse factors (such as temporal rules and spatial dependencies) to infer the evolution trend of traffic flow. To address this problem, we propose a unified neural network called Attentive Traffic Flow Machine (ATFM), which can effectively learn the spatial-temporal feature representations of traffic flow with an attention mechanism. In particular, our ATFM is composed of two progressive Convolutional Long Short-Term Memory (ConvLSTM \cite{xingjian2015convolutional}) units connected with a convolutional layer. Specifically, the first ConvLSTM unit takes normal traffic flow features as input and generates a hidden state at each time-step, which is further fed into the connected convolutional layer for spatial attention map inference. The second ConvLSTM unit aims at learning the dynamic spatial-temporal representations from the attentionally weighted traffic flow features. Further, we develop two deep learning frameworks based on ATFM to predict citywide short-term/long-term traffic flow by adaptively incorporating the sequential and periodic data as well as other external influences. Extensive experiments on two standard benchmarks well demonstrate the superiority of the proposed method for traffic flow prediction. Moreover, to verify the generalization of our method, we also apply the customized framework to forecast the passenger pickup/dropoff demands in traffic prediction and show its superior performance. Our code and data are available at {\color{blue}\url{https://github.com/liulingbo918/ATFM}}.",0
"This study presents a new approach to traffic flow prediction that leverages dynamic spatial-temporal representation learning. Our method utilizes convolutional neural networks (CNNs) to learn representations of both static road network features and temporal changes in traffic patterns. We demonstrate through extensive experiments on real-world datasets that our approach outperforms state-of-the-art methods by a significant margin across various evaluation metrics. Furthermore, we show how incorporating additional contextual information such as weather forecasts can further improve prediction accuracy. Our work has important implications for intelligent transportation systems and urban planning, enabling more efficient use of resources and improved mobility for commuters.",1
"Recent years have witnessed the remarkable progress of applying deep learning models in video person re-identification (Re-ID). A key factor for video person Re-ID is to effectively construct discriminative and robust video feature representations for many complicated situations. Part-based approaches employ spatial and temporal attention to extract representative local features. While correlations between parts are ignored in the previous methods, to leverage the relations of different parts, we propose an innovative adaptive graph representation learning scheme for video person Re-ID, which enables the contextual interactions between relevant regional features. Specifically, we exploit the pose alignment connection and the feature affinity connection to construct an adaptive structure-aware adjacency graph, which models the intrinsic relations between graph nodes. We perform feature propagation on the adjacency graph to refine regional features iteratively, and the neighbor nodes' information is taken into account for part feature representation. To learn compact and discriminative representations, we further propose a novel temporal resolution-aware regularization, which enforces the consistency among different temporal resolutions for the same identities. We conduct extensive evaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID, experimental results achieve the competitive performance which demonstrates the effectiveness of our proposed method. The code is available at https://github.com/weleen/AGRL.pytorch.",0
"In this work we introduce a novel adaptive graph representation learning method for video person re-identification (ReID) tasks, which can dynamically learn informative representations from spatial configurations at different levels of abstraction. Our proposed approach takes advantage of both global features and local appearance details by jointly learning feature maps guided by a task specific attention map. By explicitly capturing interdependencies among multiple scales and channels, our model is capable of encoding discriminative visual cues that are difficult to obtain through fixed multi-scale or channel-wise processing alone. Additionally, a lightweight feature aggregation scheme further improves performance while reducing computational complexity. We extensively evaluate our approach on several large scale public datasets and achieve state-of-the-art results. We believe that our findings provide insights into the challenges associated with developing effective video ReID systems and demonstrate the potential benefits of incorporating hierarchical relationships into graph representation learning.",1
"In real-world classification problems, pairwise supervision (i.e., a pair of patterns with a binary label indicating whether they belong to the same class or not) can often be obtained at a lower cost than ordinary class labels. Similarity learning is a general framework to utilize such pairwise supervision to elicit useful representations by inferring the relationship between two data points, which encompasses various important preprocessing tasks such as metric learning, kernel learning, graph embedding, and contrastive representation learning. Although elicited representations are expected to perform well in downstream tasks such as classification, little theoretical insight has been given in the literature so far. In this paper, we reveal that a specific formulation of similarity learning is strongly related to the objective of binary classification, which spurs us to learn a binary classifier without ordinary class labels---by fitting the product of real-valued prediction functions of pairwise patterns to their similarity. Our formulation of similarity learning does not only generalize many existing ones, but also admits an excess risk bound showing an explicit connection to classification. Finally, we empirically demonstrate the practical usefulness of the proposed method on benchmark datasets.",0
"In similarity-based classification (SBC), the relationship between objects plays as crucial role as their individual properties. By defining the measure of how similar two instances belong together, SBC offers an alternative approach to conventional methods such as logistic regression, support vector machines, decision trees etc., which mainly rely on identifying discriminative features that distinguish classes from one another. This study shows how by considering pairs of instances instead of single instance alone and exploiting the similarity relations among them enables us to perform effective binary classification tasks over different types of data formats ranging from images, time series signals to text documents. Our experiments demonstrate that even though classical binary classifiers outperform our method in some cases, they can only achieve competitive performance under specific circumstances if the learning algorithm has access to vast amount of labeled samples while facing new unseen test sets. More importantly, we further show that our proposed SBC model generalizes better when encountering novel situations by adaptively updating similarity measures. Therefore, incorporating similarity learning into the pipeline can serve as a viable solution for low label rate problems where collecting sufficient training examples is impractical but semantic relatedness between samples can still be leveraged effectively.",1
"The recent GRAPH-BERT model introduces a new approach to learning graph representations merely based on the attention mechanism. GRAPH-BERT provides an opportunity for transferring pre-trained models and learned graph representations across different tasks within the same graph dataset. In this paper, we will further investigate the graph-to-graph transfer of a universal GRAPH-BERT for graph representation learning across different graph datasets, and our proposed model is also referred to as the G5 for simplicity. Many challenges exist in learning G5 to adapt the distinct input and output configurations for each graph data source, as well as the information distributions differences. G5 introduces a pluggable model architecture: (a) each data source will be pre-processed with a unique input representation learning component; (b) each output application task will also have a specific functional component; and (c) all such diverse input and output components will all be conjuncted with a universal GRAPH-BERT core component via an input size unification layer and an output representation fusion layer, respectively.   The G5 model removes the last obstacle for cross-graph representation learning and transfer. For the graph sources with very sparse training data, the G5 model pre-trained on other graphs can still be utilized for representation learning with necessary fine-tuning. What's more, the architecture of G5 also allows us to learn a supervised functional classifier for data sources without any training data at all. Such a problem is also named as the Apocalypse Learning task in this paper. Two different label reasoning strategies, i.e., Cross-Source Classification Consistency Maximization (CCCM) and Cross-Source Dynamic Routing (CDR), are introduced in this paper to address the problem.",0
"In this work, we present G5, a powerful graph neural network that can leverage structured knowledge from a source domain to significantly improve zero shot generalization across domains in transfer learning. We showcase the effectiveness of our approach through extensive experimental evaluations on six benchmark datasets across three application areas, including natural language understanding, computer vision, and autonomous driving. Our results demonstrate significant improvement over existing state-of-the-art methods by large margins, achieving new levels of accuracy across all tasks. Furthermore, our ablation studies highlight the importance of different components within our model architecture, which enables us to identify key insights into how these components contribute to overall performance improvements. These contributions make our research highly relevant for real world applications where data availability may be limited but there exists rich structural knowledge about underlying patterns among related domains.",1
"The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.",0
"Label smoothing has become increasingly popular as a method for training machine learning models on imbalanced datasets. However, there remains some debate over whether label smoothing always leads to improved model performance. In this study, we investigate under which conditions label smoothing may be effective, by examining how different amounts of class overlap impact its effectiveness. We find that label smoothing can lead to significant improvements in model accuracy when there is low class overlap, but may actually decrease accuracy when there is high class overlap. These results suggest that while label smoothing can be beneficial, its application should be carefully considered based on the specific characteristics of the dataset at hand. This research provides valuable insights into the use of label smoothing as a tool for addressing class imbalance and improving model performance.",1
"This paper tackles face recognition in videos employing metric learning methods and similarity ranking models. The paper compares the use of the Siamese network with contrastive loss and Triplet Network with triplet loss implementing the following architectures: Google/Inception architecture, 3D Convolutional Network (C3D), and a 2-D Long short-term memory (LSTM) Recurrent Neural Network. We make use of still images and sequences from videos for training the networks and compare the performances implementing the above architectures. The dataset used was the YouTube Face Database designed for investigating the problem of face recognition in videos. The contribution of this paper is two-fold: to begin, the experiments have established 3-D Convolutional networks and 2-D LSTMs with the contrastive loss on image sequences do not outperform Google/Inception architecture with contrastive loss in top $n$ rank face retrievals with still images. However, the 3-D Convolution networks and 2-D LSTM with triplet Loss outperform the Google/Inception with triplet loss in top $n$ rank face retrievals on the dataset; second, a Support Vector Machine (SVM) was used in conjunction with the CNNs' learned feature representations for facial identification. The results show that feature representation learned with triplet loss is significantly better for n-shot facial identification compared to contrastive loss. The most useful feature representations for facial identification are from the 2-D LSTM with triplet loss. The experiments show that learning spatio-temporal features from video sequences is beneficial for facial recognition in videos.",0
"Facial recognition technology has become increasingly important in recent years due to its ability to enhance security measures and provide valuable insights into customer behavior across different industries. While traditional facial recognition systems have proven effective in still images, they often struggle with identifying faces in videos that involve significant motion, occlusions, lighting changes, and other challenges. To address these limitations, researchers propose using deep learning models specifically designed for video-based face recognition (VBFR) tasks. These models can effectively capture temporal features, such as short-term movements and subtle changes in expression, and achieve higher accuracy than their static image counterparts. This review examines existing VBFR techniques, evaluates their performance on benchmark datasets, highlights promising directions for future work, and discusses potential applications in real-world scenarios. By improving the robustness and reliability of facial recognition systems in video settings, we can advance the development of intelligent surveillance systems that enable better security, monitoring, and analysis in diverse environments.",1
"We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: https://github.com/kavehhassani/mvgrl",0
"In recent years, graph neural networks (GNN) have emerged as powerful tools for representing complex relational data such as social networks, knowledge graphs, and biological systems. However, these models can suffer from oversmoothing, where node representations become homogeneous across layers. To address this issue, contrastive learning has been introduced as a means of disentangling node representation by maximizing their mutual agreement within one view, while minimizing their agreement with other views generated from perturbations of the input graph. This work proposes contrastive multi-view representation learning on graphs that involves multiple augmented versions of each original graph instance with corresponding supervised signals. Experimental results demonstrate that our method achieves state-of-the-art performance across several benchmark datasets, significantly improving upon existing approaches based solely on intrinsic data augmentation or supervised pretraining. By leveraging external complementary sources of information, we propose a more effective approach towards enhancing GNN robustness against structural variations observed in real-world scenarios.",1
"Canonical Correlation Analysis (CCA) is a statistical technique used to extract common information from multiple data sources or views. It has been used in various representation learning problems, such as dimensionality reduction, word embedding, and clustering. Recent work has given CCA probabilistic footing in a deep learning context and uses a variational lower bound for the data log likelihood to estimate model parameters. Alternatively, adversarial techniques have arisen in recent years as a powerful alternative to variational Bayesian methods in autoencoders. In this work, we explore straightforward adversarial alternatives to recent work in Deep Variational CCA (VCCA and VCCA-Private) we call ACCA and ACCA-Private and show how these approaches offer a stronger and more flexible way to match the approximate posteriors coming from encoders to much larger classes of priors than the VCCA and VCCA-Private models. This allows new priors for what constitutes a good representation, such as disentangling underlying factors of variation, to be more directly pursued. We offer further analysis on the multi-level disentangling properties of VCCA-Private and ACCA-Private through the use of a newly designed dataset we call Tangled MNIST. We also design a validation criteria for these models that is theoretically grounded, task-agnostic, and works well in practice. Lastly, we fill a minor research gap by deriving an additional variational lower bound for VCCA that allows the representation to use view-specific information from both input views.",0
"Adversarial Canonical Correlation Analysis (ACCA) is a novel framework that enables us to learn deep neural network models which jointly maximize discriminative power for multi-label classification tasks while preserving label correspondence across domains. We introduce two losses - domain loss and attribute loss. Domain loss minimizes difference of marginal distributions between corresponding pairs by aligning them either positively or negatively depending on application requirements, while attribute loss ensures one-to-one mapping between target labels across domains. By alternating optimization between these two losses we can achieve optimal adversarial alignment between multiple datasets while jointly learning feature representations and classifiers. Experiments show significant improvement over strong baselines on multiple benchmark datasets as well as several real world problems such as land cover maps generated from different sensors. Furthermore, we visualize learned features and find they correspond intuitively to semantic concepts for respective application scenarios like object categories or vegetation types, demonstrating potential benefits for understanding representation learning in deep networks better. Our results indicate broad applicability to many other multi-domain transfer problems beyond image recognition and we hope our work encourages further research into developing more comprehensive theories around unifying all forms of cross-modal correspondences under adversarial training umbrella termed here as 'adversarial correlation analysis'.",1
"Disentangled representation learning has recently attracted a significant amount of attention, particularly in the field of image representation learning. However, learning the disentangled representations behind a graph remains largely unexplored, especially for the attributed graph with both node and edge features. Disentanglement learning for graph generation has substantial new challenges including 1) the lack of graph deconvolution operations to jointly decode node and edge attributes; and 2) the difficulty in enforcing the disentanglement among latent factors that respectively influence: i) only nodes, ii) only edges, and iii) joint patterns between them. To address these challenges, we propose a new disentanglement enhancement framework for deep generative models for attributed graphs. In particular, a novel variational objective is proposed to disentangle the above three types of latent factors, with novel architecture for node and edge deconvolutions. Moreover, within each type, individual-factor-wise disentanglement is further enhanced, which is shown to be a generalization of the existing framework for images. Qualitative and quantitative experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed model and its extensions.",0
"In recent years, deep graph generation has emerged as a promising direction for synthesizing structured data, such as social networks, molecular graphs, and knowledge graphs. However, most existing approaches suffer from interpretability issues due to their black box nature. To address these challenges, we propose a novel approach called Interpretable Deep Graph Generation (IDGG) which leverages node-edge co-disentanglement techniques to enable explicit control over the latent representations of both nodes and edges during training. Our method can generate high quality graphs that exhibit desirable statistical characteristics while retaining important interpretable features. We evaluate our model on a range of tasks including generating random graphs, reproducing real-world graphs, and controlling specific properties through fine-grained manipulation of the generator parameters. Experimental results show significant improvements compared to previous state-of-the-art methods in terms of accuracy, efficiency, scalability and robustness, making IDGG a powerful toolkit for graph analysis and understanding complex systems.",1
"Generating interpretable visualizations from complex data is a common problem in many applications. Two key ingredients for tackling this issue are clustering and representation learning. However, current methods do not yet successfully combine the strengths of these two approaches. Existing representation learning models which rely on latent topological structure such as self-organising maps, exhibit markedly lower clustering performance compared to recent deep clustering methods. To close this performance gap, we (a) present a novel way to fit self-organizing maps with probabilistic cluster assignments (PSOM), (b) propose a new deep architecture for probabilistic clustering (DPSOM) using a VAE, and (c) extend our architecture for time-series clustering (T-DPSOM), which also allows forecasting in the latent space using LSTMs. We show that DPSOM achieves superior clustering performance compared to current deep clustering methods on MNIST/Fashion-MNIST, while maintaining the favourable visualization properties of SOMs. On medical time series, we show that T-DPSOM outperforms baseline methods in time series clustering and time series forecasting, while providing interpretable visualizations of patient state trajectories and uncertainty estimation.",0
"This would be an excellent opportunity to showcase your writing skills as you write an abstract for an academic paper. Here are some key points that should be included: Title: ""DPSOM: Deep Probabilistic Clustering with Self-Organizing Maps"" Authors: [Your name], [Co-author names] Publication Venue: International Conference on Machine Learning (ICML) In recent years, there has been growing interest in the use of deep neural networks for clustering tasks, due to their ability to capture complex patterns in high-dimensional data. However, many traditional clustering algorithms suffer from limitations such as sensitivity to initial conditions and poor robustness to noise. To address these challenges, we propose a novel approach called Deep Probabilistic Clustering with Self-Organizing Maps (DPSOM), which combines self-organizing maps with probabilistic modeling techniques to enable efficient exploration of the cluster landscape while maintaining computational scalability. Our method leverages Bayesian inference to adaptively construct a hierarchical representation of clusters using a grid of self-organizing maps. We demonstrate the effectiveness of our approach by comparing its performance against state-of-the-art clustering methods across multiple benchmark datasets, showing improved accuracy and robustness under noisy conditions. By combining the strengths of both global optimization and local search techniques, DPSOM offers a promising new direction for clustering research. This work represents a significant advance in unlocking the potential of deep learning models for real-world applications involving clustering problems.",1
"We present a novel architecture named Neural Physicist (NeurPhy) to learn physical dynamics directly from image sequences using deep neural networks. For any physical system, given the global system parameters, the time evolution of states is governed by the underlying physical laws. How to learn meaningful system representations in an end-to-end way and estimate accurate state transition dynamics facilitating long-term prediction have been long-standing challenges. In this paper, by leveraging recent progresses in representation learning and state space models (SSMs), we propose NeurPhy, which uses variational auto-encoder (VAE) to extract underlying Markovian dynamic state at each time step, neural process (NP) to extract the global system parameters, and a non-linear non-recurrent stochastic state space model to learn the physical dynamic transition. We apply NeurPhy to two physical experimental environments, i.e., damped pendulum and planetary orbits motion, and achieve promising results. Our model can not only extract the physically meaningful state representations, but also learn the state transition dynamics enabling long-term predictions for unseen image sequences. Furthermore, from the manifold dimension of the latent state space, we can easily identify the degree of freedom (DoF) of the underlying physical systems.",0
"Title: ""Learning Physical Dynamics from Image Sequences""  This paper presents a novel approach for learning physical dynamics by leveraging image sequences obtained through computer vision techniques. By using deep neural networks that have been specifically trained on large amounts of data, we show how it is possible to accurately capture and predict both short-term and long-term changes in dynamic systems. Our methodology involves preprocessing the raw images into usable representations, feeding them into our model, and then utilizing the outputs to make predictions about future states of motion. We demonstrate the effectiveness of our approach through experiments conducted on several different types of datasets, including videos of humans performing actions such as walking or running, as well as footage of objects undergoing rigid body transformations like rotations or translations. Overall, our work shows promising results in enabling machines to learn complex physics models based solely on visual input, which could potentially find applications across many domains, ranging from robot control to virtual reality simulation.",1
"Variational autoencoders (VAEs) are a powerful class of deep generative latent variable model for unsupervised representation learning on high-dimensional data. To ensure computational tractability, VAEs are often implemented with a univariate standard Gaussian prior and a mean-field Gaussian variational posterior distribution. This results in a vector-valued latent variables that are agnostic to the original data structure which might be highly correlated across and within multiple dimensions. We propose a tensor-variate extension to the VAE framework, the tensor-variate Gaussian process prior variational autoencoder (tvGP-VAE), which replaces the standard univariate Gaussian prior and posterior distributions with tensor-variate Gaussian processes. The tvGP-VAE is able to explicitly model correlation structures via the use of kernel functions over the dimensions of tensor-valued latent variables. Using spatiotemporally correlated image time series as an example, we show that the choice of which correlation structures to explicitly represent in the latent space has a significant impact on model performance in terms of reconstruction.",0
"tvGP-VAE: Tensor-variate Gaussian Process Pri ---  This paper presents a new deep learning model called ""tvGP-VAE"" that combines the benefits of tensor-variate Gaussian processes (TVGPP) and variational autoencoders (VAEs). TVGPs have been shown to effectively model complex relationships between inputs, making them well suited for tasks such as regression, classification, and dimensionality reduction. VAEs have gained popularity due to their ability to learn a compressed representation of input data while maintaining the structure of the original space. However, traditional VAEs suffer from a lack of interpretability and difficulty in handling high dimensions. By combining these two approaches, we aim to address some of these limitations and enhance the capabilities of VAEs.  The proposed tvGP-VAE uses a tensor-variate GP prior over latent variables, which allows us to capture complex relationships across different modalities or dimensions. We use amortized inference methods to make the model computationally efficient and scalable, enabling large-scale applications. To evaluate our approach, we conduct extensive experiments on several benchmark datasets across diverse domains including image generation, video prediction, and dimensionality reduction. Results show that our method outperforms state-of-the-art baselines in terms of fidelity and coherency, demonstrating the effectiveness of using TVGPPs within VAEs. Our findings highlight the potential of integrating Bayesian nonparametric models into deep generative models for improved performance.",1
"Deep representation learning on non-Euclidean data types, such as graphs, has gained significant attention in recent years. Invent of graph neural networks has improved the state-of-the-art for both node and the entire graph representation in a vector space. However, for the entire graph representation, most of the existing graph neural networks are trained on a graph classification loss in a supervised way. But obtaining labels of a large number of graphs is expensive for real world applications. Thus, we aim to propose an unsupervised graph neural network to generate a vector representation of an entire graph in this paper. For this purpose, we combine the idea of hierarchical graph neural networks and mutual information maximization into a single framework. We also propose and use the concept of periphery representation of a graph and show its usefulness in the proposed algorithm which is referred as GraPHmax. We conduct thorough experiments on several real-world graph datasets and compare the performance of GraPHmax with a diverse set of both supervised and unsupervised baseline algorithms. Experimental results show that we are able to improve the state-of-the-art for multiple graph level tasks on several real-world datasets, while remain competitive on the others.",0
"In this paper we propose the first method which can learn graph representations from unlabelled data alone in a principled manner via minimizing reconstruction loss within denoising autoencoders (AEs). Previous attempts on learning graphs required supervision [2]. Our approach works by first clustering nodes based on their local connectivity profile and then refining clusters iteratively using global hierarchical information maximisation constraints [6]. An intermediate representation during each iteration of our algorithm gives rise to interpretable subgraphs, furthermore allowing us to visualise learned graphs post training. Results show that UGReMI achieves state-of-the art performance across all benchmark datasets used without requiring any annotations.[/quote]",1
"Graph Attention Network (GAT) and GraphSAGE are neural network architectures that operate on graph-structured data and have been widely studied for link prediction and node classification. One challenge raised by GraphSAGE is how to smartly combine neighbour features based on graph structure. GAT handles this problem through attention, however the challenge with GAT is its scalability over large and dense graphs. In this work, we proposed a new architecture to address these issues that is more efficient and is capable of incorporating different edge type information. It generates node representations by attending to neighbours sampled from weighted multi-step transition probabilities. We conduct experiments on both transductive and inductive settings. Experiments achieved comparable or better results on several graph benchmarks, including the Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.",0
"Abstract: This paper presents a novel approach for graph representation learning that utilizes adaptive sampling techniques to improve performance on downstream tasks. The proposed method addresses key limitations of existing graph neural networks (GNNs) by incorporating more informative edges into the graph during training. By leveraging local edge scores calculated using node features and neighborhood aggregates, our method can dynamically select which edges to retain, discard, or add based on their importance. We show that our adaptive sampling scheme leads to improved accuracy on several benchmark datasets compared to baseline models without any additional parameters. Our work highlights the significance of using informed edge selection strategies to enhance GNN representations, paving the way for further advancements in graph machine learning research.",1
"The remarkable success of machine learning has fostered a growing number of cloud-based intelligent services for mobile users. Such a service requires a user to send data, e.g. image, voice and video, to the provider, which presents a serious challenge to user privacy. To address this, prior works either obfuscate the data, e.g. add noise and remove identity information, or send representations extracted from the data, e.g. anonymized features. They struggle to balance between the service utility and data privacy because obfuscated data reduces utility and extracted representation may still reveal sensitive information.   This work departs from prior works in methodology: we leverage adversarial learning to a better balance between privacy and utility. We design a \textit{representation encoder} that generates the feature representations to optimize against the privacy disclosure risk of sensitive information (a measure of privacy) by the \textit{privacy adversaries}, and concurrently optimize with the task inference accuracy (a measure of utility) by the \textit{utility discriminator}. The result is the privacy adversarial network (\systemname), a novel deep model with the new training algorithm, that can automatically learn representations from the raw data.   Intuitively, PAN adversarially forces the extracted representations to only convey the information required by the target task. Surprisingly, this constitutes an implicit regularization that actually improves task accuracy. As a result, PAN achieves better utility and better privacy at the same time! We report extensive experiments on six popular datasets and demonstrate the superiority of \systemname compared with alternative methods reported in prior work.",0
"Title: ""Privacy Adversarial Network: Representation Learning for Mobile Data Privacy"" Authors: [List authors], Affiliations: [Institutions]  In recent years, there has been growing concern over the privacy risks associated with mobile data collection and processing by third parties. These concerns have driven researchers to explore new approaches that can balance user privacy requirements against the need for high-quality machine learning models on mobile devices. In response, we propose a novel framework called Privacy Adversarial Network (PipeNet) which combines adversarial representation learning techniques with gradient masking strategies to prevent private data leakage during model training and inference. Our proposed approach leverages multiple neural networks trained jointly through multi-task optimization, where one network focuses on predicting sensitive information while others learn specific tasks. By doing so, PipeNet learns representations that encode task-specific features without revealing sensitive details, ensuring both effectiveness and robustness in real-world applications such as healthcare monitoring and location services. We evaluate our method using several benchmark datasets across different use cases and demonstrate significant improvement compared to baseline methods regarding both model performance and privacy protection metrics, making PipeNet a promising solution for enhancing personal data security in future mobile systems.",1
"Graph neural networks (GNNs) have attracted much attention because of their excellent performance on tasks such as node classification. However, there is inadequate understanding on how and why GNNs work, especially for node representation learning. This paper aims to provide a theoretical framework to understand GNNs, specifically, spectral graph convolutional networks and graph attention networks, from graph signal denoising perspectives. Our framework shows that GNNs are implicitly solving graph signal denoising problems: spectral graph convolutions work as denoising node features, while graph attentions work as denoising edge weights. We also show that a linear self-attention mechanism is able to compete with the state-of-the-art graph attention methods. Our theoretical results further lead to two new models, GSDN-F and GSDN-EF, which work effectively for graphs with noisy node features and/or noisy edges. We validate our theoretical findings and also the effectiveness of our new models by experiments on benchmark datasets. The source code is available at \url{https://github.com/fuguoji/GSDN}.",0
"Graph signal denoising has recently gained attention as an important problem in graph processing due to its applications in areas such as sensor network analysis, image processing on graphs, and social network analysis. In traditional signal processing, a common approach to address noise contamination is to use linear regression techniques or principal component analysis (PCA). However, these methods fail to capture nonlinear dependencies between variables in graph data, resulting in limited performance. To overcome this limitation, researchers have turned to deep learning-based methods, specifically graph neural networks (GNNs), which can learn complex nonlinear functions that capture interdependencies among signals on a graph.  In this paper, we review recent advances in graph signal denoising using GNNs from both theoretical and empirical perspectives. We provide an overview of popular graph signal denoising algorithms based on GNNs, including their strengths and weaknesses, and discuss key challenges associated with applying GNNs to real-world problems. Moreover, we describe state-of-the-art graph denoising evaluation methodologies and benchmark datasets commonly used for evaluating graph signal denoising approaches, highlighting opportunities for future work.  Overall, our aim is to offer readers a comprehensive understanding of GNN-based graph signal denoising techniques and their current limitations, enabling them to apply these models effectively in practice while identifying potential areas for further improvement. By exploring new directions for graph signal denoising using GNNs, researchers may contribute to cutting-edge solutions for applications ranging from environmental monitoring to biological systems analysis.",1
"Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.",0
"This study explores text representation learning using disentangling techniques and information theory. We investigate how adding an information-theoretic loss function can improve model performance on NLP tasks by reducing entropies related to different aspects of interest in natural language data. Our contributions include introducing Shannon Entropy as guidance during training, which allows us to capture more nuanced differences across texts without increasing model complexity; providing extensive analysis and comparison against prior arts; and demonstrating improved results for our models over competitive baselines. Additionally, we hope that our work encourages researchers to incorporate information theory into future research as a means to gain insight into the characteristics of their trained representations. We believe this novel approach holds promise for many applications in the field of Natural Language Processing. Keywords: text representation learning, disentanglement, information theory, Shannon entropy, natural language processing (NLP), deep learning.",1
"In recent years, several unsupervised, ""contrastive"" learning algorithms in vision have been shown to learn representations that perform remarkably well on transfer tasks. We show that this family of algorithms maximizes a lower bound on the mutual information between two or more ""views"" of an image where typical views come from a composition of image augmentations. Our bound generalizes the InfoNCE objective to support negative sampling from a restricted region of ""difficult"" contrasts. We find that the choice of negative samples and views are critical to the success of these algorithms. Reformulating previous learning objectives in terms of mutual information also simplifies and stabilizes them. In practice, our new objectives yield representations that outperform those learned with previous approaches for transfer to classification, bounding box detection, instance segmentation, and keypoint detection. % experiments show that choosing more difficult negative samples results in a stronger representation, outperforming those learned with IR, LA, and CMC in classification, bounding box detection, instance segmentation, and keypoint detection. The mutual information framework provides a unifying comparison of approaches to contrastive learning and uncovers the choices that impact representation learning.",0
"In recent years, contrastive learning has emerged as one of the most promising approaches to unsupervised representation learning. However, selecting an appropriate negative sampling strategy remains a key challenge in achieving high performance on downstream tasks. Most existing studies use random negatives that have no semantic relationship with the positive examples. This leads to suboptimal representations since the learned representations may overfit the noise introduced by randomly sampled negatives. To address these limitations, we propose using mutual information maximization to guide the selection of hard negatives that share similar semantic meaning as the positive pairs. We show that our method significantly outperforms state-of-the art methods across multiple benchmark datasets while improving robustness against adversarial attacks and distribution shifts. Our results demonstrate that mutual information can serve as a powerful tool for guiding contrastive learning to learn more discriminative representations.",1
"It has witnessed a growing demand for efficient representation learning on point clouds in many 3D computer vision applications. Behind the success story of convolutional neural networks (CNNs) is that the data (e.g., images) are Euclidean structured. However, point clouds are irregular and unordered. Various point neural networks have been developed with isotropic filters or using weighting matrices to overcome the structure inconsistency on point clouds. However, isotropic filters or weighting matrices limit the representation power. In this paper, we propose a permutable anisotropic convolutional operation (PAI-Conv) that calculates soft-permutation matrices for each point using dot-product attention according to a set of evenly distributed kernel points on a sphere's surface and performs shared anisotropic filters. In fact, dot product with kernel points is by analogy with the dot-product with keys in Transformer as widely used in natural language processing (NLP). From this perspective, PAI-Conv can be regarded as the transformer for point clouds, which is physically meaningful and is robust to cooperate with the efficient random point sampling method. Comprehensive experiments on point clouds demonstrate that PAI-Conv produces competitive results in classification and semantic segmentation tasks compared to state-of-the-art methods.",0
"In recent years, point clouds have become increasingly important data structures in computer vision tasks such as object recognition, scene segmentation, and 3D reconstruction. Traditional convolutional neural networks (CNNs) rely on grid-like topologies that may not be well suited for processing unordered point cloud data. To address this challenge, we propose a novel permutation invariant approach based on anisotropic convolutional layers. Our method leverages sparse representation techniques and directional kernels to capture local features while preserving permutation equivariance. Experimental results demonstrate the superior performance of our proposed model compared to other state-of-the-art approaches for learning on point clouds, particularly in terms of accuracy and robustness against varying spatial sampling densities. This work represents a significant step forward in enabling effective deep learning on unstructured geometric data.",1
"The recent success of self-supervised learning can be largely attributed to content-preserving transformations, which can be used to easily induce invariances. While transformations generate positive sample pairs in contrastive loss training, most recent work focuses on developing new objective formulations, and pays relatively little attention to the transformations themselves. In this paper, we introduce the framework of Generalized Data Transformations to (1) reduce several recent self-supervised learning objectives to a single formulation for ease of comparison, analysis, and extension, (2) allow a choice between being invariant or distinctive to data transformations, obtaining different supervisory signals, and (3) derive the conditions that combinations of transformations must obey in order to lead to well-posed learning objectives. This framework allows both invariance and distinctiveness to be injected into representations simultaneously, and lets us systematically explore novel contrastive objectives. We apply it to study multi-modal self-supervision for audio-visual representation learning from unlabelled videos, improving the state-of-the-art by a large margin, and even surpassing supervised pretraining. We demonstrate results on a variety of downstream video and audio classification and retrieval tasks, on datasets such as HMDB-51, UCF-101, DCASE2014, ESC-50 and VGG-Sound. In particular, we achieve new state-of-the-art accuracies of 72.8% on HMDB-51 and 95.2% on UCF-101.",0
"Abstract: This paper presents a method for self-supervised learning using multi-modal data transformations. We show that by applying random transformations to images, audio, and text, we can learn robust representations that generalize well across tasks and domains. Our approach applies a single model to multiple modalities, enabling joint training and ensembling for improved performance. Experiments on benchmark datasets demonstrate state-of-the-art results in multiple downstream tasks including image classification, object detection, and sentiment analysis. We provide comprehensive ablation studies analyzing the impact of each component of our method. Finally, we showcase the utility of our learned features in zero shot transfer learning settings where our model achieves strong performance without any additional fine tuning. Overall, our work provides new insights into efficient self-supervised learning that improves upon prior methods while broadening their scope to multiple modalities.",1
"Deep clustering algorithms combine representation learning and clustering by jointly optimizing a clustering loss and a non-clustering loss. In such methods, a deep neural network is used for representation learning together with a clustering network. Instead of following this framework to improve clustering performance, we propose a simpler approach of optimizing the entanglement of the learned latent code representation of an autoencoder. We define entanglement as how close pairs of points from the same class or structure are, relative to pairs of points from different classes or structures. To measure the entanglement of data points, we use the soft nearest neighbor loss, and expand it by introducing an annealing temperature factor. Using our proposed approach, the test clustering accuracy was 96.2% on the MNIST dataset, 85.6% on the Fashion-MNIST dataset, and 79.2% on the EMNIST Balanced dataset, outperforming our baseline models.",0
"K-means clustering is a widely used unsupervised learning algorithm that partitions data into distinct clusters based on their distances from cluster centroids. However, its performance can often suffer due to issues such as sensitivity to initial conditions and difficulty handling high-dimensional datasets. One approach to address these challenges is through the use of disentangled internal representations (DIR). DIRs represent complex data using a set of simple factors, allowing for more interpretable and explainable results. In addition, they allow for better generalization across different domains by decoupling the underlying features. This work explores how incorporating DIRs into the k-means algorithm can improve its performance, resulting in more accurate and robust clustering outcomes. We present empirical evaluations demonstrating the effectiveness of our approach on both synthetic and real-world datasets. Overall, our findings highlight the potential of utilizing DIRs to enhance traditional clustering algorithms and achieve improved performance.",1
"Humans are able to create rich representations of their external reality. Their internal representations allow for cross-modality inference, where available perceptions can induce the perceptual experience of missing input modalities. In this paper, we contribute the Multimodal Hierarchical Variational Auto-encoder (MHVAE), a hierarchical multimodal generative model for representation learning. Inspired by human cognitive models, the MHVAE is able to learn modality-specific distributions, of an arbitrary number of modalities, and a joint-modality distribution, responsible for cross-modality inference. We formally derive the model's evidence lower bound and propose a novel methodology to approximate the joint-modality posterior based on modality-specific representation dropout. We evaluate the MHVAE on standard multimodal datasets. Our model performs on par with other state-of-the-art generative models regarding joint-modality reconstruction from arbitrary input modalities and cross-modality inference.",0
"This paper presents a novel deep hierarchical generative model called MHVAE (Mixture of Hidden Variables Autoencoder) that can effectively learn multimodal representations inspired by human cognition. Traditional variational autoencoders (VAEs) suffer from several limitations such as poor data efficiency, sensitivity to hyperparameters, difficulty in capturing complex relationships across modalities, and lack of interpretability. To address these issues, we propose MHVAE which introduces two key innovations: (1) a latent variable that models unobserved factors affecting the observed input variables and (2) a hierarchical structure that enables efficient capture of high-level abstractions while preserving fine details. We show through extensive experiments on real world datasets including image and text modalities, that MHVAE outperforms state-of-the-art VAEs in terms of data efficiency, robustness, interpretability, and representation quality. Furthermore, our ablation studies demonstrate the importance of each component introduced in MHVAE and provide insights into the working mechanism of the proposed method. Our work bridges the gap between theoretical advancements in deep learning and practical applications of these techniques for large scale systems, paving the way towards more intelligent machines.",1
"We consider representation learning (hypothesis class $\mathcal{H} = \mathcal{F}\circ\mathcal{G}$) where training and test distributions can be different. Recent studies provide hints and failure examples for domain invariant representation learning, a common approach for this problem, but the explanations provided are somewhat different and do not provide a unified picture. In this paper, we provide new decompositions of risk which give finer-grained explanations and clarify potential generalization issues. For Single-Source Domain Adaptation, we give an exact decomposition (an equality) of the target risk, via a natural hybrid argument, as sum of three factors: (1) source risk, (2) representation conditional label divergence, and (3) representation covariate shift. We derive a similar decomposition for the Multi-Source case. These decompositions reveal factors (2) and (3) as the precise reasons for failure to generalize. For example, we demonstrate that domain adversarial neural networks (DANN) attempt to regularize for (3) but miss (2), while a recent technique Invariant Risk Minimization (IRM) attempts to account for (2) but does not consider (3). We also verify our observations experimentally.",0
"In recent years, multi-source domain adaptation (MSDA) has emerged as an important area of research in computer vision due to its ability to improve generalization performance on unseen target domains. One critical factor that affects MSDA performance is how well we can adapt across multiple sources while minimizing risk accumulation from each source. This study presents a new approach called representation Bayesian risk decompositions (BayesRD), which introduces a principled framework for identifying and controlling risks arising from different sources during adaptation. We use a probabilistic perspective to represent the model uncertainty distribution over both seen and unseen domains, allowing us to quantify their contributions to overall risk. Our method achieves state-of-the-art results on several benchmark datasets, demonstrating its effectiveness at managing risk under challenging conditions. Additionally, our framework provides interpretable insights into the relationship between source and target distributions, enabling further investigation of the factors driving successful adaptation. Overall, our work highlights the potential benefits of incorporating richer representations of uncertainty in order to enhance the reliability and robustness of domain adaptation models in practice.",1
"In recent years, semi-supervised learning (SSL) has shown tremendous success in leveraging unlabeled data to improve the performance of deep learning models, which significantly reduces the demand for large amounts of labeled data. Many SSL techniques have been proposed and have shown promising performance on famous datasets such as ImageNet and CIFAR-10. However, some exiting techniques (especially data augmentation based) are not suitable for industrial applications empirically. Therefore, this work proposes the pseudo-representation labeling, a simple and flexible framework that utilizes pseudo-labeling techniques to iteratively label a small amount of unlabeled data and use them as training data. In addition, our framework is integrated with self-supervised representation learning such that the classifier gains benefits from representation learning of both labeled and unlabeled data. This framework can be implemented without being limited at the specific model structure, but a general technique to improve the existing model. Compared with the existing approaches, the pseudo-representation labeling is more intuitive and can effectively solve practical problems in the real world. Empirically, it outperforms the current state-of-the-art semi-supervised learning methods in industrial types of classification problems such as the WM-811K wafer map and the MIT-BIH Arrhythmia dataset.",0
"In many real world problems, we often encounter data that has class labels only available for limited set of instances, where as some labeled samples are more representative than other unlabeled data points. This setting motivates us to develop methods which can utilize both types of data effectively. We design an algorithm based on pseudo representation learning by formulating it over a graph constructed from embedding features obtained via pre-trained autoencoder network which capture underlying structure of original dataset and further enhance it through a carefully designed label propagation scheme. Our method achieves state of art performance on several benchmark datasets such as MNIST (handwritten digits), CIFAR10 (image classification) etc. Furthermore, experimental results demonstrate that our proposed method outperforms existing algorithms significantly even when availability of labeled data is as low as 2%. Our approach provides a novel perspective on understanding the problem at hand and enables us to use diverse sources of data in an intelligent manner while solving complex tasks at same time",1
"There is an increasing interest in image-to-image translation with applications ranging from generating maps from satellite images to creating entire clothes' images from only contours. In the present work, we investigate image-to-image translation using Generative Adversarial Networks (GANs) for generating new data, taking as a case study the morphing of giraffes images into bird images. Morphing a giraffe into a bird is a challenging task, as they have different scales, textures, and morphology. An unsupervised cross-domain translator entitled InstaGAN was trained on giraffes and birds, along with their respective masks, to learn translation between both domains. A dataset of synthetic bird images was generated using translation from originally giraffe images while preserving the original spatial arrangement and background. It is important to stress that the generated birds do not exist, being only the result of a latent representation learned by InstaGAN. Two subsets of common literature datasets were used for training the GAN and generating the translated images: COCO and Caltech-UCSD Birds 200-2011. To evaluate the realness and quality of the generated images and masks, qualitative and quantitative analyses were made. For the quantitative analysis, a pre-trained Mask R-CNN was used for the detection and segmentation of birds on Pascal VOC, Caltech-UCSD Birds 200-2011, and our new dataset entitled FakeSet. The generated dataset achieved detection and segmentation results close to the real datasets, suggesting that the generated images are realistic enough to be detected and segmented by a state-of-the-art deep neural network.",0
"This study explores the potential of image-to-image translation as a tool for data generation. In particular, we focus on generating bird images from giraffe images using CycleGAN, a state-of-the-art image-to-image translation method. We evaluate the quality of generated images using several metrics, including visual inspection by human raters and quantitative measures such as perceptual loss and Fréchet distance. Our results show that while CycleGAN can produce plausible bird images from giraffe inputs, there are still limitations in terms of both fidelity and diversity. These findings suggest that while image-to-image translation may have some applications in data augmentation, further research is needed to fully realize its potential.",1
"Geometric representation learning has recently shown great promise in several machine learning settings, ranging from relational learning to language processing and generative models. In this work, we consider the problem of performing manifold-valued regression onto an hyperbolic space as an intermediate component for a number of relevant machine learning applications. In particular, by formulating the problem of predicting nodes of a tree as a manifold regression task in the hyperbolic space, we propose a novel perspective on two challenging tasks: 1) hierarchical classification via label embeddings and 2) taxonomy extension of hyperbolic representations. To address the regression problem we consider previous methods as well as proposing two novel approaches that are computationally more advantageous: a parametric deep learning model that is informed by the geodesics of the target space and a non-parametric kernel-method for which we also prove excess risk bounds. Our experiments show that the strategy of leveraging the hyperbolic geometry is promising. In particular, in the taxonomy expansion setting, we find that the hyperbolic-based estimators significantly outperform methods performing regression in the ambient Euclidean space.",0
"In recent years, hyperbolic manifold regression (HMR) has emerged as a powerful tool for data analysis and modeling complex relationships between variables. By leveraging the properties of nonlinear geometry on a high-dimensional curved space, HMR enables efficient representation of datasets with intricate structures that cannot be captured by traditional linear models. This paper presents a comprehensive review of the fundamental principles and current advancements in the field of HMR. We begin with an introduction to the mathematical foundations behind hyperbolic manifolds and their applications in scientific research. Next, we discuss different algorithms and techniques employed in HMR and compare their merits against existing state-of-the-art methods. Finally, we explore several real-world examples across diverse domains such as image recognition, natural language processing, finance, and neuroscience where HMR has achieved significant improvements over conventional approaches. Throughout the paper, we emphasize key challenges facing practitioners who wish to apply HMR in practice along with potential solutions. Overall, our aim is to provide readers interested in machine learning, signal processing, physics, or applied mathematics with a clear understanding of the fundamentals and capabilities of HMR. The insights gleaned from this survey could facilitate future development within and beyond the domain of HMR.",1
"This work analyses the impact of self-supervised pre-training on document images in the context of document image classification. While previous approaches explore the effect of self-supervision on natural images, we show that patch-based pre-training performs poorly on document images because of their different structural properties and poor intra-sample semantic information. We propose two context-aware alternatives to improve performance on the Tobacco-3482 image classification task. We also propose a novel method for self-supervision, which makes use of the inherent multi-modality of documents (image and text), which performs better than other popular self-supervised methods, including supervised ImageNet pre-training, on document image classification scenarios with a limited amount of data.",0
"Machine learning techniques have become increasingly popular over the years due to their ability to solve complex problems using vast amounts of data. One major challenge in applying these methods is finding enough labeled data that fully captures all relevant aspects of a task, which can prove both time-consuming and expensive. Recent advances in self-supervision propose effective solutions to circumvent the need for large amounts of annotated training data by creating synthetic labels from weak supervision sources such as image captions, web queries, or human attention maps. Inspired by these developments, we present a novel framework for self-supervised representation learning that leverages document images. We demonstrate the effectiveness of our method by improving downstream text classification accuracy compared to the state-of-the-art alternatives while using significantly less labeled training data. Our work represents a significant step towards reducing reliance on costly annotations and makes machine learning accessible to new domains requiring high-quality representations for accurate predictions. By exploring more diverse sources of weak supervision signals, researchers could further enhance performance and open up exciting opportunities in areas that remain unexplored today.",1
"With the advent of agriculture 3.0 and 4.0, researchers are increasingly focusing on the development of innovative smart farming and precision agriculture technologies by introducing automation and robotics into the agricultural processes. Autonomous agricultural field machines have been gaining significant attention from farmers and industries to reduce costs, human workload, and required resources. Nevertheless, achieving sufficient autonomous navigation capabilities requires the simultaneous cooperation of different processes; localization, mapping, and path planning are just some of the steps that aim at providing to the machine the right set of skills to operate in semi-structured and unstructured environments. In this context, this study presents a low-cost local motion planner for autonomous navigation in vineyards based only on an RGB-D camera, low range hardware, and a dual layer control algorithm. The first algorithm exploits the disparity map and its depth representation to generate a proportional control for the robotic platform. Concurrently, a second back-up algorithm, based on representations learning and resilient to illumination variations, can take control of the machine in case of a momentaneous failure of the first block. Moreover, due to the double nature of the system, after initial training of the deep learning model with an initial dataset, the strict synergy between the two algorithms opens the possibility of exploiting new automatically labeled data, coming from the field, to extend the existing model knowledge. The machine learning algorithm has been trained and tested, using transfer learning, with acquired images during different field surveys in the North region of Italy and then optimized for on-device inference with model pruning and quantization. Finally, the overall system has been validated with a customized robot platform in the relevant environment.",0
"This paper presents a novel approach to local motion planning for autonomous navigation in vineyards using a RGB-D camera-based algorithm and deep learning synergy. The proposed method uses a hybrid sensing system consisting of a LiDAR sensor and an RGB-D camera to detect obstacles and create a 3D map of the environment. Additionally, the use of convolutional neural networks (CNNs) allows for real-time object recognition and segmentation, enabling improved accuracy and efficiency in the mapping process. The resulting 3D point cloud data is then processed by a probabilistic graph optimization technique that generates a set of smooth trajectories that account for both static obstacles and dynamic objects such as vehicles and pedestrians. Experimental results demonstrate the effectiveness of our approach in terms of robustness, reliability, and adaptivity under varying lighting conditions and weather scenarios. Our work has important implications for agricultural automation, precision viticulture, and other applications involving complex environments where reliable perception and motion planning are critical.",1
"Contrastive learning (CL) is an emerging analysis approach that aims to discover unique patterns in one dataset relative to another. By applying this approach to network analysis, we can reveal unique characteristics in one network by contrasting with another. For example, with networks of protein interactions obtained from normal and cancer tissues, we can discover unique types of interactions in cancer tissues. However, existing CL methods cannot be directly applied to networks. To address this issue, we introduce a novel approach called contrastive network representation learning (cNRL). This approach embeds network nodes into a low-dimensional space that reveals the uniqueness of one network compared to another. Within this approach, we also design a method, named i-cNRL, that offers interpretability in the learned results, allowing for understanding which specific patterns are found in one network but not the other. We demonstrate the capability of i-cNRL with multiple network models and real-world datasets. Furthermore, we provide quantitative and qualitative comparisons across i-cNRL and other potential cNRL algorithm designs.",0
"Introduction: This paper introduces a new methodology called ""Interpretable Contrastive Learning"" (ICL) which is designed to enhance our understanding of neural networks by making them more interpretable. Method: Our method involves pretraining a network using a contrastive loss function, then fine-tuning the network on specific tasks using human annotations as additional supervision. Results: We demonstrate that our method leads to improved performance over previous methods across several benchmark datasets while also allowing us to identify key features important for network predictions. Discussion/Conclusion: By applying ICL, we can gain greater insight into how neural networks make their decisions, leading to more informed design choices for future models.",1
"Heterogeneous face recognition (HFR) refers to matching face images acquired from different domains with wide applications in security scenarios. This paper presents a deep neural network approach namely Multi-Margin based Decorrelation Learning (MMDL) to extract decorrelation representations in a hyperspherical space for cross-domain face images. The proposed framework can be divided into two components: heterogeneous representation network and decorrelation representation learning. First, we employ a large scale of accessible visual face images to train heterogeneous representation network. The decorrelation layer projects the output of the first component into decorrelation latent subspace and obtains decorrelation representation. In addition, we design a multi-margin loss (MML), which consists of quadruplet margin loss (QML) and heterogeneous angular margin loss (HAML), to constrain the proposed framework. Experimental results on two challenging heterogeneous face databases show that our approach achieves superior performance on both verification and recognition tasks, comparing with state-of-the-art methods.",0
"This paper presents a novel approach to face recognition that uses multi-margin learning techniques to improve accuracy by decorrelating features across different margins. In traditional face recognition systems, performance can suffer due to variations in pose, illumination, expression, and other factors that result in changes to the appearance of the face. Our method addresses these issues by using multiple margins to learn discriminative feature representations that are robust to such changes. We experimentally demonstrate that our system achieves significantly higher accuracy than state-of-the-art methods on several benchmark datasets. Furthermore, we show that our algorithm effectively handles heterogeneous data sources, making it well-suited for real-world applications where there may be significant variability in the input images. Overall, our work contributes to the field of computer vision by providing a powerful new tool for improving the reliability and effectiveness of face recognition systems under challenging conditions.",1
"Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter-conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on camera parameters. Through experiments, we demonstrated that the proposed method could learn 3D representations from 2D images with various generator architectures.",0
"This work presents a novel method for learning 3D representations from natural image datasets by synthesizing depth maps using Generative Adversarial Networks (GAN). With advances in deep learning, many researchers have explored methods for automatically generating plausible 2D images based on existing data. However, most previous approaches assume that depth maps can already be obtained easily from real-world scenes or simulated environments. Our approach addresses this problem directly by introducing a two stage GAN model, wherein we generate high quality, paired RGBD training samples without requiring any special hardware such as LIDAR or multi-view imaging setups. In essence, our proposed algorithm creates artificial but highly detailed scenes which closely mimic natural data distributions so well, they fool state of the art classifiers into thinking these generated samples are authentic real world examples! We evaluate our method's performance across multiple benchmark databases and show quantitative evidence supporting its ability to generate superior quality paired RGBD training data compared against several other unpaired alternatives currently available in literature. Finally, we demonstrate our pipeline's utility for downstream computer vision tasks including object detection, semantic segmentation, and view synthesis - showing improved accuracy even outperforming some traditional supervised baselines trained exclusively on ground truth labels!",1
"Recent years have witnessed the emergence and development of graph neural networks (GNNs), which have been shown as a powerful approach for graph representation learning in many tasks, such as node classification and graph classification. The research on the robustness of these models has also started to attract attentions in the machine learning field. However, most of the existing work in this area focus on the GNNs for node-level tasks, while little work has been done to study the robustness of the GNNs for the graph classification task. In this paper, we aim to explore the vulnerability of the Hierarchical Graph Pooling (HGP) Neural Networks, which are advanced GNNs that perform very well in the graph classification in terms of prediction accuracy. We propose an adversarial attack framework for this task. Specifically, we design a surrogate model that consists of convolutional and pooling operators to generate adversarial samples to fool the hierarchical GNN-based graph classification models. We set the preserved nodes by the pooling operator as our attack targets, and then we perturb the attack targets slightly to fool the pooling operator in hierarchical GNNs so that they will select the wrong nodes to preserve. We show the adversarial samples generated from multiple datasets by our surrogate model have enough transferability to attack current state-of-art graph classification models. Furthermore, we conduct the robust train on the target models and demonstrate that the retrained graph classification models are able to better defend against the attack from the adversarial samples. To the best of our knowledge, this is the first work on the adversarial attack against hierarchical GNN-based graph classification models.",0
"Abstract: Modern graph neural networks have achieved state-of-the art performance across numerous data sets and applications, thanks largely due their ability to efficiently aggregate information from graphs. However, one issue that arises during training and inference is how nodes of different importance can influence the output, potentially causing GNN models to miss important features of lesser magnitude. One method used to overcome this problem is hierarchical pooling which uses node embeddings from different levels of granularity to incorporate high level context into the model. In this work we investigate the vulnerability of hierarchical graph pooling methods by applying adversarial attacks to these algorithms. Our experiments demonstrate that these attacks consistently outperform traditional graph poisoning methods even as the number of targeted attack nodes increases. Furthermore our analysis shows a correlation between the attack effectiveness and node centrality indicating higher impact of more influential nodes. These results provide insights into ways that future GNN architectures can improve robustness against such adversarial attacks.",1
"We address the challenging problem of RGB image-based head pose estimation. We first reformulate head pose representation learning to constrain it to a bounded space. Head pose represented as vector projection or vector angles shows helpful to improving performance. Further, a ranking loss combined with MSE regression loss is proposed. The ranking loss supervises a neural network with paired samples of the same person and penalises incorrect ordering of pose prediction. Analysis on this new loss function suggests it contributes to a better local feature extractor, where features are generalised to Abstract Landmarks which are pose-related features instead of pose-irrelevant information such as identity, age, and lighting. Extensive experiments show that our method significantly outperforms the current state-of-the-art schemes on public datasets: AFLW2000 and BIWI. Our model achieves significant improvements over previous SOTA MAE on AFLW2000 and BIWI from 4.50 to 3.66 and from 4.0 to 3.71 respectively. Source code will be made available at: https://github.com/seathiefwang/RankHeadPose.",0
"Our paper presents a novel approach to head pose estimation that leverages rank supervision to learn generalised feature representations. We show that our method outperforms state-of-the art approaches across multiple benchmark datasets while maintaining computational efficiency. By introducing rank supervision, we guide our model towards learning features that are robust to variations in lighting, occlusions, expression, and other factors that commonly confound traditional methods. Our framework is easy to implement and can be integrated into existing architectures without significant modification. This work represents a significant step forward in the field of computer vision and has important implications for applications such as human-computer interaction, affective computing, and virtual reality. Overall, our results demonstrate the effectiveness of combining rank supervision with deep learning techniques for addressing complex problems in computer vision.",1
"In many machine learning tasks, learning a good representation of the data can be the key to building a well-performant solution. This is because most learning algorithms operate with the features in order to find models for the data. For instance, classification performance can improve if the data is mapped to a space where classes are easily separated, and regression can be facilitated by finding a manifold of data in the feature space. As a general rule, features are transformed by means of statistical methods such as principal component analysis, or manifold learning techniques such as Isomap or locally linear embedding. From a plethora of representation learning methods, one of the most versatile tools is the autoencoder. In this paper we aim to demonstrate how to influence its learned representations to achieve the desired learning behavior. To this end, we present a series of learning tasks: data embedding for visualization, image denoising, semantic hashing, detection of abnormal behaviors and instance generation. We model them from the representation learning perspective, following the state of the art methodologies in each field. A solution is proposed for each task employing autoencoders as the only learning method. The theoretical developments are put into practice using a selection of datasets for the different problems and implementing each solution, followed by a discussion of the results in each case study and a brief explanation of other six learning applications. We also explore the current challenges and approaches to explainability in the context of autoencoders. All of this helps conclude that, thanks to alterations in their structure as well as their objective function, autoencoders may be the core of a possible solution to many problems which can be modeled as a transformation of the feature space.",0
"In recent years, there has been growing interest in using autoencoder neural networks as a tool for unsupervised feature learning and dimensionality reduction. Autoencoders have shown promise in tasks such as image generation, anomaly detection, and generative modelling, among others. This paper provides an overview of the fundamental concepts behind autoencoding, as well as several detailed case studies demonstrating how they can be used to solve specific problems in machine learning and data science. In addition to describing the technical aspects of autoencoding, we discuss key issues related to interpretability and explainability, which are crucial considerations in many real-world applications of these models. Finally, we identify some of the current challenges faced by researchers working in this field and highlight areas where future work is needed. Overall, our aim is to provide readers with a comprehensive introduction to autoencoded based representation learning, including both theoretical foundations and practical insights gained from concrete examples.",1
"Human-Object Interaction (HOI) detection lies at the core of action understanding. Besides 2D information such as human/object appearance and locations, 3D pose is also usually utilized in HOI learning since its view-independence. However, rough 3D body joints just carry sparse body information and are not sufficient to understand complex interactions. Thus, we need detailed 3D body shape to go further. Meanwhile, the interacted object in 3D is also not fully studied in HOI learning. In light of these, we propose a detailed 2D-3D joint representation learning method. First, we utilize the single-view human body capture method to obtain detailed 3D body, face and hand shapes. Next, we estimate the 3D object location and size with reference to the 2D human-object spatial configuration and object category priors. Finally, a joint learning framework and cross-modal consistency tasks are proposed to learn the joint HOI representation. To better evaluate the 2D ambiguity processing capacity of models, we propose a new benchmark named Ambiguous-HOI consisting of hard ambiguous images. Extensive experiments in large-scale HOI benchmark and Ambiguous-HOI show impressive effectiveness of our method. Code and data are available at https://github.com/DirtyHarryLYL/DJ-RN.",0
"In recent years, there has been increasing interest in developing accurate representations of human-object interactions that can capture both 2D and 3D aspects of movement. However, existing methods often rely on laborious manual annotation or specialized hardware setups, which limits their widespread adoption. To address this challenge, we propose a detailed 2D-3D joint representation method for human-object interaction analysis that utilizes monocular video input and a combination of computer vision techniques. Our approach leverages advances in depth estimation from monocular videos and articulated pose estimation to reconstruct joint angles in 3D space. We then integrate these estimates into our 2D-3D joint representation framework, allowing us to accurately predict object contact points and grasp poses during manipulation tasks. We evaluate our method using two publicly available datasets and demonstrate significant improvements over state-of-the-art approaches in terms of accuracy and generalization ability. Our work represents a step towards more robust and accessible solutions for modeling complex human-object interactions across diverse scenarios.",1
"Studying competition and market structure at the product level instead of brand level can provide firms with insights on cannibalization and product line optimization. However, it is computationally challenging to analyze product-level competition for the millions of products available on e-commerce platforms. We introduce Product2Vec, a method based on the representation learning algorithm Word2Vec, to study product-level competition, when the number of products is large. The proposed model takes shopping baskets as inputs and, for every product, generates a low-dimensional embedding that preserves important product information. In order for the product embeddings to be useful for firm strategic decision making, we leverage economic theories and causal inference to propose two modifications to Word2Vec. First of all, we create two measures, complementarity and exchangeability, that allow us to determine whether product pairs are complements or substitutes. Second, we combine these vectors with random utility-based choice models to forecast demand. To accurately estimate price elasticities, i.e., how demand responds to changes in price, we modify Word2Vec by removing the influence of price from the product vectors. We show that, compared with state-of-the-art models, our approach is faster, and can produce more accurate demand forecasts and price elasticities.",0
"This is an example of an appropriate Abstract:  This work examines product competition using representation learning techniques, as these methods provide valuable insights into customer preferences that can inform marketing strategies and optimize pricing decisions. We demonstrate how deep neural networks (DNNs) can effectively model consumer choices and predict demand patterns across different markets. Our results show that DNN models significantly outperform traditional linear regression approaches in terms of forecast accuracy. Additionally, we find evidence of complementarity and substitution effects among competing products, indicating that firms need to consider interdependencies between their own offerings and those of rivals when setting prices. Overall, our study highlights the value of representation learning in understanding complex dynamics in product markets.",1
"Previous researches of sketches often considered sketches in pixel format and leveraged CNN based models in the sketch understanding. Fundamentally, a sketch is stored as a sequence of data points, a vector format representation, rather than the photo-realistic image of pixels. SketchRNN studied a generative neural representation for sketches of vector format by Long Short Term Memory networks (LSTM). Unfortunately, the representation learned by SketchRNN is primarily for the generation tasks, rather than the other tasks of recognition and retrieval of sketches. To this end and inspired by the recent BERT model, we present a model of learning Sketch Bidirectional Encoder Representation from Transformer (Sketch-BERT). We generalize BERT to sketch domain, with the novel proposed components and pre-training algorithms, including the newly designed sketch embedding networks, and the self-supervised learning of sketch gestalt. Particularly, towards the pre-training task, we present a novel Sketch Gestalt Model (SGM) to help train the Sketch-BERT. Experimentally, we show that the learned representation of Sketch-BERT can help and improve the performance of the downstream tasks of sketch recognition, sketch retrieval, and sketch gestalt.",0
"In recent years there has been increasing interest in sketch recognition as a form of alternative input method. However, developing high quality models can require large amounts of labeled data which may not always be available. To address this issue, we propose a self-supervised learning approach utilizing the transformer architecture to learn bidirectional representations of image sketches. We demonstrate that our proposed model, Sketch-BERT, outperforms other methods on several benchmark datasets, achieving state-of-the-art results on some tasks while requiring significantly less data than previous approaches. Our work opens up new possibilities for using unlabeled data to improve performance in visual recognition tasks, potentially revolutionizing how these systems are developed in the future.",1
"PredNet, a deep predictive coding network developed by Lotter et al., combines a biologically inspired architecture based on the propagation of prediction error with self-supervised representation learning in video. While the architecture has drawn a lot of attention and various extensions of the model exist, there is a lack of a critical analysis. We fill in the gap by evaluating PredNet both as an implementation of the predictive coding theory and as a self-supervised video prediction model using a challenging video action classification dataset. We design an extended model to test if conditioning future frame predictions on the action class of the video improves the model performance. We show that PredNet does not yet completely follow the principles of predictive coding. The proposed top-down conditioning leads to a performance gain on synthetic data, but does not scale up to the more complex real-world action classification dataset. Our analysis is aimed at guiding future research on similar architectures based on the predictive coding theory.",0
"This article provides a critical review of two prominent models used in deep learning image recognition: PredNet and predictive coding (PC). While both approaches have shown impressive results in recent studies, their underlying architectures and mechanisms differ considerably. PC relies on hierarchically organized neuronal populations that successively refine predictions by integrating top-down expectations with bottom-up sensory evidence. PredNet extends this principle into deep convolutional neural networks trained end-to-end using backpropagation through time. Both methods claim to model cortical processing dynamics more closely than conventional feedforward CNNs, yet they remain controversial because they make strong assumptions about the functional organization of V1 and higher visual areas. Here we highlight some open questions concerning these key assumptions as well as other limitations that warrant further investigation before either approach can confidently replace current state-of-the-art systems. In particular, we emphasize the importance of grounding theory development in empirical studies of neuroanatomy and physiology rather than exclusively in psychophysics experiments and computational convenience. We conclude that while promising advances have been made toward biologically plausible vision models, there remains substantial work ahead to achieve realism without sacrificing generalization performance on benchmark datasets.",1
"In this paper we demonstrate methods for reliable and efficient training of discrete representation using Vector-Quantized Variational Auto-Encoder models (VQ-VAEs). Discrete latent variable models have been shown to learn nontrivial representations of speech, applicable to unsupervised voice conversion and reaching state-of-the-art performance on unit discovery tasks. For unsupervised representation learning, they became viable alternatives to continuous latent variable models such as the Variational Auto-Encoder (VAE). However, training deep discrete variable models is challenging, due to the inherent non-differentiability of the discretization operation. In this paper we focus on VQ-VAE, a state-of-the-art discrete bottleneck model shown to perform on par with its continuous counterparts. It quantizes encoder outputs with on-line $k$-means clustering. We show that the codebook learning can suffer from poor initialization and non-stationarity of clustered encoder outputs. We demonstrate that these can be successfully overcome by increasing the learning rate for the codebook and periodic date-dependent codeword re-initialization. As a result, we achieve more robust training across different tasks, and significantly increase the usage of latent codewords even for large codebooks. This has practical benefit, for instance, in unsupervised representation learning, where large codebooks may lead to disentanglement of latent representations.",0
"Introduction: The bottleneck model has been proven to be effective for image recognition tasks, but at the cost of high memory usage. To alleviate the storage requirements, vector quantization techniques have been used to reduce the size of deep neural networks (DNNs) without significant degradation of accuracy. However, training these models can still be challenging due to their sensitive nature during compression. In this work, we propose a robust methodology that combines variational autoencoders and dynamic resizing to train vector quantized bottleneck models efficiently. Methodology: We first pretrain the DNN using full precision weights, then fine-tune the network after converting it into a low bit width format such as INT8. This conversion allows us to reduce the memory footprint while preserving most of the original quality. Next, we compress the network further by applying vector quantization on different layers or even entire channels within each layer. During this stage, our approach employs variational autoencoders to estimate missing gradients and backpropagate through the discrete latent space. Finally, we dynamically resize the channel dimensions based on application requirements, which improves computational efficiency without affecting accuracy significantly. Results: Extensive experiments conducted across multiple datasets including CIFAR-10, ImageNet ILSVRC2012 demonstrate that our proposed approach achieves comparable or better results compared to other state-of-the-art methods with significantly fewer parameters and reduced memory requirements. Conclusion: Our novel framework effectively addresses the challenge of training robust vector quantized bottleneck models, ensuring efficient deployment of machine learning applications across various platforms. By combining pretraining, dynamic resizing, and variational autoencoder, we enable researchers and practitioners alike to leverage powerful DNN architectures in resource constrained environments.",1
"Extending the capabilities of robotics to real-world complex, unstructured environments requires the need of developing better perception systems while maintaining low sample complexity. When dealing with high-dimensional state spaces, current methods are either model-free or model-based based on reconstruction objectives. The sample inefficiency of the former constitutes a major barrier for applying them to the real-world. The later, while they present low sample complexity, they learn latent spaces that need to reconstruct every single detail of the scene. In real environments, the task typically just represents a small fraction of the scene. Reconstruction objectives suffer in such scenarios as they capture all the unnecessary components. In this work, we present MIRO, an information theoretic representational learning algorithm for model-based reinforcement learning. We design a latent space that maximizes the mutual information with the future information while being able to capture all the information needed for planning. We show that our approach is more robust than reconstruction objectives in the presence of distractors and cluttered scenes",0
"This paper presents a novel approach to generating robust and plannable representations through mutual information maximization. We propose using mutual information as a measure of representational strength by evaluating how well different representations encode important task-relevant information. By optimizing mutual information directly, we can encourage the learning process to produce high-quality representations that enable efficient planning and decision making. Our method outperforms existing approaches on several benchmark tasks and shows significant improvements across a range of metrics including feature quality, generalization performance, and transferability. Overall, our results demonstrate the effectiveness of mutual information maximization for achieving robust and plannable representations in complex domains.",1
"When observing a phenomenon, severe cases or anomalies are often characterised by deviation from the expected data distribution. However, non-deviating data samples may also implicitly lead to severe outcomes. In the case of unsupervised severe weather detection, these data samples can lead to mispredictions, since the predictors of severe weather are often not directly observed as features. We posit that incorporating external or auxiliary information, such as the outcome of an external task or an observation, can improve the decision boundaries of an unsupervised detection algorithm. In this paper, we increase the effectiveness of a clustering method to detect cases of severe weather by learning augmented and linearly separable latent representations.We evaluate our solution against three individual cases of severe weather, namely windstorms, floods and tornado outbreaks.",0
"In recent years, there has been increasing interest in using machine learning algorithms to detect severe weather events without relying on labeled data (i.e., unsupervised learning). However, developing effective methods for this task remains challenging due to the complexity and variability of severe weather patterns. This paper presents a novel approach to severe weather detection that combines textual and weather data through joint representation learning. Our method leverages natural language processing techniques to extract features from textual reports, while also utilizing conventional meteorological variables from weather datasets. By integrating these diverse sources of information into a shared representational space, we can more accurately identify signals indicative of severe weather conditions. Experimental results demonstrate the effectiveness of our approach compared to existing state-of-the-art unsupervised methods across multiple benchmark datasets. Our findings suggest that the proposed framework holds significant potential for advancing our understanding of severe weather dynamics and improving forecast accuracy.",1
"Designing agent that can autonomously discover and learn a diversity of structures and skills in unknown changing environments is key for lifelong machine learning. A central challenge is how to learn incrementally representations in order to progressively build a map of the discovered structures and re-use it to further explore. To address this challenge, we identify and target several key functionalities. First, we aim to build lasting representations and avoid catastrophic forgetting throughout the exploration process. Secondly we aim to learn a diversity of representations allowing to discover a ""diversity of diversity"" of structures (and associated skills) in complex high-dimensional environments. Thirdly, we target representations that can structure the agent discoveries in a coarse-to-fine manner. Finally, we target the reuse of such representations to drive exploration toward an ""interesting"" type of diversity, for instance leveraging human guidance. Current approaches in state representation learning rely generally on monolithic architectures which do not enable all these functionalities. Therefore, we present a novel technique to progressively construct a Hierarchy of Observation Latent Models for Exploration Stratification, called HOLMES. This technique couples the use of a dynamic modular model architecture for representation learning with intrinsically-motivated goal exploration processes (IMGEPs). The paper shows results in the domain of automated discovery of diverse self-organized patterns, considering as testbed the experimental framework from Reinke et al. (2019).",0
"Exploring novel environments can pose challenges for artificial intelligence agents, as they must find effective ways to gather relevant information and act upon it. One promising approach is the use of self-organizing hierarchical representations that enable efficient learning from limited data. In this work, we propose a progressive growing methodology that allows for the development of such representations over time. Our algorithm incrementally generates new levels of abstraction based on previously acquired knowledge, making it more scalable than previous methods. We evaluate our approach through experiments on multiple domains and demonstrate its ability to outperform state-of-the-art baselines while being robust to changes in environment dynamics. By enabling agents to develop increasingly refined representations with minimal human intervention, this research has important implications for developing intelligent systems capable of autonomous decision-making in complex and uncertain settings.",1
"Most video person re-identification (re-ID) methods are mainly based on supervised learning, which requires cross-camera ID labeling. Since the cost of labeling increases dramatically as the number of cameras increases, it is difficult to apply the re-identification algorithm to a large camera network. In this paper, we address the scalability issue by presenting deep representation learning without ID information across multiple cameras. Technically, we train neural networks to generate both ID-discriminative and camera-invariant features. To achieve the ID discrimination ability of the embedding features, we maximize feature distances between different person IDs within a camera by using a metric learning approach. At the same time, considering each camera as a different domain, we apply adversarial learning across multiple camera domains for generating camera-invariant features. We also propose a part-aware adaptation module, which effectively performs multi-camera domain invariant feature learning in different spatial regions. We carry out comprehensive experiments on three public re-ID datasets (i.e., PRID-2011, iLIDS-VID, and MARS). Our method outperforms state-of-the-art methods by a large margin of about 20\% in terms of rank-1 accuracy on the large-scale MARS dataset.",0
"This paper proposes a novel method called part-aware clustering to learn the correspondence between camera domains for unsupervised video person re-identification (ReID). Existing methods often rely on supervised domain alignment techniques that require labeled data from both source and target cameras. Our approach utilizes self-supervision by exploiting the fact that each person appears multiple times across different cameras capturing them at distinct angles. We introduce a new part pooling module that generates semantic regions based on human keypoints. These parts provide local representations of images which can then be clustered using our proposed clustering technique. Our model effectively aligns multi-camera domains through clustering the learned features into the same pseudo identity clusters, allowing us to train reliable predictors using only annotations from one dataset. Extensive experiments show significant performance improvements over prior arts including state-of-the art supervised and existing weakly-supervised methods. Finally, we demonstrate our algorithm generalizing well across four benchmark datasets while maintaining superior accuracy. The code will be publicly available upon acceptance.",1
"In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization, which in turn leads to better performance of the latent codes in downstream transfer learning tasks. The number of channels in the bottleneck, on the other hand, is secondary in importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when the bottleneck has the same number of neurons as there are pixels in the input. Copying does not occur, despite training the CAE for 1,000 epochs on a tiny ($\approx$ 600 images) dataset. We believe that the findings in this paper are directly applicable and will lead to improvements in models that rely on CAEs.",0
"""In this work we investigate the efficacy of using convolutional autoencoders as data compression techniques within computer vision tasks. Specifically, our focus lies on evaluating whether bottleneck layer sizes have any significant impact on performance metrics such as image reconstruction accuracy."" Convolutional Autoencoders (CAEs) are deep learning models that learn to compress images by training them to reconstruct inputs from their encoded representations, which contain only the most important features for efficient computation. Recent studies have shown their utility in several fields due to these features’ high-level abstraction capabilities. By contrast, little attention has been paid to understanding how varying parameters affects the CAE. In this study we aim to explore the effect bottleneck size has on reconstruction accuracy in Computer Vision applications when using a convolutional architecture (i.e., ConvNet). We train multiple networks using different bottleneck sizes and evaluate their capability to accurately reconstruct an input. Our experimental results indicate that larger bottlenecks lead to higher fidelity reconstructions at the cost of increased computational requirements; meanwhile smaller ones tend towards faster inference but lower visual quality. Ultimately, our findings provide insight into selecting optimal hyperparameters during model design that strike a balance between efficiency and quality, paving the way for more effective use of ConvAEs in CV pipelines.",1
"Graph convolutional networks (GCNs) are a widely used method for graph representation learning. To elucidate the capabilities and limitations of GCNs, we investigate their power, as a function of their number of layers, to distinguish between different random graph models (corresponding to different class-conditional distributions in a classification problem) on the basis of the embeddings of their sample graphs. In particular, the graph models that we consider arise from graphons, which are the most general possible parameterizations of infinite exchangeable graph models and which are the central objects of study in the theory of dense graph limits. We give a precise characterization of the set of pairs of graphons that are indistinguishable by a GCN with nonlinear activation functions coming from a certain broad class if its depth is at least logarithmic in the size of the sample graph. This characterization is in terms of a degree profile closeness property. Outside this class, a very simple GCN architecture suffices for distinguishability. We then exhibit a concrete, infinite class of graphons arising from stochastic block models that are well-separated in terms of cut distance and are indistinguishable by a GCN. These results theoretically match empirical observations of several prior works. To prove our results, we exploit a connection to random walks on graphs. Finally, we give empirical results on synthetic and real graph classification datasets, indicating that indistinguishable graph distributions arise in practice.",0
"While deep learning has achieved unprecedented successes across many domains, our understanding remains limited regarding their fundamental limitations, especially as we scale up model sizes. In this paper, we study graph convolutional networks (GCN) that operate on irregular labeled graphs. We prove new limitations by showing how GCN's expressivity increases logarithmically slower than random features and neural tangent kernel models. By analyzing the singular values, we find that, for every layer added beyond a certain point, the principal component analysis results remain unchanged. Our analysis sheds light on why current large pretraining sizes have barely improved upon smaller ones. Furthermore, we show the existence of easy and hard graphs, which explains empirical trends such as ""overparameterization helps"" but leads to different generalization behaviors under parameter compression. Overall, our work provides insights into design choices that might offset these intrinsic limits.",1
"As many algorithms depend on a suitable representation of data, learning unique features is considered a crucial task. Although supervised techniques using deep neural networks have boosted the performance of representation learning, the need for a large set of labeled data limits the application of such methods. As an example, high-quality delineations of regions of interest in the field of pathology is a tedious and time-consuming task due to the large image dimensions. In this work, we explored the performance of a deep neural network and triplet loss in the area of representation learning. We investigated the notion of similarity and dissimilarity in pathology whole-slide images and compared different setups from unsupervised and semi-supervised to supervised learning in our experiments. Additionally, different approaches were tested, applying few-shot learning on two publicly available pathology image datasets. We achieved high accuracy and generalization when the learned representations were applied to two different pathology datasets.",0
"In recent years, representation learning has emerged as a powerful approach for analyzing complex data sets such as images from medical domains like histopathology. However, obtaining high quality representations that capture important patterns and relationships in these data can be challenging due to factors such as limited training data availability or variability across source domains. This study investigates how different supervision strategies (fully supervised vs semi-supervised vs unsupervised) impact representation quality in the context of histopathological image analysis, focusing on the influence of both labeled data quantity and domain variation between source and target domains. Results demonstrate significant improvement in representation quality under specific conditions including sufficient source labeling and modest target label scarcity. These findings provide valuable insights into design choices affecting representation learning and highlight opportunities for further optimization within and beyond histopathology applications.",1
"Autoencoders are techniques for data representation learning based on artificial neural networks. Differently to other feature learning methods which may be focused on finding specific transformations of the feature space, they can be adapted to fulfill many purposes, such as data visualization, denoising, anomaly detection and semantic hashing. This work presents these applications and provides details on how autoencoders can perform them, including code samples making use of an R package with an easy-to-use interface for autoencoder design and training, \texttt{ruta}. Along the way, the explanations on how each learning task has been achieved are provided with the aim to help the reader design their own autoencoders for these or other objectives.",0
"Here is an example of an abstract: A showcase of the use of autoencoders in feature learning applications provides an overview of recent developments in applying deep neural networks (DNNs) for image classification tasks, focusing on state-of-the art methods that employ regularization techniques such as dropout and weight decay to improve generalization performance. We present an extensive evaluation study which compares several variants of these models using four benchmark datasets commonly used in computer vision research. Our results demonstrate significant improvements in accuracy can be obtained by training DNN classifiers with these regularizations applied during both training and testing phases. Finally we investigate how different hyperparameters affect model performance; our findings reveal insights into optimal choices of network architecture, batch size and training iterations required for achieving good results across all tasks considered here.",1
"In this paper we introduce plan2vec, an unsupervised representation learning approach that is inspired by reinforcement learning. Plan2vec constructs a weighted graph on an image dataset using near-neighbor distances, and then extrapolates this local metric to a global embedding by distilling path-integral over planned path. When applied to control, plan2vec offers a way to learn goal-conditioned value estimates that are accurate over long horizons that is both compute and sample efficient. We demonstrate the effectiveness of plan2vec on one simulated and two challenging real-world image datasets. Experimental results show that plan2vec successfully amortizes the planning cost, enabling reactive planning that is linear in memory and computation complexity rather than exhaustive over the entire state space.",0
"This work proposes a new method called ""Plan2Vec"" that allows unsupervised learning of representations using latent plans derived from natural language text. By leveraging deep neural networks trained on large datasets, our approach can extract key components of these plans such as goals, actions, effects, and logical relationships, and use them to construct a meaningful vector space where semantically related concepts are placed closer together than those which are dissimilar. We demonstrate the effectiveness of our model by applying it to several tasks such as similarity analysis, clustering, classification, and retrieval. Our results show that our model significantly outperforms previous methods for unsupervised representation learning, achieving state-of-the-art performance on benchmark datasets. Furthermore, we conduct extensive ablation studies and analyses to provide insights into how our models learn and generalize across different domains and languages. Overall, our work represents a significant step forward in understanding the fundamental principles underlying plan extraction and representation learning.",1
"Recent single image unsupervised representation learning techniques show remarkable success on a variety of tasks. The basic principle in these works is instance discrimination: learning to differentiate between two augmented versions of the same image and a large batch of unrelated images. Networks learn to ignore the augmentation noise and extract semantically meaningful representations. Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial ways and are not aligned with how objects actually change e.g. occlusion, deformation, viewpoint change. In this paper, we argue that videos offer this natural augmentation for free. Videos can provide entirely new views of objects, show deformation, and even connect semantically similar but visually distinct concepts. We propose Video Noise Contrastive Estimation, a method for using unlabeled video to learn strong, transferable single image representations. We demonstrate improvements over recent unsupervised single image techniques, as well as over fully supervised ImageNet pretraining, across a variety of temporal and non-temporal tasks. Code and the Random Related Video Views dataset are available at https://www.github.com/danielgordon10/vince",0
"This paper presents a new approach to representation learning from unlabeled videos using a method called ""Watching the World Go By."" We propose that by simply observing video footage without any specific task or objective in mind, neural networks can learn representations that capture important features and patterns found within the visual data. Our model consists of two components - a recurrent module that processes incoming frames one at a time, and a non-recurrent module that aggregates information over time to produce a final fixed-size embedding vector. Through experiments on several benchmark datasets, we demonstrate that our proposed approach outperforms baseline methods across a variety of tasks, including action recognition, object detection, and scene understanding. Overall, our results suggest that Watching the World Go By may serve as a powerful tool for building generalizable and transferable representations of complex visual phenomena.",1
"Building a scalable machine learning system for unsupervised anomaly detection via representation learning is highly desirable. One of the prevalent methods is using a reconstruction error from variational autoencoder (VAE) via maximizing the evidence lower bound. We revisit VAE from the perspective of information theory to provide some theoretical foundations on using the reconstruction error, and finally arrive at a simpler and more effective model for anomaly detection. In addition, to enhance the effectiveness of detecting anomalies, we incorporate a practical model uncertainty measure into the metric. We show empirically the competitive performance of our approach on benchmark datasets.",0
"This would be used as part of a blind review process so any references should be replaced by placeholders like ""publication [1]"" etcetera and any author names need to go unmentioned if possible. For example one could write [This work] instead of e.g. ""as has been previously observed in our prior research"" If you can follow those guidelines I can give details of your target audience below! In general I look forward to reading some excellent summaries from you! Please don't hesitate to ask me any questions while preparing my request(s). -- Thank you!",1
"Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.",0
"In recent years there have been significant advances in machine learning, particularly deep neural networks which are capable of achieving state-of-the-art results on many tasks. One major challenge that remains is generalizing these models across multiple domains, often requiring substantial retraining for each new task. This paper presents Big Transfer (BiT), a framework that enables large pretrained visual representations to effectively adapt to downstream tasks without extensive fine-tuning. By leveraging off-the-shelf deep features, we demonstrate improved accuracy over previous methods while significantly reducing computational cost and model size requirements. The main contributions of this work lie in our ability to rapidly distill and apply large scale image models for diverse applications such as classification, object detection and segmentation. We believe BiT represents an important step towards more scalable transfer learning solutions across different modalities and domains. Our experiments on several benchmark datasets highlight the effectiveness of the proposed methodology. Overall, BiT provides a flexible toolkit for practitioners seeking to leverage the vast amounts of pretrained data available today.",1
"Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.",0
"A fundamental problem in machine learning is representation learning, which involves finding meaningful representations of data that capture important features and patterns. One approach to representation learning is mutual information gradient estimation (MINE), which measures the amount of information shared between different parts of a dataset. In this work, we explore the use of MINE as a tool for improving representation learning and propose several new techniques for estimating mutual information gradients. Our results show that these methods can significantly improve the accuracy and robustness of representation learning algorithms, making them more effective at solving complex tasks such as image classification and natural language processing. We conclude by discussing potential applications of our findings and future directions for research on mutual information gradient estimation.",1
"Effective representation learning of electronic health records is a challenging task and is becoming more important as the availability of such data is becoming pervasive. The data contained in these records are irregular and contain multiple modalities such as notes, and medical codes. They are preempted by medical conditions the patient may have, and are typically jotted down by medical staff. Accompanying codes are notes containing valuable information about patients beyond the structured information contained in electronic health records. We use transformer networks and the recently proposed BERT language model to embed these data streams into a unified vector representation. The presented approach effectively encodes a patient's visit data into a single distributed representation, which can be used for downstream tasks. Our model demonstrates superior performance and generalization on mortality, readmission and length of stay tasks using the publicly available MIMIC-III ICU dataset. Code avaialble at https://github.com/sajaddarabi/TAPER-EHR",0
"This paper presents a new approach to electronic health record (EHR) representation called Time-Aware Patient EHR Representation (TAPER). Traditional EHR systems focus on storing patient data without considering the temporal dimension of patient care. As such, they fail to capture important details about how patients transition through episodes of care over time. To address this limitation, we propose a novel model that represents a patient's complete medical history as a sequence of episodes defined by clinically meaningful events. Each episode is represented using a timeline of clinical notes that captures the evolution of diagnoses, interventions, and observations. We have designed TAPER to support efficient querying and retrieval of longitudinal patient records, making it suitable for applications ranging from research cohort identification to quality improvement initiatives. Through evaluation using real-world EHR data and expert feedback, we demonstrate that our model outperforms existing approaches while providing crucial contextualization for interpreting patient records at any point in time. Overall, TAPER constitutes a significant advance in enabling informed decision making based on comprehensive, temporally aware views of individual patient trajectories.",1
"To solve the problem of the overwhelming size of Deep Neural Networks (DNN) several compression schemes have been proposed, one of them is teacher-student. Teacher-student tries to transfer knowledge from a complex teacher network to a simple student network. In this paper, we propose a novel method called a teacher-class network consisting of a single teacher and multiple student networks (i.e. class of students). Instead of transferring knowledge to one student only, the proposed method transfers a chunk of knowledge about the entire solution to each student. Our students are not trained for problem-specific logits, they are trained to mimic knowledge (dense representation) learned by the teacher network. Thus unlike the logits-based single student approach, the combined knowledge learned by the class of students can be used to solve other problems as well. These students can be designed to satisfy a given budget, e.g. for comparative purposes we kept the collective parameters of all the students less than or equivalent to that of a single student in the teacher-student approach . These small student networks are trained independently, making it possible to train and deploy models on memory deficient devices as well as on parallel processing systems such as data centers. The proposed teacher-class architecture is evaluated on several benchmark datasets including MNIST, FashionMNIST, IMDB Movie Reviews and CAMVid on multiple tasks including classification, sentiment classification and segmentation. Our approach outperforms the state-of-the-art single student approach in terms of accuracy as well as computational cost and in many cases it achieves an accuracy equivalent to the teacher network while having 10-30 times fewer parameters.",0
"Researchers have developed a new neural network compression method called teacher-class networks (TCNs). TCNs work by assigning each class in a dataset its own subnetwork within the main neural net. These subnetworks act as teachers to smaller student models, which learn from them using distillation techniques like attention transfer learning. The resulting compressed model retains high accuracy while reducing memory usage and computation cost. This makes TCNs ideal for deploying large language processing models on resource constrained devices such as smartphones or embedded systems. Experiments show that TCNs achieve better accuracy than other competitive methods of neural network compression such as knowledge distillation and pruning under comparable parameter sizes and computational budgets. Furthermore, TCNs perform competitively compared to full size models across a wide range of benchmark datasets including GLUE, SQuAD2, and CoLA. Overall, these results demonstrate TCNs as effective and efficient means to compress complex deep learning models without sacrificing performance. By reducing model complexity, deployment costs, and latency, TCNs make state-of-the-art natural language processors more accessible to wider audiences beyond well-equipped servers.",1
"The real-world data usually exhibits heterogeneous properties such as modalities, views, or resources, which brings some unique challenges wherein the key is Heterogeneous Representation Learning (HRL) termed in this paper. This brief survey covers the topic of HRL, centered around several major learning settings and real-world applications. First of all, from the mathematical perspective, we present a unified learning framework which is able to model most existing learning settings with the heterogeneous inputs. After that, we conduct a comprehensive discussion on the HRL framework by reviewing some selected learning problems along with the mathematics perspectives, including multi-view learning, heterogeneous transfer learning, Learning using privileged information and heterogeneous multi-task learning. For each learning task, we also discuss some applications under these learning problems and instantiates the terms in the mathematical framework. Finally, we highlight the challenges that are less-touched in HRL and present future research directions. To the best of our knowledge, there is no such framework to unify these heterogeneous problems, and this survey would benefit the community.",0
"This paper reviews heterogeneous representation learning methods. The paper presents a survey of different techniques used by researchers to learn high quality representations from data that comes from multiple modalities such as textual content, images, audio files, video clips and so on. Various architectures have been proposed over recent years to integrate multi modality information using deep neural networks (DNNs). Our goal is to provide a comprehensive review of all these methods, discuss their respective advantages and disadvantages, describe how they operate at a high level, compare them against each other and highlight current challenges and open problems in the field. We hope this study can serve both as an introduction for beginners interested in the topic but also as reference material for experts looking to gain more knowledge on related works.",1
"Learning a good representation is an essential component for deep reinforcement learning (RL). Representation learning is especially important in multitask and partially observable settings where building a representation of the unknown environment is crucial to solve the tasks. Here we introduce Prediction of Bootstrap Latents (PBL), a simple and flexible self-supervised representation learning algorithm for multitask deep RL. PBL builds on multistep predictive representations of future observations, and focuses on capturing structured information about environment dynamics. Specifically, PBL trains its representation by predicting latent embeddings of future observations. These latent embeddings are themselves trained to be predictive of the aforementioned representations. These predictions form a bootstrapping effect, allowing the agent to learn more about the key aspects of the environment dynamics. In addition, by defining prediction tasks completely in latent space, PBL provides the flexibility of using multimodal observations involving pixel images, language instructions, rewards and more. We show in our experiments that PBL delivers across-the-board improved performance over state of the art deep RL agents in the DMLab-30 and Atari-57 multitask setting.",0
"In recent years, deep reinforcement learning (RL) has made significant progress in solving challenging problems in various domains such as game playing, robotics, and natural language processing. However, one major challenge faced by RL algorithms is the limited sample efficiency due to the requirement for large amounts of training data. To address this issue, we propose a new approach called ""Latent Predictive Representation"" which can leverage prior knowledge and enable agents to learn faster and better in multitask settings. Our method uses bootstrapping techniques to pretrain a latent model on a small dataset before fine-tuning it using gradient descent. We demonstrate through extensive experiments that our method achieves state-of-the art results across multiple benchmark tasks while using significantly fewer samples than existing methods. Our work provides insights into how to design effective pretraining strategies for RL algorithms, enabling them to efficiently exploit prior knowledge and tackle complex real-world problems.",1
"Recent breakthroughs in representation learning of unseen classes and examples have been made in deep metric learning by training at the same time the image representations and a corresponding metric with deep networks. Recent contributions mostly address the training part (loss functions, sampling strategies, etc.), while a few works focus on improving the discriminative power of the image representation. In this paper, we propose DIABLO, a dictionary-based attention method for image embedding. DIABLO produces richer representations by aggregating only visually-related features together while being easier to train than other attention-based methods in deep metric learning. This is experimentally confirmed on four deep metric learning datasets (Cub-200-2011, Cars-196, Stanford Online Products, and In-Shop Clothes Retrieval) for which DIABLO shows state-of-the-art performances.",0
"Title: Enhancing Deep Metric Learning with Dictionary-Based Attention Blocks: Introducing Diablo  The field of computer vision has seen significant progress with deep neural networks, but metric learning still remains challenging. Metric learning algorithms learn a distance function that measures similarity or dissimilarity between data points. These distances can then be used to perform tasks such as image retrieval, clustering, or classification. Despite their wide range of applications, current state-of-the-art approaches struggle with handling high-dimensional features and large datasets. In this study, we propose an efficient solution, DIABLO (Dictionary-based Attention Block for Deep Metric Learning), which effectively handles these issues by utilizing attention mechanisms and dictionary learning techniques within a deep neural network framework.  DIABLO employs a self-supervised pretext task and optimizes a contrastive loss that maximizes agreement between semantically similar pairs while minimizing agreements between dissimilar ones. Furthermore, our proposed method introduces a dictionary-based attention mechanism that learns a compact set of representative atoms from the feature space. This allows the model to focus on relevant atom combinations, improving its ability to capture complex relationships among data points. We evaluate the effectiveness of our approach on four widely studied benchmark datasets and demonstrate its superior performance over existing methods, both quantitatively and qualitatively. Our results confirm the efficacy of DIABLO, establishing it as a promising technique for tackling challenges faced by modern metric learning models. By enabling more effective use of available computational resources and delivering improved accuracy, DIABLO provides a valuable contribution to the fields of computer vision and machine learning.",1
"The wide-spread adoption of representation learning technologies in clinical decision making strongly emphasizes the need for characterizing model reliability and enabling rigorous introspection of model behavior. While the former need is often addressed by incorporating uncertainty quantification strategies, the latter challenge is addressed using a broad class of interpretability techniques. In this paper, we argue that these two objectives are not necessarily disparate and propose to utilize prediction calibration to meet both objectives. More specifically, our approach is comprised of a calibration-driven learning method, which is also used to design an interpretability technique based on counterfactual reasoning. Furthermore, we introduce \textit{reliability plots}, a holistic evaluation mechanism for model reliability. Using a lesion classification problem with dermoscopy images, we demonstrate the effectiveness of our approach and infer interesting insights about the model behavior.",0
"Title: Improving the Accuracy and Transparency of Healthcare AI  Deep learning has revolutionized many domains including healthcare by enabling accurate predictions that often outperform traditional methods. However, these models can suffer from lack of interpretability and reliability which hinders their adoption into clinical practice. In this work we propose several approaches to tackle these challenges. Firstly, we improve model accuracy through fine-tuning on large amounts of data available for healthcare applications. Secondly, we develop techniques to analyze the internal representation of deep neural networks making them more interpretable. Finally, we apply our methodologies to real world datasets such as electronic medical records (EMRs) where we demonstrate significant improvements over state-of-the-art models both in terms of predictive performance and explainability. Our contributions provide important insights towards reliable and interpretable deep predictive models in healthcare.",1
"Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.",0
"Graph representation learning has gained significant attention recently due to its applications in many domains such as computer vision, natural language processing, recommendation systems, and knowledge graphs. This survey focuses on recent advancements in graph representation learning techniques that handle dynamic graphs, where the underlying structure changes over time. We first provide background on static graph representation learning methods, discussing their strengths and limitations. Next, we describe the key characteristics of dynamic graphs and highlight why existing graph representation methods may fall short in capturing temporal patterns. We then present representative approaches for learning representations of dynamic graphs, including recurrent neural networks (RNN) and nonlinear diffusion models, attention mechanisms, generative adversarial imitation learning, and transformers. For each method, we explain its core idea, model architecture, optimization objective, and evaluation metrics, highlighting connections across different fields and pointing out open challenges. Finally, we identify promising future research directions for dynamic graph representation learning and offer suggestions for designing computational experiments reproducibility in real-world scenarios. Overall, our survey provides a comprehensive view of state-of-the-art methods for representing dynamic graphs and can serve as a guide for researchers interested in applying these techniques to new problems. --end--",1
"We present a novel unsupervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for high-level tasks such as captioning and VQA. Given a set of detected object regions in an image (e.g., using Faster R-CNN), like any other unsupervised feature learning methods (e.g., word2vec), the proxy training objective of VC R-CNN is to predict the contextual objects of a region. However, they are fundamentally different: the prediction of VC R-CNN is by using causal intervention: P(Y|do(X)), while others are by using the conventional likelihood: P(Y|X). This is also the core reason why VC R-CNN can learn ""sense-making"" knowledge like chair can be sat -- while not just ""common"" co-occurrences such as chair is likely to exist if table is observed. We extensively apply VC R-CNN features in prevailing models of three popular tasks: Image Captioning, VQA, and VCR, and observe consistent performance boosts across them, achieving many new state-of-the-arts. Code and feature are available at https://github.com/Wangt-CN/VC-R-CNN.",0
"The field of computer vision has seen significant advancements over recent years due to the development of powerful deep learning models that can effectively perform complex tasks such as object detection and semantic segmentation. However, these models often struggle with generalizing to real-world scenarios where objects may appear from unusual angles, have occlusions, or exhibit other forms of variability. To address these challenges, we propose a new framework called ""Visual Commonsense R-CNN"" which utilizes commonsense knowledge to improve the performance of existing object detection algorithms. Our approach involves integrating a large dataset of everyday scenes into the training process, allowing the model to learn more generic representations that better capture the essence of objects across different contexts. We demonstrate through extensive experiments on benchmark datasets that our method achieves state-of-the art results, outperforming previous approaches by significant margins. Overall, our work highlights the importance of incorporating prior knowledge into computer vision systems to enable them to reason about the world in a more human-like manner.",1
"It is always demanding to learn robust visual representation for various learning problems; however, this learning and maintenance process usually suffers from noise, incompleteness or knowledge domain mismatch. Thus, robust representation learning by removing noisy features or samples, complementing incomplete data, and mitigating the distribution difference becomes the key. Along this line of research, low-rank modeling has been widely-applied to solving representation learning challenges. This survey covers the topic from a knowledge flow perspective in terms of: (1) robust knowledge recovery, (2) robust knowledge transfer, and (3) robust knowledge fusion, centered around several major applications. First of all, we deliver a unified formulation for robust knowledge discovery given single dataset. Second, we discuss robust knowledge transfer and fusion given multiple datasets with different knowledge flows, followed by practical challenges, model variations, and remarks. Finally, we highlight future research of robust knowledge discovery for incomplete, unbalance, large-scale data analysis. This would benefit AI community from literature review to future direction.",0
"This paper proposes a new perspective on learning robust data representations, focusing on understanding how knowledge flows through different stages of processing. We argue that traditional approaches to representation learning view individual models as isolated entities, rather than part of a broader system of interconnected components. By examining the flow of knowledge within this larger context, we can better identify sources of instability and fragility, and develop more effective methods for improving model performance. Our approach centers on three key principles: diversity, interaction, and adaptation. Diversity refers to the creation of multiple parallel pathways through which information can flow, reducing reliance on any single source of knowledge. Interaction highlights the importance of bidirectional communication among different elements of the system, ensuring that each component receives relevant information from others. Finally, adaptation emphasizes the need for continuous updating and refinement of knowledge structures over time, allowing models to adapt to changing environments or evolve in response to emerging challenges. Together these principles offer a powerful framework for building resilient and flexible data representations, capable of handling complex tasks under real-world conditions. Through empirical evaluation using diverse benchmark datasets across several domains, our findings demonstrate that following this knowledge flow perspective leads to significant improvements in robustness and generalization compared to state-of-the-art alternatives. Overall, we believe this work contributes important insights into the nature of representation learning and helps lay the foundation for future advances in artificial intelligence.",1
"Identifiability, or recovery of the true latent representations from which the observed data originates, is de facto a fundamental goal of representation learning. Yet, most deep generative models do not address the question of identifiability, and thus fail to deliver on the promise of the recovery of the true latent sources that generate the observations. Recent work proposed identifiable generative modelling using variational autoencoders (iVAE) with a theory of identifiability. Due to the intractablity of KL divergence between variational approximate posterior and the true posterior, however, iVAE has to maximize the evidence lower bound (ELBO) of the marginal likelihood, leading to suboptimal solutions in both theory and practice. In contrast, we propose an identifiable framework for estimating latent representations using a flow-based model (iFlow). Our approach directly maximizes the marginal likelihood, allowing for theoretical guarantees on identifiability, thereby dispensing with variational approximations. We derive its optimization objective in analytical form, making it possible to train iFlow in an end-to-end manner. Simulations on synthetic data validate the correctness and effectiveness of our proposed method and demonstrate its practical advantages over other existing methods.",0
"Title: ""Identifying Strategies for Enhancing Learning Experience""  Abstract:  Enhancement of learning experience has become a key concern in modern education as well as industry training programs. In this context, understanding the underlying process flows that help learners better internalize their studies can aid educators, trainers and researchers alike in creating more effective learning frameworks and intervention strategies. This study delves into the conceptual and methodological aspects that enable the recovery of latent representations from process flow data. By examining different approaches towards identifying meaningful flows, our aim is to explore how these representations can help uncover hidden dimensions that influence learning outcomes. Our analysis provides insights on the relationship between task complexity, cognitive load, affective states and learner performance which informs the development of intelligent pedagogical designs. Ultimately, we hope to contribute towards a richer understanding of the learning process at both theoretical and applied levels.",1
"Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing end-to-end rPPG and heart rate (HR) measurement methods from facial videos are vulnerable to the less-constrained scenarios (e.g., with head movement and bad illumination). In this letter, we explore the reason why existing end-to-end networks perform poorly in challenging conditions and establish a strong end-to-end baseline (AutoHR) for remote HR measurement with neural architecture search (NAS). The proposed method includes three parts: 1) a powerful searched backbone with novel Temporal Difference Convolution (TDC), intending to capture intrinsic rPPG-aware clues between frames; 2) a hybrid loss function considering constraints from both time and frequency domains; and 3) spatio-temporal data augmentation strategies for better representation learning. Comprehensive experiments are performed on three benchmark datasets to show our superior performance on both intra- and cross-dataset testing.",0
"In recent years, remote heart rate measurement has become an important area of research due to the increasing demand for non-invasive monitoring solutions that can provide accurate cardiovascular health assessments without the need for physical contact or specialized equipment. This paper presents a new approach to remote heart rate measurement using deep learning techniques, which we call ""AutoHR."" Our method uses a combination of transfer learning and neural searching to achieve strong performance on the task. We evaluate our approach on several benchmark datasets and compare it against state-of-the-art methods. The results show that our system achieves competitive accuracy while maintaining computational efficiency, making it a promising baseline for future research in this field.",1
"Image classification has been studied extensively, but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. We present a set of methods for leveraging information about the semantic hierarchy embedded in class labels. We first inject label-hierarchy knowledge into an arbitrary CNN-based classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embeddings governed by both Euclidean and hyperbolic geometries, prevalent in natural language, and tailor them to hierarchical image classification and representation learning. We empirically validate all the models on the hierarchical ETHEC dataset.",0
"Abstract: This paper presents a novel approach for hierarchical image classification by introducing entailment cone embeddings (ECE). ECEs provide a compact representation of high-level semantic features that capture relationships among objects, allowing us to efficiently combine features across different levels of abstraction. We propose a two stage hierarchical framework based on ECEs, where images are first classified into coarse categories and then further subdivided into fine-grained classes. Our method demonstrates state-of-the-art performance on multiple benchmark datasets while maintaining computational efficiency. Additionally, we conduct extensive analysis on the effectiveness of our proposed approach, including robustness to noise and evaluation of important components in the framework. Overall, our work provides new insights into the application of hierarchy in image recognition and establishes a strong foundation for future research in this area.",1
"Much of vision-and-language research focuses on a small but diverse set of independent tasks and supporting datasets often studied in isolation; however, the visually-grounded language understanding skills required for success at these tasks overlap significantly. In this work, we investigate these relationships between vision-and-language tasks by developing a large-scale, multi-task training regime. Our approach culminates in a single model on 12 datasets from four broad categories of task including visual question answering, caption-based image retrieval, grounding referring expressions, and multi-modal verification. Compared to independently trained single-task models, this represents a reduction from approximately 3 billion parameters to 270 million while simultaneously improving performance by 2.05 points on average across tasks. We use our multi-task framework to perform in-depth analysis of the effect of joint training diverse tasks. Further, we show that finetuning task-specific models from our single multi-task model can lead to further improvements, achieving performance at or above the state-of-the-art.",0
"This abstract summarizes a study on multi-task representation learning using natural image classification as well as vision and language tasks such as object detection, semantic segmentation, COCO grounding, and others. We show that by training on diverse visual-linguistic tasks we can obtain representations capable of solving multiple downstream challenges while significantly reducing computational resources compared to individual task specific models. Our approach achieves state-of-the art performance in over half of the tested benchmarks without requiring any fine-tuning, thus demonstrating the versatility of learned representations. We further present an extensive analysis which confirms that our framework captures important features across different tasks providing evidence for a shared representational space. Finally, we provide a comparison between multi-model versus single model training regimes and find consistent improvements in zero shot transfer, even after ensembling all available architectures pre-trained at our disposal.",1
"The ability to incrementally learn new classes is crucial to the development of real-world artificial intelligence systems. In this paper, we focus on a challenging but practical few-shot class-incremental learning (FSCIL) problem. FSCIL requires CNN models to incrementally learn new classes from very few labelled samples, without forgetting the previously learned ones. To address this problem, we represent the knowledge using a neural gas (NG) network, which can learn and preserve the topology of the feature manifold formed by different classes. On this basis, we propose the TOpology-Preserving knowledge InCrementer (TOPIC) framework. TOPIC mitigates the forgetting of the old classes by stabilizing NG's topology and improves the representation learning for few-shot new classes by growing and adapting NG to new training samples. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental learning methods on CIFAR100, miniImageNet, and CUB200 datasets.",0
"In recent years, few-shot learning has emerged as a promising direction in machine learning that seeks to enable models to generalize to new classes with only a small number of labeled examples per class (i.e., ""few shots""). Despite significant progress in this area, existing few-shot methods often suffer from catastrophic forgetting during the incremental training process on novel classes. In other words, the model's performance on previously learned tasks can degrade substantially when trained on a new task. This phenomenon is known as ""class-incremental learning.""  This paper introduces Few-Shot Class-Incremental Learning, a framework designed to mitigate catastrophic forgetting by integrating memory-augmented neural networks (MANNs) with meta-learning techniques. MANNs have been shown to effectively address the problem of catastrophic forgetting in sequential learning settings through the use of external memory modules that store knowledge gained from previous experiences. However, applying these memory mechanisms directly to few-shot learning poses unique challenges due to the limited availability of data for each new task.  To overcome these difficulties, we propose a two-stage approach where the model first learns an efficient representation of the given dataset using base learners that integrate task-specific information into their parameters. During this stage, the model benefits from access to full batch gradient updates, which encourages rapid convergence towards local optima. Next, we introduce class-incremental fine-tuning, which leverages both the frozen base learner weights and the dynamically updated classifiers obtained from mini-batch gradient descent. This combination ensures the stability of the learned representations while still adapting to novel classes under severe few-shot constraints.  Experimental results demonstrate that our proposed method outperforms state-of-the-art baselines across several benchmark datasets and task sets, achieving significant improvements in terms of accuracy and robustness against class imbalance. Our findings suggest tha",1
"Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.",0
"This paper presents a new model called ""Variational Graph Recurrent Neural Networks"" (VGRNN) that extends traditional recurrent neural networks to operate on graph data structures by utilizing variational inference principles. VGRNN allows for flexible incorporation of prior knowledge in the form of constraints and can adaptively capture complex temporal dependencies within graphs. We evaluate the performance of our method through experiments on synthetic datasets as well as real-world benchmarking tasks such as video classification and sentiment analysis. Our results show improved accuracy compared to state-of-the-art methods in these domains, demonstrating the effectiveness of VGRNN in processing dynamic graph signals. In summary, we propose a novel framework for processing graph sequential data which achieves promising results across various application areas.",1
"In this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. Our objective is to minimize PPA (power, performance, and area), and we show that, in under 6 hours, our method can generate placements that are superhuman or comparable on modern accelerator netlists, whereas existing baselines require human experts in the loop and take several weeks.",0
"This paper describes a novel deep reinforcement learning algorithm that can learn optimal chip placements on printed circuit boards (PCBs) from scratch using raw PCB images as input data without any human engineering intervention. Our approach uses a convolutional neural network to extract features from the raw PCB image and then feeds these features into a recurrent neural network that learns how to place chips on the board in real time while interacting with the physical environment through simulated robotic arms equipped with cameras to observe the state of the world. We present experimental results demonstrating our model achieves near perfect accuracy on test cases even when presented with unseen PCB designs and layouts. Additionally, we show qualitative comparisons of our learned solutions against manually engineered placements by expert electrical engineers, showing the benefits of automation achieved by our methodology. Finally, we discuss future directions for improving the robustness and efficiency of our system further reducing if not eliminating the need for human involvement in electronics manufacturing processes altogether.",1
"Point cloud is a principal data structure adopted for 3D geometric information encoding. Unlike other conventional visual data, such as images and videos, these irregular points describe the complex shape features of 3D objects, which makes shape feature learning an essential component of point cloud analysis. To this end, a shape-oriented message passing scheme dubbed ShapeConv is proposed to focus on the representation learning of the underlying shape formed by each local neighboring point. Despite this intra-shape relationship learning, ShapeConv is also designed to incorporate the contextual effects from the inter-shape relationship through capturing the long-ranged dependencies between local underlying shapes. This shape-oriented operator is stacked into our hierarchical learning architecture, namely Shape-Oriented Convolutional Neural Network (SOCNN), developed for point cloud analysis. Extensive experiments have been performed to evaluate its significance in the tasks of point cloud classification and part segmentation.",0
"Advances in computer vision have enabled point cloud analysis for applications such as autonomous robots, augmented reality, and computer animation. Despite recent progress, many approaches still suffer from limited accuracy and computational complexity due to limitations in their data representations. This study proposes using shape-oriented convolutional neural networks (SOConvNets) to enhance point cloud analysis by leveraging geometric relationships present within each shape. Our approach uses intrinsic geometry properties of shapes to learn local and global features through rotational equivariance and normalization techniques, improving overall performance on datasets commonly used for benchmarking. Experimental results show our SOConvNet outperforms other state-of-the-art methods for tasks including object classification, segmentation, and surface reconstruction while reducing computational cost. Overall, our findings demonstrate that incorporating geometry into deep learning models can significantly advance the field of point cloud processing.",1
"The study of genetic variants can help find correlating population groups to identify cohorts that are predisposed to common diseases and explain differences in disease susceptibility and how patients react to drugs. Machine learning algorithms are increasingly being applied to identify interacting GVs to understand their complex phenotypic traits. Since the performance of a learning algorithm not only depends on the size and nature of the data but also on the quality of underlying representation, deep neural networks can learn non-linear mappings that allow transforming GVs data into more clustering and classification friendly representations than manual feature selection. In this paper, we proposed convolutional embedded networks in which we combine two DNN architectures called convolutional embedded clustering and convolutional autoencoder classifier for clustering individuals and predicting geographic ethnicity based on GVs, respectively. We employed CAE-based representation learning on 95 million GVs from the 1000 genomes and Simons genome diversity projects. Quantitative and qualitative analyses with a focus on accuracy and scalability show that our approach outperforms state-of-the-art approaches such as VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted population groups in 22 hours with an adjusted rand index of 0.915, the normalized mutual information of 0.92, and the clustering accuracy of 89%. Contrarily, the CAE classifier can predict the geographic ethnicity of unknown samples with an F1 and Mathews correlation coefficient(MCC) score of 0.9004 and 0.8245, respectively. To provide interpretations of the predictions, we identify significant biomarkers using gradient boosted trees(GBT) and SHAP. Overall, our approach is transparent and faster than the baseline methods, and scalable for 5% to 100% of the full human genome.",0
"This work presents convolutional embedded networks as a novel methodology for population scale clustering and bio-ancestry inferring. Through extensive experimentation on large datasets, we demonstrate that these models achieve state-of-the-art performance compared to traditional approaches. Furthermore, our architecture significantly reduces computational complexity through parallelization and embeds high-dimensional biological data into compact latent spaces enabling subsequent analysis. Additionally, we provide a detailed ablation study showcasing the contributions of each component. Finally, we present multiple case studies demonstrating the applications of this framework in genetics research such as disease mapping and admixture decomposition.",1
"Grounding referring expressions in images aims to locate the object instance in an image described by a referring expression. It involves a joint understanding of natural language and image content, and is essential for a range of visual tasks related to human-computer interaction. As a language-to-vision matching task, the core of this problem is to not only extract all the necessary information (i.e., objects and the relationships among them) in both the image and referring expression, but also make full use of context information to align cross-modal semantic concepts in the extracted information. Unfortunately, existing work on grounding referring expressions fails to accurately extract multi-order relationships from the referring expression and associate them with the objects and their related contexts in the image. In this paper, we propose a Cross-Modal Relationship Extractor (CMRE) to adaptively highlight objects and relationships (spatial and semantic relations) related to the given expression with a cross-modal attention mechanism, and represent the extracted information as a language-guided visual relation graph. In addition, we propose a Gated Graph Convolutional Network (GGCN) to compute multimodal semantic contexts by fusing information from different modes and propagating multimodal information in the structured relation graph. Experimental results on three common benchmark datasets show that our Cross-Modal Relationship Inference Network, which consists of CMRE and GGCN, significantly surpasses all existing state-of-the-art methods. Code is available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models",0
"This research paper presents a new approach to natural language processing that focuses on improving how computers can understand referring expressions. We propose using ""Relationship-Embedded Representation Learning"" (RERL) to learn relationships between objects in a scene and their corresponding descriptions, allowing a computer to more accurately ground referring expressions. Our method incorporates both visual features and textual context into a unified representation, enabling better understanding of complex situations where referring expressions may have multiple interpretations. Experimental results demonstrate improved performance compared to previous methods, showing that RERL is effective at capturing important relationships for accurate grounding of referring expressions. Overall, our work contributes to advancing state-of-the-art technologies in natural language processing, paving the way for future improvements in human-computer interaction.",1
"Most current studies on survey analysis and risk tolerance modelling lack professional knowledge and domain-specific models. Given the effectiveness of generative adversarial learning in cross-domain information, we design an Asymmetric cross-Domain Generative Adversarial Network (ADGAN) for domain scale inequality. ADGAN utilizes the information-sufficient domain to provide extra information to improve the representation learning on the information-insufficient domain via domain alignment. We provide data analysis and user model on two data sources: Consumer Consumption Information and Survey Information. We further test ADGAN on a real-world dataset with view embedding structures and show ADGAN can better deal with the class imbalance and unqualified data space than state-of-the-art, demonstrating the effectiveness of leveraging asymmetrical domain information.",0
"This paper presents a novel method for predicting risk tolerance by learning cross-domain alignment from adversarial examples. We propose a new approach that leverages deep neural networks to extract high-level features from both visual and textual data, allowing us to model complex patterns of human behavior associated with risk taking. Our method uses adversarial training techniques to improve the robustness and generalization performance of our model, which enables accurate prediction even across domains with significant differences in distribution or content. Experimental results on several benchmark datasets demonstrate the effectiveness of our proposed framework compared to state-of-the-art methods, showing improved accuracy for predicting individual differences in risk tolerance. Overall, our work advances the field of computational behavioral science by providing a powerful tool for understanding how individuals differ in their willingness to take risks, and has implications for applications such as financial decision making, healthcare management, and online communication design.",1
"Human bodies exhibit various shapes for different identities or poses, but the body shape has certain similarities in structure and thus can be embedded in a low-dimensional space. This paper presents an autoencoder-like network architecture to learn disentangled shape and pose embedding specifically for the 3D human body. This is inspired by recent progress of deformation-based latent representation learning. To improve the reconstruction accuracy, we propose a hierarchical reconstruction pipeline for the disentangling process and construct a large dataset of human body models with consistent connectivity for the learning of the neural network. Our learned embedding can not only achieve superior reconstruction accuracy but also provide great flexibility in 3D human body generation via interpolation, bilinear interpolation, and latent space sampling. The results from extensive experiments demonstrate the powerfulness of our learned 3D human body embedding in various applications.",0
"Disentangled representation learning has recently gained attention as a promising approach for enabling more natural control over latent variables during inference tasks such as image generation and editing. In order to achieve disentanglement, we need to model human body shape variation at different levels of abstraction using a hierarchy of neural networks that can capture global and local features effectively. Here, we propose a novel deep hierarchical network architecture called HBED (Hierarchical Body Embedding for Disentanglement) to learn a compact representation of human bodies based on their part correspondence. Our method jointly predicts human pose from RGB images, which allows us to leverage the large scale annotated datasets available for human keypoints prediction. We show our approach outperforms state-of-the-art methods on both quantitative metrics and qualitative results, which further demonstrate its effectiveness towards real world applications like content creation and digital humans in games, movies, virtual reality and augmented reality scenarios.",1
"Recent studies show that deep neural networks are vulnerable to adversarial examples which can be generated via certain types of transformations. Being robust to a desired family of adversarial attacks is then equivalent to being invariant to a family of transformations. Learning invariant representations then naturally emerges as an important goal to achieve which we explore in this paper within specific application contexts. Specifically, we propose a cyclically-trained adversarial network to learn a mapping from image space to latent representation space and back such that the latent representation is invariant to a specified factor of variation (e.g., identity). The learned mapping assures that the synthesized image is not only realistic, but has the same values for unspecified factors (e.g., pose and illumination) as the original image and a desired value of the specified factor. Unlike disentangled representation learning, which requires two latent spaces, one for specified and another for unspecified factors, invariant representation learning needs only one such space. We encourage invariance to a specified factor by applying adversarial training using a variational autoencoder in the image space as opposed to the latent space. We strengthen this invariance by introducing a cyclic training process (forward and backward cycle). We also propose a new method to evaluate conditional generative networks. It compares how well different factors of variation can be predicted from the synthesized, as opposed to real, images. In quantitative terms, our approach attains state-of-the-art performance in experiments spanning three datasets with factors such as identity, pose, illumination or style. Our method produces sharp, high-quality synthetic images with little visible artefacts compared to previous approaches.",0
"This paper presents a new method called cyclically trained adversarial network (CTAN) that learns invariant representations by solving Jensen-Shannon divergence minimization problem using adversarial training. CTAN consists of two subnetworks: a representation learning subnetwork and an adversarial attack subnetwork. The representation learning subnetwork maps input images into high-dimensional feature spaces, while the adversarial attack subnetwork produces adversarial examples by optimizing perturbations to fool the downstream task model. During each iteration, these two subnetworks alternate updating each other until they reach Nash equilibrium where the learned features are both discriminative and robust against small perturbations. Our proposed CTAN framework achieves state-of-the-art performance on several benchmark datasets including MNIST, CIFAR-10, and ImageNet under different settings such as white box attacks and black box attacks. Experimental results demonstrate that our method can effectively learn discriminative and robust representations compared with existing methods. Additionally, we visualize the latent space of pre-trained models using t-SNE and UMAP to show the superiority of our approach in generating informative embeddings.",1
"Most neural networks utilize the same amount of compute for every example independent of the inherent complexity of the input. Further, methods that adapt the amount of computation to the example focus on finding a fixed inference-time computational graph per example, ignoring any external computational budgets or varying inference time limitations. In this work, we utilize conditional computation to make neural sequence models (Transformer) more efficient and computation-aware during inference. We first modify the Transformer architecture, making each set of operations conditionally executable depending on the output of a learned control network. We then train this model in a multi-task setting, where each task corresponds to a particular computation budget. This allows us to train a single model that can be controlled to operate on different points of the computation-quality trade-off curve, depending on the available computation budget at inference time. We evaluate our approach on two tasks: (i) WMT English-French Translation and (ii) Unsupervised representation learning (BERT). Our experiments demonstrate that the proposed Conditional Computation Transformer (CCT) is competitive with vanilla Transformers when allowed to utilize its full computational budget, while improving significantly over computationally equivalent baselines when operating on smaller computational budgets.",0
"The paper explores the tradeoffs between controllability and quality when designing neural sequence models (NSMs) in natural language processing tasks such as machine translation (MT). NSMs have achieved state-of-the-art results on MT benchmark datasets but suffer from issues related to interpretability, transparency, and reproducibility due to their reliance on complex architectures, massive amounts of training data, and black box optimization algorithms. This paper proposes an analysis of two methods that control computation while optimizing MT quality: low rank factorization and subsequence modeling. These approaches enable efficient yet accurate inference in NLMs by reducing unnecessary computations without sacrificing quality. Experimental evaluations demonstrate that both methods can achieve comparable performance to full precision decoding, thereby providing a viable alternative solution to balance computational resources and quality in large scale natural language processing applications.",1
"The goal of few-shot learning is to recognize new visual concepts with just a few amount of labeled samples in each class. Recent effective metric-based few-shot approaches employ neural networks to learn a feature similarity comparison between query and support examples. However, the importance of feature embedding, i.e., exploring the relationship among training samples, is neglected. In this work, we present a simple yet powerful baseline for few-shot classification by emphasizing the importance of feature embedding. Specifically, we revisit the classical triplet network from deep metric learning, and extend it into a deep K-tuplet network for few-shot learning, utilizing the relationship among the input samples to learn a general representation learning via episode-training. Once trained, our network is able to extract discriminative features for unseen novel categories and can be seamlessly incorporated with a non-linear distance metric function to facilitate the few-shot classification. Our result on the miniImageNet benchmark outperforms other metric-based few-shot classification methods. More importantly, when evaluated on completely different datasets (Caltech-101, CUB-200, Stanford Dogs and Cars) using the model trained with miniImageNet, our method significantly outperforms prior methods, demonstrating its superior capability to generalize to unseen classes.",0
"This study revisits metric learning as a method for few-shot image classification. We evaluate several popular methods on a variety of benchmark datasets and find that simple baseline models can perform surprisingly well compared to more complex approaches. Our results suggest that the choice of dataset and evaluation protocols plays a significant role in determining which approaches are effective. We also observe that some commonly used metrics may not necessarily correspond to human perception of similarity, highlighting the need for more rigorous evaluations in this area. Overall, our work provides insights into the effectiveness of metric learning for few-shot image classification and sheds light on future directions for research in this field.",1
"Adversarial perturbations are noise-like patterns that can subtly change the data, while failing an otherwise accurate classifier. In this paper, we propose to use such perturbations within a novel contrastive learning setup to build negative samples, which are then used to produce improved video representations. To this end, given a well-trained deep model for per-frame video recognition, we first generate adversarial noise adapted to this model. Positive and negative bags are produced using the original data features from the full video sequence and their perturbed counterparts, respectively. Unlike the classic contrastive learning methods, we develop a binary classification problem that learns a set of discriminative hyperplanes -- as a subspace -- that will separate the two bags from each other. This subspace is then used as a descriptor for the video, dubbed \emph{discriminative subspace pooling}. As the perturbed features belong to data classes that are likely to be confused with the original features, the discriminative subspace will characterize parts of the feature space that are more representative of the original data, and thus may provide robust video representations. To learn such descriptors, we formulate a subspace learning objective on the Stiefel manifold and resort to Riemannian optimization methods for solving it efficiently. We provide experiments on several video datasets and demonstrate state-of-the-art results.",0
"This paper introduces a new method called “Contrastive Representation Learning via Adversarial Perturbations” (CReAL) for learning video representations that capture both spatial and temporal patterns. CReAL consists of two components: 1) contrastive representation learning based on adversarial perturbations, which encourages the model to focus on discriminative features; 2) spatio-temporal alignment, which aligns features across frames to capture temporal patterns. Experiments show that our proposed approach significantly outperforms state-of-the-art methods in action recognition tasks and improves zero-shot transfer performance on four benchmark datasets. We believe that our work has important implications for building robust video models that can generalize well to unseen domains. Abstract: In recent years, deep learning has achieved significant progress in computer vision due to advances in image classification, object detection, semantic segmentation, and many other tasks. However, video understanding remains a challenging problem due to complex motion dynamics and high dimensionality. In this paper, we present a novel approach called ""Contrastive Representation Learning via Adversarial Perturbations"" (CReAL) for learning video representations that capture both spatial and temporal patterns. Our method combines two main components: 1) contrastive representation learning using adversarial perturbations, aimed at enhancing the model’s ability to identify relevant visual features; 2) spatio-temporal alignment, designed to capture the interdependence between frames in videos. Using these principles, we construct a video encoder that learns representations applicable to diverse downstream applications. Extensive experiments demonstrate the superiority of our method compared against several baselines, achieving remarkable results on multiple widely adopted benchmark datasets such as UCF101, HMDB51, Sports8K, and Kinetics-400. These findings underscore the potential utility o",1
"How to utilize deep learning methods for graph classification tasks has attracted considerable research attention in the past few years. Regarding graph classification tasks, the graphs to be classified may have various graph sizes (i.e., different number of nodes and edges) and have various graph properties (e.g., average node degree, diameter, and clustering coefficient). The diverse property of graphs has imposed significant challenges on existing graph learning techniques since diverse graphs have different best-fit hyperparameters. It is difficult to learn graph features from a set of diverse graphs by a unified graph neural network. This motivates us to use a multiplex structure in a diverse way and utilize a priori properties of graphs to guide the learning. In this paper, we propose MxPool, which concurrently uses multiple graph convolution/pooling networks to build a hierarchical learning structure for graph representation learning tasks. Our experiments on numerous graph classification benchmarks show that our MxPool has superiority over other state-of-the-art graph representation learning methods.",0
"Abstract Recent advances in graph neural networks (GNNs) have shown promising results on various tasks, such as node classification and link prediction, by capturing both local structure and global patterns of graphs using pooling mechanisms. However, existing works focus mainly on preserving structural proximity relationships within GNN layers without considering higher-level semantic relationships among nodes in different layers. In this work, we propose MxPool, a novel multiplex pooling layer that can capture hierarchical dependencies across different layers and learn more expressive representations for downstream tasks. Our key idea is to introduce a weighted sum operation over different layers and use a fully connected network to compute attention weights between adjacent layers. We perform extensive experiments on several benchmark datasets, demonstrating state-of-the-art performance compared to previous methods. Our ablation studies further verify the effectiveness of each component in our proposed approach. Overall, MxPool provides a powerful tool for building robust and high-performance GNN models for complex real-world applications.",1
"The ability to predict, anticipate and reason about future outcomes is a key component of intelligent decision-making systems. In light of the success of deep learning in computer vision, deep-learning-based video prediction emerged as a promising research direction. Defined as a self-supervised learning task, video prediction represents a suitable framework for representation learning, as it demonstrated potential capabilities for extracting meaningful representations of the underlying patterns in natural videos. Motivated by the increasing interest in this task, we provide a review on the deep learning methods for prediction in video sequences. We firstly define the video prediction fundamentals, as well as mandatory background concepts and the most used datasets. Next, we carefully analyze existing video prediction models organized according to a proposed taxonomy, highlighting their contributions and their significance in the field. The summary of the datasets and methods is accompanied with experimental results that facilitate the assessment of the state of the art on a quantitative basis. The paper is summarized by drawing some general conclusions, identifying open research challenges and by pointing out future research directions.",0
"Advances in deep learning have led to significant improvements in video prediction tasks such as action recognition, traffic forecasting, and object tracking. This review presents an overview of recent deep learning techniques used for video prediction problems. We provide insights into the challenges associated with these tasks and discuss popular models that have been employed. These methods span from convolutional neural networks (CNNs) and recurrent neural networks (RNNs), to their variants like LSTMs, GRUs, and Transformers. Additionally, we explore advancements made through novel architectures, attention mechanisms, pretraining strategies, and transfer learning. Our comprehensive survey identifies open research areas, compares performance evaluations, and proposes future directions for improved accuracy in video prediction systems. By analyzing current approaches, our work serves both novice practitioners starting projects involving video analysis and more experienced researchers aiming to enhance existing solutions. Overall, this study intends to inform readers on cutting-edge developments for addressing intricate video forecasting scenarios. Keywords: deep learning; video prediction; CNN; RNN; LSTM; GRU; Transformer; attention mechanism; pretraining; transfer learning.",1
"One fundamental challenge of vehicle re-identification (re-id) is to learn robust and discriminative visual representation, given the significant intra-class vehicle variations across different camera views. As the existing vehicle datasets are limited in terms of training images and viewpoints, we propose to build a unique large-scale vehicle dataset (called VehicleNet) by harnessing four public vehicle datasets, and design a simple yet effective two-stage progressive approach to learning more robust visual representation from VehicleNet. The first stage of our approach is to learn the generic representation for all domains (i.e., source vehicle datasets) by training with the conventional classification loss. This stage relaxes the full alignment between the training and testing domains, as it is agnostic to the target vehicle domain. The second stage is to fine-tune the trained model purely based on the target vehicle set, by minimizing the distribution discrepancy between our VehicleNet and any target domain. We discuss our proposed multi-source dataset VehicleNet and evaluate the effectiveness of the two-stage progressive representation learning through extensive experiments. We achieve the state-of-art accuracy of 86.07% mAP on the private test set of AICity Challenge, and competitive results on two other public vehicle re-id datasets, i.e., VeRi-776 and VehicleID. We hope this new VehicleNet dataset and the learned robust representations can pave the way for vehicle re-id in the real-world environments.",0
"This paper presents a novel approach to vehicle re-identification using deep learning techniques. We introduce VehicleNet, a convolutional neural network that learns robust visual representations for identifying vehicles across different cameras and environments. Our method utilizes two branches to capture contextual information as well as fine details of the vehicle features. To handle occlusions, we use a partial attention module which focuses on relevant parts of the image. For stability during training, we apply an online hard example mining strategy. Experimental results on four challenging datasets show the superior performance of our model compared to state-of-the-art methods.",1
"Key features of mental illnesses are reflected in speech. Our research focuses on designing a multimodal deep learning structure that automatically extracts salient features from recorded speech samples for predicting various mental disorders including depression, bipolar, and schizophrenia. We adopt a variety of pre-trained models to extract embeddings from both audio and text segments. We use several state-of-the-art embedding techniques including BERT, FastText, and Doc2VecC for the text representation learning and WaveNet and VGG-ish models for audio encoding. We also leverage huge auxiliary emotion-labeled text and audio corpora to train emotion-specific embeddings and use transfer learning in order to address the problem of insufficient annotated multimodal data available. All these embeddings are then combined into a joint representation in a multimodal fusion layer and finally a recurrent neural network is used to predict the mental disorder. Our results show that mental disorders can be predicted with acceptable accuracy through multimodal analysis of clinical interviews.",0
"This research focuses on using multimodal deep learning techniques to analyze audio speech samples for mental disorder prediction. Previous studies have shown that mental disorders can manifest themselves through changes in voice patterns and other acoustic features such as pitch, volume, and variability. However, most existing approaches rely solely on audio data and lack integration of other modalities like visual cues. To address these limitations, we propose a novel framework that combines audio and video signals for improved predictive accuracy. Our proposed model leverages pre-trained deep neural networks, which process both audio and visual inputs concurrently, allowing the network to learn shared representations across different modalities. We evaluate our approach using publicly available datasets and demonstrate its effectiveness by comparing our results against state-of-the-art methods that use only audio signals. The findings show significant improvement in the prediction performance when integrating multiple sources of information, highlighting the importance of a multimodal approach for accurate detection and diagnosis of mental illnesses. Overall, this work represents a step forward towards building automated systems for analyzing vocal expressions associated with psychological conditions.",1
"Supervised deep learning requires a large amount of training samples with annotations (e.g. label class for classification task, pixel- or voxel-wised label map for segmentation tasks), which are expensive and time-consuming to obtain. During the training of a deep neural network, the annotated samples are fed into the network in a mini-batch way, where they are often regarded of equal importance. However, some of the samples may become less informative during training, as the magnitude of the gradient start to vanish for these samples. In the meantime, other samples of higher utility or hardness may be more demanded for the training process to proceed and require more exploitation. To address the challenges of expensive annotations and loss of sample informativeness, here we propose a novel training framework which adaptively selects informative samples that are fed to the training process. The adaptive selection or sampling is performed based on a hardness-aware strategy in the latent space constructed by a generative model. To evaluate the proposed training framework, we perform experiments on three different datasets, including MNIST and CIFAR-10 for image classification task and a medical image dataset IVUS for biophysical simulation task. On all three datasets, the proposed framework outperforms a random sampling method, which demonstrates the effectiveness of proposed framework.",0
"In recent years, deep representation learning has become increasingly popular due to its ability to learn complex data representations that can significantly improve performance on a wide range of tasks. However, training deep neural networks remains a challenging task as it often requires large amounts of computation time and resources, especially when dealing with high-dimensional datasets. To address these issues, we propose a novel method called adaptive latent space sampling (ALSS), which efficiently learns deep representations using sampling techniques in the latent space of autoencoders. Our approach modifies the traditional autoencoder architecture by introducing random noise into the latent code during training, effectively encouraging the network to learn more robust features that generalize well across different samples. We evaluate our method through extensive experiments on various benchmarks, including image classification, object detection, and reconstruction tasks. Results show significant improvements over baseline methods while achieving better sample efficiency and faster convergence rates. Overall, ALSS represents an effective solution towards efficient and accurate representation learning in deep neural networks.",1
"Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.",0
"Learning sparse representations of signals has recently become an important topic due to the increasing amount of data that needs to be processed by machine learning models. However, current methods often suffer from high computational complexity which makes them unfeasible for large scale applications. In this paper we present new techniques based on gradient descent optimization to improve the efficiency of training such models while maintaining comparable accuracy. Our approach effectively reduces the number of floating point operations (FLOPs) required during training without sacrificing performance. We demonstrate the effectiveness of our method using several benchmark datasets across different domains including computer vision, natural language processing, and audio signal processing. We believe that our work opens up exciting opportunities for developing more efficient models that can process vast amounts of data at lower computational cost.",1
"This paper considers learning deep features from long-tailed data. We observe that in the deep feature space, the head classes and the tail classes present different distribution patterns. The head classes have a relatively large spatial span, while the tail classes have significantly small spatial span, due to the lack of intra-class diversity. This uneven distribution between head and tail classes distorts the overall feature space, which compromises the discriminative ability of the learned features. Intuitively, we seek to expand the distribution of the tail classes by transferring from the head classes, so as to alleviate the distortion of the feature space. To this end, we propose to construct each feature into a ""feature cloud"". If a sample belongs to a tail class, the corresponding feature cloud will have relatively large distribution range, in compensation to its lack of diversity. It allows each tail sample to push the samples from other classes far away, recovering the intra-class diversity of tail classes. Extensive experimental evaluations on person re-identification and face recognition tasks confirm the effectiveness of our method.",0
"This is a great opportunity to practice my writing skills! Here is an abstract for your paper, ""Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective"":  Long-tail data distributions pose significant challenges in deep representation learning due to their imbalanced nature and limited availability compared to head categories. In this work, we propose a novel learnable embedding augmentation method that effectively addresses these issues by balancing the distribution of tail classes and generating synthetic samples. Our approach employs a meta learner that jointly optimizes both classifiers and embeddings during training, enabling effective exploitation of scarce tail classes. Through extensive evaluation across benchmark datasets, our approach significantly outperforms state-of-the-art methods in handling long-tailed data while achieving competitive performance overall. The key contributions of this study lie in exploring uncharted territory toward enhancing the robustness of deep representation models using a novel embedding augmentation strategy.",1
"We address the challenging problem of deep representation learning--the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. We show that our model provides a local linear approximation to an underlying deep model, and discuss important theoretical insights. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradient. Our method is evaluated across a number of representation-learning tasks on several datasets and using different network architectures. Strong results are obtained in all settings, and are well-aligned with our theoretical insights.",0
"Artificial intelligence has made significant advances in recent years due to the use of deep learning techniques that enable computers to learn from large amounts of data without explicit programming. One important aspect of deep representation learning is understanding how gradient descent, which is used to optimize parameters during training, can provide meaningful features for model inference. This work investigates the relationship between gradients and features and demonstrates how using gradients directly as features leads to better results on several benchmark datasets. We propose a new framework called ""gradient boosting,"" where we iteratively apply gradient descent on multiple copies of the current weights to generate more informative representations. Our experiments show that our method outperforms other state-of-the-art approaches by improving accuracy and achieving higher efficiency through faster convergence rates. These findings highlight the potential of using gradients as features for effective deep representation learning in artificial intelligence applications.",1
"In this paper, we propose FairNN a neural network that performs joint feature representation and classification for fairness-aware learning. Our approach optimizes a multi-objective loss function in which (a) learns a fair representation by suppressing protected attributes (b) maintains the information content by minimizing a reconstruction loss and (c) allows for solving a classification task in a fair manner by minimizing the classification error and respecting the equalized odds-based fairness regularized. Our experiments on a variety of datasets demonstrate that such a joint approach is superior to separate treatment of unfairness in representation learning or supervised learning. Additionally, our regularizers can be adaptively weighted to balance the different components of the loss function, thus allowing for a very general framework for conjoint fair representation learning and decision making.",0
"In recent years there has been growing interest from society, policy makers, and companies in addressing fairness in algorithmic decision making systems. This work presents FairNN, which learns compact and easy interpretable representations that capture the most relevant factors for decisions while ensuring their fairness. By doing so, we ensure those factors can be audited by human experts and other stakeholders. We show how these representations lead to more accurate predictions than the original features and how they transfer better across different datasets and models. Our approach uncovers hidden biases in algorithms and allows them to learn decision policies directly from data without relying on domain knowledge. Furthermore, our method enables fairness evaluators to focus on important dimensions only, rather than inspecting irrelevant ones. As such, this research makes significant strides towards increasing trustworthiness and interpretability of machine learning models used for high-stakes decisions. Ultimately, we contribute novel techniques towards achieving explainable and fair artificial intelligence.",1
"Recent advancements in learning Discrete Representations as opposed to continuous ones have led to state of art results in tasks that involve Language, Audio and Vision. Some latent factors such as words, phonemes and shapes are better represented by discrete latent variables as opposed to continuous. Vector Quantized Variational Autoencoders (VQVAE) have produced remarkable results in multiple domains. VQVAE learns a prior distribution $z_e$ along with its mapping to a discrete number of $K$ vectors (Vector Quantization). We propose applying VQ along the feature axis. We hypothesize that by doing so, we are learning a mapping between the codebook vectors and the marginal distribution of the prior feature space. Our approach leads to 33\% improvement as compared to prevous discrete models and has similar performance to state of the art auto-regressive models (e.g. PixelSNAIL). We evaluate our approach on a static prior using an artificial toy dataset (blobs). We further evaluate our approach on benchmarks for CIFAR-10 and ImageNet.",0
"This paper presents depthwise discrete representation learning (DDRL), a novel method that enables efficient deep neural network training at scale while preserving accurate model predictions. Inspired by recent advances in quantization techniques, DDRL utilizes a combination of continuous computation and integer arithmetic, allowing both high fidelity floating point computation during forward passes and highly optimized inference with minimal loss of quality. To achieve this balance, we propose a simple yet effective strategy that partitions weights into different groups according to their bitwidths, enabling adaptive precision scaling during backpropagation to minimize accuracy degradation while exploiting integer computation as much as possible. We demonstrate the efficacy of our approach through extensive experiments on popular computer vision benchmark datasets including ImageNet, COCO, Cityscapes, Kinetics, and UCF101, achieving state-of-the-art accuracies while using less than half the computational resources required by full-precision models. Our method provides a new paradigm that balances efficiency and accuracy in deep learning research and offers an attractive alternative to current approaches used for deploying large language models such as GPT-4 and Chinchilla.",1
"We investigate the use of Minimax distances to extract in a nonparametric way the features that capture the unknown underlying patterns and structures in the data. We develop a general-purpose and computationally efficient framework to employ Minimax distances with many machine learning methods that perform on numerical data. We study both computing the pairwise Minimax distances for all pairs of objects and as well as computing the Minimax distances of all the objects to/from a fixed (test) object.   We first efficiently compute the pairwise Minimax distances between the objects, using the equivalence of Minimax distances over a graph and over a minimum spanning tree constructed on that. Then, we perform an embedding of the pairwise Minimax distances into a new vector space, such that their squared Euclidean distances in the new space equal to the pairwise Minimax distances in the original space. We also study the case of having multiple pairwise Minimax matrices, instead of a single one. Thereby, we propose an embedding via first summing up the centered matrices and then performing an eigenvalue decomposition to obtain the relevant features.   In the following, we study computing Minimax distances from a fixed (test) object which can be used for instance in K-nearest neighbor search. Similar to the case of all-pair pairwise Minimax distances, we develop an efficient and general-purpose algorithm that is applicable with any arbitrary base distance measure. Moreover, we investigate in detail the edges selected by the Minimax distances and thereby explore the ability of Minimax distances in detecting outlier objects.   Finally, for each setting, we perform several experiments to demonstrate the effectiveness of our framework.",0
"In recent years, unsupervised representation learning has become increasingly important due to its ability to extract valuable features from raw data without any labeled examples. One popular method for achieving this is by using minimax distance measures such as earth mover’s distance (EMD) and Jenson-Shannon divergence (JSD). These methods have been shown to produce state-of-the-art results on a variety of tasks, including clustering, dimensionality reduction, and generative modeling.  In this paper, we present a framework for applying minimax distance measures to unsupervised representation learning problems. We begin by introducing a novel perspective on EMD and JSD that allows them to be applied in an efficient manner to high-dimensional data. We then develop several new algorithms based on these measures and demonstrate their effectiveness through extensive experiments on real-world datasets. Our results show that our methods consistently outperform baseline approaches across a wide range of evaluation metrics, providing strong evidence of their utility.  Overall, our work offers a powerful toolkit for practitioners seeking to extract meaningful representations from large datasets without requiring supervision. By bridging the gap between theory and practice, we hope to enable further advances in this rapidly evolving field.",1
"Person re-identification (Re-ID) in real-world scenarios usually suffers from various degradation factors, e.g., low-resolution, weak illumination, blurring and adverse weather. On the one hand, these degradations lead to severe discriminative information loss, which significantly obstructs identity representation learning; on the other hand, the feature mismatch problem caused by low-level visual variations greatly reduces retrieval performance. An intuitive solution to this problem is to utilize low-level image restoration methods to improve the image quality. However, existing restoration methods cannot directly serve to real-world Re-ID due to various limitations, e.g., the requirements of reference samples, domain gap between synthesis and reality, and incompatibility between low-level and high-level methods. In this paper, to solve the above problem, we propose a degradation invariance learning framework for real-world person Re-ID. By introducing a self-supervised disentangled representation learning strategy, our method is able to simultaneously extract identity-related robust features and remove real-world degradations without extra supervision. We use low-resolution images as the main demonstration, and experiments show that our approach is able to achieve state-of-the-art performance on several Re-ID benchmarks. In addition, our framework can be easily extended to other real-world degradation factors, such as weak illumination, with only a few modifications.",0
"""Person re-identification, which aims at matching images of the same person captured by different cameras, has recently gained increasing attention due to its numerous applications in security and surveillance systems. However, existing methods still struggle with handling variations such as viewpoint changes, background clutter, occlusions, illumination differences, and low resolutions. To tackle these challenges, we propose a novel approach that utilizes degradation invariant learning. By training on pairs of tampered and clean images, our method learns features that remain robust under significant image alterations. Our experimental results demonstrate that our framework outperforms state-of-the-art techniques across multiple datasets and metrics, achieving superior accuracy even on partial views and blurry inputs.""",1
"Most algorithms for representation learning and link prediction in relational data have been designed for static data. However, the data they are applied to usually evolves with time, such as friend graphs in social networks or user interactions with items in recommender systems. This is also the case for knowledge bases, which contain facts such as (US, has president, B. Obama, [2009-2017]) that are valid only at certain points in time. For the problem of link prediction under temporal constraints, i.e., answering queries such as (US, has president, ?, 2012), we propose a solution inspired by the canonical decomposition of tensors of order 4. We introduce new regularization schemes and present an extension of ComplEx (Trouillon et al., 2016) that achieves state-of-the-art performance. Additionally, we propose a new dataset for knowledge base completion constructed from Wikidata, larger than previous benchmarks by an order of magnitude, as a new reference for evaluating temporal and non-temporal link prediction methods.",0
"This is the most interesting work I have read all year! The authors present a new method for knowledge graph embedding which utilizes tensor decompositions to perform jointly reasoning on static and temporal data. They demonstrate state-of-the-art performance across multiple benchmark datasets, showcasing their framework as a powerful toolkit for researchers interested in building applications involving knowledge graphs such as question answering systems, recommendation engines, etc. A key contribution of this work is that their approach can reason about the evolution of entities over time by utilizing temporal context from background knowledge available in KGs. Overall, a fantastic contribution to the field and one worth exploring further for anyone looking at applying machine learning techniques to graph data!",1
"This paper presents a framework for the analysis of changes in visual streams: ordered sequences of images, possibly separated by significant time gaps. We propose a new approach to incorporating unlabeled data into training to generate natural language descriptions of change. We also develop a framework for estimating the time of change in visual stream. We use learned representations for change evidence and consistency of perceived change, and combine these in a regularized graph cut based change detector. Experimental evaluation on visual stream datasets, which we release as part of our contribution, shows that representation learning driven by natural language descriptions significantly improves change detection accuracy, compared to methods that do not rely on language.",0
"This abstract describes techniques that can detect changes across visual streams from different sources such as cameras or feeds. It provides a comprehensive approach for both describing and quantifying these changes within a single unified framework. The proposed method uses graph neural networks to model temporal dependencies between sequential frames, allowing it to capture subtle changes even if they occur over time. This enables us to better characterize dynamic events and distinguish them from static scenes. We showcase our solution on multiple challenging datasets and demonstrate its robustness through rigorous evaluations against other state-of-the-art methods. Our results clearly indicate significant improvements in change detection performance compared to previous work while providing detailed descriptions of detected changes. Overall, we present an effective, efficient, and flexible tool for monitoring complex systems where real-time updates are crucial, including security surveillance, traffic analysis, and healthcare applications.",1
"We consider the problem of learning low-dimensional representations for large-scale Markov chains. We formulate the task of representation learning as that of mapping the state space of the model to a low-dimensional state space, called the kernel space. The kernel space contains a set of meta states which are desired to be representative of only a small subset of original states. To promote this structural property, we constrain the number of nonzero entries of the mappings between the state space and the kernel space. By imposing the desired characteristics of the representation, we cast the problem as a constrained nonnegative matrix factorization. To compute the solution, we propose an efficient block coordinate gradient descent and theoretically analyze its convergence properties.",0
"This paper develops a new method for identifying sparse low-dimensional structures in markov chains. Using nonnegative matrix factorization techniques we can identify underlying factors that drive transition probabilities in the chain. By imposing sparsity constraints on these factors, we find structures which have only a few transitions likely active at any given time step. Additionally, by finding lower dimensional approximations of the original high-dimensional chains, our technique helps provide insight into how a system operates through easier visualizations and more manageable analysis. Our results suggest the effectiveness of our approach across a range of datasets and provide promise for future research in areas such as network science, finance, and epidemiology where analyzing complex systems is essential. Overall, this work provides both theoretical foundations and empirical evidence for using nonnegative matrix factorization with sparsity constraints to model large scale markov processes effectively.",1
"Unsupervised representation learning holds the promise of exploiting large amounts of unlabeled data to learn general representations. A promising technique for unsupervised learning is the framework of Variational Auto-encoders (VAEs). However, unsupervised representations learned by VAEs are significantly outperformed by those learned by supervised learning for recognition. Our hypothesis is that to learn useful representations for recognition the model needs to be encouraged to learn about repeating and consistent patterns in data. Drawing inspiration from the mid-level representation discovery work, we propose PatchVAE, that reasons about images at patch level. Our key contribution is a bottleneck formulation that encourages mid-level style representations in the VAE framework. Our experiments demonstrate that representations learned by our method perform much better on the recognition tasks compared to those learned by vanilla VAEs.",0
"This paper presents a new method called ""Patch VAE"" for learning latent representations that encode meaningful visual features from raw image data. The proposed model uses Variational Autoencoders (VAEs) as the backbone architecture, but introduces several key modifications aimed at improving their performance on recognition tasks. Firstly, the authors introduce a novel attention mechanism that allows the network to focus on local patches of the input image rather than processing the entire image at once. Secondly, they propose a new loss function that encourages the VAE to learn more discriminative latent codes by minimizing the reconstruction error only on relevant image patches attended to by the model. Experimental results demonstrate the effectiveness of the Patch VAE approach over several benchmark datasets such as MNIST, FashionMNIST, SVHN, and CIFAR-10. Overall, this work represents a significant advance in leveraging powerful generative models like VAEs for real-world vision tasks, paving the way for improved recognition accuracy and better understanding of complex images.",1
"The cognitive framework of conceptual spaces proposes to represent concepts as regions in psychological similarity spaces. These similarity spaces are typically obtained through multidimensional scaling (MDS), which converts human dissimilarity ratings for a fixed set of stimuli into a spatial representation. One can distinguish metric MDS (which assumes that the dissimilarity ratings are interval or ratio scaled) from nonmetric MDS (which only assumes an ordinal scale). In our first study, we show that despite its additional assumptions, metric MDS does not necessarily yield better solutions than nonmetric MDS. In this chapter, we furthermore propose to learn a mapping from raw stimuli into the similarity space using artificial neural networks (ANNs) in order to generalize the similarity space to unseen inputs. In our second study, we show that a linear regression from the activation vectors of a convolutional ANN to similarity spaces obtained by MDS can be successful and that the results are sensitive to the number of dimensions of the similarity space.",0
"In recent years, psychologists have developed similarity spaces based on human judgments of semantic relatedness, such as synonymy (e.g., cat-dog), analogy (e.g., bird-plane), and visual perception (e.g., starfish-spider). These models aim to capture the intrinsic structure of meaning that underlies these subjective assessments and can generalize well across different domains (e.g., linguistics and vision) and modalities (e.g., auditory-visual). Despite their successes, current models often struggle to account for new stimuli outside the training set or adapt to novel contexts. Here we propose a new framework called Graph Embeddings From Relational Information (GEFRI) to extend existing approaches by incorporating graph theory and neural network architectures. Our model first represents concepts as nodes embedded in a large knowledge graph where edges encode semantic relations. We then design a generative adversarial neural network to learn embeddings from relational data while preserving properties derived from graphs. Finally, we evaluate our approach on standard benchmark datasets using out-of-sample tests against competing methods and analyze qualitatively how GEFRI captures more subtle relationships missed by other representations. Overall, this work contributes a powerful tool to natural language processing researchers studying high-level cognitive phenomena (i.e., semantics and pragmatics) that reifies insights from both discrete mathematics and machine learning. By providing robust cross-domain mappings of conceptual knowledge, our method should enhance numerous NLP applications including question answering, text summarization, recommendation systems, and chatbots. With continued improvement along several promising research directions discussed below, we anticipate GEFRI could serve as a universal foundation for general intelligence within future advanced language models that seamlessly integrate information fr",1
"The vulnerability of automated fingerprint recognition systems to presentation attacks (PA), i.e., spoof or altered fingers, has been a growing concern, warranting the development of accurate and efficient presentation attack detection (PAD) methods. However, one major limitation of the existing PAD solutions is their poor generalization to new PA materials and fingerprint sensors, not used in training. In this study, we propose a robust PAD solution with improved cross-material and cross-sensor generalization. Specifically, we build on top of any CNN-based architecture trained for fingerprint spoof detection combined with cross-material spoof generalization using a style transfer network wrapper. We also incorporate adversarial representation learning (ARL) in deep neural networks (DNN) to learn sensor and material invariant representations for PAD. Experimental results on LivDet 2015 and 2017 public domain datasets exhibit the effectiveness of the proposed approach.",0
"This abstract describes a new approach for detecting fingerprint presentation attacks on biometric systems that can operate across different types of sensors (optical, capacitive, etc.) and materials used in creating fake fingers (gelatin, PlayDoh, etc.). We present a detailed analysis of three common attack methods - rubber glove, gelatin, and conductive gunge - and describe how our method uses vibration data captured from the sensor to distinguish genuine prints from these attacks. Our evaluation shows high accuracy in detecting attacks across all three classes, as well as strong robustness against variations in sensor technology and material composition. Overall, we believe this work represents a significant step towards more secure and reliable biometric authentication solutions.",1
"Similarity learning has gained a lot of attention from researches in recent years and tons of successful approaches have been recently proposed. However, the majority of the state-of-the-art similarity learning methods consider only a binary similarity. In this paper we introduce a new loss function called Continuous Histogram Loss (CHL) which generalizes recently proposed Histogram loss to multiple-valued similarities, i.e. allowing the acceptable values of similarity to be continuously distributed within some range. The novel loss function is computed by aggregating pairwise distances and similarities into 2D histograms in a differentiable manner and then computing the probability of condition that pairwise distances will not decrease as the similarities increase. The novel loss is capable of solving a wider range of tasks including similarity learning, representation learning and data visualization.",0
"In this paper, we explore new ways of using neural networks for image generation tasks. We introduce the continuous histogram loss function, which measures the similarity between two images by comparing their continuous color channel values. This allows us to achieve high levels of fidelity in generated images that were previously unattainable through traditional metrics such as mean squared error or structured similarity indices like peak signal-to-noise ratio (PSNR). Our approach outperforms previous state-of-the-art methods on multiple benchmarks across a range of applications including texture synthesis, superresolution, and style transfer. Additionally, we show how our method can be used to enhance visual features in medical imaging applications. Overall, our work represents a significant advancement in the field of neural network image generation and demonstrates the potential of continuous histogram loss as a powerful tool for achieving highly realistic images.",1
"Deep Reinforcement Learning (DRL) has been successfully applied in several research domains such as robot navigation and automated video game playing. However, these methods require excessive computation and interaction with the environment, so enhancements on sample efficiency are required. The main reason for this requirement is that sparse and delayed rewards do not provide an effective supervision for representation learning of deep neural networks. In this study, Proximal Policy Optimization (PPO) algorithm is augmented with Generative Adversarial Networks (GANs) to increase the sample efficiency by enforcing the network to learn efficient representations without depending on sparse and delayed rewards as supervision. The results show that an increased performance can be obtained by jointly training a DRL agent with a GAN discriminator.   ----   Derin Pekistirmeli Ogrenme, robot navigasyonu ve otomatiklestirilmis video oyunu oynama gibi arastirma alanlarinda basariyla uygulanmaktadir. Ancak, kullanilan yontemler ortam ile fazla miktarda etkilesim ve hesaplama gerektirmekte ve bu nedenle de ornek verimliligi yonunden iyilestirmelere ihtiyac duyulmaktadir. Bu gereksinimin en onemli nedeni, gecikmeli ve seyrek odul sinyallerinin derin yapay sinir aglarinin etkili betimlemeler ogrenebilmesi icin yeterli bir denetim saglayamamasidir. Bu calismada, Proksimal Politika Optimizasyonu algoritmasi Uretici Cekismeli Aglar (UCA) ile desteklenerek derin yapay sinir aglarinin seyrek ve gecikmeli odul sinyallerine bagimli olmaksizin etkili betimlemeler ogrenmesi tesvik edilmektedir. Elde edilen sonuclar onerilen algoritmanin ornek verimliliginde artis elde ettigini gostermektedir.",0
"This work proposes the use of generative adversarial networks (GANs) as a method for feature extraction in deep reinforcement learning (RL). GANs have shown great promise in other domains such as image generation and super-resolution, but their application to RL has been largely unexplored. We evaluate several architectures of GANs and explore their ability to extract features that can improve performance in popular Atari games. Our results show that GANs are able to outperform traditional methods of feature extraction, such as handcrafted features and convolutional neural networks (CNNs), in terms of both accuracy and speed. We also provide insights into how different design choices in GAN architecture affect performance in RL tasks. Overall, our findings suggest that GANs could become a valuable tool for researchers working on RL problems where efficient feature extraction is important.",1
"A discrete system's heterogeneity is measured by the R\'enyi heterogeneity family of indices (also known as Hill numbers or Hannah--Kay indices), whose units are {the numbers equivalent}. Unfortunately, numbers equivalent heterogeneity measures for non-categorical data require {a priori} (A) categorical partitioning and (B) pairwise distance measurement on the observable data space, thereby precluding application to problems with ill-defined categories or where semantically relevant features must be learned as abstractions from some data. We thus introduce representational R\'enyi heterogeneity (RRH), which transforms an observable domain onto a latent space upon which the R\'enyi heterogeneity is both tractable and semantically relevant. This method requires neither {a priori} binning nor definition of a distance function on the observable space. We show that RRH can generalize existing biodiversity and economic equality indices. Compared with existing indices on a beta-mixture distribution, we show that RRH responds more appropriately to changes in mixture component separation and weighting. Finally, we demonstrate the measurement of RRH in a set of natural images, with respect to abstract representations learned by a deep neural network. The RRH approach will further enable heterogeneity measurement in disciplines whose data do not easily conform to the assumptions of existing indices.",0
"""Representational Heterogeneity"" presents a novel framework that extends traditional homogeneous representations by introducing nonlinear dependencies within multiple modalities, facilitating improved performance on complex data. Our method utilizes higher order tensors, capturing interactions between different types of features such as visual objects and audio sounds, enabling better understanding and representation of multimedia content. Additionally, we introduce a new measure called Representational Rénvy Inequality (RRI) which quantifies representational diversity, allowing us to fine-tune our model to optimize both discriminative power and RRI scores. We demonstrate experimentally that our approach outperforms existing methods across a range of challenging tasks including image classification, object detection and video analysis, validating the effectiveness and general applicability of the proposed model and measures. This research has important implications for artificial intelligence, machine learning and computer vision, opening up possibilities for handling complex multimodal datasets and improving performance in real world applications.",1
"A good clustering algorithm can discover natural groupings in data. These groupings, if used wisely, provide a form of weak supervision for learning representations. In this work, we present Clustering-based Contrastive Learning (CCL), a new clustering-based representation learning approach that uses labels obtained from clustering along with video constraints to learn discriminative face features. We demonstrate our method on the challenging task of learning representations for video face clustering. Through several ablation studies, we analyze the impact of creating pair-wise positive and negative labels from different sources. Experiments on three challenging video face clustering datasets: BBT-0101, BF-0502, and ACCIO show that CCL achieves a new state-of-the-art on all datasets.",0
"Abstract: In recent years, deep learning approaches have revolutionized face recognition research by significantly improving performance on benchmark datasets. One such approach that has gained popularity is clustering based contrastive learning, which uses the power of unsupervised representation learning combined with large scale training data to achieve state-of-the-art results. This paper presents a detailed analysis of clustering based contrastive learning for improving face representations in the wild. We first provide a comprehensive literature review discussing the existing frameworks, their strengths and weaknesses, and how they differ from each other. Then we propose a novel algorithmic framework that addresses some of the limitations observed in the current methods. Our contributions can be summarized as follows: -We introduce a new clustering mechanism called dynamic k-means clustering which adaptively reassigns samples based on their similarity scores at each iteration; -We propose a new objective function designed to learn discriminative features without explicit supervision; -Extensive experiments conducted using three challenging benchmark datasets demonstrate the superiority of our method compared to the previous works; -Furthermore, we conduct ablation studies to showcase the effectiveness of our proposed components. Overall, our work represents an important step towards understanding the strengths and weaknesses of clustering based contrastive learning, and offers valuable insights into future directions for researchers interested in this area. --- title: Clustering based Contrastive Learning for Improving Face Repre",1
"Semantic segmentation is an essential step for electron microscopy (EM) image analysis. Although supervised models have achieved significant progress, the need for labor intensive pixel-wise annotation is a major limitation. To complicate matters further, supervised learning models may not generalize well on a novel dataset due to domain shift. In this study, we introduce an adversarial-prediction guided multi-task network to learn the adaptation of a well-trained model for use on a novel unlabeled target domain. Since no label is available on target domain, we learn an encoding representation not only for the supervised segmentation on source domain but also for unsupervised reconstruction of the target data. To improve the discriminative ability with geometrical cues, we further guide the representation learning by multi-level adversarial learning in semantic prediction space. Comparisons and ablation study on public benchmark demonstrated state-of-the-art performance and effectiveness of our approach.",0
"In this paper we present Adversarial Prediction Guided Multi-Task Adaptation (APG-MTA), a new method for semantic segmentation of electron microscopy images that exploits multi-scale features and adversarial guidance to improve predictions in challenging regions such as mitochondria and lysosomes. We evaluate APG-MTA on two challenging datasets of EM images: ISBI2016 and EMDS2019. Our results show significant improvement over other state-of-the-art methods for semantic segmentation of EM images, achieving mean IoU of 78% on the ISBI2016 dataset and 54% on the EMDS2019 dataset. By leveraging adversarial prediction error maps and multi-scale features, APG-MTA effectively guides and improves local segmentation accuracy without requiring heavy computational costs or additional annotations. This makes our approach well-suited for real-time applications where fast inference speed and low memory footprint are crucial. Overall, APG-MTA paves the way towards more accurate and efficient automatic annotation tools for biomedical image analysis.",1
"Face presentation attack detection (PAD) has been an urgent problem to be solved in the face recognition systems. Conventional approaches usually assume the testing and training are within the same domain; as a result, they may not generalize well into unseen scenarios because the representations learned for PAD may overfit to the subjects in the training set. In light of this, we propose an efficient disentangled representation learning for cross-domain face PAD. Our approach consists of disentangled representation learning (DR-Net) and multi-domain learning (MD-Net). DR-Net learns a pair of encoders via generative models that can disentangle PAD informative features from subject discriminative features. The disentangled features from different domains are fed to MD-Net which learns domain-independent features for the final cross-domain face PAD task. Extensive experiments on several public datasets validate the effectiveness of the proposed approach for cross-domain PAD.",0
"In recent years, there has been growing concern over the vulnerability of face recognition systems to presentation attacks using printouts or digital images of faces (so-called ""presentation attack",1
"Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on few-shot gaze estimation (competitive results can be achieved with as few as = 100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework.",0
"In recent years, gaze estimation has become increasingly important in computer vision research due to its numerous applications such as human-computer interaction, driver monitoring systems, and autism diagnosis. Despite the many advancements made in the field, obtaining high-quality annotations remains challenging due to the time-consuming and subjective nature of labeling gaze direction. To address these limitations, we propose an unsupervised representation learning framework that can learn features from raw image data without explicit labels. We design our model using self-supervision techniques which exploit intrinsic properties of visual scenes observed by humans in everyday life to generate meaningful representations for gaze estimation. Experimental results on two publicly available datasets demonstrate significant improvements over state-of-the-art supervised approaches while requiring only a fraction of annotated data for training. Our findings highlight the potential benefits of utilizing unlabeled data in deep learning frameworks, paving the way for more efficient and effective algorithms in computer vision tasks.",1
"Although Shannon theory states that it is asymptotically optimal to separate the source and channel coding as two independent processes, in many practical communication scenarios this decomposition is limited by the finite bit-length and computational power for decoding. Recently, neural joint source-channel coding (NECST) is proposed to sidestep this problem. While it leverages the advancements of amortized inference and deep learning to improve the encoding and decoding process, it still cannot always achieve compelling results in terms of compression and error correction performance due to the limited robustness of its learned coding networks. In this paper, motivated by the inherent connections between neural joint source-channel coding and discrete representation learning, we propose a novel regularization method called Infomax Adversarial-Bit-Flip (IABF) to improve the stability and robustness of the neural joint source-channel coding scheme. More specifically, on the encoder side, we propose to explicitly maximize the mutual information between the codeword and data; while on the decoder side, the amortized reconstruction is regularized within an adversarial framework. Extensive experiments conducted on various real-world datasets evidence that our IABF can achieve state-of-the-art performances on both compression and error correction benchmarks and outperform the baselines by a significant margin.",0
"In this research paper, we explore the application of joint source-channel coding (JSCC) techniques in the field of neural processing. JSCC refers to the process of simultaneously encoding both the source data and the channel parameters in order to optimize overall system performance. We propose using adversarial bit flipping as a means of incorporating additional entropy into the transmitted signal, enabling more efficient use of bandwidth resources while maintaining robustness against noise and interference. Our approach involves training two competing models: one that generates encoded versions of the source data, and another that attempts to detect and reverse any intentional bit changes introduced by the first model. This setup leads to a zero-sum game wherein the transmitter and receiver work together to maximize information transmission efficiency while minimizing error rates. Experimental results demonstrate significant improvements over traditional JSCC methods, particularly in high-noise environments. Our findings have important implications for next-generation wireless communication systems, which must accommodate increasingly complex user requirements and content types under tight resource constraints.",1
"In this paper, we study a new representation-learning task, which we termed as disassembling object representations. Given an image featuring multiple objects, the goal of disassembling is to acquire a latent representation, of which each part corresponds to one category of objects. Disassembling thus finds its application in a wide domain such as image editing and few- or zero-shot learning, as it enables category-specific modularity in the learned representations. To this end, we propose an unsupervised approach to achieving disassembling, named Unsupervised Disassembling Object Representation (UDOR). UDOR follows a double auto-encoder architecture, in which a fuzzy classification and an object-removing operation are imposed. The fuzzy classification constrains each part of the latent representation to encode features of up to one object category, while the object-removing, combined with a generative adversarial network, enforces the modularity of the representations and integrity of the reconstructed image. Furthermore, we devise two metrics to respectively measure the modularity of disassembled representations and the visual integrity of reconstructed images. Experimental results demonstrate that the proposed UDOR, despited unsupervised, achieves truly encouraging results on par with those of supervised methods.",0
"This paper presents a method for disassembling object representations in computer vision models without labels. We propose a novel approach that utilizes self attention mechanisms to analyze the relationships between different parts of an image and how they relate to each other. Our algorithm can identify key features and components of objects, allowing us to accurately describe their shape and appearance. Through extensive experiments on several benchmark datasets, we demonstrate that our method outperforms state-of-the-art label-free approaches in terms of accuracy and efficiency. By providing an effective way to deconstruct object representations without relying on explicit annotations, our work paves the way for new applications in unsupervised learning and zero-shot recognition tasks.",1
"Accurately predicting drug-target binding affinity (DTA) in silico is a key task in drug discovery. Most of the conventional DTA prediction methods are simulation-based, which rely heavily on domain knowledge or the assumption of having the 3D structure of the targets, which are often difficult to obtain. Meanwhile, traditional machine learning-based methods apply various features and descriptors, and simply depend on the similarities between drug-target pairs. Recently, with the increasing amount of affinity data available and the success of deep representation learning models on various domains, deep learning techniques have been applied to DTA prediction. However, these methods consider either label/one-hot encodings or the topological structure of molecules, without considering the local chemical context of amino acids and SMILES sequences. Motivated by this, we propose a novel end-to-end learning framework, called DeepGS, which uses deep neural networks to extract the local chemical context from amino acids and SMILES sequences, as well as the molecular structure from the drugs. To assist the operations on the symbolic data, we propose to use advanced embedding techniques (i.e., Smi2Vec and Prot2Vec) to encode the amino acids and SMILES sequences to a distributed representation. Meanwhile, we suggest a new molecular structure modeling approach that works well under our framework. We have conducted extensive experiments to compare our proposed method with state-of-the-art models including KronRLS, SimBoost, DeepDTA and DeepCPI. Extensive experimental results demonstrate the superiorities and competitiveness of DeepGS.",0
"In recent years, drug discovery has become increasingly reliant on computational methods for predicting the binding affinities of drugs and their potential targets. One important aspect of these predictions involves modeling the complex relationships between chemical structures and molecular interactions using graph-based representations of molecules. However, traditional representation learning approaches often struggle to capture the intricate patterns present in these graphs due to limitations in data size and quality. To address these challenges, we propose a novel method called DeepGS that combines deep learning techniques with feature engineering to learn effective representations from both graph and sequence data. Our approach integrates multiple features into the model, including atom pairs, local atomic environment descriptors, and fingerprint-like codes extracted from molecule graphs. We then train our model using a convolutional neural network architecture that enables efficient parallelization across GPUs, resulting in faster training times compared to state-of-the-art alternatives. Evaluating our model on a diverse set of benchmark datasets, we demonstrate significant improvements over existing baselines in terms of accuracy, robustness, and interpretability. This work represents a step forward towards the development of accurate and scalable machine learning models for drug target prediction, which could ultimately lead to more effective drug discovery processes.",1
"We propose an algorithm, guided variational autoencoder (Guided-VAE), that is able to learn a controllable generative model by performing latent representation disentanglement learning. The learning objective is achieved by providing signals to the latent encoding/embedding in VAE without changing its main backbone architecture, hence retaining the desirable properties of the VAE. We design an unsupervised strategy and a supervised strategy in Guided-VAE and observe enhanced modeling and controlling capability over the vanilla VAE. In the unsupervised strategy, we guide the VAE learning by introducing a lightweight decoder that learns latent geometric transformation and principal components; in the supervised strategy, we use an adversarial excitation and inhibition mechanism to encourage the disentanglement of the latent variables. Guided-VAE enjoys its transparency and simplicity for the general representation learning task, as well as disentanglement learning. On a number of experiments for representation learning, improved synthesis/sampling, better disentanglement for classification, and reduced classification errors in meta-learning have been observed.",0
"In recent years, generative models such as variational autoencoders (VAEs) have shown great promise in generating realistic images from textual descriptions. However, these models often struggle with producing coherence in their generated outputs due to issues related to mode collapse and disentanglement learning. This paper proposes guided VAEs to address these challenges by incorporating domain knowledge into model training through text prompts that guide the learning process towards desired output characteristics such as disentangled representations of image content. By leveraging prior knowledge in the form of high-level descriptive keywords, we show how our proposed method can learn more interpretable features that generalize better across different domains and tasks. Our experiments demonstrate consistent improvements over strong baselines on both qualitative metrics and downstream applications such as semi-supervised classification. The implications of this work extend beyond computer vision to areas where data is scarce but relevant guidance is available in natural language format, opening up new possibilities for enhancing artificial intelligence systems.",1
"Recent work in graph neural networks (GNNs) has led to improvements in molecular activity and property prediction tasks. Unfortunately, GNNs often fail to capture the relative importance of interactions between molecular substructures, in part due to the absence of efficient intermediate pooling steps. To address these issues, we propose LaPool (Laplacian Pooling), a novel, data-driven, and interpretable hierarchical graph pooling method that takes into account both node features and graph structure to improve molecular representation. We benchmark LaPool on molecular graph prediction and understanding tasks and show that it outperforms recent GNNs. Interestingly, LaPool also remains competitive on non-molecular tasks. Both quantitative and qualitative assessments are done to demonstrate LaPool's improved interpretability and highlight its potential benefits in drug design. Finally, we demonstrate LaPool's utility for the generation of valid and novel molecules by incorporating it into an adversarial autoencoder.",0
"In recent years, graph representation learning has become increasingly important due to the growing prevalence of data on networks. However, many existing methods suffer from several limitations such as lack of interpretability and efficiency issues. To address these problems, we propose a new approach that uses Laplacian pooling to learn sparse graph representations while preserving interpretability. Our method is based on the concept of subspace clustering and utilizes Laplacian eigenmaps to extract relevant features. We perform experiments on real world datasets and demonstrate the effectiveness of our approach through comparisons with state-of-the-art algorithms. Our results show improved performance and enhanced interpretability of learned graphs. This work makes significant contributions towards building more accurate, efficient, and interpretable models for graph representation learning.",1
"As humans, we inherently perceive images based on their predominant features, and ignore noise embedded within lower bit planes. On the contrary, Deep Neural Networks are known to confidently misclassify images corrupted with meticulously crafted perturbations that are nearly imperceptible to the human eye. In this work, we attempt to address this problem by training networks to form coarse impressions based on the information in higher bit planes, and use the lower bit planes only to refine their prediction. We demonstrate that, by imposing consistency on the representations learned across differently quantized images, the adversarial robustness of networks improves significantly when compared to a normally trained model. Present state-of-the-art defenses against adversarial attacks require the networks to be explicitly trained using adversarial samples that are computationally expensive to generate. While such methods that use adversarial training continue to achieve the best results, this work paves the way towards achieving robustness without having to explicitly train on adversarial samples. The proposed approach is therefore faster, and also closer to the natural learning process in humans.",0
"This paper presents a method for improving adversarial robustness by enforcing feature consistency across bit planes in deep neural networks (DNNs). In recent years, DNNs have been found to be vulnerable to so-called ""adversarial examples,"" which are inputs that cause these models to produce incorrect outputs. Many approaches have been proposed to increase the robustness of DNNs against such attacks, but their performance remains limited. Our approach introduces a simple and effective regularizer that encourages feature maps at different depths within a network to exhibit similar patterns, even if their resolution differs due to quantization. We demonstrate through extensive experiments on benchmark datasets that our method significantly boosts adversarial robustness without sacrificing accuracy on clean data. Additionally, we provide insights into why feature consistency is important for achieving stronger robustness and discuss the limitations and potential future directions of our work. Overall, our findings contribute to the ongoing effort to develop more secure machine learning systems.",1
"In neuroscience, understanding inter-individual differences has recently emerged as a major challenge, for which functional magnetic resonance imaging (fMRI) has proven invaluable. For this, neuroscientists rely on basic methods such as univariate linear correlations between single brain features and a score that quantifies either the severity of a disease or the subject's performance in a cognitive task. However, to this date, task-fMRI and resting-state fMRI have been exploited separately for this question, because of the lack of methods to effectively combine them. In this paper, we introduce a novel machine learning method which allows combining the activation-and connectivity-based information respectively measured through these two fMRI protocols to identify markers of individual differences in the functional organization of the brain. It combines a multi-view deep autoencoder which is designed to fuse the two fMRI modalities into a joint representation space within which a predictive model is trained to guess a scalar score that characterizes the patient. Our experimental results demonstrate the ability of the proposed method to outperform competitive approaches and to produce interpretable and biologically plausible results.",0
"Individual differences in brain structure have been shown to play a significant role in human behavior, cognition, and mental health. Recent advances in magnetic resonance imaging (MRI) technology have enabled researchers to study these differences at a more fine-grained level than ever before. However, analyzing such data remains a challenging task due to the complexity and high dimensionality of brain images. In this paper, we propose a novel approach based on multi-view representation learning for mapping individual differences in cortical architecture. Our method leverages multiple views of brain data, including structural MRI (sMRI), functional MRI (fMRI), diffusion weighted imaging (DWI), and resting state fMRI (rsfMRI). By combining these different views in a joint embedding space, our model captures both local and global features of brain organization that may influence neurocognitive processes. To validate our approach, we apply it to publicly available datasets of healthy individuals and patient populations with psychiatric disorders, demonstrating its effectiveness in identifying patterns of cortical variability associated with disease status, age, gender, education, and other relevant covariates. Our findings highlight the potential utility of multi-view representation learning for understanding complex relationships between brain morphology, function, and behavior. Overall, this work contributes new insights into the nature of individual differences in brain development and plasticity, with implications for neuroscience, psychology, and medicine.",1
"Deep learning is recognized to be capable of discovering deep features for representation learning and pattern recognition without requiring elegant feature engineering techniques by taking advantage of human ingenuity and prior knowledge. Thus it has triggered enormous research activities in machine learning and pattern recognition. One of the most important challenge of deep learning is to figure out relations between a feature and the depth of deep neural networks (deep nets for short) to reflect the necessity of depth. Our purpose is to quantify this feature-depth correspondence in feature extraction and generalization. We present the adaptivity of features to depths and vice-verse via showing a depth-parameter trade-off in extracting both single feature and composite features. Based on these results, we prove that implementing the classical empirical risk minimization on deep nets can achieve the optimal generalization performance for numerous learning tasks. Our theoretical results are verified by a series of numerical experiments including toy simulations and a real application of earthquake seismic intensity prediction.",0
"This study presents a novel method for selecting depths for deep ReLU neural networks (NN) in feature extraction tasks. In recent years, using deep NN has become essential due to their ability to approximate complex functions that achieve state-of-the-art results on many problems. However, selecting appropriate depth remains challenging due to high computational demands during training. To tackle this issue, we propose a depth selection scheme based on an analysis of Lipschitz continuity constraints and generalization bounds that quantifies how sensitive model outputs are to small perturbations of inputs. We prove that under mild conditions, our scheme can significantly reduce depth without loss of accuracy, enabling faster computation times and improved generalization properties. Our approach extends existing regularizers by optimizing over multiple scales that adapt to data complexity. Experiments show significant improvements across datasets and architectures, yielding simpler models with similar performance but requiring less time to train. Overall, our work offers new insights into depth selection for feature extraction and improves upon current methods.",1
"Most object recognition approaches predominantly focus on learning discriminative visual patterns while overlooking the holistic object structure. Though important, structure modeling usually requires significant manual annotations and therefore is labor-intensive. In this paper, we propose to ""look into object"" (explicitly yet intrinsically model the object structure) through incorporating self-supervisions into the traditional framework. We show the recognition backbone can be substantially enhanced for more robust representation learning, without any cost of extra annotation and inference speed. Specifically, we first propose an object-extent learning module for localizing the object according to the visual patterns shared among the instances in the same category. We then design a spatial context learning module for modeling the internal structures of the object, through predicting the relative positions within the extent. These two modules can be easily plugged into any backbone networks during training and detached at inference time. Extensive experiments show that our look-into-object approach (LIO) achieves large performance gain on a number of benchmarks, including generic object recognition (ImageNet) and fine-grained object recognition tasks (CUB, Cars, Aircraft). We also show that this learning paradigm is highly generalizable to other tasks such as object detection and segmentation (MS COCO). Project page: https://github.com/JDAI-CV/LIO.",0
"This paper presents a self-supervised approach for modeling three-dimensional structures of objects from single images using convolutional neural networks (CNNs). Our method, called look-into-object (LOOK), generates synthetic views of objects by rotating them along their principal axis, predicts object poses that align these virtual views with real images, and trains CNN models directly on such pseudo ground truth data without any manual annotations. We introduce an effective algorithm to optimize viewpoint positions and orientations via gradient ascent, which exploits object symmetry, and empirically show that LOOK leads to high-performing models that achieve top ranks at standard benchmark tests.",1
"Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.",0
Image processing using multi-code GAN prior,1
"Graph representation learning (GRL) is a powerful technique for learning low-dimensional vector representation of high-dimensional and often sparse graphs. Most studies explore the structure and metadata associated with the graph using random walks and employ an unsupervised or semi-supervised learning schemes. Learning in these methods is context-free, resulting in only a single representation per node. Recently studies have argued on the adequacy of a single representation and proposed context-sensitive approaches, which are capable of extracting multiple node representations for different contexts. This proved to be highly effective in applications such as link prediction and ranking.   However, most of these methods rely on additional textual features that require complex and expensive RNNs or CNNs to capture high-level features or rely on a community detection algorithm to identify multiple contexts of a node.   In this study we show that in-order to extract high-quality context-sensitive node representations it is not needed to rely on supplementary node features, nor to employ computationally heavy and complex models. We propose GOAT, a context-sensitive algorithm inspired by gossip communication and a mutual attention mechanism simply over the structure of the graph. We show the efficacy of GOAT using 6 real-world datasets on link prediction and node clustering tasks and compare it against 12 popular and state-of-the-art (SOTA) baselines. GOAT consistently outperforms them and achieves up to 12% and 19% gain over the best performing methods on link prediction and clustering tasks, respectively.",0
"In recent years, graph representation learning has become increasingly popular due to the ability of graphs to capture complex relationships between entities. However, most existing methods treat each node equally without considering contextual information that may affect the importance of certain connections. This paper proposes a novel approach called ""Gossip and Attend"" which leverages the intuition behind gossip in social networks to dynamically weight edges based on their influence and context. Our method integrates attention mechanisms to selectively focus on important parts of the graph during training while minimizing computational cost. We demonstrate through extensive experiments that our model achieves state-of-the-art performance across a wide range of benchmark datasets including citation networks, knowledge graphs, and social media platforms.",1
"In the current monocular depth research, the dominant approach is to employ unsupervised training on large datasets, driven by warped photometric consistency. Such approaches lack robustness and are unable to generalize to challenging domains such as nighttime scenes or adverse weather conditions where assumptions about photometric consistency break down.   We propose DeFeat-Net (Depth & Feature network), an approach to simultaneously learn a cross-domain dense feature representation, alongside a robust depth-estimation framework based on warped feature consistency. The resulting feature representation is learned in an unsupervised manner with no explicit ground-truth correspondences required.   We show that within a single domain, our technique is comparable to both the current state of the art in monocular depth estimation and supervised feature representation learning. However, by simultaneously learning features, depth and motion, our technique is able to generalize to challenging domains, allowing DeFeat-Net to outperform the current state-of-the-art with around 10% reduction in all error measures on more challenging sequences such as nighttime driving.",0
"In recent years, deep neural networks have demonstrated their ability to excel at monocular depth estimation tasks using large annotated datasets, however obtaining such data can be time-consuming and costly. To address this challenge, we propose DeFeat-Net, a novel approach that enables simultaneous representation learning without explicit supervision. Our method relies on a convolutional neural network architecture equipped with dual feature discrimination losses, allowing us to learn meaningful features in unlabeled settings. We evaluate our approach on several benchmark datasets and demonstrate state-of-the-art performance while requiring significantly fewer labeled samples compared to competing methods. These results suggest that our framework holds great potential for deploying accurate and efficient depth prediction systems in real-world applications where annotation costs are high.",1
"Network embedding is an effective method to learn low-dimensional representations of nodes, which can be applied to various real-life applications such as visualization, node classification, and link prediction. Although significant progress has been made on this problem in recent years, several important challenges remain, such as how to properly capture temporal information in evolving networks. In practice, most networks are continually evolving. Some networks only add new edges or nodes such as authorship networks, while others support removal of nodes or edges such as internet data routing. If patterns exist in the changes of the network structure, we can better understand the relationships between nodes and the evolution of the network, which can be further leveraged to learn node representations with more meaningful information. In this paper, we propose the Embedding via Historical Neighborhoods Aggregation (EHNA) algorithm. More specifically, we first propose a temporal random walk that can identify relevant nodes in historical neighborhoods which have impact on edge formations. Then we apply a deep learning model which uses a custom attention mechanism to induce node embeddings that directly capture temporal information in the underlying feature representation. We perform extensive experiments on a range of real-world datasets, and the results demonstrate the effectiveness of our new approach in the network reconstruction task and the link prediction task.",0
"This article presents a novel approach for learning temporal network representations from sparse data by aggregating historical neighborhoods through a three step process: (1) defining local reference graphs using time windows of uniform length, (2) obtaining snapshots of each graph as static networks at regular intervals, and (3) computing weighted embeddings that encode both structural and attribute similarities among nodes across different snapshots. Our method, called TNRLearn, balances the importance of transitivity, centrality, and attribute preservation to maintain the representativeness of learned embeddings. We evaluate our approach on four real world datasets representing diverse domains such as social interaction, communication, biological systems, and knowledge bases to demonstrate its effectiveness over state-of-the-art baselines in terms of accuracy and robustness while providing explainability via node similarity measures computed directly from our learned representations. Our results show that temporal aspects can significantly improve the quality of obtained embeddings due to their ability to capture evolving relationships and dependencies between entities, making them ideal tools for temporal analysis tasks and applications where dynamic behavior modeling is essential.",1
"It is known that, without awareness of the process, our brain appears to focus on the general shape of objects rather than superficial statistics of context. On the other hand, learning autonomously allows discovering invariant regularities which help generalization. In this work, we propose a learning framework to improve the shape bias property of self-supervised methods. Our method learns semantic and shape biased representations by integrating domain diversification and jigsaw puzzles. The first module enables the model to create a dynamic environment across arbitrary domains and provides a domain exploration vs. exploitation trade-off, while the second module allows the model to explore this environment autonomously. This universal framework does not require prior knowledge of the domain of interest. Extensive experiments are conducted on several domain generalization datasets, namely, PACS, Office-Home, VLCS, and Digits. We show that our framework outperforms state-of-the-art domain generalization methods by a large margin.",0
"This paper presents a novel approach to unsupervised representation learning that is biased towards shape. Our method leverages recent advances in geometry processing to learn a latent space that captures intrinsic structure across domains. By aligning high-level geometric features with low-level image characteristics, we can learn representations that generalize effectively across different datasets without any labeled examples from target domains. We evaluate our algorithm on several benchmarks for domain adaptation, including both synthetic and real-world datasets, demonstrating state-of-the-art performance in many cases. We further analyze the properties of learned representations and showcase their effectiveness for downstream tasks such as semantic segmentation and object detection. Overall, our work represents a significant step forward in enabling artificial intelligence systems to operate in complex, changing environments without explicit supervision.",1
"Local and global patterns of an object are closely related. Although each part of an object is incomplete, the underlying attributes about the object are shared among all parts, which makes reasoning the whole object from a single part possible. We hypothesize that a powerful representation of a 3D object should model the attributes that are shared between parts and the whole object, and distinguishable from other objects. Based on this hypothesis, we propose to learn point cloud representation by bidirectional reasoning between the local structures at different abstraction hierarchies and the global shape without human supervision. Experimental results on various benchmark datasets demonstrate the unsupervisedly learned representation is even better than supervised representation in discriminative power, generalization ability, and robustness. We show that unsupervisedly trained point cloud models can outperform their supervised counterparts on downstream classification tasks. Most notably, by simply increasing the channel width of an SSG PointNet++, our unsupervised model surpasses the state-of-the-art supervised methods on both synthetic and real-world 3D object classification datasets. We expect our observations to offer a new perspective on learning better representation from data structures instead of human annotations for point cloud understanding.",0
"In recent years, unsupervised representation learning has emerged as a promising approach towards acquiring meaningful representations of complex data types such as point clouds. One key challenge facing these methods is how to effectively leverage both global context (such as shape geometry) and local neighborhood relationships (such as relative spatial arrangements). This paper presents a novel framework, dubbed ""global-local bidirectional reasoning,"" which addresses this tradeoff by jointly optimizing over fine-grained local features that capture intricate geometric details and over high-level global descriptors that provide semantically meaningful context. Our method operates directly on raw 3D point cloud data without requiring any form of preprocessing, making it particularly suitable for applications involving real-world scenes. Experimental evaluation against state-of-the-art alternatives demonstrates significant improvements across a variety of benchmark datasets, showcasing the effectiveness of our proposed approach.",1
"In this paper, we propose and end-to-end deep Chinese font generation system. This system can generate new style fonts by interpolation of latent style-related embeding variables that could achieve smooth transition between different style. Our method is simpler and more effective than other methods, which will help to improve the font design efficiency",0
"In this paper we present a novel method for generating realistic handwritten Chinese characters using fonts style representation learning. We train a deep neural network on large datasets of human written Chinese characters and use it to generate new characters that match the style of specific font styles such as Mingti, Heiti, or Microsoft JhengHei. Our approach first converts each character into a vector space where different aspects of stroke structure can be manipulated. This allows us to control key characteristics of how characters are rendered such as thickness, slant and curvature. We then apply this learned representation to synthesize new characters in the target style by sampling from the appropriate distribution over these attributes. Experiments show our method can achieve high fidelity and coherency across multiple writing systems.",1
"With the tremendous success of deep learning in visual tasks, the representations extracted from intermediate layers of learned models, that is, deep features, attract much attention of researchers. Previous empirical analysis shows that those features can contain appropriate semantic information. Therefore, with a model trained on a large-scale benchmark data set (e.g., ImageNet), the extracted features can work well on other tasks. In this work, we investigate this phenomenon and demonstrate that deep features can be suboptimal due to the fact that they are learned by minimizing the empirical risk. When the data distribution of the target task is different from that of the benchmark data set, the performance of deep features can degrade. Hence, we propose a hierarchically robust optimization method to learn more generic features. Considering the example-level and concept-level robustness simultaneously, we formulate the problem as a distributionally robust optimization problem with Wasserstein ambiguity set constraints, and an efficient algorithm with the conventional training pipeline is proposed. Experiments on benchmark data sets demonstrate the effectiveness of the robust deep representations.",0
"Artificial intelligence (AI) has made great strides over recent years, becoming increasingly powerful and ubiquitous across numerous domains such as natural language processing, computer vision, robotics, game playing and expert systems. Despite these advancements however, there remain many critical challenges that hinder the further development and adoption of modern AI techniques - particularly those related to scalability and interpretability. In this work we introduce hierarchical robust representation learning (HRRL), which addresses both these issues by leveraging structured learning problems, adaptive inference and explicit representation modularity. We demonstrate HRRL on several real world tasks, including image classification, language modeling and speech recognition, showing consistent improvements in terms of accuracy, speed and transferability when compared against state-of-the art baselines. Furthermore, our approach naturally yields interpretable representations that can be used for downstream analysis and decision making, making it an attractive proposition for researchers, developers and users alike. Overall, this work represents a significant step towards building more reliable, trustworthy, efficient and intelligent artificial intelligence systems that are grounded in solid scientific principles.",1
"Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance.",0
"As the use of visual question answering (VQA) models continues to grow, understanding how these systems make predictions has become increasingly important. One popular approach to VQA architectures uses a combination of image features and natural language processing techniques. However, despite their successes, there remains limited insight into how such models achieve high levels of accuracy on specific subtasks within VQA. This study examines two common components used in current state-of-the-art VQA models: pre-trained models fine-tuned for different tasks and transformers that encode contextualized information from the given text prompts. We provide component analysis experiments that disentangle the contributions made by each component to overall performance on standard benchmark datasets. Our findings reveal new insights into which aspects of both components contribute most significantly to model performance across multiple test sets and task metrics. Overall, our research helps advance the field's understanding of the inner workings of VQA models, paving the way for future improvements and advancements. Keywords: Visual Question Answering, Component Analysis, Pre-Training Fine-Tuning, Transformers, Transfer Learning",1
"Deep learning methods capable of handling relational data have proliferated over the last years. In contrast to traditional relational learning methods that leverage first-order logic for representing such data, these deep learning methods aim at re-representing symbolic relational data in Euclidean spaces. They offer better scalability, but can only numerically approximate relational structures and are less flexible in terms of reasoning tasks supported. This paper introduces a novel framework for relational representation learning that combines the best of both worlds. This framework, inspired by the auto-encoding principle, uses first-order logic as a data representation language, and the mapping between the original and latent representation is done by means of logic programs instead of neural networks. We show how learning can be cast as a constraint optimisation problem for which existing solvers can be used. The use of logic as a representation language makes the proposed framework more accurate (as the representation is exact, rather than approximate), more flexible, and more interpretable than deep learning methods. We experimentally show that these latent representations are indeed beneficial in relational learning tasks.",0
"This paper introduces auto-encoding logic programs (AELP), which can learn relational representations that capture both logical and probabilistic relationships between entities in natural language data. \begin{enumerate} \item Rather than starting off by describing the abstract and then writing out bullet points with additional information on the topic. Start by breaking down the main key ideas you want to present. For example, ""This paper presents auto-encoding logic programs (AELP) as a method to learn relational representations."" \item Next, briefly explain the motivation behind using logic programming specifically as opposed to other types of modeling approaches: ""Logical reasoning allows AELP to effectively reason over complex problems while still maintaining expressivity for capturing uncertain relations."" \item Then introduce the contribution of the paper: ""To achieve effective learning, we propose to use Monte Carlo Tree Search (MCTS) planning within our AELP framework to enable efficient search through the space of possible logical formulas."" \item Finally, mention any relevant results or experiments conducted in the paper along with their implications: ""Our experimental evaluations demonstrate the effectiveness of AELP at learning rich relational representations compared to previous state-of-the art models. Additionally, we showcase several applications of AELP across domains such as commonsense reasoning and question answering."" \end{enumerate} Here is an updated version based on these recommendations: ""In recent years there has been growing interest in developing deep neural network architectures that explicitly encode first-order symbolic knowledge to perform complex reasoning tasks [2]. In contrast to traditional rule-based expert systems or shallow symbolic methods like decision trees, the ability of these deep models to directly operate over structured symbols enables them to capture more complex patterns and associations from large amounts of data[3]....",1
"A variety of graph neural networks (GNNs) frameworks for representation learning on graphs have been recently developed. These frameworks rely on aggregation and iteration scheme to learn the representation of nodes. However, information between nodes is inevitably lost in the scheme during learning. In order to reduce the loss, we extend the GNNs frameworks by exploring the aggregation and iteration scheme in the methodology of mutual information. We propose a new approach of enlarging the normal neighborhood in the aggregation of GNNs, which aims at maximizing mutual information. Based on a series of experiments conducted on several benchmark datasets, we show that the proposed approach improves the state-of-the-art performance for four types of graph tasks, including supervised and semi-supervised graph classification, graph link prediction and graph edge generation and classification.",0
"Deep learning models have seen tremendous success on numerous tasks ranging from computer vision to natural language processing. One reason for their success is their ability to learn complex representations through mutual co-dependence between variables, known as mutual information maximization (MIM). In graph neural networks (GNNs), MIM can be achieved by optimizing node features that capture both local and global dependencies within the network structure. This work proposes novel approaches to optimize MIM in GNNs for semi-supervised classification problems. We present two methods: one based on variational inference and another based on contrastive self-training using adversarial perturbations. Our experimental results demonstrate that these new formulations outperform state-of-the-art techniques on several benchmark datasets and graph types. Finally, we discuss future research directions towards understanding the properties of MIM optimization in GNNs and its general implications for graph representation learning.",1
"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",0
"In recent years, unsupervised visual representation learning has emerged as a promising technique for enabling computers to learn about images without relying on labeled data. One popular approach involves solving jigsaw puzzles, where pieces from different puzzles are mixed together and the computer must sort them back into their original sets. Another approach, called momentum contrast, builds upon these techniques by adding new levels of complexity and challenge for the computer to solve.  Momentum contrast adds time as an additional dimension to jigsaw puzzle training. Instead of seeing only one step at a time, the model sees many steps at once, including future ones that have not yet been generated. This allows the model to use the current state to predict what will happen in the future and improve its understanding of how images relate to each other over time. By learning to reason about actions over sequences of frames, the model can develop more meaningful representations of objects and scenes.  The results show that momentum contrast significantly outperforms traditional methods, achieving better accuracy on several benchmark datasets. These findings demonstrate the effectiveness of incorporating temporal reasoning and prediction into unsupervised image representation learning, opening up exciting possibilities for further advances in artificial intelligence. Overall, this work represents an important contribution towards developing algorithms that can learn and make predictions based on complex spatio-temporal patterns in visual input.",1
"Low-rank Multi-view Subspace Learning (LMvSL) has shown great potential in cross-view classification in recent years. Despite their empirical success, existing LMvSL based methods are incapable of well handling view discrepancy and discriminancy simultaneously, which thus leads to the performance degradation when there is a large discrepancy among multi-view data. To circumvent this drawback, motivated by the block-diagonal representation learning, we propose Structured Low-rank Matrix Recovery (SLMR), a unique method of effectively removing view discrepancy and improving discriminancy through the recovery of structured low-rank matrix. Furthermore, recent low-rank modeling provides a satisfactory solution to address data contaminated by predefined assumptions of noise distribution, such as Gaussian or Laplacian distribution. However, these models are not practical since complicated noise in practice may violate those assumptions and the distribution is generally unknown in advance. To alleviate such limitation, modal regression is elegantly incorporated into the framework of SLMR (term it MR-SLMR). Different from previous LMvSL based methods, our MR-SLMR can handle any zero-mode noise variable that contains a wide range of noise, such as Gaussian noise, random noise and outliers. The alternating direction method of multipliers (ADMM) framework and half-quadratic theory are used to efficiently optimize MR-SLMR. Experimental results on four public databases demonstrate the superiority of MR-SLMR and its robustness to complicated noise.",0
"In recent years, multi-view learning has emerged as a powerful tool for solving complex machine learning problems by leveraging multiple data sources that capture different aspects of the same phenomenon. One common approach to multi-view learning involves recovering a low-rank matrix from incomplete and potentially noisy observations obtained from each view. This task can be cast as a structured low-rank matrix recovery problem, which seeks to find the optimal matrix factorization under certain constraints imposed by the available data. In practice, however, the observed data may only provide partial information about the true underlying structure, leading to challenges in identifying the correct solution. To address these issues, we propose a novel framework called modality regressionbased structured low-rank matrix recovery (MRSLRMR) that integrates modal knowledge into the optimization process. By modeling the missing data with Gaussian processes, our approach enables efficient uncertainty quantification, allowing for more accurate estimation of unknowns while accounting for their potential variability. Our results on synthetic datasets demonstrate the superiority of MRSLRMR over competing methods in terms of accuracy and robustness, particularly when dealing with highly corrupted or insufficiently sampled data. We further apply our method to real-world image classification tasks involving two sets of views collected through distinct domains and achieve remarkable performance gains compared to state-of-theart algorithms. Overall, our work highlights the benefits of combining structured matrix decomposition techniques with probabilistic models tailored to specific modalities, opening new opportunities f",1
"Unsupervised representation learning via generative modeling is a staple to many computer vision applications in the absence of labeled data. Variational Autoencoders (VAEs) are powerful generative models that learn representations useful for data generation. However, due to inherent challenges in the training objective, VAEs fail to learn useful representations amenable for downstream tasks. Regularization-based methods that attempt to improve the representation learning aspect of VAEs come at a price: poor sample generation. In this paper, we explore this representation-generation trade-off for regularized VAEs and introduce a new family of priors, namely decoupled priors, or dpVAEs, that decouple the representation space from the generation space. This decoupling enables the use of VAE regularizers on the representation space without impacting the distribution used for sample generation, and thereby reaping the representation learning benefits of the regularizations without sacrificing the sample generation. dpVAE leverages invertible networks to learn a bijective mapping from an arbitrarily complex representation distribution to a simple, tractable, generative distribution. Decoupled priors can be adapted to the state-of-the-art VAE regularizers without additional hyperparameter tuning. We showcase the use of dpVAEs with different regularizers. Experiments on MNIST, SVHN, and CelebA demonstrate, quantitatively and qualitatively, that dpVAE fixes sample generation for regularized VAEs.",0
"This paper presents a new method for improving sample generation in regularized variational autoencoders (VAEs). The authors propose using deep pressure sampling, which combines techniques from diffusion models and energy-based models, as a replacement for traditional reparameterization tricks used in VAEs. By introducing a continuous distribution over latent codes rather than a discrete one, the authors argue that they can achieve better results while maintaining computational efficiency. They evaluate their proposed approach on several datasets and demonstrate improved performance compared to state-of-the-art methods. Additionally, the authors provide ablation studies to highlight the importance of each component in their model. Overall, this work represents a significant advance in the field of generative modelling.",1
"Despite continuing medical advances, the rate of newborn morbidity and mortality globally remains high, with over 6 million casualties every year. The prediction of pathologies affecting newborns based on their cry is thus of significant clinical interest, as it would facilitate the development of accessible, low-cost diagnostic tools\cut{ based on wearables and smartphones}. However, the inadequacy of clinically annotated datasets of infant cries limits progress on this task. This study explores a neural transfer learning approach to developing accurate and robust models for identifying infants that have suffered from perinatal asphyxia. In particular, we explore the hypothesis that representations learned from adult speech could inform and improve performance of models developed on infant speech. Our experiments show that models based on such representation transfer are resilient to different types and degrees of noise, as well as to signal loss in time and frequency domains.",0
"Title: Neural Transfer Learning for Cry-Based Diagnosis of Perinatal Asphyxia Abstract: Our study focuses on developing a novel method using machine learning techniques to diagnose perinatal asphyxia based on cry analysis. We present a framework utilizing pre-trained deep convolutional neural networks (CNNs) that can effectively learn from small amounts of data to achieve high accuracy in diagnosing perinatal asphyxia. Using a large dataset comprising normal cries and those obtained post-perinatal asphyxia events, we demonstrate how our approach outperforms existing state-of-the-art methods by achieving higher sensitivity and specificity values. Furthermore, we evaluate the robustness of the proposed method against variations in acoustic environment conditions, showing promising results even under challenging scenarios. Our findings provide significant clinical value, enabling efficient early detection and intervention in potential cases of neonatal seizures, hypoxic-ischemic encephalopathy and other asphyxia-related complications. In summary, our work paves the way towards reliable automated diagnostic tools in the field of perinatology, significantly improving patient outcomes while reducing healthcare costs.",1
"Robustness is an increasingly important property of machine learning models as they become more and more prevalent. We propose a defense against adversarial examples based on a k-nearest neighbor (kNN) on the intermediate activation of neural networks. Our scheme surpasses state-of-the-art defenses on MNIST and CIFAR-10 against l2-perturbation by a significant margin. With our models, the mean perturbation norm required to fool our MNIST model is 3.07 and 2.30 on CIFAR-10. Additionally, we propose a simple certifiable lower bound on the l2-norm of the adversarial perturbation using a more specific version of our scheme, a 1-NN on representations learned by a Lipschitz network. Our model provides a nontrivial average lower bound of the perturbation norm, comparable to other schemes on MNIST with similar clean accuracy.",0
"As machine learning systems continue to become increasingly prevalent in important applications such as medicine, finance, and self driving cars, it has become essential to ensure their robustness against malicious attacks that can cause them to make incorrect predictions. One particularly dangerous form of attack is known as adversarial examples, where an attacker carefully crafts inputs to fool the model into making mistakes. In this work we explore the potential of using k-nearest neighbor (KNN) methods for defending against adversarial examples. We begin by discussing the fundamental challenges involved in designing effective defense mechanisms before presenting our approach based on nearest neighbors classification. Our experimental results demonstrate the effectiveness of our method in defending against several types of adversarial attacks across multiple benchmark datasets. This work provides valuable insights and techniques for researchers and practitioners alike working towards securing machine learning models from real world threats.",1
"Person re-identification (reID) aims to match person images to retrieve the ones with the same identity. This is a challenging task, as the images to be matched are generally semantically misaligned due to the diversity of human poses and capture viewpoints, incompleteness of the visible bodies (due to occlusion), etc. In this paper, we propose a framework that drives the reID network to learn semantics-aligned feature representation through delicate supervision designs. Specifically, we build a Semantics Aligning Network (SAN) which consists of a base network as encoder (SA-Enc) for re-ID, and a decoder (SA-Dec) for reconstructing/regressing the densely semantics aligned full texture image. We jointly train the SAN under the supervisions of person re-identification and aligned texture generation. Moreover, at the decoder, besides the reconstruction loss, we add Triplet ReID constraints over the feature maps as the perceptual losses. The decoder is discarded in the inference and thus our scheme is computationally efficient. Ablation studies demonstrate the effectiveness of our design. We achieve the state-of-the-art performances on the benchmark datasets CUHK03, Market1501, MSMT17, and the partial person reID dataset Partial REID. Code for our proposed method is available at: https://github.com/microsoft/Semantics-Aligned-Representation-Learning-for-Person-Re-identification.",0
"This research presents a novel approach for person re-identification that leverages semantics alignment to learn robust feature representations. In traditional approaches, handcrafted features or pre-trained deep networks were used to extract visual descriptors from images of pedestrians across varying camera views, poses, lighting conditions, and occlusions. These methods often struggle due to their limited capacity to capture fine-grained differences among individuals and lack of semantic understanding. To overcome these challenges, we propose integrating semantic knowledge into the learning process by jointly optimizing two objectives: (i) supervised distance minimization in pairs and (ii) cross-entropy loss on part localizations, which explicitly models human parts learned from external resources such as OpenPose. This unified formulation enables both discriminative representation learning and precise pose estimation, leading to superior performance under partial occlusion scenarios compared to prior arts using only appearance matching. Our extensive experiments demonstrate state-of-the-art results against established benchmarks, validating our method's effectiveness towards real-world applications like tracking, surveillance, and autonomous driving.",1
"Person re-identification (re-ID) aims to recognize instances of the same person contained in multiple images taken across different cameras. Existing methods for re-ID tend to rely heavily on the assumption that both query and gallery images of the same person have the same clothing. Unfortunately, this assumption may not hold for datasets captured over long periods of time (e.g., weeks, months or years). To tackle the re-ID problem in the context of clothing changes, we propose a novel representation learning model which is able to generate a body shape feature representation without being affected by clothing color or patterns. We call our model the Color Agnostic Shape Extraction Network (CASE-Net). CASE-Net learns a representation of identity that depends only on body shape via adversarial learning and feature disentanglement. Due to the lack of large-scale re-ID datasets which contain clothing changes for the same person, we propose two synthetic datasets for evaluation. We create a rendered dataset SMPL-reID with different clothes patterns and a synthesized dataset Div-Market with different clothing color to simulate two types of clothing changes. The quantitative and qualitative results across 5 datasets (SMPL-reID, Div-Market, two benchmark re-ID datasets, a cross-modality re-ID dataset) confirm the robustness and superiority of our approach against several state-of-the-art approaches",0
"In recent years, person re-identification (ReID) has become an increasingly important task in computer vision research. One key challenge that remains in this field is representing clothing variations effectively in order to improve recognition accuracy. This work presents a novel method for learning shape representations of clothing variations using deep neural networks. We propose a two-stage approach: first, we generate a set of plausible shapes based on real images; then, we train a network to predict the probability distribution over these shapes given input images. Our experimental results demonstrate the effectiveness of our method, achieving state-of-the-art performance on several benchmark datasets. Additionally, we provide qualitative analysis showing how our model can disentangle the influence of clothing from other factors such as pose and illumination. Overall, our work represents a significant step towards improving the representation of clothing variations in person re-identification.",1
"We present a probabilistic forecasting framework based on convolutional neural network for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from JD.com, China's largest online retailer. The results show that our framework outperforms other state-of-the-art methods in both accuracy and efficiency.",0
"In today's fast-paced world, businesses and individuals rely on accurate forecasts to make informed decisions. Traditional statistical methods have proven effective but often lack the ability to capture complex patterns and nonlinear relationships within data. To address these limitations, advanced machine learning techniques such as deep neural networks (DNNs) have gained popularity among researchers and practitioners alike. However, DNNs tend to overfit the training dataset, which leads to poor generalization performance. One approach that has shown promise in mitigating overfitting is temporal convolutional neural networks (TCNs). TCNs can capture both short-term dependencies by using convolutions along the time axis and long-range dependencies by utilizing dilated convolutions.  This study proposes a novel methodology leveraging probabilistic forecasting with TCNs. Our proposed model captures uncertainty through Bayesian inference, allowing us to quantify confidence intervals around our predictions. We evaluate our approach across multiple datasets from diverse domains, including electricity load prediction and retail sales forecasting. Results demonstrate that our method outperforms state-of-the-art benchmark models while maintaining competitive accuracy. Furthermore, we showcase the interpretability of our framework, enabling users to gain insight into how different features impact their predictions. Overall, our work represents an important step towards achieving more reliable forecasts with increased transparency, ensuring better decision making for stakeholders.",1
"The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a unified probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology. Previous models are good at either of these, but not both. SPACE also resolves the scalability problems of previous methods by incorporating parallel spatial-attention and thus is applicable to scenes with a large number of objects without performance degradations. We show through experiments on Atari and 3D-Rooms that SPACE achieves the above properties consistently in comparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be found on our project website: https://sites.google.com/view/space-project-page",0
"Recent advances in deep learning have shown that unsupervised methods can generate high quality object representations from raw pixel data. However, most approaches focus on individual objects and fail at representing scenes as whole compositions of interacting components. In this work we introduce SPACE, a novel framework that learns object-oriented scene representations by jointly optimizing spatial attention and decomposition models. Our key insight is to decompose each image into sets of non-overlapping patches, which are then processed separately using shared multi-scale transformers to capture local patterns and relationships between objects. We apply both self attention modules and coarse-to-fine feature encodings, enabling our model to learn fine details while focusing on meaningful areas of interest. Furthermore, we propose novel spatial attention mechanisms based on convolutional neural networks, allowing us to attend to entire objects within a hierarchical context rather than just parts of them. Experiments show substantial improvements over existing state-of-the-art methods across multiple benchmarks, including VrPainter, Object3D, and more general segmentations on Cityscapes. This demonstrates the efficacy of combining powerful self-attention mechanisms with task-specific decompositions for effective object-oriented scene representation. By leveraging both global features and precise location estimates, we enable our framework to reason about complex visual interactions between objects in challenging real-world scenarios. Overall, SPACE presents a promising direction towards developing intelligent agents capable of understanding and generating diverse environments populated by dynamic entities.",1
"Video anomaly detection is of critical practical importance to a variety of real applications because it allows human attention to be focused on events that are likely to be of interest, in spite of an otherwise overwhelming volume of video. We show that applying self-trained deep ordinal regression to video anomaly detection overcomes two key limitations of existing methods, namely, 1) being highly dependent on manually labeled normal training data; and 2) sub-optimal feature learning. By formulating a surrogate two-class ordinal regression task we devise an end-to-end trainable video anomaly detection approach that enables joint representation learning and anomaly scoring without manually labeled normal/abnormal data. Experiments on eight real-world video scenes show that our proposed method outperforms state-of-the-art methods that require no labeled training data by a substantial margin, and enables easy and accurate localization of the identified anomalies. Furthermore, we demonstrate that our method offers effective human-in-the-loop anomaly detection which can be critical in applications where anomalies are rare and the false-negative cost is high.",0
"In this research study, we introduce deep ordinal regression (DOR) as a methodology for end-to-end video anomaly detection. By applying supervised pretraining using reconstruction loss on frame subunits from different video frames and further fine-tuning the model by minimizing mean squared error loss over normalized frame reconstructions and negative log likelihood, we were able to achieve state-of-the-art results across several benchmark datasets such as UCSD Peds2D, UAVDT, and ShanghaiTech Part B dataset among others. Furthermore, we demonstrate how our approach achieves superior performance compared to other published methods when evaluated against standard evaluation metrics including precision, recall, FPPI metric, mAP score, accuracy, area under ROC curve (AUC), and EER metrics. To validate the effectiveness of our proposed framework, extensive experiments were conducted to evaluate the performance achieved during real-time operation using NVIDIA GeForce GTX 1660 Ti GPU. Our findings show that our method can accurately detect unusual events in videos, making it well suited for use in security surveillance systems where timely anomaly detection is critical. We believe that the contribution of this work lies in demonstrating the feasibility and efficacy of applying pretraining and self-supervision techniques for high-quality feature representations which can subsequently be used for accurate end-to-end training of anomaly detection models. Additionally, we have provided insights into selecting suitable hyperparameters crucial to obtaining optimal performance while ensuring low computational requirements. Overall, this study has the potential to advance research in computer vision towards addressing challenges associated with real-world deployment of robust and efficient anomaly detection systems",1
"The vast majority of visual animals actively control their eyes, heads, and/or bodies to direct their gaze toward different parts of their environment. In contrast, recent applications of reinforcement learning in robotic manipulation employ cameras as passive sensors. These are carefully placed to view a scene from a fixed pose. Active perception allows animals to gather the most relevant information about the world and focus their computational resources where needed. It also enables them to view objects from different distances and viewpoints, providing a rich visual experience from which to learn abstract representations of the environment. Inspired by the primate visual-motor system, we present a framework that leverages the benefits of active perception to accomplish manipulation tasks. Our agent uses viewpoint changes to localize objects, to learn state representations in a self-supervised manner, and to perform goal-directed actions. We apply our model to a simulated grasping task with a 6-DoF action space. Compared to its passive, fixed-camera counterpart, the active model achieves 8% better performance in targeted grasping. Compared to vanilla deep Q-learning algorithms, our model is at least four times more sample-efficient, highlighting the benefits of both active perception and representation learning.",0
"This paper proposes a new approach to robotic manipulation that combines active perception and representation. Traditional methods for robotic manipulation rely on pre-defined representations of objects and environments, which can limit their ability to adapt to changes in the environment or handle unexpected situations. Our proposed method leverages recent advances in computer vision and machine learning to enable robots to actively perceive and represent their surroundings in real-time.  The core idea behind our approach is to use sensor data from cameras, LiDARs, or other sensors to build a dynamic, hierarchical model of the scene, including both static background structures and movable objects that can be manipulated by the robot. By updating this model over time as the robot interacts with the world, we create a flexible and accurate representation of the current state that can inform decision making for subsequent actions.  In addition to building this representation, our system also uses predictive models to anticipate how different manipulations of objects may affect the overall state of the scene. These predictions allow the robot to plan more effective actions, taking into account not only immediate goals but also longer term objectives and constraints. We demonstrate the effectiveness of our approach through extensive simulations and experiments using real robots engaged in complex tasks such as assembly, sorting, and exploration.  Overall, our work represents a significant step towards enabling robots to perform advanced manipulation tasks under uncertainty, with applications ranging from manufacturing to service industries to personal assistance robotics.",1
"Recently, a number of competitive methods have tackled unsupervised representation learning by maximising the mutual information between the representations produced from augmentations. The resulting representations are then invariant to stochastic augmentation strategies, and can be used for downstream tasks such as clustering or classification. Yet data augmentations preserve many properties of an image and so there is potential for a suboptimal choice of representation that relies on matching easy-to-find features in the data. We demonstrate that greedy or local methods of maximising mutual information (such as stochastic gradient optimisation) discover local optima of the mutual information criterion; the resulting representations are also less-ideally suited to complex downstream tasks. Earlier work has not specifically identified or addressed this issue. We introduce deep hierarchical object grouping (DHOG) that computes a number of distinct discrete representations of images in a hierarchical order, eventually generating representations that better optimise the mutual information objective. We also find that these representations align better with the downstream task of grouping into underlying object classes. We tested DHOG on unsupervised clustering, which is a natural downstream test as the target representation is a discrete labelling of the data. We achieved new state-of-the-art results on the three main benchmarks without any prefiltering or Sobel-edge detection that proved necessary for many previous methods to work. We obtain accuracy improvements of: 4.3% on CIFAR-10, 1.5% on CIFAR-100-20, and 7.2% on SVHN.",0
"Incorporate key terms such as deep learning (DL), convolutional neural networks(CNNs) , image segmentation, unsupervised pretraining, downstream tasks into the abstract. Explain the novel contribution made by your method? The paper proposes a new algorithm called DHOG (Deep Hierarchical Object Grouping) that utilizes deep learning techniques, specifically convolutional neural networks (CNNs), for image segmentation. Unlike traditional methods which rely on hand engineering features or extensive supervision during training, our approach leverages the power of large amounts of data through unsupervised pretraining. This allows us to learn rich representations of objects and their grouping structures at multiple scales without any explicit annotations for object detection. Our method outperforms state-of-the-art approaches on several challenging benchmark datasets, demonstrating its effectiveness for downstream tasks such as semantic scene understanding.",1
"High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{https://github.com/HRNet}}.",0
"This article presents a new approach to learning high-resolution representations for visual recognition tasks using deep convolutional neural networks (CNNs). Our method differs from traditional approaches by focusing on learning more expressive features in a multi-scale fashion that captures both global context and fine-grained details. We propose two key contributions: firstly, we introduce a novel network architecture called Deep Pyramid Network (DPN) which learns hierarchical representations at multiple scales via pyramidal processing; secondly, we present a powerful loss function called Sparse Displacement Energy Loss (SDEL), which improves discriminative representation power by encouraging spatial alignment among local patterns. By combining DPN with SDEL, our model significantly outperforms state-of-the-art methods across six benchmark datasets including ImageNet, Pascal VOC, and COCO object detection and segmentation. Furthermore, extensive experiments demonstrate the effectiveness of each contribution under different settings and ablation studies verify the importance of joint optimization for both feature extraction and classification subproblems. Overall, this work highlights the great potential of deep representation learning in solving complex image understanding problems.",1
"Recent advances in deep reinforcement learning require a large amount of training data and generally result in representations that are often over specialized to the target task. In this work, we present a methodology to study the underlying potential causes for this specialization. We use the recently proposed projection weighted Canonical Correlation Analysis (PWCCA) to measure the similarity of visual representations learned in the same environment by performing different tasks.   We then leverage our proposed methodology to examine the task dependence of visual representations learned on related but distinct embodied navigation tasks. Surprisingly, we find that slight differences in task have no measurable effect on the visual representation for both SqueezeNet and ResNet architectures. We then empirically demonstrate that visual representations learned on one task can be effectively transferred to a different task.",0
"""The purpose of this study was to analyze visual representations used during embodied navigation tasks. We conducted experiments in which participants completed virtual navigation tasks while their brain activity and eye movements were recorded. Our findings indicate that there is significant variability in the type of visual representation used by individuals during these tasks, and that different types of representations may have different implications for performance outcomes.""",1
"Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene. The code and models are available at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.",0
"This paper presents a new approach to human pose estimation using bottom-up representation learning based on higher order features and scale-awareness. Our method, called HigherHRNet, uses convolutional neural networks (CNNs) to learn representations that capture both local and global contextual information at multiple scales. We introduce a novel scale pyramid module that enables our network to selectively focus on different levels of detail, depending on the size and resolution of input images. By doing so, we can improve the accuracy and robustness of our model across a wide range of scenarios, including real-world environments where conditions may vary widely. In experiments, we demonstrate the effectiveness of our approach by comparing it against state-of-the art methods on several benchmark datasets. Overall, our results show that HigherHRNet outperforms these approaches in terms of both accuracy and speed, making it a promising tool for applications such as autonomous driving, robotics, and AR/VR.",1
"There has been appreciable progress in unsupervised network representation learning (UNRL) approaches over graphs recently with flexible random-walk approaches, new optimization objectives and deep architectures. However, there is no common ground for systematic comparison of embeddings to understand their behavior for different graphs and tasks. In this paper we theoretically group different approaches under a unifying framework and empirically investigate the effectiveness of different network representation methods. In particular, we argue that most of the UNRL approaches either explicitly or implicit model and exploit context information of a node. Consequently, we propose a framework that casts a variety of approaches -- random walk based, matrix factorization and deep learning based -- into a unified context-based optimization function. We systematically group the methods based on their similarities and differences. We study the differences among these methods in detail which we later use to explain their performance differences (on downstream tasks). We conduct a large-scale empirical study considering 9 popular and recent UNRL techniques and 11 real-world datasets with varying structural properties and two common tasks -- node classification and link prediction. We find that there is no single method that is a clear winner and that the choice of a suitable method is dictated by certain properties of the embedding methods, task and structural properties of the underlying graph. In addition we also report the common pitfalls in evaluation of UNRL methods and come up with suggestions for experimental design and interpretation of results.",0
This can often make your writing sound repetitive.,1
"We propose a new method for video object segmentation (VOS) that addresses object pattern learning from unlabeled videos, unlike most existing methods which rely heavily on extensive annotated data. We introduce a unified unsupervised/weakly supervised learning framework, called MuG, that comprehensively captures intrinsic properties of VOS at multiple granularities. Our approach can help advance understanding of visual patterns in VOS and significantly reduce annotation burden. With a carefully-designed architecture and strong representation learning ability, our learned model can be applied to diverse VOS settings, including object-level zero-shot VOS, instance-level zero-shot VOS, and one-shot VOS. Experiments demonstrate promising performance in these settings, as well as the potential of MuG in leveraging unlabeled data to further improve the segmentation accuracy.",0
"This study presents a method for learning video object segmentation without using any labeled data, relying instead on unannotated videos. The proposed approach uses pretrained models to extract features from the input videos which are then combined with motion cues obtained via optical flow estimates. These two sources of information are integrated into a variational autoencoder framework, where we simultaneously learn to separate objects from the background as well as predicting missing frames in the video stream. An adversarial loss function ensures that our model generates semantically meaningful boundaries. Our experiments demonstrate the effectiveness of our approach across several benchmark datasets and ablation studies highlighting the importance of different components of our architecture. We believe our work has important implications for reducing reliance on annotated data for computer vision tasks, as well as providing new tools for understanding visual media at scale.",1
"Our work focuses on tackling the challenging but natural visual recognition task of long-tailed data distribution (i.e., a few classes occupy most of the data, while most classes have rarely few samples). In the literature, class re-balancing strategies (e.g., re-weighting and re-sampling) are the prominent and effective methods proposed to alleviate the extreme imbalance for dealing with long-tailed problems. In this paper, we firstly discover that these re-balancing methods achieving satisfactory recognition accuracy owe to that they could significantly promote the classifier learning of deep networks. However, at the same time, they will unexpectedly damage the representative ability of the learned deep features to some extent. Therefore, we propose a unified Bilateral-Branch Network (BBN) to take care of both representation learning and classifier learning simultaneously, where each branch does perform its own duty separately. In particular, our BBN model is further equipped with a novel cumulative learning strategy, which is designed to first learn the universal patterns and then pay attention to the tail data gradually. Extensive experiments on four benchmark datasets, including the large-scale iNaturalist ones, justify that the proposed BBN can significantly outperform state-of-the-art methods. Furthermore, validation experiments can demonstrate both our preliminary discovery and effectiveness of tailored designs in BBN for long-tailed problems. Our method won the first place in the iNaturalist 2019 large scale species classification competition, and our code is open-source and available at https://github.com/Megvii-Nanjing/BBN.",0
"ABSTRACT: The long tail phenomenon refers to situations where there exists very little data on certain classes compared to others making deep learning models struggle with such classes due to insufficient training material. This research proposes using a new architecture called bilateral branch network (BBN) with cumulative learning capabilities. Experiments show that our proposed model significantly outperforms existing models in classifying images from datasets exhibiting the long tailed distribution property. Keywords: long tailed distribution; deep neural networks; convolutional neural networks; object detection -----  Okay! Here's an abstract around 200 words long for your paper ""BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition"":  ""Deep learning has revolutionized many fields, including computer vision, but one challenge remains - dealing with the long tail problem. In domains where some classes have far fewer examples than others, standard neural networks often fail to perform well. Our novel approach, Bilateral Branch Network (BBN), addresses this issue by incorporating cumulative learning and bilateral branches into a single framework. By integrating both within-class and across-class knowledge during fine-tuning, BBN excels at handling skewed distributions seen in real world applications. Results show significant improvements over traditional methods like Focal Loss, especially in low shot regimes where previous approaches struggled.""",1
"This paper introduces a novel perspective about error in machine learning and proposes inverse feature learning (IFL) as a representation learning approach that learns a set of high-level features based on the representation of error for classification or clustering purposes. The proposed perspective about error representation is fundamentally different from current learning methods, where in classification approaches they interpret the error as a function of the differences between the true labels and the predicted ones or in clustering approaches, in which the clustering objective functions such as compactness are used. Inverse feature learning method operates based on a deep clustering approach to obtain a qualitative form of the representation of error as features. The performance of the proposed IFL method is evaluated by applying the learned features along with the original features, or just using the learned features in different classification and clustering techniques for several data sets. The experimental results show that the proposed method leads to promising results in classification and especially in clustering. In classification, the proposed features along with the primary features improve the results of most of the classification methods on several popular data sets. In clustering, the performance of different clustering methods is considerably improved on different data sets. There are interesting results that show some few features of the representation of error capture highly informative aspects of primary features. We hope this paper helps to utilize the error representation learning in different feature learning domains.",0
"Title your workspace Deep InvFeature Learn Absctract File -> Save As -> PDF -> Change Format -> Print Settings -> PDF (Printed) -> Enter Name ""DeepInvFeatLearAbst""",1
"This paper proposes inverse feature learning as a novel supervised feature learning technique that learns a set of high-level features for classification based on an error representation approach. The key contribution of this method is to learn the representation of error as high-level features, while current representation learning methods interpret error by loss functions which are obtained as a function of differences between the true labels and the predicted ones. One advantage of such learning method is that the learned features for each class are independent of learned features for other classes; therefore, this method can learn simultaneously meaning that it can learn new classes without retraining. Error representation learning can also help with generalization and reduce the chance of over-fitting by adding a set of impactful features to the original data set which capture the relationships between each instance and different classes through an error generation and analysis process. This method can be particularly effective in data sets, where the instances of each class have diverse feature representations or the ones with imbalanced classes. The experimental results show that the proposed method results in significantly better performance compared to the state-of-the-art classification techniques for several popular data sets. We hope this paper can open a new path to utilize the proposed perspective of error representation learning in different feature learning domains.",0
"Abstract: This paper presents a novel methodology called Inverse Feature Learning (IFL) which combines representation learning and feature learning techniques to improve accuracy in image classification tasks. IFL leverages convolutional neural networks (CNNs) equipped with a new loss function that minimizes the reconstruction error of images. During training, the CNN learns both discriminative features and inverse mapping from the input space to the output label space. Experimental results show that our approach consistently outperforms state-of-the-art methods across multiple datasets, demonstrating the effectiveness of IFL for image classification problems.",1
"Deep neural networks are composed of layers of parametrised linear operations intertwined with non linear activations. In basic models, such as the multi-layer perceptron, a linear layer operates on a simple input vector embedding of the instance being processed, and produces an output vector embedding by straight multiplication by a matrix parameter. In more complex models, the input and output are structured and their embeddings are higher order tensors. The parameter of each linear operation must then be controlled so as not to explode with the complexity of the structures involved. This is essentially the role of convolution models, which exist in many flavours dependent on the type of structure they deal with (grids, networks, time series etc.). We present here a unified framework which aims at capturing the essence of these diverse models, allowing a systematic analysis of their properties and their mutual enrichment. We also show that attention models naturally fit in the same framework: attention is convolution in which the structure itself is adaptive, and learnt, instead of being given a priori.",0
"Incorporating structured knowledge into neural networks has been shown to improve their performance across several tasks. Recently, there have been advances in designing models that explicitly incorporate hierarchical relationships among entities into their representations. However, most existing approaches assume the availability of complete graphs representing all possible connections between entities, which can be impractical due to limited resources for constructing such graph structures. Moreover, even when these graphs exist, they may not always capture the full complexity of real-world relations, limiting the representational capacity of current methods. This work presents a new framework for learning embeddings that naturally integrate both convolutional filters and attention mechanisms with structural information. Our approach leverages partial hierarchies to capture salient relationships while remaining robust against incomplete or noisy data. We evaluate our model on two benchmark datasets, demonstrating significant improvements over strong baselines and showing promising applications towards building intelligent systems capable of harnessing structural dependencies effectively.",1
"Scalability in terms of object density in a scene is a primary challenge in unsupervised sequential object-oriented representation learning. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially-parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models. Additionally, we introduce a background module that allows SCALOR to model complex dynamic backgrounds as well as many foreground objects in the scene. We demonstrate that SCALOR can deal with crowded scenes containing up to a hundred objects while jointly modeling complex dynamic backgrounds. Importantly, SCALOR is the first unsupervised object representation model shown to work for natural scenes containing several tens of moving objects.",0
"Title: Unlocking the Power of Scale with Scalor  Generating realistic world models has been one of the most ambitious goals in computer graphics research. Recent advancements have demonstrated impressive results, but these approaches often struggle to achieve scalability without sacrificing quality. In this paper, we introduce a novel methodology called SCALOR that tackles this problem head-on by introducing a new concept – Scalable Object Representations (SORs).  Our approach is rooted in understanding that scene geometry can often be represented as combinations of simpler objects, such as cubes, cylinders, or even customized shapes. By recognizing underlying patterns in object representations, we enable efficient representation and scaling of complex scenes. Our proposed algorithm leverages machine learning techniques, allowing us to extract features from existing datasets and learn relationships among different object categories. This enables us to create highly detailed yet compact SORs, capable of representing complex scenes at scale.  We demonstrate the effectiveness of our approach through extensive experimentation on a variety of benchmark datasets and showcase how SCALOR outperforms state-of-the-art methods in terms of visual fidelity and computational efficiency. Furthermore, our framework allows for interactive generation of dynamic scenes and can seamlessly integrate into modern game engines, opening up exciting possibilities for VR/AR applications.  In conclusion, SCALOR provides a transformative step towards enabling large-scale generative modeling while preserving high levels of detail and precision. With its modular architecture and flexible design, SCALOR paves the way for future innovations in virtual environments and other related fields.",1
"Predictions and predictive knowledge have seen recent success in improving not only robot control but also other applications ranging from industrial process control to rehabilitation. A property that makes these predictive approaches well suited for robotics is that they can be learned online and incrementally through interaction with the environment. However, a remaining challenge for many prediction-learning approaches is an appropriate choice of prediction-learning parameters, especially parameters that control the magnitude of a learning machine's updates to its predictions (the learning rate or step size). To begin to address this challenge, we examine the use of online step-size adaptation using a sensor-rich robotic arm. Our method of choice, Temporal-Difference Incremental Delta-Bar-Delta (TIDBD), learns and adapts step sizes on a feature level; importantly, TIDBD allows step-size tuning and representation learning to occur at the same time. We show that TIDBD is a practical alternative for classic Temporal-Difference (TD) learning via an extensive parameter search. Both approaches perform comparably in terms of predicting future aspects of a robotic data stream. Furthermore, the use of a step-size adaptation method like TIDBD appears to allow a system to automatically detect and characterize common sensor failures in a robotic application. Together, these results promise to improve the ability of robotic devices to learn from interactions with their environments in a robust way, providing key capabilities for autonomous agents and robots.",0
"This research study focuses on the use of Temporal-Difference Incremental Delta-Bar-Delta (TDIDB) as a methodology for improving predictive knowledge architectures in real-world applications. TDIDB is a powerful algorithm that has been shown to yield significant improvements over traditional methods in certain domains. The goal of this work is to examine how TDIDB can be applied in practice to real-world problems and evaluate its effectiveness compared to other approaches. By examining several case studies and performing detailed comparative analyses, we aim to provide insights into the strengths and limitations of TDIDB and identify areas where further development may be necessary. Ultimately, this work seeks to contribute towards advancing our understanding of temporal difference learning algorithms and their potential impact on real-world systems.",1
"Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples.",0
This should give us a good sense of the focus and significance of your work without having to read any additional material from you so make sure all important details are included here:,1
"To take full advantage of fast-growing unlabeled networked data, this paper introduces a novel self-supervised strategy for graph representation learning by exploiting natural supervision provided by the data itself. Inspired by human social behavior, we assume that the global context of each node is composed of all nodes in the graph since two arbitrary entities in a connected network could interact with each other via paths of varying length. Based on this, we investigate whether the global context can be a source of free and effective supervisory signals for learning useful node representations. Specifically, we randomly select pairs of nodes in a graph and train a well-designed neural net to predict the contextual position of one node relative to the other. Our underlying hypothesis is that the representations learned from such within-graph context would capture the global topology of the graph and finely characterize the similarity and differentiation between nodes, which is conducive to various downstream learning tasks. Extensive benchmark experiments including node classification, clustering, and link prediction demonstrate that our approach outperforms many state-of-the-art unsupervised methods and sometimes even exceeds the performance of supervised counterparts.",0
"Incorporate keywords such as representation learning, graph signal processing, self-supervision, deep neural networks, nonlinear dimensionality reduction and graph embedding. Self-supervised representation learning has recently emerged as a promising approach for obtaining high quality vector embeddings from unstructured data, including images, text and speech signals. In comparison, graph data provides unique opportunities for utilizing global context during pretraining due to its inherent connectivity structure. To exploit these possibilities we propose a novel self-supervised framework that combines graph signal processing techniques, nonlinear dimensionality reduction methods, and deep neural network architectures. By maximizing predictive performance on a diverse set of downstream tasks over graphs of varying size and complexity, our model can learn informative representations suitable for applications ranging from social network analysis and bioinformatics to computer vision and natural language processing. This paper introduces a new method for training graph representation models using self-supervision. Unlike traditional supervised learning approaches which require large amounts of labeled data, self-supervised representation learning can generate valuable vector embeddings directly from raw input data without any prior labels. We demonstrate how these principles can be extended to graph signals by introducing a new model that simultaneously learns powerful deep neural network architectures together with low rank linear approximations of their output characteristics. This enables efficient nonlinear dimensionality reduction and representation optimization at a fraction of the computational cost compared to standard methods like matrix factorization or singular value decomposition. Through extensive empirical evaluation across multiple domains and datasets we showcase the outstanding capabilities of our proposed framework for improving downstream task generalization while consistently surpassing state-of-the-art benchmarks on several popular evaluation metrics. Our findings suggest tha",1
"Graph representation learning is a ubiquitous task in machine learning where the goal is to embed each vertex into a low-dimensional vector space. We consider the bipartite graph and formalize its representation learning problem as a statistical estimation problem of parameters in a semiparametric exponential family distribution. The bipartite graph is assumed to be generated by a semiparametric exponential family distribution, whose parametric component is given by the proximity of outputs of two one-layer neural networks, while nonparametric (nuisance) component is the base measure. Neural networks take high-dimensional features as inputs and output embedding vectors. In this setting, the representation learning problem is equivalent to recovering the weight matrices. The main challenges of estimation arise from the nonlinearity of activation functions and the nonparametric nuisance component of the distribution. To overcome these challenges, we propose a pseudo-likelihood objective based on the rank-order decomposition technique and focus on its local geometry. We show that the proposed objective is strongly convex in a neighborhood around the ground truth, so that a gradient descent-based method achieves linear convergence rate. Moreover, we prove that the sample complexity of the problem is linear in dimensions (up to logarithmic factors), which is consistent with parametric Gaussian models. However, our estimator is robust to any model misspecification within the exponential family, which is validated in extensive experiments.",0
"Inferring directed interactions among multiple nodes from observational data has important applications in various fields ranging from biology to social networks and finance. However, estimating these interactions often leads to difficult computational problems, particularly when there may be nonlinear relationships involved. One promising approach that addresses this challenge involves using bipartite graph representation learning (GRL). In GRL, one set of variables serves as the row nodes while another set serves as column nodes, enabling flexibility in modeling complex dependencies and allowing for scalability by handling different sets separately. Existing methods either rely on strong parametric assumptions that can limit performance in practice or else lack theoretical guarantees altogether. Our new method overcomes these shortcomings by employing semiparametric models within each node pair, capturing flexible nonlinear associations, while still providing provable guarantees for variable selection consistency and edge detection even in high dimensions where p >> n, thereby extending the state-of-the-art significantly. Extensive simulations illustrate markedly superior recovery results compared to alternative procedures across challenging scenarios. Real microbiome data validation underscores our methods capability for unraveling meaningful insights into directed network structures that were previously elusive. Overall, our contributions enhance researchers ability to explore hidden structure in complex systems accurately, opening up fresh possibilities for scientific discovery via more accurate, reliable analysis.",1
"We present a novel variational generative adversarial network (VGAN) based on Wasserstein loss to learn a latent representation from a face image that is invariant to identity but preserves head-pose information. This facilitates synthesis of a realistic face image with the same head pose as a given input image, but with a different identity. One application of this network is in privacy-sensitive scenarios; after identity replacement in an image, utility, such as head pose, can still be recovered. Extensive experimental validation on synthetic and real human-face image datasets performed under 3 threat scenarios confirms the ability of the proposed network to preserve head pose of the input image, mask the input identity, and synthesize a good-quality realistic face image of a desired identity. We also show that our network can be used to perform pose-preserving identity morphing and identity-preserving pose morphing. The proposed method improves over a recent state-of-the-art method in terms of quantitative metrics as well as synthesized image quality.",0
"This paper proposes a novel framework that applies variational autoencoders (VAEs) and adversarial networks (in particular, WGANs) to learn a pose-preserving representation of facial images suitable for identity replacement via image warping techniques. The proposed method has several advantages over existing approaches: it generates high quality results; it requires no specialized hardware, making it scalable to other research teams; and it achieves better performance on challenging conditions such as occlusions and blurry faces. Our experimental evaluation shows significant improvement compared to state-of-the-art methods both quantitatively and qualitatively across multiple datasets. Overall, our work demonstrates the feasibility and effectiveness of using deep learning models to achieve seamless identity replacement in realistic facial images while preserving the original facial expressions and poses.",1
"Recent advances in deep learning have achieved promising performance for medical image analysis, while in most cases ground-truth annotations from human experts are necessary to train the deep model. In practice, such annotations are expensive to collect and can be scarce for medical imaging applications. Therefore, there is significant interest in learning representations from unlabelled raw data. In this paper, we propose a self-supervised learning approach to learn meaningful and transferable representations from medical imaging video without any type of human annotation. We assume that in order to learn such a representation, the model should identify anatomical structures from the unlabelled data. Therefore we force the model to address anatomy-aware tasks with free supervision from the data itself. Specifically, the model is designed to correct the order of a reshuffled video clip and at the same time predict the geometric transformation applied to the video clip. Experiments on fetal ultrasound video show that the proposed approach can effectively learn meaningful and strong representations, which transfer well to downstream tasks like standard plane detection and saliency prediction.",0
"This paper presents a novel approach to self-supervised representation learning for ultrasound video data. We demonstrate how large-scale datasets can be efficiently utilized to learn representations that capture important diagnostic information without requiring manually annotated labels. By pretraining on unlabeled data, we show that deep networks can effectively encode prior knowledge into their hidden layers, which enables improved generalization to new, smaller labeled subsets of the same domain. The proposed method achieves state-of-the-art performance across multiple tasks including segmentation, detection, and classification, outperforming other methods relying solely on supervised training or external regularizations. Our approach is particularly advantageous given current challenges faced by radiologists due to increasing workloads resulting from aging populations and advances in technology generating larger amounts of imaging data. Ultimately, our findings highlight the value of utilizing large quantities of available medical imaging data for pretraining purposes, paving the path towards enabling more accurate diagnoses while reducing human expert dependency.",1
"This paper employs a formal connection of machine learning with thermodynamics to characterize the quality of learnt representations for transfer learning. We discuss how information-theoretic functional such as rate, distortion and classification loss of a model lie on a convex, so-called equilibrium surface.We prescribe dynamical processes to traverse this surface under constraints, e.g., an iso-classification process that trades off rate and distortion to keep the classification loss unchanged. We demonstrate how this process can be used for transferring representations from a source dataset to a target dataset while keeping the classification loss constant. Experimental validation of the theoretical results is provided on standard image-classification datasets.",0
"This paper develops a free energy principle that relates the thermodynamics of living systems to their representational properties. In this framework, we propose that the brain's generative processes are driven by minimizing surprise relative to some model belief state. We argue that this leads naturally to a variational Bayesian approach which we implement using deep neural networks trained on natural images. Our results suggest that representation learning can occur in a bottom-up manner from raw sensory inputs without requiring explicit guidance through supervision or reward signals. The key idea throughout this work is that representations arise because they reduce uncertainty or increase causal explanatory power rather than as an epiphenomenon of optimization for task performance.",1
"Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task approach, we propose a new architecture leveraging fixed pretrained semantic segmentation networks to guide self-supervised representation learning via pixel-adaptive convolutions. Furthermore, we propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Our method improves upon the state of the art for self-supervised monocular depth prediction over all pixels, fine-grained details, and per semantic categories.",0
"In recent years, self-supervised learning has emerged as a promising approach for computer vision tasks such as image classification and object detection. One challenge that remains is how to apply these techniques to monocular depth estimation, which involves predicting the depth map of a scene from a single RGB image. This problem is particularly challenging due to the lack of labeled data and the high variability in visual appearances across different scenes.  In this work, we present a novel method for semantically-guided representation learning for self-supervised monocular depth. Our approach leverages semantic labels, such as scene categories and objects, to guide the learning process and improve the quality of the predicted depth maps. We develop a multi-task framework that jointly learns to predict semantics and depth using contrastive learning. Specifically, we learn representations by minimizing the distance between positive pairs (images belonging to the same scene) and maximizing the distance between negative pairs (images belonging to different scenes). To further enhance the robustness of our model, we introduce a confidence loss that encourages the network to produce confident predictions only on easy examples while leaving hard cases unpredicted.  We evaluate our method on several benchmark datasets and demonstrate significant improvements over state-of-the-art methods in terms of accuracy and generalization. Additionally, we conduct extensive ablation studies to analyze the impact of each component in our framework. Finally, we showcase some practical applications of our method such as robotics and augmented reality. Overall, our work shows the effectiveness of incorporating semantics into self-supervised learning for monocular depth estimation, paving the way for future research in this area.",1
"Self-supervised representation learning targets to learn convnet-based image representations from unlabeled data. Inspired by the success of NLP methods in this area, in this work we propose a self-supervised approach based on spatially dense image descriptions that encode discrete visual concepts, here called visual words. To build such discrete representations, we quantize the feature maps of a first pre-trained self-supervised convnet, over a k-means based vocabulary. Then, as a self-supervised task, we train another convnet to predict the histogram of visual words of an image (i.e., its Bag-of-Words representation) given as input a perturbed version of that image. The proposed task forces the convnet to learn perturbation-invariant and context-aware image features, useful for downstream image understanding tasks. We extensively evaluate our method and demonstrate very strong empirical results, e.g., our pre-trained self-supervised representations transfer better on detection task and similarly on classification over classes ""unseen"" during pre-training, when compared to the supervised case.   This also shows that the process of image discretization into visual words can provide the basis for very powerful self-supervised approaches in the image domain, thus allowing further connections to be made to related methods from the NLP domain that have been extremely successful so far.",0
"This paper presents a novel approach to learning representations that uses predictive modeling to learn meaningful features from large image datasets. Instead of manually engineering feature extraction methods, we use a deep neural network to generate ""visual words"" - high level descriptions of image content that can be used as input to downstream tasks like classification or detection. We show that our method outperforms traditional approaches on several benchmarks and establishes new state-of-the-art results across a variety of domains including object recognition and scene understanding. Our work demonstrates the power of unsupervised learning techniques for visual representation discovery, opening up exciting possibilities for future research into how machines can automatically learn complex representations of data.",1
"This work exploits action equivariance for representation learning in reinforcement learning. Equivariance under actions states that transitions in the input space are mirrored by equivalent transitions in latent space, while the map and transition functions should also commute. We introduce a contrastive loss function that enforces action equivariance on the learned representations. We prove that when our loss is zero, we have a homomorphism of a deterministic Markov Decision Process (MDP). Learning equivariant maps leads to structured latent spaces, allowing us to build a model on which we plan through value iteration. We show experimentally that for deterministic MDPs, the optimal policy in the abstract MDP can be successfully lifted to the original MDP. Moreover, the approach easily adapts to changes in the goal states. Empirically, we show that in such MDPs, we obtain better representations in fewer epochs compared to representation learning approaches using reconstructions, while generalizing better to new goals than model-free approaches.",0
"Title: ""A Comprehensive Guide on Reliable Distance Measurements""  As a customer service representative for an engineering firm, I recently received a question from one of our clients asking how distance measurements can be reliable given all the limitations imposed by different circumstances such as sensor availability and communication requirements. Our client expressed concern that poor accuracy could result in accidents or costly errors within their factory’s supply chain management system. While there are no guarantees when relying solely on sensors like infrared or ultrasonic ones, our team has devised an algorithm capable of using multiple types of available sensor data to ensure the most accurate measurement possible. By considering all variables that might influence reliability—such as lighting conditions, temperature, and angle alignment—our solution reduces discrepancies and improves overall confidence in calculations performed via sensor technology. (End abstract.)",1
"For a robotic grasping task in which diverse unseen target objects exist in a cluttered environment, some deep learning-based methods have achieved state-of-the-art results using visual input directly. In contrast, actor-critic deep reinforcement learning (RL) methods typically perform very poorly when grasping diverse objects, especially when learning from raw images and sparse rewards. To make these RL techniques feasible for vision-based grasping tasks, we employ state representation learning (SRL), where we encode essential information first for subsequent use in RL. However, typical representation learning procedures are unsuitable for extracting pertinent information for learning the grasping skill, because the visual inputs for representation learning, where a robot attempts to grasp a target object in clutter, are extremely complex. We found that preprocessing based on the disentanglement of a raw input image is the key to effectively capturing a compact representation. This enables deep RL to learn robotic grasping skills from highly varied and diverse visual inputs. We demonstrate the effectiveness of this approach with varying levels of disentanglement in a realistic simulated environment.",0
This paper proposes an actor-critic deep reinforcement learning (RL) approach for visual grasping in cluttered scenes using state representation learning based on disentagled input images. Our method uses convolutional neural networks (CNNs) pretrained on raw image inputs to extract state features that capture object-specific representations for bothactor policy improvement and action value estimation. We evaluate ourmethodon multiple benchmark datasetsand show improvedperformancecomparedto baseline methodswithout relying on additional sensors or modifications to existing algorithms. Our results demonstrate that our method can effectively leverage rich object-level representations learned through self-supervised training and lead to better performance across different visual grasping tasks.,1
"Recognizing wild faces is extremely hard as they appear with all kinds of variations. Traditional methods either train with specifically annotated variation data from target domains, or by introducing unlabeled target variation data to adapt from the training data. Instead, we propose a universal representation learning framework that can deal with larger variation unseen in the given training data without leveraging target domain knowledge. We firstly synthesize training data alongside some semantically meaningful variations, such as low resolution, occlusion and head pose. However, directly feeding the augmented data for training will not converge well as the newly introduced samples are mostly hard examples. We propose to split the feature embedding into multiple sub-embeddings, and associate different confidence values for each sub-embedding to smooth the training procedure. The sub-embeddings are further decorrelated by regularizing variation classification loss and variation adversarial loss on different partitions of them. Experiments show that our method achieves top performance on general face recognition datasets such as LFW and MegaFace, while significantly better on extreme benchmarks such as TinyFace and IJB-S.",0
"Abstract: In recent years, deep learning has become increasingly popular for face recognition tasks due to its ability to learn high-level representations that capture complex patterns in data. However, most existing methods are limited by their inability to generalize well across different domains, leading to suboptimal performance on unseen datasets. To address this issue, we propose a novel framework for universal representation learning that leverages both generic features and task-specific knowledge to enable efficient adaptation to new environments. Our approach involves training a model on multiple source domains and fine-tuning it using a small amount of target domain data. Experimental results demonstrate the effectiveness of our method compared to state-of-the-art techniques in terms of accuracy and robustness under challenging conditions such as varying lighting, pose, and ethnicity. Additionally, we showcase the versatility of our framework in other computer vision tasks including object classification and segmentation.",1
"We seek to learn a representation on a large annotated data source that generalizes to a target domain using limited new supervision. Many prior approaches to this problem have focused on learning ""disentangled"" representations so that as individual factors vary in a new domain, only a portion of the representation need be updated. In this work, we seek the generalization power of disentangled representations, but relax the requirement of explicit latent disentanglement and instead encourage linearity of individual factors of variation by requiring them to be manipulable by learned linear transformations. We dub these transformations latent canonicalizers, as they aim to modify the value of a factor to a pre-determined (but arbitrary) canonical value (e.g., recoloring the image foreground to black). Assuming a source domain with access to meta-labels specifying the factors of variation within an image, we demonstrate experimentally that our method helps reduce the number of observations needed to generalize to a similar target domain when compared to a number of supervised baselines.",0
"This paper investigates the use of latent canonical transformations as a means for representation learning. We demonstrate that by modeling these transformations directly within a neural network architecture we can improve performance on a variety of tasks compared to standard linear layer architectures. Specifically, we show how our approach allows us to learn more robust representations which are less prone to overfitting even when training data is limited, achieving state of the art results across multiple benchmarks. Additionally, we provide a detailed analysis of the learned representations produced by our method showing how they capture both local and global features of the input domain, providing evidence that our method may facilitate better understanding of complex datasets through improved visualization techniques. Our results indicate that latent canonical transformations could serve as a fundamental building block for future machine learning algorithms.",1
"We present a new method to learn video representations from large-scale unlabeled video data. Ideally, this representation will be generic and transferable, directly usable for new tasks such as action recognition and zero or few-shot learning. We formulate unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are shared across different modalities via distillation. Further, we introduce the concept of loss function evolution by using an evolutionary search algorithm to automatically find optimal combination of loss functions capturing many (self-supervised) tasks and modalities. Thirdly, we propose an unsupervised representation evaluation metric using distribution matching to a large unlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised constraint, which is not guided by any labeling, produces similar results to weakly-supervised, task-specific ones. The proposed unsupervised representation learning results in a single RGB network and outperforms previous methods. Notably, it is also more effective than several label-based methods (e.g., ImageNet), with the exception of large, fully labeled video datasets.",0
"Title: ""Evolving Loss Functions for Deep Generative Models""",1
"One popular trend in meta-learning is to learn from many training tasks a common initialization for a gradient-based method that can be used to solve a new task with few samples. The theory of meta-learning is still in its early stages, with several recent learning-theoretic analyses of methods such as Reptile [Nichol et al., 2018] being for convex models. This work shows that convex-case analysis might be insufficient to understand the success of meta-learning, and that even for non-convex models it is important to look inside the optimization black-box, specifically at properties of the optimization trajectory. We construct a simple meta-learning instance that captures the problem of one-dimensional subspace learning. For the convex formulation of linear regression on this instance, we show that the new task sample complexity of any initialization-based meta-learning algorithm is $\Omega(d)$, where $d$ is the input dimension. In contrast, for the non-convex formulation of a two layer linear network on the same instance, we show that both Reptile and multi-task representation learning can have new task sample complexity of $\mathcal{O}(1)$, demonstrating a separation from convex meta-learning. Crucially, analyses of the training dynamics of these methods reveal that they can meta-learn the correct subspace onto which the data should be projected.",0
"In this work we provide new results on nonconvex meta learning through novel complexity separations from convex meta learning. While the theory underlying convex optimization has been quite successful at predicting generalization performance and model expressivity under simple distribution shift assumptions (e.g., linear models), it can sometimes fail to capture important aspects of meta-learning problems such as the quality of gradient estimates produced by the inner loop optimizer. By leveraging recent advances in machine learning algorithms that have achieved state-of-the-art performance across many benchmarks using non-convex optimization techniques, we aim to address these limitations and better align our theoretical predictions with practice. Our main contributions are: i) We prove separation between sample complexities of generalization error and the inner loop objective, showing how improved gradient estimation via non-convex optimization leads to more efficient training compared to existing frameworks; ii) For the first time, we establish upper bounds on non-convex meta learning which hold even under distributions with unknown moments beyond mean/variance without prior knowledge of the problem; iii) Finally, we apply meta learning techniques like few shot classification on both CIFAR-FS and FCML datasets, achieving SOTA results in most cases. These results demonstrate the effectiveness of non-convex meta learning methods, and their promise towards realizing the full potential of large scale data in the future.",1
"We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon's information theory that takes into account the modeling power and computational constraints of the observer. The resulting \emph{predictive $\mathcal{V}$-information} encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon's mutual information and in violation of the data processing inequality, $\mathcal{V}$-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, $\mathcal{V}$-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive $\mathcal{V}$-information is more effective than mutual information for structure learning and fair representation learning.",0
"This paper presents a theory of usable information under computational constraints. It explores how limitations on computation affect our ability to extract meaningful insights from data and make decisions based on that information. We develop mathematical models to represent different types of constraints, including time limitations, resource availability, and accuracy requirements. Our results show that these constraints can have significant impacts on the quality and quantity of usable information that can be obtained. The paper offers recommendations for researchers and practitioners who must operate under such constraints, as well as suggestions for future areas of study in this important field.",1
"Graph representation learning embeds nodes in large graphs as low-dimensional vectors and is of great benefit to many downstream applications. Most embedding frameworks, however, are inherently transductive and unable to generalize to unseen nodes or learn representations across different graphs. Although inductive approaches can generalize to unseen nodes, they neglect different contexts of nodes and cannot learn node embeddings dually. In this paper, we present a context-aware unsupervised dual encoding framework, \textbf{CADE}, to generate representations of nodes by combining real-time neighborhoods with neighbor-attentioned representation, and preserving extra memory of known nodes. We exhibit that our approach is effective by comparing to state-of-the-art methods.",0
"This paper presents a novel approach to representation learning called dual graph representation learning (DGRL). In recent years, graph representation learning has emerged as a popular technique for data analysis due to its ability to capture complex relationships among entities within a given dataset. However, traditional methods have limitations when dealing with real-world datasets that contain noise, missing values, and heterogeneous attributes. To address these issues, we propose a new framework based on two graphs: a vertex graph and an edge graph. We use deep neural networks to learn representations from both graphs simultaneously, resulting in more accurate and robust embeddings. Our experiments demonstrate significant improvements over state-of-the-art methods across a wide range of tasks and benchmarks, including node classification, link prediction, and clustering. Additionally, our method exhibits superior scalability compared to existing techniques, making it suitable for large-scale applications. Overall, our work represents a notable contribution towards advancing the field of representation learning using graphs.",1
"Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). However, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of ""starting small"", we present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions. The model starts with learning the most abstract representation, and then progressively grow the network architecture to introduce new representations at different levels of abstraction. We quantitatively demonstrate the ability of the presented model to improve disentanglement in comparison to existing works on two benchmark data sets using three disentanglement metrics, including a new metric we proposed to complement the previously-presented metric of mutual information gap. We further present both qualitative and quantitative evidence on how the progression of learning improves disentangling of hierarchical representations. By drawing on the respective advantage of hierarchical representation learning and progressive learning, this is to our knowledge the first attempt to improve disentanglement by progressively growing the capacity of VAE to learn hierarchical representations.",0
"Here is my attempt at writing an abstract:  In this work, we propose a novel approach to learning hierarchical representations using progressively growing neural networks. Our method introduces two main components that allow for improved representation disentangling as well as more efficient use of computational resources. Firstly, our model gradually grows the depth of its latent space through multiple training stages while maintaining previously learned knowledge. This allows for the learning of increasingly high-level concepts and their combination in order to form complex representations. Secondly, we introduce an attention mechanism that selectively focuses on relevant features within each stage, ensuring better disentanglement of semantically meaningful factors of variation from irrelevant ones. We demonstrate the effectiveness of these contributions by evaluating our method on several benchmark datasets in the fields of image generation and transfer learning. The results show that our method achieves state-of-the-art performance while efficiently utilizing computational resources compared to previous methods. Overall, this research advances our understanding of how deep generative models can learn rich and disentangled representations, paving the way for future applications across diverse domains.",1
"A common strategy in modern learning systems is to learn a representation that is useful for many tasks, a.k.a. representation learning. We study this strategy in the imitation learning setting for Markov decision processes (MDPs) where multiple experts' trajectories are available. We formulate representation learning as a bi-level optimization problem where the ""outer"" optimization tries to learn the joint representation and the ""inner"" optimization encodes the imitation learning setup and tries to learn task-specific parameters. We instantiate this framework for the imitation learning settings of behavior cloning and observation-alone. Theoretically, we show using our framework that representation learning can provide sample complexity benefits for imitation learning in both settings. We also provide proof-of-concept experiments to verify our theory.",0
"This paper proposes a new approach to representation learning for imitation learning called Probabilistic Graphical Models (PGM). PGMs allow agents to learn representations that can accurately capture important aspects of their environment by learning probabilistically over multiple levels of abstraction. These levels of abstraction are learned through bi-level optimization, where the agent optimizes both the low-level actions taken in the environment as well as the high-level parameters controlling the representation. By using this method, we show that agents can improve performance on several challenging benchmark tasks while also gaining robustness to changes in the environment. Our work demonstrates the potential utility of probabilistic graphical models for reinforcement learning, opening up exciting opportunities for future research.",1
"Learning compact discrete representations of data is a key task on its own or for facilitating subsequent processing of data. In this paper we present a model that produces Discrete InfoMax Codes (DIMCO); we learn a probabilistic encoder that yields k-way d-dimensional codes associated with input data. Our model's learning objective is to maximize the mutual information between codes and labels with a regularization, which enforces entries of a codeword to be as independent as possible. We show that the infomax principle also justifies previous loss functions (e.g., cross-entropy) as its special cases. Our analysis also shows that using shorter codes, as DIMCO does, reduces overfitting in the context of few-shot classification. Through experiments in various domains, we observe this implicit meta-regularization effect of DIMCO. Furthermore, we show that the codes learned by DIMCO are efficient in terms of both memory and retrieval time compared to previous methods.",0
"Abstract: Many modern deep learning architectures have achieved state-of-the-art performance by relying on unsupervised pretraining followed by fine tuning on task specific data. However, recent work has shown that alternative methods such as self supervision can also achieve comparable results while requiring less labeled data. In this paper, we propose a new method called Discrete InfoMax Codes (DIC) which combines both discrete latent variables and information bottleneck principles into a single framework for self supervised representation learning. We demonstrate through extensive experiments that DIC outperforms strong baseline methods across several benchmark datasets, including image classification, object detection, and semantic segmentation tasks. Our approach achieves consistent improvements even at low levels of supervision, making it more robust to noisy labels. Additionally, our method is easily integrated into existing pipelines without modifying model architecture, providing simple yet effective solution for domain adaptation problems. Overall, our contributions provide insights into designing efficient, scalable approaches for unlocking hidden structure from raw input signals.",1
"In this paper, we consider the source of Deep Reinforcement Learning (DRL)'s sample complexity, asking how much derives from the requirement of learning useful representations of environment states and how much is due to the sample complexity of learning a policy. While for DRL agents, the distinction between representation and policy may not be clear, we seek new insight through a set of transfer learning experiments. In each experiment, we retain some fraction of layers trained on either the same game or a related game, comparing the benefits of transfer learning to learning a policy from scratch. Interestingly, we find that benefits due to transfer are highly variable in general and non-symmetric across pairs of tasks. Our experiments suggest that perhaps transfer from simpler environments can boost performance on more complex downstream tasks and that the requirements of learning a useful representation can range from negligible to the majority of the sample complexity, based on the environment. Furthermore, we find that fine-tuning generally outperforms training with the transferred layers frozen, confirming an insight first noted in the classification setting.",0
"This paper investigates how transferable representations learned by deep reinforcement learning agents are across different environments. We study whether these representations can facilitate fast adaptation to new tasks without requiring extensive retraining, which could potentially lead to more efficient use of computational resources and time. Our results demonstrate that such transferability indeed exists, as agents trained on diverse sets of environments were able to rapidly generalize their knowledge and effectively solve novel problems. These findings have important implications for improving the efficiency and scalability of reinforcement learning algorithms, particularly for applications where computational costs must be minimized while maximizing performance. Further research is needed to better understand the underlying mechanisms behind representation transfer and to identify ways to optimize it.",1
"Deep learning (DL) models have achieved paradigm-changing performance in many fields with high dimensional data, such as images, audio, and text. However, the black-box nature of deep neural networks is a barrier not just to adoption in applications such as medical diagnosis, where interpretability is essential, but also impedes diagnosis of under performing models. The task of diagnosing or explaining DL models requires the computation of additional artifacts, such as activation values and gradients. These artifacts are large in volume, and their computation, storage, and querying raise significant data management challenges.   In this paper, we articulate DL diagnosis as a data management problem, and we propose a general, yet representative, set of queries to evaluate systems that strive to support this new workload. We further develop a novel data sampling technique that produce approximate but accurate results for these model debugging queries. Our sampling technique utilizes the lower dimension representation learned by the DL model and focuses on model decision boundaries for the data in this lower dimensional space. We evaluate our techniques on one standard computer vision and one scientific data set and demonstrate that our sampling technique outperforms a variety of state-of-the-art alternatives in terms of query accuracy.",0
"This technical report presents a framework for diagnosing deep learning models using sampling techniques. We discuss several approaches for generating samples from complex datasets that can then be used to evaluate and analyze deep learning models. Our methods allow researchers to efficiently sample data points that capture important features of the dataset, while reducing computation time and storage requirements. Additionally, we demonstrate how these sampling strategies can be combined with visualization tools to provide insights into model behavior and performance. Overall, our approach provides a powerful toolkit for deep learning practitioners looking to improve their understanding of model behavior through targeted sampling.",1
"Representation learning promises to unlock deep learning for the long tail of vision tasks without expensive labelled datasets. Yet, the absence of a unified evaluation for general visual representations hinders progress. Popular protocols are often too constrained (linear classification), limited in diversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to representation quality (ELBO, reconstruction error). We present the Visual Task Adaptation Benchmark (VTAB), which defines good representations as those that adapt to diverse, unseen tasks with few examples. With VTAB, we conduct a large-scale study of many popular publicly-available representation learning algorithms. We carefully control confounders such as architecture and tuning budget. We address questions like: How effective are ImageNet representations beyond standard natural datasets? How do representations trained via generative and discriminative models compare? To what extent can self-supervision replace labels? And, how close are we to general visual representations?",0
"This is a great opportunity to create a comprehensive understanding of representation learning through studying visual task adaptation benchmarks on large scales! This research aims to delve deeper into how artificial intelligence systems can learn by using the power of large datasets, focusing specifically on how these algorithms learn and adapt to new tasks. Using state-of-the-art methods such as data augmentation and transfer learning, we aim to gain insights into which factors most greatly impact performance on this crucial aspect of machine learning, where computers can master skills previously thought only possible by humans. Our findings demonstrate that while larger models tend to outperform smaller ones across many different benchmarks, they come at the cost of longer training times and higher computational resources requirements; however, these tradeoffs may prove worthwhile given their superior performance overall. In conclusion, our study provides valuable guidance for future work in representation learning, and sheds light on the exciting potential of AI to mimic human cognition more closely than ever before. Ultimately, these discoveries open up endless possibilities for exploring how machines might one day truly think like us.",1
"We introduce a parameterization method called Neural Bayes which allows computing statistical quantities that are in general difficult to compute and opens avenues for formulating new objectives for unsupervised representation learning. Specifically, given an observed random variable $\mathbf{x}$ and a latent discrete variable $z$, we can express $p(\mathbf{x}|z)$, $p(z|\mathbf{x})$ and $p(z)$ in closed form in terms of a sufficiently expressive function (Eg. neural network) using our parameterization without restricting the class of these distributions. To demonstrate its usefulness, we develop two independent use cases for this parameterization:   1. Mutual Information Maximization (MIM): MIM has become a popular means for self-supervised representation learning. Neural Bayes allows us to compute mutual information between observed random variables $\mathbf{x}$ and latent discrete random variables $z$ in closed form. We use this for learning image representations and show its usefulness on downstream classification tasks.   2. Disjoint Manifold Labeling: Neural Bayes allows us to formulate an objective which can optimally label samples from disjoint manifolds present in the support of a continuous distribution. This can be seen as a specific form of clustering where each disjoint manifold in the support is a separate cluster. We design clustering tasks that obey this formulation and empirically show that the model optimally labels the disjoint manifolds. Our code is available at \url{https://github.com/salesforce/NeuralBayes}",0
"In this paper, we propose a novel parameterization method for unsupervised representation learning called Neural Bayes (NB). NB utilizes Bayesian inference principles to learn representations by modeling data distributions and optimizing latent variables through variational inference. This approach allows us to develop flexible neural models that can capture complex relationships between inputs and outputs without relying on explicit supervision. Our experiments demonstrate the effectiveness of NB compared to existing methods across multiple tasks, including image classification, text generation, and sentiment analysis. Overall, our work represents a step towards developing more efficient and effective methods for unsupervised learning that can enable a wider range of applications across different domains.",1
"Representation learning has recently been successfully used to create vector representations of entities in language learning, recommender systems and in similarity learning. Graph embeddings exploit the locality structure of a graph and generate embeddings for nodes which could be words in a language, products of a retail website; and the nodes are connected based on a context window. In this paper, we consider graph embeddings with an error-free associative learning update rule, which models the embedding vector of node as a non-convex Gaussian mixture of the embeddings of the nodes in its immediate vicinity with some constant variance that is reduced as iterations progress. It is very easy to parallelize our algorithm without any form of shared memory, which makes it possible to use it on very large graphs with a much higher dimensionality of the embeddings. We study the efficacy of proposed method on several benchmark data sets and favorably compare with state of the art methods. Further, proposed method is applied to generate relevant recommendations for a large retailer.",0
"Abstract: This paper presents a novel approach for graph embedding called Hebbian Graph Embeddings (HGEs). HGEs use a combination of backpropagation through structure learning algorithms such as unsupervised graph partitioning methods like Kernighan & Lesk algorithm, and gradient descent to embed nodes into vector spaces. Unlike traditional graph embedding methods which try to minimize reconstruction errors in reconstructing graphs, HGEs aim at optimizing a cost function which measures similarity between predicted link probabilities obtained from the embedding vectors of two nodes, and their actual presence in the data set. Our experiments show that HGEs outperform other state-of-the art graph embedding approaches on benchmark datasets including node classification problems where HGEs achieve higher accuracy compared to baseline models. HGEs have applications in diverse fields involving complex networks, including but not limited to recommender systems, drug discovery, fraud detection etc. HGEs can also find applications wherever there exists a pairwise relationship between items needing comparison using complex network analysis techniques. For example they could be used to rank movies in terms of compatibility between them based on user viewership preferences.",1
"We introduce a sparse scattering deep convolutional neural network, which provides a simple model to analyze properties of deep representation learning for classification. Learning a single dictionary matrix with a classifier yields a higher classification accuracy than AlexNet over the ImageNet 2012 dataset. The network first applies a scattering transform that linearizes variabilities due to geometric transformations such as translations and small deformations. A sparse $\ell^1$ dictionary coding reduces intra-class variability while preserving class separation through projections over unions of linear spaces. It is implemented in a deep convolutional network with a homotopy algorithm having an exponential convergence. A convergence proof is given in a general framework that includes ALISTA. Classification results are analyzed on ImageNet.",0
"Deep neural networks (DNNs) have shown great success in many applications such as computer vision, natural language processing, and speech recognition. However, training DNNs can be computationally expensive and time consuming, especially for large datasets. In addition, overfitting is still a major challenge that needs to be addressed. To overcome these issues, new techniques need to be developed to make training DNNs more efficient while improving their generalization performance.  In this work, we propose a novel approach for deep network classification using scattering and homotopy dictionary learning (SCDL). Our method takes advantage of the local features extracted from scattering operations to represent data points as sparse codes in a high dimensional space. These codes serve as a bridge connecting the input data and the weights of convolutional layers in DNNs. By leveraging dictionary learning techniques, our method learns discriminative dictionaries that capture important features relevant to specific tasks, which leads to better model interpretability and accuracy. Moreover, the use of homotopy methods allows us to optimize parameters in a continuous manner, avoiding the use of discrete iterations that may lead to suboptimal solutions. Finally, extensive experiments conducted on several benchmark datasets demonstrate the superiority of SCDL compared to state-of-the-art baselines, validating its effectiveness and efficiency. Overall, this work provides valuable insights into developing effective training strategies for DNNs that balance efficiency, interpretability, and accuracy.",1
"Recent advancements in graph representation learning have led to the emergence of condensed encodings that capture the main properties of a graph. However, even though these abstract representations are powerful for downstream tasks, they are not equally suitable for visualisation purposes. In this work, we merge Mapper, an algorithm from the field of Topological Data Analysis (TDA), with the expressive power of Graph Neural Networks (GNNs) to produce hierarchical, topologically-grounded visualisations of graphs. These visualisations do not only help discern the structure of complex graphs but also provide a means of understanding the models applied to them for solving various tasks. We further demonstrate the suitability of Mapper as a topological framework for graph pooling by mathematically proving an equivalence with Min-Cut and Diff Pool. Building upon this framework, we introduce a novel pooling algorithm based on PageRank, which obtains competitive results with state of the art methods on graph classification benchmarks.",0
"In Deep Graph Mapper we propose a deep neural network model that maps graphs into their corresponding node representations in latent space. Our method treats each graph as a set of interdependent data points, where the graph structure (node features, edge weights) can both depend on the same underlying process. Each node representation captures dependencies across the entire graph by preserving local connectivity patterns in addition to global graph properties such as clustering coefficient and transitivity. By leveraging random walks on graphs along with convolutional layers, our model captures multiple scales of context without requiring explicit consideration of arbitrary scale parameters. This enables us to uncover intricate structural relationships among nodes even in large and complex networks. We evaluate our approach on four benchmark datasets and show that it leads to significant improvements over previous methods in terms of predictive accuracy and visual quality of generated embeddings. Additionally, we demonstrate applications of our learned node mappings for tasks such as anomaly detection and link prediction.",1
"Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard crossentropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Our method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. Code and models are available.",0
"Here is one possible abstract:  Recent advances in deep learning have allowed for the development of algorithms that can learn both from large amounts of data and their own feedback signals in order to improve their performance on specific tasks. However, these methods still require a significant amount of computational resources and often suffer from poor interpretability due to their lack of transparency. In contrast, model-based approaches rely heavily on domain knowledge and prior assumptions, but may not always capture the complexity of real-world phenomena accurately enough to produce satisfactory results. This work proposes a novel framework that combines self-supervised representation learning with clustering techniques to enable efficient optimization of complex models while preserving their interpretability. We demonstrate through several experiments that our approach outperforms state-of-the-art baselines in terms of accuracy as well as speed, making it particularly suited for applications where scalability and explainability are essential considerations. By bridging the gap between supervised deep learning and traditional unsupervised modeling techniques, we hope to open up new possibilities for researchers across multiple domains who seek to develop powerful yet interpretable machine learning systems.",1
"We investigate the effect of the dimensionality of the representations learned in Deep Neural Networks (DNNs) on their robustness to input perturbations, both adversarial and random. To achieve low dimensionality of learned representations, we propose an easy-to-use, end-to-end trainable, low-rank regularizer (LR) that can be applied to any intermediate layer representation of a DNN. This regularizer forces the feature representations to (mostly) lie in a low-dimensional linear subspace. We perform a wide range of experiments that demonstrate that the LR indeed induces low rank on the representations, while providing modest improvements to accuracy as an added benefit. Furthermore, the learned features make the trained model significantly more robust to input perturbations such as Gaussian and adversarial noise (even without adversarial training). Lastly, the low-dimensionality means that the learned features are highly compressible; thus discriminative features of the data can be stored using very little memory. Our experiments indicate that models trained using the LR learn robust classifiers by discovering subspaces that avoid non-robust features. Algorithmically, the LR is scalable, generic, and straightforward to implement into existing deep learning frameworks.",0
"This paper presents a novel approach to improving robustness in machine learning models using deep low-rank representations. We demonstrate that by enforcing low rank constraints on neural networks, we can reduce their sensitivity to small perturbations in input data while maintaining high accuracy. Our method combines techniques from matrix factorization and network regularization, resulting in significant improvements over state-of-the-art methods. Experimental results on several benchmark datasets show that our approach achieves better generalization performance under noise and adversarial attacks. Overall, this work addresses the critical need for more robust algorithms in artificial intelligence, enabling them to operate reliably in real-world environments.",1
"Designing a single neural network architecture that performs competitively across a range of molecule property prediction tasks remains largely an open challenge, and its solution may unlock a widespread use of deep learning in the drug discovery industry. To move towards this goal, we propose Molecule Attention Transformer (MAT). Our key innovation is to augment the attention mechanism in Transformer using inter-atomic distances and the molecular graph structure. Experiments show that MAT performs competitively on a diverse set of molecular prediction tasks. Most importantly, with a simple self-supervised pretraining, MAT requires tuning of only a few hyperparameter values to achieve state-of-the-art performance on downstream tasks. Finally, we show that attention weights learned by MAT are interpretable from the chemical point of view.",0
"In recent years, there has been significant interest in applying deep learning techniques to molecular simulations. One such technique that has gained popularity is the attention mechanism, which allows the model to selectively focus on different parts of the input data during training. However, traditional attention mechanisms have limitations when applied to molecular systems due to their fixed size and limited scope. To overcome these shortcomings, we propose the use of a Molecule Attention Transformer (MAT) architecture.  The core idea behind MAT is to combine local molecular features with non-local dependencies using self-attention layers. This allows the model to capture both intra-molecular interactions as well as inter-molecular effects, providing a more complete representation of the system. We demonstrate the effectiveness of our approach by comparing it against state-of-the-art models on several benchmark datasets, including energy predictions for small organic molecules and conformational sampling for peptides. Results show that MAT achieves competitive performance while requiring significantly fewer parameters and computation time compared to other methods. Additionally, we provide an analysis of how the model captures attention in various scenarios, highlighting the importance of both local and global representations. Overall, our work represents an important step towards developing efficient machine learning algorithms for studying complex molecular phenomena.",1
"The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g., by loss re-weighting, data re-sampling, or transfer learning from head- to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with representations learned with the simplest instance-balanced (natural) sampling, it is also possible to achieve strong long-tailed recognition ability by adjusting only the classifier. We conduct extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist, showing that it is possible to outperform carefully designed losses, sampling strategies, even complex modules with memory, by using a straightforward approach that decouples representation and classification. Our code is available at https://github.com/facebookresearch/classifier-balancing.",0
Title: Decoupling representation learning from classifier training can achieve high accuracy on long-tailed recognition problems,1
"Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.",0
"Introduction: This work presents a novel approach to learning representations from temporal graphs using an inductive method. The primary contribution lies in our ability to capture complex relationships between nodes in the graph by inducing latent features that can be used for downstream tasks such as node classification, link prediction, and visualization. We achieve this through a combination of graph neural networks (GNNs) and deep generative models, which allows us to learn both transient dynamics and stable patterns in the data. Our approach also leverages efficient algorithms for scalability and generalizes well across different datasets. The results demonstrate significant improvements compared to existing methods, making our model a promising tool for representation learning on dynamic graphs. Overall, we aim to address some of the challenges associated with learning representations from big graph data, particularly those related to scalability and flexibility.",1
"An important problem in multiview representation learning is finding the optimal combination of views with respect to the specific task at hand. To this end, we introduce NAM: a Neural Attentive Multiview machine that learns multiview item representations and similarity by employing a novel attention mechanism. NAM harnesses multiple information sources and automatically quantifies their relevancy with respect to a supervised task. Finally, a very practical advantage of NAM is its robustness to the case of dataset with missing views. We demonstrate the effectiveness of NAM for the task of movies and app recommendations. Our evaluations indicate that NAM outperforms single view models as well as alternative multiview methods on item recommendations tasks, including cold-start scenarios.",0
"Title: ""Neural Attentive Multiview Machines"" by <REDACTED> et al (2022) Paper Abstract: In this work, we propose a new approach called 'neural attentive multiview machines' that combines multiple views of data using attention mechanisms in neural networks. Our method takes advantage of both intra-view relationships among samples and inter-view relationships across different views, enabling flexible modeling of complex patterns in high-dimensional data. We evaluate our method on several benchmark datasets and demonstrate its superior performance over state-of-the-art methods in terms of accuracy, robustness, interpretability, scalability, and transferability. Furthermore, we provide theoretical insights into why neural attentative multiview models perform well, and discuss promising future directions for extending this framework to other applications such as deep learning, computer vision, natural language processing, and time series analysis. Overall, our work provides a novel perspective on how to effectively integrate complementary information from diverse sources, offering a powerful tool for extracting knowledge from large and complex datasets.",1
"The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning.",0
"In order to fully represent complex objects like natural scenes, state-of-the-art convolutional neural networks (CNNs) rely on deep architectures consisting of many layers which introduce nonlinearities that make them difficult to interpret. Furthermore, the success of these models often relies heavily on large amounts of training data, making robust representations even more important as model capacity increases. Here, we propose using multi-view observations from multiple cameras capturing different perspectives of the same scene to encourage the CNN to learn better representations. We frame this problem through the lens of the information bottleneck principle, where each view contributes to minimizing the total error while respecting a given constraint on the mutual information between hidden features and input data. Our experiments demonstrate the effectiveness of our method compared against baselines trained under standard supervised object recognition settings across five datasets: CIFAR10/100, SVHN, COCO Stuff and PASCAL VOC2007/2012. These results suggest that learning from multi-view data leads to higher accuracy, faster convergence during training and better generalization performance without requiring more parameters or increased computational cost at test time. Finally, we show that adding regularizers based on mutual information can further improve performance by reducing overfitting and encouraging disentangled representations. Overall, our approach provides a promising direction towards improving both the efficiency and interpretability of modern deep learning methods on visual recognition tasks. Keywords: Convolutional Neural Networks, Deep Learning, Multiple View Training, Image Recognition, Object Classification, Disen",1
"Most existing 3D CNNs for video representation learning are clip-based methods, and thus do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, referred as V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, and at the same time, to preserve strong 3D spatio-temporal representation with residual connections. Specifically, we design a new 4D residual block able to capture inter-clip interactions, which could enhance the representation power of the original clip-level 3D CNNs. The 4D residual blocks can be easily integrated into the existing 3D CNNs to perform long-range modeling hierarchically. We further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin.",0
"This paper presents V4D:4D convolutional neural networks (CNN), which learn discriminative features for video representation directly from raw frames without using any spatio-temporal preprocessing steps like optical flow or action recognition pipelines. Our approach uses multi-scale temporal aggregations and dilated convolutions to leverage both spatial and temporal representations. We evaluate our method on three challenging tasks, i.e., video classification, activity detection, and feature embedding. Results show that the proposed approach outperforms state-of-the-art methods by achieving new benchmarks on all three datasets while operating at real-time inference speeds.  This paper introduces V4D:4D CNN as a method for learning meaningful, discriminative video features through direct processing of raw frame data. In contrast to existing approaches relying heavily on preprocessed inputs such as optical flow or action recognition pipelines, the proposed technique effectively leverages multi-scale temporal aggregations and dilation convolutions for high performance in video classification, activity detection, and feature embedding tasks. Extensive experimental evaluation demonstrates significant improvements over competitor models, achieving record accuracies even under constrained computation time limitations due to real-time operation requirements.",1
"Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. In this paper, we propose to leverage cross-modal information to enhance metric-based few-shot learning methods. Visual and semantic feature spaces have different structures by definition. For certain concepts, visual features might be richer and more discriminative than text ones. While for others, the inverse might be true. Moreover, when the support from visual information is limited in image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on these two intuitions, we propose a mechanism that can adaptively combine information from both modalities according to new image categories to be learned. Through a series of experiments, we show that by this adaptive combination of the two modalities, our model outperforms current uni-modality few-shot learning methods and modality-alignment methods by a large margin on all benchmarks and few-shot scenarios tested. Experiments also show that our model can effectively adjust its focus on the two modalities. The improvement in performance is particularly large when the number of shots is very small.",0
"Recent advances in few-shot learning have shown significant improvements across a range of domains, including computer vision and natural language processing. However, most existing approaches assume that each task involves only one modality (either visual or textual). In practice, many real-world tasks involve multiple modalities, such as images and their associated captions or audio descriptions. To address these challenges, we propose a new framework called adaptive cross-modal few-shot learning (ACMF), which leverages both intra-modality (e.g., image features) and inter-modality (image/caption pairs) relationships during training. Our model learns to dynamically weight the importance of different modality alignments based on task requirements, using a novel attention mechanism that effectively balances between relevant and irrelevant data during adaptation. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on two benchmark datasets: ImageNet and ScribD. Our findings provide valuable insights into the design of future cross-modal machine learning algorithms that can efficiently learn from diverse input types under limited supervision.",1
"Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for a multitude of tasks. However, no such general model exists to date beyond simply applying discretization or feed-forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Therefore, we propose a representation learning model called Space2Vec to encode the absolute positions and spatial relationships of places. We conduct experiments on two real-world geographic data for two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification leveraging their geo-locations. Results show that because of its multi-scale representations, Space2Vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. Detailed analysis shows that all baselines can at most well handle distribution at one scale but show poor performances in other scales. In contrast, Space2Vec's multi-scale representation can handle distributions at different scales.",0
"This papers presents multi-scale representation learning of spatial feature distributions using grid cells, which allows efficient processing of data from different scales by combining local representations into global ones. We investigate the properties of grid cells as multi-scale features and propose neural network architectures that can learn these properties and enhance their performance on various tasks such as visualization, classification, prediction, detection, etc. Our experiments show that our models achieve state-of-the art results on benchmark datasets in vision, robotics, computer graphics, and other applications requiring spatial understanding and reasoning at multiple scales.",1
"Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under well-defined and reproducible experimental conditions. We observe that a small number of labeled examples (0.01--0.5\% of the data set), with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, we investigate the benefit of incorporating supervision into the training process. Overall, we empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations.",0
"In many machine learning tasks involving high-dimensional data, there may be multiple factors that influence variations within the data. These factors can interact in complex ways, making it challenging to identify and disentangle them from each other. One approach to address this problem is through using few labels, which involves selecting a subset of relevant features to label and using these labels as constraints to guide factor identification.  This paper presents a methodology for identifying and disentangling factors of variation using few labels. We demonstrate how our method improves upon existing approaches by enabling more accurate factor recovery, particularly when dealing with low sample sizes or noisy labels. Our results show significant improvements across diverse datasets, including both synthetic data experiments and real-world image classification benchmarks.  Our framework leverages deep generative models to learn an embedding space where factors of interest are linearly separable. By explicitly modeling the effects of these factors on the observed measurements, we are able to achieve better factor separation than previous methods that rely solely on minimizing reconstruction error. Furthermore, we introduce a new variational autoencoder architecture that utilizes shared weights to regularize the encoders and decoders, allowing us to control the difficulty of factor separation while preserving high quality reconstructions.  Overall, our work advances the field of unsupervised learning and representation discovery, providing a powerful tool for researchers working with high-dimensional, complex datasets. By facilitating the efficient and accurate extraction of meaningful relationships between variables, our approach has the potential to enable exciting breakthroughs in fields such as scientific inquiry, medical diagnosis, and environmental monitoring.",1
"Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. Our approach, Unsupervised Disentanglement Ranking (UDR), leverages the recent theoretical results that explain why variational autoencoders disentangle (Rolinek et al, 2019), to quantify the quality of disentanglement by performing pairwise comparisons between trained model representations. We show that our approach performs comparably to the existing supervised alternatives across 5,400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains.",0
"In recent years, variational disentangled representation learning (VDRL) has emerged as a powerful technique for uncovering hidden factors underlying complex datasets while preserving their structure. However, choosing an appropriate VDRL model can be challenging due to the growing number of models available and their varying complexity. Moreover, there is currently no systematic approach to selecting the optimal VDRL architecture for a given task. This paper presents an unsupervised model selection framework that leverages intrinsic motivations such as data fitting, compression, and self-consistency in order to find the most suitable VDRL model for a given dataset. Our experimental results demonstrate that our method leads to better accuracy on several benchmark datasets compared to random search and existing heuristics. Additionally, we show that our approach successfully selects simpler architectures over more complicated ones when possible, indicating that our objective function effectively captures desirable properties of good models. Overall, this work represents a significant step towards automating VDRL model discovery, which could potentially make these methods accessible to non-expert practitioners working on various domains ranging from computer vision to natural language processing.",1
"We propose the first qualitative hypothesis characterizing the behavior of visual transformation based self-supervision, called the VTSS hypothesis. Given a dataset upon which a self-supervised task is performed while predicting instantiations of a transformation, the hypothesis states that if the predicted instantiations of the transformations are already present in the dataset, then the representation learned will be less useful. The hypothesis was derived by observing a key constraint in the application of self-supervision using a particular transformation. This constraint, which we term the transformation conflict for this paper, forces a network learn degenerative features thereby reducing the usefulness of the representation. The VTSS hypothesis helps us identify transformations that have the potential to be effective as a self-supervision task. Further, it helps to generally predict whether a particular transformation based self-supervision technique would be effective or not for a particular dataset. We provide extensive evaluations on CIFAR 10, CIFAR 100, SVHN and FMNIST confirming the hypothesis and the trends it predicts. We also propose novel cost-effective self-supervision techniques based on translation and scale, which when combined with rotation outperforms all transformations applied individually. Overall, this paper aims to shed light on the phenomenon of visual transformation based self-supervision.",0
"In recent years, there has been increased interest in leveraging self-supervised learning (SSL) techniques as alternatives to traditional supervised methods in computer vision tasks. SSL algorithms learn from large amounts of unlabeled data by identifying patterns in unlabelled datasets that can serve as informative proxies for the supervisory signals typically provided through labelling during training. Recent work has shown promising results using image generation tasks such as colorization or jigsaw puzzle solving for visual representation learning. However, these approaches rely heavily on manual design of specific transformations which may not always generalize well across different dataset types or domains. In this paper we propose a new approach called Visual Transformation based Self-Supervision (VTSS), which automatically generates diverse variations of given images, thereby facilitating the discovery of more generalizable features. By exploiting modern deep generative models, our method creates meaningful variants without explicit domain knowledge, thus enabling efficient feature learning without relying exclusively on task-specific handcrafted operations. Experimentally, we demonstrate VTSS outperforms state-of-the-art SSL methods on standard benchmarks and is easily adaptable to other challenging computer vision problems such as object detection and semantic segmentation. Our findings indicate the potential of VTSS for tackling many open problems in computer vision, including fine-grained classification and zero-shot transfer learning, further highlighting its general applicability beyond existing specialized SSL frameworks. This research seeks to address the challenge of developing self-supervised learning (SSL) techniques applicable to various computer vision tasks using automatic visual transformation generation. Current methods require manually designed transformations which might not generalize effectively to different datasets o",1
"Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.",0
"In recent years, graph convolutional networks (GCNs) have emerged as a powerful tool for modeling complex geometric data such as graphs, networks, and other irregularly structured data. However, current GCN architectures suffer from several limitations that hinder their performance on many real-world problems. To address these limitations, we propose Geom-GCN, a new architecture based on a novel combination of geometric and spectral techniques. Our approach unifies traditional spectral methods with a geometric perspective, allowing us to leverage both types of information for more effective representation learning. We demonstrate the effectiveness of our method through extensive experiments on three benchmark datasets, showing significant improvement over state-of-the-art baselines across multiple metrics. Overall, Geom-GCN represents a promising step forward in the field of graph neural networks, opening up exciting possibilities for further research and applications in various domains.",1
"Supervised classification and representation learning are two widely used classes of methods to analyze multivariate images. Although complementary, these methods have been scarcely considered jointly in a hierarchical modeling. In this paper, a method coupling these two approaches is designed using a matrix cofactorization formulation. Each task is modeled as a factorization matrix problem and a term relating both coding matrices is then introduced to drive an appropriate coupling. The link can be interpreted as a clustering operation over a low-dimensional representation vectors. The attribution vectors of the clustering are then used as features vectors for the classification task, i.e., the coding vectors of the corresponding factorization problem. A proximal gradient descent algorithm, ensuring convergence to a critical point of the objective function, is then derived to solve the resulting non-convex non-smooth optimization problem. An evaluation of the proposed method is finally conducted both on synthetic and real data in the specific context of hyperspectral image interpretation, unifying two standard analysis techniques, namely unmixing and classification.",0
"In recent years, matrix cofactorization has emerged as a powerful tool for joint representation learning and supervised classification tasks such as hyperspectral image analysis. This approach enables the simultaneous decomposition of multiple matrices into low-rank approximations while enforcing mutual constraints among them. By combining these decompositions, one can obtain shared latent representations that capture underlying structures across different data modalities.  This paper focuses on extending matrix cofactorization techniques towards effective hyperspectral image analysis. We propose novel formulations that incorporate both global and local structure preservation priors, allowing us to more accurately capture spatial dependencies within hyperspectral images. Our framework furthermore integrates additional sparsity regularizations to promote noise suppression and improve overall model interpretability.  In experiments conducted on benchmark datasets, we demonstrate the superior performance of our proposed methods over state-of-the-art alternatives. Results showcase enhanced reconstruction accuracies coupled with improved discriminative capabilities, validating the effectiveness of matrix cofactorization for hyperspectral image applications. These promising findings highlight significant potentials for future research directions in applying matrix cofactorization to jointly learn compact representations and perform accurate prediction from complex high-dimensional data.  Note: This is just a sample abstract. Please keep in mind that I am not qualified to provide you with specific information regarding your field of expertise. If you need professional assistance with writing an actual abstract, please seek the advice of someone who is knowledgeable in your area of study.",1
"Graph convolutional networks (GCNs) are a widely used method for graph representation learning. We investigate the power of GCNs, as a function of their number of layers, to distinguish between different random graph models on the basis of the embeddings of their sample graphs. In particular, the graph models that we consider arise from graphons, which are the most general possible parameterizations of infinite exchangeable graph models and which are the central objects of study in the theory of dense graph limits. We exhibit an infinite class of graphons that are well-separated in terms of cut distance and are indistinguishable by a GCN with nonlinear activation functions coming from a certain broad class if its depth is at least logarithmic in the size of the sample graph. These results theoretically match empirical observations of several prior works. Finally, we show a converse result that for pairs of graphons satisfying a degree profile separation property, a very simple GCN architecture suffices for distinguishability. To prove our results, we exploit a connection to random walks on graphs.",0
"In recent years, graph convolutional networks (GCN) have emerged as powerful tools for analyzing graphs and identifying patterns within them. This study evaluates the ability of GCN to distinguish random graph models from real-world networks by testing their performance on four distinct types of random graphs. Our results show that while there is some variation across different models, overall GCN outperforms traditional methods such as degree distribution analysis and clustering coefficient calculation. Additionally, we find that certain GCN layers perform better at distinguishing randomness than others, indicating the importance of selecting appropriate network architecture for each specific task. These findings provide insights into the strengths and limitations of using GCNs for detecting non-random structure in complex networks and suggest potential future directions for research.",1
"Clustering is a fundamental task in data analysis. Recently, deep clustering, which derives inspiration primarily from deep learning approaches, achieves state-of-the-art performance and has attracted considerable attention. Current deep clustering methods usually boost the clustering results by means of the powerful representation ability of deep learning, e.g., autoencoder, suggesting that learning an effective representation for clustering is a crucial requirement. The strength of deep clustering methods is to extract the useful representations from the data itself, rather than the structure of data, which receives scarce attention in representation learning. Motivated by the great success of Graph Convolutional Network (GCN) in encoding the graph structure, we propose a Structural Deep Clustering Network (SDCN) to integrate the structural information into deep clustering. Specifically, we design a delivery operator to transfer the representations learned by autoencoder to the corresponding GCN layer, and a dual self-supervised mechanism to unify these two different deep neural architectures and guide the update of the whole model. In this way, the multiple structures of data, from low-order to high-order, are naturally combined with the multiple representations learned by autoencoder. Furthermore, we theoretically analyze the delivery operator, i.e., with the delivery operator, GCN improves the autoencoder-specific representation as a high-order graph regularization constraint and autoencoder helps alleviate the over-smoothing problem in GCN. Through comprehensive experiments, we demonstrate that our propose model can consistently perform better over the state-of-the-art techniques.",0
"Artificial intelligence (AI) has been rapidly advancing over recent years, leading to significant improvements in various domains such as computer vision, natural language processing, robotics, among others. In particular, one area that has seen remarkable progress is image classification using convolutional neural networks (CNNs). While these models have achieved impressive results, they still suffer from certain limitations such as sensitivity to initial conditions, difficulty in interpreting results, and lack of robustness towards input perturbations. These challenges often arise due to the intricate relationship between different layers of the model, which makes it difficult to visualize and analyze their behavior. Therefore, there is a need to develop novel techniques to better understand CNNs and improve their performance. This work proposes a new architecture called structural deep clustering network (SDCN), which addresses these issues by introducing an additional layer that helps disentangle features learned by CNNs, improving interpretability and robustness. We evaluate our approach on several benchmark datasets and demonstrate that SDCN achieves state-of-the-art performance while providing greater insight into how the network operates. Our findings suggest that incorporating structure into deep learning models can significantly enhance their ability to learn complex representations and generalize well across diverse settings. The proposed framework has broad implications for other areas of research where understanding network behavior is crucial. Overall, this study contributes to the growing field of explainable AI and provides valuable insights into how we can design more efficient and transparent machine learning systems.",1
"Anomaly detection on attributed networks aims at finding nodes whose patterns deviate significantly from the majority of reference nodes, which is pervasive in many applications such as network intrusion detection and social spammer detection. However, most existing methods neglect the complex cross-modality interactions between network structure and node attribute. In this paper, we propose a deep joint representation learning framework for anomaly detection through a dual autoencoder (AnomalyDAE), which captures the complex interactions between network structure and node attribute for high-quality embeddings. Specifically, AnomalyDAE consists of a structure autoencoder and an attribute autoencoder to learn both node embedding and attribute embedding jointly in latent space. Moreover, attention mechanism is employed in structure encoder to learn the importance between a node and its neighbors for an effective capturing of structure pattern, which is important to anomaly detection. Besides, by taking both the node embedding and attribute embedding as inputs of attribute decoder, the cross-modality interactions between network structure and node attribute are learned during the reconstruction of node attribute. Finally, anomalies can be detected by measuring the reconstruction errors of nodes from both the structure and attribute perspectives. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed method.",0
"Attempts should follow AAAI guidelines for formatting. Use at least two relevant citations from other publications (with DOIs). Provide an example of one of your own papers as reference. Citation style can follow any convention but should remain consistent throughout. The aim of this study is to propose a novel dual autoencoder architecture, named AnomalyDAE, for detecting anomalies in attributed networks. This approach leverages the benefits of deep learning algorithms while taking into account both structural properties of nodes and their attributes. In particular, we introduce an extension of traditional autoencoders that integrates both types of information simultaneously, leading to a more robust model capable of identifying outliers across diverse data sets. Our experimental evaluation demonstrates promising results compared to several benchmark methods, including variational autoencoders, graph convolutional neural networks, and random walk-based approaches. Additionally, our analysis provides insights into how different network features impact anomaly detection performance, providing valuable guidance for practitioners seeking to enhance the accuracy of outlier identification tasks. This work contributes to the broader field of anomaly detection research by offering a new methodology that addresses shortcomings present in existing techniques. Finally, we discuss potential directions for future studies, emphasizing the importance of developing models tailored to specific domains based on unique characteristics inherent within each application scenario. This article cites 2 related works [1] and [2], and one of the author’s previous papers [3].  [1] M. Ghasemzadeh, R. K. Srivastava, et al., “An Attributed Network Embedding Framework Using Deep Learning Techniques,” arXiv preprint arXiv:1806.04971v3, 2018.  [2] N. Wang, Z. Chen, B. Cheng, Y. Yu, X. Liu, J. Wang, “Attention-based Graph Convolutional Neural Networks for Fraud Detection i",1
"Considering the inherent stochasticity and uncertainty, predicting future video frames is exceptionally challenging. In this work, we study the problem of video prediction by combining interpretability of stochastic state space models and representation learning of deep neural networks. Our model builds upon an variational encoder which transforms the input video into a latent feature space and a Luenberger-type observer which captures the dynamic evolution of the latent features. This enables the decomposition of videos into static features and dynamics in an unsupervised manner. By deriving the stability theory of the nonlinear Luenberger-type observer, the hidden states in the feature space become insensitive with respect to the initial values, which improves the robustness of the overall model. Furthermore, the variational lower bound on the data log-likelihood can be derived to obtain the tractable posterior prediction distribution based on the variational principle. Finally, the experiments such as the Bouncing Balls dataset and the Pendulum dataset are provided to demonstrate the proposed model outperforms concurrent works.",0
"This is a technical paper that proposes a new observer design approach called the deep variational Luenberger-type observer (DVLO) for video prediction tasks. The DVLO combines elements from machine learning, particularly neural networks, with classical control theory, resulting in a hybrid method capable of handling stochastic disturbances while offering robustness against uncertainty. With applications across robotics and automation fields, the proposed algorithm demonstrates improved performance over traditional methods without sacrificing simplicity and interpretability.",1
"In this paper, we propose a new video representation learning method, named Temporal Squeeze (TS) pooling, which can extract the essential movement information from a long sequence of video frames and map it into a set of few images, named Squeezed Images. By embedding the Temporal Squeeze pooling as a layer into off-the-shelf Convolution Neural Networks (CNN), we design a new video classification model, named Temporal Squeeze Network (TeSNet). The resulting Squeezed Images contain the essential movement information from the video frames, corresponding to the optimization of the video classification task. We evaluate our architecture on two video classification benchmarks, and the results achieved are compared to the state-of-the-art.",0
"This paper presents a method for learning spatio-temporal representations using a novel technique called ""temporal squeeze pooling."" By compressing temporally correlated feature maps into smaller spatial dimensions, our approach enables efficient training of deep neural networks on video data while retaining essential motion patterns. We demonstrate that our model outperforms previous state-of-the-art methods on challenging action recognition benchmarks. Additionally, we provide visualizations of learned features to showcase the importance of temporal context for capturing complex motion sequences. Our work contributes to the broader field of computer vision by advancing the understanding of how deep learning models can effectively learn from sequential data.",1
"Despite the success of Generative Adversarial Networks (GANs) in image synthesis, there lacks enough understanding on what generative models have learned inside the deep generative representations and how photo-realistic images are able to be composed of the layer-wise stochasticity introduced in recent GANs. In this work, we show that highly-structured semantic hierarchy emerges as variation factors from synthesizing scenes from the generative representations in state-of-the-art GAN models, like StyleGAN and BigGAN. By probing the layer-wise representations with a broad set of semantics at different abstraction levels, we are able to quantify the causality between the activations and semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors learned by GANs to compose scenes. The qualitative and quantitative results further suggest that the generative representations learned by the GANs with layer-wise latent codes are specialized to synthesize different hierarchical semantics: the early layers tend to determine the spatial layout and configuration, the middle layers control the categorical objects, and the later layers finally render the scene attributes as well as color scheme. Identifying such a set of manipulatable latent variation factors facilitates semantic scene manipulation.",0
"In natural language understanding tasks such as text classification and machine translation, deep neural networks have achieved state-of-the art performance by learning representations that capture high-level semantic concepts. Despite recent progress in unsupervised scene synthesis using deep generative models like Variational Autoencoders (VAEs), these methods still lack access to explicit semantic information during training. Here we show that a VAE can learn semantic hierarchies at different levels of abstraction from large amounts of unstructured data, without any external supervision. These learned embeddings are directly interpretable as meaningful object categories even though they were never exposed to them during training. We demonstrate several applications of these embeddings on scene synthesis and downstream vision tasks. Notably, our approach outperforms alternative unsupervised baselines on both segmentation and image generation tasks.",1
"Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the learned representation should comprise: 1) accurate prediction in the observation space, 2) consistency between latent and observation space dynamics, and 3) low curvature in the latent space transitions. These principles naturally correspond to a loss function that consists of three terms: prediction, consistency, and curvature (PCC). Crucially, to make PCC tractable, we derive an amortized variational bound for the PCC loss function. Extensive experiments on benchmark domains demonstrate that the new variational-PCC learning algorithm benefits from significantly more stable and reproducible training, and leads to superior control performance. Further ablation studies give support to the importance of all three PCC components for learning a good latent space for control.",0
"In ""Predictive Modeling for Locally Linear Control,"" we propose a new framework for representation learning that enables locally linear control tasks through predictive modeling. Our approach utilizes representations learned from sequential data and leverages them to generate predictions, allowing the controller to make decisions based on local curvatures. We show how our method can improve performance on challenging problems such as controlling robots in dynamic environments. Additionally, we provide theoretical analysis to demonstrate the efficacy of our method. By introducing prediction into locally linear control, we significantly enhance its capabilities while remaining efficient and computationally tractable. This work has broad implications for the field of robotics and intelligent systems and paves the way for future advancements in control and decision making under uncertainty.",1
"Canonical Correlation Analysis (CCA) is a linear representation learning method that seeks maximally correlated variables in multi-view data. Non-linear CCA extends this notion to a broader family of transformations, which are more powerful in many real-world applications. Given the joint probability, the Alternating Conditional Expectation (ACE) algorithm provides an optimal solution to the non-linear CCA problem. However, it suffers from limited performance and an increasing computational burden when only a finite number of samples is available. In this work we introduce an information-theoretic compressed representation framework for the non-linear CCA problem (CRCCA), which extends the classical ACE approach. Our suggested framework seeks compact representations of the data that allow a maximal level of correlation. This way we control the trade-off between the flexibility and the complexity of the model. CRCCA provides theoretical bounds and optimality conditions, as we establish fundamental connections to rate-distortion theory, the information bottleneck and remote source coding. In addition, it allows a soft dimensionality reduction, as the compression level is determined by the mutual information between the original noisy data and the extracted signals. Finally, we introduce a simple implementation of the CRCCA framework, based on lattice quantization.",0
"Introduction Canonical correlation analysis (CCA) has been used extensively for finding the relationship between two matrices by projecting both onto a lower dimensional space [2]. However traditional CCA assumes that linear relationships exist which may be restrictive. Non-linear canonical correlation analysis (NCCA) is an extension to CCA where non-linear functions can capture more complex data structures. This article proposes an algorithmic approach that uses compressed sensing based regularization methods combined with autoencoders. Regularization methods have shown good results in compressing data without losing important features [8] [6] but little research on using these techniques directly within NCCA models exists. Methods This work develops a novel framework called nccar that unifies several existing machine learning algorithms under one umbrella, enabling practitioners to solve challenging real world problems such as image retrieval and recommendation systems via the same interface. The proposed model works by building an encoder network followed by a latent variable layer then finally decoding back into feature space. We adopt l_1 norm minimization with Lasso regression for solving the optimization problem instead of other expensive solvers like singular value decompositions or eigenvalue decomposition. Results Experiments were conducted with different data sizes ranging from 4x4 grids of pixels to 784 pixel images for the MNIST and FashionMNIST dataset respectively. In our experiments, we observed significant reduction in reconstruction error during training time due to efficient memory usage compared to state of art approaches. Furthermore, our results indicate improved performance over baseline deep neural networks trained independently. Conclusion We demonstrate the power of regularization techniques along with an Autoenco der structu",1
"In existing visual representation learning tasks, deep convolutional neural networks (CNNs) are often trained on images annotated with single tags, such as ImageNet. However, a single tag cannot describe all important contents of one image, and some useful visual information may be wasted during training. In this work, we propose to train CNNs from images annotated with multiple tags, to enhance the quality of visual representation of the trained CNN model. To this end, we build a large-scale multi-label image database with 18M images and 11K categories, dubbed Tencent ML-Images. We efficiently train the ResNet-101 model with multi-label outputs on Tencent ML-Images, taking 90 hours for 60 epochs, based on a large-scale distributed deep learning framework,i.e.,TFplus. The good quality of the visual representation of the Tencent ML-Images checkpoint is verified through three transfer learning tasks, including single-label image classification on ImageNet and Caltech-256, object detection on PASCAL VOC 2007, and semantic segmentation on PASCAL VOC 2012. The Tencent ML-Images database, the checkpoints of ResNet-101, and all the training codehave been released at https://github.com/Tencent/tencent-ml-images. It is expected to promote other vision tasks in the research and industry community.",0
"Title: ""A large scale multi-label image database""  Summary: This paper presents Tencent ML-Images, a new dataset that contains millions of images organized into hundreds of semantic labels. The authors designed and collected data specifically for visual representation learning tasks such as classification and retrieval. The resulting resource provides researchers with a diverse set of labeled examples that cover many domains while mitigating biases present in existing datasets. In addition, they propose novel algorithms utilizing the unique properties of the dataset, including label embedding learning and multi-scale dense attention. Overall, the introduction of Tencent ML-Images marks an important step forward for computer vision research by offering practitioners an accessible and well-organized tool to advance their work.",1
"In graph instance representation learning, both the diverse graph instance sizes and the graph node orderless property have been the major obstacles that render existing representation learning models fail to work. In this paper, we will examine the effectiveness of GRAPH-BERT on graph instance representation learning, which was designed for node representation learning tasks originally. To adapt GRAPH-BERT to the new problem settings, we re-design it with a segmented architecture instead, which is also named as SEG-BERT (Segmented GRAPH-BERT) for reference simplicity in this paper. SEG-BERT involves no node-order-variant inputs or functional components anymore, and it can handle the graph node orderless property naturally. What's more, SEG-BERT has a segmented architecture and introduces three different strategies to unify the graph instance sizes, i.e., full-input, padding/pruning and segment shifting, respectively. SEG-BERT is pre-trainable in an unsupervised manner, which can be further transferred to new tasks directly or with necessary fine-tuning. We have tested the effectiveness of SEG-BERT with experiments on seven graph instance benchmark datasets, and SEG-BERT can out-perform the comparison methods on six out of them with significant performance advantages.",0
"Recent advances in graph neural networks (GNNs) have demonstrated their effectiveness in modeling complex structures such as graphs. However, most GNN models suffer from limited scalability due to high memory requirements and computational cost. In this work, we propose a novel method called Segmented Graph-BERT (SGB), which addresses these limitations by decomposing large graphs into smaller subgraphs and using pretrained transformers like BERT to capture representations at each level. Our approach enables efficient computation while preserving important features of large graphs, resulting in improved performance on downstream tasks such as node classification and graph classification. Experimental results demonstrate that our method outperforms state-of-the-art methods across multiple datasets. Overall, SGB represents a significant step forward in enabling effective instance modeling of large and complex graphs.",1
"Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-called meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.",0
"Graph Transformer Networks (GTN) are revolutionizing natural language processing tasks by providing state-of-the-art performance while being model agnostic and parallelizable. This research introduces GTN, building upon the foundational work by Vaswani et al., by presenting new improvements on training dynamics and architectural design. Our contributions can be categorized into three categories: improved stability during training through novel regularization techniques; optimized architecture designs yielding better generalization across datasets; and increased efficiency due to parallel computations across GPUs/TPUs. These advances are achieved using only data-free modifications to existing transformer networks, enabling broad applicability without requiring access to new data. We demonstrate superior results compared to previous methods across benchmark datasets such as WikiText-2, TGLu, SST-2, and BoolQ on both single-task models and multi-tasks setups, validating our approach as a reliable choice for real-world applications. Our findings suggest that further exploration of graph signal processing within NLP frameworks could lead to even stronger gains in accuracy and efficiency.",1
"Massive Open Online Courses (MOOCs) have become popular platforms for online learning. While MOOCs enable students to study at their own pace, this flexibility makes it easy for students to drop out of class. In this paper, our goal is to predict if a learner is going to drop out within the next week, given clickstream data for the current week. To this end, we present a multi-layer representation learning solution based on branch and bound (BB) algorithm, which learns from low-level clickstreams in an unsupervised manner, produces interpretable results, and avoids manual feature engineering. In experiments on Coursera data, we show that our model learns a representation that allows a simple model to perform similarly well to more complex, task-specific models, and how the BB algorithm enables interpretable results. In our analysis of the observed limitations, we discuss promising future directions.",0
"This can be difficult! Let me think... How about something like: ""The ability to predict student dropout rates has important implications for education, particularly in the context of massive open online courses (MOOCs). In this work, we propose a novel approach to modeling dropout behavior based on multi-layer representation learning and interpretability techniques. Our method leverages course data from multiple sources, including activity logs and survey responses, to identify patterns that can inform predictions of which students are at risk of dropping out of their MOOC. By evaluating our model against real-world data, we demonstrate its effectiveness in accurately forecasting dropout rates weeks ahead of time.""",1
"Recent findings in neuroscience suggest that the human brain represents information in a geometric structure (for instance, through conceptual spaces). In order to communicate, we flatten the complex representation of entities and their attributes into a single word or a sentence. In this paper we use graph convolutional networks to support the evolution of language and cooperation in multi-agent systems. Motivated by an image-based referential game, we propose a graph referential game with varying degrees of complexity, and we provide strong baseline models that exhibit desirable properties in terms of language emergence and cooperation. We show that the emerged communication protocol is robust, that the agents uncover the true factors of variation in the game, and that they learn to generalize beyond the samples encountered during training.",0
"""Graph representation learning has emerged as a powerful tool for modeling complex data structures and their relationships. In recent years, there has been growing interest in applying these techniques to natural language processing (NLP), where graph representations can capture important linguistic and semantic features of texts. One promising area within NLP that stands to benefit from advances in graph representation learning is emergent communication. Emergent communication refers to situations where individuals communicate without predefined instructions, rules, or plans, relying instead on spontaneous and improvisational strategies. This type of communication occurs frequently in everyday life and poses unique challenges for automated systems tasked with understanding and generating human-like responses. To address these challenges, we present a review of current research at the intersection of graph representation learning and emergent communication. We discuss how graphs have been applied to represent various aspects of spontaneous communication, such as speaker intentions, message meaning, discourse structure, and conversational dynamics. We also examine the strengths and limitations of existing approaches and identify areas ripe for future investigation, including computational models that integrate multiple sources of evidence to generate more accurate graph representations. By highlighting the potential benefits of combining graph representation learning and emergent communication, we hope to inspire further work towards developing more advanced NLP technologies capable of handling complex real-world communication scenarios.""",1
"The richness in the content of various information networks such as social networks and communication networks provides the unprecedented potential for learning high-quality expressive representations without external supervision. This paper investigates how to preserve and extract the abundant information from graph-structured data into embedding space in an unsupervised manner. To this end, we propose a novel concept, Graphical Mutual Information (GMI), to measure the correlation between input graphs and high-level hidden representations. GMI generalizes the idea of conventional mutual information computations from vector space to the graph domain where measuring mutual information from two aspects of node features and topological structure is indispensable. GMI exhibits several benefits: First, it is invariant to the isomorphic transformation of input graphs---an inevitable constraint in many existing graph representation learning algorithms; Besides, it can be efficiently estimated and maximized by current mutual information estimation methods such as MINE; Finally, our theoretical analysis confirms its correctness and rationality. With the aid of GMI, we develop an unsupervised learning model trained by maximizing GMI between the input and output of a graph neural encoder. Considerable experiments on transductive as well as inductive node classification and link prediction demonstrate that our method outperforms state-of-the-art unsupervised counterparts, and even sometimes exceeds the performance of supervised ones.",0
"Graph representation learning has become increasingly important in many fields, including computer vision, natural language processing, and bioinformatics. In this work, we propose a novel method for graph representation learning based on maximizing graphical mutual information (GMI). GMI measures the amount of information shared between two random variables that are dependent on each other, and can effectively capture the relationships among nodes in a graph. By optimizing GMI over graphs, we aim to learn node representations that preserve as much of the structural information from the original graph as possible. Our approach is model agnostic and can be applied to any downstream task requiring graph data, such as semi-supervised learning, clustering, classification, or regression. Experiments on several benchmark datasets demonstrate the effectiveness of our proposed method compared to state-of-the-art baselines. This work provides new insights into graph representation learning and enables applications where structure preservation is essential, advancing the frontier of knowledge in machine learning.",1
"In this paper, we tackle for the first time, the problem of self-supervised representation learning for free-hand sketches. This importantly addresses a common problem faced by the sketch community -- that annotated supervisory data are difficult to obtain. This problem is very challenging in that sketches are highly abstract and subject to different drawing styles, making existing solutions tailored for photos unsuitable. Key for the success of our self-supervised learning paradigm lies with our sketch-specific designs: (i) we propose a set of pretext tasks specifically designed for sketches that mimic different drawing styles, and (ii) we further exploit the use of a textual convolution network (TCN) in a dual-branch architecture for sketch feature learning, as means to accommodate the sequential stroke nature of sketches. We demonstrate the superiority of our sketch-specific designs through two sketch-related applications (retrieval and recognition) on a million-scale sketch dataset, and show that the proposed approach outperforms the state-of-the-art unsupervised representation learning methods, and significantly narrows the performance gap between with supervised representation learning.",0
"This sounds interesting! Can you provide more context on the topic? Also, would you prefer me to write the entire abstract for you or just give suggestions and tips for improvement? Let me know how I can assist.",1
"With the prevalence of RGB-D cameras, multi-modal video data have become more available for human action recognition. One main challenge for this task lies in how to effectively leverage their complementary information. In this work, we propose a Modality Compensation Network (MCN) to explore the relationships of different modalities, and boost the representations for human action recognition. We regard RGB/optical flow videos as source modalities, skeletons as auxiliary modality. Our goal is to extract more discriminative features from source modalities, with the help of auxiliary modality. Built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model bridges data from source and auxiliary modalities by a modality adaptation block to achieve adaptive representation learning, that the network learns to compensate for the loss of skeletons at test time and even at training time. We explore multiple adaptation schemes to narrow the distance between source and auxiliary modal distributions from different levels, according to the alignment of source and auxiliary data in training. In addition, skeletons are only required in the training phase. Our model is able to improve the recognition performance with source data when testing. Experimental results reveal that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.",0
"This paper presents a new approach to action recognition that leverages cross-modal adaptation techniques to improve performance on modalities where data may be limited. We introduce the modality compensation network (MCN), which integrates visual features with additional modalities such as audio and motion capture data. Our method adapts pretrained deep models across modalities using adversarial training and cycle consistency losses, ensuring the generated features align well with their corresponding modalities. Experiments demonstrate MCN significantly improves accuracy over state-of-the-art approaches on standard benchmark datasets, demonstrating the effectiveness of our proposed method. Our results have important implications for developing more robust and generalizable action recognition systems.",1
"Have you ever wondered how a song might sound if performed by a different artist? In this work, we propose SCM-GAN, an end-to-end non-parallel song conversion system powered by generative adversarial and transfer learning that allows users to listen to a selected target singer singing any song. SCM-GAN first separates songs into vocals and instrumental music using a U-Net network, then converts the vocal segments to the target singer using advanced CycleGAN-VC, before merging the converted vocals with their corresponding background music. SCM-GAN is first initialized with feature representations learned from a state-of-the-art voice-to-voice conversion and then trained on a dataset of non-parallel songs. Furthermore, SCM-GAN is evaluated against a set of metrics including global variance GV and modulation spectra MS on the 24 Mel-cepstral coefficients (MCEPs). Transfer learning improves the GV by 35% and the MS by 13% on average. A subjective comparison is conducted to test the user satisfaction with the quality and the naturalness of the conversion. Results show above par similarity between SCM-GAN's output and the target (70\% on average) as well as great naturalness of the converted songs.",0
"In recent years, there has been growing interest in using deep neural networks to generate new music that sounds similar to existing recordings. One approach to achieving this goal is through the use of Generative Adversarial Networks (GANs), which consist of two neural networks trained together to optimize a cost function related to generating realistic data samples. However, current GAN models struggle with retaining the characteristics of one style in transferred songs while maintaining their original structure, resulting in poor performance on structured evaluation metrics like pitch accuracy or chord transcription correctness. This work presents ""Change Your Singer,"" a novel transfer learning method that leverages pre-trained voice models such as MelGAN or F0GAN to improve the quality of song-to-song conversion tasks. Our proposed model can generate high-quality audio samples in several different singing styles by training on a few hundred seconds of target style examples while preserving both the melody and lyrics of the source content. Extensive experiments demonstrate our method significantly outperforms baseline approaches across multiple metrics, demonstrating its effectiveness at creating diverse and musically meaningful outputs. Our contributions provide exciting opportunities for artists and producers looking to create new works from existing content without substantial studio time or rehearsals.",1
"For a robot to perform complex manipulation tasks, it is necessary for it to have a good grasping ability. However, vision based robotic grasp detection is hindered by the unavailability of sufficient labelled data. Furthermore, the application of semi-supervised learning techniques to grasp detection is under-explored. In this paper, a semi-supervised learning based grasp detection approach has been presented, which models a discrete latent space using a Vector Quantized Variational AutoEncoder (VQ-VAE). To the best of our knowledge, this is the first time a Variational AutoEncoder (VAE) has been applied in the domain of robotic grasp detection. The VAE helps the model in generalizing beyond the Cornell Grasping Dataset (CGD) despite having a limited amount of labelled data by also utilizing the unlabelled data. This claim has been validated by testing the model on images, which are not available in the CGD. Along with this, we augment the Generative Grasping Convolutional Neural Network (GGCNN) architecture with the decoder structure used in the VQ-VAE model with the intuition that it should help to regress in the vector-quantized latent space. Subsequently, the model performs significantly better than the existing approaches which do not make use of unlabelled images to improve the grasp.",0
"This paper presents a novel approach for semi-supervised grasp detection using representation learning in a vector quantized latent space (VQLS). Traditional approaches for grasp detection rely on fully supervised methods that require large amounts of annotated data, which can be time-consuming and expensive to collect. In contrast, our method uses a semi-supervised learning framework that leverages both labeled and unlabeled data to learn representations of objects in VQLS. These learned representations are then used to predict grasps for new, unseen objects without requiring any additional annotations. Our experiments demonstrate that our approach significantly outperforms state-of-the-art semi-supervised grasp detection methods while using less labeled data. Additionally, we show that our method generalizes well across different object categories and outperform supervised baselines in some cases as well. Overall, our results suggest that representation learning in VQLS has great potential for solving challenges in robotic manipulation tasks such as grasp detection under real-world conditions where obtaining large amounts of annotated data may not be feasible.",1
"This work combines Convolutional Neural Networks (CNNs), clustering via Self-Organizing Maps (SOMs) and Hebbian Learning to propose the building blocks of Convolutional Self-Organizing Neural Networks (CSNNs), which learn representations in an unsupervised and Backpropagation-free manner. Our approach replaces the learning of traditional convolutional layers from CNNs with the competitive learning procedure of SOMs and simultaneously learns local masks between those layers with separate Hebbian-like learning rules to overcome the problem of disentangling factors of variation when filters are learned through clustering. We investigate the learned representation by designing two simple models with our building blocks, achieving comparable performance to many methods which use Backpropagation, while we reach comparable performance on Cifar10 and give baseline performances on Cifar100, Tiny ImageNet and a small subset of ImageNet for Backpropagation-free methods.",0
"In recent years, deep learning has revolutionized many areas of artificial intelligence (AI), including image and speech recognition, natural language processing, robotics, and computer vision. One key component of deep learning that has contributed significantly to these advancements is convolutional neural networks (CNNs). CNNs are powerful models that can learn complex features from raw data, such as images or audio signals, by stacking multiple layers of neurons that process and transform each signal at different levels of abstraction. However, training CNNs remains computationally expensive and time consuming, especially on large datasets. To address this challenge, we propose unsupervised pretraining techniques using unsupervised backpropagation free algorithms to train our CSNN model efficiently without losing accuracy. Our results show that our approach outperforms traditional supervised methods, producing state-of-the-art results in several benchmark tasks. These results demonstrate the potential benefits of combining unsupervised learning with advanced architectures like convolutional neural networks. By doing so, researchers can improve performance while reducing computational costs, paving the way for more efficient applications of AI in various domains.",1
"Network representation learning (NRL) is a powerful technique for learning low-dimensional vector representation of high-dimensional and sparse graphs. Most studies explore the structure and metadata associated with the graph using random walks and employ an unsupervised or semi-supervised learning schemes. Learning in these methods is context-free, because only a single representation per node is learned. Recently studies have argued on the sufficiency of a single representation and proposed a context-sensitive approach that proved to be highly effective in applications such as link prediction and ranking.   However, most of these methods rely on additional textual features that require RNNs or CNNs to capture high-level features or rely on a community detection algorithm to identify multiple contexts of a node.   In this study, without requiring additional features nor a community detection algorithm, we propose a novel context-sensitive algorithm called GAP that learns to attend on different parts of a node's neighborhood using attentive pooling networks. We show the efficacy of GAP using three real-world datasets on link prediction and node clustering tasks and compare it against 10 popular and state-of-the-art (SOTA) baselines. GAP consistently outperforms them and achieves up to ~9% and ~20% gain over the best performing methods on link prediction and clustering tasks, respectively.",0
"This paper presents a new method for graph neighborhood attentive pooling, which can effectively aggregate information from neighbors of each node in a graph. Our approach uses attention mechanisms to weight the contributions of different neighbor nodes, allowing us to focus on more important neighbors while ignoring less relevant ones. We evaluate our method on several benchmark datasets and show that it outperforms state-of-the-art approaches in many cases. Furthermore, we demonstrate the effectiveness of our method in various applications such as social recommendation, bioinformatics, and natural language processing. Overall, our work advances the field of graph neural networks by improving the accuracy and interpretability of these models.",1
"The use of drug combinations often leads to polypharmacy side effects (POSE). A recent method formulates POSE prediction as a link prediction problem on a graph of drugs and proteins, and solves it with Graph Convolutional Networks (GCNs). However, due to the complex relationships in POSE, this method has high computational cost and memory demand. This paper proposes a flexible Tri-graph Information Propagation (TIP) model that operates on three subgraphs to learn representations progressively by propagation from protein-protein graph to drug-drug graph via protein-drug graph. Experiments show that TIP improves accuracy by 7%+, time efficiency by 83$\times$, and space efficiency by 3$\times$.",0
"Here is one possible abstract:  Polypharmacy, the concurrent use of multiple medications by a patient, has become increasingly common as the population ages and comorbidities rise. However, polypharmacy also increases the risk of side effects, which can negatively impact patients’ health outcomes. In order to better predict these side effects, we need methods that can capture complex interactions among drugs and their metabolites. To address this challenge, we propose a new method called tri-graph information propagation (TIP), which uses graph theory to model drug-drug interactions (DDIs) based on chemical structures and physicochemical properties of drug molecules.  We first constructed a large-scale DDI network using data from public databases and validated our results against existing benchmarks. We then developed a random walk algorithm on the DDI network to quantify the strengths of interactions between pairs of drugs, allowing us to estimate potential side effects arising from combinations of three or more drugs. Our approach incorporates both topological features of the DDI network and characteristics of individual drugs to improve prediction accuracy compared to traditional approaches. Finally, we demonstrated the effectiveness of TIP through comprehensive experiments and case studies, including comparison with state-of-the-art methods and clinical validation against drug labels.  Our work provides a valuable resource for identifying risky combinations of drugs, enabling healthcare providers to make informed decisions about prescribing and monitoring treatments that involve multiple medications. By improving polypharmacy side effect prediction, we aim to promote safer and more effective treatment practices and ultimately enhance patient safety and wellbeing.",1
"We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets.",0
"We propose gradient-isolated learning of representations (GILR), a novel architecture that eliminates the need for end-to-end training while still allowing deep neural networks to learn meaningful features from raw data. Our approach introduces auxiliary loss terms at intermediate layers which directly optimize their parameters without requiring backpropagation through them. GILR enables efficient parallelization and reduces computational costs by over two orders of magnitude compared to traditional techniques. Experiments on benchmark datasets demonstrate comparable performance on classification tasks while offering significant speed ups. Furthermore, we present ablation studies showing the importance of each component of our method. Our results suggest that GILR opens new possibilities towards fast, accurate, and model-agnostic representation learning.  This work explores alternative methods to end-to-end optimization commonly used in deep learning architectures to train neural networks. We introduce a novel framework called gradient-isolated learning of representations (GILR) that addresses key issues related to computation efficiency and scalability. By decoupling losses and gradients across different network levels, our technique allows direct optimization of intermediate representations without relying on costly backward propagation mechanisms. Our experiments highlight the effectiveness of our approach on several standard benchmarks such as MNIST, CIFAR-10, SVHN, and ImageNet. In particular, we show consistent improvement over competing methods while significantly reducing overall training times up to threefold. We believe that our findings have far-reaching implications beyond computer vision applications, enabling faster adoption and deployment of deep learning models for numerous real-world scenarios where time efficiency may be critical, such as autonomous systems, robotics, natural language processing, etc. Therefore, GILR represents a fundamental step forward towards achieving more practical use cases for artificial intelligence technology.",1
"""Deep Archetypal Analysis"" generates latent representations of high-dimensional datasets in terms of fractions of intuitively understandable basic entities called archetypes. The proposed method is an extension of linear ""Archetypal Analysis"" (AA), an unsupervised method to represent multivariate data points as sparse convex combinations of extremal elements of the dataset. Unlike the original formulation of AA, ""Deep AA"" can also handle side information and provides the ability for data-driven representation learning which reduces the dependence on expert knowledge. Our method is motivated by studies of evolutionary trade-offs in biology where archetypes are species highly adapted to a single task. Along these lines, we demonstrate that ""Deep AA"" also lends itself to the supervised exploration of chemical space, marking a distinct starting point for de novo molecular design. In the unsupervised setting we show how ""Deep AA"" is used on CelebA to identify archetypal faces. These can then be superimposed in order to generate new faces which inherit dominant traits of the archetypes they are based on.",0
"Title: ""Exploring the Depths of Human Experience through Archetypes""  Deep archetypal analysis offers a unique perspective on human experience by delving into the collective unconscious to identify underlying patterns that shape our behavior, thoughts, and emotions. Drawing from Carl Jung's concept of archetypes, this approach enables us to better understand ourselves and the world around us by exploring universal themes such as good versus evil, innocence vs. experience, masculinity/femininity, etc., which are represented symbolically across cultures throughout history. By recognizing these deep psychic structures within ourselves and others, we can gain insight into personal growth, relationships, and societal issues like never before. Ultimately, this study seeks to illuminate how understanding archetypes empowers individuals and society to navigate life more effectively and meaningfully.",1
"Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.",0
"Title: ""Maximizing Mutual Information for Efficient Representation Learning""  Abstract: Deep learning has revolutionized the field of artificial intelligence by enabling powerful representations that can learn complex patterns from large amounts of data. However, one major challenge faced by deep learning models is their lack of interpretability, making it difficult for humans to comprehend how they make decisions. In recent years, researchers have proposed methods that maximize mutual information (MI) as a means to address this issue while simultaneously improving performance on tasks. This work explores the use of MI maximization as a regularizer during representation learning to improve model performance and interpretability. We evaluate our method across multiple benchmark datasets and demonstrate that it achieves state-of-the-art results while providing more meaningful representations. Our findings suggest that incorporating MI maximization into representation learning may serve as a promising direction for improving human understanding of machine decision processes.",1
"Autoencoder-based learning has emerged as a staple for disciplining representations in unsupervised and semi-supervised settings. This paper analyzes a framework for improving generalization in a purely supervised setting, where the target space is high-dimensional. We motivate and formalize the general framework of target-embedding autoencoders (TEA) for supervised prediction, learning intermediate latent representations jointly optimized to be both predictable from features as well as predictive of targets---encoding the prior that variations in targets are driven by a compact set of underlying factors. As our theoretical contribution, we provide a guarantee of generalization for linear TEAs by demonstrating uniform stability, interpreting the benefit of the auxiliary reconstruction task as a form of regularization. As our empirical contribution, we extend validation of this approach beyond existing static classification applications to multivariate sequence forecasting, verifying their advantage on both linear and nonlinear recurrent architectures---thereby underscoring the further generality of this framework beyond feedforward instantiations.",0
"Abstract:  Supervised representation learning has emerged as one of the most promising approaches to unsupervised feature extraction from large datasets. In recent years, autoencoder models have proven particularly effective at capturing important underlying features that can then be used for downstream tasks such as classification or regression. However, traditional autoencoders often suffer from poor generalization performance due to their reliance on reconstruction loss alone, which can lead to overfitting and suboptimal representations. To address these limitations, we propose target-embedding autoencoders (TEA), a novel framework designed to learn high-quality latent representations through joint optimization of both reconstruction loss and classifier objectives. Our method utilizes task-specific target embeddings to regularize the encoding process and provide supervision during training, allowing for better discrimination and more powerful feature extractors. We evaluate our approach against several state-of-the-art methods on benchmark datasets across a range of modalities including image, text, speech and bioinformatics data, demonstrating significant improvements in accuracy and robustness. These results highlight the promise of TEA models as a valuable tool for unsupervised feature learning and representation discovery, holding great potential for advancing machine learning research in a variety of domains.",1
We propose a representation learning framework for medical diagnosis domain. It is based on heterogeneous network-based model of diagnostic data as well as modified metapath2vec algorithm for learning latent node representation. We compare the proposed algorithm with other representation learning methods in two practical case studies: symptom/disease classification and disease prediction. We observe a significant performance boost in these task resulting from learning representations of domain data in a form of heterogeneous network.,0
"In recent years, medical data has become increasingly important due to advances in digitalization, leading to large amounts of complex and high dimensional datasets that require efficient representation learning methods. This study proposes novel techniques for representing medical images and time series data, addressing challenges such as nonlinearity and variability across different domains. Our approach uses deep neural networks to learn latent representations from raw clinical signals, which can then be used for downstream tasks such as classification and prediction. We evaluate our method on multiple real world datasets including MRI brain scans and electrocardiograms (ECGs), showing significant improvements over state of the art approaches. Overall, these findings have implications for improving diagnostic accuracy and patient outcomes through better use of medical data.",1
"Uncertainty quantification for deep learning is a challenging open problem. Bayesian statistics offer a mathematically grounded framework to reason about uncertainties; however, approximate posteriors for modern neural networks still require prohibitive computational costs. We propose a family of algorithms which split the classification task into two stages: representation learning and uncertainty estimation. We compare four specific instances, where uncertainty estimation is performed via either an ensemble of Stochastic Gradient Descent or Stochastic Gradient Langevin Dynamics snapshots, an ensemble of bootstrapped logistic regressions, or via a number of Monte Carlo Dropout passes. We evaluate their performance in terms of \emph{selective} classification (risk-coverage), and their ability to detect out-of-distribution samples. Our experiments suggest there is limited value in adding multiple uncertainty layers to deep classifiers, and we observe that these simple methods strongly outperform a vanilla point-estimate SGD in some complex benchmarks like ImageNet.",0
"In recent years there has been significant progress in our understanding of deep learning models as well as their applications. However, despite these advancements, existing state-of-the-art methods still struggle with providing accurate uncertainty estimates on their predictions. This paper presents a novel approach that tackles the problem by explicitly decoupling representation learning from uncertainty estimation into two separate steps, leading to better calibrated confidence intervals without sacrificing accuracy. Our method applies the technique of adversarial training to generate pseudo-ensembles of predictors at inference time. We demonstrate the effectiveness of our proposed last-layer algorithm through extensive experiments across various benchmark datasets, showing improved performance compared to existing baseline models. Overall, our findings suggest that addressing the tradeoff between uncertainty estimation and model fidelity can lead to more reliable decision making under uncertainty scenarios.",1
"Existing graph neural networks may suffer from the ""suspended animation problem"" when the model architecture goes deep. Meanwhile, for some graph learning scenarios, e.g., nodes with text/image attributes or graphs with long-distance node correlations, deep graph neural networks will be necessary for effective graph representation learning. In this paper, we propose a new graph neural network, namely DIFNET (Graph Diffusive Neural Network), for graph representation learning and node classification. DIFNET utilizes both neural gates and graph residual learning for node hidden state modeling, and includes an attention mechanism for node neighborhood information diffusion. Extensive experiments will be done in this paper to compare DIFNET against several state-of-the-art graph neural network models. The experimental results can illustrate both the learning performance advantages and effectiveness of DIFNET, especially in addressing the ""suspended animation problem"".",0
"Title: Solving the Suspended Animation Problem with Deep Diffusive Neural Networks on Graph Semi-supervised Classification  Abstract: The suspended animation problem (SAP) refers to instances where deep neural networks struggle to provide accurate predictions due to issues such as data sparsity, limited annotations, or domain shift. This can lead to poor performance on downstream tasks that rely on these predictions. To address this challenge, we propose a novel approach using deep diffusive neural network (DDNN) architectures trained on graph semi-supervised classification (SSC). Our method leverages the strengths of both diffusion models and deep learning techniques to improve accuracy and robustness in the presence of SAP scenarios. By adopting a SSC framework, our model utilizes unlabeled data along with limited labeled examples during training, enabling better generalization across different domains. We evaluate our approach on several benchmark datasets facing typical forms of SAP, demonstrating significant improvements over state-of-the-art methods for each case study. These results highlight the effectiveness of our proposed solution towards solving real-world problems affected by the suspended animation issue.",1
"How can we enable machines to make sense of the world, and become better at learning? To approach this goal, I believe viewing intelligence in terms of many integral aspects, and also a universal two-term tradeoff between task performance and complexity, provides two feasible perspectives. In this thesis, I address several key questions in some aspects of intelligence, and study the phase transitions in the two-term tradeoff, using strategies and tools from physics and information. Firstly, how can we make the learning models more flexible and efficient, so that agents can learn quickly with fewer examples? Inspired by how physicists model the world, we introduce a paradigm and an AI Physicist agent for simultaneously learning many small specialized models (theories) and the domain they are accurate, which can then be simplified, unified and stored, facilitating few-shot learning in a continual way. Secondly, for representation learning, when can we learn a good representation, and how does learning depend on the structure of the dataset? We approach this question by studying phase transitions when tuning the tradeoff hyperparameter. In the information bottleneck, we theoretically show that these phase transitions are predictable and reveal structure in the relationships between the data, the model, the learned representation and the loss landscape. Thirdly, how can agents discover causality from observations? We address part of this question by introducing an algorithm that combines prediction and minimizing information from the input, for exploratory causal discovery from observational time series. Fourthly, to make models more robust to label noise, we introduce Rank Pruning, a robust algorithm for classification with noisy labels. I believe that building on the work of my thesis we will be one step closer to enable more intelligent machines that can make sense of the world.",0
"Machine Learning (ML) uses data analysis techniques that leverage mathematical models, optimization algorithms and statistics to identify patterns from large datasets and solve complex problems. Despite recent advances in ML research and applications, there remain significant challenges due to a fundamental conflict between model complexity/accuracy on one hand, and interpretability/generalization performance on the other. This tension arises from difficulties associated with selecting relevant features from high dimensional data while avoiding overfitting which can result in poor generalization results. In addition, there exists another layer of difficulty: how to balance model fit versus physical consistency? In the context of image processing problems, images must exhibit meaningful visual characteristics like edges and textures so as to be interpretable by humans and machines alike. However such structure may not be present in all possible solution classes (e.g., natural scenes). How should we then compare ML solutions across competing domains to ensure fairness if their respective underlying constraints are vastly different? To address these questions, we propose three main contributions. Firstly, we design novel architectures using stateof-the-art techniques such as attention mechanisms and transfer learning to improve the robustness and effectiveness of our models to deal with both small and large training sets. Secondly, we study conditions under which it becomes impossible to train any model to achieve low validation loss despite large amounts of available labeled data, providing insights into why some systems work well in practice but not in others. Finally, building upon these insights, we design automated tools to systematically guide practitioners toward more accurate solutions without explicitly specifying their own formulation, making it easier for users to apply powerful deep learning methods even with modest expertise or computational resources",1
"The problem of finding the sparsest vector (direction) in a low dimensional subspace can be considered as a homogeneous variant of the sparse recovery problem, which finds applications in robust subspace recovery, dictionary learning, sparse blind deconvolution, and many other problems in signal processing and machine learning. However, in contrast to the classical sparse recovery problem, the most natural formulation for finding the sparsest vector in a subspace is usually nonconvex. In this paper, we overview recent advances on global nonconvex optimization theory for solving this problem, ranging from geometric analysis of its optimization landscapes, to efficient optimization algorithms for solving the associated nonconvex optimization problem, to applications in machine intelligence, representation learning, and imaging sciences. Finally, we conclude this review by pointing out several interesting open problems for future research.",0
"Abstract: This work presents a comprehensive study on finding sparse vectors within a subspace under different conditions. We discuss the theoretical framework behind the problem and provide various algorithms that can effectively solve it. Our approach considers multiple scenarios such as linear independence constraints, mixed norm minimization, and unitarily invariant metrics. Additionally, we demonstrate how our methods can be applied to real world applications like compressed sensing and machine learning. Our results show significant improvements compared to previous state-of-the-art techniques while maintaining computational efficiency. Overall, this research contributes valuable insights into understanding sparsity patterns and efficient ways to locate them.  Keywords: Compressive sampling; Lp-norm minimization; Unitarily invariant metrics; Linear dependence constraints  Note: I did not use any fillers (This/That) instead started directly with first sentence. Plz tell me if u need more than one version! Also feel free to request for similar content on any topic - I am your assistant after all :D",1
"Effective modeling of electronic health records (EHR) is rapidly becoming an important topic in both academia and industry. A recent study showed that using the graphical structure underlying EHR data (e.g. relationship between diagnoses and treatments) improves the performance of prediction tasks such as heart failure prediction. However, EHR data do not always contain complete structure information. Moreover, when it comes to claims data, structure information is completely unavailable to begin with. Under such circumstances, can we still do better than just treating EHR data as a flat-structured bag-of-features? In this paper, we study the possibility of jointly learning the hidden structure of EHR while performing supervised prediction tasks on EHR data. Specifically, we discuss that Transformer is a suitable basis model to learn the hidden EHR structure, and propose Graph Convolutional Transformer, which uses data statistics to guide the structure learning process. The proposed model consistently outperformed previous approaches empirically, on both synthetic data and publicly available EHR data, for various prediction tasks such as graph reconstruction and readmission prediction, indicating that it can serve as an effective general-purpose representation learning algorithm for EHR data.",0
"In recent years, electronic health records (EHRs) have become increasingly important as a source of medical data for research purposes. However, accessing and analyzing EHRs remains challenging due to their complex structure and large volume. To tackle these issues, we propose using graph convolutional transformers (GCTs), which can learn structured representations of heterogeneous graphs such as EHRs. Our approach involves training a GCT model on an EHR dataset and then evaluating its performance on downstream tasks such as patient diagnosis prediction. Through our experiments, we demonstrate that our method outperforms state-of-the-art baseline models in terms of both accuracy and interpretability. Furthermore, by visualizing learned embeddings, we show that GCTs capture relevant relationships between patients and clinical variables, providing insights into how different factors impact the likelihood of specific diagnoses. Overall, our work represents a significant contribution to the field of EHR analysis, laying the foundation for more effective use of EHRs in biomedical research.",1
"This paper proposes a novel deep subspace clustering approach which uses convolutional autoencoders to transform input images into new representations lying on a union of linear subspaces. The first contribution of our work is to insert multiple fully-connected linear layers between the encoder layers and their corresponding decoder layers to promote learning more favorable representations for subspace clustering. These connection layers facilitate the feature learning procedure by combining low-level and high-level information for generating multiple sets of self-expressive and informative representations at different levels of the encoder. Moreover, we introduce a novel loss minimization problem which leverages an initial clustering of the samples to effectively fuse the multi-level representations and recover the underlying subspaces more accurately. The loss function is then minimized through an iterative scheme which alternatively updates the network parameters and produces new clusterings of the samples. Experiments on four real-world datasets demonstrate that our approach exhibits superior performance compared to the state-of-the-art methods on most of the subspace clustering problems.",0
"This paper presents a novel approach to deep subspace clustering using multi-level representation learning. We propose a framework that simultaneously learns multiple levels of representations, capturing both local and global features of the data, while enforcing cluster structures at different levels of abstraction. Our method integrates recent advances in deep learning and unsupervised feature learning, enabling the discovery of nonlinear and hierarchical relationships among data points. Experiments on several benchmark datasets show that our algorithm significantly outperforms state-of-the-art methods in terms of accuracy and robustness. Furthermore, we provide comprehensive analysis and ablation studies that demonstrate the effectiveness of each component in our proposed framework. Overall, this work represents an important contribution towards automating clustering tasks by leveraging advanced machine learning techniques.",1
"Few-shot learning algorithms aim to learn model parameters capable of adapting to unseen classes with the help of only a few labeled examples. A recent regularization technique - Manifold Mixup focuses on learning a general-purpose representation, robust to small changes in the data distribution. Since the goal of few-shot learning is closely linked to robust representation learning, we study Manifold Mixup in this problem setting. Self-supervised learning is another technique that learns semantically meaningful features, using only the inherent structure of the data. This work investigates the role of learning relevant feature manifold for few-shot tasks using self-supervision and regularization techniques. We observe that regularizing the feature manifold, enriched via self-supervised techniques, with Manifold Mixup significantly improves few-shot learning performance. We show that our proposed method S2M2 beats the current state-of-the-art accuracy on standard few-shot learning datasets like CIFAR-FS, CUB, mini-ImageNet and tiered-ImageNet by 3-8 %. Through extensive experimentation, we show that the features learned using our approach generalize to complex few-shot evaluation tasks, cross-domain scenarios and are robust against slight changes to data distribution.",0
"Abstract Despite their impressive performance on few-shot tasks, models trained through meta learning often lack generalization ability, particularly when dealing with rare classes or outlier examples. Motivated by these observations, we propose applying data augmentation techniques during inference that specifically target these underrepresented regions of the input space. Our method takes inspiration from recent work in semi supervised learning called manifold mixup. We evaluate our technique across several benchmark datasets commonly used to evaluate few shot learners and find consistent improvements over standard methods. Additionally, we provide qualitative results demonstrating the utility of this new class of techniques.",1
"This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we proposed a novel method called InfoGraph for learning graph-level representations. We maximize the mutual information between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles). By doing so, the graph-level representations encode aspects of the data that are shared across different scales of substructures. Furthermore, we further propose InfoGraph*, an extension of InfoGraph for semi-supervised scenarios. InfoGraph* maximizes the mutual information between unsupervised graph representations learned by InfoGraph and the representations learned by existing supervised methods. As a result, the supervised encoder learns from unlabeled data while preserving the latent semantic space favored by the current supervised task. Experimental results on the tasks of graph classification and molecular property prediction show that InfoGraph is superior to state-of-the-art baselines and InfoGraph* can achieve performance competitive with state-of-the-art semi-supervised models.",0
"In recent years, graph representation learning has emerged as a popular technique for analyzing graphs. However, current methods are limited by their reliance on either fully supervised training or assumptions about the underlying data distribution. To address these limitations, we propose a novel approach based on mutual information maximization for unsupervised and semi-supervised graph-level representation learning. Our method effectively captures both local and global features in the graph while requiring minimal human intervention. We evaluate our approach using several benchmark datasets and demonstrate that it outperforms existing state-of-the-art techniques across multiple evaluation metrics. Our work represents a significant advance in the field of graph representation learning and paves the way for future research in this area.",1
"We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators and can discover interpretable representations from video in a fully unsupervised manner.",0
"Title: Unifying Deep Learning with Spectral Graph Analysis Techniques Abstract Recent advances in deep learning have revolutionized many fields including computer vision, natural language processing, robotics, etc., achieving state-of-the-art performance on a wide range of tasks. Despite these successes, classical deep learning models lack interpretability, leading researchers to seek new ways of understanding and reasoning about neural networks. One promising direction lies at the intersection of graph signal processing (GSP) and deep learning. By exploiting the underlying graphs that represent the data we can improve our understanding, inference, and even performance by modeling spacial dependencies in signals and images. In this work, we present a novel framework that combines spectral techniques with feedforward neural architectures that enables the efficient capture of complex spatial relationships across different scales. Our proposed architecture effectively unites the strengths of both worlds by utilizing the power of GSP and graph filtering methods to learn better representations. We evaluate our framework across several benchmark datasets, demonstrating significant improvements over baseline methods in terms of accuracy while providing interpretable results through the use of graph properties like smoothness, connected components, conductance, effective resistance, etc. This work contributes towards developing more interpretable deep learning models which holds great promise to advance applications requiring explainability as well as high predictive accuracy.",1
"For subspace recovery, most existing low-rank representation (LRR) models performs in the original space in single-layer mode. As such, the deep hierarchical information cannot be learned, which may result in inaccurate recoveries for complex real data. In this paper, we explore the deep multi-subspace recovery problem by designing a multilayer architecture for latent LRR. Technically, we propose a new Multilayer Collabora-tive Low-Rank Representation Network model termed DeepLRR to discover deep features and deep subspaces. In each layer (2), DeepLRR bilinearly reconstructs the data matrix by the collabo-rative representation with low-rank coefficients and projection matrices in the previous layer. The bilinear low-rank reconstruc-tion of previous layer is directly fed into the next layer as the input and low-rank dictionary for representation learning, and is further decomposed into a deep principal feature part, a deep salient feature part and a deep sparse error. As such, the coher-ence issue can be also resolved due to the low-rank dictionary, and the robustness against noise can also be enhanced in the feature subspace. To recover the sparse errors in layers accurately, a dynamic growing strategy is used, as the noise level will be-come smaller for the increase of layers. Besides, a neighborhood reconstruction error is also included to encode the locality of deep salient features by deep coefficients adaptively in each layer. Extensive results on public databases show that our DeepLRR outperforms other related models for subspace discovery and clustering.",0
"This paper presents a novel approach to robust deep subspace discovery using collaborative low-rank coding networks. We introduce a multilayer architecture that combines low-rank representation learning with a collaborative filtering mechanism to improve performance on real-world datasets. Our method achieves state-of-the-art results in a variety of tasks and outperforms existing approaches by significant margins. Through comprehensive experiments, we demonstrate the effectiveness and scalability of our framework in handling large-scale problems while maintaining efficiency. Overall, our work advances the field of deep learning and has promising applications in many areas such as computer vision and natural language processing.",1
"Both the Dictionary Learning (DL) and Convolutional Neural Networks (CNN) are powerful image representation learning systems based on different mechanisms and principles, however whether we can seamlessly integrate them to improve the per-formance is noteworthy exploring. To address this issue, we propose a novel generalized end-to-end representation learning architecture, dubbed Convolutional Dictionary Pair Learning Network (CDPL-Net) in this paper, which integrates the learning schemes of the CNN and dictionary pair learning into a unified framework. Generally, the architecture of CDPL-Net includes two convolutional/pooling layers and two dictionary pair learn-ing (DPL) layers in the representation learning module. Besides, it uses two fully-connected layers as the multi-layer perception layer in the nonlinear classification module. In particular, the DPL layer can jointly formulate the discriminative synthesis and analysis representations driven by minimizing the batch based reconstruction error over the flatted feature maps from the convolution/pooling layer. Moreover, DPL layer uses l1-norm on the analysis dictionary so that sparse representation can be delivered, and the embedding process will also be robust to noise. To speed up the training process of DPL layer, the efficient stochastic gradient descent is used. Extensive simulations on real databases show that our CDPL-Net can deliver enhanced performance over other state-of-the-art methods.",0
"This work introduces the CNN-DPLV (Convolutional Dictionary Pair Learning for Visual Representations) network, which combines convolutional neural networks (CNNs), dictionary learning, and pairwise ranking loss functions to learn effective image representations from data. We show that CNN-DPLV outperforms other state-of-the-art methods on benchmark datasets for tasks such as object recognition and retrieval, demonstrating the effectiveness of our approach. Our contributions include: 1) introducing a novel method for training deep learned dictionaries by exploiting sparse coding regularization; 2) extending classical sparse representation algorithms for natural images using large-scale annotated datasets; 3) introducing a new pairwise ranking loss function based on localized distance maps that captures high-level semantic similarities; and 4) achieving state-of-the-art performance for several important visual representation learning tasks. Overall, we believe that CNN-DPLV provides a powerful framework for unsupervised feature learning from visual data.",1
"A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.",0
"Incorporate any relevant keywords from the title into the text without repeating them. Disentangling the underlying latent structure of complex data distributions is a fundamental task across many fields, ranging from neuroscience to deep learning research. However, traditional linear Independent Component Analysis (ICA) methods struggle to disentangle high-dimensional, nonlinearly mixing data sets such as those generated by natural phenomena governed by fluid mechanics principles, ecological systems, or brain imaging technologies like functional Magnetic Resonance Imaging (fMRI) or magnetoencephalography (MEG). We propose a novel framework based on general incompressible flow networks that extends classical ICA theory beyond the Gaussianity assumption and enables efficient optimization using neural network architectures designed for image generation tasks. Our approach leverages recent advances in invertible neural networks and improves interpretability compared to previous work on unsupervised generative models built on top of these frameworks while achieving state-of-the-art performance in several benchmark datasets drawn from different domains. This comprehensive study assesses the strengths and limitations of our methodology by demonstrating how its flexibility can cope with varying degrees of nonseparability, noise, and other sources of complexity inherent to real-world signals. As a result, we believe this new family of algorithms—which we call GIN—will find applications in data analysis workflows seeking to distill simpler representations capturing most essential features from high-dimensional raw measurements or simulations.  *Keywords: Nonlinear ICA, Latent Structure, Fluid Mechanics Principles, Ecological Systems, Brain Imaging Technologies, Functional Magnetic Resonance Imaging, MEG, Image Generation Tasks, Neural Network Architectures, Invertible Neural Networks, Data Analysis Workflows.*",1
"In this paper we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",0
"This paper explores the use of dynamics-aware embeddings in natural language processing (NLP) tasks. Dynamic embeddings take into account changes over time in the meaning of words and phrases, which allows them to better capture contextual meanings that static embeddings miss. We propose a novel approach to generating dynamically updated word vectors using a combination of temporal and semantic features, including part of speech tags, named entities, and contextual dependencies. Our method outperforms state-of-the-art models on two benchmark datasets, demonstrating the effectiveness of our proposed approach. In addition, we provide qualitative analysis of the generated dynamic embeddings, showing how they can provide deeper insights into text data. Overall, our results suggest that incorporating dynamics into embedding models has the potential to significantly improve NLP performance across a range of applications.",1
"Audio-visual learning, aimed at exploiting the relationship between audio and visual modalities, has drawn considerable attention since deep learning started to be used successfully. Researchers tend to leverage these two modalities either to improve the performance of previously considered single-modality tasks or to address new challenging problems. In this paper, we provide a comprehensive survey of recent audio-visual learning development. We divide the current audio-visual learning tasks into four different subfields: audio-visual separation and localization, audio-visual correspondence learning, audio-visual generation, and audio-visual representation learning. State-of-the-art methods as well as the remaining challenges of each subfield are further discussed. Finally, we summarize the commonly used datasets and performance metrics.",0
"This survey aims to provide an overview of recent developments on deep audio visual learning by analyzing works published since January 2016. Our investigation focuses on multi modal data analysis using auditory and visual stimuli, including methods that involve machine listening, computer vision, natural language processing (NLP), neural networks, generative models, transfer learning, feature extraction, speech recognition, object detection, action recognition, activity classification, image generation, music transcription, singing voice synthesis and other relevant techniques. We present several successful applications, identify challenges associated with deep audio visual learning, report the most common datasets used and highlight future research directions. This survey provides an extensive review of current deep audio visual learning approaches, covering literature from the last three years. Our discussion encompasses multimodal studies involving sound and sight, such as those leveraging artificial intelligence technology like machine perception, NLP, neural networks, and so forth. Key topics discussed within include various tasks like music/speech recognition, motion capture, etc., along with popular datasets employed during experimentation and potential future areas for exploration. Overall, our study serves as an up-to-date resource summarizing existing knowledge and pointing towards promising new frontiers in this domain.",1
"Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.",0
"Researchers at Disney have developed a deep learning algorithm that can generate high-fidelity images from textual descriptions. The method uses a latent space to encode complex data into simpler representations that allow for more efficient training. This disentanglement approach allows the model to generate images with finer details and greater visual fidelity compared to previous models. In addition, the proposed architecture incorporates attention mechanisms which enable fine-grained control over different aspects of the generated image such as color, texture, and shape. The algorithm was tested on several datasets including faces, cars, and environments achieving state-of-the-art results in terms of both quality and diversity of generated images. The ability to synthesize high-quality images from natural language text has numerous potential applications ranging from content creation to computer vision tasks. Overall, this work represents a significant advance in the field of generative adversarial networks (GANs) and highlights the importance of representation design in deep learning. This paper presents a novel approach for generating high-fidelity images using deep learning techniques. Developed by researchers at Disney, the method utilizes a disentangled representation latent space to simplify complex data and improve efficiency during training. By leveraging attention mechanisms, the algorithm offers precise control over various elements of the outputted images such as colors, textures, and shapes. Results demonstrate exceptional performance across multiple benchmark databases for diverse image types including faces, vehicles, and surroundings. With vast implications for fields like content generation and computer vision, these findings mark a major step forward in the development of generative adversarial networks (GANs) while emphasizing the significance of representation designs in deep learning. ---",1
"Self-supervised (SS) learning is a powerful approach for representation learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS tasks were proposed to address the catastrophic forgetting issue in the GAN discriminator. In this work, we perform an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, we identify issues of SS tasks which allow a severely mode-collapsed generator to excel the SS tasks. To address the issues, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples. We provide both theoretical and empirical analysis to support that our proposed SS tasks have better convergence property. We conduct experiments to incorporate our proposed SS tasks into two different GAN baseline models. Our approach establishes state-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet $32\times32$ and Stacked-MNIST datasets, outperforming existing works by considerable margins in some cases. Our unconditional GAN model approaches performance of conditional GAN without using labeled data. Our code: https://github.com/tntrung/msgan",0
"Artificial intelligence has come a long way over recent years, but one key limitation remains - without extensive human supervision, machine learning algorithms struggle to produce high quality outputs on their own. Recent developments aimed at addressing these shortcomings have focused on self-supervised methods that leverage large amounts of unlabeled data. One promising approach in this area is Generative Adversarial Networks (GAN), which consist of two neural networks trained jointly, one generating new samples and the other distinguishing real from generated samples. However, even state-of-the-art self-supervised GAN models still suffer from several fundamental limitations including instability during training, poor sample diversity, mode collapse, and sensitivity to hyperparameters. In order to overcome these difficulties, we propose a novel framework based on multi-class minimax game formulations that significantly improves performance across various metrics. Our method leads to more stable, diverse, and visually coherent results as well as better robustness to changes in network architectures and hyperparameter settings. We validate our approach by evaluating both quantitative and qualitative measures and demonstrate superior outcomes compared to previous approaches on benchmark datasets for image generation tasks. Overall, our work represents an important step towards bridging the gap between fully supervised and self-supervised paradigms in computer vision and offers valuable insights into the potential applications of GANs for artificial intelligence systems in general.",1
"A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.",0
"Many visual reasoning problems involve understanding how objects or concepts relate to one another through common sense knowledge or causality. For example, knowing that pouring water into a glass will cause it to fill up can allow us to predict the final state of the system, while recognizing that two individuals are married requires understanding their relationship status. These types of relationships can often be complex and difficult for models to learn, but disentangling them from other features could potentially make learning more efficient. To investigate whether disentanglement helps with learning these relationships, we trained three different kinds of object representations on five datasets with varying levels of complexity: supervised pretraining followed by self-supervised fine-tuning; purely self-supervised training without any form of label data; and model architecture searches using REINFORCE. Our results indicate that, overall, having some level of explicit regularization towards disentanglement improves performance significantly over simpler methods like image classification, especially as tasks become harder. While there may still be limitations to this type of model such as sensitivity to hyperparameters, our work provides evidence that disentanglement can indeed lead to better generalizability across multiple domains.",1
"Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works. As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000 experiments in a controlled and uniform framework to re-evaluate five popular models across nine common benchmarks. Moreover, by comparing GNNs with structure-agnostic baselines we provide convincing evidence that, on some datasets, structural information has not been exploited yet. We believe that this work can contribute to the development of the graph learning field, by providing a much needed grounding for rigorous evaluations of graph classification models.",0
"In recent years, graph neural networks (GNNs) have emerged as a powerful class of models for tackling graph data. GNNs can automatically learn node representations by aggregating features from neighbors using attention mechanisms, which make them well suited for several real world applications such as recommender systems and bioinformatics. However, there exists little consensus on how different hyperparameters affect their performance; previous studies mostly focused only on one specific choice of hyperparameter settings. We seek to address this limitation by systematically evaluating the impact of key hyperparameters on popular variants of GNNs. To achieve that goal, we study five architectures and three commonly used loss functions. Moreover, our approach focuses on investigating whether graph neural network architecture alone has an appreciable effect on graph classification or if other considerations drive model success. Our experiments indicate that in many cases random graphs perform comparably to those constructed using advanced node feature engineering techniques. Therefore, a clear conclusion that arises from these results is that further research should explore methods to automate node feature extraction instead of relying exclusively on manually engineered features. Additionally, the proposed methodology provides a template for conducting fair comparisons among machine learning algorithms, making it more efficient for practitioners to determine an appropriate algorithm for any given task. Overall, our work contributes new insights into how graph neural network hyperparameters influence graph classification accuracy and sets forth guidelines for designing future comparison studies in machine learning research.",1
"Variational autoencoders (VAEs) have ushered in a new era of unsupervised learning methods for complex distributions. Although these techniques are elegant in their approach, they are typically not useful for representation learning. In this work, we propose a simple yet powerful class of VAEs that simultaneously result in meaningful learned representations. Our solution is to combine traditional VAEs with mutual information maximization, with the goal to enhance amortized inference in VAEs using Information Theoretic techniques. We call this approach InfoMax-VAE, and such an approach can significantly boost the quality of learned high-level representations. We realize this through the explicit maximization of information measures associated with the representation. Using extensive experiments on varied datasets and setups, we show that InfoMax-VAE outperforms contemporary popular approaches, including Info-VAE and $\beta$-VAE.",0
"This work proposes using mutual information (MI) as a training criterion for variational autoencoders (VAEs), which can help learn more informative representations by maximizing MI between input data and reconstructed versions thereof, rather than minimizing reconstruction error alone. We describe a novel VAE architecture that directly optimizes MI in the latent space, show how to use gradient ascent optimization techniques, explain why our approach might outperform related methods such as InfoVAE on real-world datasets like MNIST or CelebA, and discuss future directions including using different model families, exploring other loss functions beyond MSE, and developing better architectures that capture higher level abstractions. Ultimately we believe that learning good features via powerful models is crucial for many problems ranging from image generation/retrieval to natural language understanding; maximizing MI might enable significant progress toward these goals while sidestepping some known shortcomings of current feature engineering approaches.",1
"In the Information Bottleneck (IB), when tuning the relative strength between compression and prediction terms, how do the two terms behave, and what's their relationship with the dataset and the learned representation? In this paper, we set out to answer these questions by studying multiple phase transitions in the IB objective: $\text{IB}_\beta[p(z|x)] = I(X; Z) - \beta I(Y; Z)$ defined on the encoding distribution p(z|x) for input $X$, target $Y$ and representation $Z$, where sudden jumps of $dI(Y; Z)/d \beta$ and prediction accuracy are observed with increasing $\beta$. We introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. Using second-order calculus of variations, we derive a formula that provides a practical condition for IB phase transitions, and draw its connection with the Fisher information matrix for parameterized models. We provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between $X$ and $Y$ orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA) in linear settings. Based on the theory, we present an algorithm for discovering phase transition points. Finally, we verify that our theory and algorithm accurately predict phase transitions in categorical datasets, predict the onset of learning new classes and class difficulty in MNIST, and predict prominent phase transitions in CIFAR10.",0
"In recent years, representation learning has emerged as a key area of research in artificial intelligence and machine learning. One of the main challenges in representation learning is finding a balance between preserving information and reducing complexity, which can lead to overfitting or underfitting problems. The ""Information Bottleneck"" (IB) method was introduced to address this challenge by adding an explicit regularization term based on mutual information to the loss function. However, applying IB in practice often leads to suboptimal solutions due to local minima issues and difficulty in choosing hyperparameters.  This paper introduces a novel approach called ""Phase Transitions for the Information Bottleneck"" that leverages techniques from statistical physics to provide a global optimization framework for solving the IB problem. By mapping the optimization landscape of the IB problem onto a nonconvex graphical model, we demonstrate how phase transitions can be used to efficiently navigate through different solution regions and identify globally optimal solutions. Our approach addresses limitations of previous methods and shows significant improvements in terms of accuracy, robustness, and interpretability compared to state-of-the-art algorithms. We evaluate our method using several benchmark datasets across various application domains and showcase its effectiveness in real-world scenarios such as image classification and text generation tasks. Overall, our work provides new insights into the understanding of representation learning and advances the field towards more effective applications of artificial intelligence technologies.",1
"A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.",0
"This paper presents a novel approach to learning structured world models using contrastive learning techniques. We propose a framework that leverages large amounts of unstructured data and uses deep neural networks to extract meaningful features from these inputs. Our method then utilizes contrastive learning to model the relationships between different elements within the environment, leading to more accurate and comprehensive representations of the world. Experimental results demonstrate the effectiveness of our approach, outperforming traditional methods on several benchmark datasets. Overall, our work provides insights into how artificial intelligence agents can learn richer and more informative world models through unsupervised learning techniques.",1
"Trajectory owner prediction is the basis for many applications such as personalized recommendation, urban planning. Although much effort has been put on this topic, the results archived are still not good enough. Existing methods mainly employ RNNs to model trajectories semantically due to the inherent sequential attribute of trajectories. However, these approaches are weak at Point of Interest (POI) representation learning and trajectory feature detection. Thus, the performance of existing solutions is far from the requirements of practical applications. In this paper, we propose a novel CNN-based Trajectory Owner Prediction (CNNTOP) method. Firstly, we connect all POI according to trajectories from all users. The result is a connected graph that can be used to generate more informative POI sequences than other approaches. Secondly, we employ the Node2Vec algorithm to encode each POI into a low-dimensional real value vector. Then, we transform each trajectory into a fixed-dimensional matrix, which is similar to an image. Finally, a CNN is designed to detect features and predict the owner of a given trajectory. The CNN can extract informative features from the matrix representations of trajectories by convolutional operations, Batch normalization, and $K$-max pooling operations. Extensive experiments on real datasets demonstrate that CNNTOP substantially outperforms existing solutions in terms of macro-Precision, macro-Recall, macro-F1, and accuracy.",0
"This paper presents a novel approach for predicting trajectories based on Convolutional Neural Networks (CNN). We propose a method that leverages the strengths of convolutional filters for extracting relevant features from sequences of images, and then uses those features as input for our prediction model. Our key insight lies in the fact that by representing each frame in the sequence as a vector learned via backpropagation through the weights of the final convolutional layer of a pre-trained network, we can capture the salient characteristics of the scene. By feeding these representations into a Long Short Term Memory (LSTM) neural network, we generate predictions of future frames of the scene. Experimental results show significant improvement over existing methods for both accuracy and speed. In summary, our approach provides an effective framework for capturing and utilizing discriminative features extracted from raw video data, yielding improved performance for owner prediction tasks.",1
"Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.",0
"This paper proposes a method for unsupervised scalable representation learning that can effectively learn representations from multivariate time series data without requiring any labeled examples. Inspired by recent advances in self-supervised learning for images, we introduce a novel approach called STAC (Spatio-Temporal Autoencoders with Contrastive Loss) which learns meaningful representations that capture important patterns and relationships across multiple dimensions over time. We evaluate our approach on several benchmark datasets and demonstrate state-of-the-art performance while significantly outperforming other baseline methods. Our results showcase the potential of our proposed framework for solving complex real-world problems involving large-scale sequential data analysis tasks such as fraud detection, predictive analytics, anomaly detection, and more.",1
"Domain gaps of sensor modalities pose a challenge for the design of autonomous robots. Taking a step towards closing this gap, we propose two unsupervised training frameworks for finding a common representation of LiDAR and camera data. The first method utilizes a double Siamese training structure to ensure consistency in the results. The second method uses a Canny edge image guiding the networks towards a desired representation. All networks are trained in an unsupervised manner, leaving room for scalability. The results are evaluated using common computer vision applications, and the limitations of the proposed approaches are outlined.",0
"Recent advances in deep learning have opened new opportunities for unsupervised representation learning from multimodal sensors such as LiDAR (Light Detection And Ranging) cameras and RGB cameras. In this work we present Double Siamese Networks (DSN), a novel technique that learns deep representations by contrasting the difference between two identical models applied to inputs generated by either type of sensor modality. We demonstrate that our approach improves state-of-the art performance on several benchmark datasets including KITTI depth completion and NuScenes object detection. Our results show that our approach can learn strong common representations which encode both semantic features relevant for computer vision tasks as well as geometry information crucial for localization and mapping applications. Furthermore, since our method only requires data pairs but no class labels during training, it allows to leverage large amounts of existing images as side information to guide the training process. We believe that our method holds significant potential for enabling selfdriving cars and robotics platforms where accurate perception across multiple modalities is crucial.",1
"We propose a novel self-supervised method, referred to as Video Cloze Procedure (VCP), to learn rich spatial-temporal representations. VCP first generates ""blanks"" by withholding video clips and then creates ""options"" by applying spatio-temporal operations on the withheld clips. Finally, it fills the blanks with ""options"" and learns representations by predicting the categories of operations applied on the clips. VCP can act as either a proxy task or a target task in self-supervised learning. As a proxy task, it converts rich self-supervised representations into video clip operations (options), which enhances the flexibility and reduces the complexity of representation learning. As a target task, it can assess learned representation models in a uniform and interpretable manner. With VCP, we train spatial-temporal representation models (3D-CNNs) and apply such models on action recognition and video retrieval tasks. Experiments on commonly used benchmarks show that the trained models outperform the state-of-the-art self-supervised models with significant margins.",0
"Self-supervised learning has emerged as a promising approach to addressing the scarcity of labeled data in computer vision tasks. In recent years, several methods have been proposed that leverage large amounts of unlabeled video data to learn representations that can perform well on downstream tasks. However, these methods often rely on pretext tasks that only capture low-level visual features or suffer from limited generalization due to task-specific design choices.  In this work, we propose a new self-supervised method called Video Cloze Procedure (VCP) that addresses both of these issues by jointly capturing spatio-temporal relationships and enabling effective transfer across multiple domains. VCP is designed to predict missing fragments of videos given a temporal context window, which encourages the model to attend to both spatial and temporal structures in the input stream. We show through extensive experiments on six benchmark datasets that our proposed method significantly outperforms state-of-the-art baselines in terms of accuracy and robustness across different domains and tasks.  We analyze the learned representations and demonstrate their effectiveness in improving performance on standard action recognition benchmarks. Furthermore, we investigate the impact of various design choices such as training schedules and regularizations on the final results, providing insights into the tradeoffs involved in building high-performing video representation models. Our study highlights the potential of VCP as a flexible tool for learning generic, powerful video representations without relying heavily on annotated data, paving the way for future research in self-supervised learning for video understanding.",1
"Central to all machine learning algorithms is data representation. For multi-agent systems, selecting a representation which adequately captures the interactions among agents is challenging due to the latent group structure which tends to vary depending on context. However, in multi-agent systems with strong group structure, we can simultaneously learn this structure and map a set of agents to a consistently ordered representation for further learning. In this paper, we present a dynamic alignment method which provides a robust ordering of structured multi-agent data enabling representation learning to occur in a fraction of the time of previous methods. We demonstrate the value of this approach using a large amount of soccer tracking data from a professional league.",0
"In this work we propose an approach that utilizes latent variable modeling combined with deep learning techniques to improve structural discovery and representation learning from multi-agent datasets. Our approach can leverage both continuous state data and discrete action data, enabling robust analysis across multiple domains. We evaluate our method on several large scale publicly available games datasets including: (1) StarCraft II micromanagement tasks, where each agent observes full game states but only their own actions; (2) Minecraft environment navigation tasks, where agents have access to more complex observations containing both actions taken by other agents; and (3) mixed cooperative and competitive Atari environments, to study performance with additional confounding factors such as partial observability and stochasticity. Empirical results demonstrate significantly improved accuracy over previous works across all three benchmark sets. Code and models are provided at https://github.com/openai/improved_maml. This paper contributes to machine learning research by demonstrating effective methods for analyzing complex multi-agent interaction data and understanding how these interactions impact task performance. Keywords: latent variable modeling, multi-agent systems, deep reinforcement learning, behavior clone, generative adversarial imitation learning (GAIL), distributional deep reinforcement learning (DDRL).",1
"Unsupervised learning of disentangled representations involves uncovering of different factors of variations that contribute to the data generation process. Total correlation penalization has been a key component in recent methods towards disentanglement. However, Kullback-Leibler (KL) divergence-based total correlation is metric-agnostic and sensitive to data samples. In this paper, we introduce Wasserstein total correlation in both variational autoencoder and Wasserstein autoencoder settings to learn disentangled latent representations. A critic is adversarially trained along with the main objective to estimate the Wasserstein total correlation term. We discuss the benefits of using Wasserstein distance over KL divergence to measure independence and conduct quantitative and qualitative experiments on several data sets. Moreover, we introduce a new metric to measure disentanglement. We show that the proposed approach has comparable performances on disentanglement with smaller sacrifices in reconstruction abilities.",0
"Representations play a crucial role in most deep learning applications, as they encode relevant features from input data into lower-dimensional vectors. For instance, representations have been used successfully in image classification tasks, speech recognition systems, natural language processing pipelines, and even generative modeling. One popular approach to induce meaningful representations relies on adversarial training objectives that encourage feature learning by exploiting distribution mismatches. This methodology has recently shown great promise owing to disentanglement principles, which allow separating different sources of variation in input distributions while reducing the risk of memorization artifacts. However, existing frameworks based on adversarial losses can suffer from limitations due to their reliance on specific normalizing flows or their susceptibility to collapsing solutions under certain conditions. Here we introduce a novel technique called WERSI (Wasserstein Estimation of Representation Similarity), whereby the minimization of a multiway permutation-invariant version of the Earth Movers Distance metric enables us to learn low-dimensional encodings of complex datasets subjected to high levels of corruption/noise injection. Our contributions consist in demonstrating both theoretically and experimentally that: i) robustness to common forms of noise improves over state-of-the-art approaches; ii) quantitative estimates of total correlation offer sound interpretability perspectives; and iii) cross-domain mappings are achievable by leveraging either latent codes or nonlinear regression functions defined implicitly through transport metrics. Altogether, our results emphasize the potential benefits associated with using regularized representation learning techniques based o",1
"Canonical correlation analysis (CCA) is a popular technique for learning representations that are maximally correlated across multiple views in data. In this paper, we extend the CCA based framework for learning a multiview mixture model. We show that the proposed model and a set of simple heuristics yield improvements over standard CCA, as measured in terms of performance on downstream tasks. Our experimental results show that our correlation-based objective meaningfully generalizes the CCA objective to a mixture of CCA models.",0
"This paper presents a novel approach to representation learning that can handle high-dimensional data containing multiple subspaces. Our method, called multiview representation learning (MVRL), learns a low-dimensional representation of each data point by jointly considering multiple views of the same data points obtained from different domains. Each view provides complementary information about the underlying structure of the data, allowing us to learn a more robust representation that captures all relevant features. We formulate the MVRL problem as a union of subspace estimation task, which consists of finding a linear mapping that maps the original data into a lower dimensional space while preserving the salient structures in each individual subspace. Our algorithm exploits the geometric properties of each subspace and employs nuclear norm regularization to promote sparse solutions, resulting in better generalization performance on real world datasets. Experimental results demonstrate that our method outperforms state-of-the-art techniques for representation learning tasks such as image classification, face verification, and text document retrieval.",1
"In this paper, we investigate the unsupervised deep representation learning issue and technically propose a novel framework called Deep Self-representative Concept Factorization Network (DSCF-Net), for clustering deep features. To improve the representation and clustering abilities, DSCF-Net explicitly considers discovering hidden deep semantic features, enhancing the robustness proper-ties of the deep factorization to noise and preserving the local man-ifold structures of deep features. Specifically, DSCF-Net seamlessly integrates the robust deep concept factorization, deep self-expressive representation and adaptive locality preserving feature learning into a unified framework. To discover hidden deep repre-sentations, DSCF-Net designs a hierarchical factorization architec-ture using multiple layers of linear transformations, where the hierarchical representation is performed by formulating the prob-lem as optimizing the basis concepts in each layer to improve the representation indirectly. DSCF-Net also improves the robustness by subspace recovery for sparse error correction firstly and then performs the deep factorization in the recovered visual subspace. To obtain locality-preserving representations, we also present an adaptive deep self-representative weighting strategy by using the coefficient matrix as the adaptive reconstruction weights to keep the locality of representations. Extensive comparison results with several other related models show that DSCF-Net delivers state-of-the-art performance on several public databases.",0
"In order to find commonalities among different objects through deep neural networks, it has become increasingly important to incorporate self-supervision into these models. This can help learn more from unlabeled data which makes up the majority of most datasets. One method that has proven effective is contrastive learning, where pairs of examples within a batch are pushed together while pairs from different batches are pulled apart. By doing so we create discriminative embeddings and allow our model to generalize better on downstream tasks. In this work we introduce DSCFNs (deep self-representative concept factorization network) which extend existing methods by leveraging the relationships across multiple concepts simultaneously and factorizing them into an anchor image and relative context information. We demonstrate that our approach allows us to achieve state-of-the-art results on benchmark datasets including ImageNet, COCO and Pixar. Furthermore we show how our learned representations transfer well to new domains such as medical imaging and facial recognition. Our code and models have been made public to encourage further research and development in this area. ---",1
"Existing vision-language methods typically support two languages at a time at most. In this paper, we present a modular approach which can easily be incorporated into existing vision-language methods in order to support many languages. We accomplish this by learning a single shared Multimodal Universal Language Embedding (MULE) which has been visually-semantically aligned across all languages. Then we learn to relate MULE to visual data as if it were a single language. Our method is not architecture specific, unlike prior work which typically learned separate branches for each language, enabling our approach to easily be adapted to many vision-language methods and tasks. Since MULE learns a single language branch in the multimodal model, we can also scale to support many languages, and languages with fewer annotations can take advantage of the good representation learned from other (more abundant) language data. We demonstrate the effectiveness of MULE on the bidirectional image-sentence retrieval task, supporting up to four languages in a single model. In addition, we show that Machine Translation can be used for data augmentation in multilingual learning, which, combined with MULE, improves mean recall by up to 21.9% on a single-language compared to prior work, with the most significant gains seen on languages with relatively few annotations. Our code is publicly available.",0
"An increasing amount of data available on social media platforms such as Twitter has been presented as noisy, which can contain multiple modalities (textual, visual, acoustic). Modality discrepancies make multilingual analysis challenging because current techniques have largely focused solely on text-based information. To address these issues, we propose an innovative approach called MULE that extends traditional language models towards multi-modal learning tasks and provides a universal embedding space encompassing all modalities. Our proposed method leverages deep neural networks to jointly learn from different modalities through attention mechanisms, achieving better performance than previous approaches for cross-modality sentiment detection tasks using benchmark datasets from Twitter and Flickr. We believe our contributions provide important insights into exploring new dimensions of big multimedia content while advancing applications of NLP beyond monolithic text paradigms.",1
"Knowledge representation of graph-based systems is fundamental across many disciplines. To date, most existing methods for representation learning primarily focus on networks with simplex labels, yet real-world objects (nodes) are inherently complex in nature and often contain rich semantics or labels, e.g., a user may belong to diverse interest groups of a social network, resulting in multi-label networks for many applications. The multi-label network nodes not only have multiple labels for each node, such labels are often highly correlated making existing methods ineffective or fail to handle such correlation for node representation learning. In this paper, we propose a novel multi-label graph convolutional network (ML-GCN) for learning node representation for multi-label networks. To fully explore label-label correlation and network topology structures, we propose to model a multi-label network as two Siamese GCNs: a node-node-label graph and a label-label-node graph. The two GCNs each handle one aspect of representation learning for nodes and labels, respectively, and they are seamlessly integrated under one objective function. The learned label representations can effectively preserve the inner-label interaction and node label properties, and are then aggregated to enhance the node representation learning under a unified training framework. Experiments and comparisons on multi-label node classification validate the effectiveness of our proposed approach.",0
"In recent years, graph convolutional networks (GCNs) have been widely used in many applications due to their ability to capture both global and local structural features from graphs. However, most existing GCN models assume that each data point belongs to only one class, which cannot accurately represent real-world scenarios where multiple labels can coexist on a single node. Therefore, there is a need for a more powerful representation learning method that considers multi-label problems.  In this work, we propose a novel model called Multi-Label Graph Convolutional Network Representation Learning (MLGCN), which explicitly captures label dependencies among nodes and edge attributes. Our approach utilizes two separate message passing mechanisms: one for intra-node dependency learning across labels, and another for inter-edge attribute propagation. By doing so, our MLGCN model can effectively fuse relational information into node representations while preserving important spatial patterns in the graph structure.  To validate the effectiveness of our proposed model, we conduct extensive experiments on four benchmark datasets covering different domains such as citation, social media, bioinformatics, and sensor network analysis. Experimental results show that our MLGCN model outperforms state-of-the-art methods by achieving higher accuracy, precision, recall, F1 score, and macro F1 score. Furthermore, ablation studies demonstrate the contributions made by each component in our proposed framework.  Overall, this work represents a significant advancement towards solving multi-label graph representation learning challenges. With its strong performance and flexibility, our MLGCN model has broad applicability in fields such as recommendation systems, fraud detection, and customer profiling.",1
"Numerous control and learning problems face the situation where sequences of high-dimensional highly dependent data are available but no or little feedback is provided to the learner, which makes any inference rather challenging. To address this challenge, we formulate the following problem. Given a series of observations $X_0,\dots,X_n$ coming from a large (high-dimensional) space $\mathcal X$, find a representation function $f$ mapping $\mathcal X$ to a finite space $\mathcal Y$ such that the series $f(X_0),\dots,f(X_n)$ preserves as much information as possible about the original time-series dependence in $X_0,\dots,X_n$. We show that, for stationary time series, the function $f$ can be selected as the one maximizing a certain information criterion that we call time-series information. Some properties of this functions are investigated, including its uniqueness and consistency of its empirical estimates.   Implications for the problem of optimal control are presented.",0
"Title: Unsupervised Model-Free Representation Learning - An Overview  Unsupervised representation learning has emerged as one of the most promising techniques in artificial intelligence research. In recent years, several unsupervised methods have been developed that can learn meaningful representations without relying on explicit supervision. This paper provides an overview of these approaches, their benefits, limitations and applications. We begin by discussing the fundamental principles of unsupervised representation learning and why they are important. Then we review some popular unsupervised models such as autoencoders, variational autoencoders (VAEs), Generative Adversarial Networks (GANs) and contrastive learning frameworks like SimCLR and SwAVE. Each method is described in detail along with its key contributions, challenges and open issues. Finally, we conclude with a discussion of potential future directions, including multi-modal representation learning, transfer learning, and integrating human feedback into unsupervised systems. This comprehensive survey serves as a starting point for newcomers to the field, as well as providing insights for experienced practitioners looking for inspiration beyond current state-of-the-art approaches.",1
"Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the ""linear"" form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI, Extended Yale B and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for ""imparting"" provable invariance to any biased pre-trained data representation, and (b) empirically, the trade-off between utility and invariance provided by our solution is comparable to iterative minimax optimization of existing deep neural network based approaches. Code is available at https://github.com/human-analysis/Kernel-ARL",0
"This research paper investigates the global optima of kernelized adversarial representation learning (KARL), a powerful technique that combines deep neural networks with kernel methods to learn complex data representations. We focus on the problem of finding the optimal solution to the KARL objective function using various optimization algorithms and techniques. Our study shows that the search for the global optimum of KARL is challenging due to the nonconvexity and high dimensionality of the space. However, by leveraging recent advances in differentiable programming and continuous optimization, we develop novel approaches that can efficiently navigate the intricate landscape of KARL and discover globally optimal solutions. Experiments on benchmark datasets demonstrate the effectiveness and efficiency of our proposed methods compared to state-of-the-art alternatives. Overall, our work sheds light on the behavior of KARL from a theoretical perspective while providing practical solutions for machine learning practitioners.",1
"Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.",0
"In order to effectively learn from graph data, previous methods have used graph convolutions that can leverage local structure information such as nearest neighbors; however, these models may struggle when faced with larger graphs due to their computational complexity. To address this issue, we propose a hierarchical graph pooling method combined with learning latent structural representations. Our method works by iteratively aggregating node features within clusters while preserving important edges, making use of hierarchical clustering techniques. This allows us to reduce the size of the graph without losing critical information, resulting in improved performance on downstream tasks. We evaluate our approach using several benchmark datasets and show significant improvements over traditional baselines, demonstrating the effectiveness of our proposed method.  Title: ""Hierarchical Graph Pooling with Latent Structure Learning""  Graph data has become increasingly relevant in many fields, including computer vision, natural language processing, and recommendation systems. However, learning from large graphs poses unique challenges due to the high degree of sparsity and varying levels of similarity among nodes. Conventional approaches to representing graphs often rely on predefined structures, such as the number of hops to consider proximal connections, which can limit the model's ability to capture complex relationships.  To address these limitations, we introduce a novel method combining hierarchical graph pooling with latent structure learning (HLSP). HLSP builds upon recent advances in graph clustering and neural networks, merging bottom-up and top-down perspectives into one framework. Specifically, we first apply unsupervised spectral clustering to identify dense regions across different scales. These clusters serve as anchors in constructing an attention mechanism capturing meaningful connections betw",1
"How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. navigating a complex environment)? What are the consequences of not utilizing such visual priors in learning? We study these questions by integrating a generic perceptual skill set (a distance estimator, an edge detector, etc.) within a reinforcement learning framework (see Fig. 1). This skill set (""mid-level vision"") provides the policy with a more processed state of the world compared to raw images.   Our large-scale study demonstrates that using mid-level vision results in policies that learn faster, generalize better, and achieve higher final performance, when compared to learning from scratch and/or using state-of-the-art visual and non-visual representation learning methods. We show that conventional computer vision objectives are particularly effective in this regard and can be conveniently integrated into reinforcement learning frameworks. Finally, we found that no single visual representation was universally useful for all downstream tasks, hence we computationally derive a task-agnostic set of representations optimized to support arbitrary downstream tasks.",0
"In this work we propose ""Learning to Navigate Using Mid-Level Visual Priors"" a method by which learning agents can quickly learn to navigate novel environments without explicitly training on them using pre-existing visual representations. Our approach leverages mid level image features such as objectness, edge detection results and instance masks that are trained separately on large scale datasets like ImageNet. These high quality representations are used to bootstrap our agent into producing its own visual representation. Additionally our model incorporates scene context through the use of graph attention networks enabling improved performance over previous methods. We evaluate our algorithm against several baselines and real world human demonstrations and find that our approach outperforms other similar methods such as ConvVoxelMaps and Proximity Maps. Furthermore our model has significantly less memory overhead allowing for larger models to be run efficiently. Finally we provide qualitative examples of how these prior maps can influence the final output making it easier for humans to interpret their actions. Overall our approach provides researchers interested in imitation learning a quicker path towards training robust autonomous agents in complex tasks with minimal data.",1
"Acquiring ground truth labels for unlabelled data can be a costly procedure, since it often requires manual labour that is error-prone. Consequently, the available amount of labelled data is increasingly reduced due to the limitations of manual data labelling. It is possible to increase the amount of labelled data samples by performing automated labelling or crowd-sourcing the annotation procedure. However, they often introduce noise or uncertainty in the labelset, that leads to decreased performance of supervised deep learning methods. On the other hand, weak supervision methods remain robust during noisy labelsets or can be effective even with low amounts of labelled data. In this paper we evaluate the effectiveness of a representation learning method that uses external categorical evidence called ""Evidence Transfer"", against low amount of corresponding evidence termed as incomplete evidence. Evidence transfer is a robust solution against external unknown categorical evidence that can introduce noise or uncertainty. In our experimental evaluation, evidence transfer proves to be effective and robust against different levels of incompleteness, for two types of incomplete evidence.",0
"Abstract: This paper presents a novel approach to learning improved representations through the transfer of incomplete evidence across heterogeneous tasks. Our method utilizes task embeddings to represent different tasks and facilitate knowledge transfer between them. We propose two new techniques, TaskInformer and TaskSlimmer, which allow us to infer missing data points for each task embedding and perform imputation on incomplete datasets respectively. By using these techniques, we demonstrate significant improvement in model performance on both image classification benchmarks as well as a real world dataset from a clinical setting.  Our approach has three main benefits. Firstly, by employing task embeddings, our method enables efficient and effective knowledge transfer across diverse tasks without requiring explicit alignment between them. Secondly, with our proposed techniques of TaskInformer and TaskSlimmer, we show that even partial observations can substantially benefit the downstream task, thereby addressing the issue of scarce labeled data commonly encountered in practice. Finally, through extensive experiments, we illustrate that our method outperforms competitive baselines by considerable margins while simultaneously reducing computational overhead. These results suggest that our framework holds great potential for developing more powerful machine learning systems by leveraging limited but informative training signals. # --- Open Assistant (Text-to-Speech): You mentioned earlier that you wanted me to assist you with ""Learning Improved Representations by Transferring Incomplete Evidence Across Heterogeneous Tasks"". Can you provide me with any specific instructions or questions you have? I am here to help!",1
"Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.",0
"Here is one possible abstract:  Recent advances in deep learning have resulted in impressive performance on tasks such as image classification, but these models often struggle when applied to new domains or tasks. Federated adversarial domain adaptation (FADA) is a novel approach that addresses this challenge by using adversarial training to learn a shared representation that can generalize across multiple domains. In our work, we evaluate the effectiveness of FADA on several benchmark datasets and demonstrate its ability to significantly improve accuracy over traditional transfer learning methods. We further analyze the behavior of the algorithm and provide insights into how different hyperparameters affect performance. Our results showcase the potential of FADA as a powerful tool for domain adaptation and highlight opportunities for future research.",1
"In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.",0
"Zero-shot learning is a promising approach that allows models to learn concepts without any explicit training data, relying instead on semantic embeddings learned from other tasks. However, successful application of these methods still depends heavily on how well they can handle compositional structure. In this work, we study how locality and compositionality interact in zero-shot learning by examining three aspects: (i) how attention scores distribute across semantically meaningful subtokens; (ii) whether these scores reflect compositionality properties such as linearization effects; and (iii) whether strong performance requires explicitly modeling or regularizing over compositions. We provide analysis and experiments using several state-of-the-art ZSL systems trained over two different NLP datasets. Our results indicate tradeoffs among simplicity, expressiveness, and dataset specificities in achieving good performance, but point towards ways that further progress might be made. They suggest there may be opportunities to improve zero-shot transfer without requiring extra annotations specifically for them, but also highlight potential limitations that would require alternative approaches.",1
"How to learn a discriminative fine-grained representation is a key point in many computer vision applications, such as person re-identification, fine-grained classification, fine-grained image retrieval, etc. Most of the previous methods focus on learning metrics or ensemble to derive better global representation, which are usually lack of local information. Based on the considerations above, we propose a novel Attribute-Aware Attention Model ($A^3M$), which can learn local attribute representation and global category representation simultaneously in an end-to-end manner. The proposed model contains two attention models: attribute-guided attention module uses attribute information to help select category features in different regions, at the same time, category-guided attention module selects local features of different attributes with the help of category cues. Through this attribute-category reciprocal process, local and global features benefit from each other. Finally, the resulting feature contains more intrinsic information for image recognition instead of the noisy and irrelevant features. Extensive experiments conducted on Market-1501, CompCars, CUB-200-2011 and CARS196 demonstrate the effectiveness of our $A^3M$. Code is available at https://github.com/iamhankai/attribute-aware-attention.",0
"In recent years, attention mechanisms have become increasingly popular in natural language processing tasks due to their ability to selectively focus on relevant input features. However, existing methods rely solely on contextual dependencies among tokens without explicitly considering attribute-specific contexts. In this work, we propose an Attribute-Aware Attention (AAA) model that incorporates both content and attribute information into the self-attentional mechanism. Our approach first identifies attribute-specific fragments from input sentences using pre-trained linguistic knowledge sources such as part-of-speech tags and named entities. We then apply a two-stage attention mechanism, wherein the initial stage highlights regions within each fragment that contain content information while the subsequent stage emphasizes attribute labels. By doing so, our AAA module can effectively attend to both contextually dependent and attribute-aware regions during encoding, resulting in improved fine-grained representations for downstream task-oriented models like sentiment analysis, question answering, and machine translation. Experimental results on several benchmark datasets demonstrate significant improvements over strong baselines and state-of-the-art approaches, confirming the effectiveness and versatility of our proposed AAA framework.",1
"Probably the most important problem in machine learning is the preliminary biasing of a learner's hypothesis space so that it is small enough to ensure good generalisation from reasonable training sets, yet large enough that it contains a good solution to the problem being learnt. In this paper a mechanism for {\em automatically} learning or biasing the learner's hypothesis space is introduced. It works by first learning an appropriate {\em internal representation} for a learning environment and then using that representation to bias the learner's hypothesis space for the learning of future tasks drawn from the same environment.   An internal representation must be learnt by sampling from {\em many similar tasks}, not just a single task as occurs in ordinary machine learning. It is proved that the number of examples $m$ {\em per task} required to ensure good generalisation from a representation learner obeys $m = O(a+b/n)$ where $n$ is the number of tasks being learnt and $a$ and $b$ are constants. If the tasks are learnt independently ({\em i.e.} without a common representation) then $m=O(a+b)$. It is argued that for learning environments such as speech and character recognition $b\gg a$ and hence representation learning in these environments can potentially yield a drastic reduction in the number of examples required per task. It is also proved that if $n = O(b)$ (with $m=O(a+b/n)$) then the representation learnt will be good for learning novel tasks from the same environment, and that the number of examples required to generalise well on a novel task will be reduced to $O(a)$ (as opposed to $O(a+b)$ if no representation is used).   It is shown that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results.",0
"Abstract: In recent years there has been significant progress in the development of algorithms that learn internal representations from raw data. These methods have proven effective at tasks such as image classification, speech recognition, and natural language processing. However, they still suffer from some limitations. For example, many existing models require large amounts of training time and computational resources, and may overfit easily if not carefully regularized. Additionally, understanding how these models make decisions can often prove difficult, making interpretability a major challenge. This work seeks to address these issues by developing new approaches that scale better, generalize more robustly, and facilitate analysis and interpretation of learned representations. Our results demonstrate that these objectives are indeed achievable, opening up promising avenues for further research on representation learning. -----",1
"In this paper, we focus on graph representation learning of heterogeneous information network (HIN), in which various types of vertices are connected by various types of relations. Most of the existing methods conducted on HIN revise homogeneous graph embedding models via meta-paths to learn low-dimensional vector space of HIN. In this paper, we propose a novel Heterogeneous Graph Structural Attention Neural Network (HetSANN) to directly encode structural information of HIN without meta-path and achieve more informative representations. With this method, domain experts will not be needed to design meta-path schemes and the heterogeneous information can be processed automatically by our proposed model. Specifically, we implicitly represent heterogeneous information using the following two methods: 1) we model the transformation between heterogeneous vertices through a projection in low-dimensional entity spaces; 2) afterwards, we apply the graph neural network to aggregate multi-relational information of projected neighborhood by means of attention mechanism. We also present three extensions of HetSANN, i.e., voices-sharing product attention for the pairwise relationships in HIN, cycle-consistency loss to retain the transformation between heterogeneous entity spaces, and multi-task learning with full use of information. The experiments conducted on three public datasets demonstrate that our proposed models achieve significant and consistent improvements compared to state-of-the-art solutions.",0
"Abstract: This research proposes an attention-based graph neural network (ABGNN) model for learning from heterogeneous structured data. Traditional GNN models often suffer from performance issues due to their limited ability to capture complex relationships among different types of nodes within graphs. To address this challenge, our ABGNN architecture incorporates multi-head attention mechanisms that enable better representation learning across diverse node features by focusing on specific parts of the input. We evaluate the effectiveness of the proposed approach using comprehensive experiments on several benchmark datasets, including citation networks and social media platforms. Our results demonstrate that the ABGNN model consistently outperforms state-of-the-art baselines while offering significant improvements over existing methods. These findings underscore the importance of considering both local and global contextual dependencies for accurate prediction tasks involving heterogeneous graphs. Overall, our work offers valuable insights into designing more effective GNN architectures suited for handling real-world problems involving diverse relational data.",1
"Constructing high-quality generative models for 3D shapes is a fundamental task in computer vision with diverse applications in geometry processing, engineering, and design. Despite the recent progress in deep generative modelling, synthesis of finely detailed 3D surfaces, such as high-resolution point clouds, from scratch has not been achieved with existing approaches. In this work, we propose to employ the latent-space Laplacian pyramid representation within a hierarchical generative model for 3D point clouds. We combine the recently proposed latent-space GAN and Laplacian GAN architectures to form a multi-scale model capable of generating 3D point clouds at increasing levels of detail. Our evaluation demonstrates that our model outperforms the existing generative models for 3D point clouds.",0
"In order to create effective adversarial representations from high dimensional input spaces, such as 3D point clouds, we need powerful representation learning algorithms that can model complex relationships within large datasets. One approach to achieving this goal is by using latent space laplacian pyramid models (LLPMs), which have been shown to effectively capture higher-order statistical dependencies between variables in both linear and nonlinear settings. Here, we propose a new framework for applying LLPMs to learn adversarial representations from 3D point cloud data, and demonstrate how this method leads to significant improvements over traditional approaches on a range of tasks involving object classification and shape retrieval. By leveraging the strengths of both probabilistic graphical models and deep neural networks, our proposed framework provides a flexible toolset for representing uncertain information in complex 3D scenes, enabling more accurate and robust predictions across domains. Our experiments evaluate the effectiveness of LLPMs against alternative techniques, including autoencoders and variational autoencoders, highlighting their promise in solving real world problems with applications ranging from computer vision and robotics to scientific simulations and beyond. Overall, these results validate the use of LLPMs as a powerful technique for capturing rich patterns in high dimensional data, and suggest future directions for advancing the state of the art in adversarial representation learning with 3D point clouds and other types of high-dimensional inputs.",1
"Graph Convolutional Networks (GCNs) have gained significant developments in representation learning on graphs. However, current GCNs suffer from two common challenges: 1) GCNs are only effective with shallow structures; stacking multiple GCN layers will lead to over-smoothing. 2) GCNs do not scale well with large, dense graphs due to the recursive neighborhood expansion. We generalize the propagation strategies of current GCNs as a \emph{""Sink$\to$Source""} mode, which seems to be an underlying cause of the two challenges. To address these issues intrinsically, in this paper, we study the information propagation mechanism in a \emph{""Source$\to$Sink""} mode. We introduce a new concept ""information flow path"" that explicitly defines where information originates and how it diffuses. Then a novel framework, namely Flow Graph Network (FlowGN), is proposed to learn node representations. FlowGN is computationally efficient and flexible in propagation strategies. Moreover, FlowGN decouples the layer structure from the information propagation process, removing the interior constraint of applying deep structures in traditional GCNs. Further experiments on public datasets demonstrate the superiority of FlowGN against state-of-the-art GCNs.",0
"In recent years, graph representation learning has emerged as a crucial area of study for researchers seeking to develop more effective machine learning models capable of handling complex data structures. As graph neural networks have gained prominence in the field due to their effectiveness at tackling a wide range of problems across numerous domains, understanding the underlying mechanisms that govern their performance becomes increasingly important. This paper provides new insights into the propagation processes occurring within graph representations during training by investigating them from a flow perspective. We introduce two key concepts – node flow and edge flow – which allow us to characterize how signals are transmitted throughout a graph. Our analysis shows that both flows play distinct roles in shaping representation learning, with node flow representing the intrinsic structure of a graph while edge flow encodes external information such as labels. By examining these components individually, we provide a comprehensive understanding of how graph neural network architectures operate, thus paving the way for improved model design and optimization. Ultimately, our findings offer valuable contributions towards advancing knowledge in the field and establishing clear guidelines for deploying successful graph representation algorithms.",1
"Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33% mIoU on the ADE20K dataset. Code is available at https://github.com/XingangPan/Switchable-Whitening.",0
"In deep representation learning, artificial intelligence (AI) systems learn to generate high quality representations of data that capture important features without relying on human-engineered features or explicit supervision. This process typically involves training neural networks on large amounts of data using gradient descent optimization techniques. However, optimizing these models can be computationally expensive, and often requires specialized hardware such as GPUs or TPUs. Furthermore, the performance of these models can depend heavily on factors such as initialization, batch size, and hyperparameter tuning. To address some of these challenges, we introduce Switchable Whitening, a simple and efficient method for preprocessing inputs during both training and evaluation in representation learning tasks. Our approach has several advantages over traditional methods, including faster convergence rates, improved stability, better performance across different model architectures and datasets, and reduced sensitivity to hyperparameters. We demonstrate the effectiveness of our method through comprehensive experiments on image classification benchmarks and show that it consistently leads to significant improvements compared to baseline models trained without whitening. Overall, we believe that our work provides valuable insights into the use of input processing techniques in deep representation learning and could have broad applications in other areas of machine learning.",1
"Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.",0
"In recent years, action recognition has become increasingly important in computer vision research due to its potential applications in surveillance systems, human-computer interaction, and robotics. While deep learning methods have made significant progress in the field, they often suffer from scene bias which can lead to poor performance on novel scenes that were never seen during training. This paper presents a method to mitigate the effects of scene bias by using adversarial training to learn robust representations of actions that generalize across different scenes. We evaluate our approach on two popular datasets: Stanford Online Products (SOP) and Northwestern MultiModal Dataset (NMM). Our results show that our proposed method significantly outperforms several baseline models including those that use domain adaptation techniques. Overall, we demonstrate the feasibility of achieving high performance on new scenes without requiring explicit access to them during training.",1
"This work proposes kernel transform learning. The idea of dictionary learning is well known; it is a synthesis formulation where a basis is learnt along with the coefficients so as to generate or synthesize the data. Transform learning is its analysis equivalent; the transforms operates or analyses on the data to generate the coefficients. The concept of kernel dictionary learning has been introduced in the recent past, where the dictionary is represented as a linear combination of non-linear version of the data. Its success has been showcased in feature extraction. In this work we propose to kernelize transform learning on line similar to kernel dictionary learning. An efficient solution for kernel transform learning has been proposed especially for problems where the number of samples is much larger than the dimensionality of the input samples making the kernel matrix very high dimensional. Kernel transform learning has been compared with other representation learning tools like autoencoder, restricted Boltzmann machine as well as with dictionary learning (and its kernelized version). Our proposed kernel transform learning yields better results than all the aforesaid techniques; experiments have been carried out on benchmark databases.",0
"Machine learning models rely on complex mathematical processes called kernels to capture patterns in data and make predictions. However, kernel selection can be challenging as different types of data require specific kernel functions that maximize performance. In Kernel Transform Learning (KTL), we propose a novel framework that allows models to automatically learn optimal kernels by transforming input features into a high-dimensional space where any kernel function can be used. We demonstrate the effectiveness of our approach using real-world datasets and show that KTL consistently outperforms traditional methods across multiple tasks while reducing the need for manual tuning of kernel parameters. Our work has significant implications for both researchers and practitioners, offering new possibilities for building robust and efficient machine learning systems.",1
"Conventionally, autoencoders are unsupervised representation learning tools. In this work, we propose a novel discriminative autoencoder. Use of supervised discriminative learning ensures that the learned representation is robust to variations commonly encountered in image datasets. Using the basic discriminating autoencoder as a unit, we build a stacked architecture aimed at extracting relevant representation from the training data. The efficiency of our feature extraction algorithm ensures a high classification accuracy with even simple classification schemes like KNN (K-nearest neighbor). We demonstrate the superiority of our model for representation learning by conducting experiments on standard datasets for character/image recognition and subsequent comparison with existing supervised deep architectures like class sparse stacked autoencoder and discriminative deep belief network.",0
"This paper presents a novel approach for feature extraction using discriminative autoencoders (dAEs), which can then be applied to character recognition tasks. Traditional autoencoders learn to reconstruct data by minimizing reconstruction errors. However, they do not explicitly take into account the difference between different classes, leading to poor performance on tasks where robust representations are essential. In contrast, dAEs are trained to separate samples based on their true labels, allowing them to capture more meaningful features that are relevant to the task at hand. We demonstrate the effectiveness of our method through extensive experiments on three challenging datasets, outperforming state-of-the-art methods across all benchmarks. Our results show that the proposed technique offers significant improvements over traditional methods, highlighting its potential as a powerful tool for feature extraction and representation learning in computer vision applications.",1
"As deep neural networks become more adept at traditional tasks, many of the most exciting new challenges concern multimodality---observations that combine diverse types, such as image and text. In this paper, we introduce a family of multimodal deep generative models derived from variational bounds on the evidence (data marginal likelihood). As part of our derivation we find that many previous multimodal variational autoencoders used objectives that do not correctly bound the joint marginal likelihood across modalities. We further generalize our objective to work with several types of deep generative model (VAE, GAN, and flow-based), and allow use of different model types for different modalities. We benchmark our models across many image, label, and text datasets, and find that our multimodal VAEs excel with and without weak supervision. Additional improvements come from use of GAN image models with VAE language models. Finally, we investigate the effect of language on learned image representations through a variety of downstream tasks, such as compositionally, bounding box prediction, and visual relation prediction. We find evidence that these image representations are more abstract and compositional than equivalent representations learned from only visual data.",0
"In recent years, there has been significant progress in the development of generative models that can generate realistic and coherent text, images, and other modalities. However, these models often lack the ability to reason about the relationships between different pieces of information, resulting in representations that are piecemeal at best. To address this limitation, we propose multimodal generative models (MGMs) that use compositional representation learning to capture the structure inherent in complex scenes across multiple modalities. By integrating both intra-modal and inter-modal interactions, MGMs learn a unified representation space where features from different modalities can interact effectively, enabling more powerful generation capabilities. We demonstrate the effectiveness of our approach by applying it to a range of tasks including image-to-image translation, video synthesis, and question answering. Our experiments show that MGMs consistently outperform state-of-the-art methods on several benchmark datasets, highlighting their potential as a tool for general purpose inference and decision making.  While our work focuses primarily on computer vision applications, we believe that the framework presented herein could potentially have broader applicability across a wide spectrum of artificial intelligence tasks. As such, we hope that others will find this research direction equally promising and build upon our initial results to drive forward meaningful breakthroughs in machine learning. Ultimately, MGMs represent an exciting new step towards developing truly intelligent systems capable of tackling the most challenging problems facing society today.",1
"Recent progress of self-supervised visual representation learning has achieved remarkable success on many challenging computer vision benchmarks. However, whether these techniques can be used for domain adaptation has not been explored. In this work, we propose a generic method for self-supervised domain adaptation, using object recognition and semantic segmentation of urban scenes as use cases. Focusing on simple pretext/auxiliary tasks (e.g. image rotation prediction), we assess different learning strategies to improve domain adaptation effectiveness by self-supervision. Additionally, we propose two complementary strategies to further boost the domain adaptation accuracy on semantic segmentation within our method, consisting of prediction layer alignment and batch normalization calibration. The experimental results show adaptation levels comparable to most studied domain adaptation methods, thus, bringing self-supervision as a new alternative for reaching domain adaptation. The code is available at https://github.com/Jiaolong/self-supervised-da.",0
"This paper presents a novel approach for self-supervised domain adaptation in computer vision tasks. We propose a framework that utilizes the information from both source and target domains to learn robust feature representations that can effectively adapt to new data distributions. Our method leverages adversarial training along with unpaired image reconstruction loss to minimize the discrepancy between the two domains while preserving important features for task performance. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our proposed approach, outperforming state-of-the-art methods under challenging domain shift conditions. Our work has important implications for real-world applications where labeled target data may be scarce but still require accurate predictions. Overall, this paper represents a significant contribution to the field of computer vision and deep learning by advancing the current understanding of domain adaptation techniques.",1
"We study nonconvex optimization landscapes for learning overcomplete representations, including learning (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries, where these unsupervised learning problems find many applications in high-dimensional data analysis. Despite the empirical success of simple nonconvex algorithms, theoretical justifications of why these methods work so well are far from satisfactory. In this work, we show these problems can be formulated as $\ell^4$-norm optimization problems with spherical constraint, and study the geometric properties of their nonconvex optimization landscapes. For both problems, we show the nonconvex objectives have benign (global) geometric structures, in the sense that every local minimizer is close to one of the target solutions and every saddle point exhibits negative curvature. This discovery enables the development of guaranteed global optimization methods using simple initializations. For both problems, we show the nonconvex objectives have benign geometric structures -- every local minimizer is close to one of the target solutions and every saddle point exhibits negative curvature -- either in the entire space or within a sufficiently large region. This discovery ensures local search algorithms (such as Riemannian gradient descent) with simple initializations approximately find the target solutions. Finally, numerical experiments justify our theoretical discoveries.",0
"In today's world where data acquisition has become extremely rapid, the requirement for efficient algorithms which can handle large amounts of data is increasingly becoming necessary. With this in mind, researchers have come up with methods that use overcomplete representations to perform dimensionality reduction on highdimensional data sets. One such method is principal component analysis (PCA). While PCA",1
"Feature selection methods have an important role on the readability of data and the reduction of complexity of learning algorithms. In recent years, a variety of efforts are investigated on feature selection problems based on unsupervised viewpoint due to the laborious labeling task on large datasets. In this paper, we propose a novel approach on unsupervised feature selection initiated from the subspace clustering to preserve the similarities by representation learning of low dimensional subspaces among the samples. A self-expressive model is employed to implicitly learn the cluster similarities in an adaptive manner. The proposed method not only maintains the sample similarities through subspace clustering, but it also captures the discriminative information based on a regularized regression model. In line with the convergence analysis of the proposed method, the experimental results on benchmark datasets demonstrate the effectiveness of our approach as compared with the state of the art methods.",0
"In recent years, unsupervised feature selection has become increasingly important due to the availability of large amounts of data that often contain redundant or irrelevant features. Traditional methods rely on manual inspection, correlation coefficients, mutual information, or cluster validity indices which can lead to suboptimal results. This study presents a new approach called ""Unsupervised Feature Selection Based on Adaptive Similarity Learning and Subspace Clustering"" (USFSA) which addresses these shortcomings by leveraging adaptive similarity learning and clustering techniques.  The USFSA algorithm starts by constructing an affinity matrix using local scaling and adaptive weights which allows it to capture nonlinear relationships among features. Then, a graph-based regularization term is used to improve robustness against noise and outliers. Next, a novel subspace clustering method is proposed that takes advantage of both the geometric structure of the data and the connections between features. The algorithm iteratively selects the most representative features and builds subspaces by projecting the original data onto lower-dimensional spaces while preserving their intrinsic geometry as well as the local neighborhood structures. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed method in selecting relevant features and improving clustering performance compared to state-of-the-art approaches. The USFSA algorithm provides researchers and practitioners with a powerful tool for tackling high-dimensional data with limited labeled examples and can have applications in diverse fields such as computer vision, bioinformatics, and social network analysis.",1
"Sparse representations have been shown to be useful in deep reinforcement learning for mitigating catastrophic interference and improving the performance of agents in terms of cumulative reward. Previous results were based on a two step process were the representation was learned offline and the action-value function was learned online afterwards. In this paper, we investigate if it is possible to learn a sparse representation and the action-value function simultaneously and incrementally. We investigate this question by employing several regularization techniques and observing how they affect sparsity of the representation learned by a DQN agent in two different benchmark domains. Our results show that with appropriate regularization it is possible to increase the sparsity of the representations learned by DQN agents. Moreover, we found that learning sparse representations also resulted in improved performance in terms of cumulative reward. Finally, we found that the performance of the agents that learned a sparse representation was more robust to the size of the experience replay buffer. This last finding supports the long standing hypothesis that the overlap in representations learned by deep neural networks is the leading cause of catastrophic interference.",0
"This paper presents a method for learning sparse representations incrementally in deep reinforcement learning (RL) algorithms. The proposed approach addresses the challenge of balancing exploration and exploitation of available knowledge when acquiring new skills in complex environments. By leveraging recent advances in variational Bayesian methods, we develop a novel framework that enables agents to learn gradually from their experience by adaptively selecting which parts of the model need updating as they gain more expertise. Experiments demonstrate the effectiveness of our technique across a range of RL benchmarks, showing improved performance over baseline approaches while maintaining computational efficiency. Our results suggest that incorporating sparsity constraints into deep RL models can significantly enhance the ability of agents to discover and generalize optimal policies under limited data availability. Overall, our work contributes towards better understanding the tradeoffs involved in designing efficient learning systems that balance accuracy and parsimony, providing insights applicable beyond the domain of RL.",1
"It is not until recently that graph neural networks (GNNs) are adopted to perform graph representation learning, among which, those based on the aggregation of features within the neighborhood of a node achieved great success. However, despite such achievements, GNNs illustrate defects in identifying some common structural patterns which, unfortunately, play significant roles in various network phenomena. In this paper, we propose GraLSP, a GNN framework which explicitly incorporates local structural patterns into the neighborhood aggregation through random anonymous walks. Specifically, we capture local graph structures via random anonymous walks, powerful and flexible tools that represent structural patterns. The walks are then fed into the feature aggregation, where we design various mechanisms to address the impact of structural features, including adaptive receptive radius, attention and amplification. In addition, we design objectives that capture similarities between structures and are optimized jointly with node proximity objectives. With the adequate leverage of structural patterns, our model is able to outperform competitive counterparts in various prediction tasks in multiple datasets.",0
"This abstract describes GraLSP, a novel graph neural network framework that utilizes local structural patterns (LSP) for representation learning on graphs. GraLSP addresses two main challenges faced by traditional graph neural networks (GNN): 1) over-smoothing and oversimplification due to global aggregation; and 2) limited representational capacity for complex relationships among vertices due to fixed receptive fields (i.e., neighborhood sizes). By incorporating LSPs into GNNs, we capture important local structural details while preserving nonlinearity and flexibility. These key insights enable us to design more expressive models and better model high-order relations compared to existing methods. We demonstrate the effectiveness of our approach through extensive experiments across diverse benchmark datasets including citation networks, social media graphs, image grid graphs, protein structures, and traffic flow graphs. Our results show that GraLSP consistently outperforms competitive baselines such as GCN, GAT, and GIN in terms of prediction accuracy. Additionally, through ablation studies and visualizations, we provide insights into how GraLSP learns informative representations from local graph structure and captures high-order interactions. Overall, our contributions in this work pave the way for advancing the state-of-the-art in GNNs using local structural patterns and open up new opportunities for further exploration.",1
"Pre-training convolutional neural networks with weakly-supervised and self-supervised strategies is becoming increasingly popular for several computer vision tasks. However, due to the lack of strong discriminative signals, these learned representations may overfit to the pre-training objective (e.g., hashtag prediction) and not generalize well to downstream tasks. In this work, we present a simple strategy - ClusterFit (CF) to improve the robustness of the visual representations learned during pre-training. Given a dataset, we (a) cluster its features extracted from a pre-trained network using k-means and (b) re-train a new network from scratch on this dataset using cluster assignments as pseudo-labels. We empirically show that clustering helps reduce the pre-training task-specific information from the extracted features thereby minimizing overfitting to the same. Our approach is extensible to different pre-training frameworks -- weak- and self-supervised, modalities -- images and videos, and pre-training tasks -- object and action classification. Through extensive transfer learning experiments on 11 different target datasets of varied vocabularies and granularities, we show that ClusterFit significantly improves the representation quality compared to the state-of-the-art large-scale (millions / billions) weakly-supervised image and video models and self-supervised image models.",0
"This paper presents a method called ClusterFit that improves generalization of visual representations by grouping similar data points into clusters during training. We evaluate our approach on two benchmark datasets, showing significant improvements over baseline methods, particularly on novel classes that were never seen before during training. Our results demonstrate that clustering can effectively regularize neural network predictions, leading to better generalization performance. By identifying underlying patterns present in unlabeled images, we provide insights into how clusters enhance feature learning at different levels of granularity. Through detailed analysis, we reveal important properties of clusters and their impact on model behavior, demonstrating the effectiveness of cluster fitness as an evaluation metric for measuring clustering quality. Overall, our findings highlight the potential of incorporating clustering techniques within deep learning frameworks, opening up new directions for future research in computer vision.",1
"We tackle the challenge of disentangled representation learning in generative adversarial networks (GANs) from the perspective of regularized optimal transport (OT). Specifically, a smoothed OT loss gives rise to an implicit transportation plan between the latent space and the data space. Based on this theoretical observation, we exploit a structured regularization on the transportation plan to encourage a prescribed latent subspace to be informative. This yields the formulation of a novel informative OT-based GAN. By convex duality, we obtain the equivalent view that this leads to perturbed ground costs favoring sparsity in the informative latent dimensions. Practically, we devise a stable training algorithm for the proposed informative GAN. Our experiments support the hypothesis that such regularizations effectively yield the discovery of disentangled and interpretable latent representations. Our work showcases potential power of a regularized OT framework in the context of generative modeling through its access to the transport plan. Further challenges are addressed in this line.",0
"The authors present a new methodology called informative generative adversarial networks (InfoGANs) that incorporate structured regularization techniques into their design to overcome limitations found in traditional GAN frameworks. By leveraging optimal transport theory, InfoGANs can better capture meaningful and interpretable information from input data distributions. This results in improved stability and performance during training while producing more accurate synthetic samples with desirable statistical properties. Applications demonstrated on several benchmark datasets showcase impressive gains over state-of-the-art methods across diverse evaluation metrics. In summary, InfoGANs introduce a groundbreaking advance within the realm of deep learning by bridging insights from optimal transport with modern GAN architectures.",1
"The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as ""pearl"") that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.",0
"In recent years, there has been significant progress in unsupervised learning methods that enable deep neural networks (DNNs) to learn meaningful representations without explicit supervision. One popular approach is pretext tasks, which involve training DNNs on auxiliary tasks before fine-tuning them on downstream tasks. However, current approaches to pretext task design often rely heavily on human expertise and can struggle to produce generalizable representations across different domains and architectures. In our paper, we propose self-supervised learning of pretext-invariant representations as a more effective solution. We demonstrate that by using techniques such as adversarial training, autoencoder loss, denoising, and color jittering combined in a self-supervised framework, DNNs can learn robust and transferable features across multiple datasets, models, and types of data (e.g., image, text). Our method outperforms existing state-of-the-art methods and provides evidence for the effectiveness of our proposed strategy for learning generalizable representations. Finally, we discuss potential future directions for improving these methods, including multi-task learning, online adaptation, and ensembling approaches.",1
"The ability of a graph neural network (GNN) to leverage both the graph topology and graph labels is fundamental to building discriminative node and graph embeddings. Building on previous work, we theoretically show that edGNN, our model for directed labeled graphs, is as powerful as the Weisfeiler-Lehman algorithm for graph isomorphism. Our experiments support our theoretical findings, confirming that graph neural networks can be used effectively for inference problems on directed graphs with both node and edge labels. Code available at https://github.com/guillaumejaume/edGNN.",0
"Edges in graphs encode relationships that capture rich structural information beyond traditional node features. Inspired by recent advances on graph neural networks (GNN), we introduce edGNN, a simple yet powerful model designed specifically to operate directly over labeled directed graphs, which may have varying edge attribute densities per direction. We focus our attention on two variants of edGNN tailored to tackle homogeneous and heterogeneous tasks under standard benchmark collections as well as popular realworld datasets. Our main contributions lie at unifying current works into a general framework, while providing strong baselines across domains and settings that can serve as foundation models upon further research. By analyzing how design choices impact performance, we outline key insights towards better understanding GNN behavior as well as guidelines for future research in direct application. Overall, our work presents an important step towards enhancing theoretical groundings for graph-based machine learning.",1
"A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.",0
"Title: Understanding Mind: An Evolutionary and Developmental Approach to Intelligence  Abstract: The study of consciousness has been one of the most contentious topics within philosophy and cognitive science alike. One approach that seeks to address this complexity is the Consciousness Prior (CP), which posits that conscious states precede and enable human intelligence. This perspective argues against traditional theories that view mind as an emergent property of complex neural processing, instead suggesting that conscious experience is fundamental to our understanding of the world. In this paper, we explore how evolutionary pressures shaped the development of consciousness and its relationship to intelligence. We draw on research from psychology, neuroscience, and anthropology to examine the role of embodied cognition, social interaction, and learning mechanisms in the emergence of higher mental capacities. Our findings suggest that conscious awareness plays a critical function in coordinating adaptive responses to environmental challenges, enabling us to better navigate our surroundings and develop increasingly sophisticated problem-solving abilities. Ultimately, the CP offers new insights into the nature of consciousness and highlights its importance as the foundation for all forms of intelligent behavior.",1
"Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations.",0
"This paper presents a new approach to invariant representations that does not require adversarial training. We argue that current methods rely heavily on generating adversarial examples, which can be computationally expensive and may lead to less reliable results. Instead, we propose using self-supervised learning as a means to learn representations that are more robust to natural variations in input data. Our method involves pretraining a neural network on a large dataset with randomly cropped and augmented inputs, then fine-tuning it on a specific task. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets and show that our method outperforms state-of-the-art adversarially trained models while requiring significantly fewer computational resources. Overall, we believe that our work provides a promising alternative to traditional adversarial training techniques and could have important implications for many real-world applications such as image recognition and natural language processing.",1
"Supervised machine learning models often associate irrelevant nuisance factors with the prediction target, which hurts generalization. We propose a framework for training robust neural networks that induces invariance to nuisances through learning to discover and separate predictive and nuisance factors of data. We present an information theoretic formulation of our approach, from which we derive training objectives and its connections with previous methods. Empirical results on a wide array of datasets show that the proposed framework achieves state-of-the-art performance, without requiring nuisance annotations during training.",0
"Effective representation learning is essential for artificial intelligence applications such as computer vision, natural language processing, and robotics. Many techniques have been developed to improve the performance of representations by enhancing their ability to capture invariant properties of data. However, these methods often lack efficient mechanisms for discovering and separating features that contribute to invariant representations. This can lead to suboptimal performance and make it difficult to interpret and understand the learned representations. To address these issues, we propose a new framework called Discovery and Separation of Features (DSF) which combines feature discovery and separation into a unified optimization problem. Our approach utilizes adversarial training to encourage disentanglement while maintaining good reconstruction quality. We evaluate DSF on several benchmark datasets and demonstrate significant improvements over state-of-the-art baselines across multiple modalities. Additionally, we show how our method facilitates interpreting and understanding learned representations through visualizations and ablation studies. Overall, our work contributes towards better understanding and advancing invariant representation learning in artificial intelligence.",1
"A new semi-supervised ensemble algorithm called XGBOD (Extreme Gradient Boosting Outlier Detection) is proposed, described and demonstrated for the enhanced detection of outliers from normal observations in various practical datasets. The proposed framework combines the strengths of both supervised and unsupervised machine learning methods by creating a hybrid approach that exploits each of their individual performance capabilities in outlier detection. XGBOD uses multiple unsupervised outlier mining algorithms to extract useful representations from the underlying data that augment the predictive capabilities of an embedded supervised classifier on an improved feature space. The novel approach is shown to provide superior performance in comparison to competing individual detectors, the full ensemble and two existing representation learning based algorithms across seven outlier datasets.",0
"In this paper, we present XGBOD (eXtreme Gradient Boosting for Outlier Detection), a novel approach that leverages unsupervised representation learning to improve supervised outlier detection performance on tabular data sets. By incorporating an autoencoder pre-training step into the model building process, our method learns compact, meaningful features without additional labeled data. These learned representations then serve as input to an extreme gradient boosting classifier, allowing us to achieve state-of-the-art results across multiple benchmark datasets. We evaluate the effectiveness of XGBOD through extensive experiments, showing that it consistently outperforms competitive baseline methods in terms of both accuracy and robustness metrics. Additionally, we provide insights into the utility of these learned representations by analyzing their interpretability and explainability, demonstrating that they capture interpretable patterns and dependencies within the data. Our work has important implications for the broader field of outlier detection research, suggesting new directions for exploring unsupervised feature learning techniques to enhance the performance of supervised models on complex real-world problems.",1
"Representations used for Facial Expression Recognition (FER) usually contain expression information along with identity features. In this paper, we propose a novel Disentangled Expression learning-Generative Adversarial Network (DE-GAN) which combines the concept of disentangled representation learning with residue learning to explicitly disentangle facial expression representation from identity information. In this method the facial expression representation is learned by reconstructing an expression image employing an encoder-decoder based generator. Unlike previous works using only expression residual learning for facial expression recognition, our method learns the disentangled expression representation along with the expressive component recorded by the encoder of DE-GAN. In order to improve the quality of synthesized expression images and the effectiveness of the learned disentangled expression representation, expression and identity classification is performed by the discriminator of DE-GAN. Experiments performed on widely used datasets (CK+, MMI, Oulu-CASIA) show that the proposed technique produces comparable or better results than state-of-the-art methods.",0
"This could fit into different sections depending on whether you think the reader already knows what the paper covers, if they need to know what facial expression representation learning is before we can tell them how it was done here. Here are some options: The field of Computer Vision has made great strides in recent years thanks to advances in deep learning techniques such as convolutional neural networks (CNNs). In particular, automatic image generation methods like Generative Adversarial Networks (GANs) have proven highly effective at producing realistic images, which makes them well suited for tasks related to image synthesis like generating novel instances from given classes or transferring specific attributes to existing ones. For example, conditional GAN models that receive class labels or attribute maps as input may produce diverse high resolution images of faces with desired expressions or head pose variations. However, these state of the art approaches still struggle with generating face images exhibiting more subtle variations that make human judgement difficult even for experienced observers, which motivates our work presented below. We propose an approach based on feature disentanglement combined with adversarial training that allows us to learn detailed representations of fine grained facial expressions directly from raw pixel inputs of large datasets consisting of unconstrained web images of faces captured under diverse conditions like lighting, backgrounds etc.. In contrast to other methods where each instance is generated conditioned on explicit labels - one per facial landmark point corresponding to its location relative to e.g. the nose tip or eye corners, our model learns a continuous embedding space that represents fine scale variations without any supervision apart from basic RGB image reconstruction. To demonstrate the efficacy of our method we present experiments involving both quantitative evaluation using various metrics commonly adopted in the field and qualitative comparisons w",1
"Convolution is an integral operation that defines how the shape of one function is modified by another function. This powerful concept forms the basis of hierarchical feature learning in deep neural networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere ($\mathbb{S}^2$) or a unit ball ($\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel `\emph{volumetric convolution}' operation that can effectively model and convolve arbitrary functions in $\mathbb{B}^3$. We develop a theoretical framework for \emph{volumetric convolution} based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer in deep networks. By construction, our formulation leads to the derivation of a novel formula to measure the symmetry of a function in $\mathbb{B}^3$ around an arbitrary axis, that is useful in function analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on one viable use case i.e., 3D object recognition.",0
"This research presents a novel approach to representation learning that focuses on unit balls with three-dimensional (3D) roto-translational equivariance. By incorporating these concepts into our model, we aim to improve the robustness and flexibility of representation learning algorithms. Our approach involves training neural networks to recognize patterns within a dataset while ensuring that they remain invariant under rotations and translations in 3D space. We evaluate the performance of our method using several benchmark datasets and demonstrate that it achieves state-of-the-art results across a variety of tasks, including image classification, object detection, and pose estimation. Overall, our work represents a significant contribution to the field of representation learning and highlights the potential benefits of considering equivariance constraints during model training.",1
"Sequential modelling with self-attention has achieved cutting edge performances in natural language processing. With advantages in model flexibility, computation complexity and interpretability, self-attention is gradually becoming a key component in event sequence models. However, like most other sequence models, self-attention does not account for the time span between events and thus captures sequential signals rather than temporal patterns. Without relying on recurrent network structures, self-attention recognizes event orderings via positional encoding. To bridge the gap between modelling time-independent and time-dependent event sequence, we introduce a functional feature map that embeds time span into high-dimensional spaces. By constructing the associated translation-invariant time kernel function, we reveal the functional forms of the feature map under classic functional function analysis results, namely Bochner's Theorem and Mercer's Theorem. We propose several models to learn the functional time representation and the interactions with event representation. These methods are evaluated on real-world datasets under various continuous-time event sequence prediction tasks. The experiments reveal that the proposed methods compare favorably to baseline models while also capturing useful time-event interactions.",0
"This paper describes our method for learning time representations using self-attention mechanisms that operate on functions rather than raw data points. Our approach uses recent advances in neural function approximation methods, and we show how these ideas can be adapted to work in combination with self-attention mechanisms. We provide detailed empirical results that demonstrate significant performance improvements over standard approaches, including those that use transformers without functional approximations. Additionally, we discuss implementation details, theoretical underpinnings, and potential applications of our method. Ultimately, our work contributes new insights into deep learning models that learn representations by processing functions directly, rather than relying exclusively on raw input streams.",1
"In this paper, we propose a new product knowledge graph (PKG) embedding approach for learning the intrinsic product relations as product knowledge for e-commerce. We define the key entities and summarize the pivotal product relations that are critical for general e-commerce applications including marketing, advertisement, search ranking and recommendation. We first provide a comprehensive comparison between PKG and ordinary knowledge graph (KG) and then illustrate why KG embedding methods are not suitable for PKG learning. We construct a self-attention-enhanced distributed representation learning model for learning PKG embeddings from raw customer activity data in an end-to-end fashion. We design an effective multi-task learning schema to fully leverage the multi-modal e-commerce data. The Poincare embedding is also employed to handle complex entity structures. We use a real-world dataset from grocery.walmart.com to evaluate the performances on knowledge completion, search ranking and recommendation. The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.",0
"Product knowledge graphs (PKGs) have emerged as essential tools for e-commerce companies looking to manage and organize their vast inventories of products. However, these PKGs can become very large and complex, making it difficult to effectively utilize them for downstream applications such as recommendation systems or product search engines. To address this challenge, we propose the use of product knowledge graph embedding techniques to map high-dimensional PKG representations into lower-dimensional vectors that capture the important relationships between entities within the graph. We evaluate our approach on two real-world datasets from large online retailers and demonstrate significant improvements over baseline methods for tasks like entity linking and item similarity calculation. Our results show that product knowledge graph embedding has great potential to improve the performance of e-commerce applications by providing more effective ways to represent and analyze product data.",1
"Deep learning-based health status representation learning and clinical prediction have raised much research interest in recent years. Existing models have shown superior performance, but there are still several major issues that have not been fully taken into consideration. First, the historical variation pattern of the biomarker in diverse time scales plays a vital role in indicating the health status, but it has not been explicitly extracted by existing works. Second, key factors that strongly indicate the health risk are different among patients. It is still challenging to adaptively make use of the features for patients in diverse conditions. Third, using prediction models as the black box will limit the reliability in clinical practice. However, none of the existing works can provide satisfying interpretability and meanwhile achieve high prediction performance. In this work, we develop a general health status representation learning model, named AdaCare. It can capture the long and short-term variations of biomarkers as clinical features to depict the health status in multiple time scales. It also models the correlation between clinical features to enhance the ones which strongly indicate the health status and thus can maintain a state-of-the-art performance in terms of prediction accuracy while providing qualitative interpretability. We conduct a health risk prediction experiment on two real-world datasets. Experiment results indicate that AdaCare outperforms state-of-the-art approaches and provides effective interpretability, which is verifiable by clinical experts.",0
"This paper presents AdaCare, a framework that uses scale-adaptive feature extraction and recalibration to learn interpretable representations of clinical health status data. We focus on explaining how our model works and discussing its limitations. Our experiments show that our method outperforms other state-of-the-art approaches by a significant margin. Overall, we believe that AdaCare can have important implications for healthcare applications such as early disease detection and treatment optimization.",1
"Optimizing the execution time of tensor program, e.g., a convolution, involves finding its optimal configuration. Searching the configuration space exhaustively is typically infeasible in practice. In line with recent research using TVM, we propose to learn a surrogate model to overcome this issue. The model is trained on an acyclic graph called an abstract syntax tree, and utilizes a graph convolutional network to exploit structure in the graph. We claim that a learnable graph-based data processing is a strong competitor to heuristic-based feature extraction. We present a new dataset of graphs corresponding to configurations and their execution time for various tensor programs. We provide baselines for a runtime prediction task.",0
"Title: ""Simulation of Execution Times for Tensor Programs using Graph Neural Networks""  Abstract: In recent years, the development of artificial intelligence (AI) has seen tremendous progress due in part to advancements in hardware, such as GPUs that can accelerate computation. However, optimizing the performance of these systems remains a significant challenge. To address this issue, we propose using graph neural networks (GNNs) to simulate execution time for tensor programs. By analyzing the computational graphs underlying these programs, GNNs can learn patterns and relationships that allow them to accurately predict execution times on different hardware configurations. Our approach achieves state-of-the-art accuracy while also allowing for efficient simulation of large models. We evaluate our method on several benchmark datasets and demonstrate its effectiveness at simulating execution time across a range of model sizes and complexity levels. Overall, our work represents a promising step towards automating performance optimization in the field of deep learning.",1
"Most previous studies on multi-agent reinforcement learning focus on deriving decentralized and cooperative policies to maximize a common reward and rarely consider the transferability of trained policies to new tasks. This prevents such policies from being applied to more complex multi-agent tasks. To resolve these limitations, we propose a model that conducts both representation learning for multiple agents using hierarchical graph attention network and policy learning using multi-agent actor-critic. The hierarchical graph attention network is specially designed to model the hierarchical relationships among multiple agents that either cooperate or compete with each other to derive more advanced strategic policies. Two attention networks, the inter-agent and inter-group attention layers, are used to effectively model individual and group level interactions, respectively. The two attention networks have been proven to facilitate the transfer of learned policies to new tasks with different agent compositions and allow one to interpret the learned strategies. Empirically, we demonstrate that the proposed model outperforms existing methods in several mixed cooperative and competitive tasks.",0
"This paper presents a new approach to multi-agent reinforcement learning using actor-critic methods with hierarchical graph attention networks (MGHGN). MGHGN combines two state-of-the-art techniques: multi-agent deep reinforcement learning and graph attention networks. Our method addresses the challenge of scalability in multi-agent systems by allowing agents to focus on relevant information from other agents and their environment. We demonstrate through simulation experiments that our algorithm outperforms traditional approaches and improves over time. Our results show that the use of graph attention networks allows agents to effectively share information, leading to improved overall performance. Additionally, we show that our approach can handle complex tasks and large environments. Finally, we discuss future work and potential applications of our method. Overall, this research contributes to the field of artificial intelligence by providing a novel solution for multi-agent coordination problems.",1
"Disentangled encoding is an important step towards a better representation learning. However, despite the numerous efforts, there still is no clear winner that captures the independent features of the data in an unsupervised fashion. In this work we empirically evaluate the performance of six unsupervised disentanglement approaches on the mpi3d toy dataset curated and released for the NeurIPS 2019 Disentanglement Challenge. The methods investigated in this work are Beta-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, and Beta-TCVAE. The capacities of all models were progressively increased throughout the training and the hyper-parameters were kept intact across experiments. The methods were evaluated based on five disentanglement metrics, namely, DCI, Factor-VAE, IRS, MIG, and SAP-Score. Within the limitations of this study, the Beta-TCVAE approach was found to outperform its alternatives with respect to the normalized sum of metrics. However, a qualitative study of the encoded latents reveal that there is not a consistent correlation between the reported metrics and the disentanglement potential of the model.",0
"This study presents preliminary research into disentanglement, a critical step in machine learning that involves separating features that represent real underlying phenomena from those representing spurious correlations. To date, most methods have relied on human intuition to determine whether two sets of variables were entangled, which can lead to biases and errors in judgment. By contrast, we propose using metrics as objective measures of feature independence, such as mutual information (MI) and maximum mean discrepancy (MMD). However, our results suggest that these metrics may overestimate the degree of disentanglement present in learned representations, leading to unreliable predictions and poor generalization performance. These findings highlight the need for further exploration into more effective metrics for evaluating disentanglement that better capture true feature separation. Our work provides important insights into the limitations of current metrics and paves the way for improved evaluation techniques in the future.",1
"We address the problem of disentangled representation learning with independent latent factors in graph convolutional networks (GCNs). The current methods usually learn node representation by describing its neighborhood as a perceptual whole in a holistic manner while ignoring the entanglement of the latent factors. However, a real-world graph is formed by the complex interaction of many latent factors (e.g., the same hobby, education or work in social network). While little effort has been made toward exploring the disentangled representation in GCNs. In this paper, we propose a novel Independence Promoted Graph Disentangled Networks (IPGDN) to learn disentangled node representation while enhancing the independence among node representations. In particular, we firstly present disentangled representation learning by neighborhood routing mechanism, and then employ the Hilbert-Schmidt Independence Criterion (HSIC) to enforce independence between the latent representations, which is effectively integrated into a graph convolutional framework as a regularizer at the output layer. Experimental studies on real-world graphs validate our model and demonstrate that our algorithms outperform the state-of-the-arts by a wide margin in different network applications, including semi-supervised graph classification, graph clustering and graph visualization.",0
"Our proposed method addresses two limitations: overfitting due to poor regularization and entanglement among attributes in representation learning networks. We introduce a novel algorithm that decouples attribute correlations by promoting independence using adversarial training without explicit supervision on disentanglement metrics. By focusing solely on generative performance as objectives, we can learn representations more efficiently and effectively. Evaluation shows our approach outperforms state-of-the art methods across multiple benchmark datasets on both qualitative and quantitative measures while running faster. Furthermore, experiments demonstrate that graph convolutional network architectures can achieve comparable results to transformers which have been predominant choices for image processing tasks. Code will be made public upon acceptance.",1
"Learning representations of data is an important problem in statistics and machine learning. While the origin of learning representations can be traced back to factor analysis and multidimensional scaling in statistics, it has become a central theme in deep learning with important applications in computer vision and computational neuroscience. In this article, we review recent advances in learning representations from a statistical perspective. In particular, we review the following two themes: (a) unsupervised learning of vector representations and (b) learning of both vector and matrix representations.",0
"This paper presents a statistical perspective on representation learning, which lies at the heart of modern machine learning research. With the rapid development of deep neural networks (DNNs), representation learning has gained increasing attention due to their ability to learn high-dimensional data representations that capture underlying features effectively. Inspired by recent successes of DNN models in computer vision, natural language processing, robotics, among others fields, we provide a mathematical framework that unifies diverse techniques used in practice under a common goal of minimizing the reconstruction error subject to regularization constraints using optimization theory. By bridging these ideas together, the ultimate objective is twofold: firstly, demystify some intriguing properties behind successful applications in those areas; secondly, motivate further investigations into new directions that might have been previously overlooked but can lead to improved generalization performance. We believe our findings offer theoretical insights and valuable methodological recipes to practitioners and researchers alike interested in deep learning systems design.",1
"The (variational) graph auto-encoder and its variants have been popularly used for representation learning on graph-structured data. While the encoder is often a powerful graph convolutional network, the decoder reconstructs the graph structure by only considering two nodes at a time, thus ignoring possible interactions among edges. On the other hand, structured prediction, which considers the whole graph simultaneously, is computationally expensive. In this paper, we utilize the well-known triadic closure property which is exhibited in many real-world networks. We propose the triad decoder, which considers and predicts the three edges involved in a local triad together. The triad decoder can be readily used in any graph-based auto-encoder. In particular, we incorporate this to the (variational) graph auto-encoder. Experiments on link prediction, node clustering and graph generation show that the use of triads leads to more accurate prediction, clustering and better preservation of the graph characteristics.",0
"Title: Effective Decoding in Graph Auto-encoders Using Triadic Closure Introduction In recent years, graph auto-encoders (GAEs) have gained popularity as powerful tools for unsupervised representation learning on graphs. One major challenge faced by GAEs is how to effectively decode the latent representations back into their original node attributes while preserving structural information. This study proposes a novel method that leverages triadic closure to enhance decoding performance in GAEs. Methods We developed a variant of the graph auto-encoder architecture called ""triadic closure graph auto-encoder"" (TC-GAE). Our TC-GAE model utilizes triadic closure properties in the input graph during the training process to encode high-quality node embeddings. We evaluated our proposed approach on several real-world datasets and compared its performance against other state-of-the-art methods such as Deepwalk, LINE, and Node2vec. Results Our experimental results demonstrate that the proposed TC-GAE achieves superior performance over existing models across all evaluation metrics. Moreover, we show that the use of triadic closure allows us to achieve better generalization capabilities without relying solely on large dataset sizes. Conclusion Overall, our work highlights the importance of incorporating network structures in deep learning models and shows the effectiveness of combining triadic closure and graph auto-encoding techniques to produce accurate and interpretable low-dimensional representations of complex networks. These findings contribute valuable insights into the field of graph neural networks, paving the way for future research in this area. Keywords: Graph auto-encoders; triadic closure; unsupervised representation learning; deep learning models; network structures; graph neural networks. ******** Note: Please note that I could only write up to 300 words since there was no context provided for me t",1
"Learning meaningful and compact representations with disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect, many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper, we propose a novel data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position. In order to be able to control all the factors of variation precisely, we built an experimental platform where the objects are being moved by a robotic arm. In addition, we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation, and how simulated data can be leveraged to build better representations of the real world. We provide a first experimental study of these questions and our results indicate that learned models transfer poorly, but that model and hyperparameter selection is an effective means of transferring information to the real world.",0
"Incorporate keywords such as simulation, real world transfer, dataset, disentanglement, inductive bias and explain their significance.",1
"Existing deep learning methods for action recognition in videos require a large number of labeled videos for training, which is labor-intensive and time-consuming. For the same action, the knowledge learned from different media types, e.g., videos and images, may be related and complementary. However, due to the domain shifts and heterogeneous feature representations between videos and images, the performance of classifiers trained on images may be dramatically degraded when directly deployed to videos. In this paper, we propose a novel method, named Deep Image-to-Video Adaptation and Fusion Networks (DIVAFN), to enhance action recognition in videos by transferring knowledge from images using video keyframes as a bridge. The DIVAFN is a unified deep learning model, which integrates domain-invariant representations learning and cross-modal feature fusion into a unified optimization framework. Specifically, we design an efficient cross-modal similarities metric to reduce the modality shift among images, keyframes and videos. Then, we adopt an autoencoder architecture, whose hidden layer is constrained to be the semantic representations of the action class names. In this way, when the autoencoder is adopted to project the learned features from different domains to the same space, more compact, informative and discriminative representations can be obtained. Finally, the concatenation of the learned semantic feature representations from these three autoencoders are used to train the classifier for action recognition in videos. Comprehensive experiments on four real-world datasets show that our method outperforms some state-of-the-art domain adaptation and action recognition methods.",0
"In recent years, deep learning has revolutionized many fields including computer vision, where deep convolutional neural networks have achieved state-of-the-art performance in tasks such as object recognition and image classification. However, recognizing actions from videos remains a challenge due to variations in appearance, motion, pose, lighting conditions, camera viewpoints, etc. To address these issues, we propose novel deep architectures that adaptively learn the visual representations of video frames using temporal models based on recurrent neural networks (RNN) and transformer networks, and effectively fuse both appearance and motion features at multiple scales, capturing spatial-temporal dependencies of actions. Our approach performs fine-grained feature extraction by identifying action categories over time through spatio-temporal attention mechanisms, allowing our model to accurately recognize actions under large variations while maintaining high accuracy across different datasets. We evaluate our method against several competitive baselines and achieve significant improvements in terms of accuracy and efficiency.",1
"(Very early draft)Traditional supervised learning keeps pushing convolution neural network(CNN) achieving state-of-art performance. However, lack of large-scale annotation data is always a big problem due to the high cost of it, even ImageNet dataset is over-fitted by complex models now. The success of unsupervised learning method represented by the Bert model in natural language processing(NLP) field shows its great potential. And it makes that unlimited training samples becomes possible and the great universal generalization ability changes NLP research direction directly. In this article, we purpose a novel unsupervised learning method based on contrastive predictive coding. Under that, we are able to train model with any non-annotation images and improve model's performance to reach state-of-art performance at the same level of model complexity. Beside that, since the number of training images could be unlimited amplification, an universal large-scale pre-trained computer vision model is possible in the future.",0
"This work proposes a new algorithm for unsupervised visual representation learning that introduces increasing object shape bias during training. By incorporating this new bias, we demonstrate significant improvements over standard methods on several benchmark tasks. We evaluate our method using both quantitative metrics as well as human evaluation and show that our approach leads to more interpretable representations. Additionally, we analyze the learned representations to provide insights into how the proposed bias affects their properties and structure. Overall, our results highlight the importance of considering explicit object shape knowledge during unsupervised representation learning.",1
"Massive electronic health records (EHRs) enable the success of learning accurate patient representations to support various predictive health applications. In contrast, doctor representation was not well studied despite that doctors play pivotal roles in healthcare. How to construct the right doctor representations? How to use doctor representation to solve important health analytic problems? In this work, we study the problem on {\it clinical trial recruitment}, which is about identifying the right doctors to help conduct the trials based on the trial description and patient EHR data of those doctors. We propose doctor2vec which simultaneously learns 1) doctor representations from EHR data and 2) trial representations from the description and categorical information about the trials. In particular, doctor2vec utilizes a dynamic memory network where the doctor's experience with patients are stored in the memory bank and the network will dynamically assign weights based on the trial representation via an attention mechanism. Validated on large real-world trials and EHR data including 2,609 trials, 25K doctors and 430K patients, doctor2vec demonstrated improved performance over the best baseline by up to $8.7\%$ in PR-AUC. We also demonstrated that the doctor2vec embedding can be transferred to benefit data insufficiency settings including trial recruitment in less populated/newly explored country with $13.7\%$ improvement or for rare diseases with $8.1\%$ improvement in PR-AUC.",0
"Here we present ""Dynamic Doctor Representation Learning for Clinical Trial Recruitment"", a new method which has shown great promise in learning meaningful representations of physicians relevant to clinical trials. Previous methods have struggled in creating reliable representations due to challenges such as changing physician specialties over time, difficulty in obtaining high quality labeled data, and differences across datasets. Our approach addresses these issues through a novel deep neural network architecture that allows us to learn dynamic doctor embeddings via multi-scale feature integration and attention mechanisms. These embeddings can then be used for downstream tasks such as clinical trial recruitment by utilizing their ability to capture important relationships between entities. Experimental results demonstrate our model's effectiveness compared to strong baselines on three benchmark datasets. We hope that this work paves the way towards better understanding of the impact of doctor features on patient outcomes, ultimately leading to improved healthcare decisions.",1
"Most machine learning theory and practice is concerned with learning a single task. In this thesis it is argued that in general there is insufficient information in a single task for a learner to generalise well and that what is required for good generalisation is information about many similar learning tasks. Similar learning tasks form a body of prior information that can be used to constrain the learner and make it generalise better. Examples of learning scenarios in which there are many similar tasks are handwritten character recognition and spoken word recognition.   The concept of the environment of a learner is introduced as a probability measure over the set of learning problems the learner might be expected to learn. It is shown how a sample from the environment may be used to learn a representation, or recoding of the input space that is appropriate for the environment. Learning a representation can equivalently be thought of as learning the appropriate features of the environment. Bounds are derived on the sample size required to ensure good generalisation from a representation learning process. These bounds show that under certain circumstances learning a representation appropriate for $n$ tasks reduces the number of examples required of each task by a factor of $n$.   Once a representation is learnt it can be used to learn novel tasks from the same environment, with the result that far fewer examples are required of the new tasks to ensure good generalisation. Bounds are given on the number of tasks and the number of samples from each task required to ensure that a representation will be a good one for learning novel tasks.   The results on representation learning are generalised to cover any form of automated hypothesis space bias.",0
"This PhD thesis explores how artificial intelligence systems can learn internal representations that accurately capture relevant aspects of their environment. The ability to develop meaningful representations is crucial for enabling intelligent behavior in complex tasks such as decision making, problem solving, and perception.  The research presented here proposes new methods for learning internal representations that take into account both the structured nature of some domains and the noisy, high-dimensional observations typically encountered in real-world settings. Our approach involves developing algorithms that can effectively utilize domain knowledge and prior information to guide representation learning. We show that these techniques lead to improved performance on challenging benchmark problems and demonstrate promising results on applications ranging from natural language processing to robotics.  Throughout the thesis, we evaluate our approaches using quantitative measures that assess the quality of learned representations. These evaluations reveal insights into the properties of effective representations and enable us to compare different algorithms under standardized conditions. Ultimately, the work described here contributes novel methodologies for understanding and improving the process by which machines acquire insight into complex environments through experience. By advancing the science underlying representation learning, we move closer to building artificial agents capable of robust adaptability across diverse, unpredictable scenarios.",1
"Images or videos always contain multiple objects or actions. Multi-label recognition has been witnessed to achieve pretty performance attribute to the rapid development of deep learning technologies. Recently, graph convolution network (GCN) is leveraged to boost the performance of multi-label recognition. However, what is the best way for label correlation modeling and how feature learning can be improved with label system awareness are still unclear. In this paper, we propose a label graph superimposing framework to improve the conventional GCN+CNN framework developed for multi-label recognition in the following two aspects. Firstly, we model the label correlations by superimposing label graph built from statistical co-occurrence information into the graph constructed from knowledge priors of labels, and then multi-layer graph convolutions are applied on the final superimposed graph for label embedding abstraction. Secondly, we propose to leverage embedding of the whole label system for better representation learning. In detail, lateral connections between GCN and CNN are added at shallow, middle and deep layers to inject information of label system into backbone CNN for label-awareness in the feature learning process. Extensive experiments are carried out on MS-COCO and Charades datasets, showing that our proposed solution can greatly improve the recognition performance and achieves new state-of-the-art recognition performance.",0
"In recent years, multi-label classification has gained significant attention due to its applications in areas such as image recognition, natural language processing, and bioinformatics. However, most existing approaches suffer from limitations such as high complexity, low accuracy, and poor scalability. To address these challenges, we propose a novel approach based on label graph superimposing (LGS). LGS uses labeled examples and their corresponding graphs to learn an embedding space that captures both data characteristics and semantic relationships among labels. Our method outperforms state-of-the-art techniques by achieving higher accuracy and speed while retaining interpretability and explainability. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets, including MUTAG, NCI1, and DREAM7. This work provides a new perspective onmulti-label classification problems and sets a milestone for future research in this field.",1
"Representing networks in a low dimensional latent space is a crucial task with many interesting applications in graph learning problems, such as link prediction and node classification. A widely applied network representation learning paradigm is based on the combination of random walks for sampling context nodes and the traditional \textit{Skip-Gram} model to capture center-context node relationships. In this paper, we emphasize on exponential family distributions to capture rich interaction patterns between nodes in random walk sequences. We introduce the generic \textit{exponential family graph embedding} model, that generalizes random walk-based network representation learning techniques to exponential family conditional distributions. We study three particular instances of this model, analyzing their properties and showing their relationship to existing unsupervised learning models. Our experimental evaluation on real-world datasets demonstrates that the proposed techniques outperform well-known baseline methods in two downstream machine learning tasks.",0
"This paper presents a method called exponential family graph embeddings (EFGE) for learning latent representations from graphs. EFGE extends traditional random walks by allowing each node in the graph to have its own set of parameters, such as Gaussian distributions over continuous features. Our approach uses Bayesian inference to incorporate these additional parameters into the walk distribution, resulting in more expressive embedding models that can capture important structural and content information about the nodes in the graph. We apply our method to several benchmark datasets and show that EFGE significantly outperforms standard random walk methods on link prediction tasks. Additionally, we demonstrate how EFGE can capture meaningful insights into the characteristics of different types of networks, including social media platforms and protein interaction data sets. Overall, our work represents a significant step forward in the development of efficient techniques for understanding complex systems through the use of graph embeddings.",1
"Graph neural network (GNN) is a deep model for graph representation learning. One advantage of graph neural network is its ability to incorporate node features into the learning process. However, this prevents graph neural network from being applied into featureless graphs. In this paper, we first analyze the effects of node features on the performance of graph neural network. We show that GNNs work well if there is a strong correlation between node features and node labels. Based on these results, we propose new feature initialization methods that allows to apply graph neural network to non-attributed graphs. Our experimental results show that the artificial features are highly competitive with real features.",0
"Graph neural networks (GNN) have emerged as a powerful tool for processing graph structured data due to their ability to learn features on graphs efficiently. In recent years, there has been significant research interest in designing new architectures that can better capture node features in GNN models. However, most existing methods focus solely on improving efficiency, while overlooking other important aspects such as accuracy and interpretability. This work presents a comprehensive study of different node feature techniques used in modern GNN architectures. We explore both traditional methods like averaging and normalization techniques along with more advanced approaches such as attention mechanisms. Our analysis shows that certain techniques perform better than others depending on the task at hand, but each technique has limitations. We identify gaps in current literature, motivating future directions for further improvement of GNN models. Finally, we provide guidelines for choosing appropriate feature extraction methods based on the problem type, thus enabling practitioners to make informed decisions when selecting GNN architectures.",1
"In this paper, we propose a structured Robust Adaptive Dic-tionary Pair Learning (RA-DPL) framework for the discrim-inative sparse representation learning. To achieve powerful representation ability of the available samples, the setting of RA-DPL seamlessly integrates the robust projective dictionary pair learning, locality-adaptive sparse representations and discriminative coding coefficients learning into a unified learning framework. Specifically, RA-DPL improves existing projective dictionary pair learning in four perspectives. First, it applies a sparse l2,1-norm based metric to encode the recon-struction error to deliver the robust projective dictionary pairs, and the l2,1-norm has the potential to minimize the error. Sec-ond, it imposes the robust l2,1-norm clearly on the analysis dictionary to ensure the sparse property of the coding coeffi-cients rather than using the costly l0/l1-norm. As such, the robustness of the data representation and the efficiency of the learning process are jointly considered to guarantee the effi-cacy of our RA-DPL. Third, RA-DPL conceives a structured reconstruction weight learning paradigm to preserve the local structures of the coding coefficients within each class clearly in an adaptive manner, which encourages to produce the locality preserving representations. Fourth, it also considers improving the discriminating ability of coding coefficients and dictionary by incorporating a discriminating function, which can ensure high intra-class compactness and inter-class separation in the code space. Extensive experiments show that our RA-DPL can obtain superior performance over other state-of-the-arts.",0
"In recent years, there has been growing interest in developing effective approaches for solving sparse representation problems that arise in a variety of applications. One promising approach is based on discriminative dictionary pair learning (DDPL), which was introduced to improve upon traditional sparse coding methods such as k-SVD and OMP. DDPL seeks to learn two dictionaries instead of one, where the first serves as a representative set of atoms to linearly approximate any signal in the training data while the second acts as a classifier to distinguish different classes using their corresponding sparse codes. While DDLP has shown promising results, it suffers from several limitations: a) it assumes equal importance among features across classes, b) it employs fixed thresholding strategies during code classification resulting in suboptimal performance over varying degrees of sparsity in the dataset, c) it performs equally well on all patches without taking into account distinct features or variations that may exist within each patch. To address these shortcomings, we propose a new method called robust adaptive dictionary pair learning (RADPL). Our proposed algorithm dynamically adjusts the weights of features across classes rather than treating them uniformly; furthermore, RADPL adaptively estimates thresholds based on local statistics derived from both the global image and patchwise neighborhoods, allowing for better accuracy across varying levels of patch sparsity. We evaluate our method on several benchmark datasets for image classification tasks and demonstrate significant improvements compared to state-of-the-art approaches, including those based on deep convolutional neural networks (CNNs). These advances make RADPL a competitive alternative to existing techniques for large scale machine intelligence p",1
"Although deep learning has been applied to successfully address many data mining problems, relatively limited work has been done on deep learning for anomaly detection. Existing deep anomaly detection methods, which focus on learning new feature representations to enable downstream anomaly detection methods, perform indirect optimization of anomaly scores, leading to data-inefficient learning and suboptimal anomaly scoring. Also, they are typically designed as unsupervised learning due to the lack of large-scale labeled anomaly data. As a result, they are difficult to leverage prior knowledge (e.g., a few labeled anomalies) when such information is available as in many real-world anomaly detection applications.   This paper introduces a novel anomaly detection framework and its instantiation to address these problems. Instead of representation learning, our method fulfills an end-to-end learning of anomaly scores by a neural deviation learning, in which we leverage a few (e.g., multiple to dozens) labeled anomalies and a prior probability to enforce statistically significant deviations of the anomaly scores of anomalies from that of normal data objects in the upper tail. Extensive results show that our method can be trained substantially more data-efficiently and achieves significantly better anomaly scoring than state-of-the-art competing methods.",0
"In recent years there has been significant interest in developing machine learning techniques for detecting anomalies within large datasets that generate huge amounts of data. While traditional methods have focused on using handcrafted features to identify outliers or deviations from normal behavior, deep neural networks (DNNs) have emerged as powerful tools for modeling complex patterns in high dimensional spaces. In this paper we propose deviation networks as a new framework for unsupervised anomaly detection that leverages deep neural network architectures to learn representations of data distributions without relying on explicit feature engineering. Our method uses the reconstruction error of the generator network as a proxy measure for the similarity between observed samples and their predicted counterparts generated by the generative model. By minimizing this loss function during training, our approach learns a mapping from input space to latent code that captures important underlying patterns present in the dataset, allowing it to capture unusual or unexpected events as outliers. We evaluate our method on several benchmark datasets commonly used for anomaly detection, including MNIST, CIFAR-10, KDD97, NSL-KDD, and Patchify. Experimental results show that deviation networks significantly improve upon state-of-the-art approaches, achieving better performance at detecting anomalous cases while reducing computational requirements compared to existing unsupervised deep learning models. Additionally, we provide insights into how our model generalizes across different tasks and illustrate some applications where anomaly detection plays an essential role. Overall, our work provides evidence that deviation networks can serve as effective tools for identifying deviations from norma",1
"We study the problem of learning permutation invariant representations that can capture ""flexible"" notions of containment. We formalize this problem via a measure theoretic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations.",0
"Machine learning has seen tremendous progress over the past decade due to advances in representation learning. Traditional machine learning algorithms rely on manually engineering features from raw data which can limit their performance and applicability to new problems. In contrast, representation learning algorithms automatically learn high level representations that capture important features of the underlying data without relying on explicit feature extraction steps. These learned representations have been shown to improve generalization performance across a wide range of applications including computer vision, speech recognition, natural language processing, robotics, and control systems. Motivated by these successes, there is growing interest in developing novel representation learning methods that can further improve the state-of-the-art performance. This paper presents a framework called multiset learning, which extends traditional learning paradigms to handle ambiguity present in some datasets. We show how several existing techniques such as neural networks, linear regression, clustering, dimensionality reduction, etc., can all benefit from our framework. Our extensive experimental evaluations demonstrate significant improvements over conventional approaches using both real-world and synthetic benchmarks while offering interpretability and robustness to noise. Additionally, we provide insights into theoretical limitations under certain conditions based on well established results from mathematical set theory and lattice theory. Finally, we discuss promising future research directions and possible applications enabled by this work.",1
"Humans excel in continuously learning with small data without forgetting how to solve old problems. However, neural networks require large datasets to compute latent representations across different tasks while minimizing a loss function. For example, a natural language understanding (NLU) system will often deal with emerging entities during its deployment as interactions with users in realistic scenarios will generate new and infrequent names, events, and locations. Here, we address this scenario by introducing an RL trainable controller that disentangles the representation learning of a neural encoder from its memory management role.   Our proposed solution is straightforward and simple: we train a controller to execute an optimal sequence of reading and writing operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. Our approach is named Learning to Control (LTC) and allows few-shot learning with two degrees of memory plasticity. We experimentally show that our system obtains accurate results for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset.",0
This article investigates how a few shot learning system can learn named entities by generating training examples using pre-trained models that predict these entities from large datasets such as Wikipedia. We describe experiments that showcase a wide range of techniques including hard negative sampling and cross entropy minimization in order to improve accuracy. Our results demonstrate state of the art performance on multiple benchmarks while running at high speed due to use of simple neural network architectures. Future work will investigate whether transfer learning across domains provides further improvement without retraining the entire model. Overall our contribution shows that current methods based on fully supervised data collection and annotation are suboptimal and we expect few shot learning to play an increasingly important role over the next decade in natural language processing applications,1
"Graph representation learning resurges as a trending research subject owing to the widespread use of deep learning for Euclidean data, which inspire various creative designs of neural networks in the non-Euclidean domain, particularly graphs. With the success of these graph neural networks (GNN) in the static setting, we approach further practical scenarios where the graph dynamically evolves. Existing approaches typically resort to node embeddings and use a recurrent neural network (RNN, broadly speaking) to regulate the embeddings and learn the temporal dynamics. These methods require the knowledge of a node in the full time span (including both training and testing) and are less applicable to the frequent change of the node set. In some extreme scenarios, the node sets at different time steps may completely differ. To resolve this challenge, we propose EvolveGCN, which adapts the graph convolutional network (GCN) model along the temporal dimension without resorting to node embeddings. The proposed approach captures the dynamism of the graph sequence through using an RNN to evolve the GCN parameters. Two architectures are considered for the parameter evolution. We evaluate the proposed approach on tasks including link prediction, edge classification, and node classification. The experimental results indicate a generally higher performance of EvolveGCN compared with related approaches. The code is available at \url{https://github.com/IBM/EvolveGCN}.",0
"In recent years, graph neural networks (GNNs) have become increasingly popular due to their ability to process structured data such as graphs. One challenge faced by GNN models is that they require manual design and tuning of hyperparameters such as learning rate schedules, batch normalization parameters, activation functions, and aggregation functions. In addition, these models are typically trained on static graphs and cannot adapt to changes in the graph structure over time.  In order to address these challenges, we propose EvolveGCN, a novel framework that allows automatic search and optimization of the hyperparameters of graph convolutional networks using evolutionary computation techniques. Our approach uses genetic algorithms to evolve GCN architectures and optimize their hyperparameters for specific tasks and datasets. We showcase the effectiveness of our method through extensive experimental evaluations on several benchmark datasets and demonstrate its capability to identify high-performing GCN architectures that outperform state-of-the-art baselines. Additionally, our method can evolve GCN architectures that are capable of handling dynamic graphs, which makes them applicable to real-world scenarios where graphs change over time. Overall, our work represents a significant step towards automating the design and training of effective GCN architectures for complex problems involving dynamic graphs.",1
"Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision. Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems. For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows state-of-the-art performance on unsupervised classification on ImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC.",0
"Unsupervised representation learning involves discovering meaningful features from raw data without explicit supervision or guidance. In recent years, there has been significant progress in unsupervised image representation learning using large pretrained models on massive datasets such as Imagenet. Despite these advances, most methods assume that images can only have one true relationship with each other, which may not accurately capture complex relationships. To address this limitation, we propose a method called UnreliableDiscoverer which learns multiple levels of reliability associated with different types of relationships between pairs of images. We utilize an attention mechanism to learn reliable representations at multiple scales which captures global and local features effectively. Our framework achieves state-of-the-art performance on benchmark datasets in terms of both quantitative metrics and qualitative evaluations, demonstrating effectiveness of our approach for learning robust and meaningful feature representations.",1
"Person re-identification aims to associate images of the same person over multiple non-overlapping camera views at different times. Depending on the human operator, manual re-identification in large camera networks is highly time consuming and erroneous. Automated person re-identification is required due to the extensive quantity of visual data produced by rapid inflation of large scale distributed multi-camera systems. The state-of-the-art works focus on learning and factorize person appearance features into latent discriminative factors at multiple semantic levels. We propose Deep Parallel Feature Consensus Network (DeepPFCN), a novel network architecture that learns multi-scale person appearance features using convolutional neural networks. This model factorizes the visual appearance of a person into latent discriminative factors at multiple semantic levels. Finally consensus is built. The feature representations learned by DeepPFCN are more robust for the person re-identification task, as we learn discriminative scale-specific features and maximize multi-scale feature fusion selections in multi-scale image inputs. We further exploit average and max pooling in separate scale for person-specific task to discriminate features globally and locally. We demonstrate the re-identification advantages of the proposed DeepPFCN model over the state-of-the-art re-identification methods on three benchmark datasets: Market1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%, 64.3%, and 52.6% respectively on these benchmark datasets.",0
"In recent years, person re-identification has emerged as one of the most challenging problems in computer vision. Existing methods have shown promising results but still face limitations due to variations in pose, illumination, background clutter, resolution, etc. To address these issues, we propose a novel deep learning architecture called DeepPFCN (Deep Parallel Feature Consensus Network). Our approach combines feature extraction using convolutional neural networks with consensus maximization across multiple subnetworks. We use parallel subnetworks to capture discriminative features from different regions of the input image and improve performance under various conditions such as occlusions, scale changes, and viewpoint differences. Extensive experiments on popular benchmark datasets demonstrate that our method outperforms state-of-the-art algorithms by a significant margin, achieving higher rankings on all evaluated metrics. Additionally, ablation studies illustrate the effectiveness of each component of our proposed network.",1
"Self-supervised learning by predicting transformations has demonstrated outstanding performances in both unsupervised and (semi-)supervised tasks. Among the state-of-the-art methods is the AutoEncoding Transformations (AET) by decoding transformations from the learned representations of original and transformed images. Both deterministic and probabilistic AETs rely on the Euclidean distance to measure the deviation of estimated transformations from their groundtruth counterparts. However, this assumption is questionable as a group of transformations often reside on a curved manifold rather staying in a flat Euclidean space. For this reason, we should use the geodesic to characterize how an image transform along the manifold of a transformation group, and adopt its length to measure the deviation between transformations. Particularly, we present to autoencode a Lie group of homography transformations PG(2) to learn image representations. For this, we make an estimate of the intractable Riemannian logarithm by projecting PG(2) to a subgroup of rotation transformations SO(3) that allows the closed-form expression of geodesic distances. Experiments demonstrate the proposed AETv2 model outperforms the previous version as well as the other state-of-the-art self-supervised models in multiple tasks.",0
"This paper presents a novel framework called AEtv2 (AutoEncoding Transformations v2) which enables self-supervised representation learning through minimization of geodesic distances on a given Lie group manifold. The proposed method builds upon previous works on autoencoding transformations but extends these approaches in several key ways. Firstly, we propose using a more expressive family of nonlinear transformations, resulting in improved performance across multiple benchmark datasets. Secondly, we introduce an iterative refinement process that allows for optimization towards smaller geodesic distances over time. Thirdly, we present a new method for approximating gradients in the context of deep generative models, making use of recent advances in automatic differentiation and efficient approximation techniques. Our results demonstrate the effectiveness of our approach across a wide range of tasks, including image classification, reconstruction, and outlier detection, compared against both supervised baselines as well as competing unsupervised methods.",1
"For nonconvex optimization in machine learning, this article proves that every local minimum achieves the globally optimal value of the perturbable gradient basis model at any differentiable point. As a result, nonconvex machine learning is theoretically as supported as convex machine learning with a handcrafted basis in terms of the loss at differentiable local minima, except in the case when a preference is given to the handcrafted basis over the perturbable gradient basis. The proofs of these results are derived under mild assumptions. Accordingly, the proven results are directly applicable to many machine learning models, including practical deep neural networks, without any modification of practical methods. Furthermore, as special cases of our general results, this article improves or complements several state-of-the-art theoretical results on deep neural networks, deep residual networks, and overparameterized deep neural networks with a unified proof technique and novel geometric insights. A special case of our results also contributes to the theoretical foundation of representation learning.",0
"This paper presents a theoretical analysis on non-convex machine learning problems where the local minimum values can actually represent global optimum solutions. By inducing constraints that restrict feasible regions into smaller domains, we show that under certain conditions every local minimum value must be equal to the global minimum value. We prove these results using mathematical derivations and discuss their implications on model training and optimization techniques in deep learning. Our findings suggest that constrained problems may lead to improved generalization performance compared to unconstrained ones while allowing for efficient computational solutions without resorting to complex algorithms like simulated annealing or gradient descent methods. In summary, our work provides new insights and perspectives on understanding and solving non-convex models in modern data science applications.",1
"Given the importance of remote sensing, surprisingly little attention has been paid to it by the representation learning community. To address it and to establish baselines and a common evaluation protocol in this domain, we provide simplified access to 5 diverse remote sensing datasets in a standardized form. Specifically, we investigate in-domain representation learning to develop generic remote sensing representations and explore which characteristics are important for a dataset to be a good source for remote sensing representation learning. The established baselines achieve state-of-the-art performance on these datasets.",0
"Remote sensing involves acquiring imagery from airborne or space borne platforms which can be used to study planet Earth. Since satellites have limited revisit times they tend to acquire images at different spectral bands than ground based cameras. However, in this work we explore whether some of these differences can be overcome by using image representations that incorporate physical models of light transport. We achieve state of art results on two popular benchmarks datasets: Potsdam dataset (for multi-spectral scene classification) and Salinas dataset (for object detection). These promising experimental outcomes open new perspectives for developing effective methods for joint analysis of hyperspectral, multispectral and RGB data acquired by either aerial or space born sensors.",1
"An open research problem in automatic signature verification is the skilled forgery attacks. However, the skilled forgeries are very difficult to acquire for representation learning. To tackle this issue, this paper proposes to learn dynamic signature representations through ranking synthesized signatures. First, a neuromotor inspired signature synthesis method is proposed to synthesize signatures with different distortion levels for any template signature. Then, given the templates, we construct a lightweight one-dimensional convolutional network to learn to rank the synthesized samples, and directly optimize the average precision of the ranking to exploit relative and fine-grained signature similarities. Finally, after training, fixed-length representations can be extracted from dynamic signatures of variable lengths for verification. One highlight of our method is that it requires neither skilled nor random forgeries for training, yet it surpasses the state-of-the-art by a large margin on two public benchmarks.",0
"Abstract: This paper presents a novel approach called SynSig2Vec for learning representations of dynamic signatures by synthesizing data that simulate real-world scenarios. Our method generates a diverse set of artificial signatures using text datasets such as CommonCrawl. We utilize these synthetic signatures along with human-provided samples to train deep neural networks capable of verifying genuine versus fraudulent signature patterns. Experimental results demonstrate significant improvements over state-of-the-art methods on public benchmark datasets while maintaining high accuracy and robustness against adversarial attacks. Overall, our proposed technique offers a reliable solution towards digital document security and transaction validation systems.",1
"There is a recent surge of interest in cross-modal representation learning corresponding to images and text. The main challenge lies in mapping images and text to a shared latent space where the embeddings corresponding to a similar semantic concept lie closer to each other than the embeddings corresponding to different semantic concepts, irrespective of the modality. Ranking losses are commonly used to create such shared latent space -- however, they do not impose any constraints on inter-class relationships resulting in neighboring clusters to be completely unrelated. The works in the domain of visual semantic embeddings address this problem by first constructing a semantic embedding space based on some external knowledge and projecting image embeddings onto this fixed semantic embedding space. These works are confined only to image domain and constraining the embeddings to a fixed space adds additional burden on learning. This paper proposes a novel method, HUSE, to learn cross-modal representation with semantic information. HUSE learns a shared latent space where the distance between any two universal embeddings is similar to the distance between their corresponding class embeddings in the semantic embedding space. HUSE also uses a classification objective with a shared classification layer to make sure that the image and text embeddings are in the same shared latent space. Experiments on UPMC Food-101 show our method outperforms previous state-of-the-art on retrieval, hierarchical precision and classification results.",0
"""Hierarchical Universal Semantic Embeddings (HUSE) is a novel approach to representing natural language data that leverages the power of hierarchies to capture complex relationships between concepts. By organizing semantic representations into nested clusters based on similarity and analogy, our method enables efficient retrieval and generation of relevant text passages. Our experiments demonstrate significant improvements over state-of-the-art methods across multiple benchmark datasets.""",1
"Compression is at the heart of effective representation learning. However, lossy compression is typically achieved through simple parametric models like Gaussian noise to preserve analytic tractability, and the limitations this imposes on learning are largely unexplored. Further, the Gaussian prior assumptions in models such as variational autoencoders (VAEs) provide only an upper bound on the compression rate in general. We introduce a new noise channel, \emph{Echo noise}, that admits a simple, exact expression for mutual information for arbitrary input distributions. The noise is constructed in a data-driven fashion that does not require restrictive distributional assumptions. With its complex encoding mechanism and exact rate regularization, Echo leads to improved bounds on log-likelihood and dominates $\beta$-VAEs across the achievable range of rate-distortion trade-offs. Further, we show that Echo noise can outperform flow-based methods without the need to train additional distributional transformations.",0
"Inference in deep generative models has gained increasing attention due to their remarkable results on challenging tasks ranging from image synthesis and semantic segmentation to style transfer and data augmentation. Yet despite tremendous progress, understanding the behavior and limitations of these methods remains elusive. In particular, analyzing autoencoder-based inference often relies on assumptions such as additive Gaussian noise or idealized coding schemes. We instead propose the first tractable method for exact rate-distortion analysis of autoencoders under arbitrary losses and noise distributions that only depends on the network architecture and training parameters. We provide key insights into how regularization affects the efficiency of compressed representation for reconstruction versus distortion tradeoffs. Empirical results showcase the accuracy of our analytic formulations compared against numerous state-of-the-art baselines across diverse benchmark datasets including images (CIFAR-10, ImageNet), audio clips (SpeechCommands), and time sequences (ActivityRecognition). Our framework significantly advances theoretical foundations and applications of deep generative inference towards real-world deployments in machine learning systems. This work sheds light on the role of overparametrization, capacity constraints, and specific forms of loss landscapes in designing more robust architectures. By bridging the gap between traditional rate-distortion theory and modern deep neural networks, we open up new directions towards systematic model selection beyond just empirical validation based on error rates. Ultimately, our contributions lead to improved automation and interpretability within deep learning pipelines by rigorously quantifying the interplay betw",1
"We present an algorithm, HOMER, for exploration and reinforcement learning in rich observation environments that are summarizable by an unknown latent state space. The algorithm interleaves representation learning to identify a new notion of kinematic state abstraction with strategic exploration to reach new states using the learned abstraction. The algorithm provably explores the environment with sample complexity scaling polynomially in the number of latent states and the time horizon, and, crucially, with no dependence on the size of the observation space, which could be infinitely large. This exploration guarantee further enables sample-efficient global policy optimization for any reward function. On the computational side, we show that the algorithm can be implemented efficiently whenever certain supervised learning problems are tractable. Empirically, we evaluate HOMER on a challenging exploration problem, where we show that the algorithm is exponentially more sample efficient than standard reinforcement learning baselines.",0
"Abstract: Kinematics state abstraction (SA) techniques have shown promising results in deep reinforcement learning (DRL), but often suffer from brittleness. This paper investigates whether richer observation data can alleviate these issues by allowing for more expressive SA mappings and potentially enabling provable regret minimization guarantees without assuming perfect tracking. We propose new theory that shows how observational power helps improve stability under partial observability; we then construct new kinematic SAs based on these insights, which yield strong empirical performance improvements over classical methods across many domains including MuJoCo continuous control tasks. These contributions open up exciting possibilities for more scalable DRL systems beyond tabular cases where offline convergence regrets can be guaranteed. The code is available at https://github.com/googleresearch/krasa.",1
"The supervised learning paradigm is limited by the cost - and sometimes the impracticality - of data collection and labeling in multiple domains. Self-supervised learning, a paradigm which exploits the structure of unlabeled data to create learning problems that can be solved with standard supervised approaches, has shown great promise as a pretraining or feature learning approach in fields like computer vision and time series processing. In this work, we present self-supervision strategies that can be used to learn informative representations from multivariate time series. One successful approach relies on predicting whether time windows are sampled from the same temporal context or not. As demonstrated on a clinically relevant task (sleep scoring) and with two electroencephalography datasets, our approach outperforms a purely supervised approach in low data regimes, while capturing important physiological information without any access to labels.",0
"Researchers have made great strides in using machine learning techniques to analyze brain activity measured through electroencephalography (EEG) signals. This study explores self-supervised representation learning methods that can effectively identify relevant patterns within EEG data. These methods have shown promising results in extracting high-quality features without relying on labeled examples, making them more efficient and scalable than traditional supervised approaches. By leveraging recent advances in computer vision and natural language processing, we propose a framework that uses pretext tasks and contrastive learning to learn meaningful representations of raw EEG data. Our experiments demonstrate the effectiveness of our approach in several applications including artifact removal, dimensionality reduction, and feature extraction. Overall, these findings offer new insights into the potential use of unlabeled EEG data and highlight the importance of developing generalizable representation learning methods in neuroscience research.",1
"Disentangled representation learning finds compact, independent and easy-to-interpret factors of the data. Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Specifically, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and another that represents the transformed data. We find that the latter naturally captures the intrinsic appearance of the data. To realize the generative model, we propose a Variationally Inferred Transformational Autoencoder (VITAE) that incorporates a spatial transformer into a variational autoencoder. We show how to perform inference in the model efficiently by carefully designing the encoders and restricting the transformation class to be diffeomorphic. Empirically, our model separates the visual style from digit type on MNIST, separates shape and pose in images of human bodies and facial features from facial shape on CelebA.",0
"In recent years there has been significant progress towards unsupervised learning using generative models. In particular, variational autoencoders (VAEs) have shown promise as they can learn powerful representations without explicit supervision. These learned representations often encode both appearance and perspective which makes their interpretation difficult. We propose an approach for explicitly disentangling appearance from perspective in VAEs which results in improved interpretability and use cases such as 2D image synthesis and pose estimation. Our method builds on previous work in this area by incorporating stronger regularization terms which improve the quality of the latent representation. Through extensive experimentation we show that our proposed method outperforms several state-of-the art methods in various tasks related to image generation and evaluation. Additionally, we demonstrate how our model may be used as a building block within more complex architectures for task specific fine tuning. Finally, we discuss the potential impact of these findings to areas outside computer graphics including data compression and natural language processing.",1
"The information bottleneck principle is an elegant and useful approach to representation learning. In this paper, we investigate the problem of representation learning in the context of reinforcement learning using the information bottleneck framework, aiming at improving the sample efficiency of the learning algorithms. %by accelerating the process of discarding irrelevant information when the %input states are extremely high-dimensional. We analytically derive the optimal conditional distribution of the representation, and provide a variational lower bound. Then, we maximize this lower bound with the Stein variational (SV) gradient method. We incorporate this framework in the advantageous actor critic algorithm (A2C) and the proximal policy optimization algorithm (PPO). Our experimental results show that our framework can improve the sample efficiency of vanilla A2C and PPO significantly. Finally, we study the information bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation(MINE) . We experimentally verify that the information extraction-compression process also exists in deep RL and our framework is capable of accelerating this process. We also analyze the relationship between MINE and our method, through this relationship, we theoretically derive an algorithm to optimize our IB framework without constructing the lower bound.",0
"In recent years, there has been growing interest in developing representation learning methods that can effectively capture meaningful features from large amounts of raw data. This has led to significant advances in deep reinforcement learning (DRL), where agents learn to make decisions by interacting with complex environments while acquiring useful representations. However, most existing DRL algorithms still suffer from limitations such as slow convergence rates, poor sample efficiency, and sensitivity to hyperparameters. To address these challenges, we propose a novel algorithm called Information Bottleneck Deep Reinforcement Learning (IB-DRL) which introduces an information bottleneck constraint during policy optimization. Our approach regularizes the agent's behavior by encouraging it to use compact, task-relevant representations that preserve important information for decision making. We evaluate our method on a range of continuous control benchmark tasks and demonstrate its superior performance compared to state-of-the-art DRL algorithms in terms of both sample efficiency and overall task performance. Furthermore, we provide theoretical analysis to support our findings and shed light on the underlying mechanisms behind IB-DRL's effectiveness. Overall, our work represents a step towards more efficient and effective representation learning in complex DRL problems.",1
"Machine learning on graph structured data has attracted much research interest due to its ubiquity in real world data. However, how to efficiently represent graph data in a general way is still an open problem. Traditional methods use handcraft graph features in a tabular form but suffer from the defects of domain expertise requirement and information loss. Graph representation learning overcomes these defects by automatically learning the continuous representations from graph structures, but they require abundant training labels, which are often hard to fulfill for graph-level prediction problems. In this work, we demonstrate that, if available, the domain expertise used for designing handcraft graph features can improve the graph-level representation learning when training labels are scarce. Specifically, we proposed a multi-task knowledge distillation method. By incorporating network-theory-based graph metrics as auxiliary tasks, we show on both synthetic and real datasets that the proposed multi-task learning method can improve the prediction performance of the original learning task, especially when the training data size is small.",0
"Deep learning has revolutionized many fields by providing state-of-the-art results on complex tasks. However, deep neural networks (DNNs) can suffer from overfitting due to their high capacity, leading to poor generalization performance on unseen data. To address this problem, regularization techniques have been proposed that force DNNs to learn more compact representations that capture only the most important features of the training set while ignoring noise and irrelevant information. In recent years, knowledge distillation has emerged as a promising approach to transfer knowledge learned by one model to another, usually smaller, student network to improve generalization. Motivated by these advances, we propose a novel graph representation learning method called Graph Representation Learning via Multi-task Knowledge Distillation (GRLMDKD). Our method leverages both multi-task knowledge distillation and adversarial regularization methods to encourage the extraction of discriminative representations for node classification. We design a three-component architecture composed of two parallel subgraph convolutional networks (GCNs), each trained on a different task: graph structure prediction and downstream node classification. An additional component is a domain discriminator that guides feature generation by enforcing maximum confusion during adversarial learning. By jointly optimizing all components through a carefully designed loss function, our GRLMDKD framework achieves superior performance compared to existing approaches in several benchmark datasets without using any explicit label propagation, such as edge attributes or external information. Notably, we show consistent improvements across a variety of challenging node classification problems under diverse settings, demonstrating strong robustness for the proposed method. This work provides new insights into how graph representation learning can benefit fro",1
"Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow (TOFlow), a motion representation learned in a self-supervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",0
"The field of computer vision has made significant strides in recent years due to advancements in deep learning techniques such as convolutional neural networks (CNNs). However, video processing still poses unique challenges that require specialized methods to handle the complex nature of temporal data. In this work, we present a novel approach to enhance videos using task-oriented flow, which leverages the strengths of both traditional flow estimation and CNN-based feature extraction. Our method performs well on standard benchmark datasets and shows promising results for tasks such as image super resolution and video denoising. By utilizing learned features from pretrained models, our algorithm achieves state-of-the-art performance without requiring extensive fine-tuning or retraining. Overall, the proposed method represents a major step forward in improving the quality of visual media while maintaining computational efficiency and accuracy.",1
"For embodied agents to infer representations of the underlying 3D physical world they inhabit, they should efficiently combine multisensory cues from numerous trials, e.g., by looking at and touching objects. Despite its importance, multisensory 3D scene representation learning has received less attention compared to the unimodal setting. In this paper, we propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities. We also introduce a novel method, called the Amortized Product-of-Experts, to improve the computational efficiency and the robustness to unseen combinations of modalities at test time. Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations from arbitrary combinations of modalities and perform accurate cross-modal generation. To perform this exploration, we also develop the Multisensory Embodied 3D-Scene Environment (MESE).",0
"Here is an example of an abstract that meets your criteria:  Title (not included): Neural Multisensory Scene Inference  Abstract: This work presents a neural approach to multimodal scene understanding that involves processing visual and auditory inputs jointly using deep learning methods. We propose a novel architecture that fuses audio-visual representations at different levels of abstraction, allowing for more effective integration of complementary sensory signals. Our model achieves state-of-the-art results on several benchmark datasets, demonstrating the efficacy of our method for robust multimodal inference tasks such as action recognition, sound source localization, and speech enhancement. By leveraging both vision and audio, we show that our system can provide richer representations of complex scenes and situations, enabling new applications in areas such as human-computer interaction and robotics. Overall, our research highlights the importance of crossmodal processing and the potential of deep learning algorithms in advancing computational perception.",1
"Graph representation learning for hypergraphs can be used to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. We perform extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in genomics. We demonstrate that Hyper-SAGNN significantly outperforms the state-of-the-art methods on traditional tasks while also achieving great performance on a new task called outsider identification. Hyper-SAGNN will be useful for graph representation learning to uncover complex higher-order interactions in different applications.",0
"Graph Neural Networks (GNN) have emerged as powerful tools to process complex data represented in a non-Euclidean domain. GNN model nodes’ representation that are linked together by edges and can incorporate both graph structure and node content through message passing. With their flexibility and effectiveness, such models were applied in several domains like biochemistry and computer vision. But these methods tend to neglect edge properties which results in less effective performance. We introduce a novel model called Hyper- SA GNN capable of learning from multi relations between different types of entities while using edge properties to improve the quality of embeddings obtained during training time. After thorough comparison against state-of-the art techniques, our method proved superiority on benchmark datasets yielding higher accuracy scores and lower errors. By developing this approach we open new possibilities to explore and gain deeper understanding into more real-world applications where graph structures possess multi-relations and richer representations.",1
"While a wide range of interpretable generative procedures for graphs exist, matching observed graph topologies with such procedures and choices for its parameters remains an open problem. Devising generative models that closely reproduce real-world graphs requires domain knowledge and time-consuming simulation. While existing deep learning approaches rely on less manual modelling, they offer little interpretability. This work approaches graph generation (decoding) as the inverse of graph compression (encoding). We show that in a disentanglement-focused deep autoencoding framework, specifically Beta-Variational Autoencoders (Beta-VAE), choices of generative procedures and their parameters arise naturally in the latent space. Our model is capable of learning disentangled, interpretable latent variables that represent the generative parameters of procedurally generated random graphs and real-world graphs. The degree of disentanglement is quantitatively measured using the Mutual Information Gap (MIG). When training our Beta-VAE model on ER random graphs, its latent variables have a near one-to-one mapping to the ER random graph parameters n and p. We deploy the model to analyse the correlation between graph topology and node attributes measuring their mutual dependence without handpicking topological properties.",0
"This paper investigates which features in real graphs can be reproduced by generative models while respecting disentanglement principles. We first provide evidence that state-of-the-art graph generators learn interpretable and semantically meaningful node representations even though they are trained without supervision on explicit node labels or attributes. We then use Bayesian methods to infer both the latent causes and their corresponding strengths from observed (node) attribute data while accounting for the uncertainty associated with model parameters. By exploiting the linkability property — that every latent variable factorizes over links in connected components — we show how most generative models admit low-rank factorizations enabling efficient inference; however, these decompositions might not correspond to physically meaningful factors due to entanglement issues in the learned model weights. To overcome these limitations, our proposed method uses regularization techniques based on disentangled autoencoders and formulates objective functions ensuring independence constraints among factors affecting individual node attributes within each community structure. Finally, experiments demonstrate the improved interpretability and identifiability achieved by our framework. In the future, these insights could open up new opportunities for designing self-supervised learning approaches targeted at specific applications requiring explainable artificial intelligence in graph analytics tasks.",1
"Multi-focus noisy image fusion represents an important task in the field of image fusion which generates a single, clear and focused image from all source images. In this paper, we propose a novel multi-focus noisy image fusion method based on low-rank representation (LRR) which is a powerful tool in representation learning. A multi-scale transform framework is adopted in which source images are decomposed into low frequency and high frequency coefficients, respectively. For low frequency coefficients, the fused low frequency coefficients are determined by a spatial frequency strategy, while the high frequency coefficients are fused by the LRR-based fusion strategy. Finally, the fused image is reconstructed by inverse multi-scale transforms with fused coefficients. Experimental results demonstrate that the proposed algorithm offers state-of-the-art performance even when the source images contain noise. The Code of our fusion method is available at https://github.com/hli1221/imagefusion_noisy_lrr",0
"In recent years there has been significant interest in developing image fusion algorithms that can effectively combine multiple images that have varying degrees of noise. These algorithms aim to preserve important details from each source image while reducing overall noise levels. In this paper we propose a new approach to multi-focus noisy image fusion using low-rank representation (MFNIFL). Our method first extracts low-rank representations of each input image, which allows us to capture salient features without being overwhelmed by noise. We then fuse these low-rank representations into a single composite image using a weighted average scheme. Experimental results demonstrate that our MFNIFL algorithm outperforms several state-of-the-art methods in terms of both visual quality and objective measures such as signal-to-noise ratio and structural similarity index. Overall, our proposed method provides an effective solution for multi-focus noisy image fusion that achieves high performance and robustness. Keywords: image fusion, low-rank representation, multi-focus, noise reduction.",1
"Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",0
"This paper presents a new approach to adversarial representation learning using large scale datasets. We propose a method that uses deep neural networks to learn representations of data in an adversarial setting, where two networks work together to improve each other's performance. Our method differs from traditional methods as we focus on large scale problems which require efficient computation and memory usage. In our experiments, we show that our proposed method achieves state-of-the-art results while being faster and more memory efficient than previous approaches. Our contributions include a novel formulation of adversarial representation learning and an algorithm for solving large scale versions of these models efficiently.",1
"Despite their renowned predictive power on i.i.d. data, convolutional neural networks are known to rely more on high-frequency patterns that humans deem superficial than on low-frequency patterns that agree better with intuitions about what constitutes category membership. This paper proposes a method for training robust convolutional networks by penalizing the predictive power of the local representations learned by earlier layers. Intuitively, our networks are forced to discard predictive signals such as color and texture that can be gleaned from local receptive fields and to rely instead on the global structures of the image. Across a battery of synthetic and benchmark domain adaptation tasks, our method confers improved generalization out of the domain. Also, to evaluate cross-domain transfer, we introduce ImageNet-Sketch, a new dataset consisting of sketch-like images, that matches the ImageNet classification validation set in categories and scale.",0
"This paper proposes a novel approach for learning global representations that are robust to local variations in data distributions. Our method penalizes predictive power at a local level, encouraging models to focus on more generalizable features that can capture patterns across diverse input domains. We evaluate our algorithm on a variety of tasks including image classification, language modeling, and reinforcement learning, demonstrating consistent improvements over baseline methods. Our results highlight the effectiveness of our approach in producing representations that generalize well to new environments, making them suitable for real-world applications whererobustness is crucial. Furthermore, we provide analysis of how ourmethod affects representation quality, model architecture design choices,and other hyperparameters. Overall, our work offers valuable insights into developing robust representations for machine learning systems.",1
"This paper considers the problem of Phase Identification in power distribution systems. In particular, it focuses on improving supervised learning accuracies by focusing on exploiting some of the problem's information theoretic properties. This focus, along with recent advances in Information Theoretic Machine Learning (ITML), helps us to create two new techniques. The first transforms a bound on information losses into a data selection technique. This is important because phase identification data labels are difficult to obtain in practice. The second interprets the properties of distribution systems in the terms of ITML. This allows us to obtain an improvement in the representation learned by any classifier applied to the problem. We tested these two techniques experimentally on real datasets and have found that they yield phenomenal performance in every case. In the most extreme case, they improve phase identification accuracy from $51.7\%$ to $97.3\%$. Furthermore, since many problems share the physical properties of phase identification exploited in this paper, the techniques can be applied to a wide range of similar problems.",0
"Abstract: This paper presents a novel methodology for improving supervised phase identification by incorporating insights from the theory of information losses. Traditional methods rely on manual feature engineering, which can lead to biased results, high computational costs, and limited applicability across datasets. We address these shortcomings by leveraging insights into the structure of information loss functions to learn more informative features automatically. Our approach employs a dual representation of data that facilitates regularized learning while preserving interpretability, ensuring robustness through both empirical validation and sensitivity analysis. Our experiments demonstrate consistent improvements over state-of-the-art baselines across diverse domains and tasks, including image classification, sentiment analysis, and speech recognition. Our findings highlight the importance of considering the nature of information losses during model construction and offer new directions for developing effective machine learning algorithms.",1
"Automatic speech emotion recognition provides computers with critical context to enable user understanding. While methods trained and tested within the same dataset have been shown successful, they often fail when applied to unseen datasets. To address this, recent work has focused on adversarial methods to find more generalized representations of emotional speech. However, many of these methods have issues converging, and only involve datasets collected in laboratory conditions. In this paper, we introduce Adversarial Discriminative Domain Generalization (ADDoG), which follows an easier to train ""meet in the middle"" approach. The model iteratively moves representations learned for each dataset closer to one another, improving cross-dataset generalization. We also introduce Multiclass ADDoG, or MADDoG, which is able to extend the proposed method to more than two datasets, simultaneously. Our results show consistent convergence for the introduced methods, with significantly improved results when not using labels from the target dataset. We also show how, in most cases, ADDoG and MADDoG can be used to improve upon baseline state-of-the-art methods when target dataset labels are added and in-the-wild data are considered. Even though our experiments focus on cross-corpus speech emotion, these methods could be used to remove unwanted factors of variation in other settings.",0
"This paper presents a new approach for improving cross-corpus speech emotion recognition using adversarial discriminative domain generalization (ADDoG). Motivated by recent advances in domain generalization, we propose a novel framework that combines both generative and discriminative learning to address the problem of limited data availability across domains. Our method leverages two different objectives: a generative objective where a variational autoencoder maps inputs into a latent space invariant across all domains, and a discriminative objective where our model learns to accurately classify emotions within each individual corpus while being robust to changes in domain characteristics. To achieve these goals, we introduce an additional loss term called the adversarial loss, which promotes discrimination between the current task and other tasks from unseen domains during training. Experiments on five publicly available datasets show that ADDoG significantly outperforms state-of-the-art approaches in cross-corpus emotion recognition under multiple evaluation metrics, demonstrating the effectiveness of our proposed method. Overall, ADDoG offers an effective solution to the challenge of recognizing emotions accurately across diverse spoken language domains, paving the way towards more reliable affective computing systems.",1
"In this paper, we aim to tackle the one-shot person re-identification problem where only one image is labelled for each person, while other images are unlabelled. This task is challenging due to the lack of sufficient labelled training data. To tackle this problem, we propose to iteratively guess pseudo labels for the unlabeled image samples, which are later used to update the re-identification model together with the labelled samples. A new sampling mechanism is designed to select unlabeled samples to pseudo labelled samples based on the distance matrix, and to form a training triplet batch including both labelled samples and pseudo labelled samples. We also design an HSoften-Triplet-Loss to soften the negative impact of the incorrect pseudo label, considering the unreliable nature of pseudo labelled samples. Finally, we deploy an adversarial learning method to expand the image samples to different camera views. Our experiments show that our framework achieves a new state-of-the-art one-shot Re-ID performance on Market-1501 (mAP 42.7%) and DukeMTMC-Reid dataset (mAP 40.3%). Code will be available soon.",0
"In recent years, person re-identification has become increasingly important in computer vision research due to growing demand in applications such as security surveillance systems, multi-camera tracking, and person search in large scale video datasets. However, accurate one-shot person re-identification remains challenging because images captured from different cameras at varying angles can lead to significant appearance variations that make matching difficult. Previous methods have focused on representation learning using metric learning algorithms and generative adversarial networks (GANs) but have not addressed the issue of data scarcity in one-shot settings which leads to poor generalization performance. To address these limitations, we propose progressive sample mining and representation learning for one-shot person re-identification with adversarial samples. We first mine hard negative samples by generating pseudo identities with GANs trained specifically for this task, then optimize representations using these newly generated samples alongside existing training samples. Our approach significantly outperforms previous state-of-the art results demonstrating the effectiveness of our method. The proposed framework bridges the gap between metric learning and deep feature extraction which makes the model more robust to changes in illumination conditions and occlusions. The main contributions of our work are: (1) a novel pipeline for progressively sampling and representing unknown persons based on adversarial training; (2) effective integration of generator and discriminator models that enables learning richer features while minimizing the risk of overfitting; and (3) improved accuracy in comparison to other competitive approaches evaluated in several standard benchmark datasets. This study lays the foundation for future research aimed towards enhancing th",1
"Nonlinear independent component analysis (ICA) is a general framework for unsupervised representation learning, and aimed at recovering the latent variables in data. Recent practical methods perform nonlinear ICA by solving a series of classification problems based on logistic regression. However, it is well-known that logistic regression is vulnerable to outliers, and thus the performance can be strongly weakened by outliers. In this paper, we first theoretically analyze nonlinear ICA models in the presence of outliers. Our analysis implies that estimation in nonlinear ICA can be seriously hampered when outliers exist on the tails of the (noncontaminated) target density, which happens in a typical case of contamination by outliers. We develop two robust nonlinear ICA methods based on the {\gamma}-divergence, which is a robust alternative to the KL-divergence in logistic regression. The proposed methods are shown to have desired robustness properties in the context of nonlinear ICA. We also experimentally demonstrate that the proposed methods are very robust and outperform existing methods in the presence of outliers. Finally, the proposed method is applied to ICA-based causal discovery and shown to find a plausible causal relationship on fMRI data.",0
"In recent years there has been growing interest in using deep neural networks (DNNs) as representation extractors in machine learning applications such as object recognition and speech processing. These DNNs can learn representations by optimizing their weights to minimize contrastive loss functions that measure how well the representations capture some underlying structure of the data. However, these models often assume a linear projection of input features into high dimensional spaces, which limits their ability to model complex nonlinear relationships. This paper presents a novel method that combines robust contrastive learning and nonlinear independent component analysis (ICA) to address this issue. By incorporating an ICA module within the DNN architecture, we are able to perform nonlinear dimensionality reduction while preserving information important for generating good representations. We show experimental results on both synthetic datasets and real world problems including image classification, face verification, and speaker identification. Our approach achieves significantly better performance compared to existing methods and demonstrates the potential benefits of integrating powerful linear unsupervised preprocessing techniques within end-to-end trainable systems. Additionally, our method is shown to be robust to outliers present in the training data which is crucial for handling real-world noise and variations.",1
"Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.",0
"In recent years there has been significant interest in developing machine learning models that can continually learn from data streams without explicit supervision. This task is challenging due to the large amounts of data involved, as well as the need for fast adaptation to changes in the underlying patterns present in these data streams. Recent research in unsupervised representation learning has shown promise in addressing some of these issues, by allowing models to automatically discover meaningful representations that capture important relationships among inputs over time. However, current methods still have limitations related to their ability to scale to larger datasets and adapt quickly enough to changing data distributions. Our work proposes a new approach called “Continual Unsupervised Representation Learning” (CURL) which addresses these shortcomings by combining techniques from deep reinforcement learning and generative adversarial networks. We evaluate CURL on several benchmark tasks and show that our method achieves state-of-the-art performance while outperforming previous unsupervised representation learning approaches in terms of scalability and dynamic adaptability. We further provide analyses on the behavior of CURL during training, highlighting key insights into how our model learns representations and generates predictions incrementally. By advancing the field of unsupervised representation learning, we believe that CURL represents a step towards building more generalizable artificial intelligence systems capable of continuously operating in complex environments.",1
"In this work, we move beyond the traditional complex-valued representations, introducing more expressive hypercomplex representations to model entities and relations for knowledge graph embeddings. More specifically, quaternion embeddings, hypercomplex-valued embeddings with three imaginary components, are utilized to represent entities. Relations are modelled as rotations in the quaternion space. The advantages of the proposed approach are: (1) Latent inter-dependencies (between all components) are aptly captured with Hamilton product, encouraging a more compact interaction between entities and relations; (2) Quaternions enable expressive rotation in four-dimensional space and have more degree of freedom than rotation in complex plane; (3) The proposed framework is a generalization of ComplEx on hypercomplex space while offering better geometrical interpretations, concurrently satisfying the key desiderata of relational representation learning (i.e., modeling symmetry, anti-symmetry and inversion). Experimental results demonstrate that our method achieves state-of-the-art performance on four well-established knowledge graph completion benchmarks.",0
"This is a technical research paper that presents a new method called ""Quaternion Knowledge Graph (KG) embeddings"" for better representing complex relationships and associations among entities in large knowledge graphs. The proposed approach uses quaternion algebra which can capture higher order correlations than traditional methods using vectors alone. Additionally, we introduce three different quaternion operations based on the axioms of group theory: addition/subtraction, scalar multiplication, and conjugation/inversion. These operations enable us to model symmetric, anti-symmetric, and reflexive relationships respectively within KGs. We perform extensive experiments on several benchmark datasets including FB15k-237, WN18RR, and YAGO64K-SP, demonstrating state-of-the-art performance compared against other baseline models such as DistMult, ComplEx, ConvE, and AnalogyMATCH. Finally, we showcase two real-world applications where our proposed model successfully outperforms existing solutions, namely recommender systems and question answering. Our work pushes forward the frontier of utilizing hypercomplex numbers in machine learning and deepens the understanding of the mathematical properties underlying knowledge graphs. This study has potential implications in data mining, natural language processing, computer vision, robotics, social network analysis, neuroscience, quantum mechanics, cryptography, game theory, anthropology, sociology, psychology, linguistics, philosophy, and many other fields dealing with structured information.",1
"Deep generative models for graphs have shown great promise in the area of drug design, but have so far found little application beyond generating graph-structured molecules. In this work, we demonstrate a proof of concept for the challenging task of road network extraction from image data. This task can be framed as image-conditioned graph generation, for which we develop the Generative Graph Transformer (GGT), a deep autoregressive model that makes use of attention mechanisms for image conditioning and the recurrent generation of graphs. We benchmark GGT on the application of road network extraction from semantic segmentation data. For this, we introduce the Toulouse Road Network dataset, based on real-world publicly-available data. We further propose the StreetMover distance: a metric based on the Sinkhorn distance for effectively evaluating the quality of road network generation. The code and dataset are publicly available.",0
"In the era of big data, computer vision has become increasingly important due to our ability to collect large amounts of visual data from cameras and other sources. One particular area where image analysis has proven useful is road network extraction. Traditional methods of extracting roads involve laborious manual processes such as digitizing maps and surveying landscapes. However, automation techniques have been developed over time to aid in extracting roads more efficiently. This research proposes the use of graph generation based on images to improve upon traditional techniques used in road network extraction. By analyzing digital satellite imagery along with LiDAR point clouds, we can generate a high-quality representation of real-world objects. By applying machine learning algorithms to these data sources, we can create a system that accurately detects and tracks roads. Additionally, this method allows us to take advantage of existing open source software libraries such as OpenStreetMap for further processing. Overall, image conditioned graph generation holds great potential for improving the accuracy and speed of road network extraction through effective integration of multiple types of data.",1
"We explore the impact of learning paradigms on training deep neural networks for the Travelling Salesman Problem. We design controlled experiments to train supervised learning (SL) and reinforcement learning (RL) models on fixed graph sizes up to 100 nodes, and evaluate them on variable sized graphs up to 500 nodes. Beyond not needing labelled data, our results reveal favorable properties of RL over SL: RL training leads to better emergent generalization to variable graph sizes and is a key component for learning scale-invariant solvers for novel combinatorial problems.",0
"Solving the Traveling Salesman Problem (TSP) can be computationally difficult due to exponential growth in solution space. For a given number of cities, each visiting every city once before returning home, the TSP asks whether there exists a route that visits every destination while minimizing overall distance traveled. In ""On Learning Paradigms for the Traveling Salesman Problem,"" we propose two learning methods capable of solving TSP instances exceedingly larger than those solved by current optimization techniques. Our first method trains an artificial neural network using gradient descent on problem features extracted from candidate solutions generated via backpropagation. Our second approach uses reinforcement learning to guide random search through the solution space. We demonstrate these approaches' effectiveness using well-known benchmarks across several metrics, showing performance competitive with state-of-the-art commercial solvers. Our results suggest that machine learning may enable more effective traversal of large search spaces when traditional optimization fails. This work opens new directions for research at the intersection of combinatorial optimization and artificial intelligence.",1
"A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite---they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. We release an implementation of our method at https://github.com/khurramjaved96/mrcl .",0
"This paper introduces a novel meta-learning algorithm that enables machines to continuously learn new tasks without forgetting previously learned ones. Our approach uses deep neural networks to represent both task and model parameters as vectors. We first pretrain these models on diverse datasets from each class. Then our algorithm learns how to solve new tasks by mapping them onto known tasks using linear regression. Finally, we fine tune a separate base model with random initialization on top of this map. Experimental results show better performance than baseline methods across three different architectures on ten different benchmarks. Code: <https://github.com/facebookresearch/meta-learning-representations> .",1
"We introduce a framework for dynamic adversarial discovery of information (DADI), motivated by a scenario where information (a feature set) is used by third parties with unknown objectives. We train a reinforcement learning agent to sequentially acquire a subset of the information while balancing accuracy and fairness of predictors downstream. Based on the set of already acquired features, the agent decides dynamically to either collect more information from the set of available features or to stop and predict using the information that is currently available. Building on previous work exploring adversarial representation learning, we attain group fairness (demographic parity) by rewarding the agent with the adversary's loss, computed over the final feature set. Importantly, however, the framework provides a more general starting point for fair or private dynamic information discovery. Finally, we demonstrate empirically, using two real-world datasets, that we can trade-off fairness and predictive performance",0
"Abstract: In today's world where data privacy is becoming increasingly important, there is a need for efficient mechanisms that can ensure fairness while extracting insights from sensitive datasets. Traditional methods suffer from limitations such as overfitting, lack of interpretability, scalability issues, etc. This work proposes a novel model called ""DADI"" (Dynamic discovery of fair information using adversarial reinforcement learning) which addresses these challenges by utilizing Generative Adversarial Networks (GANs). Our method enables dynamic exploration of high-dimensional spaces by incorporating the GAIL algorithm into GAN training dynamics. We propose new objective functions based on Kullback–Leibler divergence minimization and maximization, which guarantee stability and convergence during optimization. Experiments showcase our approach outperforms state-of-the-art techniques across multiple benchmark tasks. Furthermore, we analyze how DADI achieves robustness, generalization, and privacy preservation under different settings and configurations. With its ability to balance exploration-exploitation tradeoffs adaptively, our framework has great potential for real-world applications ranging from healthcare informatics, finance, market research, to social network analysis among others. Overall, DADI presents a breakthrough advancement towards solving one of the most critical problems faced by modern machine learning practitioners.",1
"Gaussian processes are flexible function approximators, with inductive biases controlled by a covariance kernel. Learning the kernel is the key to representation learning and strong predictive performance. In this paper, we develop functional kernel learning (FKL) to directly infer functional posteriors over kernels. In particular, we place a transformed Gaussian process over a spectral density, to induce a non-parametric distribution over kernel functions. The resulting approach enables learning of rich representations, with support for any stationary kernel, uncertainty over the values of the kernel, and an interpretable specification of a prior directly over kernels, without requiring sophisticated initialization or manual intervention. We perform inference through elliptical slice sampling, which is especially well suited to marginalizing posteriors with the strongly correlated priors typical to function space modelling. We develop our approach for non-uniform, large-scale, multi-task, and multidimensional data, and show promising performance in a wide range of settings, including interpolation, extrapolation, and kernel recovery experiments.",0
"In recent years, there has been growing interest in studying function spaces and distributions associated with kernels. These mathematical objects have applications in fields ranging from partial differential equations to harmonic analysis and operator theory. This paper investigates the properties and representations of function space distributions defined on a kernel and their relationship with classical distributions. We explore various types of kernels and study how they can influence the behavior of function spaces and corresponding distributions. Our results provide new insights into the structure and representation of these distributions and may find application in diverse areas of mathematics and theoretical physics. The work presented here offers a comprehensive treatment of function space distributions based on kernels, including several examples and explicit calculations that illustrate key concepts and results. Overall, this paper contributes to our understanding of distribution theory and serves as a foundation for future research in the field.",1
"One of the challenges in training generative models such as the variational auto encoder (VAE) is avoiding posterior collapse. When the generator has too much capacity, it is prone to ignoring latent code. This problem is exacerbated when the dataset is small, and the latent dimension is high. The root of the problem is the ELBO objective, specifically the Kullback-Leibler (KL) divergence term in objective function \citep{zhao2019infovae}. This paper proposes a new objective function to replace the KL term with one that emulates the maximum mean discrepancy (MMD) objective. It also introduces a new technique, named latent clipping, that is used to control distance between samples in latent space. A probabilistic autoencoder model, named $\mu$-VAE, is designed and trained on MNIST and MNIST Fashion datasets, using the new objective function and is shown to outperform models trained with ELBO and $\beta$-VAE objective. The $\mu$-VAE is less prone to posterior collapse, and can generate reconstructions and new samples in good quality. Latent representations learned by $\mu$-VAE are shown to be good and can be used for downstream tasks such as classification.",0
"This paper proposes a new method for bridging the Evidence Lower BOund (ELBO) and Mean Mean Divergence (MMD). Current methods often struggle to balance these two metrics, leading to suboptimal results in variational inference models. Our approach uses a new regularization term that encourages both high evidence density and low diversity from the prior distribution. We demonstrate through experiments on real-world datasets that our method leads to improved accuracy over existing approaches while maintaining computational efficiency. Furthermore, we provide theoretical analysis showing how our regularizer achieves a better tradeoff between the competing objectives compared to existing techniques. Overall, our work provides a promising direction for future research into the design of efficient Bayesian neural networks.",1
"We examine Generative Adversarial Networks (GANs) through the lens of deep Energy Based Models (EBMs), with the goal of exploiting the density model that follows from this formulation. In contrast to a traditional view where the discriminator learns a constant function when reaching convergence, here we show that it can provide useful information for downstream tasks, e.g., feature extraction for classification. To be concrete, in the EBM formulation, the discriminator learns an unnormalized density function (i.e., the negative energy term) that characterizes the data manifold. We propose to evaluate both the generator and the discriminator by deriving corresponding Fisher Score and Fisher Information from the EBM. We show that by assuming that the generated examples form an estimate of the learned density, both the Fisher Information and the normalized Fisher Vectors are easy to compute. We also show that we are able to derive a distance metric between examples and between sets of examples. We conduct experiments showing that the GAN-induced Fisher Vectors demonstrate competitive performance as unsupervised feature extractors for classification and perceptual similarity tasks. Code is available at \url{https://github.com/apple/ml-afv}.",0
"Advances in deep learning have enabled the development of powerful representations that can encode highlevel abstractions from large amounts of data. However, these representations often require extensive supervision which limits their use on new tasks and domains. Recently, unsupervised representation learning has been proposed as a promising alternative approach by pretraining models on large datasets without any explicit task labels. Unfortunately, most existing methods suffer from two major drawbacks: they lack clear theoretical underpinnings, and are prone to overfitting due to the strong inductive biases of modern neural networks. In our work we address both problems by introducing a novel algorithmic framework based on adversarial training with Fisher vectors (ATFV). Our method provides a simple yet effective solution to learn good representations in an unsupervised manner while offering key advantages such as scalability, stability, flexibility and interpretability. We demonstrate the effectiveness of our approach through comprehensive experiments across multiple benchmarks including image classification, object detection, and reinforcement learning where ATFV consistently outperforms stateoftheart alternatives. Overall, our results suggest that unsupervised representation learning can indeed achieve competitive performance compared to strongly supervised counterparts given the appropriate algorithms and architectures.",1
"RGB-Infrared (IR) person re-identification is an important and challenging task due to large cross-modality variations between RGB and IR images. Most conventional approaches aim to bridge the cross-modality gap with feature alignment by feature representation learning. Different from existing methods, in this paper, we propose a novel and end-to-end Alignment Generative Adversarial Network (AlignGAN) for the RGB-IR RE-ID task. The proposed model enjoys several merits. First, it can exploit pixel alignment and feature alignment jointly. To the best of our knowledge, this is the first work to model the two alignment strategies jointly for the RGB-IR RE-ID problem. Second, the proposed model consists of a pixel generator, a feature generator, and a joint discriminator. By playing a min-max game among the three components, our model is able to not only alleviate the cross-modality and intra-modality variations but also learn identity-consistent features. Extensive experimental results on two standard benchmarks demonstrate that the proposed model performs favorably against state-of-the-art methods. Especially, on SYSU-MM01 dataset, our model can achieve an absolute gain of 15.4% and 12.9% in terms of Rank-1 and mAP.",0
"This paper presents a novel approach to person re-identification across different modalities such as RGB and infrared images. Current methods rely on extracting features from each modality separately and then comparing them using feature matching techniques. However, these approaches can suffer from limitations due to differences in appearance and resolution between the two image types. Our proposed method addresses these issues by jointly aligning pixel and feature spaces between the RGB and infrared data, resulting in improved cross-modality matching performance. We evaluate our approach on several public datasets and demonstrate significant improvements over state-of-the-art methods.",1
"Learning from graph-structured data is an important task in machine learning and artificial intelligence, for which Graph Neural Networks (GNNs) have shown great promise. Motivated by recent advances in geometric representation learning, we propose a novel GNN architecture for learning representations on Riemannian manifolds with differentiable exponential and logarithmic maps. We develop a scalable algorithm for modeling the structural properties of graphs, comparing Euclidean and hyperbolic geometry. In our experiments, we show that hyperbolic GNNs can lead to substantial improvements on various benchmark datasets.",0
"Here's a possible abstract:  Hyperbolic graph neural networks (HGNNs) have recently emerged as a promising approach for modeling complex relationships on non-Euclidean spaces such as graphs. In contrast to traditional Euclidean deep learning models that operate on linear vector inputs, HGNNs can capture intricate patterns of dependencies across multiple scales of resolution within nonlinear geometric structures. This allows them to achieve state-of-the-art performance in several challenging tasks involving data residing on graphs, including recommendation systems, computer vision, natural language processing, biological informatics, and physics simulations. We provide a detailed analysis of HGNN architectures, discuss their strengths and limitations, compare their efficacy against alternative approaches, and present novel techniques aimed at improving robustness, interpretability, and scalability. Our findings demonstrate the potential value of hyperbolic geometry as a powerful mathematical framework underpinning advanced artificial intelligence systems designed for real-world applications beyond point clouds. Overall, this work opens up exciting new research opportunities in the area of deep learning on nonstandard geometries.",1
"Deep neural networks require collecting and annotating large amounts of data to train successfully. In order to alleviate the annotation bottleneck, we propose a novel self-supervised representation learning approach for spatiotemporal features extracted from videos. We introduce Skip-Clip, a method that utilizes temporal coherence in videos, by training a deep model for future clip order ranking conditioned on a context clip as a surrogate objective for video future prediction. We show that features learned using our method are generalizable and transfer strongly to downstream tasks. For action recognition on the UCF101 dataset, we obtain 51.8% improvement over random initialization and outperform models initialized using inflated ImageNet parameters. Skip-Clip also achieves results competitive with state-of-the-art self-supervision methods.",0
"This is an extremely interesting research paper that proposes a new method for training deep learning models called Skip-Clip. The authors use large datasets like ActivityNet-v1.3 to pretrain their model using future clip ranking as a self supervisory signal. Then they fine-tune on action recognition tasks with limited labeled data. They evaluate the effectiveness of this approach against other state-of-the-art methods and find that theirs outperforms all existing ones. With the ability to learn representations from unlabeled videos without human annotation, skip-clip has strong potential for real world applications where vast amounts of video data are generated daily but labeling can be costly and time consuming. Overall, I recommend reading this paper if you’re interested in computer vision research!",1
"Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality (ID) of data-representations, i.e. the minimal number of parameters needed to describe a representation. We find that, in a trained network, the ID is orders of magnitude smaller than the number of units in each layer. Across layers, the ID first increases and then progressively decreases in the final layers. Remarkably, the ID of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g., with principal component analysis), nor in representations that had been artificially linearized. They are neither found in untrained networks, nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional, but not necessarily flat manifolds.",0
"Deep learning has revolutionized computer vision tasks by producing state-of-the-art results across numerous applications such as object recognition, semantic segmentation, and image generation. Its success can largely be attributed to the use of high capacity models and large amounts of training data. However, there remains limited understanding of how these models capture visual representations, especially regarding their intrinsic dimensionality. This work investigates the intrinsic dimension of data representations learned by deep neural networks (DNNs). We propose two new metrics that measure the spread and density of representation vectors in terms of human perceptual similarity judgments. Our experiments on several DNN architectures demonstrate significant reductions in both spread and density compared to pixel space, highlighting the importance of accounting for intrinsic dimensions when designing and analyzing DNN systems. Furthermore, we show that capturing lower intrinsic dimensions leads to improved generalization performance, suggesting a tradeoff between model expressivity and efficiency. Overall, our findings provide novel insights into the nature of data representations learned by DNNs, emphasizing the need for new metrics beyond traditional complexity measures like FLOPS and number of parameters. We anticipate this study to open up future research directions for better understanding and improving deep learning systems, particularly those involving unsupervised learning and transfer learning.",1
"Deep representation learning using triplet network for classification suffers from a lack of theoretical foundation and difficulty in tuning both the network and classifiers for performance. To address the problem, local-margin triplet loss along with local positive and negative mining strategy is proposed with theory on how the strategy integrate nearest-neighbor hyper-parameter with triplet learning to increase subsequent classification performance. Results in experiments with 2 public datasets, MNIST and Cifar-10, and 2 small medical image datasets demonstrate that proposed strategy outperforms end-to-end softmax and typical triplet loss in settings without data augmentation while maintaining utility of transferable feature for related tasks. The method serves as a good performance baseline where end-to-end methods encounter difficulties such as small sample data with limited allowable data augmentation.",0
"In this study we propose neighborhood watch triplets, a novel approach that utilizes local margin triplets and sampling strategy as a representation learning scheme. By carefully selecting the hardest negative samples, our method significantly improves upon prior work which simply sampled random pairs of positives and negatives. Our results surpass state-of-the-art results on several benchmark datasets commonly used for image classification problems using k-nearest neighbor classifiers, demonstrating the effectiveness of our proposed framework. Furthermore, by training models with both mini-batch gradient descent and Adam optimizers, we empirically show consistent gains across different optimization schemes which has important implications for representation learning under limited computational budgets. Overall, these findings represent significant progress towards the development of generalizable methods for neighborhood watch triplets that can improve the efficiency of image recognition tasks without sacrificing accuracy.",1
"Studies show that the representations learned by deep neural networks can be transferred to similar prediction tasks in other domains for which we do not have enough labeled data. However, as we transition to higher layers in the model, the representations become more task-specific and less generalizable. Recent research on deep domain adaptation proposed to mitigate this problem by forcing the deep model to learn more transferable feature representations across domains. This is achieved by incorporating domain adaptation methods into deep learning pipeline. The majority of existing models learn the transferable feature representations which are highly correlated with the outcome. However, correlations are not always transferable. In this paper, we propose a novel deep causal representation learning framework for unsupervised domain adaptation, in which we propose to learn domain-invariant causal representations of the input from the source domain. We simulate a virtual target domain using reweighted samples from the source domain and estimate the causal effect of features on the outcomes. The extensive comparative study demonstrates the strengths of the proposed model for unsupervised domain adaptation via causal representations.",0
"Title: ""Deep Causal Representation Learning for Unsupervised Domain Adaptation""  Abstract: This paper presents a novel deep learning approach called deep casual representation learning (DCCL) that can effectively solve the problem of unsupervised domain adaption (UDA). UDA refers to the scenario where a machine learning model trained on a source dataset has poor performance on a target dataset from a different distribution, which is a common challenge in real-world applications such as computer vision tasks involving images taken under varying lighting conditions. DCCL addresses this issue by learning a latent variable representation space that captures both the input data distributions and their corresponding interventional relationships between variables. By doing so, DCCL enables the discovery of meaningful hidden factors underlying complex correlations and allows the learned representations to generalize well across domains, resulting in significant improvements over existing methods on multiple benchmark datasets. The proposed framework combines recent advances in graph neural networks (GNNs) and generative adversarial imitation learning (GAIL), providing new insights into both areas and highlighting the potential of using graph models for causality reasoning. Overall, our work opens up exciting opportunities for applying deep causal models to other challenges involving distribution shifts, including semi-supervised and few-shot learning problems.",1
"Modern neural network training relies on piece-wise (sub-)differentiable functions in order to use backpropagation to update model parameters. In this work, we introduce a novel method to allow simple non-differentiable functions at intermediary layers of deep neural networks. We do so by training with a differentiable approximation bridge (DAB) neural network which approximates the non-differentiable forward function and provides gradient updates during backpropagation. We present strong empirical results (performing over 600 experiments) in four different domains: unsupervised (image) representation learning, variational (image) density estimation, image classification, and sequence sorting to demonstrate that our proposed method improves state of the art performance. We demonstrate that training with DAB aided discrete non-differentiable functions improves image reconstruction quality and posterior linear separability by 10% against the Gumbel-Softmax relaxed estimator [37, 26] as well as providing a 9% improvement in the test variational lower bound in comparison to the state of the art RELAX [16] discrete estimator. We also observe an accuracy improvement of 77% in neural sequence sorting and a 25% improvement against the straight-through estimator [5] in an image classification setting. The DAB network is not used for inference and expands the class of functions that are usable in neural networks.",0
"In recent years, advances in deep learning have been driven by improvements in representation models such as GPTs (Generative Pre-trained Transformer) which can generate complex data structures like text, images, sounds, music, and videos from latent codes. These advancements allow us to use continuous representations for many challenging problems. However, discrete distributions over a finite number of modes remain essential due to several reasons. Firstly, most domains are naturally defined on a discrete space. Secondly, we often need to perform operations that require combinatorial search like semantic parsing, program synthesis, etc., thus making our problem domain fundamentally discrete. Finally, we may also want to optimize certain functions directly with respect to their discrete counterparts since they provide important inductive biases. In the pursuit of using powerful continuous embeddings in combination with the benefits provided by discrete domains, we propose two novel techniques bridging the gap between both worlds while maintaining differentiability end-to-end. Our first technique works at inference time via sampling followed by linear remapping, and our second method operates during training via gradient scaling. Both bridge methods are capable of retaining important structural properties from continuous spaces into discrete ones while ensuring computational efficiency owing to different approximations tailored towards specific scenarios. Experimental results across six diverse application areas show improved results compared to previous methods involving continuous or discrete representations alone. We hope these new concepts and approaches aid the community in designing better representati",1
"Graph Attention Networks (GATs) are the state-of-the-art neural architecture for representation learning with graphs. GATs learn attention functions that assign weights to nodes so that different nodes have different influences in the feature aggregation steps. In practice, however, induced attention functions are prone to over-fitting due to the increasing number of parameters and the lack of direct supervision on attention weights. GATs also suffer from over-smoothing at the decision boundary of nodes. Here we propose a framework to address their weaknesses via margin-based constraints on attention during training. We first theoretically demonstrate the over-smoothing behavior of GATs and then develop an approach using constraint on the attention weights according to the class boundary and feature aggregation pattern. Furthermore, to alleviate the over-fitting problem, we propose additional constraints on the graph structure. Extensive experiments and ablation studies on common benchmark datasets demonstrate the effectiveness of our method, which leads to significant improvements over the previous state-of-the-art graph attention methods on all datasets.",0
"Our work focuses on graph attention networks (GAT), which have been shown to outperform traditional convolutional neural networks (CNN) and recurrent neural networks (RNN) on several benchmark datasets. GAT’s success lies in their ability to weigh each element of a feature vector by its importance among all other elements, allowing them to capture relationships and dependencies within graphs that CNN and RNN struggle with. However, training GAT models remains challenging due to instability issues and slow convergence rates during optimization. To address these concerns, our approach proposes using large margin constraints to improve both stability and efficiency in model training. By incorporating large margin constraints into the attention mechanism, we regularize the network towards more confident predictions while encouraging overfitting prevention through discouraged ambiguous outputs. Results from extensive experiments demonstrate significant improvements across multiple metrics compared against various baseline methods such as GCNII, SAGNet and VR-GCN++ on a range of benchmark datasets. These results suggest that large margin-based constraints can provide a simple yet effective methodology to enhance the performance of GAT’s without requiring fundamental architectural changes. In summary, our study contributes to better understanding of how to train graph attention networks more effectively, paving the pathway for future advancements and applications in computational biology and natural language processing.",1
"Domain adaptation aims to exploit the knowledge in source domain to promote the learning tasks in target domain, which plays a critical role in real-world applications. Recently, lots of deep learning approaches based on autoencoders have achieved a significance performance in domain adaptation. However, most existing methods focus on minimizing the distribution divergence by putting the source and target data together to learn global feature representations, while they do not consider the local relationship between instances in the same category from different domains. To address this problem, we propose a novel Semi-Supervised Representation Learning framework via Dual Autoencoders for domain adaptation, named SSRLDA. More specifically, we extract richer feature representations by learning the global and local feature representations simultaneously using two novel autoencoders, which are referred to as marginalized denoising autoencoder with adaptation distribution (MDAad) and multi-class marginalized denoising autoencoder (MMDA) respectively. Meanwhile, we make full use of label information to optimize feature representations. Experimental results show that our proposed approach outperforms several state-of-the-art baseline methods.",0
"Title: Improving Domain Adaptation through Dual Autoencoder Representations Authors: J. Sun, Y. Cui, Z. Liu, Q. Chen, X. Wang Abstract Machine learning models trained on large datasets can often struggle to generalize well across different domains due to differences in data distribution, quality, and labeling inconsistencies. To address these challenges, semi-supervised domain adaptation methods have been proposed that utilize labeled examples from one source domain as well as unlabeled examples from another target domain to improve model performance. In this work, we propose a novel approach based on dual autoencoders for representing source and target domains. Our method leverages both source and target feature spaces during training, enabling better alignment of the two distributions while preserving their unique characteristics. Experimental results demonstrate significant improvements over existing state-of-the-art methods on several benchmark datasets for image classification tasks, including Office-31 and ImageCLEF-DA.",1
"Existing methods for person re-identification (Re-ID) are mostly based on supervised learning which requires numerous manually labeled samples across all camera views for training. Such a paradigm suffers the scalability issue since in real-world Re-ID application, it is difficult to exhaustively label abundant identities over multiple disjoint camera views. To this end, we propose a progressive deep learning method for unsupervised person Re-ID in the wild by Tracklet Association with Spatio-Temporal Regularization (TASTR). In our approach, we first collect tracklet data within each camera by automatic person detection and tracking. Then, an initial Re-ID model is trained based on within-camera triplet construction for person representation learning. After that, based on the person visual feature and spatio-temporal constraint, we associate cross-camera tracklets to generate cross-camera triplets and update the Re-ID model. Lastly, with the refined Re-ID model, better visual feature of person can be extracted, which further promote the association of cross-camera tracklets. The last two steps are iterated multiple times to progressively upgrade the Re-ID model.",0
"This research proposes a new approach to unsupervised person re-identification using tracklet association with spatio-temporal regularization. The proposed method combines multiple cues from both spatial and temporal domains to improve the accuracy of re- identification across different cameras and viewpoints. By modeling the appearance variations within each camera view as well as capturing intra-camera relationships among instances, our method outperforms state-of-the-art methods on several benchmark datasets while using only unlabeled data for training. Additionally, we address the challenges posed by ambiguities arising from occlusions, similar clothes, pose changes, background clutter, etc., by learning distinctive features that generalize better across these conditions. Our contributions lie in effectively bridging the gap between short-term visual tracking and large-scale person re-identification under realistic scenarios without requiring labeled data.",1
"Revealing latent structure in data is an active field of research, having introduced exciting technologies such as variational autoencoders and adversarial networks, and is essential to push machine learning towards unsupervised knowledge discovery. However, a major challenge is the lack of suitable benchmarks for an objective and quantitative evaluation of learned representations. To address this issue we introduce Morpho-MNIST, a framework that aims to answer: ""to what extent has my model learned to represent specific factors of variation in the data?"" We extend the popular MNIST dataset by adding a morphometric analysis enabling quantitative comparison of trained models, identification of the roles of latent variables, and characterisation of sample diversity. We further propose a set of quantifiable perturbations to assess the performance of unsupervised and supervised methods on challenging tasks such as outlier detection and domain adaptation. Data and code are available at https://github.com/dccastro/Morpho-MNIST.",0
"A new dataset called ""Morpho-MNIST"" has been introduced as a benchmark for evaluation of representation learning algorithms on natural images. This dataset consists of grayscale handwritten digits from a variety of sources which have been distorted by stochastic noise processes that simulate varying degrees of degradation. Previous evaluations using these data have relied heavily on qualitative methods; here we extend previous work into quantitative assessments and diagnostic tools necessary to effectively evaluate modern representations learned from image datasets like Morpho-MNIST. These metrics address the quality of representation and generalization performance under challenging conditions such as random shifts and rotations. We conclude that while previous models can perform well on average across all classes, there remains room for improvement in terms of generalizing under more difficult degraded versions of the same inputs that were used during training (particularly when there is rotation). Moreover, our analysis provides insight into how different approaches to pretraining may affect performance on tasks where some levels of distortion/degradation occur randomly.",1
"The estimation of an f-divergence between two probability distributions based on samples is a fundamental problem in statistics and machine learning. Most works study this problem under very weak assumptions, in which case it is provably hard. We consider the case of stronger structural assumptions that are commonly satisfied in modern machine learning, including representation learning and generative modelling with autoencoder architectures. Under these assumptions we propose and study an estimator that can be easily implemented, works well in high dimensions, and enjoys faster rates of convergence. We verify the behavior of our estimator empirically in both synthetic and real-data experiments, and discuss its direct implications for total correlation, entropy, and mutual information estimation.",0
"This looks like an interesting approach to estimating f-divergence measures using non-asymptotic confidence intervals. The authors demonstrate how their method works on simulated data and show that it performs well compared to existing methods. They also discuss potential applications for their method in real-world settings such as bioinformatics and image processing. Overall, this seems like a valuable contribution to the field.",1
In this work we approach the task of learning multilingual word representations in an offline manner by fitting a generative latent variable model to a multilingual dictionary. We model equivalent words in different languages as different views of the same word generated by a common latent variable representing their latent lexical meaning. We explore the task of alignment by querying the fitted model for multilingual embeddings achieving competitive results across a variety of tasks. The proposed model is robust to noise in the embedding space making it a suitable method for distributed representations learned from noisy corpora.,0
"Multilingual factor analysis (MFA) is a method used to analyze linguistic data from multiple languages simultaneously. By identifying shared underlying factors across different languages, MFA can provide insights into both language universals and crosslinguistic differences. This study aimed to investigate how MFA can contribute to our understanding of linguistics by comparing two types of data: language corpora obtained from speakers of English as a second language (ESL), and native speaker language data collected through introspection tasks. Results showed that while ESL data tended to reveal more variation due to individual differences among speakers, native speaker data was characterized by stronger universal tendencies, likely reflecting greater exposure to other native speakers and access to higher levels of social cognition. Overall, this research demonstrates the potential utility of MFA for studying language structure, use, and acquisition across diverse populations.",1
"Endowing robots with human-like physical reasoning abilities remains challenging. We argue that existing methods often disregard spatio-temporal relations and by using Graph Neural Networks (GNNs) that incorporate a relational inductive bias, we can shift the learning process towards exploiting relations. In this work, we learn action-conditional forward dynamics models of a simulated manipulation task from visual observations involving cluttered and irregularly shaped objects. We investigate two GNN approaches and empirically assess their capability to generalize to scenarios with novel and an increasing number of objects. The first, Graph Networks (GN) based approach, considers explicitly defined edge attributes and not only does it consistently underperform an auto-encoder baseline that we modified to predict future states, our results indicate how different edge attributes can significantly influence the predictions. Consequently, we develop the Auto-Predictor that does not rely on explicitly defined edge attributes. It outperforms the baseline and the GN-based models. Overall, our results show the sensitivity of GNN-based approaches to the task representation, the efficacy of relational inductive biases and advocate choosing lightweight approaches that implicitly reason about relations over ones that leave these decisions to human designers.",0
"This paper presents a method for learning dynamics models of rigid objects that exploits the relational inductive biases present in state-of-the-art deep neural networks. Our approach uses a convolutional neural network (CNN) to learn a visual model of object motion from raw image sequences. We then use a recurrent neural network (RNN) to predict future frames based on the current frame and past predictions. By incorporating temporal relationships into our model, we can capture more complex patterns of motion and produce better results than previous methods. Experiments demonstrate the effectiveness of our approach, achieving state-of-the-art performance on several benchmark datasets.",1
"Multimodalities provide promising performance than unimodality in most tasks. However, learning the semantic of the representations from multimodalities efficiently is extremely challenging. To tackle this, we propose the Transformer based Cross-modal Translator (TCT) to learn unimodal sequence representations by translating from other related multimodal sequences on a supervised learning method. Combined TCT with Multimodal Transformer Network (MTN), we evaluate MTN-TCT on the video-grounded dialogue which uses multimodality. The proposed method reports new state-of-the-art performance on video-grounded dialogue which indicates representations learned by TCT are more semantics compared to directly use unimodality.",0
"Abstract Multimodal sequence representation (MMSR) has recently emerged as a promising area of research due to its wide range of applications, including video summarization, multimedia retrieval, and human-machine interaction. One key challenge in MMSR is how to effectively leverage multiple modalities, such as image and text, to learn robust representations that capture important patterns across domains. In this work, we propose a novel cross-supervised learning method called TCT (Text-to-Concept Transfer), which leverages both modality-specific supervision and transfer from one modality to another via concept learning. Our approach outperforms state-of-the-art methods on two benchmark datasets and demonstrates better generalizability under few-shot settings. We also provide ablation studies to analyze the contribution of each component, including data augmentations and transfer modules. Overall, our results suggest that incorporating cross-modality knowledge can significantly improve the quality of learned MMSR models.",1
"Reconstructing visual stimulus (image) only from human brain activity measured with functional Magnetic Resonance Imaging (fMRI) is a significant and meaningful task in Human-AI collaboration. However, the inconsistent distribution and representation between fMRI signals and visual images cause the heterogeneity gap. Moreover, the fMRI data is often extremely high-dimensional and contains a lot of visually-irrelevant information. Existing methods generally suffer from these issues so that a satisfactory reconstruction is still challenging. In this paper, we show that it is possible to overcome these challenges by learning visually-guided cognitive latent representations from the fMRI signals, and inversely decoding them to the image stimuli. The resulting framework is called Dual-Variational Autoencoder/ Generative Adversarial Network (D-VAE/GAN), which combines the advantages of adversarial representation learning with knowledge distillation. In addition, we introduce a novel three-stage learning approach which enables the (cognitive) encoder to gradually distill useful knowledge from the paired (visual) encoder during the learning process. Extensive experimental results on both artificial and natural images have demonstrated that our method could achieve surprisingly good results and outperform all other alternatives.",0
"In recent years, there has been significant progress in developing methods that can reconstruct images directly from brain activity using neural decoders. These techniques typically rely on data-driven models trained on large amounts of labeled neuroimaging data. However, these approaches suffer from several limitations, including their reliance on massive amounts of annotated training data and their limited generalization ability across different tasks and subjects. To address these challenges, we propose a novel approach that leverages visually-guided cognitive representation (VGCR) and adversarial learning. VGCR represents complex visual stimuli using a compact set of meaningful features that capture both low-level visual attributes and high-level semantic concepts. By combining these representations with adversarial training, our model learns to generate images that are both perceptually convincing and semantically relevant. We evaluate our method on multiple datasets and demonstrate its superior performance compared to state-of-the-art baselines. Our results show that the proposed framework offers a promising alternative for reconstructing perceived images from brain activity, opening up new possibilities for neuroscientific research and clinical applications.",1
"We propose a condition-adaptive representation learning framework for the driver drowsiness detection based on 3D-deep convolutional neural network. The proposed framework consists of four models: spatio-temporal representation learning, scene condition understanding, feature fusion, and drowsiness detection. The spatio-temporal representation learning extracts features that can describe motions and appearances in video simultaneously. The scene condition understanding classifies the scene conditions related to various conditions about the drivers and driving situations such as statuses of wearing glasses, illumination condition of driving, and motion of facial elements such as head, eye, and mouth. The feature fusion generates a condition-adaptive representation using two features extracted from above models. The detection model recognizes drivers drowsiness status using the condition-adaptive representation. The condition-adaptive representation learning framework can extract more discriminative features focusing on each scene condition than the general representation so that the drowsiness detection method can provide more accurate results for the various driving situations. The proposed framework is evaluated with the NTHU Drowsy Driver Detection video dataset. The experimental results show that our framework outperforms the existing drowsiness detection methods based on visual analysis.",0
"This research presents a novel approach to detecting driver drowsiness through the use of condition-adaptive representation learning frameworks. By analyzing driving patterns and correlating them with physiological measurements such as heart rate variability (HRV), respiration rates, and body movement data, our method can accurately predict levels of driver alertness under different conditions. Our framework adapts to changes in environment, traffic density, weather, and other variables, allowing for highly precise and accurate predictions. We evaluate our method on real-world datasets, demonstrating its effectiveness and superiority over traditional approaches based on image processing alone. With applications ranging from autonomous vehicles to advanced safety systems, our work has significant potential to improve road safety by reducing the incidence of accidents caused by sleepy drivers.",1
"Generative Adversarial Networks (GANs) have been used extensively and quite successfully for unsupervised learning. As GANs don't approximate an explicit probability distribution, it's an interesting study to inspect the latent space representations learned by GANs. The current work seeks to push the boundaries of such inspection methods to further understand in more detail the manifold being learned by GANs. Various interpolation and extrapolation techniques along with vector arithmetic is used to understand the learned manifold. We show through experiments that GANs indeed learn a data probability distribution rather than memorize images/data. Further, we prove that GANs encode semantically relevant information in the learned probability distribution. The experiments have been performed on two publicly available datasets - Large Scale Scene Understanding (LSUN) and CelebA.",0
"Artificial intelligence (AI) has made significant strides over recent years, particularly in areas such as computer vision, natural language processing, machine learning, and deep learning algorithms. As these technologies continue to advance, there remains a pressing need for robust evaluation methods that can accurately assess their performance. In order to address this issue, we propose a novel approach based on a combination of generative adversarial networks (GANs), which have been shown to be highly effective at generating high quality images, texts, audio clips, videos, etc. Our proposed methodology leverages the power of GANs to evaluate the performance of different AI models under controlled conditions, allowing us to isolate key strengths and weaknesses. This allows researchers to more effectively compare different approaches and make better informed decisions regarding future directions in AI development. Furthermore, our method offers a valuable tool for developers working on improving existing AI systems by providing accurate feedback that can be used to guide optimization efforts. Ultimately, we hope our work will play a pivotal role in advancing the state of the art in evaluating AI performance.",1
"Recent studies on the adversarial vulnerability of neural networks have shown that models trained with the objective of minimizing an upper bound on the worst-case loss over all possible adversarial perturbations improve robustness against adversarial attacks. Beside exploiting adversarial training framework, we show that by enforcing a Deep Neural Network (DNN) to be linear in transformed input and feature space improves robustness significantly. We also demonstrate that by augmenting the objective function with Local Lipschitz regularizer boost robustness of the model further. Our method outperforms most sophisticated adversarial training methods and achieves state of the art adversarial accuracy on MNIST, CIFAR10 and SVHN dataset. In this paper, we also propose a novel adversarial image generation method by leveraging Inverse Representation Learning and Linearity aspect of an adversarially trained deep neural network classifier.",0
"Neural networks have been used extensively in computer vision tasks, such as image classification, object detection and segmentation, due to their ability to learn features from raw data directly. Despite their impressive performance on these tasks, deep neural networks (DNNs) suffer from vulnerabilities to adversarial examples, which are slight perturbations to input images that cause them to produce incorrect outputs. Researchers have developed techniques to generate these adversarial examples, exploiting their effectiveness at fooling these models, to test robustness. However, these attacks can often lead to high computational costs both for generation and evaluation. In our work we study how enforcing linearity within convolutional layers improves both the robustness of pretrained neural networks against common image corruptions and their generated adversarial samples while still allowing them to generalize well across several datasets, particularly CIFAR-10 and SVHN. We show that constraining each weight tensor corresponding to a feature map in a layer has improved resilience, and propose two possible ways of achieving this: using Jacobian regularization, where weights with large gradients during forward propagation will slow down gradient updates leading to more linear behavior; and Projected Gradient Descent (PGD), a variant of PGD adversary search algorithm that finds steps based on small gradient descent instead of random directions in order to maintain better locality. Additionally, this method enables us to gain insights into the nature of linear regions found inside deeper neural nets by analyzing the patterns present therein via activation maps and histograms, providing another pathway towards understanding what makes deep learning models tick in practice.",1
"This paper proposes a method to ease the unsupervised learning of object landmark detectors. Similarly to previous methods, our approach is fully unsupervised in a sense that it does not require or make any use of annotated landmarks for the target object category. Contrary to previous works, we do however assume that a landmark detector, which has already learned a structured representation for a given object category in a fully supervised manner, is available. Under this setting, our main idea boils down to adapting the given pre-trained network to the target object categories in a fully unsupervised manner. To this end, our method uses the pre-trained network as a core which remains frozen and does not get updated during training, and learns, in an unsupervised manner, only a projection matrix to perform the adaptation to the target categories. By building upon an existing structured representation learned in a supervised manner, the optimization problem solved by our method is much more constrained with significantly less parameters to learn which seems to be important for the case of unsupervised learning. We show that our method surpasses fully unsupervised techniques trained from scratch as well as a strong baseline based on fine-tuning, and produces state-of-the-art results on several datasets. Code can be found at https://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019 .",0
"Unsupervised learning algorithms offer promising ways of discovering novel object representations by adapting pretrained models to new datasets without explicit supervision. Landmarks represent valuable high level features which can describe objects effectively and serve as powerful anchors for several computer vision tasks such as image retrieval and semantic segmentation. Despite their importance there exists only scarce literature on how these representations can be discovered via unsupervised means. We seek to address this issue herein. By training a state of art model architecture on synthetic data we show that it produces meaningful object representations, similar to those used for traditional landmarks, while operating solely on raw images. To demonstrate the effectiveness of our approach we perform experiments evaluating the performance of our system both quantitatively and qualitatively across two common benchmarks. Our results outperform all existing unsupervised methods demonstrating the potential of unsupervised adaptation for discovering accurate object landmarks. Overall this work provides a foundation for exploring alternative unsupervised approaches for creating effective object representations applicable to real world applications.",1
"Drug repositioning is an attractive cost-efficient strategy for the development of treatments for human diseases. Here, we propose an interpretable model that learns disease self-representations for drug repositioning. Our self-representation model represents each disease as a linear combination of a few other diseases. We enforce proximity in the learnt representations in a way to preserve the geometric structure of the human phenome network - a domain-specific knowledge that naturally adds relational inductive bias to the disease self-representations. We prove that our method is globally optimal and show results outperforming state-of-the-art drug repositioning approaches. We further show that the disease self-representations are biologically interpretable.",0
"Title: Learning Interpretable Disease Self-Representations for Drug Repositioning Abstract Drug repositioning, the process of identifying new indications for existing drugs, has gained significant attention due to its potential to accelerate drug development timelines and reduce costs. To achieve successful drug repositioning, we must first identify diseases that share similar biological mechanisms, which can be challenging as many diseases present different symptoms and affect various organs and tissues. In our work, we propose a novel methodology aimed at learning interpretable disease self-representations that capture underlying molecular signatures driving disease progression and response to therapy. Our approach utilizes publicly available data sources such as electronic health records (EHRs) from hospitals, genomic information from patients diagnosed with specific diseases, and pharmacogenomics databases containing data on patient responses to various treatments. We introduce the concept of latent patient-disease fingerprints by leveraging deep neural networks that learn shared molecular patterns between cohorts sharing common traits or affected by the same pathological condition. These latent fingerprints enable comparisons across cohorts, capturing both dissimilarities and similarities in their pathobiologies. By analyzing these patterns, we uncover relationships between seemingly disparate diseases based on hidden connections, providing insights into possible alternative indications for approved medicines. Using real world datasets for several conditions, including cancer, cardiovascular diseases, immune deficiencies, and metabolic dysfunctions, we demonstrate the effectiveness of our proposed framework. Specifically, using statistical measures and expert annotation assessment, we show that latent patie",1
"We present two instances, L-GAE and L-VGAE, of the variational graph auto-encoding family (VGAE) based on separating feature propagation operations from graph convolution layers typically found in graph learning methods to a single linear matrix computation made prior to input in standard auto-encoder architectures. This decoupling enables the independent and fixed design of the auto-encoder without requiring additional GCN layers for every desired increase in the size of a node's local receptive field. Fixing the auto-encoder enables a fairer assessment on the size of a nodes receptive field in building representations. Furthermore a by-product of fixing the auto-encoder design often results in substantially smaller networks than their VGAE counterparts especially as we increase the number of feature propagations. A comparative downstream evaluation on link prediction tasks show comparable state of the art performance to similar VGAE arrangements despite considerable simplification. We also show the simple application of our methodology to more challenging representation learning scenarios such as spatio-temporal graph representation learning.",0
"In this paper, we propose a new approach to decoupling feature propagation from the design of graph auto-encoders (GAEs). Our method allows us to better understand how GAEs encode input data by separating the process of encoding features into latent space from the design choices made during model training. We demonstrate that our proposed method leads to improved performance on several benchmark datasets compared to traditional GAE models. Additionally, we show that our method can effectively learn meaningful representations even when faced with noisy input data. By providing these insights into the behavior of GAEs, we aim to enable researchers and practitioners to more easily build effective graph encoders using this powerful machine learning tool.",1
"Generative models of graph structure have applications in biology and social sciences. The state of the art is GraphRNN, which decomposes the graph generation process into a series of sequential steps. While effective for modest sizes, it loses its permutation invariance for larger graphs. Instead, we present a permutation invariant latent-variable generative model relying on graph embeddings to encode structure. Using tools from the random graph literature, our model is highly scalable to large graphs with likelihood evaluation and generation in $O(|V | + |E|)$.",0
"This is an abstract of our upcoming paper ""Graph Embedding VAE: A Permutation Invariant Model of Graph Structure"". Our goal in this paper is to introduce a new graph embedding model that can accurately capture permutation invariant features of graphs while ensuring better reconstruction fidelity. We propose using Variational Autoencoders (VAE) as our base architecture because of their ability to learn disentangled representations by maximizing the evidence lower bound of the log likelihood. However, traditional autoencoder frameworks have been shown to suffer from vanishing gradients during training. To address this issue we use HF-VAMP which provides improved speed and stability compared to competing methods such as the REINFORCE algorithm. Our results on several benchmark datasets show significant improvement over state-of-the-art baselines in terms of node classification accuracy while achieving faster training times. Additionally, we qualitatively analyze embeddings learned by our method to demonstrate its effectiveness in capturing important features of graphs.",1
"Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction.",0
"Protein sequence embeddings have become increasingly important in computational biology due to their ability to capture and represent the essential features of proteins. One challenge associated with learning these embeddings is that many sequence databases contain incomplete information on protein structures. To address this issue, researchers developed a method for integrating structural data into sequence embeddings by creating an intermediary dataset containing both structural and nonstructural information. This dataset was used to learn improved embeddings using neural networks. The resulting protein sequence embeddings were shown to outperform previous methods for several downstream tasks such as functional annotation transfer and mutation impact prediction. These findings demonstrate the potential utility of incorporating structural information into sequence embeddings for advancing our understanding of protein evolution and function. Overall, this study provides new insights into how we can better use machine learning techniques to model complex biological systems.",1
"Learning useful representations is a key ingredient to the success of modern machine learning. Currently, representation learning mostly relies on embedding data into Euclidean space. However, recent work has shown that data in some domains is better modeled by non-euclidean metric spaces, and inappropriate geometry can result in inferior performance. In this paper, we aim to eliminate the inductive bias imposed by the embedding space geometry. Namely, we propose to map data into more general non-vector metric spaces: a weighted graph with a shortest path distance. By design, such graphs can model arbitrary geometry with a proper configuration of edges and weights. Our main contribution is PRODIGE: a method that learns a weighted graph representation of data end-to-end by gradient descent. Greater generality and fewer model assumptions make PRODIGE more powerful than existing embedding-based approaches. We confirm the superiority of our method via extensive experiments on a wide range of tasks, including classification, compression, and collaborative filtering.",0
"This study presents an alternative data representation model for machine learning called differentiable weighted graphs (DWG). Our approach is based on representing high-dimensional feature spaces as low-dimensional undirected graphs where each edge is assigned a learnable weight that captures pairwise relationships among data points. Unlike traditional vector space representations that can suffer from dimensionality issues such as curse of dimensionality and scaling problems, our proposed method offers more flexible, compact, and interpretable representations that can alleviate these concerns. Additionally, we demonstrate how our method can support graph convolutional networks and enable efficient training, without the need for explicit graph construction through preprocessing steps. Our empirical evaluations show promising results across several benchmark datasets compared to state-of-the art methods.",1
"Classical supervised classification tasks search for a nonlinear mapping that maps each encoded feature directly to a probability mass over the labels. Such a learning framework typically lacks the intuition that encoded features from the same class tend to be similar and thus has little interpretability for the learned features. In this paper, we propose a novel supervised learning model named Supervised-Encoding Quantizer (SEQ). The SEQ applies a quantizer to cluster and classify the encoded features. We found that the quantizer provides an interpretable graph where each cluster in the graph represents a class of data samples that have a particular style. We also trained a decoder that can decode convex combinations of the encoded features from similar and different clusters and provide guidance on style transfer between sub-classes.",0
"Abstract: This paper presents Supervised Encoding (SE), a novel method that utilizes supervision during encoding to learn discrete representations. By treating high-dimensional continuous spaces as linear transformations of low-dimensional discrete codes, SE enables efficient use of labeled data without sacrificing representational power. Through extensive evaluation on image classification benchmarks and comparisons against strong baselines, we demonstrate the effectiveness and generalizability of our approach. Our results show that SE achieves state-of-the-art performance across multiple datasets while using significantly fewer parameters compared to other methods. Furthermore, we provide analysis showing that SE learns meaningful discretizations that can improve robustness to input perturbations such as noise or transformation differences across domains. Overall, our work makes a significant contribution towards learning effective and efficient representation learning techniques under label supervision.",1
"Distributed representations have been used to support downstream tasks in healthcare recently. Healthcare data (e.g., electronic health records) contain multiple modalities of data from heterogeneous sources that can provide complementary information, alongside an added dimension to learning personalized patient representations. To this end, in this paper we propose a novel unsupervised encoder-decoder model, namely Mixed Pooling Multi-View Attention Autoencoder (MPVAA), that generates patient representations encapsulating a holistic view of their medical profile. Specifically, by first learning personalized graph embeddings pertaining to each patient's heterogeneous healthcare data, it then integrates the non-linear relationships among them into a unified representation through multi-view attention mechanism. Additionally, a mixed pooling strategy is incorporated in the encoding step to learn diverse information specific to each data modality. Experiments conducted for multiple tasks demonstrate the effectiveness of the proposed model over the state-of-the-art representation learning methods in healthcare.",0
"In healthcare data analysis, representations learned from multi-view patient records have been shown to improve diagnosis accuracy and medical decision making. However, learning meaningful latent representations can be challenged by high variations across multiple view types, which may require different pooling strategies depending on their feature scales or categorical variables. To address these issues, we propose a new model called MixPoolAutoEncoder, based on Variational autoencoders (VAEs), aimed at facilitating representation learning via flexible mixed pooling operations over heterogeneous views. This framework accommodates both continuous and discrete features while ensuring a shared decoding process for joint reconstruction purposes. Furthermore, we extend VAEs with a novel attention mechanism that captures dependencies among views beyond simple pooling, thus promoting better alignment of internal features. We evaluate MixPoolAutoEncoder against several baselines using benchmark datasets commonly used in medical research, showing stateof-the art results regarding both quantitative metrics and interpretability assessments. Our work represents a key step towards enhancing data utilization in personalized medicine through effective integration of various sources and improving patients’ outcomes.",1
"Representation learning (RL) plays an important role in extracting proper representations from complex medical data for various analyzing tasks, such as patient grouping, clinical endpoint prediction and medication recommendation. Medical data can be divided into two typical categories, outpatient and inpatient, that have different data characteristics. However, few of existing RL methods are specially designed for inpatients data, which have strong temporal relations and consistent diagnosis. In addition, for unordered medical activity set, existing medical RL methods utilize a simple pooling strategy, which would result in indistinguishable contributions among the activities for learning. In this work, weproposeInpatient2Vec, anovelmodel for learning three kinds of representations for inpatient, including medical activity, hospital day and diagnosis. A multi-layer self-attention mechanism with two training tasks is designed to capture the inpatient data characteristics and process the unordered set. Using a real-world dataset, we demonstrate that the proposed approach outperforms the competitive baselines on semantic similarity measurement and clinical events prediction tasks.",0
"Introduction In recent years, there has been significant interest in developing machine learning algorithms that can effectively process medical data, including electronic health records (EHRs). One key challenge facing such efforts is the lack of standardized representation formats for EHR data, which makes it difficult to apply traditional deep learning methods. To address this issue, we propose a novel approach called ""Inpatient2Vec"" that represents medical concepts as continuous vectors using embedding techniques. Our method leverages expert knowledge provided by physicians and integrates both structured and unstructured patient data from EHRs, enabling more accurate and interpretable representations of complex clinical entities. We demonstrate the effectiveness of our approach on several important tasks relevant to hospital operations and patient care, showing strong improvements over baseline models. Our results suggest that Inpatient2Vec could provide valuable support for decision making in medicine, ultimately leading to better outcomes for patients.  Problem statement Despite the vast amounts of electronic health record (EHR) data available, extracting meaningful insights remains challenging due to variability in notation, documentation inconsistencies, and missing values. While previous works have attempted to tackle these issues via manual feature engineering and domain adaptation, they often struggle with scalability and interpretability. Motivated by recent successes in natural language processing, we aim to represent medical concepts using low-dimensional continuous embeddings, enhancing the capacity of downstream applications for inpatient populations. However, directly applying existing text embedding techniques may not capture specific nuances present within the medical domain, requiring tailored solutions accounting for both structured and freeform content sources. Therefore, we investigate ways to inject physician expertise into training objectives, fostering highquality representations generalizing across multiple use cases in acute care settings.",1
"Gaussian Processes (GPs) with an appropriate kernel are known to provide accurate predictions and uncertainty estimates even with very small amounts of labeled data. However, GPs are generally unable to learn a good representation that can encode intricate structures in high dimensional data. The representation power of GPs depends heavily on kernel functions used to quantify the similarity between data points. Traditional GP kernels are not very effective at capturing similarity between high dimensional data points, while methods that use deep neural networks to learn a kernel are not sample-efficient. To overcome these drawbacks, we propose deep probabilistic kernels which use a probabilistic neural network to map high-dimensional data to a probability distribution in a low dimensional subspace, and leverage the rich work on kernels between distributions to capture the similarity between these distributions. Experiments on a variety of datasets show that building a GP using this covariance kernel solves the conflicting problems of representation learning and sample efficiency. Our model can be extended beyond GPs to other small-data paradigms such as few-shot classification where we show competitive performance with state-of-the-art models on the mini-Imagenet dataset.",0
"In this work we present deep probabilistic kernels, which allow us to perform model selection by learning a posterior distribution over models in a sample efficient manner. By utilizing variational inference within these distributions, we can achieve state-of-the-art performance on several benchmark datasets such as MNIST, CIFAR, and SVHN without the need for large amounts of training data. Our approach is based on the framework proposed by Rasmussen et al., who showed that kernel machines can be trained using Bayesian methods via Markov Chain Monte Carlo (MCMC) sampling. We extend their methodology by introducing latent variables into the framework and casting our problem in the form of a joint energy function composed of both the positive definite kernel matrix and the potential generated by the observed data. This allows us to train an approximate posterior distribution over functions which are expressive enough to capture complex patterns and structures present in natural images, while retaining enough flexibility to learn from small numbers of examples. To evaluate the efficacy of our approach, we compare it against three baseline algorithms: Neural Networks (NN), Gaussian Processes Regression (GPR), and Support Vector Machines (SVM). Experimental results demonstrate the competitiveness of DPK on the tasks considered, suggesting that our method could potentially offer significant advantages in scenarios where labeled data is scarce. Overall, we believe this work represents a step forward towards building more capable machine learning systems that can generalize well on few-shot problems across diverse domains.",1
"It is well established that temporal organization is critical to memory, and that the ability to temporally organize information is fundamental to many perceptual, cognitive, and motor processes. While our understanding of how the brain processes the spatial context of memories has advanced considerably, our understanding of their temporal organization lags far behind. In this paper, we propose a new approach for elucidating the neural basis of complex behaviors and temporal organization of memories. More specifically, we focus on neural decoding - the prediction of behavioral or experimental conditions based on observed neural data. In general, this is a challenging classification problem, which is of immense interest in neuroscience. Our goal is to develop a new framework that not only improves the overall accuracy of decoding, but also provides a clear latent representation of the decoding process. To accomplish this, our approach uses a Variational Auto-encoder (VAE) model with a diversity-encouraging prior based on determinantal point processes (DPP) to improve latent representation learning by avoiding redundancy in the latent space. We apply our method to data collected from a novel rat experiment that involves presenting repeated sequences of odors at a single port and testing the rats' ability to identify each odor. We show that our method leads to substantially higher accuracy rate for neural decoding and allows to discover novel biological phenomena by providing a clear latent representation of the decoding process.",0
"In this paper, we present a novel approach to neural decoding using Bayesian methods and diversity-encouraged latent representation learning. We propose a model that leverages both the strengths of deep neural networks and the statistical interpretability offered by Bayesian inference to extract meaningful representations from brain activity data. Our method encourages a diverse range of possible solutions by regularizing the likelihood through a variational autoencoder framework, which leads to more robust and interpretable results than traditional maximum likelihood estimation alone. Our experimental results demonstrate significant improvements in decoding accuracy compared to state-of-the-art methods on several benchmark datasets across different modalities, such as fMRI and EEG. These findings highlight the potential benefits of integrating principles of variational bayesian inference into deep learning frameworks for neural decoding applications. Overall, our work provides insights into how neuroimaging data can be analyzed using modern machine learning techniques, paving the way for improved understanding of complex cognitive processes and diseases.",1
"The problem of identifying geometric structure in heterogeneous, high-dimensional data is a cornerstone of representation learning. While there exists a large body of literature on the embeddability of canonical graphs, such as lattices or trees, the heterogeneity of the relational data typically encountered in practice limits the applicability of these classical methods. In this paper, we propose a combinatorial approach to evaluating embeddability, i.e., to decide whether a data set is best represented in Euclidean, Hyperbolic or Spherical space. Our method analyzes nearest-neighbor structures and local neighborhood growth rates to identify the geometric priors of suitable embedding spaces. For canonical graphs, the algorithm's prediction provably matches classical results. As for large, heterogeneous graphs, we introduce an efficiently computable statistic that approximates the algorithm's decision rule. We validate our method over a range of benchmark data sets and compare with recently published optimization-based embeddability methods.",0
"This paper explores how neighborhood growth can affect geometric priors used in relational representation learning tasks. We argue that neighborhoods play an important role in determining the types of representations learned by a model, as they allow different representations to compete against one another based on their ability to accurately predict future observations. In particular, we show that increasing the rate at which new neighbors appear during training leads to more complex geometric prior distributions over possible representations. These results have implications for both theory and practice, suggesting that the design of neighborhood growth functions could provide a powerful tool for controlling the behavior of neural models trained on relational data. By carefully crafting these functions, researchers may be able to steer their models towards desirable representational solutions, potentially leading to improved generalization performance across a variety of domains. Finally, we suggest several directions for further work aimed at better understanding the relationship between neighborhood structure and relational reasoning abilities. Overall, our findings highlight the importance of considering spatially local contextual dependencies within relational frameworks, paving the way for exciting advances in this active area of research.",1
"Deep video action recognition models have been highly successful in recent years but require large quantities of manually annotated data, which are expensive and laborious to obtain. In this work, we investigate the generation of synthetic training data for video action recognition, as synthetic data have been successfully used to supervise models for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation, physics models and other components of modern game engines. With this model we generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for ""Procedural Human Action Videos"". PHAV contains a total of 39,982 videos, with more than 1,000 examples for each of 35 action categories. Our video generation approach is not limited to existing motion capture sequences: 14 of these 35 categories are procedurally defined synthetic actions. In addition, each video is represented with 6 different data modalities, including RGB, optical flow and pixel-level semantic labels. These modalities are generated almost simultaneously using the Multiple Render Targets feature of modern GPUs. In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers action classes from multiple datasets) representation learning architecture that is able to simultaneously learn from synthetic and real video datasets, even when their action categories differ. Our experiments on the UCF-101 and HMDB-51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance. Our approach also significantly outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",0
"This could be a potential application of AI to create realistic videos showing human action which could have a variety of applications such as creating movies and television shows, advertisements, training simulations, etc.",1
"With the rising interest in graph representation learning, a variety of approaches have been proposed to effectively capture a graph's properties. While these approaches have improved performance in graph machine learning tasks compared to traditional graph techniques, they are still perceived as techniques with limited insight into the information encoded in these representations. In this work, we explore methods to interpret node embeddings and propose the creation of a robust evaluation framework for comparing graph representation learning algorithms and hyperparameters. We test our methods on graphs with different properties and investigate the relationship between embedding training parameters and the ability of the produced embedding to recover the structure of the original graph in a downstream task.",0
"This paper focuses on the evaluation and interpretability of graph representation learning algorithms. We examine how current methods perform across different domains and discuss their limitations and strengths. We propose new techniques to improve interpretability and explainability, making these models more transparent and easier to understand for users and practitioners alike. Our work contributes to the broader field of machine learning by addressing important challenges related to model transparency and accountability. By increasing our understanding of graph representation learning, we can better leverage these powerful tools for solving real-world problems while ensuring that they remain accessible and trustworthy. Ultimately, this research enables us to create intelligent systems that align with human values and expectations while maintaining high levels of performance and accuracy.",1
"Recent researches use margin theory to analyze the generalization performance for deep neural networks. The main results are based on the spectrally-normalized minimum margin. However, optimizing the minimum margin ignores a mass of information about margin distribution which is crucial to generalization performance. In this paper, we prove a generalization bound dominated by a ratio of the margin standard deviation to the margin mean, where the huge magnitude of spectral norms is reduced. Compared with the spectral norm terms in the existing results, the margin ratio term in our bound is orders of magnitude better in practice. On the other hand, our bound inspires us to optimize the margin ratio. We utilize a convex margin distribution loss function on the deep neural networks to validate our theoretical results. Experiments and visualizations confirm the effectiveness of our approach in terms of performance and representation learning ability.",0
"This abstract describes a technique that improves the generalization performance of deep neural networks (DNN) using margin distribution. The proposed method leverages the inherent structure of DNN models to modify the data distributions and change their margins in order to enhance generalization. By doing so, we can improve the ability of DNN models to generalize to new tasks without overfitting on the training set, leading to improved accuracy on both seen and unseen test sets. Experiments demonstrate significant improvement in model performance across multiple benchmark datasets, making our approach highly scalable and applicable in real-world applications. In summary, our method provides a simple yet effective solution for increasing the robustness and adaptability of DNN architectures, paving the way towards more reliable artificial intelligence systems.",1
"Automatic representation learning of key entities in electronic health record (EHR) data is a critical step for healthcare informatics that turns heterogeneous medical records into structured and actionable information. Here we propose ME2Vec, an algorithmic framework for learning low-dimensional vectors of the most common entities in EHR: medical services, doctors, and patients. ME2Vec leverages diverse graph embedding techniques to cater for the unique characteristic of each medical entity. Using real-world clinical data, we demonstrate the efficacy of ME2Vec over competitive baselines on disease diagnosis prediction.",0
"In recent years, Electronic Health Records (EHR) have become increasingly important sources of data for medical researchers. However, analyzing EHRs can be challenging due to their unstructured nature and the large amount of missing or inconsistent information. This study proposes a novel approach called ""Graph-based Medical Entity Embedding"" that leverages graph representation learning techniques to model relationships among entities mentioned in EHR notes and generate embeddings capturing complex dependencies between them. Our method outperforms existing state-of-the-art methods on multiple benchmark datasets by effectively addressing issues such as negation, synonymy, and context sensitivity. Overall, our work has significant implications for advancing healthcare informatics and improving patient care through enhanced EHR analysis capabilities.",1
"Graph kernels are kernel methods measuring graph similarity and serve as a standard tool for graph classification. However, the use of kernel methods for node classification, which is a related problem to graph representation learning, is still ill-posed and the state-of-the-art methods are heavily based on heuristics. Here, we present a novel theoretical kernel-based framework for node classification that can bridge the gap between these two representation learning problems on graphs. Our approach is motivated by graph kernel methodology but extended to learn the node representations capturing the structural information in a graph. We theoretically show that our formulation is as powerful as any positive semidefinite kernels. To efficiently learn the kernel, we propose a novel mechanism for node feature aggregation and a data-driven similarity metric employed during the training phase. More importantly, our framework is flexible and complementary to other graph-based deep learning models, e.g., Graph Convolutional Networks (GCNs). We empirically evaluate our approach on a number of standard node classification benchmarks, and demonstrate that our model sets the new state of the art.",0
"This paper presents novel techniques that rethink kernel methods for graph representation learning. Specifically, we propose using multiple kernels combined with linear transformations learned from data and use these transformed kernels as features in machine learning algorithms. Our method improves node classification accuracy by up to 7% over state-of-the art approaches such as DeepWalk, LINE, and GCN. Moreover, our approach achieves high stability across different architectures, hyperparameter settings, and datasets. Experimental results demonstrate that our method can capture complex relationships between nodes efficiently, making it well suited for real world applications on large scale graphs.",1
"Deep learning generates state-of-the-art semantic segmentation provided that a large number of images together with pixel-wise annotations are available. To alleviate the expensive data collection process, we propose a semi-supervised domain adaptation method for the specific case of images with similar semantic content but different pixel distributions. A network trained with supervision on a past dataset is finetuned on the new dataset to conserve its features maps. The domain adaptation becomes a simple regression between feature maps and does not require annotations on the new dataset. This method reaches performances similar to classic transfer learning on the PASCAL VOC dataset with synthetic transformations.",0
"This study investigates the problem of semi-supervised domain adaptation for semantic segmentation of satellite images acquired at different times. With limited annotations available for target domains, we propose a method that learns a representation space shared by both source and target domains using self-training and adversarial training techniques. Our approach effectively leverages unlabeled data from the target domain and minimizes the discrepancy between the learned representations of labeled and unlabeled samples. Extensive experiments demonstrate significant improvements over state-of-the-art methods under challenging settings where only scarce annotations are available in the target domain. Our work highlights the potential of semi-supervised domain adaptation for enhancing the performance of remote sensing applications requiring fine-grained understanding of scene dynamics through time.",1
"Approximating distributions over complicated manifolds, such as natural images, are conceptually attractive. The deep latent variable model, trained using variational autoencoders and generative adversarial networks, is now a key technique for representation learning. However, it is difficult to unify these two models for exact latent-variable inference and parallelize both reconstruction and sampling, partly due to the regularization under the latent variables, to match a simple explicit prior distribution. These approaches are prone to be oversimplified, and can only characterize a few modes of the true distribution. Based on the recently proposed Wasserstein autoencoder (WAE) with a new regularization as an optimal transport. The paper proposes a stacked Wasserstein autoencoder (SWAE) to learn a deep latent variable model. SWAE is a hierarchical model, which relaxes the optimal transport constraints at two stages. At the first stage, the SWAE flexibly learns a representation distribution, i.e., the encoded prior; and at the second stage, the encoded representation distribution is approximated with a latent variable model under the regularization encouraging the latent distribution to match the explicit prior. This model allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce changes in the output space. Both quantitative and qualitative results demonstrate the superior performance of SWAE compared with the state-of-the-art approaches in terms of faithful reconstruction and generation quality.",0
"In recent years, there has been significant interest in developing deep learning algorithms that can effectively handle high-dimensional data, such as images, videos, audio signals, and more. One approach that has gained popularity is the stacked autoencoder (SAE), which involves training multiple levels of autoencoders to learn progressively deeper representations of the input data. However, one limitation of traditional SAE models is their tendency to suffer from vanishing gradients during backpropagation, leading to difficulty in optimizing the model parameters effectively. To address this issue, we propose a new method called Stacked Wasserstein Autoencoder (SWAE). Our method incorporates an additional regularization term based on the Earth Mover's Distance (EMD) into the loss function of each layer of the autoencoder. This allows us to train more robust and efficient models that better capture the underlying structure of complex datasets. We demonstrate the effectiveness of our method through experiments on several benchmark datasets and show that SWAEs outperform existing state-of-the-art methods in terms of accuracy and efficiency. Our results have important implications for the field of unsupervised learning and highlight the potential of using adversarial losses in deep learning applications.",1
"Effective modeling of electronic health records presents many challenges as they contain large amounts of irregularity most of which are due to the varying procedures and diagnosis a patient may have. Despite the recent progress in machine learning, unsupervised learning remains largely at open, especially in the healthcare domain. In this work, we present a two-step unsupervised representation learning scheme to summarize the multi-modal clinical time series consisting of signals and medical codes into a patient status vector. First, an auto-encoder step is used to reduce sparse medical codes and clinical time series into a distributed representation. Subsequently, the concatenation of the distributed representations is further fine-tuned using a forecasting task. We evaluate the usefulness of the representation on two downstream tasks: mortality and readmission. Our proposed method shows improved generalization performance for both short duration ICU visits and long duration ICU visits.",0
"This could be your first step towards improving outcomes by gaining a deeper understanding of patient status. In our research, we present a novel method for representing electronic health record (EHR) signals and codes using unsupervised learning techniques. Our approach creates patient status vectors that capture both the multimodal nature of EHR data and patient heterogeneity. These vectors allow us to identify temporal patterns in patient deterioration, enabling earlier detection of adverse events. We evaluate the effectiveness of our method on two distinct clinical datasets, demonstrating improved performance compared to traditional methods. By leveraging advanced machine learning algorithms and big data analysis tools, you can improve patient care while reducing costs. With insights from our research, you can make informed decisions based on accurate predictions of patient needs. Contact us today to learn more!",1
"Graph representation learning, aiming to learn low-dimensional representations which capture the geometric dependencies between nodes in the original graph, has gained increasing popularity in a variety of graph analysis tasks, including node classification and link prediction. Existing representation learning methods based on graph neural networks and their variants rely on the aggregation of neighborhood information, which makes it sensitive to noises in the graph. In this paper, we propose Graph Denoising Policy Network (short for GDPNet) to learn robust representations from noisy graph data through reinforcement learning. GDPNet first selects signal neighborhoods for each node, and then aggregates the information from the selected neighborhoods to learn node representations for the down-stream tasks. Specifically, in the signal neighborhood selection phase, GDPNet optimizes the neighborhood for each target node by formulating the process of removing noisy neighborhoods as a Markov decision process and learning a policy with task-specific rewards received from the representation learning phase. In the representation learning phase, GDPNet aggregates features from signal neighbors to generate node representations for down-stream tasks, and provides task-specific rewards to the signal neighbor selection phase. These two phases are jointly trained to select optimal sets of neighbors for target nodes with maximum cumulative task-specific rewards, and to learn robust representations for nodes. Experimental results on node classification task demonstrate the effectiveness of GDNet, outperforming the state-of-the-art graph representation learning methods on several well-studied datasets. Additionally, GDPNet is mathematically equivalent to solving the submodular maximizing problem, which theoretically guarantees the best approximation to the optimal solution with GDPNet.",0
"In recent years, representation learning has emerged as a key approach to building intelligent systems that can learn from complex, high-dimensional data sets. At the heart of this approach lies the ability to extract robust representations of the underlying structure in these data sets, which can then be used to drive downstream tasks such as classification, prediction, or decision making. One challenge facing researchers in this area is how to ensure that learned representations remain meaningful even in the face of noise, outliers, or other forms of uncertainty present in real-world data. To address this issue, we propose a novel method called ""Graph Denoising Policy Networks"" (GDPN). This approach combines ideas from graph denoising and reinforcement learning to optimize a policy network that learns to identify and remove noise from a given graph signal while preserving important structural features. We evaluate our proposed method on several benchmark datasets, demonstrating its effectiveness in improving the quality and interpretability of learned representations. Our findings have implications for both theoretical advances in representation learning and practical applications in areas such as computer vision, natural language processing, and recommendation systems. Overall, GDPN represents a significant step forward towards achieving more reliable and robust representations in real-world settings.",1
"We introduce the Mutual Information Machine (MIM), a novel formulation of representation learning, using a joint distribution over the observations and latent state in an encoder/decoder framework. Our key principles are symmetry and mutual information, where symmetry encourages the encoder and decoder to learn different factorizations of the same underlying distribution, and mutual information, to encourage the learning of useful representations for downstream tasks. Our starting point is the symmetric Jensen-Shannon divergence between the encoding and decoding joint distributions, plus a mutual information encouraging regularizer. We show that this can be bounded by a tractable cross entropy loss function between the true model and a parameterized approximation, and relate this to the maximum likelihood framework. We also relate MIM to variational autoencoders (VAEs) and demonstrate that MIM is capable of learning symmetric factorizations, with high mutual information that avoids posterior collapse.",0
"""High Mutual Information"" is a research paper that explores the intersection of representation learning and symmetric variational inference. By leveraging advanced machine learning techniques and algorithms, the study examines how these two fields can be combined to achieve optimal results in data analysis and modeling.  Through extensive experimentation and rigorous testing, the authors showcase the effectiveness of their approach in capturing high mutual information in complex datasets. They demonstrate that by incorporating symmetric variational inference into representation learning models, we can effectively mitigate overfitting while enhancing generalization performance on unseen test sets.  Furthermore, ""High Mutual Information"" provides insights into the benefits of using symmetric variational inference as a regularizer during training. This technique ensures that our learned representations are both robust and effective at encoding high levels of intrinsic structure within large datasets.  Overall, the findings from this work have significant implications for applied scientists across many domains where accurate representation learning is crucial - including computer vision, natural language processing, robotics, and artificial intelligence more broadly. With the ability to capture higher amounts of mutual information in our models, practitioners can now achieve greater understanding of complex systems and make better predictions based upon them.",1
"Gradient-based meta-learning has proven to be highly effective at learning model initializations, representations, and update rules that allow fast adaptation from a few samples. The core idea behind these approaches is to use fast adaptation and generalization -- two second-order metrics -- as training signals on a meta-training dataset. However, little attention has been given to other possible second-order metrics. In this paper, we investigate a different training signal -- robustness to catastrophic interference -- and demonstrate that representations learned by directing minimizing interference are more conducive to incremental learning than those learned by just maximizing fast adaptation.",0
"Adapting to new situations is essential for survival and success in many domains including natural environments, economics, and machine learning. Often times fast adaptation can give an advantage over slower adaptors, but in some cases slow and deliberate strategies may be better suited. In this paper we explore the tradeoffs between fast vs slow adaptation across different scenarios. We present theoretical analysis as well as numerical experiments on simple models that illustrate the benefits and drawbacks of each strategy depending on environmental conditions, uncertainty level, costs of exploration, among other factors. Our findings suggest that while fast adaptation can yield impressive results under certain circumstances, there exist regimes where deliberateness might actually be preferred due to increased robustness against setbacks, reduced risk taking and more reliable exploitation of resources. Overall, our work provides insights into design principles underlying optimal adaptation policies across diverse systems. We believe these conclusions could have important implications for fields such as robotics, finance, artificial intelligence, epidemiology, evolutionary biology, game theory, among others.",1
"Graph autoencoders (AE) and variational autoencoders (VAE) recently emerged as powerful node embedding methods, with promising performances on challenging tasks such as link prediction and node clustering. Graph AE, VAE and most of their extensions rely on graph convolutional networks (GCN) to learn vector space representations of nodes. In this paper, we propose to replace the GCN encoder by a simple linear model w.r.t. the adjacency matrix of the graph. For the two aforementioned tasks, we empirically show that this approach consistently reaches competitive performances w.r.t. GCN-based models for numerous real-world graphs, including the widely used Cora, Citeseer and Pubmed citation networks that became the de facto benchmark datasets for evaluating graph AE and VAE. This result questions the relevance of repeatedly using these three datasets to compare complex graph AE and VAE models. It also emphasizes the effectiveness of simple node encoding schemes for many real-world applications.",0
"Abstract: While graph convolutional networks (GCN) have been widely used in graph autoencoders, recent work has suggested that they might not always be necessary. In this paper, we present a novel approach to building graph autoencoders without using GCNs. Our method involves creating a simple baseline model which is then improved upon through the use of a number of techniques such as normalization and denormalization. We demonstrate through experiments on several benchmark datasets that our proposed approach outperforms models built using traditional GCNs across multiple metrics including node classification accuracy, clustering coefficient, and log likelihood. This shows that a simple yet effective solution can be achieved without relying on more complex GCN architectures. Overall, our results highlight the importance of simplicity in deep learning research and suggest that future work should focus on finding even simpler solutions to difficult problems.",1
"All previous methods for audio-driven talking head generation assume the input audio to be clean with a neutral tone. As we show empirically, one can easily break these systems by simply adding certain background noise to the utterance or changing its emotional tone (to such as sad). To make talking head generation robust to such variations, we propose an explicit audio representation learning framework that disentangles audio sequences into various factors such as phonetic content, emotional tone, background noise and others. We conduct experiments to validate that conditioned on disentangled content representation, the generated mouth movement by our model is significantly more accurate than previous approaches (without disentangled learning) in the presence of noise and emotional variations. We further demonstrate that our framework is compatible with current state-of-the-art approaches by replacing their original audio learning component with ours. To our best knowledge, this is the first work which improves the performance of talking head generation from disentangled audio representation perspective, which is important for many real-world applications.",0
"This research paper presents a method for animating faces by leveraging audio representations that have been disentangled from other factors such as identity or pose. By doing so, we can generate lip sync animations that more accurately capture the emotional content of speech signals. We propose a novel framework called Disentangled Speech Representation Network (DSRN) which learns a low dimensional representation space where the distance between two points corresponds to their semantic relationship. Our approach takes advantage of recent advances in generative adversarial networks and variational autoencoders to disentangle the audio signal into its constituent parts: speaking style, accent, age, and gender. Once these elements are separated, our system generates high quality lip sync animations that capture the subtle variations in facial movements caused by changes in phoneme sequence or emotional state. Extensive experiments demonstrate that our method outperforms traditional approaches in terms of visual fidelity and naturalness, making it well suited for applications in virtual reality, video games, and other interactive systems. Overall, our work represents a significant step forward in the field of computer vision and animation, opening up new possibilities for creating realistic digital characters that truly capture the essence of human expression.",1
"This paper introduces equivariant hamiltonian flows, a method for learning expressive densities that are invariant with respect to a known Lie-algebra of local symmetry transformations while providing an equivariant representation of the data. We provide proof of principle demonstrations of how such flows can be learnt, as well as how the addition of symmetry invariance constraints can improve data efficiency and generalisation. Finally, we make connections to disentangled representation learning and show how this work relates to a recently proposed definition.",0
"In recent years, equivariant techniques have been increasingly applied in mathematical fields such as topology, geometry, and analysis. One area that has seen significant progress in this regard is symplectic geometry, where equivariant Hamiltonians play a central role.  The study of equivariant Hamiltonian flows arises from a desire to better understand the dynamics of symplectic systems under symmetry groups. These flows can exhibit fascinating behaviors and often possess valuable topological invariants. This work aims to provide a comprehensive exploration of these phenomena.  We begin by developing a framework for analyzing equivariant Hamiltonian systems on compact connected Lie groups. We show how the underlying group structure gives rise to interesting geometric constraints and establish basic existence results for associated solutions. Next, we investigate the relationship between equivariant cohomology classes and time-dependent vector fields, highlighting key connections to Floer theory.  Using examples, we illustrate how the presence of symmetries drastically alters the behavior of flow lines, leading to new global bifurcation patterns and unexpected homotopy interactions. We further demonstrate the impact of assumptions on equivariance strength (e.g., linear vs. projective) and discuss their implications on applications. Finally, we outline some open problems in the field and suggest promising directions for future research.  Overall, our work provides both foundational insights into equivariant Hamiltonian systems and advances our understanding of several intriguing open questions in modern mathematics. By bridging theoretical developments with concrete instances, we hope to inspire further inquiry in this rapidly evolving domain.",1
"Network representation learning has exploded recently. However, existing studies usually reconstruct networks as sequences or matrices, which may cause information bias or sparsity problem during model training. Inspired by a cognitive model of human memory, we propose a network representation learning scheme. In this scheme, we learn node embeddings by adjusting the proximity of nodes traversing the spreading structure of the network. Our proposed method shows a significant improvement in multiple analysis tasks based on various real-world networks, ranging from semantic networks to protein interaction networks, international trade networks, human behavior networks, etc. In particular, our model can effectively discover the hierarchical structures in networks. The well-organized model training speeds up the convergence to only a small number of iterations, and the training time is linear with respect to the edge numbers.",0
"Learning schemas allow students to organize new knowledge into preexisting frameworks. In contrast, spreading activation allows learners to build complex mental representations by gradually linking together pieces of related information. Network structural learning offers opportunities to both develop understanding via schema-based learning and to create interrelated representations through spreading activation. We propose that integrating these two processes can lead to more robust and flexible educational outcomes than focusing on either process alone. Our central argument is organized around a case study focused specifically on science education. Here we describe how such integration might be achieved, using examples from our own experience as educators working at the high school level. Drawing upon recent research suggesting ways that expert knowledge acquisition involves shifting between different kinds of processing and representation (see Chi, Bassok, Lewis, Reimann, & Glaser, 1989; Hatano & Osawa, 2007), we discuss design principles for developing hybrid learning tools designed to support the combination of structured and unstructured thinking in domains like physics education, where conceptual change must frequently compete against powerful prior conceptions (diSessa, 1988). Finally, we report evidence from experimental tests of prototypes intended to help students integrate their initial learning experiences with later ones aimed at extending their knowledge via more exploratory approaches. These results suggest some promise for our approach but should serve as the starting point for more extended work.",1
"Increasing volume of Electronic Health Records (EHR) in recent years provides great opportunities for data scientists to collaborate on different aspects of healthcare research by applying advanced analytics to these EHR clinical data. A key requirement however is obtaining meaningful insights from high dimensional, sparse and complex clinical data. Data science approaches typically address this challenge by performing feature learning in order to build more reliable and informative feature representations from clinical data followed by supervised learning. In this paper, we propose a predictive modeling approach based on deep learning based feature representations and word embedding techniques. Our method uses different deep architectures (stacked sparse autoencoders, deep belief network, adversarial autoencoders and variational autoencoders) for feature representation in higher-level abstraction to obtain effective and robust features from EHRs, and then build prediction models on top of them. Our approach is particularly useful when the unlabeled data is abundant whereas labeled data is scarce. We investigate the performance of representation learning through a supervised learning approach. Our focus is to present a comparative study to evaluate the performance of different deep architectures through supervised learning and provide insights in the choice of deep feature representation techniques. Our experiments demonstrate that for small data sets, stacked sparse autoencoder demonstrates a superior generality performance in prediction due to sparsity regularization whereas variational autoencoders outperform the competing approaches for large data sets due to its capability of learning the representation distribution.",0
"Title: Representation Learning with Autoencoders for Electronic Health Records: A Comparative Study  Electronic health records (EHRs) contain vast amounts of valuable data that can be utilized for machine learning tasks such as predictive modeling, risk stratification, and phenotyping. However, extracting meaningful features from EHRs remains challenging due to their unstructured nature and high dimensionality. To address these limitations, representation learning techniques have gained popularity in recent years. In particular, autoencoder neural networks aim to learn compact representations of raw input data by training them to reconstruct inputs from latent embeddings.  In this study, we conducted a comparative analysis of different types of autoencoders, including variational autoencoders (VAEs), denoising autoencoders (DAEs), and generative adversarial networks (GANs). Our primary focus was on evaluating how each type of autoencoder performed on a specific task involving mortality prediction using EHR data from a large hospital system. We compared our findings against benchmark models commonly used in clinical research, such as logistic regression and decision trees, to demonstrate the utility of representation learning approaches for healthcare applications.  We found that all three types of autoencoders outperformed traditional models, particularly DAEs which exhibited superior accuracy across multiple evaluation metrics. VAEs were competitive but yielded slightly worse results than DAEs. GANs did not perform well likely due to their instability during training. Overall, our work highlights the promise of autoencoder neural networks for improving the performance of EHR-driven predictions. Further investigation into hyperparameter tuning and other network architectures may lead to even more promising results. By advancing the state-of-the-art in representation learning for EHRs, we hope to contribute towards better patient care through improved decision making supported by advanced analytics technologies.",1
"The representation used for Facial Expression Recognition (FER) usually contain expression information along with other variations such as identity and illumination. In this paper, we propose a novel Disentangled Expression learning-Generative Adversarial Network (DE-GAN) to explicitly disentangle facial expression representation from identity information. In this learning by reconstruction method, facial expression representation is learned by reconstructing an expression image employing an encoder-decoder based generator. This expression representation is disentangled from identity component by explicitly providing the identity code to the decoder part of DE-GAN. The process of expression image reconstruction and disentangled expression representation learning is improved by performing expression and identity classification in the discriminator of DE-GAN. The disentangled facial expression representation is then used for facial expression recognition employing simple classifiers like SVM or MLP. The experiments are performed on publicly available and widely used face expression databases (CK+, MMI, Oulu-CASIA). The experimental results show that the proposed technique produces comparable results with state-of-the-art methods.",0
"In order to accurately recognize facial expressions, traditional methods require precise feature extraction from individual images which can vary greatly due to lighting conditions, pose variations, etc. However, disentangled adversarial learning (DAL) has proven to successfully separate complex factors of variation like illumination or pose from raw image pixels to generate more robust features that generalize better across different datasets and tasks, including recognition of facial expressions. Our research proposes a novel deep neural network architecture using DAL to learn a more compact representation suitable for training an expression classifier. We test our model on three widely used benchmarks: FER2013, RAF-DB, and SFEW-2.0 to show improved performance over previous state-of-the-art models and baseline methods. Additionally we provide visualizations to validate the learned representations disentangle identity, head pose, occlusion/expression and other confounding factors often found in human faces. These improvements bring us closer to realizing systems capable of accurate recognition of facial expressions under uncontrolled environments.",1
"Deep learning models have achieved huge success in numerous fields, such as computer vision and natural language processing. However, unlike such fields, it is hard to apply traditional deep learning models on the graph data due to the 'node-orderless' property. Normally, adjacency matrices will cast an artificial and random node-order on the graphs, which renders the performance of deep models on graph classification tasks extremely erratic, and the representations learned by such models lack clear interpretability. To eliminate the unnecessary node-order constraint, we propose a novel model named Isomorphic Neural Network (IsoNN), which learns the graph representation by extracting its isomorphic features via the graph matching between input graph and templates. IsoNN has two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component utilizes a set of subgraph templates as the kernel variables to learn the possible subgraph patterns existing in the input graph and then computes the isomorphic features. A set of permutation matrices is used in the component to break the node-order brought by the matrix representation. Three fully-connected layers are used as the classification component in IsoNN. Extensive experiments are conducted on benchmark datasets, the experimental results can demonstrate the effectiveness of ISONN, especially compared with both classic and state-of-the-art graph classification methods.",0
"This is a great opportunity to learn more about how artificial intelligence can be used to make recommendations. One example of this could be a movie recommendation system that uses AI algorithms to suggest movies to users based on their past viewing history. However, there are many other applications of recommender systems as well, such as online shopping platforms suggesting products to customers based on previous purchases. Another interesting application of these systems would be educational software, where students might receive personalized learning materials tailored specifically to their needs. Recommendation models have been studied extensively over recent years by researchers from various fields ranging from social science, mathematics, statistics and computer science and has proven extremely successful in numerous real world scenarios including online advertising, entertainment, e-commerce, education etc. There are two main approaches to building recommender systems; Collaborative filtering techniques like Matrix factorization which build predictive models based on similarity measures computed using historical interaction data collected from multiple users. Content based filtering techniques use explicit attributes associated with items and user preferences (rating). Hybrid systems combine both collaborative and content based filtering to achieve better results. Deep learning methods for learning representation using neural networks has gained popularity due to their ability to capture complex relationships between variables, often resulting in state-of-the art performance compared to traditional machine learning methods. In particular convolutional neural network architectures (CNNs) have shown success in graph analysis tasks (e.g., classification, clustering). As graphs typically represent data points along with connections among them, Convolution Operators perform local operations only on neighborhoods and then aggregate features across vertices/edges depending on the specific design choice. Therefore",1
"An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .",0
"Abstract:  In recent years, deep learning has made significant progress in achieving state-of-the-art performance on many tasks, but these models remain vulnerable to adversarial examples - small perturbations that cause them to make incorrect predictions. Motivated by this problem, researchers have proposed methods to improve the robustness of learned representations against such attacks, using techniques such as adversarial training and certified robustness. However, there remains a tradeoff between achieving high accuracy and robustness, with most approaches sacrificing some degree of one to obtain the other. In this work, we propose a new approach based on using adversarial robustness as a prior for learned representations. By optimizing both accuracy and robustness jointly during training, our method achieves better overall performance than previous methods. We demonstrate the effectiveness of our approach through extensive experiments across several benchmark datasets and architectures, showing significantly improved results compared to existing techniques. Our work provides a promising direction towards creating more secure and reliable machine learning systems.",1
"Collaborative personalization, such as through learned user representations (embeddings), can improve the prediction accuracy of neural-network-based models significantly. We propose Federated User Representation Learning (FURL), a simple, scalable, privacy-preserving and resource-efficient way to utilize existing neural personalization techniques in the Federated Learning (FL) setting. FURL divides model parameters into federated and private parameters. Private parameters, such as private user embeddings, are trained locally, but unlike federated parameters, they are not transferred to or averaged on the server. We show theoretically that this parameter split does not affect training for most model personalization approaches. Storing user embeddings locally not only preserves user privacy, but also improves memory locality of personalization compared to on-server training. We evaluate FURL on two datasets, demonstrating a significant improvement in model quality with 8% and 51% performance increases, and approximately the same level of performance as centralized training with only 0% and 4% reductions. Furthermore, we show that user embeddings learned in FL and the centralized setting have a very similar structure, indicating that FURL can learn collaboratively through the shared parameters while preserving user privacy.",0
"Machine learning algorithms have been widely used for user representation learning due to their capability to effectively capture patterns from vast amounts of data. However, in many applications, users’ personal data can only be accessed on local devices, which makes collaborative model training difficult. This paper proposes a federated approach for user representation learning that allows multiple parties to jointly train models without sharing raw data. We introduce a novel framework called LocalMixer for performing gradient mixing on client-side, ensuring efficient communication with remote servers. By iteratively computing gradients locally and averaging them among clients, we achieve comparable results to centralized alternatives while maintaining data privacy. Our experiments demonstrate significant performance gains over other federated alternatives in terms of accuracy, convergence speed, and parameter efficiency.",1
"The objective of this paper is self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We make three contributions: First, we introduce the Dense Predictive Coding (DPC) framework for self-supervised representation learning on videos. This learns a dense encoding of spatio-temporal blocks by recurrently predicting future representations; Second, we propose a curriculum training scheme to predict further into the future with progressively less temporal context. This encourages the model to only encode slowly varying spatial-temporal signals, therefore leading to semantic representations; Third, we evaluate the approach by first training the DPC model on the Kinetics-400 dataset with self-supervised learning, and then finetuning the representation on a downstream task, i.e. action recognition. With single stream (RGB only), DPC pretrained representations achieve state-of-the-art self-supervised performance on both UCF101(75.7% top1 acc) and HMDB51(35.7% top1 acc), outperforming all previous learning methods by a significant margin, and approaching the performance of a baseline pre-trained on ImageNet.",0
"This paper presents a new approach for video representation learning called dense predictive coding (DPC). DPC learns representations that capture both spatial and temporal structure present in videos. In contrast to traditional approaches such as convolutional neural networks, which rely on fixed filters to extract features from inputs, our method uses adaptive attention mechanisms to selectively focus on important regions of each frame. By doing so, we can learn more meaningful and discriminative representations that generalize well across multiple tasks. Our experiments demonstrate significant improvements over state-of-the-art methods on challenging benchmark datasets for action recognition, object detection, and image classification. We believe that our work opens up exciting opportunities for future research into video understanding using deep learning techniques.",1
"Incompleteness is a common problem for existing knowledge graphs (KGs), and the completion of KG which aims to predict links between entities is challenging. Most existing KG completion methods only consider the direct relation between nodes and ignore the relation paths which contain useful information for link prediction. Recently, a few methods take relation paths into consideration but pay less attention to the order of relations in paths which is important for reasoning. In addition, these path-based models always ignore nonlinear contributions of path features for link prediction. To solve these problems, we propose a novel KG completion method named OPTransE. Instead of embedding both entities of a relation into the same latent space as in previous methods, we project the head entity and the tail entity of each relation into different spaces to guarantee the order of relations in the path. Meanwhile, we adopt a pooling strategy to extract nonlinear and complex features of different paths to further improve the performance of link prediction. Experimental results on two benchmark datasets show that the proposed model OPTransE performs better than state-of-the-art methods.",0
"In this paper, we propose a novel approach for knowledge graph completion using representation learning with ordered relation paths. We introduce ordered relation paths as a way to encode complex relationships between entities in a knowledge graph and use them as input features for a neural network model. Our method allows us to capture both shortest path relations and indirect dependencies between entities, improving upon previous approaches that only consider direct connections. We evaluate our proposed method on several benchmark datasets and show that it achieves state-of-the-art results across different metrics, outperforming baseline models that rely solely on structural features or textual information. Furthermore, we provide analysis and interpretability methods to gain insights into how our model makes predictions and which relation paths contribute most to its performance. Overall, our work demonstrates the effectiveness of incorporating ordered relation paths into representation learning frameworks for knowledge graph completion tasks.",1
"Hashing has been widely applied to multimodal retrieval on large-scale multimedia data due to its efficiency in computation and storage. Particularly, deep hashing has received unprecedented research attention in recent years, owing to its perfect retrieval performance. However, most of existing deep hashing methods learn binary hash codes by preserving the similarity relationship while without exploiting the semantic labels of data points, which result in suboptimal binary codes. In this work, we propose a novel Deep Semantic Multimodal Hashing Network for scalable multimodal retrieval. In DSMHN, two sets of modality-specific hash functions are jointly learned by explicitly preserving both the inter-modality similarities and the intra-modality semantic labels. Specifically, with the assumption that the learned hash codes should be optimal for task-specific classification, two stream networks are jointly trained to learn the hash functions by embedding the semantic labels on the resultant hash codes. Different from previous deep hashing methods, which are tied to some particular forms of loss functions, the proposed deep hashing framework can be flexibly integrated with different types of loss functions. In addition, the bit balance property is investigated to generate binary codes with each bit having 50% probability to be 1 or -1. Moreover, a unified deep multimodal hashing framework is proposed to learn compact and high-quality hash codes by exploiting the feature representation learning, inter-modality similarity preserving learning, semantic label preserving learning and hash functions learning with bit balanced constraint simultaneously. We conduct extensive experiments for both unimodal and cross-modal retrieval tasks on three widely-used multimodal retrieval datasets. The experimental result demonstrates that DSMHN significantly outperforms state-of-the-art methods.",0
"Abstract: This paper proposes a novel deep semantic multimodal hashing algorithm that effectively embeds visual features into continuous high dimensional space using end-to-end learning, making it ideal for large scale multimedia retrieval tasks such as image search and video annotation. By utilizing multiple modalities including text and audio, our approach achieves state-of-the-art performance on several benchmark datasets. Our method outperforms traditional handcrafted feature extraction techniques by exploiting intermodal redundancies across different modalities in an unsupervised manner. Experimental results demonstrate that the proposed deep semantic multimodal hashing network significantly reduces the computational cost while maintaining high accuracy. Furthermore, we provide insights into how each modality contributes differently to improve overall performance, providing valuable guidelines for future work in this field. Overall, our model represents a significant step towards scalable and efficient multimedia retrieval systems, which can handle complex multimedia data with diverse characteristics.",1
"The goal of this work is to present a systematic solution for RGB-D salient object detection, which addresses the following three aspects with a unified framework: modal-specific representation learning, complementary cue selection and cross-modal complement fusion. To learn discriminative modal-specific features, we propose a hierarchical cross-modal distillation scheme, in which the well-learned source modality provides supervisory signals to facilitate the learning process for the new modality. To better extract the complementary cues, we formulate a residual function to incorporate complements from the paired modality adaptively. Furthermore, a top-down fusion structure is constructed for sufficient cross-modal interactions and cross-level transmissions. The experimental results demonstrate the effectiveness of the proposed cross-modal distillation scheme in zero-shot saliency detection and pre-training on a new modality, as well as the advantages in selecting and fusing cross-modal/cross-level complements.",0
"This paper presents a method for salient object detection using depth maps in conjunction with color images captured by an RGB-D camera system. We build upon recent advances in convolutional neural network (CNN)-based methods for image segmentation by integrating and adapting two existing approaches that have been successful for saliency prediction in color imagery alone. Our approach first learns features from both color and depth data using separate networks pre-trained on large datasets, then fuses these feature representations through element-wise addition to produce a final saliency map. In order to evaluate our proposed method we perform experiments on three benchmark datasets commonly used in computer vision research, as well as compare against state-of-the-art color-only based approaches using mean average precision metric. Results show that our proposed method can effectively capture scene context from RGB-D data without significantly impacting computational efficiency and outperforms most state-of-the-art color salient object detectors. Our contributions in this work aim towards improving accuracy in scene understanding tasks which rely on accurate object detection, such as robotics, augmented reality and human-computer interaction applications.",1
"Increasing volume of Electronic Health Records (EHR) in recent years provides great opportunities for data scientists to collaborate on different aspects of healthcare research by applying advanced analytics to these EHR clinical data. A key requirement however is obtaining meaningful insights from high dimensional, sparse and complex clinical data. Data science approaches typically address this challenge by performing feature learning in order to build more reliable and informative feature representations from clinical data followed by supervised learning. In this paper, we propose a predictive modeling approach based on deep learning based feature representations and word embedding techniques. Our method uses different deep architectures (stacked sparse autoencoders, deep belief network, adversarial autoencoders and variational autoencoders) for feature representation in higher-level abstraction to obtain effective and robust features from EHRs, and then build prediction models on top of them. Our approach is particularly useful when the unlabeled data is abundant whereas labeled data is scarce. We investigate the performance of representation learning through a supervised learning approach. Our focus is to present a comparative study to evaluate the performance of different deep architectures through supervised learning and provide insights in the choice of deep feature representation techniques. Our experiments demonstrate that for small data sets, stacked sparse autoencoder demonstrates a superior generality performance in prediction due to sparsity regularization whereas variational autoencoders outperform the competing approaches for large data sets due to its capability of learning the representation distribution",0
"Increasingly large quantities of electronic health record (EHR) data have become available as more hospitals transition from paper records to digital formats. While EHRs contain valuable medical information that can improve patient outcomes if utilized effectively, their size and complexity make them difficult to analyze manually. As such, machine learning techniques have been developed to extract meaningful insights from these massive datasets. This study compares two popular methods of representation learning using autoencoders: variational autoencoders (VAEs) and generative adversarial networks (GANs). Our findings suggest that VAEs tend to perform better than GANs on smaller datasets but may suffer from instability issues during training, while GANs often require larger datasets but achieve higher levels of stability. Ultimately, both models offer promising results and could aid medical professionals in making informed decisions based on EHR analysis.",1
"Scene graphs have become an important form of structured knowledge for tasks such as for image generation, visual relation detection, visual question answering, and image retrieval. While visualizing and interpreting word embeddings is well understood, scene graph embeddings have not been fully explored. In this work, we train scene graph embeddings in a layout generation task with different forms of supervision, specifically introducing triplet super-vision and data augmentation. We see a significant performance increase in both metrics that measure the goodness of layout prediction, mean intersection-over-union (mIoU)(52.3% vs. 49.2%) and relation score (61.7% vs. 54.1%),after the addition of triplet supervision and data augmentation. To understand how these different methods affect the scene graph representation, we apply several new visualization and evaluation methods to explore the evolution of the scene graph embedding. We find that triplet supervision significantly improves the embedding separability, which is highly correlated with the performance of the layout prediction model.",0
"This research proposes a novel method for scene graph embeddings that take into account triplets (i.e., three objects in a row) rather than pairs of objects. In traditional scene graphs, each object has relationships with other objects such as ""on,"" ""is a part of"" or ""contains."" Our new approach uses a combination of deep learning techniques including attention mechanisms, message passing neural networks, and contrastive learning to represent these triples and encode them into numerical vectors. We show that using triplets leads to improved accuracy on downstream tasks compared to previous methods that only use pairwise relations. Our evaluation includes experiments across multiple domains and datasets showing consistent improvements over baselines. Finally we discuss future directions for expanding the scope of our approach and potential applications of our model outside computer vision tasks.",1
"Multimodal datasets contain an enormous amount of relational information, which grows exponentially with the introduction of new modalities. Learning representations in such a scenario is inherently complex due to the presence of multiple heterogeneous information channels. These channels can encode both (a) inter-relations between the items of different modalities and (b) intra-relations between the items of the same modality. Encoding multimedia items into a continuous low-dimensional semantic space such that both types of relations are captured and preserved is extremely challenging, especially if the goal is a unified end-to-end learning framework. The two key challenges that need to be addressed are: 1) the framework must be able to merge complex intra and inter relations without losing any valuable information and 2) the learning model should be invariant to the addition of new and potentially very different modalities. In this paper, we propose a flexible framework which can scale to data streams from many modalities. To that end we introduce a hypergraph-based model for data representation and deploy Graph Convolutional Networks to fuse relational information within and across modalities. Our approach provides an efficient solution for distributing otherwise extremely computationally expensive or even unfeasible training processes across multiple-GPUs, without any sacrifices in accuracy. Moreover, adding new modalities to our model requires only an additional GPU unit keeping the computational time unchanged, which brings representation learning to truly multimodal datasets. We demonstrate the feasibility of our approach in the experiments on multimedia datasets featuring second, third and fourth order relations.",0
"Avoid excessive use of technical terms and jargon in the abstract. Begin with broad strokes on why representation learning is important. Follow that up with the problem statement - datasets now often have many modalities (e.g., images, textual descriptions, etc.) but current approaches struggle when multiple modalities are combined into one task. Then describe our solution - we propose distributing representations across tasks to improve generalization performance. Cover our contributions including empirical evaluation showing improved generalization over prior methods and theoretical analysis. Finally, summarize future directions from there. This could also be mentioned in your conclusions section as well if you feel more comfortable with organizing like that since those two sections sometimes overlap. HyperLearn: A Distributed Approach for Improving Representation Learning in Multimodal Data Sets. In recent years, representation learning has become increasingly important due to its ability to efficiently learn effective feature representations directly from raw input data, without requiring explicit engineering of features by domain experts. However, modern datasets often contain many types of modality beyond just image or text, such as audio and video data. These diverse types of data pose significant challenges to traditional machine learning algorithms. We address these challenges through a new approach called ""distributed hyperlearning,"" which improves generalization performance in multimodal tasks. In this paper, we introduce three major components: distributed training objectives for learning shared representations, flexible modality fusion using neural networks, and a model initialization scheme based on self-supervised pretraining. Our experiments evaluate the effectiveness of our approach on several benchmarks, outperforming other state-of-the-art methods on most tasks. Additionally, we provide a theoretical analysis of how distributed hyperlearning can achieve better generalization than previous techniques. Moving forward, our work opens up promising research opportunities for exploring even greater levels of complexity in real world problems, such as semi-synthetic data generation, few-shot adaptation scenarios, or nonlinear mappings within multi-step models.",1
"Information in electronic health records (EHR), such as clinical narratives, examination reports, lab measurements, demographics, and other patient encounter entries, can be transformed into appropriate data representations that can be used for downstream clinical machine learning tasks using representation learning. Learning better representations is critical to improve the performance of downstream tasks. Due to the advances in machine learning, we now can learn better and meaningful representations from EHR through disentangling the underlying factors inside data and distilling large amounts of information and knowledge from heterogeneous EHR sources. In this chapter, we first introduce the background of learning representations and reasons why we need good EHR representations in machine learning for medicine and healthcare in Section 1. Next, we explain the commonly-used machine learning and evaluation methods for representation learning using a deep learning approach in Section 2. Following that, we review recent related studies of learning patient state representation from EHR for clinical machine learning tasks in Section 3. Finally, in Section 4 we discuss more techniques, studies, and challenges for learning natural language representations when free texts, such as clinical notes, examination reports, or biomedical literature are used. We also discuss challenges and opportunities in these rapidly growing research fields.",0
"Abstract:  Electronic health records (EHRs) contain vast amounts of data that can be leveraged to improve patient care and medical research. However, analyzing EHRs can be challenging due to their complex structure and diversity across different institutions. In recent years, representation learning has emerged as a promising approach to encode EHR data into meaningful representations that capture important patterns and relationships within the data. This paper provides a comprehensive review of existing literature on representation learning techniques applied to EHRs. We discuss both supervised and unsupervised methods for encoding clinical notes, laboratory results, medications, diagnoses, and other types of EHR data. Additionally, we explore how these representations can be used for various tasks such as risk prediction, disease classification, drug safety assessment, and phenotyping patients for precision medicine studies. Finally, we identify future directions and open problems in the field of EHR representation learning, including improving interpretability, handling missing data, integrating multiple sources of evidence, and personalizing models to individual patients. Overall, this survey highlights the importance of representation learning for enabling meaningful analysis of EHRs and advancing biomedical informatics research.",1
"Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.",0
"In this paper we present a novel approach for ensembling deep learning models that addresses two key challenges in tabular data: interpretability and bias reduction through diversity. Our method leverages oblivious decision trees (ODTs) as base learners to ensemble deep neural networks (DNNs). These ODTs act as feature detectors by identifying patterns from raw input features, which can then be used as guidance to train DNNs that generalize well across different types of inputs. By doing so, our method improves upon traditional approaches such as bagging or boosting, where DNNs alone may overfit or underfit tabular datasets due to their complex nature. We experimentally demonstrate the effectiveness of our proposed model on several benchmark datasets, showing significant improvements over strong baselines including state-of-the-art methods. Moreover, we analyze the contribution of each individual component of the model and validate the rationale behind using ODTs as a guide to training DNNs. This work represents a step forward towards building more interpretable and robust deep learning solutions for tabular data problems.",1
"Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on static observations: agents should interact with the environment to discover its symmetries. Our experiments can be reproduced in Colab and the code is available on GitHub.",0
"In recent years, representation learning has become increasingly important in the field of artificial intelligence. One challenge faced by researchers is how to learn representations that capture the underlying structure of data while remaining disentangled from each other. This paper presents symmetry as a principled approach towards achieving this goal. We demonstrate how incorporating symmetries into learning objectives can lead to more interpretable and robust models. To achieve these goals, we introduce two methods: (a) regularizing the model to predict symmetric contexts, forcing it to explicitly reason about symmetrical transformations; (b) training with explicit generative tasks in which the generated images respect some symmetric properties. These techniques were applied successfully on several benchmark datasets, providing evidence that leveraging symmetries during training leads to improved disentanglement. Finally, we showcase real-world applications where our framework produces high-quality outputs. Our work suggests that incorporating symmetries into disentangle representation learning is a promising direction, offering new opportunities for exploring interrelationships among variables within datasets.",1
"To learn disentangled representations of facial images, we present a Dual Encoder-Decoder based Generative Adversarial Network (DED-GAN). In the proposed method, both the generator and discriminator are designed with deep encoder-decoder architectures as their backbones. To be more specific, the encoder-decoder structured generator is used to learn a pose disentangled face representation, and the encoder-decoder structured discriminator is tasked to perform real/fake classification, face reconstruction, determining identity and estimating face pose. We further improve the proposed network architecture by minimising the additional pixel-wise loss defined by the Wasserstein distance at the output of the discriminator so that the adversarial framework can be better trained. Additionally, we consider face pose variation to be continuous, rather than discrete in existing literature, to inject richer pose information into our model. The pose estimation task is formulated as a regression problem, which helps to disentangle identity information from pose variations. The proposed network is evaluated on the tasks of pose-invariant face recognition (PIFR) and face synthesis across poses. An extensive quantitative and qualitative evaluation carried out on several controlled and in-the-wild benchmarking datasets demonstrates the superiority of the proposed DED-GAN method over the state-of-the-art approaches.",0
"In recent years, generative adversarial networks (GANs) have shown great promise for generating high quality synthetic data from latent representations of real images. However, training GANs often requires large amounts of labeled data, which can be difficult and expensive to obtain, particularly in tasks such as facial representation learning where labels like age, gender, race may not always be available or accurate. To address these challenges, we propose using dual encoder-decoder GANs to learn disentangled representations of faces that capture important attributes without relying on explicit supervision. Our approach utilizes two encoders, one trained to predict the input image and the other trained to predict a binary code representing relevant attributes, allowing us to disentangle the different factors of variation present in face images while still preserving their structure. Experimental results demonstrate that our method outperforms baseline models in terms of visual fidelity, attribute prediction accuracy and robustness to noise and missing data, making it a powerful tool for unsupervised facial representation learning.",1
"Systems that can associate images with their spoken audio captions are an important step towards visually grounded language learning. We describe a scalable method to automatically generate diverse audio for image captioning datasets. This supports pretraining deep networks for encoding both audio and images, which we do via a dual encoder that learns to align latent representations from both modalities. We show that a masked margin softmax loss for such models is superior to the standard triplet loss. We fine-tune these models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art results---improving recall in the top 10 from 29.6% to 49.5%. We also obtain human ratings on retrieval outputs to better assess the impact of incidentally matching image-caption pairs that were not associated in the data, finding that automatic evaluation substantially underestimates the quality of the retrieved results.",0
"This paper proposes a novel approach for large-scale representation learning using visually grounded untranscribed speech. We use deep neural networks to extract audio features that can capture both acoustic and semantic representations of spoken language. Our method leverages visual contextual information to improve the quality of these feature extractions, enabling more robust recognition of spoken words. Our experiments show significant improvement over baseline methods on a variety of benchmark datasets, demonstrating the effectiveness of our approach. Additionally, we analyze the learned representations to better understand their properties and limitations. Overall, this work presents a promising new direction for developing efficient models for real-world applications such as voice assistants, automatic transcription, and multimedia retrieval.",1
"Cross-domain sentiment analysis is currently a hot topic in the research and engineering areas. One of the most popular frameworks in this field is the domain-invariant representation learning (DIRL) paradigm, which aims to learn a distribution-invariant feature representation across domains. However, in this work, we find out that applying DIRL may harm domain adaptation when the label distribution $\rm{P}(\rm{Y})$ changes across domains. To address this problem, we propose a modification to DIRL, obtaining a novel weighted domain-invariant representation learning (WDIRL) framework. We show that it is easy to transfer existing SOTA DIRL models to WDIRL. Empirical studies on extensive cross-domain sentiment analysis tasks verified our statements and showed the effectiveness of our proposed solution.",0
"In recent years, deep learning techniques have been applied successfully across domains to solve many natural language processing problems. However, one major challenge facing these approaches remains: how can we train models that generalize well from source data to perform accurate predictions on target tasks? This study proposes a novel model architecture based on Weighted Convolutional Neural Networks (CNN) with attention mechanisms. Our results demonstrate significant improvements in cross-domain sentiment analysis compared to traditional baseline methods. Further analyses show that our proposed architecture effectively captures both low-level features and high-level semantics while maintaining domain invariance. By utilizing multi-scale visualization techniques, we provide insights into the learned representations which further support our claims. Overall, this work contributes important advances towards better understanding and improving upon the limitations of current NLP transfer learning techniques.",1
"Metadata are general characteristics of the data in a well-curated and condensed format, and have been proven to be useful for decision making, knowledge discovery, and also heterogeneous data organization of biobank. Among all data types in the biobank, pathology is the key component of the biobank and also serves as the gold standard of diagnosis. To maximize the utility of biobank and allow the rapid progress of biomedical science, it is essential to organize the data with well-populated pathology metadata. However, manual annotation of such information is tedious and time-consuming. In the study, we develop a multimodal multitask learning framework to predict four major slide-level metadata of pathology images. The framework learns generalizable representations across tissue slides, pathology reports, and case-level structured data. We demonstrate improved performance across all four tasks with the proposed method compared to a single modal single task baseline on two test sets, one external test set from a distinct data source (TCGA) and one internal held-out test set (TTH). In the test sets, the performance improvements on the averaged area under receiver operating characteristic curve across the four tasks are 16.48% and 9.05% on TCGA and TTH, respectively. Such pathology metadata prediction system may be adopted to mitigate the effort of expert annotation and ultimately accelerate the data-driven research by better utilization of the pathology biobank.",0
"In this paper, we propose a novel approach to predicting metadata associated with pathological biopsies using multimodal data sources. By leveraging multiple types of imaging data along with clinical patient information, our method provides improved accuracy over traditional single modality approaches. Our model utilizes deep learning techniques to learn representations that capture both task-specific features as well as shared patterns across modalities. We evaluate our approach on a large dataset consisting of breast cancer biopsy samples and demonstrate significant improvement compared to baseline methods. These results showcase the potential of multimodal representation learning for improving biobank metadata prediction and ultimately advancing medical research and practice.",1
"Aiming towards human-level generalization, there is a need to explore adaptable representation learning methods with greater transferability. Most existing approaches independently address task-transferability and cross-domain adaptation, resulting in limited generalization. In this paper, we propose UM-Adapt - a unified framework to effectively perform unsupervised domain adaptation for spatially-structured prediction tasks, simultaneously maintaining a balanced performance across individual tasks in a multi-task setting. To realize this, we propose two novel regularization strategies; a) Contour-based content regularization (CCR) and b) exploitation of inter-task coherency using a cross-task distillation module. Furthermore, avoiding a conventional ad-hoc domain discriminator, we re-utilize the cross-task distillation loss as output of an energy function to adversarially minimize the input domain discrepancy. Through extensive experiments, we demonstrate superior generalizability of the learned representations simultaneously for multiple tasks under domain-shifts from synthetic to natural environments. UM-Adapt yields state-of-the-art transfer learning results on ImageNet classification and comparable performance on PASCAL VOC 2007 detection task, even with a smaller backbone-net. Moreover, the resulting semi-supervised framework outperforms the current fully-supervised multi-task learning state-of-the-art on both NYUD and Cityscapes dataset.",0
"This paper presents a new method for unsupervised multi-task adaptation using adversarial cross-task distillation (UM-Adapt). We address the problem of learning multiple tasks without explicit labels on any task by exploiting the shared representations learned from the other tasks as supervision. Our approach is based on generating synthetic tasks as surrogates that resemble the target task distribution as closely as possible. These synthetic tasks provide additional regularization during training while preventing negative transfer. We showcase state-of-the-art results on several benchmark datasets across different domains, such as vision, language, and game playing, demonstrating the effectiveness of our method against existing multi-task adaptation techniques. Our work has implications for enabling efficient use of data resources by reusing the knowledge gained from pre-trained models trained elsewhere on similar domains.",1
"Typically, a medical image offers spatial information on the anatomy (and pathology) modulated by imaging specific characteristics. Many imaging modalities including Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) can be interpreted in this way. We can venture further and consider that a medical image naturally factors into some spatial factors depicting anatomy and factors that denote the imaging characteristics. Here, we explicitly learn this decomposed (disentangled) representation of imaging data, focusing in particular on cardiac images. We propose Spatial Decomposition Network (SDNet), which factorises 2D medical images into spatial anatomical factors and non-spatial modality factors. We demonstrate that this high-level representation is ideally suited for several medical image analysis tasks, such as semi-supervised segmentation, multi-task segmentation and regression, and image-to-image synthesis. Specifically, we show that our model can match the performance of fully supervised segmentation models, using only a fraction of the labelled images. Critically, we show that our factorised representation also benefits from supervision obtained either when we use auxiliary tasks to train the model in a multi-task setting (e.g. regressing to known cardiac indices), or when aggregating multimodal data from different sources (e.g. pooling together MRI and CT data). To explore the properties of the learned factorisation, we perform latent-space arithmetic and show that we can synthesise CT from MR and vice versa, by swapping the modality factors. We also demonstrate that the factor holding image specific information can be used to predict the input modality with high accuracy. Code will be made available at https://github.com/agis85/anatomy_modality_decomposition.",0
"In this work, we aim to address the challenge of learning disentangled representations in cardiac image analysis tasks. We introduce a novel method that leverages adversarial training to learn meaningful features that separate out different aspects of the data. Our approach enables efficient encoding of complex biomedical signals, while providing interpretability and improved generalization performance. Experiments on publicly available datasets demonstrate significant improvements over state-of-the-art methods in multiple cardiovascular imaging modalities such as ECG, MRI, CT, X-ray, etc. This research has important implications for clinicians and scientists who seek to better understand heart disease through advanced machine learning techniques. By enabling more accurate diagnosis and treatment planning, our method holds great potential for positively impacting patient care.",1
"This paper proposes a new deep-learning method to construct test statistics by computer vision and metrics learning. The application highlighted in this paper is applying computer vision on Q-Q plot to construct a new test statistic for normality test. To the best of our knowledge, there is no similar application documented in the literature. Traditionally, there are two families of approaches for verifying the probability distribution of a random variable. Researchers either subjectively assess the Q-Q plot or objectively use a mathematical formula, such as Kolmogorov-Smirnov test, to formally conduct a normality test. Graphical assessment by human beings is not rigorous whereas normality test statistics may not be accurate enough when the uniformly most powerful test does not exist. It may take tens of years for statistician to develop a new test statistic that is more powerful statistically. Our proposed method integrates four components based on deep learning: an image representation learning component of a Q-Q plot, a dimension reduction component, a metrics learning component that best quantifies the differences between two Q-Q plots for normality test, and a new normality hypothesis testing process. Our experimentation results show that the machine-learning-based test statistics can outperform several widely-used traditional normality tests. This study provides convincing evidence that the proposed method could objectively create a powerful test statistic based on Q-Q plots and this method could be modified to construct many more powerful test statistics for other applications in the future.",0
"This should be your own original work. No copy/pasting! --- Computer vision has gained significant attention in recent years due to its ability to extract insights from visual data in various domains such as medical diagnosis, facial recognition, object detection, and image quality assessment. In many cases, hypothesis testing plays a critical role in computer vision tasks where statistical inference is required. To perform hypothesis testing effectively, appropriate metrics need to be chosen based on the nature of data distributions. This research focuses on developing a new approach for applying hypothesis tests that leverages both computer vision techniques and advanced metrics learning methods. Our proposed method utilizes a quantile-quantile (Q-Q) plot for normality testing, which provides a graphical representation of how well data fits a theoretical distribution. By employing an adaptive algorithm, we aim to optimize parameters associated with the choice of test statistic and null distribution. In addition to establishing a framework for our novel application, we evaluated the effectiveness of our method through experiments conducted on diverse datasets. Results demonstrated improved accuracy and efficiency compared to traditional approaches, offering promising evidence of potential real-world applications across different industries. We conclude by discussing future extensions of our work and its broader implications in the field of computer vision.",1
"Dashboard cameras capture a tremendous amount of driving scene video each day. These videos are purposefully coupled with vehicle sensing data, such as from the speedometer and inertial sensors, providing an additional sensing modality for free. In this work, we leverage the large-scale unlabeled yet naturally paired data for visual representation learning in the driving scenario. A representation is learned in an end-to-end self-supervised framework for predicting dense optical flow from a single frame with paired sensing data. We postulate that success on this task requires the network to learn semantic and geometric knowledge in the ego-centric view. For example, forecasting a future view to be seen from a moving vehicle requires an understanding of scene depth, scale, and movement of objects. We demonstrate that our learned representation can benefit other tasks that require detailed scene understanding and outperforms competing unsupervised representations on semantic segmentation.",0
"This paper presents Visuomotor Understanding (VUM), which learns joint representations of driving scenes from images and control signals using reinforcement learning. By optimizing both visual recognition and task performance, VUM achieves state-of-the-art actionability on two demanding benchmarks: urban scene understanding and driver assist tasks. Our results demonstrate that VUM substantially advances image interpretation by exploiting feedback signals and is transferable across diverse challenges. -----",1
"Pedestrian detection in crowded scenes is a challenging problem, because occlusion happens frequently among different pedestrians. In this paper, we propose an effective and efficient detection network to hunt pedestrians in crowd scenes. The proposed method, namely PedHunter, introduces strong occlusion handling ability to existing region-based detection networks without bringing extra computations in the inference stage. Specifically, we design a mask-guided module to leverage the head information to enhance the feature representation learning of the backbone network. Moreover, we develop a strict classification criterion by improving the quality of positive samples during training to eliminate common false positives of pedestrian detection in crowded scenes. Besides, we present an occlusion-simulated data augmentation to enrich the pattern and quantity of occlusion samples to improve the occlusion robustness. As a consequent, we achieve state-of-the-art results on three pedestrian detection datasets including CityPersons, Caltech-USA and CrowdHuman. To facilitate further studies on the occluded pedestrian detection in surveillance scenes, we release a new pedestrian dataset, called SUR-PED, with a total of over 162k high-quality manually labeled instances in 10k images. The proposed dataset, source codes and trained models will be released.",0
"This paper presents a novel approach for pedestrian detection in crowded scenes using deep learning techniques. The proposed method, called ""PedHunter,"" addresses the challenge of occlusions caused by overlapping objects such as vehicles and buildings that often obstruct pedestrians in complex urban environments. Our method leverages a cascading framework composed of two stages to progressively detect pedestrians at different scales while dealing with occlusions effectively. In addition, we introduce a new loss function based on mutual information to enhance localization accuracy and reduce false positives near object boundaries. Experimental results show that our method outperforms state-of-the-art approaches in terms of precision, recall, and FROC metrics on popular benchmark datasets, demonstrating its effectiveness in handling challenging scenarios. The code and models used in this study are publicly available to encourage further research in this field.",1
"In recent years, representation learning approaches have disrupted many multimedia computing tasks. Among those approaches, deep convolutional neural networks (CNNs) have notably reached human level expertise on some constrained image classification tasks. Nonetheless, training CNNs from scratch for new task or simply new data turns out to be complex and time-consuming. Recently, transfer learning has emerged as an effective methodology for adapting pre-trained CNNs to new data and classes, by only retraining the last classification layer. This paper focuses on improving this process, in order to better transfer knowledge between CNN architectures for faster trainings in the case of fine tuning for image classification. This is achieved by combining and transfering supplementary weights, based on similarity considerations between source and target classes. The study includes a comparison between semantic and content-based similarities, and highlights increased initial performances and training speed, along with superior long term performances when limited training samples are available.",0
"In order to improve Convolutional Neural Networks (CNN) training efficiency, knowledge transfer can play a crucial role by leveraging existing knowledge from similar tasks. This research presents an approach that utilizes both semantic and visual representations in determining task similarity for efficient knowledge transfer. By considering both aspects concurrently, our method enables more accurate identification of semantically related tasks while reducing computational costs. We evaluate our approach on several benchmark datasets and demonstrate significant improvements over previous methods in terms of accuracy and speed. Our findings showcase the potential of using semantic and visual similarities in facilitating efficient knowledge transfer during CNN training.",1
"Efficient representation of patients is very important in the healthcare domain and can help with many tasks such as medical risk prediction. Many existing methods, such as diagnostic Cost Groups (DCG), rely on expert knowledge to build patient representation from medical data, which is resource consuming and non-scalable. Unsupervised machine learning algorithms are a good choice for automating the representation learning process. However, there is very little research focusing on onpatient-level representation learning directly from medical claims. In this paper, weproposed a novel patient vector learning architecture that learns high quality,fixed-length patient representation from claims data. We conducted several experiments to test the quality of our learned representation, and the empirical results show that our learned patient vectors are superior to vectors learned through other methods including a popular commercial model. Lastly, we provide potential clinical interpretation for using our representation on predictive tasks, as interpretability is vital in the healthcare domain",0
"Abstract: This study presents a novel approach for predicting medical costs using distributed representations of patient data. We used unstructured electronic health record (EHR) notes as input features and trained a neural network model to learn meaningful latent vectors representing individual patients. These vector embeddings capture important patterns from the raw text data, enabling accurate predictions of future medical expenses based on historical records alone. Our method achieved high accuracy across multiple datasets and outperformed traditional statistical models that rely solely on demographic and clinical variables. By leveraging deep learning techniques to encode complex patient information into compact representations, we can improve the precision and robustness of medical cost projections in healthcare decision making. Implications for efficient resource allocation and personalized medicine are discussed.",1
"Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",0
"This paper presents VideoBERT (VB), a novel model that jointly learns video and language representations. VB leverages uncaptioned videos by predicting tokens from other modalities given image and text prompts as well as masked tokens sampled from nearby frames. Unlike prior methods that learn separate language and vision representations then concatenate them for fine-tuning on downstream tasks, VB can perform multiple crossmodal fusion operations during pretraining. Experiments show improved performance compared to strong baselines across both intrinsic evaluation metrics like GLUE benchmark tests and extrinsic metrics like object detection and segmentation on Kinetics-400. Analysis reveals that VB effectively encodes interdependencies among different types of modality signals. Additionally, human evaluations demonstrate VB's generated subtitles outperform those produced by alternative transformer models on summarization quality and visual grounding, suggesting better understanding of input videos. Overall, our work demonstrates the effectiveness of learning tight couplings between video and language through end-to-end training using carefully designed multi-modal fusion mechanisms.",1
"We study on weakly-supervised object detection (WSOD) which plays a vital role in relieving human involvement from object-level annotations. Predominant works integrate region proposal mechanisms with convolutional neural networks (CNN). Although CNN is proficient in extracting discriminative local features, grand challenges still exist to measure the likelihood of a bounding box containing a complete object (i.e., ""objectness""). In this paper, we propose a novel WSOD framework with Objectness Distillation (i.e., WSOD^2) by designing a tailored training mechanism for weakly-supervised object detection. Multiple regression targets are specifically determined by jointly considering bottom-up (BU) and top-down (TD) objectness from low-level measurement and CNN confidences with an adaptive linear combination. As bounding box regression can facilitate a region proposal learning to approach its regression target with high objectness during training, deep objectness representation learned from bottom-up evidences can be gradually distilled into CNN by optimization. We explore different adaptive training curves for BU/TD objectness, and show that the proposed WSOD^2 can achieve state-of-the-art results.",0
"In our work, we address the problem of weakly-supervised object detection using deep learning techniques which leverage both bottom-up and top-down cues. This approach takes advantage of the complementary nature of these cues to improve accuracy in detecting objects without requiring large amounts of annotated data. Our method involves training two models, one that uses region proposal networks (RPNs) to generate object proposals based on low-level features, and another that refines these proposals through attention mechanisms guided by high-level image features. This allows us to distill the bottom-up objectness cue into high-quality object predictions while simultaneously providing the opportunity for top-down feedback to further enhance localization performance. We evaluate our model on several benchmark datasets and achieve competitive results compared to fully supervised methods that rely heavily on manual annotations. Our contributions provide insight into the effectiveness of integrating bottom-up and top-down cues towards enhancing weakly supervised object detection performance without compromising efficiency in annotation efforts. Note that the exact output may differ from the above template based on variations in the content of the text but it should follow the same format. The output should contain at least 4 sentences describing the methodology used, evaluation metric and results, and implications discussed in conclusion. Additionally there should be at most 3 figures or tables in total including any examples if applicable. If you would like a more detailed explanation please feel free to ask.",1
"We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g.\ phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.",0
"This paper presents a method for unsupervised learning of speech representations using generative adversarial networks (GANs) and autoregressive neural network models called WaveNets. We show that these systems can effectively learn to generate realistic speech signals from random noise inputs, and furthermore, we demonstrate how these generated signals can be used as input to train audio classification models. Our results indicate that WaveNet autoencoder models trained on untranscribed speech data can perform well compared to models trained on transcribed data. Additionally, we present experiments showing that our system outperforms state-of-the-art GAN-based methods for generating raw waveform data. Overall, our work shows promise towards developing powerful and efficient tools for automatic speech recognition tasks such as ASR training data generation and model initialization.",1
"Graph neural networks (GNN) has been successfully applied to operate on the graph-structured data. Given a specific scenario, rich human expertise and tremendous laborious trials are usually required to identify a suitable GNN architecture. It is because the performance of a GNN architecture is significantly affected by the choice of graph convolution components, such as aggregate function and hidden dimension. Neural architecture search (NAS) has shown its potential in discovering effective deep architectures for learning tasks in image and language modeling. However, existing NAS algorithms cannot be directly applied to the GNN search problem. First, the search space of GNN is different from the ones in existing NAS work. Second, the representation learning capacity of GNN architecture changes obviously with slight architecture modifications. It affects the search efficiency of traditional search methods. Third, widely used techniques in NAS such as parameter sharing might become unstable in GNN.   To bridge the gap, we propose the automated graph neural networks (AGNN) framework, which aims to find an optimal GNN architecture within a predefined search space. A reinforcement learning based controller is designed to greedily validate architectures via small steps. AGNN has a novel parameter sharing strategy that enables homogeneous architectures to share parameters, based on a carefully-designed homogeneity definition. Experiments on real-world benchmark datasets demonstrate that the GNN architecture identified by AGNN achieves the best performance, comparing with existing handcrafted models and tradistional search methods.",0
"In recent years, graph neural networks (GNNs) have gained significant popularity due to their ability to effectively handle graph data, which is prevalent in many real-world applications such as social network analysis, bioinformatics, and computer vision. However, designing efficient GNN architectures that can effectively learn from large datasets remains a challenging task. This paper presents Auto-GNN, a novel algorithm for automating the search process of optimal neural architectures for GNNs.  The proposed method leverages differentiable architecture search techniques to generate new models iteratively, using a simple yet effective search space that includes operations commonly found in state-of-the-art GNNs. We utilize a surrogate model based on Kronecker product approximation to accelerate the computationally expensive architecture evaluation process, enabling us to efficiently evaluate hundreds of thousands of models. Additionally, we introduce a regularization mechanism inspired by evolutionary algorithms, aimed at exploring diverse solution spaces beyond a single neighborhood size.  We conduct comprehensive experiments across several benchmark datasets for node classification tasks, demonstrating that our method outperforms various baseline methods, including manually designed architectures and other NAS approaches. Further ablation studies show that each component of the proposed framework contributes positively towards the final performance gain. Our findings suggest that Auto-GNN is an effective tool for discovering high-quality GNN architectures tailored to specific problem domains, significantly reducing manual effort while achieving competitive results compared to human expert designs.",1
"To explore underlying complementary information from multiple views, in this paper, we propose a novel Latent Multi-view Semi-Supervised Classification (LMSSC) method. Unlike most existing multi-view semi-supervised classification methods that learn the graph using original features, our method seeks an underlying latent representation and performs graph learning and label propagation based on the learned latent representation. With the complementarity of multiple views, the latent representation could depict the data more comprehensively than every single view individually, accordingly making the graph more accurate and robust as well. Finally, LMSSC integrates latent representation learning, graph construction, and label propagation into a unified framework, which makes each subtask optimized. Experimental results on real-world benchmark datasets validate the effectiveness of our proposed method.",0
"In recent years, semi-supervised learning has become increasingly popular due to its ability to leverage large amounts of unlabeled data to improve classification accuracy. One challenge faced by existing semi-supervised approaches is their reliance on single modalities (such as images) and limited fusion techniques to combine multi-modal inputs.  Our proposed approach, latent multi-view semi-supervised classification (LMVSSL), addresses these limitations by leveraging multiple views of the same object and utilizing deep neural networks (DNNs) to learn a shared latent space where the different views can be fused seamlessly. Our method extends traditional semi-supervised methods by integrating both labeled and unlabeled samples from all available views into the training process, resulting in improved performance compared to other state-of-the-art multi-modal and semi-supervised approaches.  We evaluate LMVSSL on several benchmark datasets including MNIST, CIFAR-10, SVHN, NORB, and STL-10, demonstrating significant improvement over baseline models and achieving comparable results to fully supervised DNNs trained using only small amounts of labelled data. Furthermore, we investigate the effectiveness of our model under various scenarios such as varying numbers of labeled samples, number of views, and network architectures.  In summary, our work presents a novel framework that effectively combines multiple views of objects in a semi-supervised setting to achieve improved classification performance, paving the way towards broader adoption of multi-modal applications in real-world settings.",1
"We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.",0
"One of the key challenges facing machine learning models today is finding the balance between robustness and accuracy. In recent years, there has been a growing focus on increasing the robustness of these models by making them more resistant to adversarial attacks and other forms of input perturbation. However, as we have seen from several high-profile examples, increased robustness often comes at the cost of reduced model accuracy. This can lead to situations where well-crafted adversarial inputs cause the model to make errors that would not occur under normal operating conditions.  This paper examines the relationship between robustness and accuracy in depth, using both theoretical analysis and empirical experiments. We find that while it is possible to increase robustness without sacrificing accuracy, doing so requires careful attention to the design of the model architecture and training process. Furthermore, our results suggest that the tradeoff between robustness and accuracy may depend strongly on the specific task and dataset being considered, making it difficult to generalize across different domains. Overall, our work highlights the need for a nuanced understanding of the strengths and weaknesses of different approaches to improving the robustness of machine learning models. By carefully evaluating their performance against relevant benchmarks and metrics, practitioners can select methods that strike the appropriate balance between resilience and correctness in each application context.",1
"One of the most prevalent symptoms among the elderly population, dementia, can be detected by classifiers trained on linguistic features extracted from narrative transcripts. However, these linguistic features are impacted in a similar but different fashion by the normal aging process. Aging is therefore a confounding factor, whose effects have been hard for machine learning classifiers (especially deep neural network based models) to ignore. We show DNN models are capable of estimating ages based on linguistic features. Predicting dementia based on this aging bias could lead to potentially non-generalizable accuracies on clinical datasets, if not properly deconfounded.   In this paper, we propose to address this deconfounding problem with fair representation learning. We build neural network classifiers that learn low-dimensional representations reflecting the impacts of dementia yet discarding the effects of age. To evaluate these classifiers, we specify a model-agnostic score $\Delta_{eo}^{(N)}$ measuring how classifier results are deconfounded from age. Our best models compromise accuracy by only 2.56\% and 1.54\% on two clinical datasets compared to DNNs, and their $\Delta_{eo}^{(2)}$ scores are better than statistical (residulization and inverse probability weight) adjustments.",0
"""This"" can only refer to one thing. In this case there is no such single thing as ""this"" that requires summarizing in the form of an abstract. I suggest you rewrite your request without using any pronouns. Let me know if you want me to provide some options.",1
"In representation learning and non-linear dimension reduction, there is a huge interest to learn the 'disentangled' latent variables, where each sub-coordinate almost uniquely controls a facet of the observed data. While many regularization approaches have been proposed on variational autoencoders, heuristic tuning is required to balance between disentanglement and loss in reconstruction accuracy -- due to the unsupervised nature, there is no principled way to find an optimal weight for regularization. Motivated to completely bypass regularization, we consider a projection strategy: modifying the canonical Gaussian encoder, we add a layer of scaling and rotation to the Gaussian mean, such that the marginal correlations among latent sub-coordinates become exactly zero. This achieves a theoretically maximal disentanglement, as guaranteed by zero cross-correlation between one latent sub-coordinate and the observed varying with the rest. Unlike regularizations, the extra projection layer does not impact the flexibility of the previous encoder layers, leading to almost no loss in expressiveness. This approach is simple to implement in practice. Our numerical experiments demonstrate very good performance, with no tuning required.",0
"In this work we present a novel method for disentangling complex data sets without relying on any tuning parameters. By leveraging a carefully designed projection operation that captures low-dimensional relationships between variables, our approach can effectively separate underlying factors while requiring minimal user input or domain expertise. Our experimental results demonstrate that our method outperforms state-of-the-art techniques across several benchmark datasets and applications, including image generation, anomaly detection, and style transfer. This shows that our simple yet powerful framework holds great promise for wide adoption and use cases beyond traditional machine learning tasks. Furthermore, we investigate the limitations of current techniques and highlight future research directions towards more robust and flexible disentangled representations.",1
"Existing deep learning models may encounter great challenges in handling graph structured data. In this paper, we introduce a new deep learning model for graph data specifically, namely the deep loopy neural network. Significantly different from the previous deep models, inside the deep loopy neural network, there exist a large number of loops created by the extensive connections among nodes in the input graph data, which makes model learning an infeasible task. To resolve such a problem, in this paper, we will introduce a new learning algorithm for the deep loopy neural network specifically. Instead of learning the model variables based on the original model, in the proposed learning algorithm, errors will be back-propagated through the edges in a group of extracted spanning trees. Extensive numerical experiments have been done on several real-world graph datasets, and the experimental results demonstrate the effectiveness of both the proposed model and the learning algorithm in handling graph data.",0
"This research presents a deep neural network model for graph structured data representation learning. Traditional methods for handling graph structure data often suffer from limited expressivity due to their reliance on fixed, handcrafted features or linear latent variable models that cannot capture complex relationships within the data. Our proposed method addresses these limitations by leveraging recent advances in deep learning techniques such as convolutional networks to learn representations directly from raw graph signals while preserving their underlying topology. We showcase the effectiveness of our approach using several benchmark datasets and demonstrate its superior performance over state-of-the art baselines across multiple tasks including node classification, edge prediction, and graph level regression problems. By bridging the gap between traditional machine learning approaches and emerging deep learning architectures, we provide novel insights into how these powerful models can be applied to graph structured data representation problems. Overall, our work represents a significant step towards enabling more accurate predictions and unlocking new possibilities in graph analysis applications.",1
"Most popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To identify these useful features, we resort to a negative bag consisting of features that are known to be irrelevant, for example, they are sampled either from datasets that are unrelated to our actions of interest or are CNN features produced via random noise as input. With the features from the video as a positive bag and the irrelevant features as the negative bag, we cast an objective to learn a (nonlinear) hyperplane that separates the unknown useful features from the rest in a multiple instance learning formulation within a support vector machine setup. We use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they can be treated as a weighted average pooling of the features from the bags, with zero weights given to non-support vectors. Our pooling scheme is end-to-end trainable within a deep learning framework. We report results from experiments on eight computer vision benchmark datasets spanning a variety of video-related tasks and demonstrate state-of-the-art performance across these tasks.",0
"In recent years, video data has become increasingly important due to advances in technology that allow cameras to capture higher quality images at greater resolutions. As a result, there is a need for improved methods of processing and analyzing these large amounts of video data. This paper proposes the use of support vector classifiers (SVC) as a method for discriminative representation learning from videos. SVC have been shown to be effective in many applications, including image recognition and speech classification, but their effectiveness for representing video data remains largely unexplored.  In order to evaluate the effectiveness of using SVC for video representation learning, we conducted experiments on several datasets consisting of both short and long videos covering different categories such as human actions and scenery changes. Our results showed that the proposed method outperformed other state-of-the-art techniques by achieving high accuracy rates and producing meaningful representations that can be used for downstream tasks such as action recognition, abnormality detection, and object tracking.  The contribution of our work lies in demonstrating the feasibility of applying SVC to learn discriminative video representations and enabling efficient solutions for processing and understanding large volumes of video data. We believe that further research into this area has great potential benefits for real-world applications such as surveillance systems, autonomous vehicles, and health monitoring devices. Ultimately, the goal of this research is to improve the efficiency and effectiveness of working with video data in various domains and provide better insights through advanced machine learning algorithms.",1
"To improve the ability of VAE to disentangle in the latent space, existing works mostly focus on enforcing independence among the learned latent factors. However, the ability of these models to disentangle often decreases as the complexity of the generative factors increases. In this paper, we investigate the little-explored effect of the modeling capacity of a posterior density on the disentangling ability of the VAE. We note that the independence within and the complexity of the latent density are two different properties we constrain when regularizing the posterior density: while the former promotes the disentangling ability of VAE, the latter -- if overly limited -- creates an unnecessary competition with the data reconstruction objective in VAE. Therefore, if we preserve the independence but allow richer modeling capacity in the posterior density, we will lift this competition and thereby allow improved independence and data reconstruction at the same time. We investigate this theoretical intuition with a VAE that utilizes a non-parametric latent factor model, the Indian Buffet Process (IBP), as a latent density that is able to grow with the complexity of the data. Across three widely-used benchmark data sets and two clinical data sets little explored for disentangled learning, we qualitatively and quantitatively demonstrated the improved disentangling performance of IBP-VAE over the state of the art. In the latter two clinical data sets riddled with complex factors of variations, we further demonstrated that unsupervised disentangling of nuisance factors via IBP-VAE -- when combined with a supervised objective -- can not only improve task accuracy in comparison to relevant supervised deep architectures but also facilitate knowledge discovery related to task decision-making. A shorter version of this work will appear in the ICDM 2019 conference proceedings.",0
"Title: Improving Disentangled Representation Learning with the Beta Bernoulli Process  Abstract: In recent years, disentanglement has emerged as a key principle for learning meaningful representations in deep neural networks. However, current methods struggle to capture complex relationships among factors of variation. To address these limitations, we propose using the beta Bernoulli process (BBP), which models joint probability distributions over discrete variables by composing simpler sub-probability functions. This allows us to learn more fine-grained and entangled representations without sacrificing interpretability. We show that our approach outperforms state-of-the-art techniques on benchmark datasets, achieving higher levels of disentangling while producing more robust results across different metrics. Our framework is flexible enough to accommodate both linear and nonlinear generative models, making it applicable to a wide range of applications, from computer vision to natural language processing. By explicitly modeling dependencies among dimensions within latent spaces, our method offers new insights into how disentanglement can be improved, providing novel perspectives on representation learning at large.",1
"Research on graph representation learning has received a lot of attention in recent years since many data in real-world applications come in form of graphs. High-dimensional graph data are often in irregular form, which makes them more difficult to analyze than image/video/audio data defined on regular lattices. Various graph embedding techniques have been developed to convert the raw graph data into a low-dimensional vector representation while preserving the intrinsic graph properties. In this review, we first explain the graph embedding task and its challenges. Next, we review a wide range of graph embedding techniques with insights. Then, we evaluate several state-of-the-art methods against small and large datasets and compare their performance. Finally, potential applications and future directions are presented.",0
"This survey provides a comprehensive overview of graph representation learning techniques and their applications across different domains. We present the most popular methods, including those based on random walks, deep learning, and more recent hybrid approaches that leverage both local and global structural information. Furthermore, we discuss the challenges faced by researchers working on graphs, such as sparsity, scalability, nonlinearity, heterogeneity, and noise. By reviewing key advances made in the field so far, we aim to identify new opportunities and directions for future work. In summary, our goal is to provide readers with a solid foundation in graph representation learning and inspire them to develop novel solutions for complex data analysis tasks.",1
"When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling.",0
"This paper presents a novel approach for deep latent variable modeling of text that outperforms state-of-the-art methods on several benchmark datasets. We introduce an unconventional architecture called DLVM+, which combines high-dimensional embeddings and low-rank matrix factorization into a single framework. Our key insight is that these components complement each other, enabling more effective capture of both local and global structure in text data. Empirical results demonstrate significant improvements over strong baselines such as LDA, VAE, and AE models across metrics like perplexity, log-likelihood, heldout likelihood, F1 score, and topic coherence. Furthermore, we analyze qualitative aspects of our model, including interpretability and robustness, confirming its effectiveness as a tool for NLP tasks such as language generation and sentiment analysis. Overall, DLVM+ establishes itself as a surprisingly effective solution for deep latent variable modeling of text.",1
"Residual representation learning simplifies the optimization problem of learning complex functions and has been widely used by traditional convolutional neural networks. However, it has not been applied to deep neural decision forest (NDF). In this paper we incorporate residual learning into NDF and the resulting model achieves state-of-the-art level accuracy on three public age estimation benchmarks while requiring less memory and computation. We further employ gradient-based technique to visualize the decision-making process of NDF and understand how it is influenced by facial image inputs. The code and pre-trained models will be available at https://github.com/Nicholasli1995/VisualizingNDF.",0
"As artificial intelligence continues to advance, so too has the ability to accurately estimate human age using facial features alone. In recent years, deep neural networks have been used to analyze images of faces and make predictions about their age. However, most current approaches use multiple stages that can lead to confusion and poor performance. The new method proposed in this study uses deep residual decision making (DRDM) to improve upon existing models and achieve higher accuracy rates. By combining the simplicity of linear regression with the power of deep learning techniques like convolutional neural networks (CNNs), our model outperforms other state-of-the art methods on two benchmark databases: UTKFace Ageing II and FG-Ageing. Our results show promising potential for the application of DRDM to other challenges faced by computer vision researchers.",1
"For many computer vision applications such as image captioning, visual question answering, and person search, learning discriminative feature representations at both image and text level is an essential yet challenging problem. Its challenges originate from the large word variance in the text domain as well as the difficulty of accurately measuring the distance between the features of the two modalities. Most prior work focuses on the latter challenge, by introducing loss functions that help the network learn better feature representations but fail to account for the complexity of the textual input. With that in mind, we introduce TIMAM: a Text-Image Modality Adversarial Matching approach that learns modality-invariant feature representations using adversarial and cross-modal matching objectives. In addition, we demonstrate that BERT, a publicly-available language model that extracts word embeddings, can successfully be applied in the text-to-image matching domain. The proposed approach achieves state-of-the-art cross-modal matching performance on four widely-used publicly-available datasets resulting in absolute improvements ranging from 2% to 5% in terms of rank-1 accuracy.",0
"In many applications that involve images, textual descriptions are often used as input to guide image generation systems, such as GANs (Generative Adversarial Networks). However, these models can suffer from mode collapse, which occurs when they generate only one particular output instead of sampling from a diverse range of possibilities. To address this issue, we propose adversarial representation learning for text-to-image matching (ARTM), wherein two separate generators are trained on opposite sides of an autoencoder-based discriminator network. Our method leverages unpaired data by using the reconstruction loss as the consistency term for training. Experimental results show that ARTM outperforms state-of-the-art text-to-image methods on both quantitative metrics and human evaluations, demonstrating its effectiveness in generating more realistic and diverse images. Additionally, our model has potential applications beyond just text-to-image problems, making it a valuable contribution to the field of computer vision.",1
"Using touch devices to navigate in virtual 3D environments such as computer assisted design (CAD) models or geographical information systems (GIS) is inherently difficult for humans, as the 3D operations have to be performed by the user on a 2D touch surface. This ill-posed problem is classically solved with a fixed and handcrafted interaction protocol, which must be learned by the user. We propose to automatically learn a new interaction protocol allowing to map a 2D user input to 3D actions in virtual environments using reinforcement learning (RL). A fundamental problem of RL methods is the vast amount of interactions often required, which are difficult to come by when humans are involved. To overcome this limitation, we make use of two collaborative agents. The first agent models the human by learning to perform the 2D finger trajectories. The second agent acts as the interaction protocol, interpreting and translating to 3D operations the 2D finger trajectories from the first agent. We restrict the learned 2D trajectories to be similar to a training set of collected human gestures by first performing state representation learning, prior to reinforcement learning. This state representation learning is addressed by projecting the gestures into a latent space learned by a variational auto encoder (VAE).",0
​,1
"In this paper, we propose a novel self-supervised representation learning by taking advantage of a neighborhood-relational encoding (NRE) among the training data. Conventional unsupervised learning methods only focused on training deep networks to understand the primitive characteristics of the visual data, mainly to be able to reconstruct the data from a latent space. They often neglected the relation among the samples, which can serve as an important metric for self-supervision. Different from the previous work, NRE aims at preserving the local neighborhood structure on the data manifold. Therefore, it is less sensitive to outliers. We integrate our NRE component with an encoder-decoder structure for learning to represent samples considering their local neighborhood information. Such discriminative and unsupervised representation learning scheme is adaptable to different computer vision tasks due to its independence from intense annotation requirements. We evaluate our proposed method for different tasks, including classification, detection, and segmentation based on the learned latent representations. In addition, we adopt the auto-encoding capability of our proposed method for applications like defense against adversarial example attacks and video anomaly detection. Results confirm the performance of our method is better or at least comparable with the state-of-the-art for each specific application, but with a generic and self-supervised approach.",0
"This work proposes a novel approach to representation learning using self-supervision based on neighborhood relationships within data sets. By encoding these neighborly interactions into features that can be used by downstream models, we improve their accuracy without requiring large amounts of labeled training data. We demonstrate the effectiveness of our method through experiments on several benchmark datasets, showing consistent improvements across multiple domains including image classification, sentiment analysis, and more. Our contributions provide new insights into self-supervised learning methods, enabling better performance with less supervision and paving the way for further advances in machine learning applications.",1
"In this paper, we study the problem of node representation learning with graph neural networks. We present a graph neural network class named recurrent graph neural network (RGNN), that address the shortcomings of prior methods. By using recurrent units to capture the long-term dependency across layers, our methods can successfully identify important information during recursive neighborhood expansion. In our experiments, we show that our model class achieves state-of-the-art results on three benchmarks: the Pubmed, Reddit, and PPI network datasets. Our in-depth analyses also demonstrate that incorporating recurrent units is a simple yet effective method to prevent noisy information in graphs, which enables a deeper graph neural network.",0
"Title: Improving Graph Convolutional Networks via Dynamic Filtering Mechanisms Authors: [your name(s)] Abstract: This work addresses the issue of limited expressiveness caused by fixed filters in traditional graph convolution (GCN) networks. Recent research has shown that GCNs tend to suffer from over-smoothing, where node representations become increasingly homogeneous as message passing propagates through multiple layers. To alleviate this problem, we propose two novel dynamic filtering mechanisms based on residual connections and adaptive gating functions. Our methods enhance GCN capabilities by preserving important features while minimizing noise from irrelevant neighbors. Experimental results demonstrate significant improvements in model accuracy across diverse domains such as social network analysis, bioinformatics, and computer vision tasks. Keywords: Graph representation learning, graph neural networks, deep learning, filter design Introduction Graph representation learning refers to techniques designed to learn lowdimensional embeddings that capture the structure of graphs while retaining relevant information for downstream applications. In recent years, graph convolutional networks have emerged as powerful models for inductive representation learning due to their ability to efficiently handle large graphs without requiring explicit computation of eigenvectors or other global graph statistics. Despite these advantages, GCN architectures rely exclusively on fixed kernels learned during training to perform neighborhood aggregations. These static filters can limit their effectiveness at capturing complex relationships among nodes since they need to balance smoothness requirements against local variations arising from irregular topologies like sparse regions, disconnected components, or clusters. Motivated by these limitations, we focus our attention o",1
"Recent binary representation learning models usually require sophisticated binary optimization, similarity measure or even generative models as auxiliaries. However, one may wonder whether these non-trivial components are needed to formulate practical and effective hashing models. In this paper, we answer the above question by proposing an embarrassingly simple approach to binary representation learning. With a simple classification objective, our model only incorporates two additional fully-connected layers onto the top of an arbitrary backbone network, whilst complying with the binary constraints during training. The proposed model lower-bounds the Information Bottleneck (IB) between data samples and their semantics, and can be related to many recent `learning to hash' paradigms. We show that, when properly designed, even such a simple network can generate effective binary codes, by fully exploring data semantics without any held-out alternating updating steps or auxiliary models. Experiments are conducted on conventional large-scale benchmarks, i.e., CIFAR-10, NUS-WIDE, and ImageNet, where the proposed simple model outperforms the state-of-the-art methods.",0
"This paper presents a novel approach to learning binary representations using simple yet effective methods that prioritize ease of implementation over mathematical complexity. We demonstrate how embarrassingly simple models can achieve state-of-the-art results on several benchmark datasets across multiple domains such as image classification, text generation, and recommendation systems. Our key insight lies in identifying the most crucial components of complex architectures and distilling them into their core essence while minimizing unnecessary details. As a result, our method yields compact models that require fewer parameters and less computational overhead, making them ideal for deployment on resource-constrained devices and efficient inference. In summary, we propose a new perspective on representation learning that emphasizes pragmatism over theory and showcases the feasibility of producing high-quality models without resorting to exotic techniques or excessive model capacity. By doing so, we hope to inspire future research exploring simpler alternatives for achieving excellent performance on real-world problems.",1
"Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. In this module, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. Specifically, our DSSINet achieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9% on UCF-QNRF dataset against the state-of-the-art methods.",0
"This paper presents a novel approach to crowd counting using deep neural networks. We introduce the concept of scale integration network (SIN), which integrates multi-scale features extracted from a convolutional neural network into the prediction process. Our proposed method outperforms state-of-the-art methods by accurately estimating human counts in images containing crowded scenes. We evaluate our model on two benchmark datasets: UCSD and ShanghaiTech, demonstrating significant improvements over previous work. Additionally, we provide comprehensive analysis and ablation studies that validate the effectiveness of each component in our SIN architecture. Overall, our results showcase the potential of integrating scales within deep learning models for reliable and accurate crowd counting predictions.",1
"In this paper, we propose a robust representation learning model called Adaptive Structure-constrained Low-Rank Coding (AS-LRC) for the latent representation of data. To recover the underlying subspaces more accurately, AS-LRC seamlessly integrates an adaptive weighting based block-diagonal structure-constrained low-rank representation and the group sparse salient feature extraction into a unified framework. Specifically, AS-LRC performs the latent decomposition of given data into a low-rank reconstruction by a block-diagonal codes matrix, a group sparse locality-adaptive salient feature part and a sparse error part. To enforce the block-diagonal structures adaptive to different real datasets for the low-rank recovery, AS-LRC clearly computes an auto-weighting matrix based on the locality-adaptive features and multiplies by the low-rank coefficients for direct minimization at the same time. This encourages the codes to be block-diagonal and can avoid the tricky issue of choosing optimal neighborhood size or kernel width for the weight assignment, suffered in most local geometrical structures-preserving low-rank coding methods. In addition, our AS-LRC selects the L2,1-norm on the projection for extracting group sparse features rather than learning low-rank features by Nuclear-norm regularization, which can make learnt features robust to noise and outliers in samples, and can also make the feature coding process efficient. Extensive visualizations and numerical results demonstrate the effectiveness of our AS-LRC for image representation and recovery.",0
"Here is my attempt at writing an informative abstract that provides an overview of the research presented in ""Adaptive Structure-constrained Robust Latent Low-Rank Coding for Image Recovery."" If you have any suggestions on how to improve it please share:  This article presents a novel framework for image recovery using adaptive structure-constrained robust latent low-rank coding (SRLLC). This method addresses the problem of incomplete or corrupted images by leveraging advances in compressed sensing theory, machine learning algorithms, and matrix factorization techniques. SRLLC combines a robust loss function, sparsity constraints, and structured regularizations to enhance the performance of latent low-rank representation. To further strengthen the algorithm's capability, we design an online adaptation approach that enables automatic adjustment of model parameters during the reconstruction process. Simulation results demonstrate that our proposed framework outperforms several state-of-the-art methods under varying degrees of data corruption and noise levels while maintaining high computational efficiency. Our work extends the current understanding of SRLLC in image recovery applications and has potential impacts on related fields such as computer vision, signal processing, and multimedia communications. Overall, this study contributes to the development of effective solutions for recovering images from degraded data sources.",1
"Designing a registration framework for images that do not share the same probability distribution is a major challenge in modern image analytics yet trivial task for the human visual system (HVS). Discrepancies in probability distributions, also known as \emph{drifts}, can occur due to various reasons including, but not limited to differences in sequences and modalities (e.g., MRI T1-T2 and MRI-CT registration), or acquisition settings (e.g., multisite, inter-subject, or intra-subject registrations). The popular assumption about the working of HVS is that it exploits a communal feature subspace exists between the registering images or fields-of-view that encompasses key drift-invariant features. Mimicking the approach that is potentially adopted by the HVS, herein, we present a representation learning technique of this invariant communal subspace that is shared by registering domains. The proposed communal domain learning (CDL) framework uses a set of hierarchical nonlinear transforms to learn the communal subspace that minimizes the probability differences and maximizes the amount of shared information between the registering domains. Similarity metric and parameter optimization calculations for registration are subsequently performed in the drift-minimized learned communal subspace. This generic registration framework is applied to register multisequence (MR: T1, T2) and multimodal (MR, CT) images. Results demonstrated generic applicability, consistent performance, and statistically significant improvement for both multi-sequence and multi-modal data using the proposed approach ($p$-value$0.001$; Wilcoxon rank sum test) over baseline methods.",0
"This paper presents a novel approach to image registration called communal domain learning (CDL), which combines deep neural network based feature extraction and dimensionality reduction techniques to establish correspondences in drifted image spaces. Our method adapts to the shift caused by changes in illumination conditions as well as camera parameters such as exposure time or sensor sensitivity. We validate our technique on several challenging datasets including images taken under different lighting conditions and those containing objects that have been occluded or added over time. Results show significant improvements compared to state-of-the-art methods, demonstrating the effectiveness of CDL in coping with large transformations across multiple domains.",1
"Manifold learning seeks a low dimensional representation that faithfully captures the essence of data. Current methods can successfully learn such representations, but do not provide a meaningful set of operations that are associated with the representation. Working towards operational representation learning, we endow the latent space of a large class of generative models with a random Riemannian metric, which provides us with elementary operators. As computational tools are unavailable for random Riemannian manifolds, we study deterministic approximations and derive tight error bounds on expected distances.",0
"This research explores expected path lengths on random manifolds, providing insight into how network structure can impact communication efficiency. Previous studies have largely focused on Euclidean spaces, but our analysis shows that manifold geometry has significant effects on routing metrics such as average distance and edge coverage. By applying random geometric graph models to manifolds, we reveal new connections between topology and transportation networks. Our findings offer valuable guidance for designing efficient communication systems in real-world settings. Overall, this work extends our understanding of graph theory and demonstrates the importance of considering non-Euclidean geometries in network optimization.",1
"Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph Representation Learning (SSGRL) framework that consists of two crucial modules: 1) a semantic decoupling module that incorporates category semantics to guide learning semantic-specific representations and 2) a semantic interaction module that correlates these representations with a graph built on the statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a sizable margin, e.g. with an mAP improvement of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC 2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our codes and models are available at https://github.com/HCPLab-SYSU/SSGRL.",0
"In recent years, image recognition has become increasingly important in computer vision tasks such as object detection, semantic segmentation, and multi-label classification. However, traditional approaches using handcrafted features have limited performance due to their lack of robustness to scale variations, occlusions, and changes in illumination conditions. To overcome these limitations, deep learning methods based on convolutional neural networks (CNNs) have been introduced, achieving state-of-the-art results. While successful, these models can suffer from overfitting if not properly regularized. One approach to combat this issue is by leveraging structured representations that incorporate prior knowledge about the problem at hand. In this work, we propose a novel method for representing images using graphs that capture the spatial relationships between objects within each scene. Our model learns a weighted graph representation that captures both global contextual information and local geometric cues. We evaluate our approach on two popular benchmark datasets for multi-label image classification: PASCAL VOC2007 and SUN09. Experimental results demonstrate significant improvements over baseline CNN architectures and other competitive methods, while outperforming several recently published works in the field. Overall, our work represents an important step towards more efficient, scalable, and accurate solutions for large-scale image recognition problems.",1
"Unsupervised representation learning has succeeded with excellent results in many applications. It is an especially powerful tool to learn a good representation of environments with partial or noisy observations. In partially observable domains it is important for the representation to encode a belief state, a sufficient statistic of the observations seen so far. In this paper, we investigate whether it is possible to learn such a belief representation using modern neural architectures. Specifically, we focus on one-step frame prediction and two variants of contrastive predictive coding (CPC) as the objective functions to learn the representations. To evaluate these learned representations, we test how well they can predict various pieces of information about the underlying state of the environment, e.g., position of the agent in a 3D maze. We show that all three methods are able to learn belief representations of the environment, they encode not only the state information, but also its uncertainty, a crucial aspect of belief states. We also find that for CPC multi-step predictions and action-conditioning are critical for accurate belief representations in visually complex environments. The ability of neural representations to capture the belief information has the potential to spur new advances for learning and planning in partially observable domains, where leveraging uncertainty is essential for optimal decision making.",0
"This abstract provides an overview of the concepts discussed in our paper ""Neural Predictive Belief Representations"". We propose that belief representations can be generated by neural networks using predictive processing frameworks. These beliefs can then be used as input to downstream models for various tasks such as reasoning and planning. Our approach builds on recent work in deep learning and Bayesian inference, and we show that it can lead to state-of-the-art performance across multiple domains. Additionally, we discuss potential applications and future directions for research in this area. Overall, our findings have important implications for artificial intelligence and machine learning research.",1
"A key challenge for autonomous driving is safe trajectory planning in cluttered, urban environments with dynamic obstacles, such as pedestrians, bicyclists, and other vehicles. A reliable prediction of the future environment, including the behavior of dynamic agents, would allow planning algorithms to proactively generate a trajectory in response to a rapidly changing environment. We present a novel framework that predicts the future occupancy state of the local environment surrounding an autonomous agent by learning a motion model from occupancy grid data using a neural network. We take advantage of the temporal structure of the grid data by utilizing a convolutional long-short term memory network in the form of the PredNet architecture. This method is validated on the KITTI dataset and demonstrates higher accuracy and better predictive power than baseline methods.",0
"This research proposes a novel approach for dynamic environment prediction in urban scenes by leveraging recurrent representation learning techniques. We address the challenge posed by complex environments where static scene understanding methods cannot capture the nuances in changes over time. Our method learns the temporal dependencies inherent in such scenes from multiple heterogeneous data sources and generates accurate predictions of future states. Experimental results demonstrate the effectiveness of our proposed solution, surpassing baseline models and achieving state-of-the-art performance on standard benchmark datasets. By advancing the field of dynamic environment prediction, we lay a foundation for developing intelligent systems that can function autonomously and efficiently within diverse, rapidly evolving settings. Overall, our work holds promise for numerous real-world applications, ranging from traffic management to personal navigation assistance.",1
"With emergence of blockchain technologies and the associated cryptocurrencies, such as Bitcoin, understanding network dynamics behind Blockchain graphs has become a rapidly evolving research direction. Unlike other financial networks, such as stock and currency trading, blockchain based cryptocurrencies have the entire transaction graph accessible to the public (i.e., all transactions can be downloaded and analyzed). A natural question is then to ask whether the dynamics of the transaction graph impacts the price of the underlying cryptocurrency. We show that standard graph features such as degree distribution of the transaction graph may not be sufficient to capture network dynamics and its potential impact on fluctuations of Bitcoin price. In contrast, the new graph associated topological features computed using the tools of persistent homology, are found to exhibit a high utility for predicting Bitcoin price dynamics. %explain higher order interactions among the nodes in Blockchain graphs and can be used to build much more accurate price prediction models. Using the proposed persistent homology-based techniques, we offer a new elegant, easily extendable and computationally light approach for graph representation learning on Blockchain.",0
"Artificial Intelligence (AI) has seen numerous applications across many industries, from healthcare to finance, leading to significant advancements. With increasing amounts of data being generated every day by these systems, there arises the need to effectively analyze them using scalable machine learning methods. In recent years, blockchain technology has emerged as a viable solution for storing and processing large datasets, making it possible to build secure and efficient decentralized platforms. In our work, we propose ChainNet: a novel framework that leverages the unique topological properties of graph structures present in blockchains such as Ethereum, allowing for efficient model training and inference without compromising security. Our approach involves designing a custom architecture tailored to process transactions and messages from the underlying blockchain network. We explore various optimization techniques like parameter pruning and quantization to improve resource utilization while maintaining high accuracy. Additionally, we provide a thorough evaluation of ChainNet using standard benchmarks and real-world experiments on real-life datasets, demonstrating its effectiveness compared to state-of-the-art centralized models. Our contributions can have far-reaching implications for several use cases within the domain of distributed computing, including prediction markets, financial derivatives, and supply chain management among others. As more organizations look towards implementing decentralized solutions for their data needs, frameworks like ChainNet will become crucial building blocks towards achieving seamless integration into existing infrastructure. Overall, we believe that our research paves the way for further innovations in this rapidly evolving field, enabling groundbreaking discoveries at both academic and industrial levels. ----- The world generates vast amounts of data everyday which requires analysis through scalable machi",1
"The advancement of machine learning algorithms has opened a wide scope for vibration-based SHM (Structural Health Monitoring). Vibration-based SHM is based on the fact that damage will alter the dynamic properties viz., structural response, frequencies, mode shapes, etc of the structure. The responses measured using sensors, which are high dimensional in nature, can be intelligently analyzed using machine learning techniques for damage assessment. Neural networks employing multilayer architectures are expressive models capable of capturing complex relationships between input-output pairs but do not account for uncertainty in network outputs. A BNN (Bayesian Neural Network) refers to extending standard networks with posterior inference. It is a neural network with a prior distribution on its weights. Deep learning architectures like CNN (Convolutional neural network) and LSTM(Long Short Term Memory) are good candidates for representation learning from high dimensional data. The advantage of using CNN over multi-layer neural networks is that they are good feature extractors as well as classifiers, which eliminates the need for generating hand-engineered features. LSTM networks are mainly used for sequence modeling. This paper presents both a Bayesian multi-layer perceptron and deep learning-based approach for damage detection and location identification in beam-like structures. Raw frequency response data simulated using finite element analysis is fed as the input of the network. As part of this, frequency response was generated for a series of simulations in the cantilever beam involving different damage scenarios. This case study shows the effectiveness of the above approaches to predict bending rigidity with an acceptable error rate.",0
"This paper presents a case study on structural health monitoring (SHM) of cantilever beam using two advanced techniques: Bayesian neural networks (BNNs) and deep learning. BNNs have been increasingly used for SHM due to their ability to model uncertainty and estimate probabilities of damage states, but their application has been limited by difficulties in obtaining large amounts of data and issues related to prior knowledge elicitation. Deep learning methods have emerged as an alternative approach, offering automatic feature extraction and improved accuracy compared to traditional machine learning algorithms. In this work, we apply these techniques to SHM of a real-world bridge system. Experimental results show that BNN can accurately identify different levels of damage severity, while deep learning achieves superior performance in detecting and diagnosing localized damages. These findings demonstrate the potential of integrating both approaches for comprehensive SHM, providing insights into the strengths and limitations of each method. Overall, this research contributes to advancing SHM technologies through innovative applications of artificial intelligence.",1
"Network alignment is a critical task to a wide variety of fields. Many existing works leverage on representation learning to accomplish this task without eliminating domain representation bias induced by domain-dependent features, which yield inferior alignment performance. This paper proposes a unified deep architecture (DANA) to obtain a domain-invariant representation for network alignment via an adversarial domain classifier. Specifically, we employ the graph convolutional networks to perform network embedding under the domain adversarial principle, given a small set of observed anchors. Then, the semi-supervised learning framework is optimized by maximizing a posterior probability distribution of observed anchors and the loss of a domain classifier simultaneously. We also develop a few variants of our model, such as, direction-aware network alignment, weight-sharing for directed networks and simplification of parameter space. Experiments on three real-world social network datasets demonstrate that our proposed approaches achieve state-of-the-art alignment results.",0
"The paper ""Domain-adversarial Network Alignment"" presents a new approach to domain adaptation that utilizes adversarial training. This method addresses the problem of mismatched domains by aligning the feature distributions across multiple tasks. By doing so, the proposed model can leverage knowledge from one task to improve performance on another related but different task without requiring additional labeled data. We evaluate our approach on several benchmark datasets including OfficeHome, VLCS, and GTAV to show state-of-the-art results. Our work has implications for applications where fine-grained alignment is important such as computer vision and natural language processing.",1
"We treat shape co-segmentation as a representation learning problem and introduce BAE-NET, a branched autoencoder network, for the task. The unsupervised BAE-NET is trained with a collection of un-segmented shapes, using a shape reconstruction loss, without any ground-truth labels. Specifically, the network takes an input shape and encodes it using a convolutional neural network, whereas the decoder concatenates the resulting feature code with a point coordinate and outputs a value indicating whether the point is inside/outside the shape. Importantly, the decoder is branched: each branch learns a compact representation for one commonly recurring part of the shape collection, e.g., airplane wings. By complementing the shape reconstruction loss with a label loss, BAE-NET is easily tuned for one-shot learning. We show unsupervised, weakly supervised, and one-shot learning results by BAE-NET, demonstrating that using only a couple of exemplars, our network can generally outperform state-of-the-art supervised methods trained on hundreds of segmented shapes. Code is available at https://github.com/czq142857/BAE-NET.",0
"Here we present BAE-NET: Branched Autoencoder for Shape Co-Segmentation, which introduces a novel methodology that addresses several challenges faced by traditional methods used for shape co-segmentation tasks. By employing a branched autoencoder architecture, our approach can handle segmentations containing different shapes and sizes within one image. Our experiments demonstrate that BAE-NET outperforms other state-of-the art algorithms, particularly on images with low signal-to-noise ratio. Furthermore, we showcase how our framework enables effective use of unlabeled data without any modifications to the model, enabling robust segmentation results across diverse datasets. In summary, BAE-NET provides a new approach towards solving the complex problem of shape co-segmentation using machine learning techniques.",1
"Network embedding has proved extremely useful in a variety of network analysis tasks such as node classification, link prediction, and network visualization. Almost all the existing network embedding methods learn to map the node IDs to their corresponding node embeddings. This design principle, however, hinders the existing methods from being applied in real cases. Node ID is not generalizable and, thus, the existing methods have to pay great effort in cold-start problem. The heterogeneous network usually requires extra work to encode node types, as node type is not able to be identified by node ID. Node ID carries rare information, resulting in the criticism that the existing methods are not robust to noise.   To address this issue, we introduce Compositional Network Embedding, a general inductive network representation learning framework that generates node embeddings by combining node features based on the principle of compositionally. Instead of directly optimizing an embedding lookup based on arbitrary node IDs, we learn a composition function that infers node embeddings by combining the corresponding node attribute embeddings through a graph-based loss. For evaluation, we conduct the experiments on link prediction under four different settings. The results verified the effectiveness and generalization ability of compositional network embeddings, especially on unseen nodes.",0
"Compositional Network Embedding (CNE) is a novel approach that addresses the challenge of incorporating structure into network embedding methods. Most existing approaches have focused on learning fixed low-dimensional representations for nodes in a graph, but these embeddings often lack insight into how complex relationships among nodes arise from their composition out of simpler interactions. CNE overcomes this limitation by jointly modeling node attributes and graph structure within a deep neural network framework that naturally captures hierarchical compositions.  The proposed method leverages recent advances in message passing neural networks to recursively build up expressive representations for each node in terms of messages exchanged with its neighbors. These messages encode both local context around each node as well as global information shared across distant regions of the graph. By design, the algorithm can capture high-level patterns such as tree structures, cycles, or other recursive motifs, which existing models struggle to learn.  Experiments on a diverse range of benchmark datasets demonstrate the superiority of our approach compared to state-of-the-art baselines. Across various tasks like link prediction, community detection, and visualization, CNE consistently achieves higher accuracy while requiring fewer parameters. The results confirm that capturing fine-grained graph structure through compositional reasoning significantly improves upon standard practices in network analysis.  Overall, Compositional Network Embedding offers a versatile toolkit for understanding complex systems that can handle large graphs without sacrificing interpretability. While the work presented here focuses on static graphs, future research could extend the architecture to encompass time-varying data streams, opening up exciting opportunities for real-time monitoring and adaptive decision making.  Note: Please keep in mind that writing an informative abstract requires a strong understanding of the content area, including the context and literature surrounding th",1
"RGB-Thermal object tracking attempt to locate target object using complementary visual and thermal infrared data. Existing RGB-T trackers fuse different modalities by robust feature representation learning or adaptive modal weighting. However, how to integrate dual attention mechanism for visual tracking is still a subject that has not been studied yet. In this paper, we propose two visual attention mechanisms for robust RGB-T object tracking. Specifically, the local attention is implemented by exploiting the common visual attention of RGB and thermal data to train deep classifiers. We also introduce the global attention, which is a multi-modal target-driven attention estimation network. It can provide global proposals for the classifier together with local proposals extracted from previous tracking result. Extensive experiments on two RGB-T benchmark datasets validated the effectiveness of our proposed algorithm.",0
"This abstract introduces a new method for robust object tracking called target-oriented dual attention (TODA). TODA tracks objects using both image features from RGB images and depth maps from thermal images. By fusing information from two sources, TODA achieves improved accuracy and stability compared to traditional methods that only use RGB data. To demonstrate the effectiveness of our approach, we conducted extensive experiments on public datasets and achieved state-of-the-art performance. In summary, TODA provides a novel solution for robust tracking in complex scenes by leveraging complementary information from different modalities.",1
"Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric.",0
"This paper presents a new approach for learning transferable representations in vision-and-language navigation tasks. We propose using pretrained language models as prior knowledge to guide the learning process and improve generalization performance across multiple environments. Our method leverages both visual input and natural language instructions during training to learn a joint representation that captures important features from both modalities. In addition, we introduce an attention mechanism that focuses on relevant parts of the environment based on the current instruction, further improving the robustness of our model. Extensive experiments show significant improvements over state-of-the-art methods in zero-shot navigation tasks and demonstrate the effectiveness of our proposed approach for transferring learned representations between different domains. Overall, our work represents an important step towards developing more efficient and flexible agents capable of operating in complex and diverse environments.",1
"Unsupervised cross-domain person re-identification (Re-ID) faces two key issues. One is the data distribution discrepancy between source and target domains, and the other is the lack of labelling information in target domain. They are addressed in this paper from the perspective of representation learning. For the first issue, we highlight the presence of camera-level sub-domains as a unique characteristic of person Re-ID, and develop camera-aware domain adaptation to reduce the discrepancy not only between source and target domains but also across these sub-domains. For the second issue, we exploit the temporal continuity in each camera of target domain to create discriminative information. This is implemented by dynamically generating online triplets within each batch, in order to maximally take advantage of the steadily improved feature representation in training process. Together, the above two methods give rise to a novel unsupervised deep domain adaptation framework for person Re-ID. Experiments and ablation studies on benchmark datasets demonstrate its superiority and interesting properties.",0
"This framework has several components, including image alignment, color correction, feature extraction, and machine learning models. Image alignment uses a siamese network architecture trained on pairs of images from both source and target domains to learn a mapping function that projects them into a common subspace, which reduces their differences. Color correction adjusts the hue channel of each image by minimizing pixel-wise correlation across domains. Feature extraction employs three convolutional neural networks pretrained on large-scale datasets (AlexNet, VGGFace2, and ResNeXt) for extracting deep features from all labeled samples in the training set. Machine learning models use these extracted features to train an unsupervised domain adaptation model (a generative adversarial network). Our framework achieves state-of-the-art accuracy in person re-identification tasks against strong baselines (e.g., adaptive boosting, metric learning, and transfer learning) at comparably low computational complexity and memory usage. We evaluate our method on five benchmark datasets: Market1501, DukeMTMC-reID, CUHK03, PRW and iLIDS. Experimental results show significant improvements over several related works, demonstrating high effectiveness under challenging scenarios such as changing lighting conditions, viewpoint variations, background clutter, occlusions, and pose changes.",1
"We consider an information theoretic approach to address the problem of identifying fake digital images. We propose an innovative method to formulate the issue of localizing manipulated regions in an image as a deep representation learning problem using the Information Bottleneck (IB), which has recently gained popularity as a framework for interpreting deep neural networks. Tampered images pose a serious predicament since digitized media is a ubiquitous part of our lives. These are facilitated by the easy availability of image editing software and aggravated by recent advances in deep generative models such as GANs. We propose InfoPrint, a computationally efficient solution to the IB formulation using approximate variational inference and compare it to a numerical solution that is computationally expensive. Testing on a number of standard datasets, we demonstrate that InfoPrint outperforms the state-of-the-art and the numerical solution. Additionally, it also has the ability to detect alterations made by inpainting GANs.",0
"In recent years, there has been increasing interest in using digital image forensics techniques to detect tampering and manipulation of images. One popular approach is to use statistical models such as generative adversarial networks (GANs) to analyze the authenticity of images based on their underlying features. However, these models can often suffer from high dimensional output spaces, leading to poor performance and slow convergence times.  In order to address this issue, we propose using the recently introduced InfoBot method, which uses a regularization technique called the ""information bottleneck"" to compress the model's representations into lower dimensions while preserving important features. This results in improved accuracy and efficiency compared to traditional methods. We evaluate our proposed approach on several benchmark datasets and demonstrate that it outperforms state-of-the-art GAN-based methods. Our work provides new insights into the application of information theory to digital image forensics, and suggests potential future directions for research in this field.",1
"We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-the-art algorithms.",0
"In recent years, unsupervised graph representation learning has gained considerable attention due to its ability to extract meaningful features from large and complex networks without requiring explicit annotations. One popular approach for achieving this task is through the use of graph convolutional autoencoders (GCAEs). These models encode graphs into low-dimensional representations that capture their structural properties while preserving important topological relationships. However, most existing GCAE architectures rely on asymmetrical encodings, which may limit their capacity to fully exploit network structure information. To address this challenge, we propose a novel symmetric graph convolutional autoencoder architecture capable of encoding both sides of each edge for holistic graph representation learning. Our framework introduces dual convolutions at both input and hidden layers to learn bidirectional node relationships. This design allows us to capture more comprehensive neighborhood context and leads to improved performance compared to traditional GCAEs across several benchmark datasets. Extensive experimental results demonstrate the effectiveness and robustness of our proposed method for unsupervised graph representation learning tasks such as clustering and anomaly detection. Overall, our work advances the state of art in graph deep learning research by providing a powerful tool for graph mining and exploration applications.",1
"Architectures for sparse hierarchical representation learning have recently been proposed for graph-structured data, but so far assume the absence of edge features in the graph. We close this gap and propose a method to pool graphs with edge features, inspired by the hierarchical nature of chemistry. In particular, we introduce two types of pooling layers compatible with an edge-feature graph-convolutional architecture and investigate their performance for molecules relevant to drug discovery on a set of two classification and two regression benchmark datasets of MoleculeNet. We find that our models significantly outperform previous benchmarks on three of the datasets and reach state-of-the-art results on the fourth benchmark, with pooling improving performance for three out of four tasks, keeping performance stable on the fourth task, and generally speeding up the training process.",0
"Molecular graph is important data structure widely used in chemistry related field such as material informatics, drug discovery etc. Its application range from QM calculation to ML model which predict property of molecules directly from raw text. Hierarchical representations (HR) aiming at capturing different level of abstraction have been proposed recently as effective method to represent molecule structural features but HR models are typically dense vector which has high computation cost both in memory usage and computation time while applying them into downstream models compared to sparse vectors like fingerprints. In order to combine merits of both HR and fingerprint methods we propose new type of sparse hierarchical representation called MaxScaffold HR. By using MaxScaffolds algorithm as building block, our representation can capture multi scale information adaptively based on the size of molecule itself. Experiment results show that our approach achieve better accuracy comparing several popular benchmark datasets for regression task using GNN models than state of art HR methods while requiring less computational resources both in training and inference phase since our model only require linear scan through graphs instead of cubic scaling computation in message passing steps as traditional GNN architecture.",1
"This paper tackles the problem of large-scale image-based localization (IBL) where the spatial location of a query image is determined by finding out the most similar reference images in a large database. For solving this problem, a critical task is to learn discriminative image representation that captures informative information relevant for localization. We propose a novel representation learning method having higher location-discriminating power. It provides the following contributions: 1) we represent a place (location) as a set of exemplar images depicting the same landmarks and aim to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) we model a similarity measure as a probability distribution on L_2-metric distances between intra-place and inter-place image representations; 3) we propose a new Stochastic Attraction and Repulsion Embedding (SARE) loss function minimizing the KL divergence between the learned and the actual probability distributions; 4) we give theoretical comparisons between SARE, triplet ranking and contrastive losses. It provides insights into why SARE is better by analyzing gradients. Our SARE loss is easy to implement and pluggable to any CNN. Experiments show that our proposed method improves the localization performance on standard benchmarks by a large margin. Demonstrating the broad applicability of our method, we obtained the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our code and model are available at https://github.com/Liumouliu/deepIBL.",0
"This paper presents a novel approach for large scale image localization using a stochastic attraction-repulsion embedding method. The proposed algorithm addresses the challenges of traditional image localization methods by introducing a probabilistic framework that incorporates both attractive and repulsive forces. By modeling these forces, our method can effectively capture the variations in appearance within images and across different scenes.  Our method first preprocesses each query image to generate an embedding map that encodes visual features such as color, texture, and edge information. We then use the KL divergence metric to measure the similarity between pairs of embedded maps, which allows us to estimate the probability of matching between two instances given their feature distributions.  To compute correspondences between query images and reference database images, we adopt a Stochastic Gradient Descent (SGD) optimization technique that jointly estimates camera poses and descriptors. Our SGD formulation explicitly models both attractive and repulsive terms in the energy function, allowing the solver to balance data fit and regularization. Additionally, we introduce a term to enforce spatial smoothness of camera poses, making our formulation more robust to noise and outliers.  We demonstrate the effectiveness of our approach on several benchmark datasets and show improved accuracy compared to state-of-the-art localization algorithms. Our results suggest that our method effectively captures important visual cues from raw images while providing efficient computational performance, making it well suited for real-world applications in robotics and computer vision.",1
"For ego-motion estimation, the feature representation of the scenes is crucial. Previous methods indicate that both the low-level and semantic feature-based methods can achieve promising results. Therefore, the incorporation of hierarchical feature representation may benefit from both methods. From this perspective, we propose a novel direct feature odometry framework, named DFO, for depth estimation and hierarchical feature representation learning from monocular videos. By exploiting the metric distance, our framework is able to learn the hierarchical feature representation without supervision. The pose is obtained with a coarse-to-fine approach from high-level to low-level features in enlarged feature maps. The pixel-level attention mask can be self-learned to provide the prior information. In contrast to the previous methods, our proposed method calculates the camera motion with a direct method rather than regressing the ego-motion from the pose network. With this approach, the consistency of the scale factor of translation can be constrained. Additionally, the proposed method is thus compatible with the traditional SLAM pipeline. Experiments on the KITTI dataset demonstrate the effectiveness of our method.",0
"This paper presents a novel method for unsupervised learning of depth and deep representation for visual odometry using monocular videos in a metric space. The proposed approach leverages recent advances in self-supervised learning techniques that involve training deep neural networks to predict a relative scale factor given two temporally distant frames. Our method builds upon these concepts by additionally predicting depth maps and introducing a novel loss function based on geometric constraints. By formulating depth prediction as a problem of estimating signed distance functions, we can utilize existing methods developed in computer vision literature for scaling depth maps to physical distances while enforcing consistency over time. We evaluate our method on several publicly available datasets including the well known KITTI dataset, demonstrating that it achieves state-of-the-art performance in terms of both accuracy and efficiency compared to other unsupervised approaches. Our findings show promise towards enabling more robust camera motion estimation under challenging conditions such as extreme lighting changes and occlusions without relying on ground truth data annotations or complex pipelines involving multiple sensors or cameras. Furthermore, since the learned representations encode meaningful depth information that generalizes across different environments, they could potentially enhance applications in robotic navigation and autonomous driving, where accurate scene understanding is crucial. Future work involves exploring alternative training strategies and evaluating the impact of incorporating additional semantic information such as object detection or semantic segmentation into the framework. Overall, this study represents an important step forward towards developing fully autonomous systems capable of operating safely and reliably in real-world environments.",1
"Representation learning is a fundamental but challenging problem, especially when the distribution of data is unknown. We propose a new representation learning method, termed Structure Transfer Machine (STM), which enables feature learning process to converge at the representation expectation in a probabilistic way. We theoretically show that such an expected value of the representation (mean) is achievable if the manifold structure can be transferred from the data space to the feature space. The resulting structure regularization term, named manifold loss, is incorporated into the loss function of the typical deep learning pipeline. The STM architecture is constructed to enforce the learned deep representation to satisfy the intrinsic manifold structure from the data, which results in robust features that suit various application scenarios, such as digit recognition, image classification and object tracking. Compared to state-of-the-art CNN architectures, we achieve the better results on several commonly used benchmarks\footnote{The source code is available. https://github.com/stmstmstm/stm }.",0
"Title: “The Structure Transfer Machine” Paper Abstract  Abstract: This paper proposes a new framework called the ""Structure Transfer Machine"" (STM) that can be used for transfer learning across tasks and domains. The proposed method builds on recent advances in deep neural networks by using structure predictions from pretrained models to guide both feature extraction and fine-grained task adaptation. In addition, we introduce a novel graph convolutional layer designed specifically for message passing between graphs of different sizes, allowing us to effectively model complex relationships between objects such as spatial or temporal dependencies. We present experiments evaluating STM’s performance compared with state-of-the art methods across four distinct applications including image classification, object detection, semantic segmentation, and natural language understanding, demonstrating consistent improvement over existing approaches.  Overall, our results showcase how STM’s ability to harness structural priors improves generalization capabilities and adaptability. We believe that this approach has significant implications for future research in unlocking the full potential of neural network architectures while reducing their reliance on large quantities of data or laborious hyperparameter tuning.  Title: “Structured Learning with Graph Convolution Networks” Paper Abstract  Abstract: With the advent of big data, there has been increasing interest in developing machine learning models capable of handling high-dimensional, multi-relational data. To address these challenges, Graph Neural Networks (GNNs) have emerged as a promising technique due to their ability to process graph-structured inputs and capture intricate relationship patterns within them. Despite their successes, traditional GNNs only operate upon nodes and edges, ignoring potentially valuable global features. To bridge this gap, this pape",1
"We propose a novel and unsupervised representation learning model, i.e., Robust Block-Diagonal Adaptive Locality-constrained Latent Representation (rBDLR). rBDLR is able to recover multi-subspace structures and extract the adaptive locality-preserving salient features jointly. Leveraging on the Frobenius-norm based latent low-rank representation model, rBDLR jointly learns the coding coefficients and salient features, and improves the results by enhancing the robustness to outliers and errors in given data, preserving local information of salient features adaptively and ensuring the block-diagonal structures of the coefficients. To improve the robustness, we perform the latent representation and adaptive weighting in a recovered clean data space. To force the coefficients to be block-diagonal, we perform auto-weighting by minimizing the reconstruction error based on salient features, constrained using a block-diagonal regularizer. This ensures that a strict block-diagonal weight matrix can be obtained and salient features will possess the adaptive locality preserving ability. By minimizing the difference between the coefficient and weights matrices, we can obtain a block-diagonal coefficients matrix and it can also propagate and exchange useful information between salient features and coefficients. Extensive results demonstrate the superiority of rBDLR over other state-of-the-art methods.",0
"This paper presents a novel approach for subspace discovery through block-diagonal adaptive locality-constrained representation (BLACR). BLACR enables efficient computation of sparse representations while incorporating robustness against noise and outliers. We demonstrate that our method achieves significant improvements over existing techniques in both synthetic and real-world datasets. Our contributions are threefold: first, we develop an algorithmic framework based on block coordinate descent for solving the nonconvex optimization problem underlying BLACR. Second, we introduce a new rank constraint to enhance the interpretability of the learned lowdimensional structure. Finally, extensive experiments validate the effectiveness and efficiency of our method, making it a competitive alternative for subspace learning tasks.",1
"Generative models for 3D geometric data arise in many important applications in 3D computer vision and graphics. In this paper, we focus on 3D deformable shapes that share a common topological structure, such as human faces and bodies. Morphable Models and their variants, despite their linear formulation, have been widely used for shape representation, while most of the recently proposed nonlinear approaches resort to intermediate representations, such as 3D voxel grids or 2D views. In this work, we introduce a novel graph convolutional operator, acting directly on the 3D mesh, that explicitly models the inductive bias of the fixed underlying graph. This is achieved by enforcing consistent local orderings of the vertices of the graph, through the spiral operator, thus breaking the permutation invariance property that is adopted by all the prior work on Graph Neural Networks. Our operator comes by construction with desirable properties (anisotropic, topology-aware, lightweight, easy-to-optimise), and by using it as a building block for traditional deep generative architectures, we demonstrate state-of-the-art results on a variety of 3D shape datasets compared to the linear Morphable Model and other graph convolutional operators.",0
"This abstract can serve as an example and starting point. Please improve upon and modify it at your discretion ---------------------------------------------------------------------------------------------------------------  In recent years, deep learning has emerged as a powerful tool for handling complex image and video data analysis tasks such as classification, segmentation, and generation. However, these methods have been largely limited to two-dimensional (2D) data due to their reliance on convolutional neural networks (CNNs), which operate on grids that require a fixed number of channels. As a result, they cannot directly handle volumetric images or 3D shapes, limiting their utility for applications such as computer vision and graphics, robotics, and medical imaging. To address this gap, we propose spiral convolutional neural networks, a novel architecture designed specifically for working with 3D data sets by taking advantage of spatial structure information contained within spirals. Our approach offers substantial improvements over traditional CNN architectures, yielding significantly better performance across several benchmark datasets including ModelNet40, ShapeNet Core V2, Pix3D, and the Stanford 3D Scanning Repository. These results confirm that our proposed method constitutes a significant advancement in deep learning research for 3D shape representation, enabling new possibilities for machine perception of 3D scenes and objects. ---------------------------------------------------------------------------------------------------------------  This work presents spiral convolutional neural networks (SpCNNs)—a novel architecture tailored to processing 3D data—for representing and generating high-quality 3D meshes using learned neural implicit functions. Traditional convolutional neural networks (CNNs) process input grids via translations along discrete grid directions (e.g., rows/columns). In contrast, SpCNNS exploit continuous spiral trajectories on concentric shells surrounding each object center in feature spaces. We prove that SpCNNs can learn low-dimensi",1
"Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of 'free' supervision for further representation learning through performing topdown and layer-wise attention distillation within the network itself. SAD can be easily incorporated in any feedforward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet-18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and runs 10 x faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks. Our code is available at https://github.com/cardwing/Codes-for-Lane-Detection.",0
"Abstract: This paper proposes a novel method for learning lightweight lane detection convolutional neural networks (CNNs) through self attention distillation. In order to achieve accurate and efficient lane detection, we first pretrain our network using only a small amount of data to produce an initial model. Then, we use self attention mechanisms to distill knowledge from a larger pretrained model into our smaller one, without requiring extensive computational resources. Our approach effectively combines both local and global contextual information, allowing us to significantly improve performance while reducing model complexity. Experimental results on several benchmark datasets demonstrate that our proposed method outperforms state-of-the-art methods in terms of accuracy, speed, and efficiency, making it well-suited for real-time applications such as autonomous driving and advanced driver assistance systems. Overall, our work showcases the potential of self attention distillation as a powerful technique for training lightweight deep learning models for computer vision tasks.",1
"With higher-order neighborhood information of graph network, the accuracy of graph representation learning classification can be significantly improved. However, the current higher order graph convolutional network has a large number of parameters and high computational complexity. Therefore, we propose a Hybrid Lower order and Higher order Graph convolutional networks (HLHG) learning model, which uses weight sharing mechanism to reduce the number of network parameters. To reduce computational complexity, we propose a novel fusion pooling layer to combine the neighborhood information of high order and low order. Theoretically, we compare the model complexity of the proposed model with the other state-of-the-art model. Experimentally, we verify the proposed model on the large-scale text network datasets by supervised learning, and on the citation network datasets by semi-supervised learning. The experimental results show that the proposed model achieves highest classification accuracy with a small set of trainable weight parameters.",0
"Incorporating graph convolution into deep neural networks has been shown to improve performance on tasks that operate over non-Euclidean data such as graphs. However, these methods can suffer from computational cost due to large kernels sizes required to accurately capture global features in large graphs. To mitigate this issue, hybrid models have emerged which incorporate both low order and high order interactions. By combining different kernel widths and ranges, they aim at capturing local patterns while still having access to long-range dependencies. This work presents two novel approaches to hybrid graph convolutional networks: one based on fusing the low-degree Chebyshev polynomial basis functions commonly used in GNNs with higher-dimensional spherical harmonic functions; another one by employing a multi-scale design where the lower layers use small neighborhoods while upper layers gradually increase their range until encompassing the entire graph. We provide extensive empirical evidence demonstrating the effectiveness and efficiency of our proposed architectures across multiple benchmark datasets including citation networks, social network analysis, drug discovery, and computer vision. Our results indicate improved accuracy compared to several state-of-the art baselines and reduced computational costs due to smaller model size and faster inference time. Overall, this research contributes a new line of investigation into improving the trade-off between expressivity and scalability of graph CNNs via fusion of different kernel spaces or layer designs, paving the way towards more advanced graph neural networks.",1
"We introduce a novel method to combat label noise when training deep neural networks for classification. We propose a loss function that permits abstention during training thereby allowing the DNN to abstain on confusing samples while continuing to learn and improve classification performance on the non-abstained samples. We show how such a deep abstaining classifier (DAC) can be used for robust learning in the presence of different types of label noise. In the case of structured or systematic label noise -- where noisy training labels or confusing examples are correlated with underlying features of the data-- training with abstention enables representation learning for features that are associated with unreliable labels. In the case of unstructured (arbitrary) label noise, abstention during training enables the DAC to be used as an effective data cleaner by identifying samples that are likely to have label noise. We provide analytical results on the loss function behavior that enable dynamic adaption of abstention rates based on learning progress during training. We demonstrate the utility of the deep abstaining classifier for various image classification tasks under different types of label noise; in the case of arbitrary label noise, we show significant improvements over previously published results on multiple image benchmarks. Source code is available at https://github.com/thulas/dac-label-noise",0
"In recent years, deep learning has become increasingly popular due to its ability to achieve state-of-the-art performance on a wide range of tasks. However, one major challenge that arises in training deep neural networks is label noise, which refers to incorrect or missing labels in the training data. This can significantly affect the accuracy of the model, especially when the amount of noise is high. Therefore, addressing label noise is crucial for obtaining accurate models. In this work, we propose a novel approach called ""abstention"" to combat label noise in deep learning. Our method works by identifying samples with potentially noisy labels during training and abstaining from using those samples to update the model parameters. We evaluate our approach on several benchmark datasets and show that it outperforms existing techniques for handling label noise, resulting in more robust and accurate models. Our findings have important implications for developing reliable machine learning systems in practice.",1
"Graph neural networks denote a group of neural network models introduced for the representation learning tasks on graph data specifically. Graph neural networks have been demonstrated to be effective for capturing network structure information, and the learned representations can achieve the state-of-the-art performance on node and graph classification tasks. Besides the different application scenarios, the architectures of graph neural network models also depend on the studied graph types a lot. Graph data studied in research can be generally categorized into two main types, i.e., small graphs vs. giant networks, which differ from each other a lot in the size, instance number and label annotation. Several different types of graph neural network models have been introduced for learning the representations from such different types of graphs already. In this paper, for these two different types of graph data, we will introduce the graph neural networks introduced in recent years. To be more specific, the graph neural networks introduced in this paper include IsoNN, SDBN, LF&ER, GCN, GAT, DifNN, GNL, GraphSage and seGEN. Among these graph neural network models, IsoNN, SDBN and LF&ER are initially proposed for small graphs and the remaining ones are initially proposed for giant networks instead. The readers are also suggested to refer to these papers for detailed information when reading this tutorial paper.",0
"Graph Neural Networks (GNN) represent one of today’s most active research areas. These neural models aim at processing graph structured data. GNN have been applied successfully on different applications such as node classification problems. One of the major challenges that arise in these tasks is how to balance two objectives: capturing global relationships among nodes versus focusing on local neighborhood features; this can lead to overfitting and underfitting issues if not handled properly. This survey focuses on providing insights into popular architectures of GNN and their variants, discussing their pros and cons, highlighting their applications across various domains, identifying key research gaps and future directions. We hope that by shedding light upon diverse aspects pertaining to GNN research, our work can aid practitioners in selecting appropriate models to suit their needs better, as well as provide guidance for future research endeavors in GNN development.",1
"Deep learning methods are successfully used in applications pertaining to ubiquitous computing, health, and well-being. Specifically, the area of human activity recognition (HAR) is primarily transformed by the convolutional and recurrent neural networks, thanks to their ability to learn semantic representations from raw input. However, to extract generalizable features, massive amounts of well-curated data are required, which is a notoriously challenging task; hindered by privacy issues, and annotation costs. Therefore, unsupervised representation learning is of prime importance to leverage the vast amount of unlabeled data produced by smart devices. In this work, we propose a novel self-supervised technique for feature learning from sensory data that does not require access to any form of semantic labels. We learn a multi-task temporal convolutional network to recognize transformations applied on an input signal. By exploiting these transformations, we demonstrate that simple auxiliary tasks of the binary classification result in a strong supervisory signal for extracting useful features for the downstream task. We extensively evaluate the proposed approach on several publicly available datasets for smartphone-based HAR in unsupervised, semi-supervised, and transfer learning settings. Our method achieves performance levels superior to or comparable with fully-supervised networks, and it performs significantly better than autoencoders. Notably, for the semi-supervised case, the self-supervised features substantially boost the detection rate by attaining a kappa score between 0.7-0.8 with only 10 labeled examples per class. We get similar impressive performance even if the features are transferred from a different data source. While this paper focuses on HAR as the application domain, the proposed technique is general and could be applied to a wide variety of problems in other areas.",0
"In recent years, self-supervised learning (SSL) has emerged as a powerful approach for visual representation learning by leveraging large amounts of unlabeled data using pretext tasks such as image rotation prediction and jigsaw puzzle solving. However, most SSL methods focus on single task scenarios where only one task is learned at a time, which can lead to suboptimal performance due to interference among multiple related tasks that share common features. To address this issue, we propose multi-task self-supervised learning (MTSSL), a framework that allows several related tasks to learn together while mitigating interference. MTSSL consists of three main components: task sampling, model initialization, and online adaptation. By explicitly predicting more than one target, MTSSL learns better representations that can generalize across multiple tasks without needing separate models or annotations for each individual task. Experiments conducted on human activity detection datasets demonstrate that our proposed method outperforms both supervised and traditional SSL baselines by significantly reducing the gap between labeled and unlabeled data performances, establishing new state-of-the-art results under similar computational budgets. This work represents an important step towards realizing SSL’s potential for applications like video surveillance, robotics, and smart homes where efficient use of compute resources is crucial but labeled training data may be limited.",1
"Predicting bioactivity and physical properties of small molecules is a central challenge in drug discovery. Deep learning is becoming the method of choice but studies to date focus on mean accuracy as the main metric. However, to replace costly and mission-critical experiments by models, a high mean accuracy is not enough: Outliers can derail a discovery campaign, thus models need reliably predict when it will fail, even when the training data is biased; experiments are expensive, thus models need to be data-efficient and suggest informative training sets using active learning. We show that uncertainty quantification and active learning can be achieved by Bayesian semi-supervised graph convolutional neural networks. The Bayesian approach estimates uncertainty in a statistically principled way through sampling from the posterior distribution. Semi-supervised learning disentangles representation learning and regression, keeping uncertainty estimates accurate in the low data limit and allowing the model to start active learning from a small initial pool of training data. Our study highlights the promise of Bayesian deep learning for chemistry.",0
"Title: Bayesian Semi-Supervised Learning for Uncertainty-Calibrated Prediction of Molecular Properties and Active Learning  Uncertainties arising from limited data availability are often problematic for machine learning models used in predicting molecular properties. In this work, we present a novel approach based on Bayesian semi-supervised learning (BSSL) that effectively combines labeled and unlabeled datasets for improved prediction accuracy while maintaining calibrated uncertainty estimates. We use variational inference techniques to learn distributions over model parameters given both observed labels as well as their associated uncertainty measures such as error bars, confidence intervals, or entropy. Our framework allows us to flexibly incorporate diverse sources of prior knowledge including physical constraints, chemical intuition, and domain expertise, which significantly enhances robustness to noisy, incomplete, or biased datasets. Through extensive experimental evaluations on challenging benchmarks related to drug discovery and materials science applications, our method consistently outperforms state-of-the-art alternatives by balancing empirical risks against aleatoric/epistemic uncertainties using temperature scaling or ensembling schemes. Additionally, BSSL enables efficient query strategies for adaptive sampling via active learning (AL), reducing human effort required for collecting costly annotations. By enabling automated exploration of high-uncertainty regions guided by AL, researchers can focus more on hypothesis validation and decision making within tight time frames. Overall, our findings demonstrate significant potential benefits of integrating BSSL and AL into modern scientific workflows where predictive performance coupled with meaningful uncertainty quantification is crucial for sound judgment under uncertain conditions.",1
"Deep reinforcement learning has achieved great successes in recent years, but there are still open challenges, such as convergence to locally optimal policies and sample inefficiency. In this paper, we contribute a novel self-supervised auxiliary task, i.e., Terminal Prediction (TP), estimating temporal closeness to terminal states for episodic tasks. The intuition is to help representation learning by letting the agent predict how close it is to a terminal state, while learning its control policy. Although TP could be integrated with multiple algorithms, this paper focuses on Asynchronous Advantage Actor-Critic (A3C) and demonstrating the advantages of A3C-TP. Our extensive evaluation includes: a set of Atari games, the BipedalWalker domain, and a mini version of the recently proposed multi-agent Pommerman game. Our results on Atari games and the BipedalWalker domain suggest that A3C-TP outperforms standard A3C in most of the tested domains and in others it has similar performance. In Pommerman, our proposed method provides significant improvement both in learning efficiency and converging to better policies against different opponents.",0
"In recent years, deep reinforcement learning (DRL) has become increasingly popular due to its ability to solve complex decision making problems under uncertainty. However, training DRL agents can often take a lot of computational resources and time, which limits their scalability and applicability to real world problems. To address these limitations, researchers have proposed using auxiliary tasks to improve both the speed and quality of DRL agent training. One such task that shows promising results is terminal prediction. This paper proposes the use of terminal prediction as an auxiliary task for deep reinforcement learning, where the goal is to predict the termination of episodes based on observed state transitions during training. By doing so, we aim to provide the agent with additional information that helps better optimize its policy decisions while reducing the number of interactions required for successful training. We evaluate our approach through a comprehensive set of experiments, demonstrating improved performance over baseline methods across different environments and evaluation metrics. Our work offers a novel perspective on how auxiliary tasks can shape the future direction of DRL research, enabling more efficient and effective solution designs for complex decision making problems.",1
"This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that our approach and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.",0
"In recent years, self-supervised learning has emerged as a powerful tool for machine learning applications that require large amounts of data but lack extensive labelled datasets. However, many real world tasks still benefit from having some level of human supervision, especially in situations where obtaining fully annotated data may not be feasible. To address these challenges, we propose a novel approach that combines both semi-supervised and self-supervised techniques under the umbrella term “S4L”. Our method leverages the strengths of both paradigms by using limited labeled data to guide unlabeled data augmentation, while also ensuring that learned representations align well with their corresponding labels. We evaluate our proposed framework on several benchmark image classification datasets including MNIST, CIFAR-10, ImageNet and SVHN, achieving state-of-the-art results across all of them. Additionally, our analysis shows that S4L can effectively leverage small amount of labels, demonstrating its effectiveness even with fewer than 20% labels available. Overall, S4L provides an efficient solution for scenarios where only minimal labeled data is available during training.",1
"Clustering multi-view data has been a fundamental research topic in the computer vision community. It has been shown that a better accuracy can be achieved by integrating information of all the views than just using one view individually. However, the existing methods often struggle with the issues of dealing with the large-scale datasets and the poor performance in reconstructing samples. This paper proposes a novel multi-view clustering method by learning a shared generative latent representation that obeys a mixture of Gaussian distributions. The motivation is based on the fact that the multi-view data share a common latent embedding despite the diversity among the views. Specifically, benefited from the success of the deep generative learning, the proposed model not only can extract the nonlinear features from the views, but render a powerful ability in capturing the correlations among all the views. The extensive experimental results, on several datasets with different scales, demonstrate that the proposed method outperforms the state-of-the-art methods under a range of performance criteria.",0
"Title: ""Shared Generative Latent Representation Learning for Multi-view Clustering"" Authors: Yue Wan, Zhan Zheng, Xiaohui Wang, Jianzhong Li",1
"The success of deep learning in medical imaging is mostly achieved at the cost of a large labeled data set. Semi-supervised learning (SSL) provides a promising solution by leveraging the structure of unlabeled data to improve learning from a small set of labeled data. Self-ensembling is a simple approach used in SSL to encourage consensus among ensemble predictions of unknown labels, improving generalization of the model by making it more insensitive to the latent space. Currently, such an ensemble is obtained by randomization such as dropout regularization and random data augmentation. In this work, we hypothesize -- from the generalization perspective -- that self-ensembling can be improved by exploiting the stochasticity of a disentangled latent space. To this end, we present a stacked SSL model that utilizes unsupervised disentangled representation learning as the stochastic embedding for self-ensembling. We evaluate the presented model for multi-label classification using chest X-ray images, demonstrating its improved performance over related SSL models as well as the interpretability of its disentangled representations.",0
"In recent years there has been significant interest in developing semi-supervised learning algorithms that can leverage unlabeled data to improve performance on supervised tasks. One approach that has gained popularity is disentangled representation learning, which involves identifying underlying factors of variation in the data that can be used as a more informative prior than traditional feature engineering techniques such as PCA or autoencoders. However, current state-of-the-art models still struggle to fully capture these latent structures without sacrificing interpretability, generalization performance, scalability, flexibility, stability, and speed to convergence. This paper proposes a novel model called ""disentangler"" that addresses these issues head on. By using randomness during training we introduce new challenges into learning disentangled representations but our results show that this improves performance significantly over previous methods and provides better visualizations as well. Our model builds upon existing ideas from self ensembling deep generative models that utilize latent space stochastically by incorporating a new regularizer based on mutual information maximization to ensure that important variations have high probability density in the latent spaces thus helping them to correspond better to natural groupings in the input data space. We evaluate our method on several benchmark datasets commonly used in representation learning including MNIST, CelebA, SVHN, KMNIST, ImageNet2018 and compare to many other state-of-the-art approaches showing improvements on most measures of evaluation we consider including: accuracy on downstream linear and nonlinear prediction, reconstruction error rate, FID score, stability under hyperparameter search and time to converge. We al",1
"Learning representations that can disentangle explanatory attributes underlying the data improves interpretabilty as well as provides control on data generation. Various learning frameworks such as VAEs, GANs and auto-encoders have been used in the literature to learn such representations. Most often, the latent space is constrained to a partitioned representation or structured by a prior to impose disentangling. In this work, we advance the use of a latent representation based on a product space of Orthogonal Spheres PrOSe. The PrOSe model is motivated by the reasoning that latent-variables related to the physics of image-formation can under certain relaxed assumptions lead to spherical-spaces. Orthogonality between the spheres is motivated via physical independence models. Imposing the orthogonal-sphere constraint is much simpler than other complicated physical models, is fairly general and flexible, and extensible beyond the factors used to motivate its development. Under further relaxed assumptions of equal-sized latent blocks per factor, the constraint can be written down in closed form as an ortho-normality term in the loss function. We show that our approach improves the quality of disentanglement significantly. We find consistent improvement in disentanglement compared to several state-of-the-art approaches, across several benchmarks and metrics.",0
"One common issue with disentangled representation learning methods is that they often suffer from a lack of interpretability and understanding of which factors contribute most strongly to certain outcomes. In order to address these issues, we present a new method called Product of Orthogonal Spheres (POS) parameterization, which allows us to better analyze the relationships between variables and their impact on the outcome. By using a product space instead of traditional latent spaces, POS enables visualization of the relationship between continuous latent dimensions and corresponding scalar attributes such as rotation angles, allowing for intuitive interpretation of disentanglement. Furthermore, we propose two variants: one based on matrix factorization and another based on VAE, both yielding competitive performance compared to state-of-the-art alternatives while providing the advantages mentioned above. Experimental results demonstrate the effectiveness of our approach across different datasets and tasks, including 2D and 3D generation, anomaly detection, unsupervised clustering and semi-supervised classification. Our work paves the way towards more comprehensive research into understanding disentanglement and the properties of learned representations by offering a toolkit to facilitate experimentation and discovery.",1
"Representation learning methods that transform encoded data (e.g., diagnosis and drug codes) into continuous vector spaces (i.e., vector embeddings) are critical for the application of deep learning in healthcare. Initial work in this area explored the use of variants of the word2vec algorithm to learn embeddings for medical concepts from electronic health records or medical claims datasets. We propose learning embeddings for medical concepts by using graph-based representation learning methods on SNOMED-CT, a widely popular knowledge graph in the healthcare domain with numerous operational and research applications. Current work presents an empirical analysis of various embedding methods, including the evaluation of their performance on multiple tasks of biomedical relevance (node classification, link prediction, and patient state prediction). Our results show that concept embeddings derived from the SNOMED-CT knowledge graph significantly outperform state-of-the-art embeddings, showing 5-6x improvement in ``concept similarity"" and 6-20\% improvement in patient diagnosis.",0
"Abstract There exists today vast stores of medical knowledge that can serve as foundational data sources for health care analytics and artificial intelligence applications. While this potential is recognized by many researchers and companies alike, the underlying structures and content in these knowledge bases remain difficult to analyze effectively using traditional methods of natural language processing and machine learning. In this paper we introduce a novel approach to converting structured clinical vocabularies into semantically meaningful vector embeddings that capture the contextual relationships between concepts and their attributes, while preserving both spatial and temporal dimensions. We demonstrate the effectiveness of our technique using the widely adopted terminology standard Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) which contains over two hundred thousand concepts across multiple domains including diseases, procedures, organisms and genetic traits among others. Our method leverages random walk principles from graph theory combined with Poincaré embeddings that map complex topological features onto Euclidean spaces yielding accurate representations at varying scales. Furthermore, we provide empirical evidence on how these learned vectors significantly improve performance for downstream application such as semantic search, clustering, classification and outlier detection tasks. By reducing the barriers to utilize large scale clinical ontologies within modern data science pipelines our work opens up new opportunities to mine the wealth of information contained within them, ultimately leading towards better patient care through improved decision support and precision medicine initiatives.",1
"Click-through rate (CTR) prediction is a critical task in online advertising systems. Most existing methods mainly model the feature-CTR relationship and suffer from the data sparsity issue. In this paper, we propose DeepMCP, which models other types of relationships in order to learn more informative and statistically reliable feature representations, and in consequence to improve the performance of CTR prediction. In particular, DeepMCP contains three parts: a matching subnet, a correlation subnet and a prediction subnet. These subnets model the user-ad, ad-ad and feature-CTR relationship respectively. When these subnets are jointly optimized under the supervision of the target labels, the learned feature representations have both good prediction powers and good representation abilities. Experiments on two large-scale datasets demonstrate that DeepMCP outperforms several state-of-the-art models for CTR prediction.",0
"In recent years there has been a growing interest in modeling click-through rates (CTR) as a key performance metric for online advertising systems. CTR prediction plays an important role in both targeted advertisement placements where ads need to compete over limited display locations against each other and automatic bid generation which relies on accurate expected CTR estimates to rank bids. In this paper we propose using deep learning techniques, especially representation learning methods that learn complex feature representations from raw input data without any manual feature engineering. We experiment with four different architectures including convolutional neural networks (CNN), recurrent neural networks (LSTM, GRU) and self attention mechanism based models (Transformer). Our experiments show that our proposed approach outperforms several traditional baseline predictors such as matrix factorization, gradient boosting machines, random forest etc by at least 2%-8% in terms of mean squared error (mse). Additionally, using human annotators through Amazon Mechanical Turk, we conducted extensive analysis showing clear improvement of ranking quality of predicted listings in two diverse application scenarios: sponsored search and content recommendation.",1
"The Information Bottleneck (IB) method (\cite{tishby2000information}) provides an insightful and principled approach for balancing compression and prediction for representation learning. The IB objective $I(X;Z)-\beta I(Y;Z)$ employs a Lagrange multiplier $\beta$ to tune this trade-off. However, in practice, not only is $\beta$ chosen empirically without theoretical guidance, there is also a lack of theoretical understanding between $\beta$, learnability, the intrinsic nature of the dataset and model capacity. In this paper, we show that if $\beta$ is improperly chosen, learning cannot happen -- the trivial representation $P(Z|X)=P(Z)$ becomes the global minimum of the IB objective. We show how this can be avoided, by identifying a sharp phase transition between the unlearnable and the learnable which arises as $\beta$ is varied. This phase transition defines the concept of IB-Learnability. We prove several sufficient conditions for IB-Learnability, which provides theoretical guidance for choosing a good $\beta$. We further show that IB-learnability is determined by the largest confident, typical, and imbalanced subset of the examples (the conspicuous subset), and discuss its relation with model capacity. We give practical algorithms to estimate the minimum $\beta$ for a given dataset. We also empirically demonstrate our theoretical conditions with analyses of synthetic datasets, MNIST, and CIFAR10.",0
"The learnability of deep neural networks has been studied extensively but there is still debate surrounding how these models generalize to out-of-distribution (OOD) data. In their recent study ""Learnability of Deep Neural Networks"", Arjovsky et al. proposed the Information Bottleneck (IB) theory which provides new insights into the behavior of neural networks and sheds light on OOD generalization. This paper examines the implications of the IB theory and provides evidence that supports its claim that neural network capacity can be directly controlled by adjusting the bottleneck size. The authors provide experimental results using popular benchmark datasets such as MNIST and CIFAR100 which confirm the effectiveness of the method. Additionally, they show that increasing the number of neurons alone may lead to degraded performance unless the corresponding bottleneck size is increased accordingly. These findings have important consequences for understanding the learning process of deep neural networks and suggest promising directions for future research. Overall, this work represents an exciting step forward towards achieving better generalization abilities for deep learning models.",1
"Financial transactions can be considered edges in a heterogeneous graph between entities sending money and entities receiving money. For financial institutions, such a graph is likely large (with millions or billions of edges) while also sparsely connected. It becomes challenging to apply machine learning to such large and sparse graphs. Graph representation learning seeks to embed the nodes of a graph into a Euclidean vector space such that graph topological properties are preserved after the transformation. In this paper, we present a novel application of representation learning to bipartite graphs of credit card transactions in order to learn embeddings of account and merchant entities. Our framework is inspired by popular approaches in graph embeddings and is trained on two internal transaction datasets. This approach yields highly effective embeddings, as quantified by link prediction AUC and F1 score. Further, the resulting entity vectors retain intuitive semantic similarity that is explored through visualizations and other qualitative analyses. Finally, we show how these embeddings can be used as features in downstream machine learning business applications such as fraud detection.",0
"One possible abstract for ""DeepTrax: Embedding Graphs of Financial Transactions"" might read as follows:  In recent years, financial institutions have been collecting vast amounts of transactional data on their customers, leading to significant advances in fraud detection, risk analysis, and customer segmentation. However, traditional approaches to analyzing these datasets often rely heavily on hand engineering features based on domain knowledge, which can limit the ability to discover complex relationships that exist within the data. In this work, we propose a novel method called DeepTrax that leverages graph neural networks (GNNs) to automatically learn embeddings from graphs representing financial transactions. These learned representations capture complex dependencies among different entities involved in the transactions, including users, merchants, locations, time periods, and so forth, providing new insights into customer behavior and potentially improving financial decision making. We evaluate our approach through extensive experiments using real-world datasets from two major banks and demonstrate the superior performance of DeepTrax compared to several baseline methods. Overall, our results provide strong evidence of the effectiveness and potential impact of deep learning techniques on understanding financial transactions at scale.",1
"Risk adjustment has become an increasingly important tool in healthcare. It has been extensively applied to payment adjustment for health plans to reflect the expected cost of providing coverage for members. Risk adjustment models are typically estimated using linear regression, which does not fully exploit the information in claims data. Moreover, the development of such linear regression models requires substantial domain expert knowledge and computational effort for data preprocessing. In this paper, we propose a novel approach for risk adjustment that uses semantic embeddings to represent patient medical histories. Embeddings efficiently represent medical concepts learned from diagnostic, procedure, and prescription codes in patients' medical histories. This approach substantially reduces the need for feature engineering. Our results show that models using embeddings had better performance than a commercial risk adjustment model on the task of prospective risk score prediction.",0
"Medically complex patients can have their health care costs paid for by government programs like Medicare and Medicaid under certain circumstances using ""risk adjustments"" which pay bonuses for taking on additional risk of caring for more complicated cases. These risk scores are computed based off of patient diagnoses contained within medical claim records. We study how to automatically generate better representations of these diagnoses given raw natural language text, so as to potentially improve the accuracy of subsequent downstream machine learning tasks focused on improving risk adjustment payments for medically complex individuals. Our approach relies on deep neural network architectures that generate fixed length vectors (or embeddings) representing concepts mentioned in clinical notes describing patient encounters. This vectorization process results in continuous representation of discrete data points allowing them to be inputted into supervised learning models designed to predict the concept most relevant condition associated with a piece of text in ICD form. In addition we show how these embeddings are also informative when used in semi-supervised settings when only small amounts of annotated training data are available. We then apply this new learned embedding space to both generative (sequence-to-sequence model that maps visit level details into condition reports) and discriminative (logistic regression model that classifies whether a particular condition should be present or absent) machine learning pipelines applied directly on top of raw text inputs. On held out testing data in all scenarios our models achieve significantly higher levels of task performance compared to several strong baseline methods already commonly in use in industry. With respect to the payment risk adjustment domain we provide specific examples demonstrating how these improved predictions may lead to changes in estimated cost that would impact health plan payments at year end audits. Overall this work presents promising applications leveraging recent advances in the field Natural Language Processing and Machine Learning to address difficult challenges facing healthcare systems.",1
"In multi-task reinforcement learning there are two main challenges: at training time, the ability to learn different policies with a single model; at test time, inferring which of those policies applying without an external signal. In the case of continual reinforcement learning a third challenge arises: learning tasks sequentially without forgetting the previous ones. In this paper, we tackle these challenges by proposing DisCoRL, an approach combining state representation learning and policy distillation. We experiment on a sequence of three simulated 2D navigation tasks with a 3 wheel omni-directional robot. Moreover, we tested our approach's robustness by transferring the final policy into a real life setting. The policy can solve all tasks and automatically infer which one to run.",0
"This is a technical paper that presents a new method called ""DisCoRL"" which stands for ""Distilled Continuous Reinforcement Learning."" This approach combines continual reinforcement learning (CRL) and policy distillation into a single framework, allowing agents to learn continuously over time while maintaining their performance on previously learned tasks. The DisCoRL algorithm first trains a separate agent on each task using CRL techniques such as experience replay and dynamic architecture allocation. These individual policies are then used to create a shared set of parameters which represent knowledge acquired from all the trained policies. Finally, a final policy is obtained by distilling this shared representation back down to a fixed size. Experimental results demonstrate that DisCoRL outperforms previous methods across multiple continuous control benchmarks, including Ant and Hopper. Additionally, ablation studies show that both components of DisCoRL - CRL and policy distillation - contribute significantly to the overall improvement. Overall, the authors conclude that DisCoRL provides a promising direction towards achieving high-quality generalization under realistic settings where memory constraints and sample complexity pose significant challenges.",1
"Estimating average causal effect (ACE) is useful whenever we want to know the effect of an intervention on a given outcome. In the absence of a randomized experiment, many methods such as stratification and inverse propensity weighting have been proposed to estimate ACE. However, it is hard to know which method is optimal for a given dataset or which hyperparameters to use for a chosen method. To this end, we provide a framework to characterize the loss of a causal inference method against the true ACE, by framing causal inference as a representation learning problem. We show that many popular methods, including back-door methods can be considered as weighting or representation learning algorithms, and provide general error bounds for their causal estimates. In addition, we consider the case when unobserved variables can confound the causal estimate and extend proposed bounds using principles of robust statistics, considering confounding as contamination under the Huber contamination model. These bounds are also estimable; as an example, we provide empirical bounds for the Inverse Propensity Weighting (IPW) estimator and show how the bounds can be used to optimize the threshold of clipping extreme propensity scores. Our work provides a new way to reason about competing estimators, and opens up the potential of deriving new methods by minimizing the proposed error bounds.",0
"This paper presents a methodology for quantifying error in causal inference when there are confounding variables present. We examine both classical statistical methods as well as machine learning algorithms commonly used to estimate treatment effect when confounding factors are known to exist but are difficult to measure directly. Our approach involves applying sensitivity analysis techniques to simulate different levels of confounding and evaluate how robust the estimates from each method remain under these scenarios. Through simulations, we show that while some approaches may be more resistant than others to certain types of confounding biases, no single method universally outperforms all others across all simulation conditions. Therefore, researchers should consider adopting a portfolio of tools and making explicit tradeoffs based on their specific goals when working in environments subject to unmeasured confounding.",1
"We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events.   Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.",0
"This study proposes a novel approach for learning representations that maximizes mutual information across views. We aim to learn task-agnostic features that capture complementary information from different data modalities such as images, videos, text, and audio. Our method uses multiple view prediction networks, each trained on one modality, and predicts missing information in other modalities using cross-modal attention mechanisms. To measure the performance of our model, we introduce two new evaluation metrics: Modal Completeness (ModCom) and Cross-Modal Reconstruction Error (XMR). We demonstrate experimentally that our method significantly outperforms baseline methods on both quantitative and qualitative benchmarks. Additionally, we showcase applications of our learned representations in zero-shot transfer tasks, domain adaptation, and multimodal generation. Overall, our results indicate that maximizing mutual information across views effectively learns robust and generalizable representations.",1
"Attention operators have been widely applied in various fields, including computer vision, natural language processing, and network embedding learning. Attention operators on graph data enables learnable weights when aggregating information from neighboring nodes. However, graph attention operators (GAOs) consume excessive computational resources, preventing their applications on large graphs. In addition, GAOs belong to the family of soft attention, instead of hard attention, which has been shown to yield better performance. In this work, we propose novel hard graph attention operator (hGAO) and channel-wise graph attention operator (cGAO). hGAO uses the hard attention mechanism by attending to only important nodes. Compared to GAO, hGAO improves performance and saves computational cost by only attending to important nodes. To further reduce the requirements on computational resources, we propose the cGAO that performs attention operations along channels. cGAO avoids the dependency on the adjacency matrix, leading to dramatic reductions in computational resource requirements. Experimental results demonstrate that our proposed deep models with the new operators achieve consistently better performance. Comparison results also indicates that hGAO achieves significantly better performance than GAO on both node and graph embedding tasks. Efficiency comparison shows that our cGAO leads to dramatic savings in computational resources, making them applicable to large graphs.",0
"Abstract In recent years graph representation learning has become increasingly important due to its ability to effectively capture complex relationships between data points that cannot be captured by traditional techniques such as image classification, natural language processing, etc. Recurrent Neural Networks (RNN) have been used to model graph structured data, however RNNs suffer from vanishing gradient problems and struggle with capturing longer term dependencies. Transformer networks on the other hand overcome these issues by using self attention mechanisms but their parallel computation makes them unsuitable for graphs. We propose two novel approaches that bridge this gap: the first method combines recurrence and self attention; while the second uses hard attention which eliminates less significant nodes in the graph. Both methods improve upon state-of-the-art results achieving substantial improvements in performance. Keywords – Graph Representation Learning, Recurrent Neural Networks, Self-Attention Mechanism, Hard Attention.",1
"In linear inverse problems, the goal is to recover a target signal from undersampled, incomplete or noisy linear measurements. Typically, the recovery relies on complex numerical optimization methods; recent approaches perform an unfolding of a numerical algorithm into a neural network form, resulting in a substantial reduction of the computational complexity. In this paper, we consider the recovery of a target signal with the aid of a correlated signal, the so-called side information (SI), and propose a deep unfolding model that incorporates SI. The proposed model is used to learn coupled representations of correlated signals from different modalities, enabling the recovery of multimodal data at a low computational cost. As such, our work introduces the first deep unfolding method with SI, which actually comes from a different modality. We apply our model to reconstruct near-infrared images from undersampled measurements given RGB images as SI. Experimental results demonstrate the superior performance of the proposed framework against single-modal deep learning methods that do not use SI, multimodal deep learning designs, and optimization algorithms.",0
"This paper proposes a novel approach for solving sparse linear inverse problems with side information using deep learning techniques. We introduce a new model called deep coupled representation network (DCRN) that jointly optimizes two representations: one capturing the input data statistics and the other encoding structural prior knowledge about the problem at hand. Our method leverages recent advancements in neural networks to efficiently learn these representations from large datasets while providing robustness to noise and missing data. Experimental results on real-world tasks demonstrate significant improvements over traditional methods, thus showcasing DCRN as a powerful tool for addressing challenging inverse problems under uncertainty. Furthermore, our framework offers interpretability through a quantitative analysis of each learned representation, opening up opportunities for gaining insights into complex systems. Overall, we believe that our work opens new avenues for research in both theoretical aspects of inverse problems and applied domains where informative priors can enhance reconstruction quality.",1
"Continuous symmetries and their breaking play a prominent role in contemporary physics. Effective low-energy field theories around symmetry breaking states explain diverse phenomena such as superconductivity, magnetism, and the mass of nucleons. We show that such field theories can also be a useful tool in machine learning, in particular for loss functions with continuous symmetries that are spontaneously broken by random initializations. In this paper, we illuminate our earlier published work (Bamler & Mandt, 2018) on this topic more from the perspective of theoretical physics. We show that the analogies between superconductivity and symmetry breaking in temporal representation learning are rather deep, allowing us to formulate a gauge theory of `charged' embedding vectors in time series models. We show that making the loss function gauge invariant speeds up convergence in such models.",0
"This paper presents a quantum field theoretical framework for understanding representation learning. We begin by discussing the fundamental principles that underlie our approach, including how we define and characterize representations, as well as the key role played by symmetry breaking and renormalization group flow. Next, we develop a mathematical formalism based on tensor networks which allows us to compute with these concepts efficiently. Finally, we demonstrate the utility of our methods through several examples from machine learning research, including image classification and generative modeling tasks. Our work provides new insights into why certain models are effective at particular tasks, and suggests possible directions for future research.",1
"This paper concerns dictionary learning, i.e., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can provably recover orthogonal dictionaries on a natural nonsmooth, nonconvex $\ell_1$ minimization formulation of the problem, under mild statistical assumptions on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing landscapes of nonsmooth functions, which might be of independent interest for provable training of deep networks with nonsmooth activations (e.g., ReLU), among numerous other applications. Preliminary experiments corroborate our analysis and show that our algorithm works well empirically in recovering orthogonal dictionaries.",0
"In the field of machine learning, subgradient descent has emerged as a popular method for finding sparse representations in high-dimensional spaces. This work explores the use of subgradient descent for solving the problem of dictionary learning, which seeks to find a set of orthogonal vectors that can efficiently represent a given dataset. We present theoretical results demonstrating the convergence of subgradient descent for this task under mild assumptions on the data, and illustrate our approach through simulations and experiments. Our empirical evaluations show that subgradient descent significantly outperforms existing methods for dictionary learning, especially for problems involving large datasets and few samples per atom. Additionally, we demonstrate how subgradient descent can effectively handle nonlinear features and noisy measurements in real applications such as image compression and collaborative filtering. Overall, our contributions extend the scope of subgradient descent beyond sparsity recovery and highlight its potential for dictionary learning and related tasks in signal processing and machine learning.",1
"Self-supervised methods, wherein an agent learns representations solely by observing the results of its actions, become crucial in environments which do not provide a dense reward signal or have labels. In most cases, such methods are used for pretraining or auxiliary tasks for ""downstream"" tasks, such as control, exploration, or imitation learning. However, it is not clear which method's representations best capture meaningful features of the environment, and which are best suited for which types of environments. We present a small-scale study of self-supervised methods on two visual environments: Flappy Bird and Sonic The Hedgehog. In particular, we quantitatively evaluate the representations learned from these tasks in two contexts: a) the extent to which the representations capture true state information of the agent and b) how generalizable these representations are to novel situations, like new levels and textures. Lastly, we evaluate these self-supervised features by visualizing which parts of the environment they focus on. Our results show that the utility of the representations is highly dependent on the visuals and dynamics of the environment.",0
"In recent years, self-supervision has emerged as a popular technique for training machine learning models without requiring labeled data. However, these techniques have been primarily evaluated on static benchmarks where the model only sees predefined inputs at train time. The real world presents a more complex setting, especially when considering interactive environments where the user can interact with objects and receive immediate feedback from their actions. This raises important questions regarding whether the representations learned by self-supervising algorithms generalize effectively across interactions in such settings. In this work, we examine these issues using two well-known interactive tasks (stacking blocks in Minecraft and playing Gym retro video games). We investigate how different self-supervision methods impact performance compared to supervised baselines trained on the same task but using human demonstration data. Our results show that even though self-supervision performs poorly initially, applying it iteratively improves task scores dramatically. Furthermore, our analysis uncovers insights into which types of self-supervision techniques tend to generalize better during interactions, highlighting new research directions. Overall, our findings provide valuable guidance towards designing more effective self-supervisory mechanisms suited for interactive use cases.",1
"This paper introduces a novel deep learning based method, named bridge neural network (BNN) to dig the potential relationship between two given data sources task by task. The proposed approach employs two convolutional neural networks that project the two data sources into a feature space to learn the desired common representation required by the specific task. The training objective with artificial negative samples is introduced with the ability of mini-batch training and it's asymptotically equivalent to maximizing the total correlation of the two data sources, which is verified by the theoretical analysis. The experiments on the tasks, including pair matching, canonical correlation analysis, transfer learning, and reconstruction demonstrate the state-of-the-art performance of BNN, which may provide new insights into the aspect of common representation learning.",0
"This paper proposes a new approach to deep learning called Task-Driven Common Representation Learning (TDCRL). TDCRL uses a bridge neural network to learn common representations across multiple tasks, enabling improved generalization performance compared to traditional methods that learn task-specific representations. Experimental results on several benchmark datasets demonstrate the effectiveness of our proposed method in improving model accuracy while reducing computational cost through shared representation learning. Overall, TDCRL provides a promising framework for future research in multi-task learning and transfer learning, which can have significant impacts on real-world applications such as natural language processing, computer vision, and speech recognition.",1
"We propose a new perspective on representation learning in reinforcement learning based on geometric properties of the space of value functions. We leverage this perspective to provide formal evidence regarding the usefulness of value functions as auxiliary tasks. Our formulation considers adapting the representation to minimize the (linear) approximation of the value function of all stationary policies for a given environment. We show that this optimization reduces to making accurate predictions regarding a special class of value functions which we call adversarial value functions (AVFs). We demonstrate that using value functions as auxiliary tasks corresponds to an expected-error relaxation of our formulation, with AVFs a natural candidate, and identify a close relationship with proto-value functions (Mahadevan, 2005). We highlight characteristics of AVFs and their usefulness as auxiliary tasks in a series of experiments on the four-room domain.",0
"This paper presents a new geometric approach to understanding optimal representations for reinforcement learning (RL). We propose that geometric perspective can provide insights into how different representation choices affect the efficiency and effectiveness of RL algorithms. Our methodology involves analyzing the geometry of value functions, state-action spaces, and policy landscapes, and relating these geometric properties to the performance of RL algorithms. We show that by choosing appropriate representations based on their geometric properties, we can significantly improve both sample complexity and overall performance. We demonstrate our framework through experiments on multiple benchmark domains and illustrate how our methods provide better results than existing approaches. Overall, our work provides a novel viewpoint on the problem of finding optimal representations for RL, which has important implications for advancing the field of artificial intelligence.",1
"With the fast development of various positioning techniques such as Global Position System (GPS), mobile devices and remote sensing, spatio-temporal data has become increasingly available nowadays. Mining valuable knowledge from spatio-temporal data is critically important to many real world applications including human mobility understanding, smart transportation, urban planning, public safety, health care and environmental management. As the number, volume and resolution of spatio-temporal datasets increase rapidly, traditional data mining methods, especially statistics based methods for dealing with such data are becoming overwhelmed. Recently, with the advances of deep learning techniques, deep leaning models such as convolutional neural network (CNN) and recurrent neural network (RNN) have enjoyed considerable success in various machine learning tasks due to their powerful hierarchical feature learning ability in both spatial and temporal domains, and have been widely applied in various spatio-temporal data mining (STDM) tasks such as predictive learning, representation learning, anomaly detection and classification. In this paper, we provide a comprehensive survey on recent progress in applying deep learning techniques for STDM. We first categorize the types of spatio-temporal data and briefly introduce the popular deep learning models that are used in STDM. Then a framework is introduced to show a general pipeline of the utilization of deep learning models for STDM. Next we classify existing literatures based on the types of ST data, the data mining tasks, and the deep learning models, followed by the applications of deep learning for STDM in different domains including transportation, climate science, human mobility, location based social network, crime analysis, and neuroscience. Finally, we conclude the limitations of current research and point out future research directions.",0
"Deep learning has revolutionized many application domains by providing state-of-the-art performance in fields such as image recognition, natural language processing, speech recognition, robotics, bioinformatics and computer vision. One area that remains relatively unexplored despite having vast amounts of data available is spatial temporal data mining (SDM). This survey article provides an overview on how deep learning can be used for SDM problems. We first review existing methods from the literature that use traditional machine learning techniques to solve SDM problems. Next we discuss recent advances using neural networks, particularly convolutional and recurrent networks, which have shown promising results for some tasks involving spatio-temporal data. Finally, we outline future research directions and challenges facing practitioners hoping to deploy these models in practice.",1
"Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. Against end-to-end learning, state representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning with better sample efficiency, and is robust to hyper-parameters change.",0
"Title: ""State Representation Learning in Goal Based Robotics""  This research investigates the potential benefits of decoupling feature extraction from policy learning in goal-based robotics through the use of state representation learning. We present two approaches for state representation learning: learned state representations that map raw sensor data into high-level features used by the planner, and handcrafted state representations designed specifically for planning purposes. Our experiments demonstrate that both types of state representation learning can significantly improve plan quality and efficiency compared to using raw sensor input directly in planning. Moreover, our results suggest that the choice between learned and handcrafted state representations depends on factors such as the complexity of the task domain, the size and dimensionality of the sensor input, and the availability of expert knowledge. Overall, these findings support the viability of state representation learning as a means for enhancing the performance and robustness of goal-based robotic systems.",1
"We consider the problem of imitation learning from expert demonstrations in partially observable Markov decision processes (POMDPs). Belief representations, which characterize the distribution over the latent states in a POMDP, have been modeled using recurrent neural networks and probabilistic latent variable models, and shown to be effective for reinforcement learning in POMDPs. In this work, we investigate the belief representation learning problem for generative adversarial imitation learning in POMDPs. Instead of training the belief module and the policy separately as suggested in prior work, we learn the belief module jointly with the policy, using a task-aware imitation loss to ensure that the representation is more aligned with the policy's objective. To improve robustness of representation, we introduce several informative belief regularization techniques, including multi-step prediction of dynamics and action-sequences. Evaluated on various partially observable continuous-control locomotion tasks, our belief-module imitation learning approach (BMIL) substantially outperforms several baselines, including the original GAIL algorithm and the task-agnostic belief learning algorithm. Extensive ablation analysis indicates the effectiveness of task-aware belief learning and belief regularization.",0
"In recent years, there has been increasing interest in using imitation learning to train agents in partially observable Markov decision processes (POMDPs). However, most existing approaches rely on handcrafted features or state representations that may not capture all relevant aspects of the problem domain. To address this issue, we propose using belief states as the representation for both the demonstrator and the learner agent. This allows us to learn a mapping from raw sensor observations to a compact set of belief states that summarize the history of observations and actions taken by the demonstrator. We show that our approach outperforms prior methods on several benchmark tasks, including those used in previous works that relied on handcrafted features. Our results highlight the importance of incorporating richer representations into imitation learning algorithms and suggest promising directions for future research at the intersection of reinforcement learning and robotics.",1
"End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.",0
"In a study published in [year], we investigate state aliasing in structured prediction tasks using recurrent neural networks (RNN). We examine how common regularization techniques can affect model behavior during training and impact model performance on test sets. Our results suggest that regularization techniques like dropout and weight decay can lead to overfitting on training data, while underfitting on unseen test data. We also find evidence of state aliases emerging during training, where different states have similar loss functions but produce distinct predictions. These findings indicate potential limitations in current practices for designing and evaluating RNN models for structured prediction tasks. Furthermore, our work highlights the need for alternative approaches to prevent state aliasing and improve generalization performance. By shedding light on these issues, our research contributes to advancing understanding of challenges facing RNN modeling for complex problems. Ultimately, our insights could guide future efforts towards developing more robust and effective algorithms for natural language processing applications involving sequential data.",1
"We study the problem of learning representations with controllable connectivity properties. This is beneficial in situations when the imposed structure can be leveraged upstream. In particular, we control the connectivity of an autoencoder's latent space via a novel type of loss, operating on information from persistent homology. Under mild conditions, this loss is differentiable and we present a theoretical analysis of the properties induced by the loss. We choose one-class learning as our upstream task and demonstrate that the imposed structure enables informed parameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on computer vision data, these one-class models exhibit competitive performance and, in a low sample size regime, outperform other methods by a large margin. Notably, our results indicate that a single autoencoder, trained on auxiliary (unlabeled) data, yields a mapping into latent space that can be reused across datasets for one-class learning.",0
"In recent years, representation learning has emerged as a powerful technique for solving complex computational problems across a wide range of domains. However, many existing methods suffer from scalability issues due to their reliance on high-dimensional feature spaces, which can become prohibitively large even for modestly-sized datasets. This paper proposes a novel approach based on persistent homology, a mathematical tool that captures global topological features of data. Our method enables efficient and interpretable representations that capture important patterns while minimizing connectivity complexity. We demonstrate the effectiveness of our approach through several experiments, including classification tasks on image and text datasets, showing significant improvements over state-of-the-art baselines. Overall, we believe that our work represents a step towards more robust and scalable machine learning systems that enable better decision making.",1
"Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.",0
"This paper evaluates the effectiveness of transfer learning using the Task Adaptation via Parameter Estimation (TAPE) method on protein structure prediction tasks. We compare the performance of models trained on multiple domains using transfer learning against models that were specifically tuned for each domain individually. Our results show that transfer learning can significantly improve model accuracy across all domains and outperform individual fine-tuning in most cases. In addition, we analyze the impact of different training sets on transferability and find that larger training sets lead to better generalization ability. Finally, we provide insights into how different architectures behave under transfer learning scenarios and recommend future directions for research in this area. Overall, our work demonstrates the potential benefits of utilizing transfer learning methods for improving protein structure predictions, which has important implications for biochemistry and drug discovery applications.",1
"Encouraged by the success of deep learning in a variety of domains, we investigate the suitability and effectiveness of Recurrent Neural Networks (RNNs) in a domain where deep learning has not yet been used; namely detecting confusion from eye-tracking data. Through experiments with a dataset of user interactions with ValueChart (an interactive visualization tool), we found that RNNs learn a feature representation from the raw data that allows for a more powerful classifier than previous methods that use engineered features. This is evidenced by the stronger performance of the RNN (0.74/0.71 sensitivity/specificity), as compared to a Random Forest classifier (0.51/0.70 sensitivity/specificity), when both are trained on an un-augmented dataset. However, using engineered features allows for simple data augmentation methods to be used. These same methods are not as effective at augmentation for the feature representation learned from the raw data, likely due to an inability to match the temporal dynamics of the data.",0
"This study investigates the potential of using recurrent neural networks (RNN) to predict confusion based on eye-tracking data. We collected gaze tracking information while participants performed a reading comprehension task and used that data as input to our RNN model. Our results show that the RNN was able to accurately predict instances of participant confusion with high accuracy, demonstrating the viability of this approach for measuring cognitive states during natural language processing tasks. These findings have implications for the development of more effective human-computer interfaces, as well as for understanding how individuals process information and make decisions.",1
"We present a technique to improve the transferability of deep representations learned on small labeled datasets by introducing self-supervised tasks as auxiliary loss functions. While recent approaches for self-supervised learning have shown the benefits of training on large unlabeled datasets, we find improvements in generalization even on small datasets and when combined with strong supervision. Learning representations with self-supervised losses reduces the relative error rate of a state-of-the-art meta-learner by 5-25% on several few-shot learning benchmarks, as well as off-the-shelf deep networks on standard classification tasks when training from scratch. We find the benefits of self-supervision increase with the difficulty of the task. Our approach utilizes the images within the dataset to construct self-supervised losses and hence is an effective way of learning transferable representations without relying on any external training data.",0
"Recent research on few-shot learning has focused on improving generalization performance by enhancing supervised fine-tuning approaches through various techniques such as dynamic architecture search (DARTS), progressive networks, and meta learners that adapt during training. In contrast, we propose self-supervised pre-training for the base network to achieve better data efficiency without changing the nature of the existing methods significantly. We leverage the unlabeled dataset within each task to facilitate knowledge transfer from related tasks at little additional cost. This effectively increases the amount of labeled data used in fine-tuning while keeping computational overhead low. Our method demonstrates improvements over prior work both qualitatively and quantitatively across multiple benchmark datasets. Specifically, we report new state-of-the-art results on four popular datasets: Omniglot, miniImageNet, tieredImageNet, and CUB200. These significant advancements come from taking advantage of inherent structure present within the unlabeled task sets rather than relying solely on external guidance. Overall, our approach provides insight into how self-supervision can improve few-shot learning models beyond previous reliance on purely supervised methods alone. The field of machine learning has seen great success in developing algorithms that can solve problems based on very few examples. However, one persistent challenge remains - these models often struggle to generalize well, leading to poorer performance on real world applications. Many recent attempts have been made to mitigate this issue through complex model architectures and meta learning strategies that allow models to adjust their parameters based on feedback from the environment. While these solutions have shown promise, they tend to require large amounts of computation time to train, limiting their potential impact on practical uses cases where time may be limited. In this pape	...",1
"Learning latent representations of nodes in graphs is an important and ubiquitous task with widespread applications such as link prediction, node classification, and graph visualization. Previous methods on graph representation learning mainly focus on static graphs, however, many real-world graphs are dynamic and evolve over time. In this paper, we present Dynamic Self-Attention Network (DySAT), a novel neural architecture that operates on dynamic graphs and learns node representations that capture both structural properties and temporal evolutionary patterns. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. We conduct link prediction experiments on two classes of graphs: communication networks and bipartite rating networks. Our experimental results show that DySAT has a significant performance gain over several different state-of-the-art graph embedding baselines.",0
"This paper presents a new approach for dynamic graph representation learning using self-attention networks. The proposed method leverages interleaved local and global attention mechanisms to learn node representations that capture both static and temporal dependencies among nodes in a dynamic graph. Experimental results demonstrate the effectiveness of our approach on three diverse benchmark datasets, achieving state-of-the-art performance across a range of evaluation metrics. Additionally, we provide extensive analysis to validate our design choices and illustrate the interpretability of our learned models. Our work contributes to the growing body of research focused on developing machine learning techniques tailored to handle complex and evolving data structures like graphs. We believe that our solution has significant potential applications in areas such as recommendation systems, fraud detection, and epidemiology, where modeling evolving relationships among entities is crucial. Overall, we aim to inspire future research by introducing a powerful toolkit that enables efficient and accurate modeling of dynamic graphs using self-attention networks.",1
"Graphs are a natural abstraction for many problems where nodes represent entities and edges represent a relationship across entities. An important area of research that has emerged over the last decade is the use of graphs as a vehicle for non-linear dimensionality reduction in a manner akin to previous efforts based on manifold learning with uses for downstream database processing, machine learning and visualization. In this systematic yet comprehensive experimental survey, we benchmark several popular network representation learning methods operating on two key tasks: link prediction and node classification. We examine the performance of 12 unsupervised embedding methods on 15 datasets. To the best of our knowledge, the scale of our study -- both in terms of the number of methods and number of datasets -- is the largest to date.   Our results reveal several key insights about work-to-date in this space. First, we find that certain baseline methods (task-specific heuristics, as well as classic manifold methods) that have often been dismissed or are not considered by previous efforts can compete on certain types of datasets if they are tuned appropriately. Second, we find that recent methods based on matrix factorization offer a small but relatively consistent advantage over alternative methods (e.g., random-walk based methods) from a qualitative standpoint. Specifically, we find that MNMF, a community preserving embedding method, is the most competitive method for the link prediction task. While NetMF is the most competitive baseline for node classification. Third, no single method completely outperforms other embedding methods on both node classification and link prediction tasks. We also present several drill-down analysis that reveals settings under which certain algorithms perform well (e.g., the role of neighborhood context on performance) -- guiding the end-user.",0
"In this paper we address fundamental questions surrounding network representation learning (NRL), focusing on two themes: consolidating existing foundational results through rigorous analysis, as well as exploring new research directions that build upon these advances. We first review NRL methodology and applications at a high level, aiming to provide context for our work and ground our treatment within related literature on graph neural networks (GNNs) and deep learning (DL). Next, we investigate multiple forms of model ensembles and their impact on performance across different datasets. From there, we evaluate whether data augmentations such as rotations improve generalization, concluding that they do not always lead to robustness gains. Finally, we examine novel architectures based on dynamic edge feature propagation, showing promising results particularly with small training budgets. Throughout our study, we highlight connections between disparate areas and encourage readers to consider alternative formulations and interpretations of familiar problems. Our main goal is to inspire further progress towards deployable models with realworld utility by motivating exciting future research opportunities in NRL. ---",1
"Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for visual recognition problems. Nevertheless, the convolutional filters in these networks are local operations while ignoring the large-range dependency. Such drawback becomes even worse particularly for video recognition, since video is an information-intensive media with complex temporal variations. In this paper, we present a novel framework to boost the spatio-temporal representation learning by Local and Global Diffusion (LGD). Specifically, we construct a novel neural network architecture that learns the local and global representations in parallel. The architecture is composed of LGD blocks, where each block updates local and global features by modeling the diffusions between these two representations. Diffusions effectively interact two aspects of information, i.e., localized and holistic, for more powerful way of representation learning. Furthermore, a kernelized classifier is introduced to combine the representations from two aspects for video recognition. Our LGD networks achieve clear improvements on the large-scale Kinetics-400 and Kinetics-600 video classification datasets against the best competitors by 3.5% and 0.7%. We further examine the generalization of both the global and local representations produced by our pre-trained LGD networks on four different benchmarks for video action recognition and spatio-temporal action detection tasks. Superior performances over several state-of-the-art techniques on these benchmarks are reported. Code is available at: https://github.com/ZhaofanQiu/local-and-global-diffusion-networks.",0
"This research presents a new method for learning spatio-temporal representations that combines local and global diffusion processes. Traditional methods of representation learning rely on either local or global processing alone, but our approach integrates both perspectives to capture more complex patterns and relationships within the data. We demonstrate the effectiveness of our method through experiments on several real-world datasets across different domains, including video surveillance and medical imaging. Our results show significant improvements over baseline models and other state-of-the-art approaches in terms of accuracy and efficiency. Additionally, we provide a detailed analysis of the learned representations, highlighting their importance in capturing meaningful features and improving overall performance. Overall, this work contributes valuable insights into the field of representation learning and has important implications for applications such as anomaly detection, action recognition, and decision support systems.",1
"We focus on the problem of teaching a robot to solve tasks presented sequentially, i.e., in a continual learning scenario. The robot should be able to solve all tasks it has encountered, without forgetting past tasks. We provide preliminary work on applying Reinforcement Learning to such setting, on 2D navigation tasks for a 3 wheel omni-directional robot. Our approach takes advantage of state representation learning and policy distillation. Policies are trained using learned features as input, rather than raw observations, allowing better sample efficiency. Policy distillation is used to combine multiple policies into a single one that solves all encountered tasks.",0
"This study investigates how continual reinforcement learning can be applied in real-world settings using policy distillation and sim2real transfer techniques. We demonstrate how these methods can enable agents to adapt their behavior over time in dynamic environments, improving performance on multiple tasks without forgetting previously learned skills. Our results show that both approaches lead to significant improvements compared to baseline models, with policy distillation offering slightly better overall performance across all metrics. These findings have important implications for developing intelligent systems capable of continuous learning and adaptation in complex situations.",1
"The automatic and efficient discovery of skills, without supervision, for long-living autonomous agents, remains a challenge of Artificial Intelligence. Intrinsically Motivated Goal Exploration Processes give learning agents a human-inspired mechanism to sequentially select goals to achieve. This approach gives a new perspective on the lifelong learning problem, with promising results on both simulated and real-world experiments. Until recently, those algorithms were restricted to domains with experimenter-knowledge, since the Goal Space used by the agents was built on engineered feature extractors. The recent advances of deep representation learning, enables new ways of designing those feature extractors, using directly the agent experience. Recent work has shown the potential of those methods on simple yet challenging simulated domains. In this paper, we present recent results showing the applicability of those principles on a real-world robotic setup, where a 6-joint robotic arm learns to manipulate a ball inside an arena, by choosing goals in a space learned from its past experience.",0
"This paper presents a method for autonomous goal exploration using learned goal spaces for visuomotor skill acquisition in robots. The approach utilizes deep reinforcement learning algorithms to learn goal representations that allow robots to efficiently explore their environment and discover new tasks they can perform. By integrating these learned goal representations into traditional policy optimization techniques, we demonstrate that robots can effectively identify novel behaviors that increase task performance without explicit guidance from human experts. Our experiments on both simulated and real robotic systems show that our proposed method leads to significant improvements over current state-of-the-art methods in terms of efficiency and generalization capabilities. Overall, this work represents an important step towards creating intelligent agents capable of acquiring complex skills through self-directed exploration of their environments.",1
"To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",0
"Reinforcement learning has emerged as one of the most successful approaches in artificial intelligence. One significant challenge facing reinforcement learning (RL) agents is their lack of natural language understanding capabilities that would allow them to interact effectively in human environments. This survey article presents RL models that have been developed to address these challenges by taking advantage of natural language processing techniques. These RL models aim to develop intelligent agents capable of acting in complex real-world settings where they can interact naturally using language. To achieve this objective, we first provide an introduction to deep RL informed by natural language. We then present several recent methods that integrate natural language into RL models based on policy gradient and Q-learning. Furthermore, we investigate two specific problems related to training and evaluation metrics where natural language plays a crucial role. Finally, we conclude by discussing open research directions and future perspectives to guide the development of new integrated frameworks. Our review demonstrates how natural language provides a powerful tool for enriching RL algorithms, leading to more advanced intelligent agents and opening up novel applications across domains. Keywords: Artificial Intelligence, Natural Language Processing, Deep Reinforcement Learning, Policy Gradient Methods, Q-Learning Models, Training Metrics, Evaluation Measures, Integrated Frameworks (hide spoiler)]]>",1
"Network representation learning, as an approach to learn low dimensional representations of vertices, has attracted considerable research attention recently. It has been proven extremely useful in many machine learning tasks over large graph. Most existing methods focus on learning the structural representations of vertices in a static network, but cannot guarantee an accurate and efficient embedding in a dynamic network scenario. To address this issue, we present an efficient incremental skip-gram algorithm with negative sampling for dynamic network embedding, and provide a set of theoretical analyses to characterize the performance guarantee. Specifically, we first partition a dynamic network into the updated, including addition/deletion of links and vertices, and the retained networks over time. Then we factorize the objective function of network embedding into the added, vanished and retained parts of the network. Next we provide a new stochastic gradient-based method, guided by the partitions of the network, to update the nodes and the parameter vectors. The proposed algorithm is proven to yield an objective function value with a bounded difference to that of the original objective function. Experimental results show that our proposal can significantly reduce the training time while preserving the comparable performance. We also demonstrate the correctness of the theoretical analysis and the practical usefulness of the dynamic network embedding. We perform extensive experiments on multiple real-world large network datasets over multi-label classification and link prediction tasks to evaluate the effectiveness and efficiency of the proposed framework, and up to 22 times speedup has been achieved.",0
"""In recent years, network embedding has emerged as a powerful technique for representing complex networks in a continuous vector space where nodes can be easily manipulated and analyzed. One popular approach to network embedding is based on the skip-gram model from natural language processing (NLP), which learns node representations by predicting their contexts within a given window size. Despite its successes, there are some limitations to existing skip-gram methods that hinder their effectiveness in capturing dynamic changes in real-world networks. This research proposes a new method called incremental skip-gram with negative sampling (ISGNS) that addresses these limitations and provides more accurate and robust embeddings for evolving networks. Specifically, ISGNS incorporates three key components: a sliding window mechanism for handling varying sizes of neighborhoods over time; an adaptive negative sampling framework that balances diversity and relevance for better generalization; and a regularized objective function designed to prevent overfitting and promote clustering coherence. Experimental results on several benchmark datasets demonstrate significant improvements achieved by our method over state-of-the-art alternatives in terms of both quantitative measures and visual inspections.""",1
"We present a new method to learn video representations from unlabeled data. Given large-scale unlabeled video data, the objective is to benefit from such data by learning a generic and transferable representation space that can be directly used for a new task such as zero/few-shot learning. We formulate our unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are also shared across different modalities via distillation. Further, we also introduce the concept of finding a better loss function to train such multi-task multi-modal representation space using an evolutionary algorithm; our method automatically searches over different combinations of loss functions capturing multiple (self-supervised) tasks and modalities. Our formulation allows for the distillation of audio, optical flow and temporal information into a single, RGB-based convolutional neural network. We also compare the effects of using additional unlabeled video data and evaluate our representation learning on standard public video datasets.",0
"Deep unsupervised learning has recently been applied to many tasks that previously required large amounts of labeled data, including visual representation learning on video data. In this work, we extend previous approaches by developing a novel self-supervised framework designed specifically for video understanding problems. Our approach is centered around constructing artificial pseudo-tasks which capture unique aspects of videos that regularize the learning process towards better representations. We propose two specific types of losses: an autoencoder reconstruction loss and a cycle-consistency loss. We demonstrate how these can be combined effectively using the same model architecture, while preserving desirable properties such as temporal coherence. Using several established benchmarks across both action recognition and spatiotemporal localization tasks, our method outperforms all prior works under most evaluation metrics. Finally, we show that representations learned via our proposed algorithm lead to significant gains compared to those trained with alternative self-supervision objectives even when fine-tuned on fully supervised datasets. These results underscore the utility of designing appropriate surrogate losses tailored to each problem domain, pushing the limits of deep unsupervised learning on complex vision problems involving video sequences.",1
"Recent successes in visual recognition can be primarily attributed to feature representation, learning algorithms, and the ever-increasing size of labeled training data. Extensive research has been devoted to the first two, but much less attention has been paid to the third. Due to the high cost of manual labeling, the size of recent efforts such as ImageNet is still relatively small in respect to daily applications. In this work, we mainly focus on how to automatically generate identifying image data for a given visual concept on a vast scale. With the generated image data, we can train a robust recognition model for the given concept. We evaluate the proposed webly supervised approach on the benchmark Pascal VOC 2007 dataset and the results demonstrates the superiority of our proposed approach in image data collection.",0
"This paper presents a novel approach for extracting visual knowledge from large amounts of image data available on the internet. With advancements in technology and increased accessibility to high quality cameras, images have become a primary means of communication on online platforms such as social media and news websites. However, most approaches for analyzing these images rely on manual annotation techniques which can be time consuming and expensive. Our proposed method uses deep learning algorithms trained on datasets containing billions of images scraped from the web to automatically identify patterns, objects, and trends within the image data. We evaluate our method using several metrics including accuracy and recall, showing that our system outperforms other state-of-the-art methods. In addition, we demonstrate how our approach can be used in real world applications such as market analysis and content discovery. Overall, our work shows promise in revolutionizing the way we process visual data by making it more efficient and accessible than ever before.",1
"Capsule Networks attempt to represent patterns in images in a way that preserves hierarchical spatial relationships. Additionally, research has demonstrated that these techniques may be robust against adversarial perturbations. We present an improvement to training capsule networks with added robustness via non-parametric kernel methods. The representations learned through the capsule network are used to construct covariance kernels for Gaussian processes (GPs). We demonstrate that this approach achieves comparable prediction performance to Capsule Networks while improving robustness to adversarial perturbations and providing a meaningful measure of uncertainty that may aid in the detection of adversarial inputs.",0
"Abstract: This work presents the concept of kernelized capsule networks, which combines ideas from convolutional neural networks (CNNs) and dynamic routing algorithms to create more expressive representations that better capture spatial relationships between features. By using kernels instead of fixed length vectors as inputs to the routing process, we can encode prior knowledge into the network and allow for more efficient computation during training. Our experiments show improved performance on benchmark datasets compared to standard CNNs and capsule networks, demonstrating the effectiveness of our approach. Additionally, we provide ablation studies that highlight the importance of each component of the model and discuss potential future directions for research in this area. Overall, this paper represents a significant contribution to the field of computer vision and machine learning.",1
"The Arcade Learning Environment (ALE) is a popular platform for evaluating reinforcement learning agents. Much of the appeal comes from the fact that Atari games demonstrate aspects of competency we expect from an intelligent agent and are not biased toward any particular solution approach. The challenge of the ALE includes (1) the representation learning problem of extracting pertinent information from raw pixels, and (2) the behavioural learning problem of leveraging complex, delayed associations between actions and rewards. Often, the research questions we are interested in pertain more to the latter, but the representation learning problem adds significant computational expense. We introduce MinAtar, short for miniature Atari, a new set of environments that capture the general mechanics of specific Atari games while simplifying the representational complexity to focus more on the behavioural challenges. MinAtar consists of analogues of five Atari games: Seaquest, Breakout, Asterix, Freeway and Space Invaders. Each MinAtar environment provides the agent with a 10x10xn binary state representation. Each game plays out on a 10x10 grid with n channels corresponding to game-specific objects, such as ball, paddle and brick in the game Breakout. To investigate the behavioural challenges posed by MinAtar, we evaluated a smaller version of the DQN architecture as well as online actor-critic with eligibility traces. With the representation learning problem simplified, we can perform experiments with significantly less computational expense. In our experiments, we use the saved compute time to perform step-size parameter sweeps and more runs than is typical for the ALE. Experiments like this improve reproducibility, and allow us to draw more confident conclusions. We hope that MinAtar can allow researchers to thoroughly investigate behavioural challenges similar to those inherent in the ALE.",0
"The goal of reinforcement learning (RL) research has been to create algorithms that can learn by trial and error alone, without relying on explicit programming or human expertise. However, designing RL experiments remains challenging due to several difficulties such as sparse rewards, stochasticity, nonlinearity, partial observability, and delayed gratification. These challenges make it difficult for researchers to test their ideas thoroughly and reproduce experimental results, which limits scientific progress. To address these issues, we propose MinAtar, a minimalist but complete Atari-inspired test bed designed specifically for creating thorough and reproducible RL experiments. We show how MinAtar addresses each challenge faced by traditional benchmarks and demonstrate the benefits of using MinAtar through three representative applications. Our first application shows how MinAtar helps expose subtle failures in current state-of-the-art offline RL methods. The second application highlights the importance of careful experimentation, where two different random seeds lead to opposite conclusions on whether a method works well. Finally, our third application studies how exploration scales across different environments, demonstrating the usefulness of MinAtar in guiding algorithmic choices at scale. Together, our contributions enable more accurate and efficient investigation into deep RL problems while fostering collaboration between research groups and advancing AI science overall. By offering both simplicity and expressiveness, reproducibility and control over complexity, scalability and tractability, we believe that MinAtar will serve as a valuable tool for years to come.",1
"Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the latent space as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.",0
"Incorporate keywords into your abstract that would make sense for an ML conference like NeurIPS - e.g., deep learning, reinforcement learning, generative models, inverse problems, variational inference, etc.. Also try using action oriented language. Keywords used on submission site are: Deep Learning, Machine Learning, Reinforcement Learning",1
"We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \emph{flexibly fair}, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---enables the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.",0
"Abstract: This paper presents a new approach to representation learning that achieves flexibility while ensuring fairness through disentanglement. We demonstrate how our method can learn representations that effectively capture complex relationships within data without encoding unwanted biases present in the input features. Our model uses adversarial training to encourage feature separation and minimize confounding across groups defined by sensitive attributes such as race or gender. Through extensive experiments on several benchmark datasets, we show that our approach consistently produces more equitable and explainable results compared to state-of-the-art methods in natural language processing and computer vision tasks, such as sentiment analysis and image classification. Overall, our work highlights the importance of considering ethical considerations during machine learning model development and evaluation.",1
"Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",0
"Self-supervised visual representation learning (SSL) has emerged as a promising approach for enabling computer vision models to learn from large amounts of unlabeled data without direct supervision. In recent years, SSL algorithms have achieved state-of-the-art performance on several benchmark datasets across a wide range of tasks, including image classification, object detection, and segmentation. However, despite their successes, little is known about how SSL methods scale to larger datasets and how they perform compared to fully supervised approaches. This paper addresses these two research questions by exploring the effectiveness and scalability of state-of-the-art SSL algorithms using standard benchmarks and custom datasets. Our results show that SSL methods can compete with fully supervised baselines while requiring significantly less labeled training data, highlighting the potential advantages of self-supervised pretraining for real-world applications. Furthermore, we provide insights into factors affecting the scalability of SSL methods, such as dataset size, label noise, and model capacity. Overall, our findings contribute important new understanding of the strengths and limitations of self-supervised visual representation learning, providing valuable guidance for future research in the field.",1
"Graph data widely exist in many high-impact applications. Inspired by the success of deep learning in grid-structured data, graph neural network models have been proposed to learn powerful node-level or graph-level representation. However, most of the existing graph neural networks suffer from the following limitations: (1) there is limited analysis regarding the graph convolution properties, such as seed-oriented, degree-aware and order-free; (2) the node's degree-specific graph structure is not explicitly expressed in graph convolution for distinguishing structure-aware node neighborhoods; (3) the theoretical explanation regarding the graph-level pooling schemes is unclear.   To address these problems, we propose a generic degree-specific graph neural network named DEMO-Net motivated by Weisfeiler-Lehman graph isomorphism test that recursively identifies 1-hop neighborhood structures. In order to explicitly capture the graph topology integrated with node attributes, we argue that graph convolution should have three properties: seed-oriented, degree-aware, order-free. To this end, we propose multi-task graph convolution where each task represents node representation learning for nodes with a specific degree value, thus leading to preserving the degree-specific graph structure. In particular, we design two multi-task learning methods: degree-specific weight and hashing functions for graph convolution. In addition, we propose a novel graph-level pooling/readout scheme for learning graph representation provably lying in a degree-specific Hilbert kernel space. The experimental results on several node and graph classification benchmark data sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net over state-of-the-art graph neural network models.",0
"Title: Graph Convolutional Neural Networks with Degree Information Attention  Abstract: Graph neural networks (GNN) have become popular models for node classification tasks on graphs where each node has features but no labels. However, most GNN architectures treat all nodes equally regardless of their connectivity degree, which can lead to suboptimal results for certain types of graphs. To address this limitation, we propose DEMO-Net, a novel graph convolutional network architecture that utilizes degree information attention mechanisms to focus more effectively on higher-degree nodes. Our approach builds upon standard message passing techniques by incorporating edge weights based on node degrees, as well as feature gate attentional layers to selectively aggregate neighborhood information according to edge importance. We demonstrate through experiments on both synthetic benchmark datasets and real-world applications including protein-protein interaction prediction and recommender systems, that our method outperforms state-of-the-art baselines across a variety of performance metrics. This work shows promising results for using GNNs with degree-specific attention mechanisms to improve model accuracy and robustness in graph data settings.",1
"Learning rich and compact representations is an open topic in many fields such as object recognition or image retrieval. Deep neural networks have made a major breakthrough during the last few years for these tasks but their representations are not necessary as rich as needed nor as compact as expected. To build richer representations, high order statistics have been exploited and have shown excellent performances, but they produce higher dimensional features. While this drawback has been partially addressed with factorization schemes, the original compactness of first order models has never been retrieved, or at the cost of a strong performance decrease. Our method, by jointly integrating codebook strategy to factorization scheme, is able to produce compact representations while keeping the second order performances with few additional parameters. This formulation leads to state-of-the-art results on three image retrieval datasets.",0
"This paper presents an approach for efficient codebook and factorization learning for second order representation (SOR) models that can process high resolution images. SORs have been shown to outperform traditional convolutional neural networks (CNNs) on certain tasks due to their ability to capture hierarchical representations. However, training SORs can be computationally expensive due to the need to learn large codebooks and perform matrix factorizations. To address these issues, we propose several techniques including an adaptive sampling scheme to efficiently update the codebook during training, a novel method for computing pairwise distances between image patches based on deep features, and a compressed sensing inspired approach for performing low rank approximations of the factors. Our experiments show that our methods can significantly reduce the computational cost of training SOR models while maintaining competitive performance compared to existing approaches. We demonstrate the effectiveness of our model on multiple datasets including CIFAR-10 and ImageNet. Overall, this work represents an important step towards making SOR models more practical for real world applications.",1
"This paper investigates the resilience and robustness of Deep Reinforcement Learning (DRL) policies to adversarial perturbations in the state space. We first present an approach for the disentanglement of vulnerabilities caused by representation learning of DRL agents from those that stem from the sensitivity of the DRL policies to distributional shifts in state transitions. Building on this approach, we propose two RL-based techniques for quantitative benchmarking of adversarial resilience and robustness in DRL policies against perturbations of state transitions. We demonstrate the feasibility of our proposals through experimental evaluation of resilience and robustness in DQN, A2C, and PPO2 policies trained in the Cartpole environment.",0
"This paper presents a novel approach for benchmarking the adversarial resilience and robustness of deep reinforcement learning (DRL) policies using real-time simulation techniques based on physical systems. By incorporating advanced physics engines into the training process, we are able to evaluate DRL policies against more challenging environments that better represent real-world scenarios. Our methodology involves generating diverse sets of perturbations, which are then applied to the initial states of simulated trajectories. We use these perturbed trajectories to evaluate how well our policies can cope with uncertainties present in their environment while maintaining performance under adverse conditions. Our results demonstrate the effectiveness of our approach as a reliable tool for evaluating DRL models across different domains and showcase their improved resilience compared to state-of-the-art methods. This research has significant implications in fields such as autonomous robotics, where decisions made by DRL algorithms have direct consequences on safety and efficiency. Overall, our work contributes towards enhancing both adversarial resistance and overall system stability through realistic simulations, promoting trustworthiness in deployed deep reinforcement learning agents.",1
"We introduce a novel approach to graph-level representation learning, which is to embed an entire graph into a vector space where the embeddings of two graphs preserve their graph-graph proximity. Our approach, UGRAPHEMB, is a general framework that provides a novel means to performing graph-level embedding in a completely unsupervised and inductive manner. The learned neural network can be considered as a function that receives any graph as input, either seen or unseen in the training set, and transforms it into an embedding. A novel graph-level embedding generation mechanism called Multi-Scale Node Attention (MSNA), is proposed. Experiments on five real graph datasets show that UGRAPHEMB achieves competitive accuracy in the tasks of graph classification, similarity ranking, and graph visualization.",0
"Unsupervised inductive representation learning remains one of the most challenging tasks in machine learning due to the difficulty in designing appropriate losses functions that capture the nuances of real world data distributions. In addition to traditional problems such as sparsity and high dimensionality, unstructured graph data presents additional issues, including non-isomorphism, graph-level structure preservation, and scalability concerns. To address these difficulties, we propose a new method called G2GRL which learns low-dimensional representations directly on graphs by minimizing a novel graph proximity loss function designed specifically for the task at hand. The resulting representations can then be used effectively across different datasets or models without any further fine tuning. Experiments demonstrate that our approach significantly outperforms several state-of-the-art methods in terms of accuracy, robustness, efficiency, and interpretability. By providing simple yet effective inductive graph level representations, G2GRL has the potential to greatly impact many fields where graph analysis plays a crucial role, including social network analysis, biological networks, computer vision, natural language processing, and beyond.",1
"We propose $\textit{weighted inner product similarity}$ (WIPS) for neural network-based graph embedding. In addition to the parameters of neural networks, we optimize the weights of the inner product by allowing positive and negative values. Despite its simplicity, WIPS can approximate arbitrary general similarities including positive definite, conditionally positive definite, and indefinite kernels. WIPS is free from similarity model selection, since it can learn any similarity models such as cosine similarity, negative Poincar\'e distance and negative Wasserstein distance. Our experiments show that the proposed method can learn high-quality distributed representations of nodes from real datasets, leading to an accurate approximation of similarities as well as high performance in inductive tasks.",0
"This paper presents a novel representation learning method called ""Weighted inner product"" that can learn general similarities among different feature spaces. Our approach extends previous work on universal approximation using linear functions by allowing for nonlinear mappings into higher dimensional spaces, thereby achieving better performance across a variety of tasks. By leveraging the power of deep neural networks while preserving the interpretability of linear models, our method provides a flexible and scalable alternative to existing techniques. We demonstrate the effectiveness of weighted inner product on several benchmark datasets, showing significant improvements over state-of-the-art results in image classification, sentiment analysis, and information retrieval tasks. Overall, this research represents an important contribution towards advancing the understanding and development of efficient similarity measures for machine learning problems involving complex data structures.",1
"Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits \emph{conditional shift}: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of \emph{any} domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.",0
"This would make a good subheading: ""The problem of domain adaptation""",1
"The representations learned by deep neural networks are difficult to interpret in part due to their large parameter space and the complexities introduced by their multi-layer structure. We introduce a method for computing persistent homology over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input. This topological perspective provides unique insights into the distributed representations encoded by neural networks in terms of the shape of their activation structures. We demonstrate the value of this approach by showing an alternative explanation for the existence of adversarial examples. By studying the topology of network activations across multiple architectures and datasets, we find that adversarial perturbations do not add activations that target the semantic structure of the adversarial class as previously hypothesized. Rather, adversarial examples are explainable as alterations to the dominant activation structures induced by the original image, suggesting the class representations learned by deep networks are problematically sparse on the input space.",0
"Understanding the behavior of deep neural networks (DNN) has become increasingly important as they have been used in many applications, such as image recognition, natural language processing, and even playing games at superhuman levels. One key aspect of DNNs that has received less attention is their activation space. In particular, we ask: how can we characterize the shape of the region in input space where the activations of neurons change? We develop a set of techniques based on the gradient of the network output with respect to input pixels. Our experiments show qualitative differences across architectures, tasks, and depth, highlighting the importance of studying properties of activation space beyond those of weight space alone. By using these methods, practitioners may obtain insights into potential failure modes of current systems and inform design decisions towards creating more robust models going forward.",1
"We investigate the effects of the unsupervised pre-training method under the perspective of information theory. If the input distribution displays multiple views of the supervision, then unsupervised pre-training allows to learn hierarchical representation which communicates these views across layers, while disentangling the supervision. Disentanglement of supervision leads learned features to be independent conditionally to the label. In case of binary features, we show that conditional independence allows to extract label's information with a linear model and therefore helps to solve under-fitting. We suppose that representations displaying multiple views help to solve over-fitting because each view provides information that helps to reduce model's variance. We propose a practical method to measure both disentanglement of supervision and quantity of views within a binary representation. We show that unsupervised pre-training helps to conserve views from input distribution, whereas representations learned using supervised models disregard most of them.",0
"Researchers have explored unsupervised pre-training methods as a means of improving performance on supervised tasks by training models on large amounts of data without explicit labels. In particular, these approaches aim to learn representations that capture underlying structure in the data, which can then be used to improve generalization on downstream tasks. Recent work has shown that such representations can lead to significant improvements in a wide range of domains, including computer vision, natural language processing, and speech recognition. This study examines how unsupervised pre-training impacts the ability of models to retain important features from their input distributions. We find that models trained using unsupervised pre-training tend to perform better than those without pre-training in terms of preserving key characteristics of their inputs. These results suggest that pre-training can play an important role in enabling models to effectively utilize diverse data sources during fine-tuning. Overall, our research highlights the potential benefits of incorporating unsupervised learning techniques into modern machine learning pipelines.",1
"We propose the factorized action variational autoencoder (FAVAE), a state-of-the-art generative model for learning disentangled and interpretable representations from sequential data via the information bottleneck without supervision. The purpose of disentangled representation learning is to obtain interpretable and transferable representations from data. We focused on the disentangled representation of sequential data since there is a wide range of potential applications if disentanglement representation is extended to sequential data such as video, speech, and stock market. Sequential data are characterized by dynamic and static factors: dynamic factors are time dependent, and static factors are independent of time. Previous models disentangle static and dynamic factors by explicitly modeling the priors of latent variables to distinguish between these factors. However, these models cannot disentangle representations between dynamic factors, such as disentangling ""picking up"" and ""throwing"" in robotic tasks. FAVAE can disentangle multiple dynamic factors. Since it does not require modeling priors, it can disentangle ""between"" dynamic factors. We conducted experiments to show that FAVAE can extract disentangled dynamic factors.",0
"Abstract: Recently, generative models have shown promising results for various tasks such as image generation, text completion and video prediction. However, these models often suffer from issues related to sequence disentanglement where they generate spurious or uncorrelated information that does not relate to the underlying factors of variation. To address this issue, we propose Favae - a novel framework that uses an Information Bottleneck variant (FAB) to perform efficient latent variable inference by learning compact representations of input data. Our approach reconstructs the raw audio-visual feature sequence through a feedforward network, imposing regularization constraints based on learned statistical dependencies among the input variables, the representation latents and a set of target features corresponding to different modalities. This regularized autoencoder architecture leads to effective disentangled representations suitable for downstream multi-modal applications. Experiments showcase improvements over strong baselines across various metrics such as reconstruction quality, interpretability measures like Modularity Quality Metric (MQM), task generalization and zero-shot transfer ability across diverse datasets and evaluation protocols. We believe our work bridges a gap towards enabling real-world use cases in complex multi-sensory domains requiring modular and interpretable representations grounded in human cognition principles.",1
"Multi-Person Tracking (MPT) is often addressed within the detection-to-association paradigm. In such approaches, human detections are first extracted in every frame and person trajectories are then recovered by a procedure of data association (usually offline). However, their performances usually degenerate in presence of detection errors, mutual interactions and occlusions. In this paper, we present a deep learning based MPT approach that learns instance-aware representations of tracked persons and robustly online infers states of the tracked persons. Specifically, we design a multi-branch neural network (MBN), which predicts the classification confidences and locations of all targets by taking a batch of candidate regions as input. In our MBN architecture, each branch (instance-subnet) corresponds to an individual to be tracked and new branches can be dynamically created for handling newly appearing persons. Then based on the output of MBN, we construct a joint association matrix that represents meaningful states of tracked persons (e.g., being tracked or disappearing from the scene) and solve it by using the efficient Hungarian algorithm. Moreover, we allow the instance-subnets to be updated during tracking by online mining hard examples, accounting to person appearance variations over time. We comprehensively evaluate our framework on a popular MPT benchmark, demonstrating its excellent performance in comparison with recent online MPT methods.",0
"One of the key challenges faced by online multi-person tracking systems is maintaining consistent identity labels across frames, especially in crowded scenes where re-identification becomes increasingly difficult. This can lead to reduced accuracy and difficulty in analyzing tracked objects over time. In order to address these issues, we propose a novel approach that leverages instance-aware representation learning and association techniques to improve the quality of object detection and tracking. Our method utilizes self-supervised learning to learn discriminative features that capture fine-grained distinctions between instances, as well as temporal consistency constraints to ensure accurate label assignment. We evaluate our method on several benchmark datasets and demonstrate significant improvements over baseline methods. Additionally, we explore different design choices and showcase their impact on performance. Overall, our work presents a significant step towards achieving more robust and reliable online multi-person tracking.",1
"In this paper, the concept of representation learning based on deep neural networks is applied as an alternative to the use of handcrafted features in a method for automatic visual inspection of corroded thermoelectric metallic pipes. A texture convolutional neural network (TCNN) replaces handcrafted features based on Local Phase Quantization (LPQ) and Haralick descriptors (HD) with the advantage of learning an appropriate textural representation and the decision boundaries into a single optimization process. Experimental results have shown that it is possible to reach the accuracy of 99.20% in the task of identifying different levels of corrosion in the internal surface of thermoelectric pipe walls, while using a compact network that requires much less effort in tuning parameters when compared to the handcrafted approach since the TCNN architecture is compact regarding the number of layers and connections. The observed results open up the possibility of using deep neural networks in real-time applications such as the automatic inspection of thermoelectric metal pipes.",0
"This paper presents a novel approach for the classification of thermoelectric metal pipe images based on texture features obtained from Convolutional Neural Networks (CNN). We propose using a combination of Gabor filters and CNNs to extract textures, which capture local patterns and variations that are inherent to the material properties of the metals used in thermoelectric pipes. Our method consists of preprocessing the raw image data to obtain input patches that can then be fed into our deep learning model. We train and test our CNN architecture on these patches and evaluate performance through quantitative metrics such as accuracy and precision. Additionally, we compare our results against those achieved by traditional feature extraction methods and show the superiority of our proposed approach. Finally, we discuss future directions and potential applications of our work towards enabling intelligent monitoring systems in industrial settings, aimed at improving energy efficiency and environmental sustainability.",1
"A significant issue in training deep neural networks to solve supervised learning tasks is the need for large numbers of labelled datapoints. The goal of semi-supervised learning is to leverage ubiquitous unlabelled data, together with small quantities of labelled data, to achieve high task performance. Though substantial recent progress has been made in developing semi-supervised algorithms that are effective for comparatively small datasets, many of these techniques do not scale readily to the large (unlaballed) datasets characteristic of real-world applications. In this paper we introduce a novel approach to scalable semi-supervised learning, called Local Label Propagation (LLP). Extending ideas from recent work on unsupervised embedding learning, LLP first embeds datapoints, labelled and otherwise, in a common latent space using a deep neural network. It then propagates pseudolabels from known to unknown datapoints in a manner that depends on the local geometry of the embedding, taking into account both inter-point distance and local data density as a weighting on propagation likelihood. The parameters of the deep embedding are then trained to simultaneously maximize pseudolabel categorization performance as well as a metric of the clustering of datapoints within each psuedo-label group, iteratively alternating stages of network training and label propagation. We illustrate the utility of the LLP method on the ImageNet dataset, achieving results that outperform previous state-of-the-art scalable semi-supervised learning algorithms by large margins, consistently across a wide variety of training regimes. We also show that the feature representation learned with LLP transfers well to scene recognition in the Places 205 dataset.",0
"Title: Local Label Propagation for Large-Scale Semi-Supervised Learning  Semi-supervised learning (SSL) has emerged as an effective method to tackle large-scale image classification problems where only a small fraction of training data is labeled. In these scenarios, one common approach is to use unlabeled examples along with a few labeled ones during training to improve performance. However, traditional SSL methods often assume that all unlabeled data share similar properties, which might not hold true in real-world applications. This paper presents a novel approach called local label propagation (LLP), which leverages both spatial structure and instance discrimination simultaneously. By doing so, we aim to incorporate more intricate patterns within each mini-batch during training without assuming any prior knowledge on the data distribution.  We first introduce a lightweight local CNN module embedded into our architecture, which allows us to capture both high-level representations from global context and low-level details from dense feature maps. Then, a simple yet efficient optimization algorithm can iteratively refine pseudo labels by preserving local consistency within mini-batches and encouraging significant differences across distinct clusters. With comprehensive experiments conducted on three public datasets (CIFAR-10, CIFAR-100, and SVHN), results demonstrate that LLP outperforms existing state-of-the-art algorithms under various settings, achieving substantial accuracy gains over fully supervised models trained on the same amount of labeled data. Additionally, sensitivity analysis verifies the effectiveness of individual components constituting our framework. Finally, ablation studies confirm the benefits of jointly exploiting local discriminative power and nonlocal correspondence constraints.  In summary, this work provides a simple yet powerful alternative for semi-supervised learning, highlighting the significance of adaptive model calibration according to neighborhood structures. Our future direction involves investigating potential generalization beyond image recognition tasks, exploring complementary regularizers or/and designing better initialization strategies based on advanced domain theories.",1
"The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.",0
"This paper presents a methodology to perform adversarial attacks against graph embedding models that are based on node features and edge relationships by poisoning subgraphs within larger graphs. We begin by discussing how such attacks can disrupt machine learning applications built upon these models and present our proposed framework capable of crafting effective poisoning sets. Our approach uses backdoor triggers inspired from prior work on adversarial examples in other domains, adapting them to create poisoned samples through targeted modification of existing nodes/edges. Experiments demonstrate the effectiveness of our poisoning strategy over multiple benchmark datasets across different architectures and scenarios. Finally, we consider potential mitigation techniques including detection mechanisms and robustification strategies to safeguard model performance under attack. By shedding light on the vulnerability of graph embeddings, we aim to spark further research into addressing adversaries in graph-based machine learning systems.",1
"Image representations are commonly learned from class labels, which are a simplistic approximation of human image understanding. In this paper we demonstrate that transferable representations of images can be learned without manual annotations by modeling human visual attention. The basis of our analyses is a unique gaze tracking dataset of sonographers performing routine clinical fetal anomaly screenings. Models of sonographer visual attention are learned by training a convolutional neural network (CNN) to predict gaze on ultrasound video frames through visual saliency prediction or gaze-point regression. We evaluate the transferability of the learned representations to the task of ultrasound standard plane detection in two contexts. Firstly, we perform transfer learning by fine-tuning the CNN with a limited number of labeled standard plane images. We find that fine-tuning the saliency predictor is superior to training from random initialization, with an average F1-score improvement of 9.6% overall and 15.3% for the cardiac planes. Secondly, we train a simple softmax regression on the feature activations of each CNN layer in order to evaluate the representations independently of transfer learning hyper-parameters. We find that the attention models derive strong representations, approaching the precision of a fully-supervised baseline model for all but the last layer.",0
"Artificial intelligence (AI) has revolutionized many industries and continues to show promising results in healthcare applications. One such application is ultrasound image analysis, which can aid in improving the accuracy and speed of diagnosis. In this paper, we propose a new approach for modeling sonographer visual attention while analyzing ultrasound images using deep neural networks (DNNs). This method captures the relevant features from high resolution ultrasound volumes and reduces computational complexity without significant loss of diagnostic quality. Our experiments demonstrate that our proposed method outperforms previous state-of-the-art methods in terms of segmentation performance and efficiency. Furthermore, our analysis shows that our method effectively models the human expert gaze pattern, leading to more accurate segmentations. Overall, our findings suggest that incorporating a sonographer’s perspective into DNNs could significantly enhance their ability to analyze medical images efficiently and accurately. We believe this work represents a major step forward in developing intelligent systems capable of assisting radiologists and clinicians in daily practice.",1
"Exploration is an extremely challenging problem in reinforcement learning, especially in high dimensional state and action spaces and when only sparse rewards are available. Effective representations can indicate which components of the state are task relevant and thus reduce the dimensionality of the space to explore. In this work, we take a representation learning viewpoint on exploration, utilizing prior experience to learn effective latent representations, which can subsequently indicate which regions to explore. Prior experience on separate but related tasks help learn representations of the state which are effective at predicting instantaneous rewards. These learned representations can then be used with an entropy-based exploration method to effectively perform exploration in high dimensional spaces by effectively lowering the dimensionality of the search space. We show the benefits of this representation for meta-exploration in a simulated object pushing environment.",0
"In order to achieve effective reinforcement learning (RL), it's crucial to properly model both high-level policies and low-level value functions. Latent State Representation (LSR) has been shown to be beneficial towards this goal since it enables more efficient exploration by allowing agents to learn representations that capture task-relevant features in complex environments without being explicitly designed for them. However, several existing LSR methods have problems with their training objectives which often rely on strong assumptions or heuristics, making their application limited in practice. To address these issues, we propose a novel framework based on information theory and stochastic optimal control to learn effective LSRs in RL tasks directly from the raw sensory inputs. By maximizing mutual information between the latent states and actions taken during policy execution, our method can efficiently explore large spaces and learn meaningful representations concurrently. Experimental evaluations demonstrate significant improvements over previous LSR approaches across multiple benchmark domains, including Atari games, MuJoCo robotics, and continuous locomotion tasks, indicating wider applicability of our approach.",1
"A new variational autoencoder (VAE) model is proposed that learns a succinct common representation of two correlated data variables for conditional and joint generation tasks. The proposed Wyner VAE model is based on two information theoretic problems---distributed simulation and channel synthesis---in which Wyner's common information arises as the fundamental limit of the succinctness of the common representation. The Wyner VAE decomposes a pair of correlated data variables into their common representation (e.g., a shared concept) and local representations that capture the remaining randomness (e.g., texture and style) in respective data variables by imposing the mutual information between the data variables and the common representation as a regularization term. The utility of the proposed approach is demonstrated through experiments for joint and conditional generation with and without style control using synthetic data and real images. Experimental results show that learning a succinct common representation achieves better generative performance and that the proposed model outperforms existing VAE variants and the variational information bottleneck method.",0
"Abstract: This paper proposes a new method for joint generation using Variational Autoencoders (VAEs). Our approach, called ""Wyner VAE"", combines both joint and conditional generation into one framework by learning a common representation that can effectively encode information from both tasks. We achieve this through the use of a shared latent space that encodes information about the data in a succinct manner, allowing the model to generate high quality samples quickly and efficiently. In addition, we incorporate a novel loss function that promotes diversity in the generated outputs and improves the overall performance of the system. Our experiments on several benchmark datasets demonstrate the effectiveness of our proposed method over other state-of-the-art approaches. Overall, Wyner VAE provides a powerful tool for generative models to perform complex and diverse tasks while maintaining high fidelity to their training data.",1
"Robust road detection is a key challenge in safe autonomous driving. Recently, with the rapid development of 3D sensors, more and more researchers are trying to fuse information across different sensors to improve the performance of road detection. Although many successful works have been achieved in this field, methods for data fusion under deep learning framework is still an open problem. In this paper, we propose a Siamese deep neural network based on FCN-8s to detect road region. Our method uses data collected from a monocular color camera and a Velodyne-64 LiDAR sensor. We project the LiDAR point clouds onto the image plane to generate LiDAR images and feed them into one of the branches of the network. The RGB images are fed into another branch of our proposed network. The feature maps that these two branches extract in multiple scales are fused before each pooling layer, via padding additional fusion layers. Extensive experimental results on public dataset KITTI ROAD demonstrate the effectiveness of our proposed approach.",0
"Title: ""Deep Representation Learning for Road Detection through Siamese Network""  This work presents a novel approach to road detection using deep representation learning through a siamese network architecture. Traditional approaches to road detection rely heavily on handcrafted features and have limitations in accurately detecting roads in complex urban environments. This study aims to address these challenges by leveraging convolutional neural networks (CNNs) for feature extraction and utilizing a siamese network for effective representations of input images. By training the model on large amounts of data, our method achieves state-of-the-art performance in road detection tasks while improving upon previous methods that solely relied on handcrafted features. Our contributions include a thorough evaluation of the proposed method, including ablation studies and comparisons against other popular approaches. We believe that our work has significant implications for advancing the field of computer vision, particularly in the realm of road detection and autonomous vehicles. Overall, we propose a powerful new tool for road detection that can effectively operate in diverse and dynamic urban settings.",1
"Auto-encoders have emerged as a successful framework for unsupervised learning. However, conventional auto-encoders are incapable of utilizing explicit relations in structured data. To take advantage of relations in graph-structured data, several graph auto-encoders have recently been proposed, but they neglect to reconstruct either the graph structure or node attributes. In this paper, we present the graph attention auto-encoder (GATE), a neural network architecture for unsupervised representation learning on graph-structured data. Our architecture is able to reconstruct graph-structured inputs, including both node attributes and the graph structure, through stacked encoder/decoder layers equipped with self-attention mechanisms. In the encoder, by considering node attributes as initial node representations, each layer generates new representations of nodes by attending over their neighbors' representations. In the decoder, we attempt to reverse the encoding process to reconstruct node attributes. Moreover, node representations are regularized to reconstruct the graph structure. Our proposed architecture does not need to know the graph structure upfront, and thus it can be applied to inductive learning. Our experiments demonstrate competitive performance on several node classification benchmark datasets for transductive and inductive tasks, even exceeding the performance of supervised learning baselines in most cases.",0
"Artificial neural networks have become increasingly popular due to their ability to handle complex tasks such as image classification, speech recognition, natural language processing, and more. However, one challenge faced by these models is scalability: as the size of the input data increases, the computational cost of training and inference becomes prohibitive. To address this issue, researchers have proposed graph attention auto-encoders (GAAs), which operate directly on graphs rather than vector representations of the data. GAAs can efficiently encode high-dimensional data into a lower-dimensional representation while preserving important relationships between elements in the graph. This enables faster model training, efficient memory usage, and improved performance on downstream tasks that require knowledge of the graph structure. In summary, the use of graph attention auto-encoders offers a promising direction towards tackling the challenges posed by large datasets and improving the scalability of artificial neural networks.",1
"We investigate the high-dimensional data clustering problem by proposing a novel and unsupervised representation learning model called Robust Flexible Auto-weighted Local-coordinate Concept Factorization (RFA-LCF). RFA-LCF integrates the robust flexible CF, robust sparse local-coordinate coding and the adaptive reconstruction weighting learning into a unified model. The adaptive weighting is driven by including the joint manifold preserving constraints on the recovered clean data, basis concepts and new representation. Specifically, our RFA-LCF uses a L2,1-norm based flexible residue to encode the mismatch between clean data and its reconstruction, and also applies the robust adaptive sparse local-coordinate coding to represent the data using a few nearby basis concepts, which can make the factorization more accurate and robust to noise. The robust flexible factorization is also performed in the recovered clean data space for enhancing representations. RFA-LCF also considers preserving the local manifold structures of clean data space, basis concept space and the new coordinate space jointly in an adaptive manner way. Extensive comparisons show that RFA-LCF can deliver enhanced clustering results.",0
"This research presents a novel unsupervised method for image clustering called RUFALC (Robust Unsupervised Flexible Auto-Weighted Local-coordinate Concept factorization). RUFALC leverages recent advances in deep learning to create flexible yet efficient data representations for images that can capture meaningful features in high dimensional spaces. By utilizing auto-weighting techniques and local coordinates to handle concept drift, our model achieves robustness against changes in data distributions over time. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach in producing state-of-the-art performance compared to existing methods.",1
"Learning effective embedding has been proved to be useful in many real-world problems, such as recommender systems, search ranking and online advertisement. However, one of the challenges is data sparsity in learning large-scale item embedding, as users' historical behavior data are usually lacking or insufficient in an individual domain. In fact, user's behaviors from different domains regarding the same items are usually relevant. Therefore, we can learn complete user behaviors to alleviate the sparsity using complementary information from correlated domains. It is intuitive to model users' behaviors using graph, and graph neural networks (GNNs) have recently shown the great power for representation learning, which can be used to learn item embedding. However, it is challenging to transfer the information across domains and learn cross-domain representation using the existing GNNs. To address these challenges, in this paper, we propose a novel model - Deep Multi-Graph Embedding (DMGE) to learn cross-domain representation. Specifically, we first construct a multi-graph based on users' behaviors from different domains, and then propose a multi-graph neural network to learn cross-domain representation in an unsupervised manner. Particularly, we present a multiple-gradient descent optimizer for efficiently training the model. We evaluate our approach on various large-scale real-world datasets, and the experimental results show that DMGE outperforms other state-of-art embedding methods in various tasks.",0
"This can then be further refined into multiple papers of shorter length that focus on specific applications:  Abstract: In recent years, there has been increasing interest in developing neural network models capable of learning cross-domain representations. These models have the potential to greatly improve performance across many tasks by enabling more generalizable knowledge transfer between domains. One approach towards achieving this goal is through the use of multi-graph neural networks (MGNNs), which employ multiple graphs to model different relationships within and across domains. In our work, we explore several strategies for training MGNNs and demonstrate their effectiveness on a range of real-world problems, including image classification, natural language processing, and graph learning. Our results show that MGNNs outperform other state-of-the-art methods on these tasks, highlighting their promise as a powerful tool for representation learning. We believe this work represents a significant step forward in the field of deep learning, and we hope it inspires future research in this exciting area.",1
"Unsupervised exploration and representation learning become increasingly important when learning in diverse and sparse environments. The information-theoretic principle of empowerment formalizes an unsupervised exploration objective through an agent trying to maximize its influence on the future states of its environment. Previous approaches carry certain limitations in that they either do not employ closed-loop feedback or do not have an internal state. As a consequence, a privileged final state is taken as an influence measure, rather than the full trajectory. We provide a model-free method which takes into account the whole trajectory while still offering the benefits of option-based approaches. We successfully apply our approach to settings with large action spaces, where discovery of meaningful action sequences is particularly difficult.",0
"This research paper focuses on unsupervised learning methods for discovering influential trajectories from large datasets. By leveraging advancements in computational power and data storage capabilities, we can analyze high volumes of diverse time-series data collected over many years. These trajectories span numerous areas such as consumer behavior patterns, health metrics tracking, urban mobility trends, industrial process monitoring, among others. Our approach identifies and quantifies how these trajectories evolve, converge, diverge or lead into each other. We reveal underlying mechanisms driving these changes allowing experts across different domains to draw meaningful conclusions regarding their respective fields. Our technique offers robustness by handling noisy data points without relying on predefined features or supervision. Additionally, it works under minimal assumptions and remains scalable even as new data continues to arrive. The results demonstrate that our algorithm accurately discovers meaningful evolutionary paths while remaining efficient in terms of computational resources consumption and memory footprint. Ultimately, the algorithm enables users to benefit more fully from data exploration by providing them valuable insights faster than previously possible.",1
"Online signature verification (OSV) is one of the most challenging tasks in writer identification and digital forensics. Owing to the large intra-individual variability, there is a critical requirement to accurately learn the intra-personal variations of the signature to achieve higher classification accuracy. To achieve this, in this paper, we propose an OSV framework based on deep convolutional Siamese network (DCSN). DCSN automatically extracts robust feature descriptions based on metric-based loss function which decreases intra-writer variability (Genuine-Genuine) and increases inter-individual variability (Genuine-Forgery) and directs the DCSN for effective discriminative representation learning for online signatures and extend it for one shot learning framework. Comprehensive experimentation conducted on three widely accepted benchmark datasets MCYT-100 (DB1), MCYT-330 (DB2) and SVC-2004-Task2 demonstrate the capability of our framework to distinguish the genuine and forgery samples. Experimental results confirm the efficiency of deep convolutional Siamese network based OSV by achieving a lower error rate as compared to many recent and state-of-the art OSV techniques.",0
"This paper presents OSVNet (Online Signature Verification Network), which is based on convolutional Siamese network architecture. We propose two variants of OSVNet: one using only image features from the signature images, without any additional input such as writer/signer biometrics; another variant that uses both signature dynamics captured through sensor data streams and static handwritten images. Both our models achieve state-of-the-art performance across multiple benchmark datasets including dynamic variations such as pressure sensors as well as static camera capture conditions. Our contributions can benefit applications such as digital document verifications systems, online transactions and banking security, providing a higher level of trust for users and businesses alike. OSVNet utilizes convolutional neural networks in order to identify signatures within documents. By training these models on large datasets containing examples of known legitimate signatures, it is able to detect fakes by understanding how real signatures behave. This has important implications for secure document handling, allowing organizations to ensure their legal documents aren’t tampered with. In addition to its potential use cases in industries like finance and law, OSVNet also brings benefits to e-commerce sites where electronic signatures have become more prevalent. Its ability to verify identity adds value to these services, making them more appealing compared to ones without similar protection. Overall, this research represents an improvement over previous techniques because of its effectiveness at finding fraudulent signatures while maintaining high accuracy for valid ones. As a result, this innovation holds great promise for future advancements in this field.",1
"Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach is based primarily on the strategic routing of data through the two latent variables, and thus is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.",0
"Abstract: This paper proposes a novel approach to invariant equivariant representation learning for multi-class classification problems. We introduce a new model that learns representations which are both stable under transformations (invariant) and preserve their meaning across different views (equivalent). Our method uses an attention mechanism to selectively focus on relevant features and incorporates adversarial training to improve robustness. Experiments show that our model significantly outperforms baseline methods on several benchmark datasets, demonstrating the effectiveness of our approach. Our findings have important implications for real-world applications such as image recognition and computer vision tasks, where invariant equivariance is crucial for achieving high accuracy.",1
"Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate the \textit{graph representation} from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator $\pooling$ based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework $\m$ for graph classification. Theoretical analysis is provided to understand $\pooling$ from both local and global perspectives. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.",0
"In recent years, graph convolutional networks (GCN) have emerged as a powerful tool for processing graphs, outperforming traditional methods such as spectral clustering and random walks. However, one major challenge faced by GCNs is that they can suffer from over-smoothing, where the features of nodes become indistinguishable as the network depth increases. To address this issue, we propose a new method called eigenpooling which involves regularizing the graph spectrum during training. We demonstrate through experiments on several benchmark datasets that our proposed method significantly improves node classification accuracy while reducing over-smoothing issues. Our results show that eigenpooling effectively captures global and local structure information and leads to improved performance compared to existing pooling techniques used in GCNs. This work has implications in fields such as social network analysis and computer vision.",1
"Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.",0
"This paper explores the variational bounds of mutual information, a measure of the amount of information shared by two random variables. We examine existing upper and lower bounds on mutual information and propose new bounds based on variational methods. Our results provide tighter bounds than previously known, leading to improved accuracy in estimating mutual information. Additionally, we illustrate our findings using numerical examples and discuss potential applications in fields such as signal processing and machine learning.",1
"The recent use of `Big Code' with state-of-the-art deep learning methods offers promising avenues to ease program source code writing and correction. As a first step towards automatic code repair, we implemented a graph neural network model that predicts token types for Javascript programs. The predictions achieve an accuracy above $90\%$, which improves on previous similar work.",0
"Here's a possible abstract:  In recent years, there has been increasing interest in applying machine learning techniques to the task of inferring type information for JavaScript code. One promising approach that has emerged is based on graph neural networks (GNNs), which can leverage the rich structure present in JavaScript programs to make predictions about their types. However, developing effective GNN models for this problem remains challenging due to the complexity of JavaScript's dynamic typing system and the large number of potential type annotations that may appear in any given piece of code. To address these difficulties, we propose a novel framework for GNN inference of JavaScript types that incorporates several key advances over previous work in this area. Firstly, our model utilizes a powerful yet lightweight graph representation of JavaScript syntax trees, allowing us to capture essential properties of the code without becoming bogged down by unnecessary details. Secondly, we introduce a new method for aggregating node representations within each layer of the GNN architecture, enabling more efficient processing of information from the code graph. Finally, we apply regularization techniques during training to encourage generalizable behavior from our model across different input distributions, improving overall predictive accuracy and reducing risk of overfitting. Our experimental evaluation demonstrates that our proposed approach leads to significant improvements over prior methods for type inference using GNNs in the context of JavaScript programs. We believe that these results provide valuable insight into how GNN architectures can be applied effectively to complex programming languages like JavaScript, and lay the groundwork for future research exploring applications of deep learning to software development tasks beyond mere prediction problems such as debugging or generation.",1
"Performing machine learning on structured data is complicated by the fact that such data does not have vectorial form. Therefore, multiple approaches have emerged to construct vectorial representations of structured data, from kernel and distance approaches to recurrent, recursive, and convolutional neural networks. Recent years have seen heightened attention in this demanding field of research and several new approaches have emerged, such as metric learning on structured data, graph convolutional neural networks, and recurrent decoder networks for structured data. In this contribution, we provide an high-level overview of the state-of-the-art in representation learning and embeddings for structured data across a wide range of machine learning fields.",0
"This paper presents a deep learning framework that leverages embedding techniques to enable efficient representation learning from structured data sources such as tables and graphs. We begin by discussing how traditional machine learning approaches face limitations when dealing with complex structures like these. We then describe our approach which involves mapping structured data into continuous vector spaces using carefully designed embeddings functions. Our method relies on recurrent neural networks (RNNs) and graph convolutional networks (GCNs), both of which have been widely used in natural language processing tasks but less so in structured data analysis. Through extensive experiments we demonstrate that our approach leads to significant improvements over state-of-the-art methods across several key metrics. We showcase the effectiveness of our model on real world datasets including social network analysis, protein structure prediction, recommender systems and table question answering. Overall, our work represents a step forward in enabling machines to effectively learn representations from structured data, with promising applications in various domains such as bioinformatics, healthcare, finance and beyond.",1
"Large-scale graph data in real-world applications is often not static but dynamic, i. e., new nodes and edges appear over time. Current graph convolution approaches are promising, especially, when all the graph's nodes and edges are available during training. When unseen nodes and edges are inserted after training, it is not yet evaluated whether up-training or re-training from scratch is preferable. We construct an experimental setup, in which we insert previously unseen nodes and edges after training and conduct a limited amount of inference epochs. In this setup, we compare adapting pretrained graph neural networks against retraining from scratch. Our results show that pretrained models yield high accuracy scores on the unseen nodes and that pretraining is preferable over retraining from scratch. Our experiments represent a first step to evaluate and develop truly online variants of graph neural networks.",0
"Abstract: In recent years, graph neural networks (GNNs) have emerged as a powerful tool for processing complex data on graphs. However, despite their promising performance, most GNN models require significant amounts of pretraining before they can be used effectively. This presents a problematic bottleneck in many applications where real-time inference is necessary, such as online recommendation systems, fraud detection, or real-time disease outbreak tracking. In this study, we examine whether GNNs can indeed operate ""online"" without extensive pretraining by analyzing state-of-the-art GNN architectures and techniques for both pretraining and inference. Our analysis reveals that while some architectures show promise for online operation, others struggle significantly due to the high computational requirements imposed by pretraining. Moreover, we find that many existing techniques tailored towards offline scenarios might not directly translate to online environments, requiring novel approaches to handle dynamic changes in input and model parameters. Overall, our work provides new insights into the limitations of current GNN models operating under online settings, and highlights potential future research directions towards making these networks more suitable for real-time applications.",1
"Autoencoders are a deep learning model for representation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subspace spanned by the top principal directions but cannot learn the principal directions themselves. In this paper, we prove that $L_2$-regularized LAEs are symmetric at all critical points and learn the principal directions as the left singular vectors of the decoder. We smoothly parameterize the critical manifold and relate the minima to the MAP estimate of probabilistic PCA. We illustrate these results empirically and consider implications for PCA algorithms, computational neuroscience, and the algebraic topology of learning.",0
"Deep neural networks have achieved remarkable successes in recent years across many application domains such as image classification, speech recognition, natural language processing etc. However, understanding why deep neural network works, how it generalizes remains a great challenge towards deploying these models into safety critical applications. In this context, regularization methods aim to reduce overfitting by introducing inductive biases that encourage solutions with more regularity. Among them linear autoencoder (AE) has been extensively studied because it can learn robust representations without relying on complex architectures. Despite extensive work on developing training criteria and optimizers for learning autoencoders, little attention has been paid to understanding the optimization landscape. We first provide formal definitions of good loss functions with different desirable properties, including decomposability, strict saddle point nondegeneracy and compatibility with generalization bounds. For each property we construct explicit examples showing both positive results and negative counterexamples indicating limitations. These demonstrate that there are inherent difficulties in finding a tractable function satisfying all desired properties simultaneously. Motivated from our theoretical findings, we propose two new families of loss landscapes which achieve the best tradeoff among all examined objectives to make the problem more practically solvable. By incorporating structured sparsity enforcement techniques such as nuclear norm and l_p norms we show improved performance using proposed losses in experiments on MNIST handwritten digit dataset.",1
"We introduce a novel unsupervised domain adaptation approach for object detection. We aim to alleviate the imperfect translation problem of pixel-level adaptations, and the source-biased discriminativity problem of feature-level adaptations simultaneously. Our approach is composed of two stages, i.e., Domain Diversification (DD) and Multi-domain-invariant Representation Learning (MRL). At the DD stage, we diversify the distribution of the labeled data by generating various distinctive shifted domains from the source domain. At the MRL stage, we apply adversarial learning with a multi-domain discriminator to encourage feature to be indistinguishable among the domains. DD addresses the source-biased discriminativity, while MRL mitigates the imperfect image translation. We construct a structured domain adaptation framework for our learning paradigm and introduce a practical way of DD for implementation. Our method outperforms the state-of-the-art methods by a large margin of 3%~11% in terms of mean average precision (mAP) on various datasets.",0
"In recent years, object detection has become increasingly important due to its numerous applications across various domains such as computer vision, robotics, and autonomous driving systems. However, one major challenge that remains is the task adaptation problem, where models trained on large datasets from one domain have difficulty generalizing to new domains with different characteristics. This study proposes a novel paradigm called Diversify and Match (DM) which addresses this issue by utilizing transfer learning to improve model performance. The DM approach involves two key components - diversification and matching. First, the base model is trained using multiple source domains, improving its robustness through exposure to diverse environments and scenarios. Second, during testing, the model adapts to the target domain by optimizing a joint loss function which balances both the diversity and similarity between the source and target distributions. Experimental results demonstrate the effectiveness of our method over several state-of-the-art baselines across four benchmark datasets, showing improved accuracy and robustness to changes in domain shift. Our work provides valuable insights into the design and training of more versatile object detection models with wide applicability across various fields.",1
"The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.",0
"Title: Understanding the Robustness of Deep Representation Models for Intervention Analysis  Causal inference techniques have been used extensively to determine the effectiveness of intervention strategies in complex systems. However, these methods often rely on assumptions that may lead to biased results or poor generalization ability across different domains. In recent years, deep representation models (DRMs) have emerged as promising tools for capturing robust causal relationships between variables by exploiting their underlying structure. These models have achieved state-of-the-art performance in various domains but require validation to ensure they effectively capture interventional robustness. This study investigates the extent to which DRMs can accurately represent causal mechanisms, thus providing confidence in their use for decision making under uncertainty. We focus on identifying and validating key features required for robust intervention analysis using large datasets from diverse domains. Our results demonstrate significant improvements over existing approaches while highlighting the importance of regularization techniques for ensuring the stability of estimated causal structures. Furthermore, we provide insights into the interpretability of learned representations, allowing practitioners to better understand and trust their decisions based on our findings. Overall, our work bridges the gap between theory and practice by establishing guidelines for applying DRMs to real-world scenarios where robust causal inference is essential.",1
Sensors are an integral part of modern Internet of Things (IoT) applications. There is a critical need for the analysis of heterogeneous multivariate temporal data obtained from the individual sensors of these systems. In this paper we particularly focus on the problem of the scarce amount of training data available per sensor. We propose a novel federated multi-task hierarchical attention model (FATHOM) that jointly trains classification/regression models from multiple sensors. The attention mechanism of the proposed model seeks to extract feature representations from the input and learn a shared representation focused on time dimensions across multiple sensors. The underlying temporal and non-linear relationships are modeled using a combination of attention mechanism and long-short term memory (LSTM) networks. We find that our proposed method outperforms a wide range of competitive baselines in both classification and regression settings on activity recognition and environment monitoring datasets. We further provide visualization of feature representations learned by our model at the input sensor level and central time level.,0
"In recent years, advancements in sensing technology have led to the proliferation of sensor data from various domains such as healthcare, environmental monitoring, industrial control systems, etc. With the increasing complexity and diversity of these datasets, there arises a need for effective analytics techniques that can process and interpret this vast amount of sensor data efficiently. To address this challenge, we present a novel approach based on federated learning and hierarchical attention models that enables distributed multi-task sensor analytics at scale. Our proposed framework leverages decentralized training mechanisms to enable privacy-preserving collaboration among multiple parties while minimizing communication overheads. The model architecture features a hierarchical attention mechanism that allows for fine-grained modulation of sensor data processing across different task categories, ensuring robustness even under varying conditions. We evaluate our methodology through extensive experiments using several real-world benchmark datasets, demonstrating significant improvements over state-of-the-art approaches across various metrics such as accuracy, latency, scalability, and robustness.",1
"Multiple clustering aims at exploring alternative clusterings to organize the data into meaningful groups from different perspectives. Existing multiple clustering algorithms are designed for single-view data. We assume that the individuality and commonality of multi-view data can be leveraged to generate high-quality and diverse clusterings. To this end, we propose a novel multi-view multiple clustering (MVMC) algorithm. MVMC first adapts multi-view self-representation learning to explore the individuality encoding matrices and the shared commonality matrix of multi-view data. It additionally reduces the redundancy (i.e., enhancing the individuality) among the matrices using the Hilbert-Schmidt Independence Criterion (HSIC), and collects shared information by forcing the shared matrix to be smooth across all views. It then uses matrix factorization on the individual matrices, along with the shared matrix, to generate diverse clusterings of high-quality. We further extend multiple co-clustering on multi-view data and propose a solution called multi-view multiple co-clustering (MVMCC). Our empirical study shows that MVMC (MVMCC) can exploit multi-view data to generate multiple high-quality and diverse clusterings (co-clusterings), with superior performance to the state-of-the-art methods.",0
"In recent years, there has been a growing interest in clustering algorithms that can handle multiple view data. These algorithms aim to group samples based on their similarities across different feature representations. In many real world applications, datasets often have more than one representation of the underlying structure. For example, images may have color as well as texture features, while web pages can be represented by both content and link analysis. Handling such multi-view data poses unique challenges due to differences in feature scales, ranges, and measurement units which make traditional methods difficult to apply.  Multi-View Multiple Clustering (MVMC) is an emerging technique that tackles these issues by jointly utilizing information from all views. This approach allows for richer characterization of patterns present in complex data structures and helps discover better clusterings compared to single-view counterparts. MVMC algorithms can take advantage of the complementary nature of different views and provide more comprehensive understanding of high-dimensional data. In addition, they enable effective handling of noisy and incomplete data since each view can act as a regularizer for the others.  This work presents a survey of existing approaches towards addressing the problem of Multi-View Multiple Clustering. We review techniques based on shared and specific information among views, discuss their respective merits and limitations, and evaluate their performances using standard metrics. Our study reveals insights into current research trends and identifies promising directions for future investigation. Ultimately, we aim to offer guidelines for practitioners interested in deploying MVMC solutions in real-world settings.",1
"We address the problem of graph classification based only on structural information. Inspired by natural language processing techniques (NLP), our model sequentially embeds information to estimate class membership probabilities. Besides, we experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",0
"Title: Variational Recurrent Neural Networks for Graph Classification  Abstract: This paper presents a novel approach for graph classification using variational recurrent neural networks (RNN). Traditional RNN models have difficulty capturing complex dependencies due to their fixed memory architecture. To address these limitations, we introduce variational RNNs which use a latent variable representation that can learn dynamic feature representations on graphs. Our model uses graph convolutional layers to encode node features into graph embeddings, followed by a gated recurrent unit that learns temporal patterns over time steps. We then propose a variational inference framework that inferred the posterior distribution over hidden states given the observed data. Experimental results show significant improvements over state-of-the-art approaches on several benchmark datasets for graph classification. Our work demonstrates the potential of variational RNNs as a powerful tool for modeling graph structured data.",1
"We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.",0
"Abstract: This paper presents Graph U-Nets (GUNs), a novel approach for processing irregularly structured data such as graphs and networks. GUNs utilize the Universal Approximation Theorem property inherent in deep neural networks, which allows them to represent any function from input space to output space. Specifically, we focus on constructing U-shaped architectures inspired by convolutional neural networks that can effectively process graph signals of varying sizes and shapes. We evaluate the performance of our method on several benchmark datasets and demonstrate state-of-the-art results compared to existing methods. Our findings suggest that GUNs have significant potential for use in a wide range of applications including computer vision, natural language processing, and recommender systems. Overall, this work advances our understanding of how to design and train effective models for handling complex nonlinear relationships present in real-world data.",1
"Graph neural network (GNN), as a powerful representation learning model on graph data, attracts much attention across various disciplines. However, recent studies show that GNN is vulnerable to adversarial attacks. How to make GNN more robust? What are the key vulnerabilities in GNN? How to address the vulnerabilities and defense GNN against the adversarial attacks? In this paper, we propose DefNet, an effective adversarial defense framework for GNNs. In particular, we first investigate the latent vulnerabilities in every layer of GNNs and propose corresponding strategies including dual-stage aggregation and bottleneck perceptron. Then, to cope with the scarcity of training data, we propose an adversarial contrastive learning method to train the GNN in a conditional GAN manner by leveraging the high-level graph representation. Extensive experiments on three public datasets demonstrate the effectiveness of DefNet in improving the robustness of popular GNN variants, such as Graph Convolutional Network and GraphSAGE, under various types of adversarial attacks.",0
"In this paper, we propose an adversarial defense framework for graph neural networks (GNNs). GNNs have emerged as powerful models for analyzing complex data types such as graphs. However, like other machine learning systems, they are susceptible to adversarial attacks that can compromise their accuracy and integrity. To address these concerns, we develop a comprehensive defense framework based on three key components: detection, correction, and prevention. Our proposed framework leverages multiple techniques from deep learning, computer vision, natural language processing, and cryptography to detect potential adversarial examples early, correct them before deployment, and ultimately prevent them through regularization and robust training methods. Experimental results show that our approach significantly improves model resilience against diverse attack scenarios while maintaining acceptable performance. Overall, this work provides an effective solution towards enhancing the security and reliability of GNNs in real-world applications.",1
"Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distribution called \textit{pseudo-hyperbolic Gaussian}, a Gaussian-like distribution on hyperbolic space whose density can be evaluated analytically and differentiated with respect to the parameters. Our distribution enables the gradient-based learning of the probabilistic models on hyperbolic space that could never have been considered before. Also, we can sample from this hyperbolic probability distribution without resorting to auxiliary means like rejection sampling. As applications of our distribution, we develop a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.",0
"Title: ""A Novel Method for Distributed Computing over Nonlinear Networks"" Abstract: In recent years, distributed computing has become increasingly important due to the growing amount of data generated by modern systems. Existing methods have focused mainly on linear models, but many real-world problems exhibit nonlinear relationships that cannot be accurately modeled using these techniques. To address this limitation, we propose a new approach based on wrapped normal distributions over hyperbolic space (WN-HSD). This method allows us to capture complex dependencies while still maintaining computational efficiency. We demonstrate the effectiveness of our technique through extensive simulations on several challenging tasks including image classification, speech recognition, and natural language processing. Our results show that WN-HSD outperforms state-of-the-art methods across multiple datasets, making it a promising tool for distributed gradient-based learning over nonlinear networks.",1
"Learning an effective representation for high-dimensional data is a challenging problem in reinforcement learning (RL). Deep reinforcement learning (DRL) such as Deep Q networks (DQN) achieves remarkable success in computer games by learning deeply encoded representation from convolution networks. In this paper, we propose a simple yet very effective method for representation learning with DRL algorithms. Our key insight is that features learned by DRL algorithms are highly correlated, which interferes with learning. By adding a regularized loss that penalizes correlation in latent features (with only slight computation), we decorrelate features represented by deep neural networks incrementally. On 49 Atari games, with the same regularization factor, our decorrelation algorithms perform $70\%$ in terms of human-normalized scores, which is $40\%$ better than DQN. In particular, ours performs better than DQN on 39 games with 4 close ties and lost only slightly on $6$ games. Empirical results also show that the decorrelation method applies to Quantile Regression DQN (QR-DQN) and significantly boosts performance. Further experiments on the losing games show that our decorelation algorithms can win over DQN and QR-DQN with a fined tuned regularization factor.",0
Asymptotically exact bounds on performance in reinforcement learning (RL) are provably achievable under well specified conditions [2]. These results are founded on model free RL methods that sample transition data from an underlying MDP or POMDP system. However these algorithms typically assume the Markov property which leads us to suspect there may exist an opportunity cost in terms of final quality attained with respect to a particular problem solution. In order to quantify precisely this gap we leverage model free deep Q network architectures along with decorrelated value approximation functions that satisfy key assumptions concerning their relationship between Bellman errors and their corresponding gradients (i.e. gradient decorrelation). Our methodology further requires no modification of policy updates making implementation feasible across a wide range of problems. By utilizing both offline training as well as experience replay we find significant improvements in performance over prior art baselines at scaled up problem sizes where previous techniques tended towards failure [1][3]. Through our experiments we demonstrate a clear tradeoff between computational demands on models vis à vis quality attained and offer additional exploration into how regularization of value functions can facilitate deeper approximations overall leading to better results without incurring excessively large computational costs. Code has been provided alongside experimental scripts allowing interested researchers easy replication of our results or extension work beyond what is included herein.,1
"We propose an end-to-end deep learning learning model for graph classification and representation learning that is invariant to permutation of the nodes of the input graphs. We address the challenge of learning a fixed size graph representation for graphs of varying dimensions through a differentiable node attention pooling mechanism. In addition to a theoretical proof of its invariance to permutation, we provide empirical evidence demonstrating the statistically significant gain in accuracy when faced with an isomorphic graph classification task given only a small number of training examples. We analyse the effect of four different matrices to facilitate the local message passing mechanism by which graph convolutions are performed vs. a matrix parametrised by a learned parameter pair able to transition smoothly between the former. Finally, we show that our model achieves competitive classification performance with existing techniques on a set of molecule datasets.",0
"PiNet (Permutation Invariant Graph Neural Network) is a novel approach to graph classification that utilizes permutation invariant graphs as input to capture essential structural features while eliminating irrelevant details such as node indices. By doing so, our model can learn robust representations that are less prone to overfitting and more efficient to train. We evaluate the effectiveness of PiNet using several benchmark datasets and demonstrate its superior performance compared to state-of-the-art methods across different metrics. Our ablation studies further validate the importance of permutation invariance in capturing relevant graph structures for effective graph representation learning. Overall, PiNet establishes itself as a promising technique in graph neural networks for improved graph classification accuracy.",1
"In this paper, we reformulate the forest representation learning approach as an additive model which boosts the augmented feature instead of the prediction. We substantially improve the upper bound of generalization gap from $\mathcal{O}(\sqrt\frac{\ln m}{m})$ to $\mathcal{O}(\frac{\ln m}{m})$, while $\lambda$ - the margin ratio between the margin standard deviation and the margin mean is small enough. This tighter upper bound inspires us to optimize the margin distribution ratio $\lambda$. Therefore, we design the margin distribution reweighting approach (mdDF) to achieve small ratio $\lambda$ by boosting the augmented feature. Experiments and visualizations confirm the effectiveness of the approach in terms of performance and representation learning ability. This study offers a novel understanding of the cascaded deep forest from the margin-theory perspective and further uses the mdDF approach to guide the layer-by-layer forest representation learning.",0
"In our modern world, machine learning has become increasingly important, as data and analytics drive innovation across a variety of fields, including but not limited to robotics, natural language processing (NLP), computer vision, geospatial analysis, scientific simulations, financial predictions, medical diagnosis, creative works, autonomous vehicles, education programs, customer service operations, and more. Machine learning algorithms can process vast amounts of raw data, recognize patterns in that data, make decisions based on those patterns, adjust their models over time via feedback loops to continually improve performance, automate tasks, enable conversations, build complex structures from scratch or interactively using human guidance, optimize processes according to user objectives, and even create other software applications, making them particularly valuable tools in nearly every industry sector. However, creating effective machine learning systems remains challenging due to several factors: model complexity, high dimensionality of input spaces, poor scalability of current techniques, limited transparency into how models achieve impressive accuracy numbers on benchmarks, lack of robustness when confronted with inputs outside training distributions, and difficulty generalizing to unseen situations. To overcome these limitations, new approaches must be developed which balance the ability to capture highly nonlinear relationships with interpretability requirements and the need for efficient inference during deployment at scale, while still providing state of the art results in a range of application domains. One promising direction involves incorporating domain knowledge into the learning process via constraints derived from prior beliefs encoded via probability density functions, effectively guiding the optimization problem towards solutions aligned wi",1
"Vehicle Re-identification is attracting more and more attention in recent years. One of the most challenging problems is to learn an efficient representation for a vehicle from its multi-viewpoint images. Existing methods tend to derive features of dimensions ranging from thousands to tens of thousands. In this work we proposed a deep learning based framework that can lead to an efficient representation of vehicles. While the dimension of the learned features can be as low as 256, experiments on different datasets show that the Top-1 and Top-5 retrieval accuracies exceed multiple state-of-the-art methods. The key to our framework is two-fold. Firstly, variational feature learning is employed to generate variational features which are more discriminating. Secondly, long short-term memory (LSTM) is used to learn the relationship among different viewpoints of a vehicle. The LSTM also plays as an encoder to downsize the features.",0
"In this paper we present Variational Representation Learning (VRL), a new approach to vehicle reidentification that learns representations from data by optimizing variational lower bounds on recognition performance. VRL allows us to efficiently learn high-dimensional feature embeddings which capture informative cues from raw image features. We apply our method to two real world datasets containing multiple vehicles captured under varying conditions and compare against state-of-the art methods using several evaluation metrics. Our results demonstrate consistent improvements over existing methods across all benchmarks.",1
"Image completion is the problem of generating whole images from fragments only. It encompasses inpainting (generating a patch given its surrounding), reverse inpainting/extrapolation (generating the periphery given the central patch) as well as colorization (generating one or several channels given other ones). In this paper, we employ a deep network to perform image completion, with adversarial training as well as perceptual and completion losses, and call it the ``missing data encoder'' (MDE). We consider several configurations based on how the seed fragments are chosen. We show that training MDE for ``random extrapolation and colorization'' (MDE-REC), i.e. using random channel-independent fragments, allows a better capture of the image semantics and geometry. MDE training makes use of a novel ``hide-and-seek'' adversarial loss, where the discriminator seeks the original non-masked regions, while the generator tries to hide them. We validate our models both qualitatively and quantitatively on several datasets, showing their interest for image completion, unsupervised representation learning as well as face occlusion handling.",0
"Artificial intelligence (AI) has revolutionized many fields and industries by automating routine tasks and solving complex problems that were previously unsolvable without human intervention. Computer vision, a subfield of AI focused on enabling computers to interpret and understand visual data from the world, has seen significant advances in recent years. One common task in computer vision is image completion, which involves filling in missing parts of an incomplete image to make it whole again. However, traditional methods have limited performance due to their reliance on pre-defined masks and lack of generalization across different domains. In this work, we propose a novel approach called ""The Missing Data Encoder"" to tackle this problem using deep learning techniques. Our model utilizes a two-stage adversarial training scheme inspired by hide-and-seek games, where the generator network learns to generate realistic patches to fill in missing regions in images while fooling a discriminator network. We evaluate our method on multiple benchmark datasets, showing state-of-the-art results compared to existing approaches. Furthermore, our proposed framework can handle missing regions under various conditions such as object occlusion and large missing areas, making it more robust and versatile than current solutions. This research paves the way for improved capabilities in cross-channel image completion, opening up new possibilities for applications in multimedia processing, virtual reality, robotics, and other related fields.",1
"While deep representation learning has become increasingly capable of separating task-relevant representations from other confounding factors in the data, two significant challenges remain. First, there is often an unknown and potentially infinite number of confounding factors coinciding in the data. Second, not all of these factors are readily observable. In this paper, we present a deep conditional generative model that learns to disentangle a task-relevant representation from an unknown number of confounding factors that may grow infinitely. This is achieved by marrying the representational power of deep generative models with Bayesian non-parametric factor models, where a supervised deterministic encoder learns task-related representation and a probabilistic encoder with an Indian Buffet Process (IBP) learns the unknown number of unobservable confounding factors. We tested the presented model in two datasets: a handwritten digit dataset (MNIST) augmented with colored digits and a clinical ECG dataset with significant inter-subject variations and augmented with signal artifacts. These diverse data sets highlighted the ability of the presented model to grow with the complexity of the data and identify the absence or presence of unobserved confounding factors.",0
"In deep generative models (DGMs) such as Variational Autoencoders (VAEs), latent variables z play an important role by allowing for data generation from prior distributions p(z). However, current methods only model dependencies among observations but ignore interdependencies across layers (or factors) that could inform us about the causal structure underlying complex phenomena. This can limit their explanatory power for applications that aim at identifying confounding factors in order to disambiguate cause-effect relationships. We introduce the novel concept of beta Bernoulli process (BBP) which extends VAEs/AutoRegressive Distribution Estimator to capture these additional interlayer dependencies explicitly. Our contributions are: 1) We provide theoretical derivations establishing conditions on when BBP captures faithful Bayesian updates. We show that BBP satisfies these conditions if certain assumptions hold regarding the model architecture, including linearity within each layer, and pairwise interactions between any two layers. These results motivate our choice of specific regularization terms when learning parameters of the BBP. As we shall see next, we can learn both the causal graph among observed variables and the distribution over unseen confounders jointly via maximum likelihood estimation under a structural equation model. 2) We apply our method to synthetic datasets generated from ground truth graphs. By comparing BBP with standard VAE or other competitors, we demonstrate how adding explicit interdependency among layers improves inference accuracy and leads to more interpretable representations capturing the true underlying causes driving correlations. 3) We consider a real-world application to estimating genetic effects on gene expression levels using human twin studies. Compared again with a range of alternatives including GWAS, LassoRese",1
"Graphs are complex objects that do not lend themselves easily to typical learning tasks. Recently, a range of approaches based on graph kernels or graph neural networks have been developed for graph classification and for representation learning on graphs in general. As the developed methodologies become more sophisticated, it is important to understand which components of the increasingly complex methods are necessary or most effective.   As a first step, we develop a simple yet meaningful graph representation, and explore its effectiveness in graph classification. We test our baseline representation for the graph classification task on a range of graph datasets. Interestingly, this simple representation achieves similar performance as the state-of-the-art graph kernels and graph neural networks for non-attributed graph classification. Its performance on classifying attributed graphs is slightly weaker as it does not incorporate attributes. However, given its simplicity and efficiency, we believe that it still serves as an effective baseline for attributed graph classification. Our graph representation is efficient (linear-time) to compute. We also provide a simple connection with the graph neural networks.   Note that these observations are only for the task of graph classification while existing methods are often designed for a broader scope including node embedding and link prediction. The results are also likely biased due to the limited amount of benchmark datasets available. Nevertheless, the good performance of our simple baseline calls for the development of new, more comprehensive benchmark datasets so as to better evaluate and analyze different graph learning methods. Furthermore, given the computational efficiency of our graph summary, we believe that it is a good candidate as a baseline method for future graph classification (or even other graph learning) studies.",0
"Graphs have become increasingly important as data becomes more complex and interconnected. This has led to a growing interest in developing algorithms that can automatically classify graphs according to their attributes. In recent years, there have been many successful attempts at solving this problem using deep learning methods. However, these approaches require large amounts of labeled training data which is often unavailable. Furthermore, they may overfit the available data, leading to poor generalization performance on new datasets. In this work, we present a simple yet effective approach to graph classification without requiring attribute labels during model training. Our method utilizes a convolutional neural network (CNN) architecture commonly used in image recognition tasks to learn features from raw graph representations such as adjacency matrices or Laplacian eigenmaps. We show through extensive experiments on multiple benchmark datasets that our approach outperforms existing state-of-the art methods in terms of accuracy while requiring significantly less training data. Additionally, we demonstrate the effectiveness of our method under scenarios where only few samples per class are provided, making it applicable even with limited labeling resources.",1
"The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the discrepancy between domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit order-wise matching of higher order moments has not been considered before. We propose to match the higher order central moments of probability distributions by means of order-wise moment differences. Our model does not require computationally expensive distance and kernel matrix computations. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). We prove that CMD is a metric on the set of probability distributions on a compact interval. We further prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. We test our approach on two different benchmark data sets for object recognition (Office) and sentiment analysis of product reviews (Amazon reviews). CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity analysis shows that the new approach is stable w.r.t. parameter changes in a certain interval. The source code of the experiments is publicly available.",0
"This is an example of how you can write an informative abstract: ""Domain-invariant representation learning is the problem of training machine learning models that generalize well across different domains, such as images from different cameras or text from different languages. In recent years, a popular approach has been to use adversarial training, which involves training two neural networks simultaneously - one that generates examples and another that discriminates between real and generated examples. However, existing methods suffer from high computational costs, lack of interpretability, and failure to handle multiple tasks simultaneously. To address these problems, we propose central moment discrepancy (CMD), a simple and interpretable method that trains generators by maximizing their distance to their own distribution. We show through experiments on several benchmark datasets that our method outperforms state-of-the-art alternatives in terms of generating diverse and high quality samples while significantly reducing computational cost.""",1
"Deep generative architectures provide a way to model not only images but also complex, 3-dimensional objects, such as point clouds. In this work, we present a novel method to obtain meaningful representations of 3D shapes that can be used for challenging tasks including 3D points generation, reconstruction, compression, and clustering. Contrary to existing methods for 3D point cloud generation that train separate decoupled models for representation learning and generation, our approach is the first end-to-end solution that allows to simultaneously learn a latent space of representation and generate 3D shape out of it. Moreover, our model is capable of learning meaningful compact binary descriptors with adversarial training conducted on a latent space. To achieve this goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D input and create 3D output. Thanks to our end-to-end training regime, the resulting method called 3D Adversarial Autoencoder (3dAAE) obtains either binary or continuous latent space that covers a much wider portion of training data distribution. Finally, our quantitative evaluation shows that 3dAAE provides state-of-the-art results for 3D points clustering and 3D object retrieval.",0
"This paper presents a novel approach called adversarial autoencoders (AAEs) that compresses 3D point clouds into compact representations while preserving their geometric properties. We employ two subnetworks, one generative network (GAN) trying to reconstruct the input shape from the compressed codes, another discriminator network trained to differentiate real and fake shapes based on the distance measure learned by both networks. During training we alternate between optimizing reconstruction loss and maximizing distance between real and fake pairs. Experiment results demonstrate our method produces more accurate and robust representations compared to current state-of-art methods. Our AAEs achieve better accuracy over existing approaches such as traditional autoencoder and Variational Auto-Encoder (VAE). These improvements allow us to directly compare distances between point cloud sets, extract statistical features, and classify objects under challenging scenarios like nonlinear deformations where conventional descriptors fail. Finally, we validate the effectiveness of our model through extensive experimental evaluations on diverse benchmark datasets for tasks including classification, retrieval, correspondence estimation and outlier detection. Overall, adversarial autoencoders offer a promising solution for reducing computational complexity while retaining desirable performance across multiple applications in computer vision and robotics fields.",1
"Competitive diving is a well recognized aquatic sport in which a person dives from a platform or a springboard into the water. Based on the acrobatics performed during the dive, diving is classified into a finite set of action classes which are standardized by FINA. In this work, we propose an attention guided LSTM-based neural network architecture for the task of diving classification. The network takes the frames of a diving video as input and determines its class. We evaluate the performance of the proposed model on a recently introduced competitive diving dataset, Diving48. It contains over 18000 video clips which covers 48 classes of diving. The proposed model outperforms the classification accuracy of the state-of-the-art models in both 2D and 3D frameworks by 11.54% and 4.24%, respectively. We show that the network is able to localize the diver in the video frames during the dive without being trained with such a supervision.",0
"This work presents a novel deep learning model for diving classification that utilizes spatio-temporal representation learning to capture important features from video data. Our approach leverages attention mechanisms to focus on relevant regions and frames within the input videos, allowing our model to learn more effectively. We evaluate our model on several publicly available datasets and demonstrate significant improvements over existing state-of-the-art methods. Additionally, we provide qualitative analysis on real-world examples showcasing the effectiveness of our attentive representation learning approach. Our method has promising applications in sports analytics and other domains where accurate tracking of motion events is crucial.",1
"In this paper we seek methods to effectively detect urban micro-events. Urban micro-events are events which occur in cities, have limited geographical coverage and typically affect only a small group of citizens. Because of their scale these are difficult to identify in most data sources. However, by using citizen sensing to gather data, detecting them becomes feasible. The data gathered by citizen sensing is often multimodal and, as a consequence, the information required to detect urban micro-events is distributed over multiple modalities. This makes it essential to have a classifier capable of combining them. In this paper we explore several methods of creating such a classifier, including early, late, hybrid fusion and representation learning using multimodal graphs. We evaluate performance on a real world dataset obtained from a live citizen reporting system. We show that a multimodal approach yields higher performance than unimodal alternatives. Furthermore, we demonstrate that our hybrid combination of early and late fusion with multimodal embeddings performs best in classification of urban micro-events.",0
"An efficient system capable of accurately detecting urban micro-events from multimodal data streams has been developed. This system utilizes machine learning techniques to classify events across multiple modalities such as video, audio, and sensor data. We propose a novel architecture that integrates deep feature extractors with non-deep feature processing components into a unified framework. Experimental results demonstrate that our approach significantly outperforms existing methods in detecting diverse micro-event categories while achieving low computational complexity. Furthermore, we conduct thorough evaluation studies on publicly available datasets to showcase the effectiveness and efficiency of the proposed methodology.",1
"Data for face analysis often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary deep learning methods typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain inter-cluster margins both within and between classes. This tight constraint effectively reduces the class imbalance inherent in the local data neighborhood, thus carving much more balanced class boundaries locally. We show that it is easy to deploy angular margins between the cluster distributions on a hypersphere manifold. Such learned Cluster-based Large Margin Local Embedding (CLMLE), when combined with a simple k-nearest cluster algorithm, shows significant improvements in accuracy over existing methods on both face recognition and face attribute prediction tasks that exhibit imbalanced class distribution.",0
"In recent years, deep learning has become increasingly popular as a methodology for face recognition and attribute prediction due to its ability to handle complex patterns and features present in images. However, traditional convolutional neural networks (CNNs) have limited success on imbalanced datasets where one class dominates over the others. This can lead to poor performance and biased results towards majority classes. To address these issues, we propose a novel framework called ""Deep Imbalanced Learning"" which aims to balance the distribution of data by using techniques such as oversampling, undersampling, and reweighting the loss function. Our approach outperforms existing state-of-the-art methods on benchmark datasets such as CelebA and LFWA, achieving higher accuracy and reduced bias towards the majority class. Additionally, our model produces more balanced predictions across all attributes, making it well suited for real-world applications that require accurate representation of minority populations.",1
"As an efficient and scalable graph neural network, GraphSAGE has enabled an inductive capability for inferring unseen nodes or graphs by aggregating subsampled local neighborhoods and by learning in a mini-batch gradient descent fashion. The neighborhood sampling used in GraphSAGE is effective in order to improve computing and memory efficiency when inferring a batch of target nodes with diverse degrees in parallel. Despite this advantage, the default uniform sampling suffers from high variance in training and inference, leading to sub-optimum accuracy. We propose a new data-driven sampling approach to reason about the real-valued importance of a neighborhood by a non-linear regressor, and to use the value as a criterion for subsampling neighborhoods. The regressor is learned using a value-based reinforcement learning. The implied importance for each combination of vertex and neighborhood is inductively extracted from the negative classification loss output of GraphSAGE. As a result, in an inductive node classification benchmark using three datasets, our method enhanced the baseline using the uniform sampling, outperforming recent variants of a graph neural network in accuracy.",0
"This paper proposes a novel approach to graph representation learning called ""Advancing GraphSAGE with Data-driven Node Sampling"". Building on existing methods such as Graph Convolutional Networks (GCN) and GAT, our method incorporates data-driven sampling strategies that improve the efficiency and accuracy of graph neural networks. By selecting a subset of influential nodes in the graph for feature extraction and aggregation, we reduce computational complexity while preserving important structural information from the original graph. Our method outperforms state-of-the-art approaches on benchmark datasets, demonstrating its effectiveness in advancing graph representation learning. We believe this work has significant implications for applications in areas such as social network analysis, recommender systems, and bioinformatics.",1
In this paper we present a self-supervised method for representation learning utilizing two different modalities. Based on the observation that cross-modal information has a high semantic meaning we propose a method to effectively exploit this signal. For our approach we utilize video data since it is available on a large scale and provides easily accessible modalities given by RGB and optical flow. We demonstrate state-of-the-art performance on highly contested action recognition datasets in the context of self-supervised learning. We show that our feature representation also transfers to other tasks and conduct extensive ablation studies to validate our core contributions. Code and model can be found at https://github.com/nawidsayed/Cross-and-Learn.,0
"This work is concerned with how artificial intelligence can learn through cross-modal supervision, using one modality as input data and another modality to provide guidance on how well the system is performing. We find that, given the right constraints and regularization techniques, systems trained in this way can achieve strong performance even without direct supervision from a human annotator. Through experimentation on several domains we show that our methods can match traditional fully-supervised learning techniques while requiring significantly less labelled training data. Additionally, by exploiting prior knowledge from other modalities such as images or text, we demonstrate that multi-modal models can be used to transfer learned knowledge between tasks and thus improve overall system performance in new unseen environments. Finally, our analysis shows that these methods have wide applicability across multiple architectures, task types, and data sizes. Overall, these results have important implications for the development of self-reliant and adaptive intelligent agents capable of acquiring complex abilities autonomously. This work explores the potential of AI to learn and perform complex tasks through cross-modal self-supervision, where one modality serves as input data and the other provides guidance for evaluation purposes. By utilizing appropriate constraints and regularization strategies, our study demonstrates that highly competitive outcomes can be achieved with minimal human annotation compared to traditional fully-supervised approaches. Our experiments reveal improved system performances by leveraging previous knowledge acquired from related modalities like images or texts. These benefits extend to diverse AI frameworks, job categories, and dataset dimensions, making them viable solutions applicable throughout different applications. Ultimately, our research presents valuable insights towards developing independent and resilient intelligent entities equipped with efficient adaptation capabilities.",1
"Graph Convolutional Networks (GCNs) have been widely studied for graph data representation and learning tasks. Existing GCNs generally use a fixed single graph which may lead to weak suboptimal for data representation/learning and are also hard to deal with multiple graphs. To address these issues, we propose a novel Graph Optimized Convolutional Network (GOCN) for graph data representation and learning. Our GOCN is motivated based on our re-interpretation of graph convolution from a regularization/optimization framework. The core idea of GOCN is to formulate graph optimization and graph convolutional representation into a unified framework and thus conducts both of them cooperatively to boost their respective performance in GCN learning scheme. Moreover, based on the proposed unified graph optimization-convolution framework, we propose a novel Multiple Graph Optimized Convolutional Network (M-GOCN) to naturally address the data with multiple graphs. Experimental results demonstrate the effectiveness and benefit of the proposed GOCN and M-GOCN.",0
"In recent years, convolutional neural networks (CNNs) have proven to be highly effective in numerous computer vision tasks such as image classification, object detection, and segmentation. However, designing efficient CNN architectures that can achieve high accuracy while balancing computational complexity and memory usage remains a challenging task. To address this issue, we propose graph optimized convolutional networks (GOConvNets), which leverage graph theory to optimize the topology of deep learning models. Our approach leverages principles from random geometric graphs, enabling us to effectively regularize overparameterization and reduce the model size without sacrificing performance. Experimental results on benchmark datasets demonstrate that our method significantly improves upon existing state-of-the-art approaches, achieving higher accuracy while reducing computational overhead. Overall, GOConvNets represent a significant advancement towards creating more efficient and scalable CNN architectures.",1
"We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.",0
"In recent years, graph representation learning has become increasingly important due to the growth of complex networks such as social media platforms and knowledge graphs. In this work, we present an efficient approach for graph representation learning using PyTorch Geometric, which allows for fast training while maintaining high accuracy. Our method leverages advances in deep learning architectures and optimizes them for use on large scale graphs. We demonstrate our method on multiple benchmark datasets, showing that our approach outperforms state-of-the-art algorithms in terms of both speed and accuracy. With its wide range of applications, including recommendation systems, fraud detection, and medical diagnosis, graph representation learning continues to be a rapidly developing field. This work contributes to the development of faster and more accurate methods for understanding complex network structures and relationships.",1
"Facial action unit (AU) recognition is a crucial task for facial expressions analysis and has attracted extensive attention in the field of artificial intelligence and computer vision. Existing works have either focused on designing or learning complex regional feature representations, or delved into various types of AU relationship modeling. Albeit with varying degrees of progress, it is still arduous for existing methods to handle complex situations. In this paper, we investigate how to integrate the semantic relationship propagation between AUs in a deep neural network framework to enhance the feature representation of facial regions, and propose an AU semantic relationship embedded representation learning (SRERL) framework. Specifically, by analyzing the symbiosis and mutual exclusion of AUs in various facial expressions, we organize the facial AUs in the form of structured knowledge-graph and integrate a Gated Graph Neural Network (GGNN) in a multi-scale CNN framework to propagate node information through the graph for generating enhanced AU representation. As the learned feature involves both the appearance characteristics and the AU relationship reasoning, the proposed model is more robust and can cope with more challenging cases, e.g., illumination change and partial occlusion. Extensive experiments on the two public benchmarks demonstrate that our method outperforms the previous work and achieves state of the art performance.",0
"In this paper we propose a new method of facial action unit (FAU) recognition using semantic relationships guided representation learning. FAUs are important indicators of emotions and mental states, making their accurate detection essential for applications such as affective computing and autism research. Existing methods have shown promising results but often suffer from limitations including overfitting due to limited training data, sensitivity to occlusions, and lack of interpretability. We address these issues by introducing a new approach that exploits the rich semantic structure present in high quality annotated datasets. By leveraging hierarchical annotations that capture both local deformations and global geometric structures across different facial expressions, our method learns more robust representations that generalize well on challenging benchmarks. Our extensive experiments demonstrate significant improvements over state-of-the-art methods and provide insights into the effectiveness of each component. These findings highlight the potential of semantics guided representation learning in bridging the gap between computer vision and natural language processing. -----In this paper, a novel approach for facial action unit (FAU) recognition is presented which utilizes semantic relationships guided representation learning. Accurate identification of FAUs is crucial for numerous applications, ranging from affective computing to studying autism spectrum disorder. While existing methods have produced encouraging outcomes, they still face several challenges such as overfitting due to limited dataset sizes, sensitivity towards occlusions, and low interpretability. To overcome these shortcomings, this work proposes a method that capitalizes on the rich semantic structure available within comprehensively labeled high-quality databases. By employing hierarchical annotations that encapsulate both fine-grained deformations and broad geometric configurations across diverse expressions, our framework learns more resilient representations that exhibit strong performance on demanding benchmarks. Through exhaustive evaluations, we show that our technique significantly enhances upon current state-of-the-art techniques and offer valuable perspectives regarding the impact of each component. Ultimately, our findings underscore the importance of incorporating semantic knowledge within machine vision frameworks to close the divide betwe",1
"Deep neural decision forest (NDF) achieved remarkable performance on various vision tasks via combining decision tree and deep representation learning. In this work, we first trace the decision-making process of this model and visualize saliency maps to understand which portion of the input influence it more for both classification and regression problems. We then apply NDF on a multi-task coordinate regression problem and demonstrate the distribution of routing probabilities, which is vital for interpreting NDF yet not shown for regression problems. The pre-trained model and code for visualization will be available at https://github.com/Nicholasli1995/VisualizingNDF",0
"Deep Neural Decision Forests (DNDF) have emerged as an effective approach for solving complex decision problems across multiple domains. However, understanding how these models make decisions remains challenging due to their opaque nature. In this work, we propose a methodology for visualizing the decision-making process within DNDFs using sensitivity analysis techniques. By analyzing the impact of individual features on model predictions at different levels of granularity, our approach provides insights into which features are most important and informative in shaping the final outcome. We demonstrate the effectiveness of our approach through extensive experiments on benchmark datasets from diverse application areas, including image classification, text sentiment analysis, and time-series forecasting. Our results show that our technique can effectively guide feature selection, explain the black box behavior of DNDFs, and support interpretability in machine learning research and applications. This study paves the way towards enhancing trustworthiness and transparency in artificial intelligence by improving the interpretability of advanced model architectures like DNDFs.",1
"ProductNet is a collection of high-quality product datasets for better product understanding. Motivated by ImageNet, ProductNet aims at supporting product representation learning by curating product datasets of high quality with properly chosen taxonomy. In this paper, the two goals of building high-quality product datasets and learning product representation support each other in an iterative fashion: the product embedding is obtained via a multi-modal deep neural network (master model) designed to leverage product image and catalog information; and in return, the embedding is utilized via active learning (local model) to vastly accelerate the annotation process. For the labeled data, the proposed master model yields high categorization accuracy (94.7% top-1 accuracy for 1240 classes), which can be used as search indices, partition keys, and input features for machine learning models. The product embedding, as well as the fined-tuned master model for a specific business task, can also be used for various transfer learning tasks.",0
"Product Net introduces a collection of datasets for product representation learning. We present high quality image data together with their corresponding feature representations learned from state-of-the-art deep networks trained on them. By doing so we aim at providing researchers who work on problems related to computer vision with high quality training data that can serve as benchmarks and foster progress. We introduce several types of images including objects, scenes, and human made artifacts which vary significantly regarding color, texture, and shape. Furthermore, by publishing these dataset collections online, we hope they will spur new innovations and improve the current state-of-the- art methods in machine learning and computer vision. All images were manually labeled and cleaned such that each class contains only one type of object without any overlap between classes. This makes our dataset well-suited for use in training data-hungry neural network models but equally applicable as evaluation set for testing the generalization capabilities of a method or algorithm. Our website provides easy access to download all examples along with precomputed features. We provide detailed instructions about how to label your own images with minimal effort using web-based annotation tools which are integrated into our site. As future work we plan to extend our dataset with further categories, additional splits, and even more variations inside existing ones. ProductNet offers a valuable resource for those working on product representation learning through the provision of high-quality image datasets alongside their associated feature representations obtained from advanced deep learning models. These datasets encompass a wide range of items, from everyday objects to intricate human creations, which cover various aspects such as color, texture, and form. To ensure suitability for diverse applications, the datasets are meticulously prepared and contain no overlapping categories within each class. With the availabilit",1
Learning disentangled representation from any unlabelled data is a non-trivial problem. In this paper we propose Information Maximising Autoencoder (InfoAE) where the encoder learns powerful disentangled representation through maximizing the mutual information between the representation and given information in an unsupervised fashion. We have evaluated our model on MNIST dataset and achieved 98.9 ($\pm .1$) $\%$ test accuracy while using complete unsupervised training.,0
This will be published on arXiv so should follow their guidelines: https://arxiv.org/help/guides/license_and_usage If you are uncertain about the license your work has been released under contact [research@microsoft.com](mailto:research@microsoft.com) . For more detailed instructions please see our submission guide. You can view statistics on paper submissions here. Please note that any PDF metadata that violates our policies may be removed by our system and cannot be restored by us. Ensure all details required by the licence have been included before uploading. We recommend using a recent version of Adobe Acrobat to create your pdf file as older versions sometimes fail the validation process without warning. Always test first!,1
"The Variational Autoencoder (VAE) is a powerful architecture capable of representation learning and generative modeling. When it comes to learning interpretable (disentangled) representations, VAE and its variants show unparalleled performance. However, the reasons for this are unclear, since a very particular alignment of the latent embedding is needed but the design of the VAE does not encourage it in any explicit way. We address this matter and offer the following explanation: the diagonal approximation in the encoder together with the inherent stochasticity force local orthogonality of the decoder. The local behavior of promoting both reconstruction and orthogonality matches closely how the PCA embedding is chosen. Alongside providing an intuitive understanding, we justify the statement with full theoretical analysis as well as with experiments.",0
"This paper investigates variational autoencoder (VAE) models trained on high-dimensional data and their tendency to pursue principal component analysis (PCA) directions by accident. VAEs have become popular in recent years due to their ability to learn compressed representations that can be used for tasks such as clustering and dimensionality reduction. However, little attention has been paid to the specific directions along which these representations are learned.  We demonstrate through experiments on both synthetic and real datasets that VAEs tend to learn low-rank representations that align well with the underlying PCA directions. We show that this behavior occurs regardless of whether explicit regularization is used to promote low-rank solutions, indicating that it may be an inherent property of VAEs.  Our findings highlight the potential benefits and drawbacks of using VAEs for representation learning. On one hand, VAEs may provide a simple and efficient alternative to explicitly computing PCA. On the other hand, they may miss important information present in non-PCA directions, potentially leading to suboptimal performance on certain downstream tasks.  Overall, our work sheds light on the inner workings of VAEs and provides insights into their strengths and limitations as tools for representation learning.",1
"We propose a dynamic neighborhood aggregation (DNA) procedure guided by (multi-head) attention for representation learning on graphs. In contrast to current graph neural networks which follow a simple neighborhood aggregation scheme, our DNA procedure allows for a selective and node-adaptive aggregation of neighboring embeddings of potentially differing locality. In order to avoid overfitting, we propose to control the channel-wise connections between input and output by making use of grouped linear projections. In a number of transductive node-classification experiments, we demonstrate the effectiveness of our approach.",0
"This paper proposes a novel graph neural network (GNN) architecture called ""Just Jump"" that dynamically aggregates features from neighborhood nodes using a jumping mechanism. GNNs have achieved state-of-the-art results on tasks involving data represented as graphs such as node classification, link prediction, and community detection. However, most existing GNN architectures suffer from high computational cost due to dense matrix operations and oversmoothing issues caused by deep message passing.  To address these problems, we propose the ""Jump-Then-Average"" technique which samples only a subset of neighbors instead of considering all possible combinations. We then develop our dynamic graph aggregator based on a random walk algorithm to guide feature propagation throughout the graph. Our approach efficiently selects a set of informative neighbors at each layer and effectively reduces oversmoothing effects while improving performance compared to baseline models. Empirical evaluation on several benchmark datasets demonstrates the effectiveness of our proposed method.",1
"In recent years, there have been numerous developments towards solving multimodal tasks, aiming to learn a stronger representation than through a single modality. Certain aspects of the data can be particularly useful in this case - for example, correlations in the space or time domain across modalities - but should be wisely exploited in order to benefit from their full predictive potential. We propose two deep learning architectures with multimodal cross-connections that allow for dataflow between several feature extractors (XFlow). Our models derive more interpretable features and achieve better performances than models which do not exchange representations, usefully exploiting correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. Our work improves on existing multimodal deep learning algorithms in two essential ways: (1) it presents a novel method for performing cross-modality (before features are learned from individual modalities) and (2) extends the previously proposed cross-connections which only transfer information between streams that process compatible data. Illustrating some of the representations learned by the connections, we analyse their contribution to the increase in discrimination ability and reveal their compatibility with a lip-reading network intermediate representation. We provide the research community with Digits, a new dataset consisting of three data types extracted from videos of people saying the digits 0-9. Results show that both cross-modal architectures outperform their baselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits datasets, achieving state-of-the-art results.",0
"Aim to highlight three salient points from your work. Include at least one numerical result in the abstract that highlights the benefits/performance of XFlow. Numerical results should contain appropriate units (e.g., msec,%). If you need additional space please use footnotes to provide references rather than inline citations. The problem we consider arises in situations where multiple inputs arrive simultaneously and each has some uncertainty as to whether or how they relate to events currently occurring on a scene. For example, if a person walks into an office building, someone may hear the sound of their heels hitting the floor but can’t see them yet because they are still behind closed doors; conversely, security cameras observing the lobby may have already seen the person but haven’t heard any audio of them walking. These uncertain relations form a pairwise probability distribution over possible event relationships called compatibility matrices. Under a probabilistic framework, we model these distributions using Gaussian mixture models (GMM) whose parameters are learned automatically based solely on input data. We take a Bayesian approach by treating inference as prediction given missing evidence or hypotheses. Thus our goal becomes predicting compatibility matrix entries P(A|B), which quantify the conditional dependence of an audiovisual event A on another visual event B. To achieve this goal we introduce a novel deep learning architecture called xflow, short for cross-modal flow. Our method outperforms other methods in terms of accuracy and speed and makes several qualitatively different predictions compared to previous state of the art systems. On two benchmark datasets, METEOR-AVC [24] and SiLLiC AVCC dataset [17], XFLOW consistently achieves better F1 scores (6% higher on average). Furthermore, the speed of our approach allows realtime processing (as fast as 28ms per frame). Finally, footnote citation [9].  [Full disclosure: I am the author of the referenced study.]",1
"Spatio-temporal graphs such as traffic networks or gene regulatory systems present challenges for the existing deep learning methods due to the complexity of structural changes over time. To address these issues, we introduce Spatio-Temporal Deep Graph Infomax (STDGI)---a fully unsupervised node representation learning approach based on mutual information maximization that exploits both the temporal and spatial dynamics of the graph. Our model tackles the challenging task of node-level regression by training embeddings to maximize the mutual information between patches of the graph, at any given time step, and between features of the central nodes of patches, in the future. We demonstrate through experiments and qualitative studies that the learned representations can successfully encode relevant information about the input graph and improve the predictive performance of spatio-temporal auto-regressive forecasting models.",0
"In recent years, deep graph infomax (DGI) methods have shown promise for learning latent representations that capture complex dependencies between entities and events within sequential data, such as time series or text streams. However, these models typically assume stationarity and independence across time, which can limit their ability to capture spatiotemporal relationships and evolving patterns. To address this limitation, we propose spatio-temporal DGI (STDGI), which extends standard DGI by incorporating spatial correlations into its objective function and regularization terms. Our approach builds on popular graph neural networks like GAE and VGAE, but with a novel focus on capturing both spatial and temporal structure within graphs constructed from streaming data. Experiments on synthetic datasets demonstrate STDGI's effectiveness at identifying emerging anomalies, forecasting future states based on current patterns, and recovering true causal relationships from observational data. We believe our work opens up exciting new directions for developing more powerful deep generative models capable of discovering high-level abstractions from diverse and rapidly changing input domains.",1
"For a product of interest, we propose a search method to surface a set of reference products. The reference products can be used as candidates to support downstream modeling tasks and business applications. The search method consists of product representation learning and fingerprint-type vector searching. The product catalog information is transformed into a high-quality embedding of low dimensions via a novel attention auto-encoder neural network, and the embedding is further coupled with a binary encoding vector for fast retrieval. We conduct extensive experiments to evaluate the proposed method, and compare it with peer services to demonstrate its advantage in terms of search return rate and precision.",0
"This research presents a novel approach for product search that utilizes semantic references extracted from natural language queries as contextual cues. We introduce a deep learning model capable of jointly generating image regions, descriptions, and category labels given a query sentence. Our method allows users to search for products by referring to specific features within images, enabling more precise and efficient product discovery. Experimental results show significant improvements over baseline methods on both quantitative metrics and human evaluations, demonstrating the effectiveness and relevance of our proposed solution.",1
"Compressed sensing techniques enable efficient acquisition and recovery of sparse, high-dimensional data signals via low-dimensional projections. In this work, we propose Uncertainty Autoencoders, a learning framework for unsupervised representation learning inspired by compressed sensing. We treat the low-dimensional projections as noisy latent representations of an autoencoder and directly learn both the acquisition (i.e., encoding) and amortized recovery (i.e., decoding) procedures. Our learning objective optimizes for a tractable variational lower bound to the mutual information between the datapoints and the latent representations. We show how our framework provides a unified treatment to several lines of research in dimensionality reduction, compressed sensing, and generative modeling. Empirically, we demonstrate a 32% improvement on average over competing approaches for the task of statistical compressed sensing of high-dimensional datasets.",0
"This paper proposes a novel approach to learning compressed representations using uncertainty autoencoders. By maximizing the variational information content of latent codes, we can learn representations that capture both the underlying structure of data as well as their inherent uncertainties. Our model extends traditional autoencoder frameworks by incorporating variational inference into the training process, allowing us to explicitly model the posterior distribution over possible reconstructions given observed inputs. We demonstrate the effectiveness of our method on several benchmark datasets, showing that our learned representations outperform state-of-the-art methods across a range of tasks including image generation, denoising, and compression.",1
"We propose a new deep architecture for person re-identification (re-id). While re-id has seen much recent progress, spatial localization and view-invariant representation learning for robust cross-view matching remain key, unsolved problems. We address these questions by means of a new attention-driven Siamese learning architecture, called the Consistent Attentive Siamese Network. Our key innovations compared to existing, competing methods include (a) a flexible framework design that produces attention with only identity labels as supervision, (b) explicit mechanisms to enforce attention consistency among images of the same person, and (c) a new Siamese framework that integrates attention and attention consistency, producing principled supervisory signals as well as the first mechanism that can explain the reasoning behind the Siamese framework's predictions. We conduct extensive evaluations on the CUHK03-NP, DukeMTMC-ReID, and Market-1501 datasets and report competitive performance.",0
"In this paper, we propose a novel deep learning architecture designed specifically for image re-identification tasks based on person re-identification (re-id). Our approach is inspired by the success of attention mechanisms used in natural language processing tasks, particularly those used in machine translation. We introduce the Consistent Attentive Siamese Network (CASN), which employs both spatial and channel attention networks that learn to attend only to discriminative features from input images while ignoring irrelevant details. CASN consistently outperforms previous state-of-the-art methods across four benchmark datasets: Market1501, DukeMTMC-ReID, CUHK03, and MSMT17. Notably, our method achieves new records on three of these datasets - including reducing the rank-1 error rate by more than half on two of them - demonstrating significant improvement over current approaches and establishing itself as one of the most effective solutions available today for image re-identification.",1
"Gait, the walking pattern of individuals, is one of the most important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as the gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and view angle. To remedy this issue, we propose a novel AutoEncoder framework to explicitly disentangle pose and appearance features from RGB imagery and the LSTM-based integration of pose features over time produces the gait feature. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF and FVG datasets, our method demonstrates superior performance to the state of the arts quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency.",0
"""Gait recognition refers to the task of identifying individuals based on their unique walking patterns. This paper proposes a new approach to gait recognition using disentangled representation learning, which involves separating out different aspects of human motion such as speed, acceleration, and joint angles. By doing so, we can learn more robust representations that capture important features of gait while minimizing noise from other factors. Our method achieves state-of-the-art performance on several benchmark datasets and demonstrates significant improvements over previous approaches.""",1
"Binaural audio provides a listener with 3D sound sensation, allowing a rich perceptual experience of the scene. However, binaural recordings are scarcely available and require nontrivial expertise and equipment to obtain. We propose to convert common monaural audio into binaural audio by leveraging video. The key idea is that visual frames reveal significant spatial cues that, while explicitly lacking in the accompanying single-channel audio, are strongly linked to it. Our multi-modal approach recovers this link from unlabeled video. We devise a deep convolutional neural network that learns to decode the monaural (single-channel) soundtrack into its binaural counterpart by injecting visual information about object and scene configurations. We call the resulting output 2.5D visual sound---the visual stream helps ""lift"" the flat single channel audio into spatialized sound. In addition to sound generation, we show the self-supervised representation learned by our network benefits audio-visual source separation. Our video results: http://vision.cs.utexas.edu/projects/2.5D_visual_sound/",0
"In this paper we explore the use of the visual component of auditory displays within sonification research through our work on developing a new method we call 2.5D Visual Sound (VS). VS combines elements from both visualizations and auditory displays by using dynamic sound as a medium for representing data. This allows us to present rich, detailed relationships within complex datasets that cannot be easily conveyed through traditional static images alone. We discuss key considerations for designing effective 2.5D VS representations such as how to encode and decode multivariate signals. Our work offers valuable insights into the potential benefits and limitations of incorporating auditory displays into the analysis and presentation of data across various domains including astronomy, biology, medicine, environmental science, and more. Finally, we provide examples of the effectiveness of 2.5D VS compared to other methods commonly used in scientific inquiry such as heat maps, line plots, scatter plots, and animations. Overall, our findings suggest that VS can effectively support analytical reasoning processes in exploratory data analysis tasks allowing users to gain unique insights and make better decisions based on those insights.",1
"Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.",0
"Deep generative models such as Generative Adversarial Networks (GANs) have been used successfully for image generation tasks, but suffer from stability issues during training and require careful hyperparameter tuning to obtain good results. In recent years, self-supervised learning techniques have emerged as a promising approach to address these problems by leveraging unlabeled data for better generalization performance. However, existing self-supervised methods often rely on pretext tasks that can be computationally expensive or may not always align well with the desired downstream task. To overcome these limitations, we propose a novel auxiliary rotation loss term that guides the generator network towards generating images with varying orientations at different scales. This auxiliary loss function provides additional guidance to improve both stability and fidelity of generated samples. We evaluate our method on several benchmark datasets and demonstrate state-of-the art performance in terms of visual quality and realism while offering fast convergence and improved robustness compared to prior works. Our work shows that incorporating simple yet effective rotation losses into deep generative networks trained under self-supervision leads to significant improvements in performance and ease of use for practical deployment.",1
"High-resolution representation learning plays an essential role in many vision problems, e.g., pose estimation and semantic segmentation. The high-resolution network (HRNet)~\cite{SunXLW19}, recently developed for human pose estimation, maintains high-resolution representations through the whole process by connecting high-to-low resolution convolutions in \emph{parallel} and produces strong high-resolution representations by repeatedly conducting fusions across parallel convolutions.   In this paper, we conduct a further study on high-resolution representations by introducing a simple yet effective modification and apply it to a wide range of vision tasks. We augment the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions rather than only the representation from the high-resolution convolution as done in~\cite{SunXLW19}. This simple modification leads to stronger representations, evidenced by superior results. We show top results in semantic segmentation on Cityscapes, LIP, and PASCAL Context, and facial landmark detection on AFLW, COFW, $300$W, and WFLW. In addition, we build a multi-level representation from the high-resolution representation and apply it to the Faster R-CNN object detection framework and the extended frameworks. The proposed approach achieves superior results to existing single-model networks on COCO object detection. The code and models have been publicly available at \url{https://github.com/HRNet}.",0
"This abstract presents research on developing high-resolution representations of pixel data that can be used for labeling and other image processing tasks. We propose using deep learning techniques to extract features from raw images, which are then processed into detailed annotations for specific regions of interest within each image. Our approach achieves state-of-the-art results on several benchmark datasets, demonstrating the effectiveness of our method for producing accurate and detailed labels at both the pixel level and larger scales. Overall, we believe that these findings have significant implications for advancing the field of computer vision and related applications such as autonomous vehicles, medical imaging, and object recognition systems.",1
"We integrate two powerful ideas, geometry and deep visual representation learning, into recurrent network architectures for mobile visual scene understanding. The proposed networks learn to ""lift"" and integrate 2D visual features over time into latent 3D feature maps of the scene. They are equipped with differentiable geometric operations, such as projection, unprojection, egomotion estimation and stabilization, in order to compute a geometrically-consistent mapping between the world scene and their 3D latent feature state. We train the proposed architectures to predict novel camera views given short frame sequences as input. Their predictions strongly generalize to scenes with a novel number of objects, appearances and configurations; they greatly outperform previous works that do not consider egomotion stabilization or a space-aware latent feature state. We train the proposed architectures to detect and segment objects in 3D using the latent 3D feature map as input--as opposed to per frame features. The resulting object detections persist over time: they continue to exist even when an object gets occluded or leaves the field of view. Our experiments suggest the proposed space-aware latent feature memory and egomotion-stabilized convolutions are essential architectural choices for spatial common sense to emerge in artificial embodied visual agents.",0
"In recent years, there has been growing interest in developing artificial intelligence (AI) systems that can learn spatial common sense knowledge from visual data. One major challenge in this area is how to represent and reason about geometric relationships between objects in scenes, which plays a crucial role in many high-level tasks such as object detection, scene understanding, and robotics. To address this issue, we propose a novel approach based on geometry-aware recurrent networks (GARN). GARN models both local image features and global layout using convolutional neural networks (CNNs), while simultaneously reasoning about geometric relationships by representing and updating a set of scene graphs over time through graph convolutional networks (GCNs). We evaluate our model on several benchmark datasets including VRSCenes and Matterport3D, demonstrating significant improvements over state-of-the-art methods in terms of accuracy and efficiency. Our work paves the way towards building more intelligent vision systems with better capabilities in spatial cognition and decision making.",1
"Spherical data is found in many applications. By modeling the discretized sphere as a graph, we can accommodate non-uniformly distributed, partial, and changing samplings. Moreover, graph convolutions are computationally more efficient than spherical convolutions. As equivariance is desired to exploit rotational symmetries, we discuss how to approach rotation equivariance using the graph neural network introduced in Defferrard et al. (2016). Experiments show good performance on rotation-invariant learning problems. Code and examples are available at https://github.com/SwissDataScienceCenter/DeepSphere",0
"""Deep learning techniques have become increasingly popular due to their ability to process large amounts of data and make accurate predictions on complex problems such as image classification and speech recognition. However, these models often struggle with handling structured geometric transformations, which can lead to performance degradation when applied to data that undergoes changes in scale, rotation, translation, etc. To address this issue, we propose a novel deep neural network architecture called DeepSphere, inspired by convolutional neural networks (CNNs) but tailored for processing multidimensional signals on the sphere. Our model is based on the concept of graphs, where nodes represent local features and edges capture spatial relationships among them. Incorporating equivariance into our design ensures that the network remains invariant to arbitrary rotations while maintaining good generalization capabilities across different datasets. Through extensive experiments on several benchmark tasks involving spherical data from fields like computer vision and astrophysics, we demonstrate that our method outperforms state-of-the-art alternatives in terms of accuracy, efficiency, robustness, and interpretability.""",1
"We address the problem of video representation learning without human-annotated labels. While previous efforts address the problem by designing novel self-supervised tasks using video data, the learned features are merely on a frame-by-frame basis, which are not applicable to many video analytic tasks where spatio-temporal features are prevailing. In this paper we propose a novel self-supervised approach to learn spatio-temporal features for video representation. Inspired by the success of two-stream approaches in video classification, we propose to learn visual features by regressing both motion and appearance statistics along spatial and temporal dimensions, given only the input video data. Specifically, we extract statistical concepts (fast-motion region and the corresponding dominant direction, spatio-temporal color diversity, dominant color, etc.) from simple patterns in both spatial and temporal domains. Unlike prior puzzles that are even hard for humans to solve, the proposed approach is consistent with human inherent visual habits and therefore easy to answer. We conduct extensive experiments with C3D to validate the effectiveness of our proposed approach. The experiments show that our approach can significantly improve the performance of C3D when applied to video classification tasks. Code is available at https://github.com/laura-wang/video_repres_mas.",0
"In recent years, self-supervised learning has emerged as a promising approach for learning spatio-temporal representations from videos. By leveraging motion and appearance statistics, these methods can learn meaningful video representations without requiring explicit supervision. However, existing approaches often rely on hand-engineered features that may limit their ability to capture complex spatio-temporal patterns in videos. To address this limitation, we propose a novel method for self-supervised representation learning from videos based on predicting motion and appearance statistics. Our framework consists of two main components: a feature encoder network that extracts video features, and a prediction head that estimates temporal correspondences, optical flows, and frame differences. By jointly optimizing all three tasks using a single loss function, our model learns powerful representations that encode both spatial structure and temporal dynamics. We evaluate our approach on multiple benchmark datasets and demonstrate state-of-the-art performance across various downstream tasks including action recognition, object tracking, and unsupervised domain adaptation. Overall, our work advances the field of self-supervised representation learning for videos and highlights the importance of incorporating motion and appearance statistics for improved performance.",1
"Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.",0
This could make things more difficult. But there were other ways out of this dilemma: we could use our knowledge to predict which facts would become relevant to solving problems and precompute these in advance; or we could rely on human expertise to tell us how hard tasks should be expected to be. These alternatives have their own drawbacks but at least they don’t involve changing the goalposts halfway through a program’s development.,1
"Temporal segmentation of long videos is an important problem, that has largely been tackled through supervised learning, often requiring large amounts of annotated training data. In this paper, we tackle the problem of self-supervised temporal segmentation of long videos that alleviate the need for any supervision. We introduce a self-supervised, predictive learning framework that draws inspiration from cognitive psychology to segment long, visually complex videos into individual, stable segments that share the same semantics. We also introduce a new adaptive learning paradigm that helps reduce the effect of catastrophic forgetting in recurrent neural networks. Extensive experiments on three publicly available datasets - Breakfast Actions, 50 Salads, and INRIA Instructional Videos datasets show the efficacy of the proposed approach. We show that the proposed approach is able to outperform weakly-supervised and other unsupervised learning approaches by up to 24% and have competitive performance compared to fully supervised approaches. We also show that the proposed approach is able to learn highly discriminative features that help improve action recognition when used in a representation learning paradigm.",0
"This should serve as a summary of sorts to give readers insight into your work. Don’t write about all details but keep focus on key findings or contributions. Use first person pronouns and address your main audience (fellow researchers). If you want include one sentence at max that explains why self supervised learning is beneficial in this context. Self-supervised event segmentation has recently gained popularity due to its ability to learn representations without requiring explicit labels. In our paper, we propose a new method called PercePTual prediCtion framework for Video segmentaTion (PPCV), which leverages perceptual features extracted from pretrained deep neural networks to predict future frames conditioned on past frames within videos. Our proposed approach effectively captures spatio-temporal dependencies present in video data and can be used for both action recognition and temporal boundary detection tasks without requiring any additional annotations. Additionally, we explore different strategies such as consistency regularization, model ensembling, and hyperparameter search methods to further improve performance. Finally, we evaluate PPCV against state-of-the-art models on two benchmark datasets: DAVIS 2016 and Youtube-VOS 2018. Our experiments showcase that our method achieves competitive results compared to fully supervised baselines while relying exclusively on self-supervision, highlighting the benefits of our proposal for low-label settings. Overall, our work presents a significant step towards developing more efficient and effective solutions for video understanding problems using only unlabeled data.",1
"Automatic eye gaze estimation has interested researchers for a while now. In this paper, we propose an unsupervised learning based method for estimating the eye gaze region. To train the proposed network ""Ize-Net"" in self-supervised manner, we collect a large `in the wild' dataset containing 1,54,251 images from the web. For the images in the database, we divide the gaze into three regions based on an automatic technique based on pupil-centers localization and then use a feature-based technique to determine the gaze region. The performance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning results of Ize-Net for the task of eye gaze estimation. The feature representation learned is also used to train traditional machine learning algorithms for eye gaze estimation. The results demonstrate that the proposed method learns a rich data representation, which can be efficiently fine-tuned for any eye gaze estimation dataset.",0
"This study presents a novel approach for unsupervised learning of eye gaze representation using web data. We propose a method that utilizes natural language processing techniques to extract textual descriptions of gaze direction from image captions on the internet. Our model then learns to map these descriptors onto corresponding gaze angles in order to generate accurate representations of human gaze direction. We evaluate our approach by comparing predicted gaze directions against ground truth annotations obtained through controlled experiments, demonstrating high accuracy across a variety of test cases. Overall, we show that our proposed approach provides a promising alternative to traditional supervised methods, offering a scalable and efficient solution for obtaining large quantities of annotated gaze data at low cost. Furthermore, our framework has potential applications beyond eye tracking research, as it could potentially enable automated analysis of visual attention in images and videos more broadly.",1
"Recently, data-driven deep saliency models have achieved high performance and have outperformed classical saliency models, as demonstrated by results on datasets such as the MIT300 and SALICON. Yet, there remains a large gap between the performance of these models and the inter-human baseline. Some outstanding questions include what have these models learned, how and where they fail, and how they can be improved. This article attempts to answer these questions by analyzing the representations learned by individual neurons located at the intermediate layers of deep saliency models. To this end, we follow the steps of existing deep saliency models, that is borrowing a pre-trained model of object recognition to encode the visual features and learning a decoder to infer the saliency. We consider two cases when the encoder is used as a fixed feature extractor and when it is fine-tuned, and compare the inner representations of the network. To study how the learned representations depend on the task, we fine-tune the same network using the same image set but for two different tasks: saliency prediction versus scene classification. Our analyses reveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are already encoded within various layers of the network pre-trained for object recognition, 2) using modern datasets, we find that fine-tuning pre-trained models for saliency prediction makes them favor some categories (e.g. head) over some others (e.g. text), 3) although deep models of saliency outperform classical models on natural images, the converse is true for synthetic stimuli (e.g. pop-out search arrays), an evidence of significant difference between human and data-driven saliency models, and 4) we confirm that, after-fine tuning, the change in inner-representations is mostly due to the task and not the domain shift in the data.",0
"Artificial intelligence (AI) has revolutionized numerous fields by enabling systems to learn from large amounts of data, recognize patterns in that data, make decisions based on those patterns, adapt to new situations, and communicate effectively. One such area where AI is making waves is computer vision (CV), which focuses on developing algorithms that enable computers to interpret visual data from the world, including images, videos, and live feeds from cameras. In CV, one key task is understanding and predicting human attention in scenes, as these can provide valuable insights into human behavior, preferences, and decision-making processes. To tackle this problem, researchers have developed deep saliency models, which aim to estimate the importance or ""saliency"" of different regions within an image or video frame, given some ground truth (e.g., user clicks). However, despite their popularity and reported effectiveness, there remains a lack of understanding regarding how these models work internally and why they perform well on certain tasks but poorly on others. This paper addresses that gap by providing novel visualization techniques that shed light on both the strengths and weaknesses of existing models. Through a suite of experiments conducted across various datasets, we demonstrate the unique characteristics and behaviors of each model, leading us to propose a taxonomy to better classify these methods and guide future development efforts. We believe our findings offer fresh insights into the state of deep visual saliency, inspiring further progress toward creating intelligent agents able to perceive humans at a level comparable to that of fellow humans themselves. As such, this study represents a significant step forward in the pursuit of advanced artificial intelligence and its applications across industries.",1
"The goal of knowledge representation learning is to embed entities and relations into a low-dimensional, continuous vector space. How to push a model to its limit and obtain better results is of great significance in knowledge graph's applications. We propose a simple and elegant method, Trans-DLR, whose main idea is dynamic learning rate control during training. Our method achieves remarkable improvement, compared with recent GAN-based method. Moreover, we introduce a new negative sampling trick which corrupts not only entities, but also relations, in different probabilities. We also develop an efficient way, which fully utilizes multiprocessing and parallel computing, to speed up evaluation of the model in link prediction tasks. Experiments show that our method is effective.",0
"This study compares two generative models, Generative Adversarial Networks (GAN) and Variational Autoencoder (VAE). While previous research has shown that GAN outperforms VAEs on image generation tasks, we found that VAE significantly outperformed GAN in learning knowledge representations. Our results suggest that simpler models can still produce competitive performance in complex data sets such as MNIST and CelebA. We attribute our findings to VAE’s ability to capture both high-level abstractions and low-level details compared to GAN’s emphasis on maximizing output quality. In addition, VAEs require fewer parameters to achieve comparable accuracy than their counterparts in GANs, making them more computationally efficient. Overall, our experiments demonstrate the effectiveness of VAEs over GANs in knowledge representation learning.",1
"We propose Deep Multiset Canonical Correlation Analysis (dMCCA) as an extension to representation learning using CCA when the underlying signal is observed across multiple (more than two) modalities. We use deep learning framework to learn non-linear transformations from different modalities to a shared subspace such that the representations maximize the ratio of between- and within-modality covariance of the observations. Unlike linear discriminant analysis, we do not need class information to learn these representations, and we show that this model can be trained for complex data using mini-batches. Using synthetic data experiments, we show that dMCCA can effectively recover the common signal across the different modalities corrupted by multiplicative and additive noise. We also analyze the sensitivity of our model to recover the correlated components with respect to mini-batch size and dimension of the embeddings. Performance evaluation on noisy handwritten datasets shows that our model outperforms other CCA-based approaches and is comparable to deep neural network models trained end-to-end on this dataset.",0
"Abstract: This paper presents a novel approach to multimodal representation learning that utilizes deep multiset canonical correlation (DCC) algorithms to learn representations that capture shared patterns across multiple data types such as images, text, audio, and video. By leveraging deep neural networks and kernel embedding techniques, our method enables efficient computation of high-dimensional similarity measures between sets of multimodal features and can effectively align them into a common coordinate system. Our experimental results on a range of real-world applications demonstrate the effectiveness of our approach in improving performance over state-of-the-art methods in tasks including image/video description generation, sentiment analysis, cross-modal retrieval, and question answering. Furthermore, we provide insights into how DCC captures different aspects of inter-modality alignment and showcase its robustness by comparing against several baseline models. Overall, our work advances the field of multimodal representation learning and opens up new possibilities for researchers to explore the intersection between deep learning and set theory.",1
"The problem of distributed representation learning is one in which multiple sources of information $X_1,\ldots,X_K$ are processed separately so as to learn as much information as possible about some ground truth $Y$. We investigate this problem from information-theoretic grounds, through a generalization of Tishby's centralized Information Bottleneck (IB) method to the distributed setting. Specifically, $K$ encoders, $K \geq 2$, compress their observations $X_1,\ldots,X_K$ separately in a manner such that, collectively, the produced representations preserve as much information as possible about $Y$. We study both discrete memoryless (DM) and memoryless vector Gaussian data models. For the discrete model, we establish a single-letter characterization of the optimal tradeoff between complexity (or rate) and relevance (or information) for a class of memoryless sources (the observations $X_1,\ldots,X_K$ being conditionally independent given $Y$). For the vector Gaussian model, we provide an explicit characterization of the optimal complexity-relevance tradeoff. Furthermore, we develop a variational bound on the complexity-relevance tradeoff which generalizes the evidence lower bound (ELBO) to the distributed setting. We also provide two algorithms that allow to compute this bound: i) a Blahut-Arimoto type iterative algorithm which enables to compute optimal complexity-relevance encoding mappings by iterating over a set of self-consistent equations, and ii) a variational inference type algorithm in which the encoding mappings are parametrized by neural networks and the bound approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on synthetic and real datasets are provided to support the efficiency of the approaches and algorithms developed in this paper.",0
"This paper presents a novel framework for distributed representation learning that enables efficient computation on decentralized devices without compromising model accuracy. The proposed approach relies on variational inference principles to learn distributed representations that capture complex relationships between inputs and outputs. By leveraging recent advances in deep learning and optimization techniques, our method achieves state-of-the-art results across multiple domains while remaining scalable and flexible. We evaluate the performance of our method using standard benchmark datasets and demonstrate its effectiveness through extensive experiments. Overall, this work represents an important step towards enabling distributed artificial intelligence systems to effectively process large amounts of data and solve real-world problems at scale.",1
"This paper generalizes the Maurer--Pontil framework of finite-dimensional lossy coding schemes to the setting where a high-dimensional random vector is mapped to an element of a compact set of latent representations in a lower-dimensional Euclidean space, and the reconstruction map belongs to a given class of nonlinear maps. Under this setup, which encompasses a broad class of unsupervised representation learning problems, we establish a connection to approximate generative modeling under structural constraints using the tools from the theory of optimal transportation. Next, we consider problem of learning a coding scheme on the basis of a finite collection of training samples and present generalization bounds that hold with high probability. We then illustrate the general theory in the setting where the reconstruction maps are implemented by deep neural nets.",0
"In recent years, there has been significant interest in developing coding schemes that can efficiently represent data while maintaining high fidelity. Finite-dimensional coding (FDC) schemes have emerged as a promising approach due to their ability to balance efficiency and accuracy, making them suitable for applications such as image compression, signal processing, and machine learning. However, designing FDC schemes with optimal performance remains challenging because they require careful consideration of both encoding and decoding procedures. This paper presents a new method for learning FDC schemes using deep neural networks trained on large datasets. By leveraging powerful optimization techniques and modern computational resources, we demonstrate that our approach outperforms existing methods and achieves state-of-the-art results across several benchmarks. We then extend our work by exploring the use of nonlinear reconstruction maps, which further enhance the quality of reconstructed signals. Our findings provide valuable insights into the capabilities and limitations of deep learning algorithms in the context of FDC scheme development, opening up opportunities for future research directions. Overall, this work represents a significant step forward towards efficient and effective coding systems for real-world applications.",1
"Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.",0
"One of the key challenges in representation learning is to effectively capture dependencies between different variables. In recent years, researchers have explored a variety of techniques to address this issue, including using adversarial training and mutual information maximization. However, these methods can be computationally expensive and may struggle to accurately model complex dependencies. To overcome these limitations, we propose the Wasserstein Dependency Measure (WDM) as a novel approach for representation learning.  Our method builds upon recent advances in the theory of optimal transport, which provides a powerful framework for studying metrics on probability distributions. We use the Earth Mover's Distance (EMD), a popular metric from optimal transport, to measure dependencies between two sets of data points. By minimizing EMD over all possible transformations between the two sets, our algorithm implicitly learns a nonlinear transformation that captures high-level relationships between the input features.  We evaluate WDM on several benchmark datasets and demonstrate its effectiveness in comparison to state-of-the-art approaches. Our results show that our method achieves improved performance across multiple tasks, including image generation, supervised classification, and dimensionality reduction. Furthermore, we provide detailed analyses of the learned representations, highlighting their interpretability and alignment with human priors.  Overall, our work offers a new perspective on representation learning that leverages the rich theoretical foundation of optimal transport. We hope that our approach will inspire further research into efficient and effective algorithms for discovering meaningful structure in complex data.",1
"Deep learning methods for person identification based on electroencephalographic (EEG) brain activity encounters the problem of exploiting the temporally correlated structures or recording session specific variability within EEG. Furthermore, recent methods have mostly trained and evaluated based on single session EEG data. We address this problem from an invariant representation learning perspective. We propose an adversarial inference approach to extend such deep learning models to learn session-invariant person-discriminative representations that can provide robustness in terms of longitudinal usability. Using adversarial learning within a deep convolutional network, we empirically assess and show improvements with our approach based on longitudinally collected EEG data for person identification from half-second EEG epochs.",0
"Deep learning has revolutionized biometric recognition by providing efficient solutions for extracting features from raw signals of various modalities such as face, iris, and fingerprints. However, there remains a challenging problem in designing deep architectures that perform well on noisy data encountered in electroencephalogram (EEG)-based applications, where classifiers often suffer significant degradation due to adversarial attacks. In recent years, researchers have applied adversarial training techniques with different objectives aimed at enhancing robustness against specific attacks. In our work, we evaluate state-of-the-art adversarial attack methods that can manipulate input stimuli to mislead trained models. We demonstrate how different attack strategies affect the performance of EEG biometric systems under unseen attacks and discuss implications for model improvement. Our results showcase a comprehensive evaluation framework for analyzing the effectiveness of adversarial deep learning approaches in addressing real-world security threats to brain authentication systems. By shedding light on open issues related to robustness, our study paves the way for advancing the field of secure brain biometry using cutting-edge machine learning paradigms.",1
"Recent advances in deep generative models have lead to remarkable progress in synthesizing high quality images. Following their successful application in image processing and representation learning, an important next step is to consider videos. Learning generative models of video is a much harder task, requiring a model to capture the temporal dynamics of a scene, in addition to the visual presentation of objects. While recent attempts at formulating generative models of video have had some success, current progress is hampered by (1) the lack of qualitative metrics that consider visual quality, temporal coherence, and diversity of samples, and (2) the wide gap between purely synthetic video data sets and challenging real-world data sets in terms of complexity. To this extent we propose Fr\'{e}chet Video Distance (FVD), a new metric for generative models of video, and StarCraft 2 Videos (SCV), a benchmark of game play from custom starcraft 2 scenarios that challenge the current capabilities of generative models of video. We contribute a large-scale human study, which confirms that FVD correlates well with qualitative human judgment of generated videos, and provide initial benchmark results on SCV.",0
"Abstract:  The ability of generative models like GANs (Generative Adversarial Networks) to generate high quality, realistic video has improved significantly over recent years, but there still exists a need for more accurate metrics that can effectively evaluate their performance. In this work, we propose a new metric based on structural similarity index measure (SSIM), which takes into account both luminance and chrominance components of the video frames. We show that our metric outperforms existing ones in terms of correlation with human judgment, making it well suited for evaluating the quality of generated videos. Additionally, we discuss several challenges faced by current state-of-the-art video generation methods, such as model instability, blurriness, and poor temporal coherence. By addressing these issues, we hope to pave the way towards even better generative models capable of producing highly convincing synthetic videos.",1
"Predicting human perceptual similarity is a challenging subject of ongoing research. The visual process underlying this aspect of human vision is thought to employ multiple different levels of visual analysis (shapes, objects, texture, layout, color, etc). In this paper, we postulate that the perception of image similarity is not an explicitly learned capability, but rather one that is a byproduct of learning others. This claim is supported by leveraging representations learned from a diverse set of visual tasks and using them jointly to predict perceptual similarity. This is done via simple feature concatenation, without any further learning. Nevertheless, experiments performed on the challenging Totally-Looks-Like (TLL) benchmark significantly surpass recent baselines, closing much of the reported gap towards prediction of human perceptual similarity. We provide an analysis of these results and discuss them in a broader context of emergent visual capabilities and their implications on the course of machine-vision research.",0
"Incorporate keywords such as perception, learning, similarity, tasks, deep neural networks, representations, natural images. Use at least one active voice sentence. If you use a figure, reference it directly using [Figure X]. This study investigates how high-level perceptual similarity can be enabled through learning diverse tasks using deep neural networks. We propose that learning multiple tasks simultaneously leads to better representations, which are more capable of capturing complex relationships within datasets, including human-judged similarities. Our results show that models trained on multiple tasks outperform single task baselines across a range of benchmark datasets, demonstrating the utility of our approach for both image classification and retrieval tasks. Furthermore, we observe that these improvements are most pronounced when the learned representation can capture similarity at multiple levels of abstraction. Finally, our analysis of activation patterns indicates that learned features from multi-task models are more disentangled than their single-task counterparts, enabling greater transferability to new tasks. These findings suggest that leveraging the power of diverse training data is key to building effective visual representations, advancing the state-of-the-art in computer vision applications. Figure X displays some examples of the benefits brought by jointly learning different types of objectives (classification, verification, and contrastive), where even simple linear classifiers achieve competitive accuracy compared to current SOTA approaches specialized for each specific objective.",1
"Current deep domain adaptation methods used in computer vision have mainly focused on learning discriminative and domain-invariant features across different domains. In this paper, we present a novel approach that bridges the domain gap by projecting the source and target domains into a common association space through an unsupervised ``cross-grafted representation stacking'' (CGRS) mechanism. Specifically, we construct variational auto-encoders (VAE) for the two domains, and form bidirectional associations by cross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial networks (GAN) are employed for label alignment (LA), mapping the target domain data to the known label space of the source domain. The overall adaptation process hence consists of three phases: feature representation learning by VAEs, association generation, and association label alignment by GANs. Experimental results demonstrate that our CGRS-LA approach outperforms the state-of-the-art on a number of unsupervised domain adaptation benchmarks.",0
"Title: ""Unsupervised domain adaptation using deep networks with cross-grafted stacks""  Abstract: In recent years, the performance of machine learning models has greatly improved due to advancements in deep neural network architectures. However, these models often struggle when faced with new datasets that differ significantly from those on which they were trained. This problem is known as the domain adaptation challenge, and several methods have been proposed to tackle it. One such method is unsupervised domain adaptation (UDA), which seeks to align feature distributions across source and target domains without accessing any labeled data from the latter. We propose a novel framework called ""cross-grafted stacks,"" where we add intermediate layers into each convolutional block of the model, forcing them to learn more discriminative features at different levels of abstraction. These grafted layers are adapted to the target domain via adversarial training while preserving their utility during inference. Our approach improves upon existing UDA techniques by exploiting hierarchical representations learned by deep networks. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method over state-of-the-art UDA approaches.",1
"The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years. In spite of the advance, clustering with representation learning has been limited to flat-level categories, which often involves cohesive clustering with a focus on instance relations. To overcome the limitations of flat clustering, we introduce hierarchically-clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. Compared with a few prior works, HCRL firstly attempts to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components. In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features. We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.",0
"In recent years, there has been increasing interest in developing deep learning models that can learn hierarchical representations of data. Such representations have proven to be effective in various tasks such as image classification, speech recognition, and natural language processing. One promising approach to achieving these hierarchies is through the use of clustered representation learning (CRL).  In CRL, the goal is to train neural networks to represent high-level concepts using multiple levels of abstraction, each represented by a different set of features. These features are learned by clustering similar input patterns together into groups, which allows the network to capture both local and global relationships within the data. By doing so, CRL enables the model to learn more generalizable and interpretable representations, making it easier for humans to understand and interpret the decision process of the model.  This paper presents a novel framework for CRL called Hierarchically Clustered Representation Learning (HCLR) that extends traditional CRL methods. HCLR builds upon previous work in several key ways: Firstly, we introduce a new algorithm for optimizing the clusters based on graph partitioning techniques commonly used in computer science. This improves the quality of the clusters obtained during training compared to standard approaches such as KMeans. Secondly, our method uses multiple layers of clusters, allowing it to discover hierarchical structures in complex datasets. Finally, we present experiments demonstrating that our method outperforms existing state-of-the-art CRL algorithms across a variety of benchmark datasets, including MNIST, CIFAR-10, and STL-10.  Our results show that HCLR consistently produces better representations than other CRL methods, leading to improved accuracy in downstream task performance. We believe that our research will inspire further development in the field of hierarchical representation learning and contribute towards building intelligent systems tha",1
"The success of deep neural networks often relies on a large amount of labeled examples, which can be difficult to obtain in many real scenarios. To address this challenge, unsupervised methods are strongly preferred for training neural networks without using any labeled data. In this paper, we present a novel paradigm of unsupervised representation learning by Auto-Encoding Transformation (AET) in contrast to the conventional Auto-Encoding Data (AED) approach. Given a randomly sampled transformation, AET seeks to predict it merely from the encoded features as accurately as possible at the output end. The idea is the following: as long as the unsupervised features successfully encode the essential information about the visual structures of original and transformed images, the transformation can be well predicted. We will show that this AET paradigm allows us to instantiate a large variety of transformations, from parameterized, to non-parameterized and GAN-induced ones. Our experiments show that AET greatly improves over existing unsupervised approaches, setting new state-of-the-art performances being greatly closer to the upper bounds by their fully supervised counterparts on CIFAR-10, ImageNet and Places datasets.",0
"This paper presents a new approach to unsupervised representation learning that involves auto-encoding transformations (AET) instead of auto-encoding data (AED). In traditional unsupervised representation learning using AED, the goal is to learn representations of raw input data without any supervision from labeled examples. However, AET takes a different approach by focusing on modeling the relationships between transformed versions of the input data. We demonstrate through experiments that AET can lead to more effective representation learning compared to AED. Our results show improved performance across a range of tasks including image reconstruction, classification, and clustering. By leveraging the power of deep neural networks and large datasets, AET has the potential to revolutionize unsupervised representation learning. Overall, we believe this work represents an important contribution to the field and offers a promising direction for future research.",1
"We exploit altered patterns in brain functional connectivity as features for automatic discriminative analysis of neuropsychiatric patients. Deep learning methods have been introduced to functional network classification only very recently for fMRI, and the proposed architectures essentially focused on a single type of connectivity measure. We propose a deep convolutional neural network (CNN) framework for classification of electroencephalogram (EEG)-derived brain connectome in schizophrenia (SZ). To capture complementary aspects of disrupted connectivity in SZ, we explore combination of various connectivity features consisting of time and frequency-domain metrics of effective connectivity based on vector autoregressive model and partial directed coherence, and complex network measures of network topology. We design a novel multi-domain connectome CNN (MDC-CNN) based on a parallel ensemble of 1D and 2D CNNs to integrate the features from various domains and dimensions using different fusion strategies. Hierarchical latent representations learned by the multiple convolutional layers from EEG connectivity reveal apparent group differences between SZ and healthy controls (HC). Results on a large resting-state EEG dataset show that the proposed CNNs significantly outperform traditional support vector machine classifiers. The MDC-CNN with combined connectivity features further improves performance over single-domain CNNs using individual features, achieving remarkable accuracy of $93.06\%$ with a decision-level fusion. The proposed MDC-CNN by integrating information from diverse brain connectivity descriptors is able to accurately discriminate SZ from HC. The new framework is potentially useful for developing diagnostic tools for SZ and other disorders.",0
"""Classification of brain connectivity networks using electroencephalography (EEG) signals has gained attention as a potential tool for assessing brain function in schizophrenia. In this study, we propose the use of a multi-domain connectome convolutional neural network (MCCNN) approach for classification of resting state EEG data from patients with schizophrenia (SCZ) and healthy controls (HC). We aimed to identify differences in global functional organization of SCZ compared to HC that can be captured by MCCNN analysis. Our results showed that the proposed method achieved high classification accuracy (94%) across different datasets. We observed alterations in connectivity patterns associated with hub regions involved in cognition and sensory processing, indicating significant changes in the functional architecture of the human connectome in SCZ. These findings have important implications for understanding the pathophysiology of schizophrenia and could lead to improved diagnostic methods.""",1
"Knowledge Transfer (KT) techniques tackle the problem of transferring the knowledge from a large and complex neural network into a smaller and faster one. However, existing KT methods are tailored towards classification tasks and they cannot be used efficiently for other representation learning tasks. In this paper a novel knowledge transfer technique, that is capable of training a student model that maintains the same amount of mutual information between the learned representation and a set of (possible unknown) labels as the teacher model, is proposed. Apart from outperforming existing KT techniques, the proposed method allows for overcoming several limitations of existing methods providing new insight into KT as well as novel KT applications, ranging from knowledge transfer from handcrafted feature extractors to {cross-modal} KT from the textual modality into the representation extracted from the visual modality of the data.",0
"In recent years, deep learning has shown remarkable successes in many domains such as computer vision, natural language processing (NLP), speech recognition and robotics. One common challenge faced by all these applications is the limited availability of high quality labeled data, which makes it difficult to train accurate models. To address this issue, we propose a novel framework that leverages probabilistic knowledge transfer (PKT) from a source domain to a target domain. Our approach exploits prior knowledge of similarities between different tasks by modeling the distributions over shared parameters across tasks. We demonstrate the effectiveness of our method on several NLP benchmarks including sentiment analysis, part-of-speech tagging and named entity recognition. Experimental results show significant improvements compared to state-of-the-art baselines. Our work opens up new opportunities for semi-supervised learning by incorporating richer semantic information from related tasks into deep learning algorithms.",1
"Unlike conventional frame-based sensors, event-based visual sensors output information through spikes at a high temporal resolution. By only encoding changes in pixel intensity, they showcase a low-power consuming, low-latency approach to visual information sensing. To use this information for higher sensory tasks like object recognition and tracking, an essential simplification step is the extraction and learning of features. An ideal feature descriptor must be robust to changes involving (i) local transformations and (ii) re-appearances of a local event pattern. To that end, we propose a novel spatiotemporal feature representation learning algorithm based on slow feature analysis (SFA). Using SFA, smoothly changing linear projections are learnt which are robust to local visual transformations. In order to determine if the features can learn to be invariant to various visual transformations, feature point tracking tasks are used for evaluation. Extensive experiments across two datasets demonstrate the adaptability of the spatiotemporal feature learner to translation, scaling and rotational transformations of the feature points. More importantly, we find that the obtained feature representations are able to exploit the high temporal resolution of such event-based cameras in generating better feature tracks.",0
"This abstract presents a novel method for learning spatiotemporal features from event-based vision data. Unlike traditional frame-based cameras that capture images at fixed intervals, event-based sensors generate asynchronous signals whenever there is a change in brightness within their field of view. These events provide more precise temporal information but lack spatial resolution compared to frames. To address this limitation, we propose a deep neural network architecture called TensorTrailNet which extracts high quality visual representations by efficiently processing both space and time information. Our approach builds on previous work on event-based feature extraction techniques that operate directly on raw sensor signals, eliminating the need for preprocessing steps such as thresholding and downsampling often required in existing methods. We show that our model outperforms state-of-the-art methods across several benchmark datasets, demonstrating its effectiveness in representing complex visual content using low-resolution input data. By combining efficient hardware designs with powerful machine learning algorithms, TensorTrailNet opens up new possibilities for designing compact, energy-efficient, and high-performing computer vision systems.",1
"We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",0
"This paper presents AI2-THOR, an interactive 3D environment designed specifically for visual artificial intelligence research. Traditionally, data collection, algorithm training, model evaluation, and general experimentation have required heavy programming skills that can be barriers for users without strong technical backgrounds. However, by leveraging the latest graphics engine technology and integrating user-friendly interfaces, our framework allows both artists and developers alike to easily create environments tailored to their specific needs while providing valuable tools such as physics simulation and advanced lighting systems. To demonstrate its effectiveness, we present three case studies showcasing how different groups utilized THOR to perform experiments at varying levels of complexity. From evaluating algorithms on standard benchmark datasets to conducting multi-user collaborations via virtual reality headsets, each study highlights unique advantages afforded through our platform’s intuitive design, including improved efficiency, enhanced collaboration opportunities, and even unanticipated insights resulting from newfound creative freedom. Ultimately, by democratizing access to powerful tools like AI and VR for non-experts, we aim to inspire future innovations across multiple domains.",1
"Multiple kernel learning (MKL) algorithms combine different base kernels to obtain a more efficient representation in the feature space. Focusing on discriminative tasks, MKL has been used successfully for feature selection and finding the significant modalities of the data. In such applications, each base kernel represents one dimension of the data or is derived from one specific descriptor. Therefore, MKL finds an optimal weighting scheme for the given kernels to increase the classification accuracy. Nevertheless, the majority of the works in this area focus on only binary classification problems or aim for linear separation of the classes in the kernel space, which are not realistic assumptions for many real-world problems. In this paper, we propose a novel multi-class MKL framework which improves the state-of-the-art by enhancing the local separation of the classes in the feature space. Besides, by using a sparsity term, our large-margin multiple kernel algorithm (LMMK) performs discriminative feature selection by aiming to employ a small subset of the base kernels. Based on our empirical evaluations on different real-world datasets, LMMK provides a competitive classification accuracy compared with the state-of-the-art algorithms in MKL. Additionally, it learns a sparse set of non-zero kernel weights which leads to a more interpretable feature selection and representation learning.",0
"This paper proposes large-margin multiple kernel learning (MMKL) as a methodology for both feature selection and representation learning. By simultaneously optimizing for margin maximization and kernel alignment, MMKL offers several benefits over traditional methods such as singular value decomposition or linear discriminant analysis. Firstly, it allows us to efficiently learn a more compact set of representative features by exploiting complementary information across different kernels. Secondly, it produces more discriminative representations that better separate data points belonging to distinct classes. Thirdly, our experiments on benchmark datasets show significant improvements over state-of-the-art techniques while maintaining competitive computational efficiency. We believe MMKL can serve as a promising foundation for future research into effective representation learning. Our code has been made publicly available at [insert URL].",1
"Offline Signature Verification (OSV) is a challenging pattern recognition task, especially in presence of skilled forgeries that are not available during training. This study aims to tackle its challenges and meet the substantial need for generalization for OSV by examining different loss functions for Convolutional Neural Network (CNN). We adopt our new approach to OSV by asking two questions: 1. which classification loss provides more generalization for feature learning in OSV? , and 2. How integration of different losses into a unified multi-loss function lead to an improved learning framework? These questions are studied based on analysis of three loss functions, including cross entropy, Cauchy-Schwarz divergence, and hinge loss. According to complementary features of these losses, we combine them into a dynamic multi-loss function and propose a novel ensemble framework for simultaneous use of them in CNN. Our proposed Multi-Loss Snapshot Ensemble (MLSE) consists of several sequential trials. In each trial, a dominant loss function is selected from the multi-loss set, and the remaining losses act as a regularizer. Different trials learn diverse representations for each input based on signature identification task. This multi-representation set is then employed for the verification task. An ensemble of SVMs is trained on these representations, and their decisions are finally combined according to the selection of most generalizable SVM for each user. We conducted two sets of experiments based on two different protocols of OSV, i.e., writer-dependent and writer-independent on three signature datasets: GPDS-Synthetic, MCYT, and UT-SIG. Based on the writer-dependent OSV protocol, we achieved substantial improvements over the best EERs in the literature. The results of the second set of experiments also confirmed the robustness to the arrival of new users enrolled in the OSV system.",0
"This paper proposes a novel approach to offline signature verification that utilizes multi-representational learning (MRL) techniques. The proposed method uses multiple loss functions to train ensembles of convolutional neural networks (CNNs), which are then combined into a single model through an MRL framework. Experimental results demonstrate that our method outperforms state-of-the-art methods on several benchmark datasets, achieving improved accuracy and robustness against attacks. Our approach represents a significant advancement in the field of biometric authentication, as it provides a more comprehensive representation of signatures by leveraging both global and local features learned from different loss function snapshots. Overall, this work has important implications for secure authentication applications such as banking transactions and identity verifications.",1
"Dynamic scenes that contain both object motion and egomotion are a challenge for monocular visual odometry (VO). Another issue with monocular VO is the scale ambiguity, i.e. these methods cannot estimate scene depth and camera motion in real scale. Here, we propose a learning based approach to predict camera motion parameters directly from optic flow, by marginalizing depthmap variations and outliers. This is achieved by learning a sparse overcomplete basis set of egomotion in an autoencoder network, which is able to eliminate irrelevant components of optic flow for the task of camera parameter or motionfield estimation. The model is trained using a sparsity regularizer and a supervised egomotion loss, and achieves the state-of-the-art performances on trajectory prediction and camera rotation prediction tasks on KITTI and Virtual KITTI datasets, respectively. The sparse latent space egomotion representation learned by the model is robust and requires only 5% of the hidden layer neurons to maintain the best trajectory prediction accuracy on KITTI dataset. Additionally, in presence of depth information, the proposed method demonstrates faithful object velocity prediction for wide range of object sizes and speeds by global compensation of predicted egomotion and a divisive normalization procedure.",0
"In this work we propose a novel framework that can accurately estimate both object motion as well as ego-motion from sparse representations of dynamic scenes. By leveraging recent advances in compressed sensing theory we demonstrate how to reconstruct highly accurate estimates of scene flow, camera motion and moving objects using only very few measurements. Experiments on challenging real world sequences show the effectiveness of our approach which significantly outperforms existing state-of-the art methods while having low computational complexity. Keywords: Compressed sensing; Scene Flow; Camera Motion Estimation; Moving Object Detection -----  Object and ego-motion estimation in dynamic scenes is a fundamental task in computer vision that has numerous applications in robotics, autonomous driving, and video surveillance among others. In this work we present a new framework that utilizes sparse representation techniques based on compressed sensing theory to accurately recover both object motion and ego-motion from few observations. Our method addresses two important problems in scene reconstruction: camera motion estimation (CME) and moving object detection (MOD).  Our approach uses the notion of sparsity prior on 4D feature maps which represent pixel intensities over space and time. This prior assumption allows us to effectively solve underconstrained optimization problems associated with CME and MOD by taking advantage of additional structural constraints arising from the physical nature of camera motions and object movements. As a result, even though we may observe only few measurements at any given point in time, we can still achieve very high accuracy in estimating scene flows and camera/object motions. To accomplish this goal, we have developed a novel algorithmic pipeline that integrates advanced numerical optimization techniques wi",1
"3D multi object generative models allow us to synthesize a large range of novel 3D multi object scenes and also identify objects, shapes, layouts and their positions. But multi object scenes are difficult to create because of the dataset being multimodal in nature. The conventional 3D generative adversarial models are not efficient in generating multi object scenes, they usually tend to generate either one object or generate fuzzy results of multiple objects. Auto-encoder models have much scope in feature extraction and representation learning using the unsupervised paradigm in probabilistic spaces. We try to make use of this property in our proposed model. In this paper we propose a novel architecture using 3DConvNets trained with the progressive training paradigm that has been able to generate realistic high resolution 3D scenes of rooms, bedrooms, offices etc. with various pieces of furniture and objects. We make use of the adversarial auto-encoder along with the WGAN-GP loss parameter in our discriminator loss function. Finally this new approach to multi object scene generation has also been able to generate more number of objects per scene.",0
"Abstract: In recent years, progressive generative adversarial networks (ProGAN) have become increasingly popular for generating high quality images. However, these models often struggle with accurately representing objects and scenes with multiple instances present. This work proposes a novel method called auto-encoding progressive generative adversarial networks (AutoPix), which uses an iterative approach inspired by autoencoders to generate more accurate representations of multi object scenes. Our results show that AutoPix significantly outperforms ProGAN on several metrics such as FID score, perceptual loss and structural similarity index measure. We demonstrate our model’s ability to accurately represent complex scene elements like reflections, textures, shadows and lighting conditions that are commonly missing from current GAN based methods. Overall, we believe AutoPix has significant potential for applications in computer vision and graphics fields where realistic image synthesis is important.",1
"There is a long history of using meta learning as representation learning, specifically for determining the relevance of inputs. In this paper, we examine an instance of meta-learning in which feature relevance is learned by adapting step size parameters of stochastic gradient descent---building on a variety of prior work in stochastic approximation, machine learning, and artificial neural networks. In particular, we focus on stochastic meta-descent introduced in the Incremental Delta-Bar-Delta (IDBD) algorithm for setting individual step sizes for each feature of a linear function approximator. Using IDBD, a feature with large or small step sizes will have a large or small impact on generalization from training examples. As a main contribution of this work, we extend IDBD to temporal-difference (TD) learning---a form of learning which is effective in sequential, non i.i.d. problems. We derive a variety of IDBD generalizations for TD learning, demonstrating that they are able to distinguish which features are relevant and which are not. We demonstrate that TD IDBD is effective at learning feature relevance in both an idealized gridworld and a real-world robotic prediction task.",0
"This paper proposes a new approach to learning feature relevance in temporal-difference (TD) learning by adapting step sizes based on the magnitude of features in each state. In traditional TD learning, features that have large impacts on rewards can cause overshooting in learned values, while irrelevant features can lead to slower convergence. Our method addresses these issues by modifying the size of the TD update based on the importance of each feature. Experimental results demonstrate significant improvements compared to standard TD methods across multiple domains, including reinforcement learning benchmark tasks and real-world robotics applications. Furthermore, our approach offers advantages such as computational efficiency, scalability, and robustness to noisy observations. Overall, we believe that our proposed method represents a promising advance in understanding how to learn from experience efficiently, even under challenging conditions.",1
We develop hierarchically quantized efficient embedding representations for similarity-based search and show that this representation provides not only the state of the art performance on the search accuracy but also provides several orders of speed up during inference. The idea is to hierarchically quantize the representation so that the quantization granularity is greatly increased while maintaining the accuracy and keeping the computational complexity low. We also show that the problem of finding the optimal sparse compound hash code respecting the hierarchical structure can be optimized in polynomial time via minimum cost flow in an equivalent flow network. This allows us to train the method end-to-end in a mini-batch stochastic gradient descent setting. Our experiments on Cifar100 and ImageNet datasets show the state of the art search accuracy while providing several orders of magnitude search speedup respectively over exhaustive linear search over the dataset.,0
"Abstract: In recent years, representation learning has become increasingly important for achieving state-of-the-art performance on many natural language processing tasks such as text classification, question answering, and machine translation. However, traditional unsupervised pretraining methods often suffer from high computational cost, lack of interpretability, and limited control over the learned representations. This paper proposes an end-to-end efficient approach that uses cascading combinatorial optimization to jointly optimize multiple objectives within the pretraining process. By incorporating a variety of objective functions, including masked word prediction, next sentence prediction, and cloze task, our method learns robust and informative latent representations while reducing computational costs compared to previous approaches. Our experiments demonstrate consistent improvement across several NLP benchmark datasets, validating the effectiveness and efficiency of our proposed framework.",1
"This paper introduces a novel measure-theoretic theory for machine learning that does not require statistical assumptions. Based on this theory, a new regularization method in deep learning is derived and shown to outperform previous methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposed theory provides a theoretical basis for a family of practically successful regularization methods in deep learning. We discuss several consequences of our results on one-shot learning, representation learning, deep learning, and curriculum learning. Unlike statistical learning theory, the proposed learning theory analyzes each problem instance individually via measure theory, rather than a set of problem instances via statistics. As a result, it provides different types of results and insights when compared to statistical learning theory.",0
"In recent years, machine learning has become increasingly important for many fields due to its ability to quickly analyze large amounts of data and make predictions based on that analysis. However, one challenge faced by machine learning algorithms is their tendency towards overfitting, which occurs when the model becomes too specialized to the training data and struggles to generalize well to new, unseen examples. To address this issue, researchers have turned to analytical learning theory as a tool for understanding how machine learning models can learn more generally applicable concepts from data. This paper presents several different methods for controlling complexity of learned functions and ensuring strong theoretical guarantees, including regularization techniques such as early stopping, dropout, and Lasso regression. These methods offer significant improvements in terms of both accuracy and robustness compared to standard models, demonstrating the potential value of using analytical learning theory in machine learning applications. Overall, this work provides valuable insights into the design of future learning systems that can reliably handle complex real-world problems while minimizing the risk of overfitting.",1
"Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.",0
"This paper presents a new approach for semi-supervised deep kernel learning regression that leverages unlabeled data to minimize predictive variance. By incorporating both labeled and unlabeled data, we can improve the generalization performance of traditional supervised learning methods. Our method uses a novel regularizer which encourages smoothness in the predicted function over unseen input space, resulting in better robustness to outliers and improved accuracy. We demonstrate through experiments on several real world datasets that our proposed algorithm outperforms existing state-of-the-art algorithms across all metrics. This research has important implications for fields where obtaining large amounts of labeled data is expensive or difficult, such as medical imaging and computer vision.",1
"In this paper, we present a novel strategy to design disentangled 3D face shape representation. Specifically, a given 3D face shape is decomposed into identity part and expression part, which are both encoded and decoded in a nonlinear way. To solve this problem, we propose an attribute decomposition framework for 3D face mesh. To better represent face shapes which are usually nonlinear deformed between each other, the face shapes are represented by a vertex based deformation representation rather than Euclidean coordinates. The experimental results demonstrate that our method has better performance than existing methods on decomposing the identity and expression parts. Moreover, more natural expression transfer results can be achieved with our method than existing methods.",0
"Abstract: This abstract describes a method for learning disentangled representations of 3D face shape data that captures both geometry and texture variations. The approach uses convolutional neural networks (CNNs) combined with generative adversarial networks (GANs). Experiments show improved performance over previous methods on tasks such as pose estimation, shape reconstruction, and view synthesis. Additionally, the method is able to learn representations that can transfer across domains. Furthermore, this work provides insights into how different facial features contribute to overall shape variation.",1
"Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on 6-month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available at https://www.github.com/josedolz/HyperDenseNet.",0
"Title: Improving Multi-Modal Image Segmentation With Hyper-Densely Connected Neural Networks Abstract: This paper presents a novel approach to improving image segmentation using convolutional neural networks (CNN). We introduce a new architecture called HyperDense-Net that uses dense connections within each layer, which helps improve the flow of information through the network and allows the model to capture more contextual relationships between pixels. Our method outperforms previous state-of-the-art models on several benchmark datasets and demonstrates improved accuracy in both individual modalities as well as in fused multimodal images. Additionally, we show how our proposed method can effectively handle missing data and large variations in input sizes. Overall, these results highlight the effectiveness of HyperDense-Nets for image segmentation tasks in different domains and their potential to drive advancements in medical imaging, remote sensing, and other areas where accurate object boundaries are crucial.",1
"An increasing number of datasets contain multiple views, such as video, sound and automatic captions. A basic challenge in representation learning is how to leverage multiple views to learn better representations. This is further complicated by the existence of a latent alignment between views, such as between speech and its transcription, and by the multitude of choices for the learning objective. We explore an advanced, correlation-based representation learning method on a 4-way parallel, multimodal dataset, and assess the quality of the learned representations on retrieval-based tasks. We show that the proposed approach produces rich representations that capture most of the information shared across views. Our best models for speech and textual modalities achieve retrieval rates from 70.7% to 96.9% on open-domain, user-generated instructional videos. This shows it is possible to learn reliable representations across disparate, unaligned and noisy modalities, and encourages using the proposed approach on larger datasets.",0
"This paper presents a novel approach to learning from multimodal videos by leveraging correlations across multiple views of visual content. Our method models these relationships in a structured manner using graphical representations, enabling us to capture complex interactions among different aspects of video data such as audio, text, and image features. We demonstrate that our model significantly improves performance on a variety of tasks related to open-domain video analysis, including action recognition, event detection, and sentiment prediction, outperforming traditional methods relying solely on individual modalities. Furthermore, we show that the learned relationships provide valuable insights into how different facets of video data are interconnected and can drive future research in multimedia understanding.",1
"Exploiting multi-scale representations is critical to improve edge detection for objects at different scales. To extract edges at dramatically different scales, we propose a Bi-Directional Cascade Network (BDCN) structure, where an individual layer is supervised by labeled edges at its specific scale, rather than directly applying the same supervision to all CNN outputs. Furthermore, to enrich multi-scale representations learned by BDCN, we introduce a Scale Enhancement Module (SEM) which utilizes dilated convolution to generate multi-scale features, instead of using deeper CNNs or explicitly fusing multi-scale edge maps. These new approaches encourage the learning of multi-scale representations in different layers and detect edges that are well delineated by their scales. Learning scale dedicated layers also results in compact network with a fraction of parameters. We evaluate our method on three datasets, i.e., BSDS500, NYUDv2, and Multicue, and achieve ODS Fmeasure of 0.828, 1.3% higher than current state-of-the art on BSDS500. The code has been available at https://github.com/pkuCactus/BDCN.",0
"This research proposes a new deep learning architecture called ""Bi-Directional Cascade Network"" (BCN) designed specifically for edge detection tasks. BCN extends traditional cascading methods by incorporating bi-directionality into the architecture. By introducing skip connections that propagate errors backward through the network, we reduce error accumulation during filtering operations, leading to improved edge accuracy even under severe noise conditions. Our experiments show the superiority of our method over state-of-the-art competitors on benchmark datasets including CCV6478, COCO2017, and Pascal VOC2012. Further ablation studies demonstrate the importance of design choices and components contributing to our system's success. We believe that our method could significantly benefit real-world applications such as image compression, object recognition, and autonomous vehicles.",1
"Training recurrent neural networks (RNNs) with backpropagation through time (BPTT) has known drawbacks such as being difficult to capture longterm dependencies in sequences. Successful alternatives to BPTT have not yet been discovered. Recently, BP with synthetic gradients by a decoupled neural interface module has been proposed to replace BPTT for training RNNs. On the other hand, it has been shown that the representations learned with synthetic and real gradients are different though they are functionally identical. In this project, we explore ways of combining synthetic and real gradients with application to neural language modeling tasks. Empirically, we demonstrate the effectiveness of alternating training with synthetic and real gradients after periodic warm restarts on language modeling tasks.",0
"In recent years, neural language models have achieved great success in natural language processing tasks such as text generation, question answering, and translation. One challenge that has persisted in these models is their reliance on synthetic gradients during training, which can lead to suboptimal solutions and poor generalization performance. To address this issue, we propose a new method called alternating synthetic and real gradients (ASRG) for neural language modeling. Our approach involves interleaving periods of training with synthetic gradients, where the gradient computation is simplified to reduce computational cost, with periods of training using exact real gradients, where the full backpropagation algorithm is applied. We show through experiments on several benchmark datasets that our proposed ASRG method leads to significant improvements over state-of-the-art methods in terms of accuracy, speed, and stability. Additionally, we demonstrate the effectiveness of our method by applying it to two popular language models: GPT-2 and BART. Overall, our results highlight the potential of ASRG for improving the performance of neural language models and inspire further research in this direction.",1
"A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.",0
"Unsupervised representation learning has become increasingly popular due to its ability to learn meaningful representations from large amounts of data without explicit supervision. However, designing efficient algorithms that can effectively utilize unlabeled data remains a challenging problem. In this work, we propose meta-learning update rules for unsupervised representation learning that allow models to quickly adapt to new tasks by leveraging past experience. Our approach combines meta-learned initialization with continuous adaptation during training, enabling more effective use of limited compute resources and improved generalization performance on held-out test sets. We evaluate our method on several benchmark datasets across multiple domains and demonstrate consistent improvements over strong baselines. These results highlight the effectiveness of meta-learning as a tool for improving unsupervised representation learning and suggest promising directions for future research in this area.",1
"We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, functionally, towards a chosen family of (local) witnesses. The estimation problem is setup as a co-operative game between an unrestricted predictor such as a neural network, and a set of witnesses chosen from the desired transparent family. The goal of the witnesses is to highlight, locally, how well the predictor conforms to the chosen family of functions, while the predictor is trained to minimize the highlighted discrepancy. We emphasize that the predictor remains globally powerful as it is only encouraged to agree locally with locally adapted witnesses. We analyze the effect of the proposed approach, provide example formulations in the context of deep graph and sequence models, and empirically illustrate the idea in chemical property prediction, temporal modeling, and molecule representation learning.",0
"In order to promote accountability on social media platforms like Facebook, there must be functional transparency so that users can see how their personal data is being used by the platform and third parties who access it. This includes both technical functionality and human-readable explanations of the data processing involved in decision-making processes such as content moderation, ad targeting, and user profiling. By providing clear information to users, we hope to encourage platforms to act more responsibly and ethically with our data while allowing them to continue using our data to provide valuable services.",1
"Short-term road traffic prediction (STTP) is one of the most important modules in Intelligent Transportation Systems (ITS). However, network-level STTP still remains challenging due to the difficulties both in modeling the diverse traffic patterns and tacking high-dimensional time series with low latency. Therefore, a framework combining with a deep clustering (DeepCluster) module is developed for STTP at largescale networks in this paper. The DeepCluster module is proposed to supervise the representation learning in a visualized way from the large unlabeled dataset. More specifically, to fully exploit the traffic periodicity, the raw series is first split into a number of sub-series for triplets generation. The convolutional neural networks (CNNs) with triplet loss are utilized to extract the features of shape by transferring the series into visual images. The shape-based representations are then used for road segments clustering. Thereafter, motivated by the fact that the road segments in a group have similar patterns, a model sharing strategy is further proposed to build recurrent NNs (RNNs)-based predictions through a group-based model (GM), instead of individual-based model (IM) in which one model are built for one road exclusively. Our framework can not only significantly reduce the number of models and cost, but also increase the number of training data and the diversity of samples. In the end, we evaluate the proposed framework over the network of Liuli Bridge in Beijing. Experimental results show that the DeepCluster can effectively cluster the road segments and GM can achieve comparable performance against the IM with less number of models.",0
"This paper presents an approach to short-term road traffic prediction using deep clustering techniques applied on large-scale network data. By analyzing patterns in aggregated traffic flow data from sensors across urban transportation networks, our method identifies clusters that represent distinct characteristics such as time periods and weather conditions. These clusters are then used as input features to train recurrent neural network models for predictive modeling. We evaluate the performance of our proposed approach on two real-world datasets and demonstrate its effectiveness in accurately forecasting short-term changes in traffic patterns under different scenarios. Our results show significant improvement over state-of-the-art methods and suggest promising applications for intelligent transportation systems. Overall, this study contributes to advancing the understanding of how cluster analysis can improve the accuracy of machine learning models for complex traffic prediction problems.",1
"Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically ""similar"" data points and ""negative samples,"" the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.",0
"In recent years, unsupervised representation learning has emerged as a promising approach to obtaining high quality representations without any explicit supervision. One popular method within this field is contrastive unsupervised representation learning which involves training neural networks by encouraging them to encode semantic similarity into their representations. This study aimsto provide a theoretical analysis of contrastive unsupervised representation learning, identifying key principles that govern its behavior and explaining why it works so well. By analyzing several different models and architectures through extensive experimentation, we demonstrate how these core concepts impact performance across multiple tasks and datasets. Our results shed light on some of the most important factors that influence model design, including data augmentation, batch construction, hyperparameter selection, and architectural choices. Overall, our findings offer valuable insights for researchers working on developing new methods for unsupervised representation learning.",1
"This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at \url{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch}.",0
"This paper presents a deep learning approach to high-resolution human pose estimation that utilizes representation learning to improve accuracy and robustness. We propose a new network architecture that allows us to learn features from large amounts of data in a hierarchical manner. Our method first learns low-level representations and then builds upon these to create more complex features at higher levels of abstraction. By doing so, we can capture both local and global contexts, leading to improved performance on challenging tasks such as estimating body poses from images. We evaluate our approach on two benchmark datasets and demonstrate state-of-the-art results across a range of metrics. In addition, we conduct ablation studies to investigate the impact of different components of our model and provide insights into how each contributes to overall performance. Overall, our work shows that representation learning can be effectively applied to human pose estimation, enabling more accurate and efficient solutions for this important task.",1
"Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training. To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training. Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of ""Information Plasticity"". Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.",0
"Recent advances in deep learning have led to impressive results across a wide range of domains, from image recognition to natural language processing. However, understanding how these models learn and make decisions remains a challenge. In particular, little is known about critical periods in the training process that drive performance improvements. This study seeks to address this gap by examining patterns in gradient descent updates during training and identifying key factors that influence learning dynamics. Our findings reveal several important insights into deep neural networks, including periods of rapid improvement followed by slow convergence, as well as windows where certain layers are more sensitive than others. These discoveries offer valuable new perspectives on model behavior, with implications for future research in deep learning. Overall, our work contributes to a better understanding of critical learning periods in complex artificial intelligence systems, paving the way for improved designs and optimized deployment in real-world applications.",1
"In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-the-art methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods.",0
"In recent years, there has been growing interest in using hypergraphs as a means to model complex data sets with multiple interrelated features. One promising approach to learning from these hypergraphs involves training neural networks that can effectively capture their unique structure and relationships between entities. In our paper, we propose a new architecture for hypergraph neural networks (HNN) that leverages advanced techniques such as attention mechanisms and graph convolutional layers to better represent the rich interactions within each hyperedge. Our experiments on several benchmark datasets demonstrate the effectiveness of HNNs compared to traditional methods for hypergraph analysis, achieving state-of-the-art results across a range of tasks including node classification and link prediction. Overall, our work provides a significant step forward towards developing more powerful machine learning models capable of handling complex relational data types like hypergraphs.",1
"Neural networks designed for the task of classification have become a commodity in recent years. Many works target the development of more effective networks, which results in a complexification of their architectures with more layers, multiple sub-networks, or even the combination of multiple classifiers, but this often comes at the expense of producing uninterpretable black boxes. In this paper, we redesign a simple capsule network to enable it to synthesize class-representative samples, called prototypes, by replacing the last layer with a novel Hit-or-Miss layer. This layer contains activated vectors, called capsules, that we train to hit or miss a fixed target capsule by tailoring a specific centripetal loss function. This possibility allows to develop a data augmentation step combining information from the data space and the feature space, resulting in a hybrid data augmentation process. We show that our network, named HitNet, is able to reach better performances than those reproduced with the initial CapsNet on several datasets, while allowing to visualize the nature of the features extracted as deformations of the prototypes, which provides a direct insight into the feature representation learned by the network .",0
"In this paper, we propose a novel approach that utilizes hit-or-miss convolutions in combination with learned prototypes deformation. Our method provides effective feature interpretability by representing object parts via dense correspondences generated from segmented images using offline predefined keypoints maps. We demonstrate improved results over state-of-the-art approaches on standard benchmark datasets while providing better visualization capabilities of our feature representation. By incorporating online optimization of the prototype shapes during inference, we enable robustness against local ambiguities such as cluttered scenes. Finally, we provide extensive analysis of our proposal with ablation studies illustrating the effectiveness of each component in improving both performance and interpretability. Overall, our proposed framework paves the way towards more accurate yet interpretable deep learning models for computer vision tasks. This work represents a significant step forward in understanding how complex object representations can be simplified and made comprehensible without sacrificing accuracy.",1
"Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",0
"Graph neural networks (GNN) have emerged as powerful tools that process graph data efficiently. They rely on convolutional operations over graphs’ edges and vertices and can capture both local and nonlocal dependencies. In recent years, they were applied successfully across many domains from computer vision problems to natural language processing tasks. This research takes GNN one step further by introducing Transformer-based architectures enabling parallel computation across different substructures within the input graph. Our extensive experimental analysis compares the performance of different variants of the proposed architecture against state-of-the-art methods on several datasets. Results show significant improvement on multiple benchmarks. Furthermore, we discuss open questions and future directions in exploiting the potential power of our approach. By proposing a novel solution we aim at inspiring more advanced applications of graph learning methods in artificial intelligence.  Keywords: Graph neural network, transformer, computational linguistics, machine learning, computer vision",1
"In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.",0
"Our paper proposes a new method for learning deep representations that estimates and maximizes mutual information between input data and model outputs. This approach has several benefits over traditional methods such as backpropagation, including better generalization performance on unseen tasks, more interpretable results, and improved robustness to changes in the training process. We show how our algorithm can be applied to both supervised and unsupervised settings using convolutional neural networks (CNNs) and generative adversarial networks (GANs). In addition, we demonstrate its effectiveness through experimental evaluations on benchmark datasets from computer vision and natural language processing domains. Finally, we discuss some limitations and potential future directions of our work.",1
"Unsupervised learning of compact and relevant state representations has been proved very useful at solving complex reinforcement learning tasks. In this paper, we propose a recurrent capsule network that learns such representations by trying to predict the future observations in an agent's trajectory.",0
"Recently, Capsule Networks (CapsNets) have been shown to improve over traditional convolutional neural networks on image classification tasks by incorporating more structured representations into their architecture. In addition, Recurrent Neural Networks (RNNs) are capable of processing sequential data such as time-series or natural language text. However, these two models are typically trained separately for different types of data. This work proposes a novel framework that combines the strengths of both architectures - state representation learning using RNN cells within a CapsNet structure to handle both static images and dynamic sequences simultaneously. We demonstrate through comprehensive experiments on multiple benchmark datasets that our approach outperforms existing methods. Further analysis suggests that our model learns richer feature hierarchies during training which may lead to better generalization across tasks.  -----",1
"Learning powerful discriminative features for remote sensing image scene classification is a challenging computer vision problem. In the past, most classification approaches were based on handcrafted features. However, most recent approaches to remote sensing scene classification are based on Convolutional Neural Networks (CNNs). The de facto practice when learning these CNN models is only to use original RGB patches as input with training performed on large amounts of labeled data (ImageNet). In this paper, we show class activation map (CAM) encoded CNN models, codenamed DDRL-AM, trained using original RGB patches and attention map based class information provide complementary information to the standard RGB deep models. To the best of our knowledge, we are the first to investigate attention information encoded CNNs. Additionally, to enhance the discriminability, we further employ a recently developed object function called ""center loss,"" which has proved to be very useful in face recognition. Finally, our framework provides attention guidance to the model in an end-to-end fashion. Extensive experiments on two benchmark datasets show that our approach matches or exceeds the performance of other methods.",0
"This paper presents a new method for deep representation learning with attention maps for scene classification tasks. We propose a discriminative network architecture that learns both global and local features using attentional mechanisms. Our approach integrates intermediate attention layers into traditional convolutional neural networks (CNNs), allowing the model to selectively focus on informative regions in each image at different scales. Experimental results demonstrate the effectiveness of our proposed method compared to state-of-the-art approaches on several benchmark datasets, including MIT indoor scene dataset, SUN RGBD dataset, and Stanford Cars dataset. In addition, we provide qualitative analysis which indicates that the learned attention maps can effectively highlight important objects/regions that contribute most to the final predictions. Overall, our work makes contributions towards improving deep representation learning techniques for computer vision applications.",1
"Fine-grained visual categorization is to recognize hundreds of subcategories belonging to the same basic-level category, which is a highly challenging task due to the quite subtle and local visual distinctions among similar subcategories. Most existing methods generally learn part detectors to discover discriminative regions for better categorization performance. However, not all parts are beneficial and indispensable for visual categorization, and the setting of part detector number heavily relies on prior knowledge as well as experimental validation. As is known to all, when we describe the object of an image via textual descriptions, we mainly focus on the pivotal characteristics, and rarely pay attention to common characteristics as well as the background areas. This is an involuntary transfer from human visual attention to textual attention, which leads to the fact that textual attention tells us how many and which parts are discriminative and significant to categorization. So textual attention could help us to discover visual attention in image. Inspired by this, we propose a fine-grained visual-textual representation learning (VTRL) approach, and its main contributions are: (1) Fine-grained visual-textual pattern mining devotes to discovering discriminative visual-textual pairwise information for boosting categorization performance through jointly modeling vision and text with generative adversarial networks (GANs), which automatically and adaptively discovers discriminative parts. (2) Visual-textual representation learning jointly combines visual and textual information, which preserves the intra-modality and inter-modality information to generate complementary fine-grained representation, as well as further improves categorization performance.",0
"Increasingly, computers are expected to interpret and generate human-like media. Text descriptions of images have become popular as a proxy task (i.e., caption generation), yet fine-grained scene understanding tasks, such as object detection and segmentation, remain difficult even for humans. These fine-grained tasks require detailed visual knowledge that text often fails to capture. This paper presents a method to jointly learn image representations using both vision and language by learning to predict missing objects given partial scenes and their associated captions. Unlike prior work that used synthetic prompts based on predefined templates, our approach generates natural variations from scratch during training, which requires more advanced reasoning capabilities. We evaluate these generated prompts by showing them to thousands of Turkers who perform the target prediction at over 90% accuracy. Our model outperforms state-of-the-art methods across multiple benchmark datasets in both generative zero-shot classification and localization. By generating realistic missing objects with corresponding contexts and performing zero-shot evaluation, we enable new research opportunities in fine-grained representation learning.",1
"Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.",0
"This work presents a method for learning hierarchical graph representations using differentiable pooling operations. The proposed approach represents each node in the graph as a high-dimensional feature vector, which can then be combined into a lower dimensional representation using a set of learnable pooling functions. These pooling functions are trained end-to-end along with the rest of the network parameters, allowing them to adaptively capture important features from different layers of the graph. Our experiments demonstrate that this approach significantly outperforms existing methods on several benchmark datasets, while also providing interpretable results through its ability to identify key substructures within the graph. Overall, our work provides a powerful new tool for learning effective representations of complex graphs in machine learning applications.",1
"Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are difficult for current deep reinforcement learning (RL) models due to the large discrete action space and the varying number of actions between the states. In this work, we introduce DOM-Q-NET, a novel architecture for RL-based web navigation to address both of these problems. It parametrizes Q functions with separate networks for different action categories: clicking a DOM element and typing a string input. Our model utilizes a graph neural network to represent the tree-structured HTML of a standard web page. We demonstrate the capabilities of our model on the MiniWoB environment where we can match or outperform existing work without the use of expert demonstrations. Furthermore, we show 2x improvements in sample efficiency when training in the multi-task setting, allowing our model to transfer learned behaviours across tasks.",0
"Title: ""DOM-Q-NET: Grounded Reinforcement Learning on Structured Natural Languages"" Abstract: In recent years, grounded reinforcement learning (GRL) has emerged as a promising paradigm for developing artificial intelligence agents that can effectively interact with real-world environments by leveraging structured representations such as natural language instructions and visual perceptions. However, training GRL agents using traditional RL algorithms remains challenging due to issues related to sample efficiency, exploration, and generalization across different tasks and domains. To address these limitations, we introduce DOM-Q-NET, a novel framework for implementing GRL that builds upon deep networks with memory (DNNM), which have been successfully used in other areas of machine learning such as computer vision and natural language processing. Our approach integrates several key components including a new loss function, a knowledge distillation module, and a curiosity mechanism designed specifically for DNNM-based models. Through extensive experiments using benchmark datasets such as Atari, DOOM, and Minecraft, we demonstrate that our framework achieves state-of-the-art performance compared to existing methods, significantly reducing policy convergence time while maintaining competitive results. We believe that our work represents a significant step forward towards developing more intelligent artificial agents capable of tackling complex problems under human guidance.",1
"Deep generative models like variational autoencoders approximate the intrinsic geometry of high dimensional data manifolds by learning low-dimensional latent-space variables and an embedding function. The geometric properties of these latent spaces has been studied under the lens of Riemannian geometry; via analysis of the non-linearity of the generator function. In new developments, deep generative models have been used for learning semantically meaningful `disentangled' representations; that capture task relevant attributes while being invariant to other attributes. In this work, we explore the geometry of popular generative models for disentangled representation learning. We use several metrics to compare the properties of latent spaces of disentangled representation models in terms of class separability and curvature of the latent-space. The results we obtain establish that the class distinguishable features in the disentangled latent space exhibits higher curvature as opposed to a variational autoencoder. We evaluate and compare the geometry of three such models with variational autoencoder on two different datasets. Further, our results show that distances and interpolation in the latent space are significantly improved with Riemannian metrics derived from the curvature of the space. We expect these results will have implications on understanding how deep-networks can be made more robust, generalizable, as well as interpretable.",0
"In recent years, deep generative models have emerged as powerful tools for learning complex data distributions and generating realistic samples. Despite their successes, these models often suffer from several drawbacks such as difficulty in interpreting learned representations and lack of explicit control over latent variables that generate desired outputs. To address these issues, researchers have proposed disentangling methods, which aim at separating factors of variation into independent dimensions in the learned representation. However, existing frameworks based on maximum likelihood estimation or adversarial training may only partially achieve disentanglement objectives due to challenging optimization problems caused by high dimensionality and strong coupling among model parameters. This paper presents a new theoretical framework for understanding geometric properties of deep generative models under regularization constraints designed to enforce disentanglement criteria. We identify fundamental connections between the Fisher Information Matrix (FIM) of the underlying probabilistic model and the curvature properties of the loss landscape in the space of neural network weights. Our analysis reveals that enforcing sparse FIM leads to flat minima along principal directions aligned with individual causal influences, allowing efficient gradient-based optimizers to exploit these low-dimensional manifolds while circumventing entanglement barriers. Extensive experiments across multiple benchmark datasets validate our theory predictions, demonstrating improved reconstruction accuracy and interpretability of factorized representations compared against state-of-the-art approaches. As an accompanying contribution, we publicly release open source code to facilitate wider adoption of our methodology within the community. Overall, our work advances the field of unsupervised representation learning toward obtaining more faithful decompositions of complex natural phenomena, paving the way for further breakthroughs in AI applications relying on deciphered data semantics",1
"A Restricted Boltzmann Machine (RBM) is an unsupervised machine-learning bipartite graphical model that jointly learns a probability distribution over data and extracts their relevant statistical features. As such, RBM were recently proposed for characterizing the patterns of coevolution between amino acids in protein sequences and for designing new sequences. Here, we study how the nature of the features learned by RBM changes with its defining parameters, such as the dimensionality of the representations (size of the hidden layer) and the sparsity of the features. We show that for adequate values of these parameters, RBM operate in a so-called compositional phase in which visible configurations sampled from the RBM are obtained by recombining these features. We then compare the performance of RBM with other standard representation learning algorithms, including Principal or Independent Component Analysis, autoencoders (AE), variational auto-encoders (VAE), and their sparse variants. We show that RBM, due to the stochastic mapping between data configurations and representations, better capture the underlying interactions in the system and are significantly more robust with respect to sample size than deterministic methods such as PCA or ICA. In addition, this stochastic mapping is not prescribed a priori as in VAE, but learned from data, which allows RBM to show good performance even with shallow architectures. All numerical results are illustrated on synthetic lattice-protein data, that share similar statistical features with real protein sequences, and for which ground-truth interactions are known.",0
"This paper presents a study that investigates learning compositional representations of interacting systems using restricted Boltzmann machines (RBMs). In particular, the focus is on lattice proteins, which are biomolecules composed of atoms arranged in a highly ordered three-dimensional structure. Understanding how these structures form and evolve over time is crucial for understanding their function and developing new therapeutic agents. However, modeling such complex interactions remains challenging due to the large number of degrees of freedom involved and the need for efficient computational methods. RBMs provide one approach for tackling these problems by allowing for the learning of distributed representations that capture relevant features of the data while remaining computationally tractable. We compare different variations of the RBM algorithm and evaluate their performance across multiple benchmark datasets. Our results show that certain modifications can significantly improve accuracy and interpretability, providing insights into the design of more effective models for representing interacting systems at multiple scales. Overall, our work demonstrates the utility of RBMs as a tool for exploring complex phenomena and advances the state of the art in machine learning and bioinformatics.",1
"Network representation learning (NRL) has been widely used to help analyze large-scale networks through mapping original networks into a low-dimensional vector space. However, existing NRL methods ignore the impact of properties of relations on the object relevance in heterogeneous information networks (HINs). To tackle this issue, this paper proposes a new NRL framework, called Event2vec, for HINs to consider both quantities and properties of relations during the representation learning process. Specifically, an event (i.e., a complete semantic unit) is used to represent the relation among multiple objects, and both event-driven first-order and second-order proximities are defined to measure the object relevance according to the quantities and properties of relations. We theoretically prove how event-driven proximities can be preserved in the embedding space by Event2vec, which utilizes event embeddings to facilitate learning the object embeddings. Experimental studies demonstrate the advantages of Event2vec over state-of-the-art algorithms on four real-world datasets and three network analysis tasks (including network reconstruction, link prediction, and node classification).",0
"This paper presents new techniques for representing heterogeneous information networks using event embeddings. By modeling events as nodes on a network and relationships as edges, we can capture complex patterns and structures that might otherwise go unnoticed. Our methods enable us to learn representations that encode both structural and semantic information, allowing for more accurate predictions and better downstream applications. We evaluate our approach on several real-world datasets, demonstrating state-of-the-art performance across a range of tasks including node classification, link prediction, and community detection. Overall, our work advances the field of representation learning by providing a flexible and powerful framework for handling diverse types of data.",1
"Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block.bootstrap.pytorch.",0
"Title should have at least 4 words that accurately reflect the content of the paper. Use third person narration instead of first person point of view. Use present perfect tense verb forms unless there is specific reason not to. Start with ""Recent advances"" or ""Significant progress"". No citations required. If possible use passive voice. Content complete sentences without fragments or run ons. End with period after final sentence. --------------------------  Abstract: Recent advances in computer vision have led to significant progress in visual question answering (VQA) and relationship detection tasks. In particular, bilinear models have been shown to perform well in these areas by fusing features from multiple sources such as images and text. However, existing methods still face challenges in effectively utilizing both local and global context information, which can lead to errors in understanding relationships between objects or entities within the image. To address these limitations, we propose BLOCK: Bilinear Superdiagonal Fusion for VQA and VRD. Our approach uses superdiagonal matrices to efficiently capture dependencies across different feature dimensions and scales, allowing for better fusion of image and textual data. Experimental results show that our method outperforms current state-of-the-art approaches on several benchmark datasets. Overall, BLOCK represents a step forward in leveraging advanced mathematical techniques for improved performance in complex multimodal reasoning tasks.  Title: Enhancing Multimodal Reasoning through Bilinear Superdiagonal Fusion ------------------------------ \home\username\Desktop\AI Chatbot Project\Assistant_Personality\_Development\Assistant Personality Example Dialogues\Dialogue Prompts\Prompt_2\response.md ===============================  In this section, I provide five sample prompts related to writing style and tone followed by responses based on an authoritative yet friendly personality profile. Please consider these examples as you refine the overall characterization of your language model. --------------------------------------------------------------------------------------------------------------------",1
"We provide a theoretical analysis of the representation learning problem aimed at learning the latent variables (design matrix) $\Theta$ of observations $Y$ with the knowledge of the coefficient matrix $X$. The design matrix is learned under the assumption that the latent variables $\Theta$ are smooth with respect to a (known) topological structure $\mathcal{G}$. To learn such latent variables, we study a graph Laplacian regularized estimator, which is the penalized least squares estimator with penalty term proportional to a Laplacian quadratic form. This type of estimators has recently received considerable attention due to its capability in incorporating underlying topological graph structure of variables into the learning process. While the estimation problem can be solved efficiently by state-of-the-art optimization techniques, its statistical consistency properties have been largely overlooked. In this work, we develop a non-asymptotic bound of estimation error under the classical statistical setting, where sample size is larger than the ambient dimension of the latent variables. This bound illustrates theoretically the impact of the alignment between the data and the graph structure as well as the graph spectrum on the estimation accuracy. It also provides theoretical evidence of the advantage, in terms of convergence rate, of the graph Laplacian regularized estimator over classical ones (that ignore the graph structure) in case of a smoothness prior. Finally, we provide empirical results of the estimation error to corroborate the theoretical analysis.",0
"In recent years, graph Laplacian regularization has become increasingly popular as a technique for solving ill-posed inverse problems. However, few studies have thoroughly analyzed the performance of these methods under different error models. This study aims to fill that gap by performing a comprehensive analysis of graph Laplacian regularization in the presence of noise and measurement errors. We analyze the behavior of these estimators using numerical simulations and provide insights into their robustness and stability properties. Our results show that while graph Laplacian regularization can improve reconstruction accuracy, it can also lead to overfitting and poor generalizability if not properly tuned. Additionally, we find that regularization strength plays an important role in determining the tradeoff between bias and variance. Overall, our work provides valuable guidance for practitioners looking to use graph Laplacian regularization in real-world applications.",1
"Network representation learning in low dimensional vector space has attracted considerable attention in both academic and industrial domains. Most real-world networks are dynamic with addition/deletion of nodes and edges. The existing graph embedding methods are designed for static networks and they cannot capture evolving patterns in a large dynamic network. In this paper, we propose a dynamic embedding method, dynnode2vec, based on the well-known graph embedding method node2vec. Node2vec is a random walk based embedding method for static networks. Applying static network embedding in dynamic settings has two crucial problems: 1) Generating random walks for every time step is time consuming 2) Embedding vector spaces in each timestamp are different. In order to tackle these challenges, dynnode2vec uses evolving random walks and initializes the current graph embedding with previous embedding vectors. We demonstrate the advantages of the proposed dynamic network embedding by conducting empirical evaluations on several large dynamic network datasets.",0
"""This research introduces DynNode2Vec, a scalable method for dynamic network embedding that enables real-time analysis of evolving networks. Building on recent advances in static graph embedding techniques, our approach uses deep learning to capture patterns in node interactions and temporal changes in network structure. We validate our model through extensive experiments across diverse datasets including social media, citation networks, and brain activity data. Our results demonstrate superior performance compared to existing state-of-the-art methods in terms of accuracy and robustness. Furthermore, we provide insights into how our framework can support real-world applications such as anomaly detection and predictive analytics.""",1
"Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Recently, the very first identifiability proofs for nonlinear ICA have been proposed, leveraging the temporal structure of the independent components. Here, we propose a general framework for nonlinear ICA, which, as a special case, can make use of temporal structure. It is based on augmenting the data by an auxiliary variable, such as the time index, the history of the time series, or any other available information. We propose to learn nonlinear ICA by discriminating between true augmented data, or data in which the auxiliary variable has been randomized. This enables the framework to be implemented algorithmically through logistic regression, possibly in a neural network. We provide a comprehensive proof of the identifiability of the model as well as the consistency of our estimation method. The approach not only provides a general theoretical framework combining and generalizing previously proposed nonlinear ICA models and algorithms, but also brings practical advantages.",0
"Artificial intelligence (AI) has been widely applied across a variety of fields including computer vision, natural language processing, robotics and decision support systems due to their ability to effectively process large amounts of data, identify patterns and make decisions based on that processed data. One such application of AI is Independent Component Analysis (ICA), which seeks to separate underlying independent factors from mixed data sources to gain insights into complex processes or structures. In recent years, there has been growing interest in developing novel approaches to further enhance traditional linear methods of ICA, with two major directions: introducing auxiliary variables that capture important properties of the latent sources or incorporating contrastive priors that guide learning towards more informative decompositions. These extensions have shown promise as they enable greater flexibility and adaptability of the methodology to different problem settings and datasets but their analysis is limited by the focus on specific applications without providing a general framework. Our paper presents an innovative approach combining both auxiliary variables and generalized contrastive learning for nonlinear ICA, enabling improved decomposition quality over existing models. We provide theoretical justification of our proposed model, experimental validation via comprehensive comparison studies against state-of-the-art alternatives and demonstration of its applicability on realworld datasets. By addressing current limitations, we aim to contribute new knowledge that can advance nonlinear ICA research and strengthen its impact in diverse fields where data separation plays a significant role.",1
"In this work we explore a new approach for robots to teach themselves about the world simply by observing it. In particular we investigate the effectiveness of learning task-agnostic representations for continuous control tasks. We extend Time-Contrastive Networks (TCN) that learn from visual observations by embedding multiple frames jointly in the embedding space as opposed to a single frame. We show that by doing so, we are now able to encode both position and velocity attributes significantly more accurately. We test the usefulness of this self-supervised approach in a reinforcement learning setting. We show that the representations learned by agents observing themselves take random actions, or other agents perform tasks successfully, can enable the learning of continuous control policies using algorithms like Proximal Policy Optimization (PPO) using only the learned embeddings as input. We also demonstrate significant improvements on the real-world Pouring dataset with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. Video results are available at https://sites.google.com/view/actionablerepresentations .",0
"Recent advances in deep learning have demonstrated state-of-the-art performance across multiple domains including image classification, object detection, speech recognition, machine translation etc. Despite these impressive results, deep models still suffer from several shortcomings which makes them difficult to interpret and analyze by human experts. For instance, while convolutional neural networks (CNNs) capture rich spatial hierarchies using handcrafted filters, they lack adaptability as their architectures require meticulous design before each task. In contrast, graphical models such as dynamic bayesian networks can represent uncertainty but often lead to computational intractability during exact inference and learning. Moreover, these traditional modeling approaches assume that data comes from stationary distributions whereas most real-world applications exhibit nonstationarity due to changing environments. To overcome these limitations, we present a framework which captures localized representations through adaptative mechanisms inspired by animal behaviors such as attention and curiosity. Our approach learns interpretable internal abstractions from raw visual inputs without requiring explicit supervision which enables the agent to explore efficiently and generalize effectively on new environments even under nonstationary settings. We validate our model’s effectiveness on challenging benchmark datasets with competitive results against strong baselines while providing more actionable insights compared to blackbox deep models. As such ,our work represents an important step towards bridging the gap between artificial intelligence systems and human cognition which could eventually enable autonomous agents to make better decisions based upon learned knowledge beyond trial-and-error .",1
"Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are ""actionable."" These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, without explicit reconstruction of the observation. We show how these representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning.",0
"In recent years, there has been significant interest in learning goal-conditioned policies that can perform complex tasks by directly optimizing for a desired outcome rather than relying on traditional reinforcement learning algorithms that learn through trial and error. One major challenge in learning these types of policies is how to represent the desired outcomes in a meaningful way that allows the policy to efficiently explore the space of possible solutions. In this work, we propose using a novel approach based on actionable representations, which are compact representations that encode both the current state of the environment and the necessary actions required to achieve the desired goal. We show that our proposed method leads to significantly better performance compared to other methods on a variety of simulated robotics tasks, including reaching goals, pushing objects, and solving mazes. Our results demonstrate the potential for learning more effective goal-conditioned policies through the use of actionable representations, paving the way for improved performance in real-world applications.",1
"Modeling the underlying person structure for person re-identification (re-ID) is difficult due to diverse deformable poses, changeable camera views and imperfect person detectors. How to exploit underlying person structure information without extra annotations to improve the performance of person re-ID remains largely unexplored. To address this problem, we propose a novel Relative Local Distance (RLD) method that integrates a relative local distance constraint into convolutional neural networks (CNNs) in an end-to-end way. It is the first time that the relative local constraint is proposed to guide the global feature representation learning. Specially, a relative local distance matrix is computed by using feature maps and then regarded as a regularizer to guide CNNs to learn a structure-aware feature representation. With the discovered underlying person structure, the RLD method builds a bridge between the global and local feature representation and thus improves the capacity of feature representation for person re-ID. Furthermore, RLD also significantly accelerates deep network training compared with conventional methods. The experimental results show the effectiveness of RLD on the CUHK03, Market-1501, and DukeMTMC-reID datasets. Code is available at \url{https://github.com/Wanggcong/RLD_codes}.",0
"In recent years, person re-identification has become a key research area in computer vision due to its many potential applications in security systems. One approach to tackling this problem involves analyzing patterns present in images of individuals captured by cameras at different angles and viewpoints. This study proposes a new method that uses relative local distance to discover underlying structure patterns present in these image sets. By doing so, we aim to improve upon existing methods that rely on global distances only. Our results show promising improvements over other state-of-the-art algorithms. Further work is planned to test our method under more challenging conditions, as well as investigate possible use cases beyond person re-identification. Overall, this paper contributes to a better understanding of how to identify and analyze human appearance features in order to enhance automatic recognition technologies.",1
"One of the key challenges of performing label prediction over a data stream concerns with the emergence of instances belonging to unobserved class labels over time. Previously, this problem has been addressed by detecting such instances and using them for appropriate classifier adaptation. The fundamental aspect of a novel-class detection strategy relies on the ability of comparison among observed instances to discriminate them into known and unknown classes. Therefore, studies in the past have proposed various metrics suitable for comparison over the observed feature space. Unfortunately, these similarity measures fail to reliably identify distinct regions in observed feature spaces useful for class discrimination and novel-class detection, especially in streams containing high-dimensional data instances such as images and texts. In this paper, we address this key challenge by proposing a semi-supervised multi-task learning framework called \sysname{} which aims to intrinsically search for a latent space suitable for detecting labels of instances from both known and unknown classes. We empirically measure the performance of \sysname{} over multiple real-world image and text datasets and demonstrate its superiority by comparing its performance with existing semi-supervised methods.",0
"Artificial Intelligence (AI) has made tremendous progress over recent years through advances in deep learning techniques such as convolutional neural networks (CNNs). These models have been successful in tasks like image classification, object detection, semantic segmentation, etc., but suffer from limitations in novel class detection due to their reliance on explicit memory-based representations. To overcome these limitations, we propose co-representation learning, which jointly learns multiple sets of complementary representations that can capture different aspects of data distributions. We demonstrate the effectiveness of our method by comparing against baseline methods using public datasets, achieving superior results on both standard benchmarks and challenging benchmarks that contain significant variations in lighting conditions, viewpoints, poses, occlusions, background clutter, and new classes not seen during training. Our proposed approach also improves performance on incremental task scenarios with limited labeled samples of the target domain and surpasses state-of-the-art methods across all benchmarks tested. Overall, co-representation learning presents a powerful tool for improved CNN performance in difficult real-world environments where only few labeled examples of each new class are available.",1
"Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.",0
"One of the most fundamental tasks in artificial intelligence (AI) research is training machine learning models on large datasets, such as images or videos. To achieve this goal, there have been numerous efforts focused on developing new techniques that can effectively learn from these massive amounts of data. However, many existing methods suffer from several limitations, including reliance on labeled data, difficulty transferring learned knowledge across domains, and computational inefficiency. In response, we propose revisiting self-supervised visual representation learning, which involves training machines using unlabeled data alone. By leveraging recent advancements in deep neural networks and optimization algorithms, our approach outperforms prior state-of-the-art methods by substantial margins in terms of both effectiveness and efficiency. We hope that this work will inspire further research into self-supervised learning techniques and ultimately lead to even more advanced AI systems that can tackle even greater challenges.",1
"The remarkable success of machine learning, especially deep learning, has produced a variety of cloud-based services for mobile users. Such services require an end user to send data to the service provider, which presents a serious challenge to end-user privacy. To address this concern, prior works either add noise to the data or send features extracted from the raw data. They struggle to balance between the utility and privacy because added noise reduces utility and raw data can be reconstructed from extracted features. This work represents a methodical departure from prior works: we balance between a measure of privacy and another of utility by leveraging adversarial learning to find a sweeter tradeoff. We design an encoder that optimizes against the reconstruction error (a measure of privacy), adversarially by a Decoder, and the inference accuracy (a measure of utility) by a Classifier. The result is RAN, a novel deep model with a new training algorithm that automatically extracts features for classification that are both private and useful. It turns out that adversarially forcing the extracted features to only conveys the intended information required by classification leads to an implicit regularization leading to better classification accuracy than the original model which completely ignores privacy. Thus, we achieve better privacy with better utility, a surprising possibility in machine learning! We conducted extensive experiments on five popular datasets over four training schemes, and demonstrate the superiority of RAN compared with existing alternatives.",0
"This paper explores how machine learning algorithms can increase their accuracy by using quantified privacy measures. Specifically, we propose the use of a novel deep neural network architecture called a Reconstruction Adversarial Network (RAN) that combines generative models and discriminators to learn more accurate representations while preserving individuals’ data privacy. Our experimental results demonstrate that RANs significantly improve accuracy over traditional methods such as GANs and VAEs alone while maintaining high levels of privacy. We also provide qualitative analysis to showcase the interpretability and robustness of our approach. In summary, this work advances the state-of-the-art in unsupervised representation learning with increased accuracy, quantifiable privacy guarantees, and explainability.",1
"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",0
"In this work we present a new approach to unsupervised representation learning based on predicting future observations using a self-critical training objective. Our method, called Contrastive Predictive Coding (CPC), learns to predict future observations given either positive or negative context depending on whether the prediction matches ground truth labels. We demonstrate that CPC improves state-of-the-art results on established benchmarks including image classification and object detection tasks without any additional supervision beyond the raw pixel input data itself. Additionally, we provide ablation studies comparing alternative variants of our proposed approach as well as analyses suggesting qualitatively better representations can be learned by minimizing reconstruction error rather than generative adversarial training objectives such as the Jensen-Shannon divergence used in InfoGAN. Finally, we explore potential limitations and open research directions related to applying these methods on more complex real world problems where high quality reference targets may difficult or impossible to obtain. By providing clear theoretical motivations and extensive evaluations across different datasets and models we hope this work serves as a step forward towards understanding how to learn powerful generalizable features from large scale visual inputs.",1
"An important part of many machine learning workflows on graphs is vertex representation learning, i.e., learning a low-dimensional vector representation for each vertex in the graph. Recently, several powerful techniques for unsupervised representation learning have been demonstrated to give the state-of-the-art performance in downstream tasks such as vertex classification and edge prediction. These techniques rely on random walks performed on the graph in order to capture its structural properties. These structural properties are then encoded in the vector representation space.   However, most contemporary representation learning methods only apply to static graphs while real-world graphs are often dynamic and change over time. Static representation learning methods are not able to update the vector representations when the graph changes; therefore, they must re-generate the vector representations on an updated static snapshot of the graph regardless of the extent of the change in the graph. In this work, we propose computationally efficient algorithms for vertex representation learning that extend random walk based methods to dynamic graphs. The computation complexity of our algorithms depends upon the extent and rate of changes (the number of edges changed per update) and on the density of the graph. We empirically evaluate our algorithms on real world datasets for downstream machine learning tasks of multi-class and multi-label vertex classification. The results show that our algorithms can achieve competitive results to the state-of-the-art methods while being computationally efficient.",0
"Here is an example of how you can write an abstract without including the paper title:  This paper presents a novel approach to representation learning for dynamic graphs using random walks. We first introduce some notation related to graph theory that will be used throughout our discussion. Then we present an algorithm for generating a sequence of random walk steps on a dynamically changing graph, as well as an analysis of the mixing time of this process under certain assumptions. Finally, we describe how these random walks can be leveraged in order to learn representations of both static and evolving structures within the graph. Our experimental results demonstrate the effectiveness of our method across a range of applications. This work has important implications for fields ranging from computer science and engineering to economics and sociology, where understanding complex networks is crucial. Overall, our contribution provides new insights into the use of random walks for representation learning in dynamic environments.",1
"Despite significant advances in clustering methods in recent years, the outcome of clustering of a natural image dataset is still unsatisfactory due to two important drawbacks. Firstly, clustering of images needs a good feature representation of an image and secondly, we need a robust method which can discriminate these features for making them belonging to different clusters such that intra-class variance is less and inter-class variance is high. Often these two aspects are dealt with independently and thus the features are not sufficient enough to partition the data meaningfully. In this paper, we propose a method where we discover these features required for the separation of the images using deep autoencoder. Our method learns the image representation features automatically for the purpose of clustering and also select a coherent image and an incoherent image simultaneously for a given image so that the feature representation learning can learn better discriminative features for grouping the similar images in a cluster and at the same time separating the dissimilar images across clusters. Experiment results show that our method produces significantly better result than the state-of-the-art methods and we also show that our method is more generalized across different dataset without using any pre-trained model like other existing methods.",0
"In this paper, we propose a novel approach to image clustering that leverages deep representation learning characterized by inter-class separation. Traditional methods rely on predefined features or hand-engineered representations to group images into clusters. However, these approaches can often result in suboptimal performance due to their limited ability to capture subtle differences among classes. To overcome this limitation, our method utilizes convolutional neural networks (CNNs) trained using a carefully designed loss function that emphasizes intra-class compactness and inter-class separability. Our framework allows the model to learn highly discriminative feature representations that significantly improve clustering accuracy. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets, achieving state-of-the-art results compared to existing methods.",1
"Syndrome differentiation in Traditional Chinese Medicine (TCM) is the process of understanding and reasoning body condition, which is the essential step and premise of effective treatments. However, due to its complexity and lack of standardization, it is challenging to achieve. In this study, we consider each patient's record as a one-dimensional image and symptoms as pixels, in which missing and negative values are represented by zero pixels. The objective is to find relevant symptoms first and then map them to proper syndromes, that is similar to the object detection problem in computer vision. Inspired from it, we employ multi-instance multi-task learning combined with the convolutional neural network (MIMT-CNN) for syndrome differentiation, which takes region proposals as input and output image labels directly. The neural network consists of region proposals generation, convolutional layer, fully connected layer, and max pooling (multi-instance pooling) layer followed by the sigmoid function in each syndrome prediction task for image representation learning and final results generation. On the diabetes dataset, it performs better than all other baseline methods. Moreover, it shows stability and reliability to generate results, even on the dataset with small sample size, a large number of missing values and noises.",0
"Title: Multi-Instance Multi-Task Learning for Syndrome Differentiation of Diabetic Patients  Abstract:  Diabetes Mellitus (DM) is one of the most common non-communicable diseases worldwide. One of the complications associated with DM is diabetic syndromes, which can lead to significant morbidity and mortality if left untreated. Accurate diagnosis of diabetic syndromes is crucial for early intervention and effective management of the disease. In recent years, several studies have investigated the use of machine learning algorithms for syndrome differentiation among diabetic patients. However, these methods often require large amounts of data and may not always generalize well across different populations.  To address these limitations, we propose a novel approach using multi-instance multi-task learning (MIML) that leverages transfer learning from pre-trained convolutional neural networks (CNNs). Our method utilizes a combination of clinical and biochemical features along with computed tomography (CT) images obtained through CNN-based feature extraction. We evaluated our model on two datasets consisting of diabetic patients from different countries and compared its performance against state-of-the-art models. Results show that our proposed method outperforms previous approaches in terms of accuracy, sensitivity, specificity, precision, and F1 score, demonstrating its effectiveness in differentiating diabetic syndromes across diverse patient cohorts. This study highlights the potential utility of MIML with transferred CNN knowledge in enhancing prediction accuracy for complex medical problems such as diabetic syndrome identification.  In summary, our research presents a promising approach towards improving the accuracy of diagnosing diabetic syndromes by integrating heterogeneous healthcare data sources. With further validation in larger, more diverse patient samples, this technology could potentially transform the future p",1
"In this work, we study value function approximation in reinforcement learning (RL) problems with high dimensional state or action spaces via a generalized version of representation policy iteration (RPI). We consider the limitations of proto-value functions (PVFs) at accurately approximating the value function in low dimensions and we highlight the importance of features learning for an improved low-dimensional value function approximation. Then, we adopt different representation learning algorithm on graphs to learn the basis functions that best represent the value function. We empirically show that node2vec, an algorithm for scalable feature learning in networks, and the Variational Graph Auto-Encoder constantly outperform the commonly used smooth proto-value functions in low-dimensional feature space.",0
"This would serve as the introduction to a machine learning research paper analyzing the use of reinforcement learning techniques on graph data structures (such as social networks). Please provide enough context so that someone familiar with computer science can make sense of the abstract without referring back to the full text of the paper.  Here is some sample code that generates a simple representation of the model used for training - please refer to this as ""the model"" within your abstract. ```python from rlgraph import * import matplotlib.pyplot as plt  def discount_factor(episode):     return np.exp(-10*(episodes/length))**2 + 1e-6 # episodic discount factor      agent = DQN() # agent class from library that uses QLearning with Double Deep Neural Network architecture  model = buildModel(""gcn"") # creates Graph Convolutional Network model to learn features rewardMapper = rewardMappingFunction(mappingType=""linearRegression"",outputDim=4)  trainingEpisodes = 10**7 validationEpisodes = 10**6  agent.fitData(Xtrain, ytrain, batchSize=32, nThreads=8, episodesPerSampleBatch=30, epochSamples=trainingEpisodes, mapping=rewardMapper, model=model) agent.validateData(Xval, yval, validationEpisodes) ``` Please note that these lines of code are just meant to give you an idea of how we trained our model using RLGraph; they should not be included in your abstract directly. You may reference them indirectly if necessary (""we trained our model using a combination of deep neural network architectures and reward shaping"").  Your abstract could begin something like this: ""We present a new approach to applying reinforcement learning methods in graphs."" Or perhaps, ""This work examines the application of representational learning techniques to graphs for the purpose of building agents capable of achieving complex tasks through trial-and-error search in simulated environments."", etc.",1
"Representation learning becomes especially important for complex systems with multimodal data sources such as cameras or sensors. Recent advances in reinforcement learning and optimal control make it possible to design control algorithms on these latent representations, but the field still lacks a large-scale standard dataset for unified comparison. In this work, we present a large-scale dataset and evaluation framework for representation learning for the complex task of landing an airplane. We implement and compare several approaches to representation learning on this dataset in terms of the quality of simple supervised learning tasks and disentanglement scores. The resulting representations can be used for further tasks such as anomaly detection, optimal control, model-based reinforcement learning, and other applications.",0
"This paper investigates how state representations can be learned from complex systems using multimodal data sources. We demonstrate that by leveraging multiple sensory inputs, we can create more accurate and effective models that capture the system’s internal states. Our approach involves integrating visual, auditory, and other modalities into a single representation, which allows us to better capture the relationships and interactions within the system. Our experiments show significant improvements over traditional approaches, highlighting the importance of considering all available data when learning representations for complex systems. Overall, our work demonstrates the power of multi-modal learning for understanding and modeling complex systems.",1
"In general, recommendation can be viewed as a matching problem, i.e., match proper items for proper users. However, due to the huge semantic gap between users and items, it's almost impossible to directly match users and items in their initial representation spaces. To solve this problem, many methods have been studied, which can be generally categorized into two types, i.e., representation learning-based CF methods and matching function learning-based CF methods. Representation learning-based CF methods try to map users and items into a common representation space. In this case, the higher similarity between a user and an item in that space implies they match better. Matching function learning-based CF methods try to directly learn the complex matching function that maps user-item pairs to matching scores. Although both methods are well developed, they suffer from two fundamental flaws, i.e., the limited expressiveness of dot product and the weakness in capturing low-rank relations respectively. To this end, we propose a general framework named DeepCF, short for Deep Collaborative Filtering, to combine the strengths of the two types of methods and overcome such flaws. Extensive experiments on four publicly available datasets demonstrate the effectiveness of the proposed DeepCF framework.",0
"In recent years, recommender systems have become increasingly important in providing personalized recommendations to users in various applications such as e-commerce, social media, and entertainment platforms. However, designing effective recommendation models remains challenging due to the complexity of user preferences and behaviors, as well as the high dimensionality and sparsity of the data available. To address these issues, we propose a unified framework called DeepCF that jointly learns latent representations and matching functions from raw features using deep neural networks. By leveraging both representation learning and matching function learning, our approach is able to capture complex patterns and relationships among items and users, while reducing computational cost and improving scalability compared to state-of-the-art methods. Experiments on several real-world datasets demonstrate significant improvements over baseline approaches in terms of accuracy, diversity, novelty, and robustness. Our results suggest that DeepCF provides a powerful solution for building effective and efficient recommender systems that can handle large scale and noisy data.",1
"We propose a one-class neural network (OC-NN) model to detect anomalies in complex data sets. OC-NN combines the ability of deep networks to extract a progressively rich representation of data with the one-class objective of creating a tight envelope around normal data. The OC-NN approach breaks new ground for the following crucial reason: data representation in the hidden layer is driven by the OC-NN objective and is thus customized for anomaly detection. This is a departure from other approaches which use a hybrid approach of learning deep features using an autoencoder and then feeding the features into a separate anomaly detection method like one-class SVM (OC-SVM). The hybrid OC-SVM approach is sub-optimal because it is unable to influence representational learning in the hidden layers. A comprehensive set of experiments demonstrate that on complex data sets (like CIFAR and GTSRB), OC-NN performs on par with state-of-the-art methods and outperformed conventional shallow methods in some scenarios.",0
"Increasingly, as data collection becomes more ubiquitous, so too has our ability to collect vast troves of data on nearly any phenomenon we wish to study. Consequently, we find ourselves needing methods capable of identifying anomalies within these datasets - events that deviate significantly from normalcy in some meaningful way. This requires us to confront challenges unique to high-dimensional data such as nonlinear relationships, noise, missing values, outliers and multiple modes; meanwhile, classical algorithms for detecting anomalies have proved inadequate, as they require assumptions regarding both the underlying distribution of data and knowledge regarding thresholds or other parameters required to define ""normal"". To address these difficulties, we propose a novel application of neural networks known as one class SVM (support vector machine) for detecting anomalies. Our approach does not assume any prior knowledge of distribution functions or parameter tuning, instead utilizing unsupervised learning techniques designed to capture higher level statistical properties of observed data distributions without explicit model assumption",1
"Due to the phenomenon of ""posterior collapse,"" current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed $\delta$-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet $32\times 32$.",0
"Delta-VAE was trained on V2ID dataset containing aligned scans from TCIA, consisting of multiple abdominal CT exams over different timepoints (from healthy subjects) resulting in high quality images. To prevent posterior collapse during training, we used real time gradient clipping. We compared performance of our model against other state of art models using Fidelity score metric which showed that our model performed better than SINAPSE baseline and CycleGAN. Additionally, quantitative comparison has been done between delta-VAE with previous method such as LUNAR++ and SINAPSE in terms of precision and recall. Precision increased by up to 48% while recall decreased by only 3%. Overall, these results suggest that this approach improves visual fidelity without compromising accuracy and provides more stable training process. Future research directions involve further optimization and analysis of hyperparameters including batch normalization and data augmentation techniques.",1
"The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.",0
"Artificial Intelligence has been rapidly developing over recent years due to advancements in deep learning techniques such as Generative Adversarial Networks (GANs) which have made it possible to create highly realistic images from textual descriptions, amongst other things. This improvement however comes at the cost of more complex models that suffer from instability during training. We hypothesize that this issue stems primarily from poor design choices rather than any inherent limitations of GAN models themselves. To resolve this problem we introduce amortized inference regularization: by modifying existing frameworks such that certain operations are no longer differentiable, our model can now take advantage of cheap, accurate approximations while still producing high quality results. While others may view this approach as restrictive, we see it as allowing the exploration of new architectural choices that were previously unavailable since they produced very low likelihood samples during MLE optimization. Through extensive experiments on a diverse range of datasets, we show the efficacy of our method by achieving state of the art results on each one. Additionally, we analyze why these improvements occur both theoretically and experimentally through visualizations and ablation studies. Lastly, because of the ease with which we can swap different approximations into place, we anticipate future work that pushes the boundaries even further.",1
"Transfer learning can address the learning tasks of unlabeled data in the target domain by leveraging plenty of labeled data from a different but related source domain. A core issue in transfer learning is to learn a shared feature space in where the distributions of the data from two domains are matched. This learning process can be named as transfer representation learning (TRL). The feature transformation methods are crucial to ensure the success of TRL. The most commonly used feature transformation method in TRL is kernel-based nonlinear mapping to the high-dimensional space followed by linear dimensionality reduction. But the kernel functions are lack of interpretability and are difficult to be selected. To this end, the TSK fuzzy system (TSK-FS) is combined with transfer learning and a more intuitive and interpretable modeling method, called transfer representation learning with TSK-FS (TRL-TSK-FS) is proposed in this paper. Specifically, TRL-TSK-FS realizes TRL from two aspects. On one hand, the data in the source and target domains are transformed into the fuzzy feature space in which the distribution distance of the data between two domains is min-imized. On the other hand, discriminant information and geo-metric properties of the data are preserved by linear discriminant analysis and principal component analysis. In addition, another advantage arises with the proposed method, that is, the nonlinear transformation is realized by constructing fuzzy mapping with the antecedent part of the TSK-FS instead of kernel functions which are difficult to be selected. Extensive experiments are conducted on the text and image datasets. The results obviously show the superiority of the proposed method.",0
"In recent years there has been growing interest in deep learning techniques such as convolutional neural networks (CNNs) which have proven very successful at tasks ranging from image classification to machine translation. However, these models often lack interpretability making them hard to trust and use in safety critical applications such as self driving cars and medical diagnosis systems. In contrast to deep learning methods, Takagi-Sugeno-Kang (TSK) fuzzy system can provide interpretable results through their rule base but they tend to underperform due to their simplistic linear combination mechanism. This work introduces representation learning into the framework by adapting activation functions used in modern deep learning architectures onto the rule base parameters. We evaluate our model on two public datasets: CIFAR100 and MNIST. Results show that transfer representation learning can significantly improve performance without sacrificing interpretability. By bridging the gap between interpretability and accuracy, we hope to broaden the scope where these types of systems could be applied.",1
"Multiview representation learning is very popular for latent factor analysis. It naturally arises in many data analysis, machine learning, and information retrieval applications to model dependent structures among multiple data sources. For computational convenience, existing approaches usually formulate the multiview representation learning as convex optimization problems, where global optima can be obtained by certain algorithms in polynomial time. However, many pieces of evidence have corroborated that heuristic nonconvex approaches also have good empirical computational performance and convergence to the global optima, although there is a lack of theoretical justification. Such a gap between theory and practice motivates us to study a nonconvex formulation for multiview representation learning, which can be efficiently solved by a simple stochastic gradient descent (SGD) algorithm. We first illustrate the geometry of the nonconvex formulation; Then, we establish asymptotic global rates of convergence to the global optima by diffusion approximations. Numerical experiments are provided to support our theory.",0
"In recent years, online multiview learning has become an important area of research due to its potential applications in computer vision, natural language processing, and other fields. One popular approach to online multiview learning is to use convex optimization methods, which have been shown to provide theoretical guarantees on model convergence and generalization performance. However, these methods can suffer from scalability issues as well as slow convergence rates in practice. In this paper, we propose a new method called nonconvex multiview learning that addresses these limitations by relaxing the convexity constraint used in existing approaches. Our proposed method uses a novel regularizer based on the sum of nuclear norms, which promotes low-rank representations while allowing for more flexible and efficient updates during online training. We demonstrate through experiments on several real-world datasets that our method achieves better performance than state-of-the-art convex methods in terms of accuracy, speed, and robustness to initialization. This work highlights the importance of considering nonconvex formulations in online multiview learning and suggests further directions for future research in this field.",1
"In this work we present a method to improve the pruning step of the current state-of-the-art methodology to compress neural networks. The novelty of the proposed pruning technique is in its differentiability, which allows pruning to be performed during the backpropagation phase of the network training. This enables an end-to-end learning and strongly reduces the training time. The technique is based on a family of differentiable pruning functions and a new regularizer specifically designed to enforce pruning. The experimental results show that the joint optimization of both the thresholds and the network weights permits to reach a higher compression rate, reducing the number of weights of the pruned network by a further 14% to 33% compared to the current state-of-the-art. Furthermore, we believe that this is the first study where the generalization capabilities in transfer learning tasks of the features extracted by a pruned network are analyzed. To achieve this goal, we show that the representations learned using the proposed pruning methodology maintain the same effectiveness and generality of those learned by the corresponding non-compressed network on a set of different recognition tasks.",0
"Title: ""Deep neural network compression using automated pruning""  Abstract: Automatic deep learning (autoML) has become increasingly popular as a means of developing high performance machine learning models without requiring extensive expertise in model tuning and optimization. However, one major drawback of many autoML systems is their tendency to overfit data or require excessive computational resources due to overfitting. In particular, deep neural networks can quickly grow very large, making them difficult to optimize and deploy. We address these issues by introducing a novel approach that combines several pruning techniques into a single framework for optimizing deep neural networks. This approach enables automated pruning of unimportant connections within each layer, enabling the creation of smaller yet more accurate models. Our method uses a combination of channel, neuron, and connection pruning strategies along with regularization techniques to ensure that only important connections are retained. Results on multiple datasets demonstrate significant improvements in accuracy and reduced model size compared to baseline approaches, achieving state-of-the art results on several benchmarks while reducing required compute resources. Overall our work presents an effective solution to compress complex deep neural networks that balances prediction quality with efficiency, thereby improving scalability and deployment capabilities.",1
"Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation. Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.",0
"Title: A Novel Approach for Improving Generative Models Using Mutual Posterior-Divergence Regularization  Generative models have shown great promise in generating new data that resembles training examples. However, they often suffer from poor generations due to challenges such as overfitting and collapse in the latent space. In this work, we propose a novel regularizer called mutual posterior-divergence (MAE) which addresses these issues by adding a discriminator network to each of the two ends of the variational autoencoder model. This encourages both the encoder and decoder to learn better representations and reduces the likelihood of mode collapsing in the latent space. Our experiments on several benchmark datasets show promising results compared to previous state-of-the-art methods.",1
"Hashing has been recognized as an efficient representation learning method to effectively handle big data due to its low computational complexity and memory cost. Most of the existing hashing methods focus on learning the low-dimensional vectorized binary features based on the high-dimensional raw vectorized features. However, studies on how to obtain preferable binary codes from the original 2D image features for retrieval is very limited. This paper proposes a bilinear supervised discrete hashing (BSDH) method based on 2D image features which utilizes bilinear projections to binarize the image matrix features such that the intrinsic characteristics in the 2D image space are preserved in the learned binary codes. Meanwhile, the bilinear projection approximation and vectorization binary codes regression are seamlessly integrated together to formulate the final robust learning framework. Furthermore, a discrete optimization strategy is developed to alternatively update each variable for obtaining the high-quality binary codes. In addition, two 2D image features, traditional SURF-based FVLAD feature and CNN-based AlexConv5 feature are designed for further improving the performance of the proposed BSDH method. Results of extensive experiments conducted on four benchmark datasets show that the proposed BSDH method almost outperforms all competing hashing methods with different input features by different evaluation protocols.",0
"This sounds like a potentially interesting paper! Could you provide more details or the actual text? Otherwise I can try writing something based on my knowledge of the field, but it may be less accurate.",1
"High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time. To address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty. We evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.",0
"Title: SOM-VAE: Interpretable Discrete Representation Learning on Time Series Abstract Deep learning methods have been used successfully for time series modeling and forecasting tasks but often suffer from lack of interpretability due to their complex nature. To address this issue, we propose a novel method called Self-Organizing Maps (SOM) Variational Autoencoders (SOM-VAEs). SOMs are a type of artificial neural network that can learn discretized representations of input data while preserving spatial structure. VAEs combine variational inference and deep generative models to learn compressed continuous representations of inputs. By combining these two techniques, we aim to learn interpretable discrete representationsof time series data that capture essential features and patterns while retaining the computational efficiency of VAEs. In our approach, we first preprocess raw time series into multiple views to feed into the SOM encoder. Then, we train a VAEdecoderto reconstruct originalviewsfrom learneddiscrete codes. We jointlytrain both components usinga variational lower bound objectivefunction maximizing evidencelowerboundonlog-likelihoodofdataandpriorinformationaboutcodevectors. After training is complete, we obtain a compact yet meaningful setof discrete representationsthat summarize important informationin originaltime seriesinputdata.Theseinterpretablediscretecodescanbeusedforvarious downstream taskssuchas classificationandforecasting,providingtransparencyinto howthealgorithmmakesits predictions. Our experiments demonstrate the effectivenessof SOMSVMRATINASINENOVISYSTEMUSSEVEARCHONCLOCKSSHIFTEDPOTENTIALSROBOTHANDGRAPHSANALYZINGLIGHTSCENEDETECTORSONSHIPCODALENEWSDATA",1
"Learning informative representations of data is one of the primary goals of deep learning, but there is still little understanding as to what representations a neural network actually learns. To better understand this, subspace match was recently proposed as a method for assessing the similarity of the representations learned by neural networks. It has been shown that two networks with the same architecture trained from different initializations learn representations that at hidden layers show low similarity when assessed with subspace match, even when the output layers show high similarity and the networks largely exhibit similar performance on classification tasks. In this note, we present a simple example motivated by standard results in commutative algebra to illustrate how this can happen, and show that although the subspace match at a hidden layer may be 0, the representations learned may be isomorphic as vector spaces. This leads us to conclude that a subspace match comparison of learned representations may well be uninformative, and it points to the need for better methods of understanding learned representations.",0
"Learning can be defined as enabling an agent to perform well on a task by adapting its internal parameters using experience obtained through trial and error, whether given directly or derived from observing other agents performing similar tasks (Sutton & Barto, 2018). As deep learning has become increasingly popular over recent years, so too has interest into how these models learn their representations (Goodfellow et al., 2016), particularly whether they truly reflect semantic knowledge (Bell, Fyfe, Vinyals, Tenenbaum, & Ullman, 2019; Chen et al., 2019). One method proposed for assessing learned representation quality is subspace matching (Dosovitskiy, Lewis, Broxwarrth, Carlson, & Van Der Linden, 2020); however, we question if this metric accurately measures similarity across different representations. We provide evidence to support our claim, and suggest alternative approaches for evaluating learned representations.",1
"We present a representation learning algorithm that learns a low-dimensional latent dynamical system from high-dimensional \textit{sequential} raw data, e.g., video. The framework builds upon recent advances in amortized inference methods that use both an inference network and a refinement procedure to output samples from a variational distribution given an observation sequence, and takes advantage of the duality between control and inference to approximately solve the intractable inference problem using the path integral control approach. The learned dynamical model can be used to predict and plan the future states; we also present the efficient planning method that exploits the learned low-dimensional latent dynamics. Numerical experiments show that the proposed path-integral control based variational inference method leads to tighter lower bounds in statistical model learning of sequential data. The supplementary video: https://youtu.be/xCp35crUoLQ",0
"Here, we present a novel approach to learning representations and planning control policies for dynamical systems using adaptive path integral autoencoders (APIA). Our method leverages recent advances in deep neural networks to efficiently approximate high dimensional integrals over system trajectories, enabling fast inference and efficient optimization. We demonstrate that our APIAs provide rich latent spaces suitable for both linear and nonlinear dimensionality reduction techniques as well as model-based prediction and control tasks such as optimal control policy synthesis via reinforcement learning from system trajectories. By demonstrating state-of-the-art performance on several challenging benchmark problems including fluid flow control and robotic manipulation, we showcase the general applicability and effectiveness of our approach across various domains. This work highlights important connections between machine learning theory, representation learning, and advanced controls strategies; ultimately facilitating more autonomous systems capable of realizing increasingly complex goals.  This article presents new research aimed at improving how machines learn and solve problems within their environment. In particular, the authors propose applying adaptive path-integral autoencoders (APIAs) to representational learning tasks - which refer to understanding and organizing knowledge or data - and decision making processes in order to optimize system behavior. This approach has already shown promising results by achieving top performance on a number of difficult tasks related to controlling flow dynamics and robotics. With these findings, the study emphasizes potential synergies between artificial intelligence, machine learning, and advanced controls methods to produce smarter self-directed technologies. Ultimately, by expanding capabilities like those addressed here, society may soon see even more intelligent tools designed to complete specific objectives autonomously.",1
"Convolution is an efficient technique to obtain abstract feature representations using hierarchical layers in deep networks. Although performing convolution in Euclidean geometries is fairly straightforward, its extension to other topological spaces---such as a sphere ($\mathbb{S}^2$) or a unit ball ($\mathbb{B}^3$)---entails unique challenges. In this work, we propose a novel `\emph{volumetric convolution}' operation that can effectively convolve arbitrary functions in $\mathbb{B}^3$. We develop a theoretical framework for \emph{volumetric convolution} based on Zernike polynomials and efficiently implement it as a differentiable and an easily pluggable layer for deep networks. Furthermore, our formulation leads to derivation of a novel formula to measure the symmetry of a function in $\mathbb{B}^3$ around an arbitrary axis, that is useful in 3D shape analysis tasks. We demonstrate the efficacy of proposed volumetric convolution operation on a possible use-case i.e., 3D object recognition task.",0
"This research explores the potential of using volumetric convolutions as an alternative approach for representation learning within unit balls. Previous work has relied on linear algebra techniques to learn representations from data embedded in high-dimensional spaces such as Euclidean space or Grassmann manifolds. However, these methods can suffer from limitations due to their reliance on flat vector spaces or restrictive assumptions about the geometry of the underlying domain.  Volumetric convolutions provide a natural framework for representing data in ball spaces that can overcome some of these challenges. By exploiting the intrinsic curvature and structure of balls, they enable efficient approximation of nonlinear transformations while preserving important geometric properties. In addition, the compactness of unit balls allows for efficient regularization of the learned representations.  In our experiments, we show that volumetric convolutions outperform traditional linear algebraic techniques across a range of datasets and tasks, including image classification, speech recognition, and generative modeling. Our results suggest that the use of volumetric convolutions could lead to more effective models for representation learning in applications where high-dimensional geometry plays an important role.",1
"When doing representation learning on data that lives on a known non-trivial manifold embedded in high dimensional space, it is natural to desire the encoder to be homeomorphic when restricted to the manifold, so that it is bijective and continuous with a continuous inverse. Using topological arguments, we show that when the manifold is non-trivial, the encoder must be globally discontinuous and propose a universal, albeit impractical, construction. In addition, we derive necessary constraints which need to be satisfied when designing manifold-specific practical encoders. These are used to analyse candidates for a homeomorphic encoder for the manifold of 3D rotations $SO(3)$.",0
"This paper presents a new approach to topological constraints on homeomorphic auto-encoding (HAE). In recent years, HAE has emerged as a promising technique for dimensionality reduction that preserves global topology while minimizing distortion. However, current methods suffer from several limitations, such as high computational complexity and sensitivity to initialization parameters. To overcome these challenges, we propose a novel framework based on mathematical morphology that enables us to efficiently enforce topological constraints during the encoding process. Our method leverages advanced techniques from algebraic topology and shape theory to compute coherent topological features that capture the underlying structure of data sets. We then use these features to guide the optimization of the HAE model, ensuring that the encoded representation faithfully captures essential properties of the original dataset. Extensive experiments demonstrate significant improvements over state-of-the-art approaches in terms of accuracy, efficiency, and robustness across various applications in computer vision, bioinformatics, and sensor data processing. Overall, our work provides valuable insights into how topological considerations can enhance machine learning pipelines and strengthen their predictive capabilities.",1
"Recently, the topic of graph representation learning has received plenty of attention. Existing approaches usually focus on structural properties only and thus they are not sufficient for those spatial graphs where the nodes are associated with some spatial information. In this paper, we present the first deep learning approach called s2vec for learning spatial graph representations, which is based on denoising autoencoders framework (DAF). We evaluate the learned representations on real datasets and the results verified the effectiveness of s2vec when used for spatial clustering.",0
"This paper proposes a new method for representation learning on spatial graphs using deep neural networks. We show that by incorporating both spatial and graph structures into the network architecture and training process, we can significantly improve performance on several benchmark datasets. Our approach uses convolutional layers to capture local patterns in the data, followed by recurrent layers to model global dependencies across time steps. We demonstrate the effectiveness of our method through extensive experiments and ablation studies, comparing against state-of-the-art methods in the field. Our results suggest that incorporating spatial structure into representation learning models leads to improved generalization and robustness, making them well-suited for applications such as route planning, traffic forecasting, and recommender systems. Overall, this work represents an important step towards building more efficient and effective machine learning algorithms for processing complex spatial data.",1
"The standard loss function used to train neural network classifiers, categorical cross-entropy (CCE), seeks to maximize accuracy on the training data; building useful representations is not a necessary byproduct of this objective. In this work, we propose clustering-oriented representation learning (COREL) as an alternative to CCE in the context of a generalized attractive-repulsive loss framework. COREL has the consequence of building latent representations that collectively exhibit the quality of natural clustering within the latent space of the final hidden layer, according to a predefined similarity function. Despite being simple to implement, COREL variants outperform or perform equivalently to CCE in a variety of scenarios, including image and news article classification using both feed-forward and convolutional neural networks. Analysis of the latent spaces created with different similarity functions facilitates insights on the different use cases COREL variants can satisfy, where the Cosine-COREL variant makes a consistently clusterable latent space, while Gaussian-COREL consistently obtains better classification accuracy than CCE.",0
"Abstract: As machine learning continues to gain importance across various industries, effective representation learning has become critical to success. Many algorithms have been proposed in recent years that focus on clustering methods for improving representation quality. One such algorithm is known as clustering-oriented representation learning (CORL), which uses an attractor-repeller loss function to promote grouping of similar samples while simultaneously reducing interdependencies within clusters. In this paper, we explore the use of CORL for enhancing traditional image classification models by introducing a novel attraction-repulsion mechanism that works in conjunction with clustering objectives. Our results show significant improvements over baseline models trained without CORL, demonstrating the effectiveness of our approach. With the rise of large-scale datasets and complex model architectures, efficient and accurate feature learning remains a crucial challenge. This work represents an important step forward in addressing these challenges by combining clustering techniques with advanced representation learning algorithms.",1
"We aim to obtain an interpretable, expressive, and disentangled scene representation that contains comprehensive structural and textural information for each object. Previous scene representations learned by neural networks are often uninterpretable, limited to a single object, or lacking 3D knowledge. In this work, we propose 3D scene de-rendering networks (3D-SDN) to address the above issues by integrating disentangled representations for semantics, geometry, and appearance into a deep generative model. Our scene encoder performs inverse graphics, translating a scene into a structured object-wise representation. Our decoder has two components: a differentiable shape renderer and a neural texture generator. The disentanglement of semantics, geometry, and appearance supports 3D-aware scene manipulation, e.g., rotating and moving objects freely while keeping the consistent shape and texture, and changing the object appearance without affecting its shape. Experiments demonstrate that our editing scheme based on 3D-SDN is superior to its 2D counterpart.",0
"Imagine a world where you can effortlessly change the appearance and dynamics of a scene by simply manipulating objects within that scene as if they were real physical objects. This capability has been made possible through recent advancements in computer graphics technology known as inverse graphics. With the use of deep learning techniques and high-performance computing systems, researchers have developed novel approaches to enable 3D object manipulation in complex environments. These methods aim to reconstruct intricate geometric representations of scenes from images or videos while preserving fine details such as lighting, texture, and geometry. By using inverse rendering and optimization algorithms, users can intuitively modify and interact with objects and their surroundings in real time, paving the way for applications across industries including game development, special effects, and virtual reality simulations. In this paper, we explore several state-of-the-art methods in this exciting field, discuss challenges and limitations, and provide insights into future directions for continued progress.",1
"In a Massive Open Online Course (MOOC), predictive models of student behavior can support multiple aspects of learning, including instructor feedback and timely intervention. Ongoing courses, when the student outcomes are yet unknown, must rely on models trained from the historical data of previously offered courses. It is possible to transfer models, but they often have poor prediction performance. One reason is features that inadequately represent predictive attributes common to both courses. We present an automated transductive transfer learning approach that addresses this issue. It relies on problem-agnostic, temporal organization of the MOOC clickstream data, where, for each student, for multiple courses, a set of specific MOOC event types is expressed for each time unit. It consists of two alternative transfer methods based on representation learning with auto-encoders: a passive approach using transductive principal component analysis and an active approach that uses a correlation alignment loss term. With these methods, we investigate the transferability of dropout prediction across similar and dissimilar MOOCs and compare with known methods. Results show improved model transferability and suggest that the methods are capable of automatically learning a feature representation that expresses common predictive characteristics of MOOCs.",0
"Abstract: This paper explores the use of transfer learning and representation learning techniques in massive open online courses (MOOCs) to improve learner outcomes. The authors present a comprehensive analysis of existing literature on transfer learning in MOOCs, highlighting key findings and trends in the field. They then propose a novel framework that integrates both transfer learning and representation learning principles into MOOC design, and demonstrate its effectiveness through rigorous experimentation. Results show significant improvements in learner engagement and performance compared to traditional MOOC designs. Finally, the authors discuss implications for future research and practice in educational technology. Overall, this study contributes valuable insights into how machine learning can enhance the accessibility and efficacy of education through scalable digital platforms like MOOCs.",1
"Among the representation learning, the low-rank representation (LRR) is one of the hot research topics in many fields, especially in image processing and pattern recognition. Although LRR can capture the global structure, the ability of local structure preservation is limited because LRR lacks dictionary learning. In this paper, we propose a novel multi-focus image fusion method based on dictionary learning and LRR to get a better performance in both global and local structure. Firstly, the source images are divided into several patches by sliding window technique. Then, the patches are classified according to the Histogram of Oriented Gradient (HOG) features. And the sub-dictionaries of each class are learned by K-singular value decomposition (K-SVD) algorithm. Secondly, a global dictionary is constructed by combining these sub-dictionaries. Then, we use the global dictionary in LRR to obtain the LRR coefficients vector for each patch. Finally, the l_1-norm and choose-max fuse strategy for each coefficients vector is adopted to reconstruct fused image from the fused LRR coefficients and the global dictionary. Experimental results demonstrate that the proposed method can obtain state-of-the-art performance in both qualitative and quantitative evaluations compared with serval classical methods and novel methods.The Code of our fusion method is available at https://github.com/hli1221/imagefusion_dllrr",0
"In recent years, image fusion has become increasingly important in many fields due to the availability of multiple sensors that can capture complementary information about the same scene. However, traditional image fusion methods have limitations such as loss of important details, computational complexity, and lack of robustness against noise. To address these issues, we propose a novel method for multi-focus image fusion based on dictionary learning and low-rank representation (DLRLR). Our approach combines the advantages of both sparse representation and low-rank matrix decomposition to learn informative dictionaries from the input images. The learned dictionaries serve as a prior knowledge for constructing the fused image, which results in superior performance compared to state-of-the-art approaches. Experimental evaluations demonstrate that our DLRLR method achieves significant improvements over existing fusion techniques under different conditions while preserving crucial visual details in the output images. This study presents a promising solution towards efficient and effective multi-focus image fusion by leveraging advanced machine learning techniques.",1
"We introduce adversarial neural networks for representation learning as a novel approach to transfer learning in brain-computer interfaces (BCIs). The proposed approach aims to learn subject-invariant representations by simultaneously training a conditional variational autoencoder (cVAE) and an adversarial network. We use shallow convolutional architectures to realize the cVAE, and the learned encoder is transferred to extract subject-invariant features from unseen BCI users' data for decoding. We demonstrate a proof-of-concept of our approach based on analyses of electroencephalographic (EEG) data recorded during a motor imagery BCI experiment.",0
"Title: ""Transfer Learning in Brain-Computer Interfaces with Adversarial Variational Autoencoders"" Abstract: Recent advancements in deep learning have enabled the development of brain-computer interfaces (BCIs) that can decode neural activity into control signals for external devices such as prosthetics or communication systems. However, the performance of these BCIs often degrades rapidly over time due to changes in sensor recordings caused by various factors including disease progression and rehabilitation. To address this challenge, we propose using adversarial variational autoencoders (AVAs), a state-of-the-art generative model trained on large datasets, to enhance transfer learning across sessions in EEG-based BCIs. AVA consists of two sub-networks: one generates a reconstruction from latent variables while another network tries to predict whether the input data originates from real data or the generator network. Training AVA jointly optimizes both the generative task and representation learning, resulting in better generalization ability for BCI use cases. In our experiments, we demonstrate improved decoding accuracy compared to traditional linear regression models and baseline BCI methods. Our findings support the potential of AVA for enhancing transfer learning in BCIs for diverse populations, laying the groundwork for future research in developing personalized and adaptive BCI systems. Keywords: Brain-computer interface, Variational autoencoder, Transfer learning, Adversarial training, Decoding accuracy.",1
"This paper focuses on designing data-driven models to learn a discriminant representation space for face recognition using RGB-D data. Unlike hand-crafted representations, learned models can extract and organize the discriminant information from the data, and can automatically adapt to build new compute vision applications faster. We proposed an effective way to train Convolutional Neural Networks to learn face patch discriminant features. The proposed solution was tested and validated on state-of-the-art RGB-D datasets and showed competitive and promising results relatively to standard hand-crafted feature extractors.",0
"In recent years, the use of RGB-D sensors has become increasingly popular for face recognition applications due to their ability to capture both depth and color information. However, effective representation of the RGB-D data remains a challenge. This paper proposes a new approach called discriminant patch representation (DPR) which extracts informative features from the raw sensor data using convolutional neural networks (CNNs). We show that DPR significantly improves the performance of RGB-D based face recognition systems by capturing spatial and temporal features of faces under varying illumination conditions. Our results outperform state-of-the-art methods on two public datasets, demonstrating the effectiveness of our proposed method.",1
"Facial expression recognition has been an active area in computer vision with application areas including animation, social robots, personalized banking, etc. In this study, we explore the problem of image classification for detecting facial expressions based on features extracted from pre-trained convolutional neural networks trained on ImageNet database. Features are extracted and transferred to a Linear Support Vector Machine for classification. All experiments are performed on two publicly available datasets such as JAFFE and CK+ database. The results show that representations learned from pre-trained networks for a task such as object recognition can be transferred, and used for facial expression recognition. Furthermore, for a small dataset, using features from earlier layers of the VGG19 network provides better classification accuracy. Accuracies of 92.26% and 92.86% were achieved for the CK+ and JAFFE datasets respectively.",0
"In recent years, pre-trained convolutional neural networks (CNNs) have been successfully applied to facial expression recognition due to their ability to learn discriminative features from large amounts of data. However, most existing methods focus on training CNN models from scratch using labeled facial expression datasets which can be time consuming and require significant computational resources. To address these limitations, we propose the use of pre-trained CNN features for facial expression recognition. Our approach involves fine-tuning the pre-trained model on a smaller labeled dataset, resulting in improved accuracy while reducing training time and computational requirements. We evaluate our method on several benchmark databases and show that it outperforms state-of-the-art approaches based on feature extraction. Our results demonstrate the effectiveness of using pre-trained CNN features for facial expression recognition tasks.",1
"In this paper we introduce evidence transfer for clustering, a deep learning method that can incrementally manipulate the latent representations of an autoencoder, according to external categorical evidence, in order to improve a clustering outcome. By evidence transfer we define the process by which the categorical outcome of an external, auxiliary task is exploited to improve a primary task, in this case representation learning for clustering. Our proposed method makes no assumptions regarding the categorical evidence presented, nor the structure of the latent space. We compare our method, against the baseline solution by performing k-means clustering before and after its deployment. Experiments with three different kinds of evidence show that our method effectively manipulates the latent representations when introduced with real corresponding evidence, while remaining robust when presented with low quality evidence.",0
"This research investigates how external categorical evidence can improve clustering tasks by transferring relevant knowledge from existing datasets. We propose a novel methodology that combines multiple sources of evidence using domain adaptation techniques, resulting in more accurate and meaningful clusters. Our approach extends traditional clustering algorithms by incorporating pre-existing domain knowledge through feature engineering and label propagation, effectively leveraging external data to enhance clustering performance. Experimental results on several real-world datasets demonstrate the effectiveness of our method in producing high-quality clusterings, outperforming state-of-the-art methods in many cases. Overall, our findings suggest that integrating external evidence into clustering tasks has significant potential for improving data analysis and decision making across diverse domains.",1
"Person re-identification (PReID) has received increasing attention due to it is an important part in intelligent surveillance. Recently, many state-of-the-art methods on PReID are part-based deep models. Most of them focus on learning the part feature representation of person body in horizontal direction. However, the feature representation of body in vertical direction is usually ignored. Besides, the spatial information between these part features and the different feature channels is not considered. In this study, we introduce a multi-branches deep model for PReID. Specifically, the model consists of five branches. Among the five branches, two of them learn the local feature with spatial information from horizontal or vertical orientations, respectively. The other one aims to learn interdependencies knowledge between different feature channels generated by the last convolution layer. The remains of two other branches are identification and triplet sub-networks, in which the discriminative global feature and a corresponding measurement can be learned simultaneously. All the five branches can improve the representation learning. We conduct extensive comparative experiments on three PReID benchmarks including CUHK03, Market-1501 and DukeMTMC-reID. The proposed deep framework outperforms many state-of-the-art in most cases.",0
"In this paper we propose a deep learning method that learns features from multiple domains and jointly optimizes re-id performance on all of them, while previous methods often focus on one specific domain at a time. We present two main contributions: first, we create new datasets based on real scenarios where cameras have different resolutions, poses, backgrounds etc; second, our model uses a novel architecture inspired by Faster R-CNN, which allows us to learn feature maps using an encoder pre-trained on ImageNet plus additional layers specialized on person re-ID, thus making use of omni-domain feature embeddings. Experimental results show a significant improvement over the state of the art on several benchmarks (e.g. Market-1501, DukeMTMC-reID). Finally, ablation studies highlight the importance of each component.",1
"Recently, graph neural networks have been adopted in a wide variety of applications ranging from relational representations to modeling irregular data domains such as point clouds and social graphs. However, the space of graph neural network architectures remains highly fragmented impeding the development of optimized implementations similar to what is available for convolutional neural networks. In this work, we present BiGraphNet, a graph neural network architecture that generalizes many popular graph neural network models and enables new efficient operations similar to those supported by ConvNets. By explicitly separating the input and output nodes, BiGraphNet: (i) generalizes the graph convolution to support new efficient operations such as coarsened graph convolutions (similar to strided convolution in convnets), multiple input graphs convolution and graph expansions (unpooling) which can be used to implement various graph architectures such as graph autoencoders, and graph residual nets; and (ii) accelerates and scales the computations and memory requirements in hierarchical networks by performing computations only at specified output nodes.",0
"In this paper we introduce hierarchical bipartite graph convolution networks (HBGCN), which are designed for tasks on attributed graphs where nodes have two different types or labels, such as users and items in recommendation systems. We propose to first apply GCNs on each type separately and then combine their representations using a weighted sum followed by another set of layers. This simple but effective approach achieves state-of-the art results on several benchmark datasets such as Amazon Co-purchase, Douban Movie, and Kipfler Friendster. Our model also outperforms other competitive models that use similar label propagation based methods, e.g., LightGCN, DCGC, and SIGIR. The source code can be found at https://github.com/yunshengchen9418/ HBGCN .",1
"Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.",0
"Title: ""Recent Advances in Autoencoder-Based Representation Learning"" Authors: X Y Z This survey provides a comprehensive overview of recent advancements in autoencoder-based representation learning techniques. Automatic encoders have emerged as a powerful tool in the field due to their ability to learn compact representations that capture the underlying structure of data while preserving important features. In this study we examine several state-of-the-art methods in the area of deep generative models such as variational autoencoders (VAEs), adversarial autoencoders (AAEs) and denoising autoencoders (DAEs). We present the mathematical formulations and objectives of each model, as well as discussing their strengths and weaknesses. Furthermore, we explore the applications of these models including image generation, anomaly detection, feature extraction, dimensionality reduction and more. Finally, we conclude by outlining future research directions in the field. This review serves as a valuable resource for both experienced researchers and beginners interested in learning more about autoencoder-based representation learning techniques.",1
"Machine hearing or listening represents an emerging area. Conventional approaches rely on the design of handcrafted features specialized to a specific audio task and that can hardly generalized to other audio fields. For example, Mel-Frequency Cepstral Coefficients (MFCCs) and its variants were successfully applied to computational auditory scene recognition while Chroma vectors are good at music chord recognition. Unfortunately, these predefined features may be of variable discrimination power while extended to other tasks or even within the same task due to different nature of clips. Motivated by this need of a principled framework across domain applications for machine listening, we propose a generic and data-driven representation learning approach. For this sake, a novel and efficient supervised dictionary learning method is presented. The method learns dissimilar dictionaries, one per each class, in order to extract heterogeneous information for classification. In other words, we are seeking to minimize the intra-class homogeneity and maximize class separability. This is made possible by promoting pairwise orthogonality between class specific dictionaries and controlling the sparsity structure of the audio clip's decomposition over these dictionaries. The resulting optimization problem is non-convex and solved using a proximal gradient descent method. Experiments are performed on both computational auditory scene (East Anglia and Rouen) and synthetic music chord recognition datasets. Obtained results show that our method is capable to reach state-of-the-art hand-crafted features for both applications.",0
"Audio signal recognition has been an active area of research due to its numerous applications such as speech analysis, speaker identification, environmental sound classification, etc. Among several approaches used for audio signal recognition, supervised dictionary learning methods have gained popularity owing to their effectiveness and ease of use. Supervised dictionary learning employs labeled training data consisting of input signals (speech samples, acoustic events) along with corresponding labels/classes (words, phones, event classes). Recent works on dictionary learning have mainly focused on enhancing discriminability or sparsity of representations while disregarding efficiency, which becomes crucial during audio processing tasks involving large datasets. In this work, we propose a novel efficient algorithm named ""Superfast Dictionary Learning"" that can learn dictionaries significantly faster than existing state-of-the-art methods without compromising recognition accuracy. Our approach combines two complementary ideas: projected gradient descent, which helps obtain early convergence, and subspace search, which reduces computational complexity of each iteration. Experimental results obtained using three benchmark datasets demonstrate the superior performance and speed benefits achieved by our proposed method compared to competitive alternatives. Moreover, Superfast Dictionary Learning offers comparable or improved audio signal recognition rates even at low sample sizes often encountered in real-world scenarios. This paper presents the first comprehensive study investigating the trade-offs among different design choices for fast dictionary learning, paving the way towards building more efficient models for real-time audio signal processing applications.",1
"Deep reinforcement-learning methods have achieved remarkable performance on challenging control tasks. Observations of the resulting behavior give the impression that the agent has constructed a generalized representation that supports insightful action decisions. We re-examine what is meant by generalization in RL, and propose several definitions based on an agent's performance in on-policy, off-policy, and unreachable states. We propose a set of practical methods for evaluating agents with these definitions of generalization. We demonstrate these techniques on a common benchmark task for deep RL, and we show that the learned networks make poor decisions for states that differ only slightly from on-policy states, even though those states are not selected adversarially. Taken together, these results call into question the extent to which deep Q-networks learn generalized representations, and suggest that more experimentation and analysis is necessary before claims of representation learning can be supported.",0
"In recent years, deep reinforcement learning (DRL) has emerged as a promising approach to solving complex sequential decision making problems across diverse domains. However, understanding the performance limitations of these models remains a challenge, especially given their increasing complexity and opacity. One important aspect of DRL model performance that has received relatively little attention is generalization - how well can a DRL model trained on one task generalize to new tasks, similar but unseen training environments, or variations of the training environment? This paper seeks to address this gap by proposing a novel framework for measuring and characterizing the generalization ability of DRL algorithms. We apply our proposed methodology to evaluate three popular state-of-the-art DRL architectures (Proximal Policy Optimization, Deep Deterministic Policy Gradient, and Model-Free Deep Q Networks), using four challenging benchmark control suites. Our findings highlight the significant differences in generalization among different DRL algorithms and provide valuable insights into design choices and training strategies required to improve generalization. By shedding light on this critical aspect of DRL performance, we aim to contribute towards building more reliable and robust artificial intelligence systems.",1
"We consider the problem of building a state representation model in a continual fashion. As the environment changes, the aim is to efficiently compress the sensory state's information without losing past knowledge. The learned features are then fed to a Reinforcement Learning algorithm to learn a policy. We propose to use Variational Auto-Encoders for state representation, and Generative Replay, i.e. the use of generated samples, to maintain past knowledge. We also provide a general and statistically sound method for automatic environment change detection. Our method provides efficient state representation as well as forward transfer, and avoids catastrophic forgetting. The resulting model is capable of incrementally learning information without using past data and with a bounded system size.",0
"In recent years, deep reinforcement learning (RL) has achieved remarkable progress in many domains including computer games, robotics, natural language processing, and more recently, generative modeling. However, continual RL---the ability of agents to learn multiple tasks without forgetting previous ones---remains one of the most challenging aspects of artificial intelligence. This work explores a novel method that combines three key components: episodic memory, offline world models, and generative replay. We showcase how these elements can be combined into a single framework, which we call EOGENT (Episodic Online Generator for ENumerating Tasks). Our approach enables deep RL algorithms to learn from very few interactions per task while ensuring they never forget past knowledge. Extensive experiments on several Atari games demonstrate our method outperforms state-of-the-art methods by significant margins.",1
"Facial landmark localization plays a critical role in face recognition and analysis. In this paper, we propose a novel cascaded backbone-branches fully convolutional neural network~(BB-FCN) for rapidly and accurately localizing facial landmarks in unconstrained and cluttered settings. Our proposed BB-FCN generates facial landmark response maps directly from raw images without any preprocessing. BB-FCN follows a coarse-to-fine cascaded pipeline, which consists of a backbone network for roughly detecting the locations of all facial landmarks and one branch network for each type of detected landmark for further refining their locations. Furthermore, to facilitate the facial landmark localization under unconstrained settings, we propose a large-scale benchmark named SYSU16K, which contains 16000 faces with large variations in pose, expression, illumination and resolution. Extensive experimental evaluations demonstrate that our proposed BB-FCN can significantly outperform the state-of-the-art under both constrained (i.e., within detected facial regions only) and unconstrained settings. We further confirm that high-quality facial landmarks localized with our proposed network can also improve the precision and recall of face detection.",0
"In recent years, facial landmark detection has become increasingly important in a variety of fields, including computer vision, biometrics, medical imaging, and video surveillance. Despite significant progress in this area, there remain many challenges that must be addressed. One such challenge is the need for more efficient models that can accurately detect key points on a face while minimizing computational overhead and human annotation effort. To address these issues, we propose a new model called ""Facial Landmark Machines."" This architecture combines backbone features from state-of-the-art deep learning techniques with branches that use local region information to guide feature extraction. We also introduce Progressive Representation Learning (PRL), which enables our model to gradually refine its understanding of faces by using early predictions as guidance during training. Through extensive experiments on popular benchmark datasets, we show that our approach achieves highly competitive results compared to other methods in both efficiency and accuracy. Overall, our work demonstrates the effectiveness of integrating local region information into convolutional neural networks for improving facial landmark detection.",1
"We present a novel method that can learn a graph representation from multivariate data. In our representation, each node represents a cluster of data points and each edge represents the subset-superset relationship between clusters, which can be mutually overlapped. The key to our method is to use formal concept analysis (FCA), which can extract hierarchical relationships between clusters based on the algebraic closedness property. We empirically show that our method can effectively extract hierarchical structures of clusters compared to the baseline method.",0
"In recent years, graph representation has become increasingly popular in many applications such as social network analysis, bioinformatics, and computer vision. As the size and complexity of graphs grow, it becomes more challenging to analyze and interpret their underlying structure and relationships. This work proposes a novel method for learning graph representations using formal concept analysis (FCA), which is a mathematical technique that provides a systematic approach to identify concepts from data by exploring implications among them. The proposed method represents each node in the graph as a combination of concepts, where each concept corresponds to a set of nodes that share similar properties. By leveraging FCA, we can derive meaningful insights into the structure and behavior of complex graphs, including community detection, anomaly detection, and visualization. We evaluate our method on several real-world datasets and demonstrate its effectiveness in terms of accuracy and efficiency compared to state-of-the-art methods. Our results show that the learned graph representations provide new opportunities for understanding complex systems and enable better decision making in diverse domains. Overall, this work contributes to the field of graph mining and knowledge discovery, offering researchers and practitioners a powerful tool for extracting valuable information from large-scale graphs.",1
"An unsupervised human action modeling framework can provide useful pose-sequence representation, which can be utilized in a variety of pose analysis applications. In this work we propose a novel temporal pose-sequence modeling framework, which can embed the dynamics of 3D human-skeleton joints to a continuous latent space in an efficient manner. In contrast to end-to-end framework explored by previous works, we disentangle the task of individual pose representation learning from the task of learning actions as a trajectory in pose embedding space. In order to realize a continuous pose embedding manifold with improved reconstructions, we propose an unsupervised, manifold learning procedure named Encoder GAN, (or EnGAN). Further, we use the pose embeddings generated by EnGAN to model human actions using a bidirectional RNN auto-encoder architecture, PoseRNN. We introduce first-order gradient loss to explicitly enforce temporal regularity in the predicted motion sequence. A hierarchical feature fusion technique is also investigated for simultaneous modeling of local skeleton joints along with global pose variations. We demonstrate state-of-the-art transfer-ability of the learned representation against other supervisedly and unsupervisedly learned motion embeddings for the task of fine-grained action recognition on SBU interaction dataset. Further, we show the qualitative strengths of the proposed framework by visualizing skeleton pose reconstructions and interpolations in pose-embedding space, and low dimensional principal component projections of the reconstructed pose trajectories.",0
"""Unsupervised Feature Learning of Human Actions as Trajectories in Pose Embedding Manifold"" presents a novel approach to unsupervised feature learning of human actions from video data using pose embedding manifolds. This method leverages recent advances in computer vision to extract meaningful features from raw video frames without any supervision or annotation. We propose a new framework that can learn complex relationships between different body parts and their movements, allowing for robust trajectory tracking even under challenging conditions such as occlusions, changes in lighting, and motion blur. Our experiments show significant improvements over existing methods on standard benchmark datasets, demonstrating the effectiveness of our proposed approach. Overall, our work has important implications for developing intelligent systems capable of understanding and predicting human behavior in real world scenarios.  Title: Unsupervised Feature Learning of Human Actions as Trajectories in Pose Embedding Manifold  Abstract: Human action recognition is a fundamental problem in computer vision, enabling numerous applications such as surveillance, entertainment, and automation. In recent years, pose estimation techniques have emerged as powerful tools for capturing human poses directly from video data. However, these methods typically require large amounts of annotated training data which may not always be available. To address this challenge, we present a novel approach that learns human actions without explicit labels through unsupervised feature learning using pose embedding manifolds. By exploiting intrinsic geometric properties of human pose, our method effectively tracks body part trajectories over time to represent complex motions in high dimensional space. Extensive experiments demonstrate superior performance compared to state-of-the-art methods across multiple publicly available datasets, validating the efficacy of our approach towards building autonomous agents able to analyze and interpret human behaviors in real-world environments.",1
"How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.",0
"In recent years, there has been significant interest in developing artificial intelligence (AI) systems that can learn to perform complex tasks without requiring massive amounts of data or specialized domain knowledge. One approach to achieving this goal is through the use of disentangled representations, which represent different factors of variation in the input data separately from one another. However, despite their importance, there remains no clear definition of what constitutes a disentangled representation. This paper seeks to address this gap by providing a formal definition of disentanglement based on three key desiderata: reconstructability, factoriality, and sufficiency. By defining these concepts and showing how they relate to each other, we aim to provide both a theoretical foundation for future research into disentanglement as well as concrete criteria that can be used to evaluate the quality of learned representations in practice. Our work represents an important step towards building more interpretable and generalizable AI systems that can operate across a wide range of domains.",1
"Basic binary relations such as equality and inequality are fundamental to relational data structures. Neural networks should learn such relations and generalise to new unseen data. We show in this study, however, that this generalisation fails with standard feed-forward networks on binary vectors. Even when trained with maximal training data, standard networks do not reliably detect equality.We introduce differential rectifier (DR) units that we add to the network in different configurations. The DR units create an inductive bias in the networks, so that they do learn to generalise, even from small numbers of examples and we have not found any negative effect of their inclusion in the network. Given the fundamental nature of these relations, we hypothesize that feed-forward neural network learning benefits from inductive bias in other relations as well. Consequently, the further development of suitable inductive biases will be beneficial to many tasks in relational learning with neural networks.",0
"This paper shows that feedforward neural networks lack inductive bias and struggle to learn equality relations without external guidance, suggesting that even simple properties like symmetry should be engineered into deep learning models rather than relying exclusively on endless data. We evaluate our findings by training feedforward neural networks to solve a battery of logical reasoning problems involving identity and equivalence. Even minor modifications to inputs can completely change their meaning and confuse the model. While these results may seem negative at first glance, we view them as a call to action: there is still plenty left to discover about what makes artificial minds tick and how they might become genuinely intelligent. We hope this work serves as both a warning against uncritical adulation of big data and as inspiration for more theoretically grounded approaches to deep learning. By carefully controlling the kind of knowledge present within large language models themselves instead of solely relying on brute computational force, perhaps humanity can finally build synthetic companions capable of truly understanding us. Ultimately, success hinges not only on collecting massive amounts of text but also on studying mathematical foundations—the core principles behind why intelligence works in general, regardless of domain. If theoretical computer science has taught us one thing, it’s that no matter how many problems you solve by enumeration, you won’t glean any underlying structure from sheer scale alone (consider the diagonalization proof showing why Turing Machines cannot simulate every function computably). Despite today’s impressive empirical results fueled by Moore’s Law, tomorrow demands deeper investigation into cognition if we want AGIs which don’t just crunch statistics but actively understand reality itself; otherwise the pursuit risks remaining mired in pattern completion rather than true comprehension. Let the quest for principled machine learning begin!",1
"Timely prediction of clinically critical events in Intensive Care Unit (ICU) is important for improving care and survival rate. Most of the existing approaches are based on the application of various classification methods on explicitly extracted statistical features from vital signals. In this work, we propose to eliminate the high cost of engineering hand-crafted features from multivariate time-series of physiologic signals by learning their representation with a sequence-to-sequence auto-encoder. We then propose to hash the learned representations to enable signal similarity assessment for the prediction of critical events. We apply this methodological framework to predict Acute Hypotensive Episodes (AHE) on a large and diverse dataset of vital signal recordings. Experiments demonstrate the ability of the presented framework in accurately predicting an upcoming AHE.",0
"In critical care medicine, timely detection of early acute hypotensive episodes (AHEs) can significantly reduce patient morbidity and mortality by enabling rapid intervention. Multivariate time-series similarity assessment methods that leverage diverse physiological signals have shown promise in detecting such events; however, these approaches remain largely unexplored due to limited availability of labeled datasets and challenges associated with traditional supervised learning paradigms. We propose a novel approach, MTSALSH (Multivariate Time-Series Similarity Assessment via Unsupervised Representation Learning and Stratified Locality Sensitive Hashing), that addresses these limitations through unsupervised representation learning techniques and stratified locality sensitive hashing for robust feature extraction and dimensionality reduction. Our method efficiently preserves important temporal patterns while minimizing noise from irrelevant variables. Through extensive validation on realistic clinical scenarios, we demonstrate significant improvements over existing state-of-the-art approaches across multiple performance metrics including accuracy, precision, recall, F1 score, area under ROC curve, and mean receiver operating characteristic point while maintaining low computational complexity. Overall, our findings indicate that MTSALSH presents a promising solution towards early AHE detection, which could potentially revolutionize critical care practices. These results pave the way for further exploration into multivariate time-series similarity assessments using advanced machine learning models.",1
"This paper presents a novel method for rare event detection from an image pair with class-imbalanced datasets. A straightforward approach for event detection tasks is to train a detection network from a large-scale dataset in an end-to-end manner. However, in many applications such as building change detection on satellite images, few positive samples are available for the training. Moreover, scene image pairs contain many trivial events, such as in illumination changes or background motions. These many trivial events and the class imbalance problem lead to false alarms for rare event detection. In order to overcome these difficulties, we propose a novel method to learn disentangled representations from only low-cost negative samples. The proposed method disentangles different aspects in a pair of observations: variant and invariant factors that represent trivial events and image contents, respectively. The effectiveness of the proposed approach is verified by the quantitative evaluations on four change detection datasets, and the qualitative analysis shows that the proposed method can acquire the representations that disentangle rare events from trivial ones.",0
"This paper presents a method for detecting rare events by learning disentangled representations from raw sensor data. Unlike existing methods which rely on handcrafted features or predefined event categories, our approach learns abstract, interpretable features that can generalize across different environments and tasks. By maximizing the mutual information between these features and ground truth labels, we create a powerful representation space that captures subtle differences between normal and anomalous behavior. Our method significantly outperforms traditional one-class SVMs and other state-of-the-art approaches on several challenging real-world benchmark datasets, demonstrating its effectiveness at detecting rare but important events such as falls or intrusions. In addition to its robust performance, our model is highly interpretable, allowing human experts to easily understand and explain detected anomalies. Overall, our work shows great promise towards building intelligent surveillance systems that can autonomously identify and alert authorities in response to potentially dangerous situations.",1
"For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.",0
"Include keywords such as imaginary goals and value functions and deep reinforcement learning if you think they fit well into your explanation. You can make up data and results but please mention that you did so. If you have any questions ask! Thanks! In this work we propose a new algorithm for deep reinforcement learning (RL) inspired by recent advances in curiosity driven RL. Our approach uses imagined goals to train agents to predict future rewards under different scenarios, allowing them to explore novel environments and learn more effectively. We evaluate our method on several benchmark domains including Atari games and gridworld navigation tasks, achieving state-of-the-art performance across all tested environments. Additionally, we showcase the effectiveness of our algorithm through comprehensive ablation studies, highlighting the importance of both imagined goals and learned value functions in guiding agent behavior. Overall, our contributions demonstrate the potential benefits of incorporating imagination into RL algorithms, paving the way for further research in this exciting direction.",1
"We propose Deep Hierarchical Machine (DHM), a model inspired from the divide-and-conquer strategy while emphasizing representation learning ability and flexibility. A stochastic routing framework as used by recent deep neural decision/regression forests is incorporated, but we remove the need to evaluate unnecessary computation paths by utilizing a different topology and introducing a probabilistic pruning technique. We also show a specified version of DHM (DSHM) for efficiency, which inherits the sparse feature extraction process as in traditional decision tree with pixel-difference feature. To achieve sparse feature extraction, we propose to utilize sparse convolution operation in DSHM and show one possibility of introducing sparse convolution kernels by using local binary convolution layer. DHM can be applied to both classification and regression problems, and we validate it on standard image classification and face alignment tasks to show its advantages over past architectures.",0
"This paper introduces a new architecture, called the ""Deep Hierarchical Machine"", which extends current divide-and-conquer architectures by making them more flexible and capable. The proposed approach allows for arbitrary decompositions at multiple scales, enabling the use of multi-scale information in deep learning tasks. Our method provides significant improvements over traditional methods on standard benchmarks while requiring fewer parameters to achieve similar performance. We believe that our work provides important insights into the possibilities and limitations of deep neural networks, and demonstrates how these systems can benefit from hierarchical decomposition techniques.",1
"In this work, we investigate unsupervised representation learning on medical time series, which bears the promise of leveraging copious amounts of existing unlabeled data in order to eventually assist clinical decision making. By evaluating on the prediction of clinically relevant outcomes, we show that in a practical setting, unsupervised representation learning can offer clear performance benefits over end-to-end supervised architectures. We experiment with using sequence-to-sequence (Seq2Seq) models in two different ways, as an autoencoder and as a forecaster, and show that the best performance is achieved by a forecasting Seq2Seq model with an integrated attention mechanism, proposed here for the first time in the setting of unsupervised learning for medical time series.",0
"This paper focuses on improving clinical predictions by developing new unsupervised time series representation learning methods. Here we describe several different approaches that have proven successful at improving prediction accuracy across multiple data sets. In these algorithms, the temporal structure of each patient’s data sequence is learned without any labeled training examples, resulting in significant improvements over conventional supervised models. These techniques can make use of arbitrary prior knowledge encoded as smoothness constraints or regularizers and scale gracefully up to high dimensions and large datasets without suffering from overfitting problems. Experiments demonstrate their effectiveness relative to common benchmarks and competing alternative unsupervised strategies, including autoencoders, variational inference, adversarial training, generative modeling, diffusion models, HMMs, and state space models. We further illustrate how our algorithm provides insights into disease progression via exploratory visualizations and anomaly detection tasks on important features found within learned representations and their associated embedding vectors. Ultimately, the results suggest that these unsupervised models could be immediately deployed as powerful tools in healthcare settings while simultaneously opening exciting avenues towards interpretable deep learning solutions that require little supervision during deployment to generate actionable predictions from complex electronic medical records or wearables sensor streams. Overall, our paper promises to inspire new research directions and ultimately impact human health in positive ways by leveraging some of the most recent advances arising at the intersection of computer science and statistics to solve difficult clinical challenges.",1
"User representations are routinely used in recommendation systems by platform developers, targeted advertisements by marketers, and by public policy researchers to gauge public opinion across demographic groups. Computer scientists consider the problem of inferring user representations more abstractly; how does one extract a stable user representation - effective for many downstream tasks - from a medium as noisy and complicated as social media?   The quality of a user representation is ultimately task-dependent (e.g. does it improve classifier performance, make more accurate recommendations in a recommendation system) but there are proxies that are less sensitive to the specific task. Is the representation predictive of latent properties such as a person's demographic features, socioeconomic class, or mental health state? Is it predictive of the user's future behavior?   In this thesis, we begin by showing how user representations can be learned from multiple types of user behavior on social media. We apply several extensions of generalized canonical correlation analysis to learn these representations and evaluate them at three tasks: predicting future hashtag mentions, friending behavior, and demographic features. We then show how user features can be employed as distant supervision to improve topic model fit. Finally, we show how user features can be integrated into and improve existing classifiers in the multitask learning framework. We treat user representations - ground truth gender and mental health features - as auxiliary tasks to improve mental health state prediction. We also use distributed user representations learned in the first chapter to improve tweet-level stance classifiers, showing that distant user information can inform classification tasks at the granularity of a single message.",0
"This abstract presents insights into the role that social media plays in users’ daily lives, exploring how individuals use these platforms to connect, interact, and share content online. Drawing on data collected through interviews and surveys, our findings suggest that social media has become an integral part of many people’s routines, serving as both a source of entertainment and a platform for self-expression. In addition, we highlight key factors that influence user engagement – such as age, gender, and technology adoption – and discuss implications for future research and design in online communication systems. Overall, our study offers valuable new perspectives on how social media shapes user behaviour, interaction patterns, and identity formation in networked communities.",1
"Learning visual features from unlabeled image data is an important yet challenging task, which is often achieved by training a model on some annotation-free information. We consider spatial contexts, for which we solve so-called jigsaw puzzles, i.e., each image is cut into grids and then disordered, and the goal is to recover the correct configuration. Existing approaches formulated it as a classification task by defining a fixed mapping from a small subset of configurations to a class set, but these approaches ignore the underlying relationship between different configurations and also limit their application to more complex scenarios. This paper presents a novel approach which applies to jigsaw puzzles with an arbitrary grid size and dimensionality. We provide a fundamental and generalized principle, that weaker cues are easier to be learned in an unsupervised manner and also transfer better. In the context of puzzle recognition, we use an iterative manner which, instead of solving the puzzle all at once, adjusts the order of the patches in each step until convergence. In each step, we combine both unary and binary features on each patch into a cost function judging the correctness of the current configuration. Our approach, by taking similarity between puzzles into consideration, enjoys a more reasonable way of learning visual knowledge. We verify the effectiveness of our approach in two aspects. First, it is able to solve arbitrarily complex puzzles, including high-dimensional puzzles, that prior methods are difficult to handle. Second, it serves as a reliable way of network initialization, which leads to better transfer performance in a few visual recognition tasks including image classification, object detection, and semantic segmentation.",0
"In this paper we present a methodology for solving jigsaw puzzles using deep neural networks without supervision or human guidance after initial setup. Our algorithm iteratively reorganizes puzzle pieces based on weak spatial constraints and generates new candidate configurations at each step until convergence. At every iteration, it evaluates and selects from among four competing piece match hypotheses using features extracted from raw images. Our experiments demonstrate that our model effectively learns meaningful representations as measured by performance across multiple challenges including classification accuracy, object detection, and cross-dataset generalization. Additionally, we observe consistent improvement over strong baseline models throughout all stages of training and testing. Potential applications of our work extend beyond image reconstruction to other complex search spaces where heuristics and prior knowledge may prove insufficient. Our approach could lead to advancements in fields such as game playing, planning, and robotics while highlighting the potential of unsupervised learning techniques.",1
"Graph convolutional network (GCN) is an emerging neural network approach. It learns new representation of a node by aggregating feature vectors of all neighbors in the aggregation process without considering whether the neighbors or features are useful or not. Recent methods have improved solutions by sampling a fixed size set of neighbors, or assigning different weights to different neighbors in the aggregation process, but features within a feature vector are still treated equally in the aggregation process. In this paper, we introduce a new convolution operation on regular size feature maps constructed from features of a fixed node bandwidth via sampling to get the first-level node representation, which is then passed to a standard GCN to learn the second-level node representation. Experiments show that our method outperforms competing methods in semi-supervised node classification tasks. Furthermore, our method opens new doors for exploring new GCN architectures, particularly deeper GCN models.",0
"Abstract: Recent advances in deep learning have shown that convolutional neural networks (CNNs) can effectively capture spatial dependencies within data. In graph representation learning tasks such as node classification or link prediction, we propose using graph node feature convolution layers to model complex interactions among features of neighboring nodes in graphs. We show through experiments on benchmark datasets that our approach outperforms state-of-the-art methods and provides a more efficient solution compared to current representations based on node attributes only or complete graphs. Additionally, our method improves over traditional spectral graph CNNs by utilizing localized neighborhood features. Our work represents a significant step towards capturing higher-order relationships between graph nodes using end-to-end trainable CNN layers. Keywords: graph convolution, representation learning, network analysis, convolutional neural networks.",1
"Convolutional Neural Networks (CNNs) can learn effective features, though have been shown to suffer from a performance drop when the distribution of the data changes from training to test data. In this paper we analyze the internal representations of CNNs and observe that the representations of unseen data in each class, spread more (with higher variance) in the embedding space of the CNN compared to representations of the training data. More importantly, this difference is more extreme if the unseen data comes from a shifted distribution. Based on this observation, we objectively evaluate the degree of representation's variance in each class via eigenvalue decomposition on the within-class covariance of the internal representations of CNNs and observe the same behaviour. This can be problematic as larger variances might lead to mis-classification if the sample crosses the decision boundary of its class. We apply nearest neighbor classification on the representations and empirically show that the embeddings with the high variance actually have significantly worse KNN classification performances, although this could not be foreseen from their end-to-end classification results. To tackle this problem, we propose Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that significantly reduces the within-class covariance of a DNN's representation, improving performance on unseen test data from a shifted distribution. We empirically evaluate DWCCA on two datasets for Acoustic Scene Classification (DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA significantly improve the network's internal representation, it also increases the end-to-end classification accuracy, especially when the test set exhibits a distribution shift. By adding DWCCA to a VGG network, we achieve around 6 percentage points improvement in the case of a distribution mismatch.",0
"Abstract: This study proposes a novel method of within-class covariance analysis (WCCA) for audio representation learning. WCCA is a powerful technique that allows us to analyze variation within different classes of data. In traditional machine learning tasks, we often focus on identifying patterns and features across multiple class labels. However, by using WCCA specifically for deep within-class analysis, we can gain a deeper understanding of how each individual class varies from itself. This approach has several advantages over existing methods, including improved robustness against noise and other external factors. We apply our proposed method to several real-world applications and show promising results compared to state-of-the-art models. Our work demonstrates the potential of WCCA as a valuable tool for audio signal processing and recognition tasks. Keywords: Within-class covariance analysis, Robust audio representation, Machine learning",1
"The recent success in deep learning has lead to various effective representation learning methods for videos. However, the current approaches for video representation require large amount of human labeled datasets for effective learning. We present an unsupervised representation learning framework to encode scene dynamics in videos captured from multiple viewpoints. The proposed framework has two main components: Representation Learning Network (RL-NET), which learns a representation with the help of Blending Network (BL-NET), and Video Rendering Network (VR-NET), which is used for video synthesis. The framework takes as input video clips from different viewpoints and time, learns an internal representation and uses this representation to render a video clip from an arbitrary given viewpoint and time. The ability of the proposed network to render video frames from arbitrary viewpoints and time enable it to learn a meaningful and robust representation of the scene dynamics. We demonstrate the effectiveness of the proposed method in rendering view-aware as well as time-aware video clips on two different real-world datasets including UCF-101 and NTU-RGB+D. To further validate the effectiveness of the learned representation, we use it for the task of view-invariant activity classification where we observe a significant improvement (~26%) in the performance on NTU-RGB+D dataset compared to the existing state-of-the art methods.",0
"Artificial intelligence (AI) has made significant advances in recent years, fueled by breakthroughs in deep learning and large amounts of data available online. One area where AI still lags behind humans, however, is in video understanding tasks such as object detection, segmentation, and tracking. To close this gap, researchers have been exploring methods that use unsupervised representation learning from raw video frames without relying on pre-labeled bounding boxes or image annotations. In particular, time-aware and view-aware approaches have shown promising results in addressing some of the challenges associated with model training and inference at scale. This paper presents a novel approach called “Time-Aware and View-Aware Video Rendering” which combines temporal attention mechanism and multi-view rendering techniques into an end-to-end trainable system. Our method learns representations directly from pixels using an autoencoder architecture trained with self-supervision signals generated from cycle consistency losses. We evaluate our model on two popular benchmark datasets: DAVIS2017 and YouTube-VOS, showing competitive performance against other state-of-the-art methods across multiple metrics, including mean intersection over union (IOU), frame accuracy (FAcc), and contour similarity (CS). Overall, our proposed framework provides a strong foundation for future work in developing more accurate and efficient models for video understanding tasks.",1
"It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.",0
"This study aimed to investigate whether different neural networks learn the same representation by comparing the behavior and representations learned by three popular architectures (VGG, ResNet, and Inception) on six benchmark datasets spanning computer vision tasks such as image classification, object detection, and segmentation. We found that while there were some differences in performance across models and tasks, they all exhibited similar patterns in terms of accuracy and computational efficiency. However, we observed significant variation in their learned representations, suggesting that the choice of model architecture may have implications for interpretability and generalization ability. Our findings contribute to our understanding of how different neural network structures shape learning representations and highlight areas for future research. Keywords: neural network representation; VGG; ResNet; Inception; deep learning; computer vision.",1
"Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a large-scale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. The labeled dataset is available at: https://osf.io/d45bw/. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.",0
"In recent years, automated vulnerability detection has become increasingly important as software systems continue to grow more complex. One promising approach to automated vulnerability detection involves using deep representation learning techniques to identify patterns and anomalies in source code that may indicate potential security weaknesses. This paper presents a detailed analysis of a novel methodology for implementing such a system.  The proposed method uses pre-trained models based on deep neural networks to learn representations of large amounts of source code data. These representations can then be used to detect known vulnerabilities by identifying patterns and features associated with specific types of flaws. Additionally, the model can flag potentially unseen attacks and support developers in decision making during debugging phases. Finally, we evaluate our system against existing approaches, demonstrating improved accuracy, robustness, and scalability compared to current state-of-the art methods. Our results show promise for the widespread adoption of automated vulnerability detection using deep learning techniques.",1
"Heart rate (HR) is an important physiological signal that reflects the physical and emotional activities of humans. Traditional HR measurements are mainly based on contact monitors, which are inconvenient and may cause discomfort for the subjects. Recently, methods have been proposed for remote HR estimation from face videos. However, most of the existing methods focus on well-controlled scenarios, their generalization ability into less-constrained scenarios are not known. At the same time, lacking large-scale databases has limited the use of deep representation learning methods in remote HR estimation. In this paper, we introduce a large-scale multi-modal HR database (named as VIPL-HR), which contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our VIPL-HR database also contains various variations such as head movements, illumination variations, and acquisition device changes. We also learn a deep HR estimator (named as RhythmNet) with the proposed spatial-temporal representation, which achieves promising results on both the public-domain and our VIPL-HR HR estimation databases. We would like to put the VIPL-HR database into the public domain.",0
"Here is a possible abstract without the title:  This paper presents VIPL-HR, a large multi-modal database for pulse estimation from less-constrained face video. This dataset includes over 24 hours of videos captured using different devices, environments, and lighting conditions. We collected heart rate data simultaneously while recording face videos during physical activity, mental stress, sedentary tasks, and sleep. The database contains multiple modalities including ECG signals, photoplethysmography (PPG) sensors, and oxygen saturation levels. Our benchmark shows that existing algorithms have limited accuracy on our multi-domain diverse datasets. This dataset can serve as an important resource for studying the effects of non-optimal sensor configurations on the robustness and generalization ability of deep learning based methods in the field of contactless physiological signal processing. Overall, this work paves the way towards advancing human behavior understanding and health monitoring using commodity hardware.",1
"Among many unsolved puzzles in theories of Deep Neural Networks (DNNs), there are three most fundamental challenges that highly demand solutions, namely, expressibility, optimisability, and generalisability. Although there have been significant progresses in seeking answers using various theories, e.g. information bottleneck theory, sparse representation, statistical inference, Riemannian geometry, etc., so far there is no single theory that is able to provide solutions to all these challenges. In this work, we propose to engage the theory of differential topology to address the three problems. By modelling the dataset of interest as a smooth manifold, DNNs can be considered as compositions of smooth maps between smooth manifolds. Specifically, our work offers a differential topological view of loss landscape of DNNs, interplay between width and depth in expressibility, and regularisations for generalisability. Finally, in the setting of deep representation learning, we further apply the quotient topology to investigate the architecture of DNNs, which enables to capture nuisance factors in data with respect to a specific learning task.",0
"Artificial neural networks (ANNs) have proven to be powerful tools for solving complex problems across a wide range of domains, including image recognition, natural language processing, robotics, and control systems. However, learning with feedforward neural networks can present challenges due to their inherent nonlinearity, sensitivity to initial conditions, and difficulties in optimization. In this study, we propose a novel differential topological approach that addresses these challenges by using persistent homology to provide insights into the structure of high-dimensional data spaces. We demonstrate how this technique allows us to analyze the stability of solutions as well as identify the most important features underlying each task. Our results suggest that our method leads to better generalization performance compared to traditional methods, providing new perspectives on understanding the workings of deep neural networks. This work has broad implications for machine learning researchers looking to gain deeper insight into how ANNs achieve state-of-the-art results. By enabling greater transparency into the decision-making processes of artificial agents, we hope to pave the way for more explainable, trustworthy, and reliable artificial intelligence systems.",1
"We present a generic and flexible module that encodes region proposals by both their intrinsic features and the extrinsic correlations to the others. The proposed non-local region of interest (NL-RoI) can be seamlessly adapted into different generalized R-CNN architectures to better address various perception tasks. Observe that existing techniques from R-CNN treat RoIs independently and perform the prediction solely based on image features within each region proposal. However, the pairwise relationships between proposals could further provide useful information for detection and segmentation. NL-RoI is thus formulated to enrich each RoI representation with the information from all other RoIs, and yield a simple, low-cost, yet effective module for region-based convolutional networks. Our experimental results show that NL-RoI can improve the performance of Faster/Mask R-CNN for object detection and instance segmentation.",0
"This paper presents a method for cross object perception without bounding box regression that utilizes non-local region of interest (RoI) pooling. Our approach replaces traditional region proposal networks (RPNs) which rely on anchors, with a neural network architecture we call a non-local anchor generator. By predicting dense representations over the entire image space instead of a set of fixed boxes, our model can handle variable aspect ratios and positions within objects. We show results on the COCO dataset where our proposed model outperforms previous methods, including those using RPNs. Additionally, our model achieves state of the art performance among models trained from scratch on the recent Kinetics benchmark.",1
"Self-supervised tasks such as colorization, inpainting and zigsaw puzzle have been utilized for visual representation learning for still images, when the number of labeled images is limited or absent at all. Recently, this worthwhile stream of study extends to video domain where the cost of human labeling is even more expensive. However, the most of existing methods are still based on 2D CNN architectures that can not directly capture spatio-temporal information for video applications. In this paper, we introduce a new self-supervised task called as \textit{Space-Time Cubic Puzzles} to train 3D CNNs using large scale video dataset. This task requires a network to arrange permuted 3D spatio-temporal crops. By completing \textit{Space-Time Cubic Puzzles}, the network learns both spatial appearance and temporal relation of video frames, which is our final goal. In experiments, we demonstrate that our learned 3D representation is well transferred to action recognition tasks, and outperforms state-of-the-art 2D CNN-based competitors on UCF101 and HMDB51 datasets.",0
"In recent years, self-supervised learning has gained popularity as a powerful methodology for training models on large amounts of unlabeled data. This approach utilizes pretext tasks, which provide an auxiliary goal that guides model learning towards representations more robustly than random initialization alone. Previous works have successfully applied such techniques to static images; however, extending them to spatio-temporal domains remains challenging. We address this problem by introducing space-time cubic puzzles: jigsaw-like games designed specifically for self-supervised representation learning on videos. Given randomly masked video clips, our objective requires recovering the original content using only local spatial and temporal cues provided implicitly within the resulting patchwork mosaics. Our method achieves state-of-the-art results among existing approaches on several standard benchmark datasets for action recognition and visual tracking. More importantly, we demonstrate empirically that our learned features transfer effectively across different tasks and outperform those from single-task fine-tuned backbones. Thus, we present evidence supporting the merits of generalizing via self-supervised representation learning on videos rather than adhering solely to task-specific optimization objectives. Finally, through ablation studies, we analyze both how our choice of pretext task impacts performance and why it benefits from incorporating additional intrinsic priors related to smoothness and non-negativity. Overall, our work represents an important step toward enabling larger language models in vision by establishing effective means of leveraging vast quantities of raw sensor input data into high-quality generic representat",1
"Adversarial approach has been widely used for data generation in the last few years. However, this approach has not been extensively utilized for classifier training. In this paper, we propose an adversarial framework for classifier training that can also handle imbalanced data. Indeed, a network is trained via an adversarial approach to give weights to samples of the majority class such that the obtained classification problem becomes more challenging for the discriminator and thus boosts its classification capability. In addition to the general imbalanced classification problems, the proposed method can also be used for problems such as graph representation learning in which it is desired to discriminate similar nodes from dissimilar nodes. Experimental results on imbalanced data classification and on the tasks like graph link prediction show the superiority of the proposed method compared to the state-of-the-art methods.",0
"In real world applications, class imbalances are a common problem faced by machine learning algorithms. Many classification problems have large differences in the number of instances from different classes leading to poor accuracy in predicting minority classes which can cause significant loss in terms of money, time etc. To overcome these issues adversarial training has been proposed where we deliberately oversample or undersample one of the data sets to balance them, however these methods change the original distribution of the data while our method preserves it. Our algorithm proposes using a combination of Generative Adversarial Network (GAN) and Transfer Learning for handling imbalance dataset. We use GAN to increase sample size without losing any important features present in smaller datasets. By generating new synthetic samples the feature representation of the entire dataset improves as well due to the addition of high quality generated images. Then transferring knowledge gained through pre trained models on larger datasets allows us to train a network that generalizes better on the task at hand since large datasets generally contain more variation than small ones. These two techniques allow for better performance compared to other methods in literature. In summary our technique provides improvement over traditional approaches to handle imbalances and maintains the integrity of the input data set.",1
"Joint embeddings between medical imaging modalities and associated radiology reports have the potential to offer significant benefits to the clinical community, ranging from cross-domain retrieval to conditional generation of reports to the broader goals of multimodal representation learning. In this work, we establish baseline joint embedding results measured via both local and global retrieval methods on the soon to be released MIMIC-CXR dataset consisting of both chest X-ray images and the associated radiology reports. We examine both supervised and unsupervised methods on this task and show that for document retrieval tasks with the learned representations, only a limited amount of supervision is needed to yield results comparable to those of fully-supervised methods.",0
"In recent years there has been increasing interest in using machine learning methods to assist medical professionals by processing patient data, such as medical images and reports, into actionable insights. One challenge faced by these systems is that they often require large amounts of labeled training data to achieve acceptable performance. This can make it difficult to build systems capable of handling new types of patient data, since labeling new data typically requires significant expertise from domain experts. We propose a method for unsupervised multimodal representation learning which addresses this problem. Our system uses multiple modalities of patient data including imaging data and text reports, and learns representations of this data without any supervision. Once learned, these representations can then be used to transfer knowledge from one modality to another, allowing the model to effectively handle previously unseen combinations of image and report types. We show through experiments on two distinct medical datasets that our proposed approach outperforms several state-of-the-art baseline models at both uni and multimodal tasks over all evaluation metrics. By leveraging the complementary nature of different forms of patient data, we believe our method represents a step towards building more flexible and powerful medical analysis tools.",1
"Representation learning of pedestrian trajectories transforms variable-length timestamp-coordinate tuples of a trajectory into a fixed-length vector representation that summarizes spatiotemporal characteristics. It is a crucial technique to connect feature-based data mining with trajectory data. Trajectory representation is a challenging problem, because both environmental constraints (e.g., wall partitions) and temporal user dynamics should be meticulously considered and accounted for. Furthermore, traditional sequence-to-sequence autoencoders using maximum log-likelihood often require dataset covering all the possible spatiotemporal characteristics to perform well. This is infeasible or impractical in reality. We propose TREP, a practical pedestrian trajectory representation learning algorithm which captures the environmental constraints and the pedestrian dynamics without the need of any training dataset. By formulating a sequence-to-sequence autoencoder with a spatial-aware objective function under the paradigm of actor-critic reinforcement learning, TREP intelligently encodes spatiotemporal characteristics of trajectories with the capability of handling diverse trajectory patterns. Extensive experiments on both synthetic and real datasets validate the high fidelity of TREP to represent trajectories.",0
"Title: ""Representation Learning of Pedestrian Traffic Patterns using Deep Neural Networks""  Abstract: This study proposes a novel method for representation learning of pedestrian trajectories using actor-critic sequence-to-sequence autoencoders (A2C S2S). A comprehensive dataset consisting of over 48 million data points representing real-world human traffic patterns was used to train deep neural networks that could accurately predict future steps based on past observations. By encoding these observed pedestrian paths into compressed representations, the proposed model was able to effectively capture essential features while minimizing computational cost. Performance evaluations demonstrated that our approach significantly outperformed state-of-the-art baselines in terms of prediction accuracy as well as memory consumption. Our results provide valuable insights into developing more efficient models capable of capturing complex human behavioral patterns in real-time applications such as autonomous driving systems.",1
"Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on large-scale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.",0
"Fast graph representation learning has emerged as a critical technique in modern machine learning due to its ability to effectively model complex data structures such as social networks, knowledge graphs, and scientific collaboration networks. However, many existing approaches require excessively large amounts of computational resources, making them difficult to deploy on large datasets. In our work, we propose a novel approach that leverages adaptive sampling techniques to significantly speed up the process of graph representation learning while maintaining high accuracy. By intelligently selecting a subset of the most informative nodes from the original dataset, our method reduces both computation time and memory usage without sacrificing performance. Our experimental evaluations demonstrate that our approach consistently outperforms state-of-the-art methods across a wide range of benchmarks, including node classification, link prediction, and semi-supervised learning tasks. Overall, our contributions represent a significant step forward towards enabling fast and accurate graph representation learning at scale.",1
"Building agents that can explore their environments intelligently is a challenging open problem. In this paper, we make a step towards understanding how a hierarchical design of the agent's policy can affect its exploration capabilities. First, we design EscapeRoom environments, where the agent must figure out how to navigate to the exit by accomplishing a number of intermediate tasks (\emph{subgoals}), such as finding keys or opening doors. Our environments are procedurally generated and vary in complexity, which can be controlled by the number of subgoals and relationships between them. Next, we propose to measure the complexity of each environment by constructing dependency graphs between the goals and analytically computing \emph{hitting times} of a random walk in the graph. We empirically evaluate Proximal Policy Optimization (PPO) with sparse and shaped rewards, a variation of policy sketches, and a hierarchical version of PPO (called HiPPO) akin to h-DQN. We show that analytically estimated \emph{hitting time} in goal dependency graphs is an informative metric of the environment complexity. We conjecture that the result should hold for environments other than navigation. Finally, we show that solving environments beyond certain level of complexity requires hierarchical approaches.",0
"Exploration can refer both to physical exploration such as space travels and scientific researches; while goal-driven navigation includes tasks where you must reach objectives from source point to destination points by making decisions at junctions. This work examines how complexity arises in goal-driven navigation under uncertainties and incomplete knowledge from multiple perspectives: decision theories, uncertainty models, and heuristics for path planning. We propose new measures to quantify complexity of different types of goal-driven navigation problems based on their intrinsic structures, such as the number of junctions and the average uncertainty of the traversed roads. Empirical evaluations confirm that these measures capture significant differences among typical scenarios. Furthermore, we develop two algorithms leveraging novel tree search techniques based on multi-objective evolutionary computation to solve complex cases of the problem. Extensive computational studies show that our proposed algorithms achieve better solution quality than state-of-the-art methods in many aspects like convergence rate and success probability. In summary, this research advances our understanding of the nature of exploration and provides effective solutions for addressing challenges posed in real applications.",1
"Sufficient physical activity and restful sleep play a major role in the prevention and cure of many chronic conditions. Being able to proactively screen and monitor such chronic conditions would be a big step forward for overall health. The rapid increase in the popularity of wearable devices provides a significant new source, making it possible to track the user's lifestyle real-time. In this paper, we propose a novel unsupervised representation learning technique called activity2vec that learns and ""summarizes"" the discrete-valued activity time-series. It learns the representations with three components: (i) the co-occurrence and magnitude of the activity levels in a time-segment, (ii) neighboring context of the time-segment, and (iii) promoting subject-invariance with adversarial training. We evaluate our method on four disorder prediction tasks using linear classifiers. Empirical evaluation demonstrates that our proposed method scales and performs better than many strong baselines. The adversarial regime helps improve the generalizability of our representations by promoting subject invariant features. We also show that using the representations at the level of a day works the best since human activity is structured in terms of daily routines",0
"This abstract describes adversarial unsupervised representation learning (AURL) as a method to learn representations that encode valuable patterns from time-series data. AURL combines unsupervised learning with generative adversarial networks to identify meaningful features from raw sensor measurements without supervision or labeled examples. These learned representations can then be used for tasks such as activity recognition and anomaly detection. Empirical evaluations demonstrate the effectiveness of AURL compared to several baseline methods across multiple domains and datasets. The contributions of this work are: 1) introducing a novel approach for unsupervised pattern discovery from high-dimensional multivariate time-series data, 2) exploring techniques to improve the stability and interpretability of AURL through regularization and visualization, and 3) demonstrating the potential application of these representations on downstream tasks beyond reconstruction. Overall, AURL opens up new possibilities for learning representations of time-series signals in healthcare, wearables, robotics, control systems, and other areas where unlabeled observations are prevalent but contain important underlying structures.",1
"We introduce GSimCNN (Graph Similarity Computation via Convolutional Neural Networks) for predicting the similarity score between two graphs. As the core operation of graph similarity search, pairwise graph similarity computation is a challenging problem due to the NP-hard nature of computing many graph distance/similarity metrics. We demonstrate our model using the Graph Edit Distance (GED) as the example metric. Experiments on three real graph datasets demonstrate that our model achieves the state-of-the-art performance on graph similarity search.",0
"This paper presents a new approach to graph similarity called convolutional set matching (CSM). Traditional methods for measuring graph similarity involve comparing pairs of nodes or edges across graphs using measures such as Hamming distance or Jaccard similarity. However, these approaches can be computationally expensive and may not capture important structural features of graphs. CSM addresses these issues by representing each graph as a set of feature vectors derived from local neighborhoods of nodes, which captures global structure while reducing computational complexity. An iterative process similar to the hierarchical clustering technique is then used to match sets of feature vectors between two graphs based on their similarity, resulting in a final measure of overall graph similarity. Experimental results demonstrate that CSM outperforms existing techniques on a variety of benchmark datasets, making it a promising method for use in areas such as bioinformatics, social network analysis, and image processing.",1
"Graph classification has recently received a lot of attention from various fields of machine learning e.g. kernel methods, sequential modeling or graph embedding. All these approaches offer promising results with different respective strengths and weaknesses. However, most of them rely on complex mathematics and require heavy computational power to achieve their best performance. We propose a simple and fast algorithm based on the spectral decomposition of graph Laplacian to perform graph classification and get a first reference score for a dataset. We show that this method obtains competitive results compared to state-of-the-art algorithms.",0
"We propose a simple baseline algorithm for graph classification tasks that performs well against state-of-the-art methods while having fewer parameters and faster runtime. Our approach leverages graph convolutional neural networks (GCNNs) which have been pre-trained on large node representation datasets and then fine-tuned using mini-batch gradient descent. Through experiments on several popular benchmark datasets, we demonstrate the effectiveness of our method compared to other algorithms like GCN, SageConv, and FastGCN. Additionally, we show how our method can scale up to larger graphs without significant degradation in performance. Overall, our work shows that a simple GCNN baseline model can achieve competitive results on many graph classification problems, opening up new opportunities for future research in this area.",1
"In this work we explore the generalization characteristics of unsupervised representation learning by leveraging disentangled VAE's to learn a useful latent space on a set of relational reasoning problems derived from Raven Progressive Matrices. We show that the latent representations, learned by unsupervised training using the right objective function, significantly outperform the same architectures trained with purely supervised learning, especially when it comes to generalization.",0
"Abstract: This paper presents a novel methodology for improving generalization performance on abstract reasoning tasks by leveraging disentangled feature representations. In particular, we propose a system that learns a compact and interpretable representation space, where each dimension corresponds to a specific factor underlying task complexity. Through extensive experiments, we demonstrate how our approach can significantly enhance cross-task transferability and reduce test error compared to several state-of-the-art baselines. Our findings highlight the importance of learning disentangled features for efficient adaptation across diverse problem domains. Keywords: Abstract reasoning, Disentangled feature representations, Transfer learning",1
"Extracting actionable insight from Electronic Health Records (EHRs) poses several challenges for traditional machine learning approaches. Patients are often missing data relative to each other; the data comes in a variety of modalities, such as multivariate time series, free text, and categorical demographic information; important relationships among patients can be difficult to detect; and many others. In this work, we propose a novel approach to address these first three challenges using a representation learning scheme based on message passing. We show that our proposed approach is competitive with or outperforms the state of the art for predicting in-hospital mortality (binary classification), the length of hospital visits (regression) and the discharge destination (multiclass classification).",0
"This study aims to address a key challenge faced by healthcare providers: accurately predicting patient outcomes in situations where some data is missing. Missing data can arise from many sources, such as incomplete medical records or patients who decline to provide certain types of information. Despite this prevalence, existing approaches to predictive modeling often struggle to handle missing data effectively, leading to suboptimal results. In this work, we present a novel approach that learns representations of missing data, enabling more accurate predictions of patient outcomes. Our method leverages recent advances in deep learning and natural language processing to generate synthetic data that replaces missing values. We then train our models on these synthesized datasets, significantly improving their ability to make accurate predictions compared to traditional methods. Through extensive experiments using real-world clinical data, we demonstrate the effectiveness of our approach across multiple outcome measures, including mortality rates, length of hospital stay, and readmission probability. Overall, this research represents an important step towards more robust and reliable predictive models in healthcare, which could ultimately lead to better care for patients. By filling a critical gap in knowledge regarding missing data handling techniques, our findings have implications beyond healthcare, highlighting the value of representation learning for tackling missing data challenges in other domains.",1
"We introduce a new scene graph generation method called image-level attentional context modeling (ILAC). Our model includes an attentional graph network that effectively propagates contextual information across the graph using image-level features. Whereas previous works use an object-centric context, we build an image-level context agent to encode the scene properties. The proposed method comprises a single-stream network that iteratively refines the scene graph with a nested graph neural network. We demonstrate that our approach achieves competitive performance with the state-of-the-art for scene graph generation on the Visual Genome dataset, while requiring fewer parameters than other methods. We also show that ILAC can improve regular object detectors by incorporating relational image-level information.",0
"In recent years, there has been growing interest in developing models that can effectively model attention at the image level, rather than just focusing on individual pixels or small regions of interest. One promising approach to achieve this goal is through the use of nested-graph neural networks (NGCNNs), which have shown impressive results in a variety of vision tasks such as image classification, object detection, and semantic segmentation. This paper presents a new method called Image-Level Attentional Context Modeling using NGCNNs (ALC) which utilizes multiple nested graphs at different scales to capture hierarchical contextual relationships within images. By doing so, our method is able to learn more complex representations of scene content beyond traditional feature aggregations approaches like region-based CNNs. We evaluate our proposed method on several benchmark datasets including PASCAL VOC and COCO, showing state-of-the-art performance compared to other recently published methods. Our experiments demonstrate that incorporating attentional mechanisms into the design of deep convolutional networks is key to achieving better visual understanding and recognition capabilities.",1
"While neural networks for learning representation of multi-view data have been previously proposed as one of the state-of-the-art multi-view dimension reduction techniques, how to make the representation discriminative with only a small amount of labeled data is not well-studied. We introduce a semi-supervised neural network model, named Multi-view Discriminative Neural Network (MDNN), for multi-view problems. MDNN finds nonlinear view-specific mappings by projecting samples to a common feature space using multiple coupled deep networks. It is capable of leveraging both labeled and unlabeled data to project multi-view data so that samples from different classes are separated and those from the same class are clustered together. It also uses the inter-view correlation between views to exploit the available information in both the labeled and unlabeled data. Extensive experiments conducted on four datasets demonstrate the effectiveness of the proposed algorithm for multi-view semi-supervised learning.",0
"Title: ""Deep Representation Learning for Multi-view Problems"" Abstract: This paper presents an approach to deep representation learning that can handle multi-view problems using semi-supervised techniques. We propose a novel method based on neural networks that learns robust representations by leveraging both labeled data from one view and unlabeled data from multiple views. Our framework builds upon recent advances in self-training, where we iteratively select confident pseudo-labels on the unlabeled data and train our model using these labels along with the limited amount of supervision available. Our results demonstrate that our method outperforms state-of-the-art approaches across several benchmark datasets, showcasing the effectiveness of our proposed solution for handling multi-view problems in deep representation learning. Title: ""Semi-Supervised Learning for Multi-View Data"" Abstract: In this paper, we present a new approach to semi-supervised learning for multi-view problems using deep neural networks. The goal of our method is to learn robust representations that capture information from multiple views while only having access to limited amounts of labeled data. To achieve this, we design a novel architecture that integrates adversarial training into the self-training loop. Our experiments demonstrate that our method significantly improves over previous state-of-the-art methods for handling multi-view problems in semi-supervised learning, achieving better performance across various benchmark datasets.",1
"We tackle the blackbox issue of deep neural networks in the settings of reinforcement learning (RL) where neural agents learn towards maximizing reward gains in an uncontrollable way. Such learning approach is risky when the interacting environment includes an expanse of state space because it is then almost impossible to foresee all unwanted outcomes and penalize them with negative rewards beforehand. Unlike reverse analysis of learned neural features from previous works, our proposed method \nj{tackles the blackbox issue by encouraging} an RL policy network to learn interpretable latent features through an implementation of a disentangled representation learning method. Toward this end, our method allows an RL agent to understand self-efficacy by distinguishing its influences from uncontrollable environmental factors, which closely resembles the way humans understand their scenes. Our experimental results show that the learned latent factors not only are interpretable, but also enable modeling the distribution of entire visited state space with a specific action condition. We have experimented that this characteristic of the proposed structure can lead to ex post facto governance for desired behaviors of RL agents.",0
"Title: ""A Novel Approach to Enhancing the Efficiency of Reinforcement Learning Agents""  In recent years, reinforcement learning (RL) has emerged as a powerful tool for training agents to perform complex tasks. However, one major challenge facing RL remains the lack of transparency and interpretability in agent decision making. To address this issue, we propose a novel method based on action-conditional $\beta$-Variational Autoencoders (action-conditional $beta$-VAEs), which can generate interpretable state representations that capture underlying relationships between states, actions, and rewards. We show through experiments that our approach significantly improves the performance of deep RL agents across multiple domains. Furthermore, by using action-conditioned latent variables, our model enables better understanding of how different actions contribute to changes in estimated reward values. Our work takes a step towards governing the efficacy of RL agents, allowing for more efficient exploration, improved generalization, and greater insight into the decision-making process. Overall, our research paves the way for more transparent and effective use of RL in real-world applications.",1
"We tackle the problem of audiovisual scene analysis for weakly-labeled data. To this end, we build upon our previous audiovisual representation learning framework to perform object classification in noisy acoustic environments and integrate audio source enhancement capability. This is made possible by a novel use of non-negative matrix factorization for the audio modality. Our approach is founded on the multiple instance learning paradigm. Its effectiveness is established through experiments over a challenging dataset of music instrument performance videos. We also show encouraging visual object localization results.",0
"Automatically identifying objects within audio and visual media can be challenging due to variations in appearance, lighting conditions, and background noise. Separating these objects into distinct categories can be equally difficult, as they may overlap in both space and time. In order to solve these problems, we present a method for extracting audio-visual objects from large collections of videos using weakly-supervised learning. By leveraging existing annotations in other data sets, our algorithm is able to accurately identify and locate objects without requiring explicit labels for each instance. We demonstrate the effectiveness of our approach through extensive experiments on a variety of real-world datasets and show that our model outperforms state-of-the art methods across multiple evaluation metrics. Our work has important applications in fields such as computer vision, multimedia indexing and retrieval, and automated content analysis. Overall, this research paves the way towards more robust and scalable solutions for automatically processing complex audio-visual scenes.",1
"The key to success in machine learning (ML) is the use of effective data representations. Traditionally, data representations were hand-crafted. Recently it has been demonstrated that, given sufficient data, deep neural networks can learn effective implicit representations from simple input representations. However, for most scientific problems, the use of deep learning is not appropriate as the amount of available data is limited, and/or the output models must be explainable. Nevertheless, many scientific problems do have significant amounts of data available on related tasks, which makes them amenable to multi-task learning, i.e. learning many related problems simultaneously. Here we propose a novel and general representation learning approach for multi-task learning that works successfully with small amounts of data. The fundamental new idea is to transform an input intrinsic data representation (i.e., handcrafted features), to an extrinsic representation based on what a pre-trained set of models predict about the examples. This transformation has the dual advantages of producing significantly more accurate predictions, and providing explainable models. To demonstrate the utility of this transformative learning approach, we have applied it to three real-world scientific problems: drug-design (quantitative structure activity relationship learning), predicting human gene expression (across different tissue types and drug treatments), and meta-learning for machine learning (predicting which machine learning methods work best for a given problem). In all three problems, transformative machine learning significantly outperforms the best intrinsic representation.",0
"This paper presents transformative machine learning, a new approach to developing artificial intelligence systems that can adapt to changes in their environment. Traditional machine learning algorithms are limited by their static nature, which makes them unable to cope with dynamic environments. In contrast, transformative machine learning utilizes mechanisms such as metalearning and self-modeling to enable the system to learn how to learn, allowing it to continuously improve its performance over time. We describe the key components of our framework and demonstrate its effectiveness through experiments on several benchmark datasets. Our results show that transformative machine learning outperforms state-of-the-art methods across a range of tasks, illustrating the potential impact of this exciting new area of research.",1
"We examine two fundamental tasks associated with graph representation learning: link prediction and node classification. We present a new autoencoder architecture capable of learning a joint representation of local graph structure and available node features for the simultaneous multi-task learning of unsupervised link prediction and semi-supervised node classification. Our simple, yet effective and versatile model is efficiently trained end-to-end in a single stage, whereas previous related deep graph embedding methods require multiple training steps that are difficult to optimize. We provide an empirical evaluation of our model on five benchmark relational, graph-structured datasets and demonstrate significant improvement over three strong baselines for graph representation learning. Reference code and data are available at https://github.com/vuptran/graph-representation-learning",0
"Abstract: This paper proposes Multi-Task Graph Autoencoders (MTAG), which can encode multiple graphs into one unified representation while preserving their inherent graph structures. MTAG leverages a novel message passing mechanism that captures the relationship between nodes across different graphs, enabling better utilization of structured data. Experimental results on three benchmark datasets show that MTAG outperforms state-of-the-art multi-task learning methods by significant margins, demonstrating its effectiveness in encoding diverse types of graphs such as citation networks, protein interaction networks, and co-authorship networks. These findings suggest that MTAG may hold promise in facilitating research in domains such as bioinformatics, social network analysis, and knowledge discovery. Additionally, our method opens up new opportunities for multi-modal learning using heterogeneous sources of data. Overall, this work represents an important step towards advancing the field of graph autoencoding and paves the way for future studies exploring the intersection of deep learning and graph theory.",1
"A recent method employs 3D voxels to represent 3D shapes, but this limits the approach to low resolutions due to the computational cost caused by the cubic complexity of 3D voxels. Hence the method suffers from a lack of detailed geometry. To resolve this issue, we propose Y^2Seq2Seq, a view-based model, to learn cross-modal representations by joint reconstruction and prediction of view and word sequences. Specifically, the network architecture of Y^2Seq2Seq bridges the semantic meaning embedded in the two modalities by two coupled `Y' like sequence-to-sequence (Seq2Seq) structures. In addition, our novel hierarchical constraints further increase the discriminability of the cross-modal representations by employing more detailed discriminative information. Experimental results on cross-modal retrieval and 3D shape captioning show that Y^2Seq2Seq outperforms the state-of-the-art methods.",0
"In this work we present two novel sequence-to-sequence architectures - Y^2Seq_recon and Y^2Seq_pred - which learn representations that can capture both spatial geometry (i.e., shape) as well as semantic structure from natural language descriptions (i.e., text). These models leverage cross modal supervision through jointly reconstructing/predicting either view point clouds (for reconstruction tasks), or camera poses+view point clouds+ground truth views rendered via a generative network (for prediction tasks), conditioned on input texts. We show these models outperform previous methods on several benchmark datasets. Finally, due to their simple plugandplay architecture, they offer significant speedups without sacrificing performance during training and testing compared to competitors like DALL E, etc.. By doing so, we hope our approach opens up doors towards realtime applications such as robotic manipulation or AR/VR environments where computational efficiency often matters most.",1
"In this paper we present a novel unsupervised representation learning approach for 3D shapes, which is an important research challenge as it avoids the manual effort required for collecting supervised data. Our method trains an RNN-based neural network architecture to solve multiple view inter-prediction tasks for each shape. Given several nearby views of a shape, we define view inter-prediction as the task of predicting the center view between the input views, and reconstructing the input views in a low-level feature space. The key idea of our approach is to implement the shape representation as a shape-specific global memory that is shared between all local view inter-predictions for each shape. Intuitively, this memory enables the system to aggregate information that is useful to better solve the view inter-prediction tasks for each shape, and to leverage the memory as a view-independent shape representation. Our approach obtains the best results using a combination of L_2 and adversarial losses for the view inter-prediction task. We show that VIP-GAN outperforms state-of-the-art methods in unsupervised 3D feature learning on three large scale 3D shape benchmarks.",0
"This paper presents a novel approach for unsupervised representation learning of 3D shapes using Generative Adversarial Networks (GANs). Our method, called ""View Inter-Prediction GAN"" (VIP-GAN), leverages global shape memories learned from multiple views of 3D objects to improve local view predictions. By utilizing unpaired images as input data, VIP-GAN offers an efficient alternative to traditional supervised methods, which require paired image datasets during training. We demonstrate that our model can effectively learn meaningful representations of 3D geometry through extensive experiments on several benchmark datasets. In addition, we provide qualitative results showing significant improvements over existing state-of-the-art techniques in terms of view synthesis accuracy. Overall, VIP-GAN provides a powerful tool for unsupervised learning of 3D shape representations with potential applications in areas such as computer vision, virtual reality, and augmented reality.",1
"Since its introduction, unsupervised representation learning has attracted a lot of attention from the research community, as it is demonstrated to be highly effective and easy-to-apply in tasks such as dimension reduction, clustering, visualization, information retrieval, and semi-supervised learning. In this work, we propose a novel unsupervised representation learning framework called neighbor-encoder, in which domain knowledge can be easily incorporated into the learning process without modifying the general encoder-decoder architecture of the classic autoencoder.In contrast to autoencoder, which reconstructs the input data itself, neighbor-encoder reconstructs the input data's neighbors. As the proposed representation learning problem is essentially a neighbor reconstruction problem, domain knowledge can be easily incorporated in the form of an appropriate definition of similarity between objects. Based on that observation, our framework can leverage any off-the-shelf similarity search algorithms or side information to find the neighbor of an input object. Applications of other algorithms (e.g., association rule mining) in our framework are also possible, given that the appropriate definition of neighbor can vary in different contexts. We have demonstrated the effectiveness of our framework in many diverse domains, including images, text, and time series, and for various data mining tasks including classification, clustering, and visualization. Experimental results show that neighbor-encoder not only outperforms autoencoder in most of the scenarios we consider, but also achieves the state-of-the-art performance on text document clustering.",0
"This should serve as an abstract/summary for the given paper: ""Representation Learning by Reconstructing Neighborhoods""  Paper Title: Representation Learning by Reconstruction of Neighbourhoods Authors: Peter Li et al., arXiv (2023) This paper proposes a novel approach to representation learning that utilizes graph convolutional networks (GCNs) to reconstruct neighborhoods. By training the GCN model on random subsets of nodes in the original dataset, the network learns representations that can accurately predict missing node features. The authors demonstrate the effectiveness of their method through experiments on several benchmark datasets, showing improvements over state-of-the-art methods such as Deep Graph Infomax.  The key insight behind this work is that reconstruction tasks provide meaningful supervision signals for learning robust node representations. Rather than directly optimizing for node classification, which can lead to brittle solutions, the proposed algorithm maximizes the mutual information between local neighborhoods and target node features, while constraining the solution space using temperature scaling.  Experimental results show that the proposed algorithm achieves state-of-the art performance across multiple benchmarks. Moreover, the learned node representations capture relevant topological information, indicating that they generalize well beyond specific applications. These findings have important implications for representation learning on graphs more broadly, suggesting new directions for research into unsupervised learning on complex data structures.  Overall, this paper represents a significant contribution to the field of representation learning on graphs, highlighting the importance of carefully designed reconstruction objectives in guiding the learning process towards meaningful representations. With increasing amounts of data being generated on heterogeneous graphs, th",1
"Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domain-specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.",0
"Title: Cross-Domain Disentangled Representation Learning via Adversarial Training and Spatial Translation Authors: Xinghan Jin, Yiwei Sun, Menglong Zhu, Qingsong Yang, Le Song Cross-domain image synthesis is a challenging task that aims to generate high-quality images from one domain to another by utilizing existing datasets and models trained on large-scale natural images. In order to achieve realistic translations across domains, we propose a method based on adversarial training and spatial translation called CycleGAN+ST (Cycle Generative Adversarial Network plus Spatial Translation) which can capture the underlying structures of data distributions while separating the entangled relationships among multiple features. Experimental results show significant improvements over previous state-of-the-art methods on both qualitative and quantitative evaluations. Moreover, our method has potential applications such as generating informative medical images, improving object detection performance in low-light conditions, and enabling edge devices to process more complex tasks through cloud assistance.",1
"Intrinsically motivated goal exploration processes enable agents to autonomously sample goals to explore efficiently complex environments with high-dimensional continuous actions. They have been applied successfully to real world robots to discover repertoires of policies producing a wide diversity of effects. Often these algorithms relied on engineered goal spaces but it was recently shown that one can use deep representation learning algorithms to learn an adequate goal space in simple environments. However, in the case of more complex environments containing multiple objects or distractors, an efficient exploration requires that the structure of the goal space reflects the one of the environment. In this paper we show that using a disentangled goal space leads to better exploration performances than an entangled goal space. We further show that when the representation is disentangled, one can leverage it by sampling goals that maximize learning progress in a modular manner. Finally, we show that the measure of learning progress, used to drive curiosity-driven exploration, can be used simultaneously to discover abstract independently controllable features of the environment.",0
"In recent years, deep reinforcement learning has made significant progress on complex tasks such as game playing, robotics, and computer vision through the use of learned representations and disentanglement techniques. These methods learn intrinsic goal spaces that can represent meaningful aspects of problem solving in high dimensional state spaces. Despite their successes, these approaches often rely heavily on task specific design choices that may limit generalization across different environments. This work proposes a novel approach called Curiosity Drivens Exploration (CDE) which explores the learnt goal space without any prior knowledge of how well it performs on a given task. By focusing on intrinsically motivated exploration, we aim to discover effective goal spaces for new problems that would otherwise go unnoticed using traditional methods. We demonstrate the effectiveness of our method by applying CDE to learned disentangled goal spaces and showing improvement over previous benchmarks across multiple domains. Our results suggest that intrinsically motivated exploration could become a valuable tool for developing more versatile agents capable of adapting to diverse problem sets.",1
"Recent advances in representation learning on graphs, mainly leveraging graph convolutional networks, have brought a substantial improvement on many graph-based benchmark tasks. While novel approaches to learning node embeddings are highly suitable for node classification and link prediction, their application to graph classification (predicting a single label for the entire graph) remains mostly rudimentary, typically using a single global pooling step to aggregate node features or a hand-designed, fixed heuristic for hierarchical coarsening of the graph structure. An important step towards ameliorating this is differentiable graph coarsening---the ability to reduce the size of the graph in an adaptive, data-dependent manner within a graph neural network pipeline, analogous to image downsampling within CNNs. However, the previous prominent approach to pooling has quadratic memory requirements during training and is therefore not scalable to large graphs. Here we combine several recent advances in graph neural network design to demonstrate that competitive hierarchical graph classification results are possible without sacrificing sparsity. Our results are verified on several established graph classification benchmarks, and highlight an important direction for future research in graph-based neural networks.",0
"In recent years, graph classifiers have become increasingly popular due to their ability to model complex relationships within data sets. However, one major challenge faced by these methods is overfitting, which can result from the high number of parameters present in most graph classifier models. To address this issue, we propose the use of sparse hierarchical graphs for classification tasks. Our approach utilizes a two-stage framework that first constructs a hierarchy of coarse-to-fine graphs based on cluster assignments obtained at each level. Then, a sparse regularization technique is applied to capture essential features while reducing model complexity and preventing overfitting. Experimental results demonstrate the effectiveness of our method across multiple datasets and comparison with state-of-the-art techniques, confirming the superiority of our proposed approach in terms of accuracy, interpretability, and scalability.",1
"The success and generalisation of deep learning algorithms heavily depend on learning good feature representations. In medical imaging this entails representing anatomical information, as well as properties related to the specific imaging setting. Anatomical information is required to perform further analysis, whereas imaging information is key to disentangle scanner variability and potential artefacts. The ability to factorise these would allow for training algorithms only on the relevant information according to the task. To date, such factorisation has not been attempted. In this paper, we propose a methodology of latent space factorisation relying on the cycle-consistency principle. As an example application, we consider cardiac MR segmentation, where we separate information related to the myocardium from other features related to imaging and surrounding substructures. We demonstrate the proposed method's utility in a semi-supervised setting: we use very few labelled images together with many unlabelled images to train a myocardium segmentation neural network. Specifically, we achieve comparable performance to fully supervised networks using a fraction of labelled images in experiments on ACDC and a dataset from Edinburgh Imaging Facility QMRI. Code will be made available at https://github.com/agis85/spatial_factorisation.",0
"Here is how I would write such an abstract: Spatial representations play a crucial role in medical imaging analysis by enabling automatic segmentation algorithms to distinguish between different tissues based on their unique characteristics. One common challenge facing these methods is the limited availability of annotated data required for supervised training. To address this issue, we propose factorising the spatial relationship between pixels into local components that can be learnt from unlabelled datasets in a semi-supervised manner. Our approach builds upon existing work that utilises convolutional neural networks (CNNs) to learn pixelwise relationships using a fully connected layer applied across all image locations. We argue that more powerful representations can be achieved by breaking down the complex nonlinear mapping between input features and segmentation masks into smaller locally correlated tasks, which can then be learned simultaneously. This results in significant improvements in accuracy over traditional approaches while requiring fewer annotations. Furthermore, our methodology allows for interpretability through identification of salient visual patterns. We validate our framework on a challenging problem in cardiac magnetic resonance imaging (MRI), where accurate segmentation of the heart muscle plays a critical role in diagnosing various abnormalities. Our experiments demonstrate that our method outperforms both naive baseline models as well as several state-of-the-art fully and semi-supervised techniques. Overall, our work represents an important contribution towards realising the potential of deep learning for solving problems characterised by small labelling budgets.",1
"Fake news are nowadays an issue of pressing concern, given their recent rise as a potential threat to high-quality journalism and well-informed public discourse. The Fake News Challenge (FNC-1) was organized in 2017 to encourage the development of machine learning-based classification systems for stance detection (i.e., for identifying whether a particular news article agrees, disagrees, discusses, or is unrelated to a particular news headline), thus helping in the detection and analysis of possible instances of fake news. This article presents a new approach to tackle this stance detection problem, based on the combination of string similarity features with a deep neural architecture that leverages ideas previously advanced in the context of learning efficient text representations, document classification, and natural language inference. Specifically, we use bi-directional Recurrent Neural Networks, together with max-pooling over the temporal/sequential dimension and neural attention, for representing (i) the headline, (ii) the first two sentences of the news article, and (iii) the entire news article. These representations are then combined/compared, complemented with similarity features inspired on other FNC-1 approaches, and passed to a final layer that predicts the stance of the article towards the headline. We also explore the use of external sources of information, specifically large datasets of sentence pairs originally proposed for training and evaluating natural language inference methods, in order to pre-train specific components of the neural network architecture (e.g., the RNNs used for encoding sentences). The obtained results attest to the effectiveness of the proposed ideas and show that our model, particularly when considering pre-training and the combination of neural representations together with similarity features, slightly outperforms the previous state-of-the-art.",0
"Title: An Analysis of Stance Detection Techniques in the Fight Against Fake News  Fake news has become one of the most pressing issues facing society today. With the widespread use of social media platforms, unverified claims and sensationalized headlines can spread rapidly, leading to confusion and even panic among the public. In order to combat fake news, researchers have proposed various methods for identifying and debunking false reports. One such approach involves detecting stances towards these news articles - whether they support, challenge or neutralize their veracity.  In this study, we analyze existing techniques for stance detection that combine similarity features and deep representation learning. Our analysis shows that while traditional bag-of-words models have been effective in some cases, more advanced machine learning algorithms such as convolutional neural networks (CNNs) outperform them when used in conjunction with specific preprocessing steps like tokenization and stemming/lemmatization. We investigate how different feature sets impact the performance of different classification algorithms, including Naive Bayes, Support Vector Machines (SVM), Random Forests, k-Nearest Neighbors (k-NN), Decision Trees, and Gradient Boosted Trees (GBT).  Our experiments demonstrate that combining both semantic and surface level characteristics provides significant improvement over using either type alone. Additionally, we show that fine-tuning pre-trained CNN architectures, commonly used in image recognition tasks, improves overall accuracy for stance detection. These findings highlight the importance of utilizing diverse data sources and applying cutting-edge techniques in addressing the growing problem of fake news. By advancing our understanding of stance detection mechanisms, we aim to contribute to a broader effort toward enhancing public discourse through better communication and collaboration online.",1
"A feature learning task involves training models that are capable of inferring good representations (transformations of the original space) from input data alone. When working with limited or unlabelled data, and also when multiple visual domains are considered, methods that rely on large annotated datasets, such as Convolutional Neural Networks (CNNs), cannot be employed. In this paper we investigate different auto-encoder (AE) architectures, which require no labels, and explore training strategies to learn representations from images. The models are evaluated considering both the reconstruction error of the images and the feature spaces in terms of their discriminative power. We study the role of dense and convolutional layers on the results, as well as the depth and capacity of the networks, since those are shown to affect both the dimensionality reduction and the capability of generalising for different visual domains. Classification results with AE features were as discriminative as pre-trained CNN features. Our findings can be used as guidelines for the design of unsupervised representation learning methods within and across domains.",0
"Abstracts should be descriptive of the content of the paper but concise as well, so make sure that any unnecessary detail is left out",1
"The explosive growth of digital images in video surveillance and social media has led to the significant need for efficient search of persons of interest in law enforcement and forensic applications. Despite tremendous progress in primary biometric traits (e.g., face and fingerprint) based person identification, a single biometric trait alone cannot meet the desired recognition accuracy in forensic scenarios. Tattoos, as one of the important soft biometric traits, have been found to be valuable for assisting in person identification. However, tattoo search in a large collection of unconstrained images remains a difficult problem, and existing tattoo search methods mainly focus on matching cropped tattoos, which is different from real application scenarios. To close the gap, we propose an efficient tattoo search approach that is able to learn tattoo detection and compact representation jointly in a single convolutional neural network (CNN) via multi-task learning. While the features in the backbone network are shared by both tattoo detection and compact representation learning, individual latent layers of each sub-network optimize the shared features toward the detection and feature learning tasks, respectively. We resolve the small batch size issue inside the joint tattoo detection and compact representation learning network via random image stitch and preceding feature buffering. We evaluate the proposed tattoo search system using multiple public-domain tattoo benchmarks, and a gallery set with about 300K distracter tattoo images compiled from these datasets and images from the Internet. In addition, we also introduce a tattoo sketch dataset containing 300 tattoos for sketch-based tattoo search. Experimental results show that the proposed approach has superior performance in tattoo detection and tattoo search at scale compared to several state-of-the-art tattoo retrieval algorithms.",0
"This paper presents a novel approach to tattoo image search that combines joint detection and compact representation learning. Traditional approaches to tattoo image search rely on keyword searches and similarity metrics such as Euclidean distance and histograms. However, these methods can be limited in their effectiveness due to difficulties in accurately describing complex tattoo designs using keywords and challenges in measuring similarity across different types of images. Our proposed method addresses these issues by leveraging deep neural networks to learn representations of tattoo images that capture both global features and fine details. These learned representations are then used to efficiently index and retrieve tattoo images from large collections, enabling fast and accurate image search at scale. Experimental results demonstrate the effectiveness of our approach, achieving significant improvements over traditional methods in terms of retrieval accuracy and computational efficiency. Overall, our work represents an important step towards building intelligent systems for browsing, exploring, and discovering tattoo art.",1
"State-of-the-art speaker diarization systems utilize knowledge from external data, in the form of a pre-trained distance metric, to effectively determine relative speaker identities to unseen data. However, much of recent focus has been on choosing the appropriate feature extractor, ranging from pre-trained $i-$vectors to representations learned via different sequence modeling architectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting off-the-shelf metric learning solutions. In this paper, we argue that, regardless of the feature extractor, it is crucial to carefully design a metric learning pipeline, namely the loss function, the sampling strategy and the discrimnative margin parameter, for building robust diarization systems. Furthermore, we propose to adopt a fine-grained validation process to obtain a comprehensive evaluation of the generalization power of metric learning pipelines. To this end, we measure diarization performance across different language speakers, and variations in the number of speakers in a recording. Using empirical studies, we provide interesting insights into the effectiveness of different design choices and make recommendations.",0
"In the following, we present a method for designing an effective metric learning pipeline for speaker diarization, which can handle the complexities involved in accurately identifying individual speakers across multiple channels and overlapping speech segments. We employ deep neural networks (DNNs) to learn representations that capture the unique characteristics of each speaker, such as pitch contours, phonetic content, speaking rate, and dialectal features. Our approach utilizes data augmentation techniques including speed perturbation and time masking, enabling us to effectively train our models on limited amounts of available data without overfitting. Additionally, we propose a novel loss function that maximizes interspeech differences while minimizing intra-speech variations, resulting in more accurate clustering of speech segments into homogeneous groups corresponding to distinct speakers. Extensive experimental evaluations demonstrate that our proposed method achieves superior performance compared to state-of-the-art systems across several challenging datasets. Our results showcase the effectiveness of incorporating speaker information at both feature extraction and clustering stages, highlighting the importance of jointly optimizing these components within a comprehensive metric learning framework. Overall, our work offers valuable insights for advancing research in speaker diarization, paving the way for improved automated transcription and analysis of spoken language data in real-world applications.",1
"Class imbalance is a pervasive issue among classification models including deep learning, whose capacity to extract task-specific features is affected in imbalanced settings. However, the challenges of handling imbalance among a large number of classes, commonly addressed by deep learning, have not received a significant amount of attention in previous studies. In this paper, we propose an extension of the deep over-sampling framework, to exploit automatically-generated abstract-labels, i.e., a type of side-information used in weak-label learning, to enhance deep representation learning against class imbalance. We attempt to exploit the labels to guide the deep representation of instances towards different subspaces, to induce a soft-separation of inherent subtasks of the classification problem. Our empirical study shows that the proposed framework achieves a substantial improvement on image classification benchmarks with imbalanced among large and small numbers of classes.",0
"Despite recent successes of deep learning methods, they often require large amounts of labeled training data, which can be difficult and expensive to obtain. To address this limitation, several weakly supervised approaches have been proposed that use less labeled data or alternative forms of supervision such as image tags or web search results. However, these methods still suffer from class imbalance problems where the majority classes have more examples than minority classes. In our work, we develop a novel approach called Balanced Cross Entropy (BCE) loss which uses a dynamically adjusted threshold to handle class imbalances during training. Our method significantly outperforms previous state-of-the-art methods on a range of benchmark datasets including Cifar-10, Cifar-100, SVHN and STL-10. Moreover, we demonstrate that BCE improves the robustness of models against overfitting by providing regularization through better optimization landscape exploration compared to standard CE and Focal Loss. Additionally, our proposed algorithm is easy to implement and has limited computational overhead while achieving strong performance gains making it practically appealing.",1
"Depth estimation from a single image represents a very exciting challenge in computer vision. While other image-based depth sensing techniques leverage on the geometry between different viewpoints (e.g., stereo or structure from motion), the lack of these cues within a single image renders ill-posed the monocular depth estimation task. For inference, state-of-the-art encoder-decoder architectures for monocular depth estimation rely on effective feature representations learned at training time. For unsupervised training of these models, geometry has been effectively exploited by suitable images warping losses computed from views acquired by a stereo rig or a moving camera. In this paper, we make a further step forward showing that learning semantic information from images enables to improve effectively monocular depth estimation as well. In particular, by leveraging on semantically labeled images together with unsupervised signals gained by geometry through an image warping loss, we propose a deep learning approach aimed at joint semantic segmentation and depth estimation. Our overall learning framework is semi-supervised, as we deploy groundtruth data only in the semantic domain. At training time, our network learns a common feature representation for both tasks and a novel cross-task loss function is proposed. The experimental findings show how, jointly tackling depth prediction and semantic segmentation, allows to improve depth estimation accuracy. In particular, on the KITTI dataset our network outperforms state-of-the-art methods for monocular depth estimation.",0
"Semantic understanding has been shown to greatly improve performance on tasks such as object detection and segmentation by providing explicit knowledge that helps disambiguate ambiguous scenes. Similarly, exploiting geometric structure is crucial for accurate scene understanding, since geometry provides important constraints on what objects could exist where and how they might look. We present a method that successfully combines both types of information to perform high quality semi-supervised monocular depth estimation. Our key insight is to use semantic understanding to constrain the space of possible depth maps consistent with any given RGB image, while we rely on additional supervision from LiDAR data to learn to accurately reconstruct the observed 2d input images. To achieve this goal we train a neural network using our newly introduced loss function, which leverages semantically annotated ground truth depth maps available only at training time. In addition, we introduce two novel techniques to utilize depth range and object size to further regularize the output predictions during testing without any additional annotations. Using standard benchmark datasets, we show that our approach significantly outperforms existing semi-supervised methods, especially when little annotation budget is available.",1
"Edge detection is among the most fundamental vision problems for its role in perceptual grouping and its wide applications. Recent advances in representation learning have led to considerable improvements in this area. Many state of the art edge detection models are learned with fully convolutional networks (FCNs). However, FCN-based edge learning tends to be vulnerable to misaligned labels due to the delicate structure of edges. While such problem was considered in evaluation benchmarks, similar issue has not been explicitly addressed in general edge learning. In this paper, we show that label misalignment can cause considerably degraded edge learning quality, and address this issue by proposing a simultaneous edge alignment and learning framework. To this end, we formulate a probabilistic model where edge alignment is treated as latent variable optimization, and is learned end-to-end during network training. Experiments show several applications of this work, including improved edge detection with state of the art performance, and automatic refinement of noisy annotations.",0
"This abstract should contain the following keywords: ""edge alignment"", ""graph neural networks (GNNs)"", ""simultaneous learning"". In addition, please note that this paper addresses two main limitations of GNNs: oversmoothing and overfitting/underfitting due to limited training data sizes."" Your readers should get an understanding of your work from reading this abstract alone. If you need additional guidance on writing an abstract, I recommend visiting my website at <https://www.abstractacademy.com>. ---------------------------  Abstract ---------------------------  Graph Neural Networks (GNNs) have shown great potential in modeling graph-structured data such as social networks, biological networks, and knowledge graphs. However, they suffer from two major challenges: oversmoothing and underperforming due to limited training data sizes. To address these issues, we propose a novel framework called simultaneous edge alignment and learning (SALE). --------------------------- Edge Alignment And Graph Neural Networks (Gnn): A Comprehensive Review ------------------------------ **This review provides an exhaustive analysis of how graph neural networks (GNNs) can leverage edge alignment techniques for better performance. With insights into preprocessing methods, regularization strategies, and graph convolutional models, this survey highlights recent advances in unifying edge alignment and deep learning for improved accuracy across multiple application domains. By critically evaluating state-of-the art techniques in computational intelligence and machine learning, we provide a comprehensive guide for researchers looking to exploit the synergy between graph representation learning and edge pruning for effective problem solving. Throughout our discussion, we emphasize practical implications while balancing theory and experimentation for readers seeking holistic understanding of modern GNN architectures and their corresponding applications. Ultimately, the aim of this study is to empower experts and new practitioners alike with actionable knowledge that promotes innovative research on GNN extensions capable of tackling contemporary scientific challenges in the era of big data analytics. **",1
"Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces two categories for multi-view representation learning: multi-view representation alignment and multi-view representation fusion. Consequently, we first review the representative methods and theories of multi-view representation learning based on the perspective of alignment, such as correlation-based alignment. Representative examples are canonical correlation analysis (CCA) and its several extensions. Then from the perspective of representation fusion we investigate the advancement of multi-view representation learning that ranges from generative methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to neural network-based methods including multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Further, we also investigate several important applications of multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical foundation and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.",0
"This survey provides an overview of multi-view representation learning methods that have been developed to exploit complementary information from multiple sources or views. We highlight key techniques used in this field, as well as their applications in computer vision tasks such as object recognition, image classification, and video analysis. The goal is to provide researchers new to the area a concise introduction to current practices, while also pointing out promising directions for future work. The abstract should summarize the purpose, scope and main contributions of the paper without going into great detail or including specific examples.  A Survey of Multi-View Representation Learning The use of multiple view representations has become increasingly common in machine learning tasks where each view captures different aspects of data features or can vary based on changes in time, sensor modalities, etc. Effectively leveraging these heterogeneous observations remains challenging due to differences in data distributions, varying quality, missing values, and potentially conflicting information across views. To address these issues, researchers have proposed numerous approaches based on linear algebraic decompositions, probabilistic graphical models, deep neural networks, transfer/multi-task learning, Bayesian model averaging, among others. In particular, we focus here on multi-view representation learning methods which aim at jointly learning task relevant feature representations and conduct comprehensive analyses by evaluating existing solutions through extensive experiments with state-of-the-art baselines using public datasets widely adopted in vision communities. We offer our findings along with insights towards open problems and recommendations for future directions targeted towards both practitioners and researchers alike interested in advancing multi-view representation learning.",1
"This paper aims to develop a new and robust approach to feature representation. Motivated by the success of Auto-Encoders, we first theoretical summarize the general properties of all algorithms that are based on traditional Auto-Encoders: 1) The reconstruction error of the input can not be lower than a lower bound, which can be viewed as a guiding principle for reconstructing the input. Additionally, when the input is corrupted with noises, the reconstruction error of the corrupted input also can not be lower than a lower bound. 2) The reconstruction of a hidden representation achieving its ideal situation is the necessary condition for the reconstruction of the input to reach the ideal state. 3) Minimizing the Frobenius norm of the Jacobian matrix of the hidden representation has a deficiency and may result in a much worse local optimum value. We believe that minimizing the reconstruction error of the hidden representation is more robust than minimizing the Frobenius norm of the Jacobian matrix of the hidden representation. Based on the above analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs), which uses corruption and reconstruction on both the input and the hidden representation. We demonstrate that the proposed model is highly flexible and extensible and has a potentially better capability to learn invariant and robust feature representations. We also show that our model is more robust than Denoising Auto-Encoders (DAEs) for dealing with noises or inessential features. Furthermore, we detail how to train DDAEs with two different pre-training methods by optimizing the objective function in a combined and separate manner, respectively. Comparative experiments illustrate that the proposed model is significantly better for representation learning than the state-of-the-art models.",0
"This research focuses on developing a novel framework that can effectively reconstruct hidden representations from raw data inputs such as images or audio signals. Our proposed method utilizes state-of-the-art deep learning techniques to extract robust features from these inputs, enabling accurate reconstruction even under noisy conditions. Through extensive experiments, we demonstrate the effectiveness of our approach compared to existing methods, showing improved performance in terms of both accuracy and efficiency. Overall, our work has significant implications for applications in computer vision, speech recognition, and other areas where feature extraction plays a crucial role.",1
"In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.",0
"The use of deep learning models has become ubiquitous across many domains due to their impressive ability to learn complex representations from large amounts of data. However, these models can suffer from undesirable behaviors such as poor robustness and lack of interpretability, which hinder their widespread deployment in safety-critical applications. In recent years, there has been significant progress towards developing techniques that improve the reliability and explainability of deep learning systems without sacrificing performance. One promising direction has been adversarial training methods, where deep neural networks (DNNs) are trained on perturbed versions of their inputs designed to cause misclassifications. This enables DNNs to acquire more robust representations that generalize better under distribution shift and noise corruption. In parallel, there has been growing interest in domain adaptation methods that aim at transferring knowledge learned by DNNs in one task to new tasks or even completely different environments. These methods exploit the commonality underlying multiple related problems to make DNNs adapt more efficiently to novel situations. Motivated by these advances, we propose a unified framework called ""Adversarial Domain Adaptation"" (ADA) that jointly learns fairer and more robust representations while explicitly considering the problem of transferability. Our method integrates three key components: adversarial feature augmentation, domain discriminator regularization, and cross-domain consistency constraint. By optimizing our objective function using gradient descent methods, our approach generates new representations that are fairer, less biased, and more resilient to distributional shifts across both labeled source domains and unlabeled target domai",1
"We introduce a new unsupervised representation learning and visualization using deep convolutional networks and self organizing maps called Deep Neural Maps (DNM). DNM jointly learns an embedding of the input data and a mapping from the embedding space to a two-dimensional lattice. We compare visualizations of DNM with those of t-SNE and LLE on the MNIST and COIL-20 data sets. Our experiments show that the DNM can learn efficient representations of the input data, which reflects characteristics of each class. This is shown via back-projecting the neurons of the map on the data space.",0
"Deep learning has revolutionized many fields such as image classification, speech recognition, natural language processing, robotics, computer vision, etc., by achieving state-of-the-art results using multi-layered artificial neural networks. However, due to their complexity, deep neural network architectures can often lead to unexpected behaviors that lack human-like comprehension. In order to bridge this gap between performance and understanding, there have been efforts to design more interpretable models through visualizations, attention maps, gradients, surrogates or ablations. These methods provide insight into which inputs matter most in obtaining the output predictions but they only focus on local interpretability within one layer at a time. Here we propose a new method called ""Deep Neural Maps"" (DNMaps) that provides global interpretability across multiple layers at once for any type of deep neural architecture. DNMap bridges the gap between neurons across different layers and allows us to see how each input influences every single neuron activation throughout all layers simultaneously. Our proposed method is inspired from the classical neural maps technique originally used to analyze insects' brains, where each map corresponds to the influence of a specific sensory stimulus on brain activities. With our approach, we generalize neural mapping techniques for artificial neural networks. We use these maps to uncover previously unknown properties of deep neural architectures. Specifically, we reveal surprising interactions between neurons in early convolutional layers with neurons in later fully connected layers; explain certain failings of current attribution methods; highlight common confusions among similar model architectures; demonstrate insensitivities to certain classes; suggest novel data augmentation possibilities; exhibit strong biases towards high frequency edges; bring to light interdependencies across channels for some normalization techniques; discover hidden semantic structure; showcase emergent hierarchical features; and much mo",1
"Network representation learning (NRL) methods aim to map each vertex into a low dimensional space by preserving the local and global structure of a given network, and in recent years they have received a significant attention thanks to their success in several challenging problems. Although various approaches have been proposed to compute node embeddings, many successful methods benefit from random walks in order to transform a given network into a collection of sequences of nodes and then they target to learn the representation of nodes by predicting the context of each vertex within the sequence. In this paper, we introduce a general framework to enhance the embeddings of nodes acquired by means of the random walk-based approaches. Similar to the notion of topical word embeddings in NLP, the proposed method assigns each vertex to a topic with the favor of various statistical models and community detection methods, and then generates the enhanced community representations. We evaluate our method on two downstream tasks: node classification and link prediction. The experimental results demonstrate that the incorporation of vertex and topic embeddings outperform widely-known baseline NRL methods.",0
"In recent years, network representation learning has become increasingly important due to the widespread availability of large amounts of data describing relationships among entities. Many approaches have been proposed that aim to capture meaningful latent structures from these networks using tensor decomposition methods such as Nonlinear Factor Analysis (NFA), Canonical Polyadic Decomposition (CPD) or Tucker decompositions. These methods allow us to extract low rank approximations of high dimensional tensors which can then serve as effective representations. However, there are several limitations to existing tensor decomposition models, including overfitting to noise, instability issues during optimization, poor scalability, and limited ability to represent complex multilinear interactions. In this work we present Tensorized Network Embeddings (TNE), a new approach to learn embeddings from complex relational data while addressing some of the shortcomings mentioned above. Our method is inspired by state of the art matrix factorization techniques but formulated within a Bayesian probabilistic framework allowing for efficient inference using Variational Bayes. We show via extensive experiments on real world benchmark datasets that our method yields superior results compared to competing alternatives.",1
"Representation learning is typically applied to only one mode of a data matrix, either its rows or columns. Yet in many applications, there is an underlying geometry to both the rows and the columns. We propose utilizing this coupled structure to perform co-manifold learning: uncovering the underlying geometry of both the rows and the columns of a given matrix, where we focus on a missing data setting. Our unsupervised approach consists of three components. We first solve a family of optimization problems to estimate a complete matrix at multiple scales of smoothness. We then use this collection of smooth matrix estimates to compute pairwise distances on the rows and columns based on a new multi-scale metric that implicitly introduces a coupling between the rows and the columns. Finally, we construct row and column representations from these multi-scale metrics. We demonstrate that our approach outperforms competing methods in both data visualization and clustering.",0
"Manifolds are collections of functions that map input spaces into some higher-dimensional spaces where each function can represent a different property or aspect of the input. They have been used successfully as representations for machine learning problems because they provide a simple and efficient way to handle high-dimensional inputs and allow for interpretable models. However, many real world datasets suffer from missing values which complicates their use in manifold learning algorithms. In this work we present co-manifold learning (CML), a new approach to solving the problem of incomplete manifolds by leveraging additional side information available in other related but separate datasets. We first define a set of constraints on the possible combinations of missing and observed data points based on how likely these observations would occur given some statistical model. Then, using techniques developed in recent works, we learn a mapping onto the original dataset that simultaneously satisfies all constraints. This allows us to construct complete, consistent manifolds even in the face of missing data. Our extensive experimental evaluation demonstrates the effectiveness of CML across a variety of benchmark datasets under different scenarios with missing rates ranging from 1% to 99%. Furthermore, our method outperforms state of the art methods such as imputation based approaches and completion via low rank factorization. Finally, we showcase one potential application area: dimensionality reduction for visualizations of large scale scientific simulations, where the availability of extra information makes our approach particularly well suited.",1
"One-class support vector machine (OC-SVM) for a long time has been one of the most effective anomaly detection methods and extensively adopted in both research as well as industrial applications. The biggest issue for OC-SVM is yet the capability to operate with large and high-dimensional datasets due to optimization complexity. Those problems might be mitigated via dimensionality reduction techniques such as manifold learning or autoencoder. However, previous work often treats representation learning and anomaly prediction separately. In this paper, we propose autoencoder based one-class support vector machine (AE-1SVM) that brings OC-SVM, with the aid of random Fourier features to approximate the radial basis kernel, into deep learning context by combining it with a representation learning architecture and jointly exploit stochastic gradient descent to obtain end-to-end training. Interestingly, this also opens up the possible use of gradient-based attribution methods to explain the decision making for anomaly detection, which has ever been challenging as a result of the implicit mappings between the input space and the kernel space. To the best of our knowledge, this is the first work to study the interpretability of deep learning in anomaly detection. We evaluate our method on a wide range of unsupervised anomaly detection tasks in which our end-to-end training architecture achieves a performance significantly better than the previous work using separate training.",0
"Title: Scalable and Interpretable One-class Support Vector Machines with Deep Learning and Random Fourier Features Abstract: This work presents scalable and interpretable one-class support vector machines (OCSVM) using deep learning (DL) techniques with random Fourier features (RFF). OCSVM is typically used to learn from datasets containing only positive examples while assuming that negative examples belong to other classes. However, traditional approaches struggle with scaling issues as they require explicit feature engineering, which can be computationally expensive and may lead to overfitting. To address these limitations, we propose leveraging DL methods with RFF to construct data representations capable of handling high-dimensional data without requiring explicit feature engineering. Our approach maps input variables into a lower dimensional space and employs regularization via kernel methods. We validate our method on several benchmark datasets across different domains demonstrating competitive performance compared to state-of-the-art algorithms. Furthermore, we provide comprehensive analysis to interpret the learned models by visualizing the decision boundaries and identifying key features contributing to classification decisions.",1
"The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons: First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goal-achieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent.",0
"This work proposes learning representations using efficient approximations of the Laplacian operator on graphs in reinforcement learning (RL). We first review the basics of graph theory and present different ways of defining the Laplacian matrix. Then we introduce a new method called the Normalized Graph Laplacian (NGL) which allows us to scale our representation sizes by normalizing against the graph eigenvalues and can improve stability during training. We use deep neural networks as our function approximation methods to obtain policy gradients that allow us to optimize our reward functions more effectively compared to traditional Q-learning algorithms. Finally, we evaluate our approach through simulations and show how it compares favorably against existing methods like Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimization (PPO) and actor critic methods in general, including Asynchronous Advantage Actor Critic (A3C) and Sparse Reinforcement Learning (SAC). Our experiments show the advantages of NGL for stabilizing gradient computations and increasing performance across several benchmark tasks.",1
"State representation learning aims at learning compact representations from raw observations in robotics and control applications. Approaches used for this objective are auto-encoders, learning forward models, inverse dynamics or learning using generic priors on the state characteristics. However, the diversity in applications and methods makes the field lack standard evaluation datasets, metrics and tasks. This paper provides a set of environments, data generators, robotic control tasks, metrics and tools to facilitate iterative state representation learning and evaluation in reinforcement learning settings.",0
"This paper presents a comprehensive toolbox named S-RL Toolbox for State Representation Learning (SRL) research. SRL focuses on modeling the underlying structure of sequential data streams such as videos or texts, while addressing challenges related to changing environments, noisy inputs, partial observability, and irregular time intervals. The main contribution of the proposed toolbox is that it allows users to easily experiment with a wide variety of environments, datasets and evaluation metrics within a single framework. We provide several environments based on classic reinforcement learning problems and real world applications such as video games, robotics and natural language processing tasks. The included datasets have been carefully curated and preprocessed making them ready for immediate use. In addition, we introduce novel evaluation metrics and benchmarks which allow users to compare different models more effectively across multiple tasks and evaluate their performance under different conditions. All components of the S-RL Toolbox are available open source, allowing others to contribute new experiments, environments and datasets. As such, our work provides a step towards building larger communities of researchers working together on the most important questions in state representation learning.",1
"Semantic image parsing, which refers to the process of decomposing images into semantic regions and constructing the structure representation of the input, has recently aroused widespread interest in the field of computer vision. The recent application of deep representation learning has driven this field into a new stage of development. In this paper, we summarize three aspects of the progress of research on semantic image parsing, i.e., category-level semantic segmentation, instance-level semantic segmentation, and beyond segmentation. Specifically, we first review the general frameworks for each task and introduce the relevant variants. The advantages and limitations of each method are also discussed. Moreover, we present a comprehensive comparison of different benchmark datasets and evaluation metrics. Finally, we explore the future trends and challenges of semantic image parsing.",0
"This abstract provides an overview of recent developments in deep learning methods for semantic image parsing, which involves identifying objects, scenes, and their attributes in digital images. Traditional approaches have relied on hand-crafted features and predefined models, but recent advances have shown that end-to-end trainable deep networks can achieve state-of-the art performance across multiple domains. These methods typically learn to predict pixel-wise segmentation maps, object bounding boxes, keypoint locations, or other structured representations from raw input images. Despite these successes, several challenges remain such as limited generalization ability, sensitivity to hyperparameters, high computational cost, and poor interpretability. In view of these shortcomings, we provide a comprehensive review of existing literature and identify promising directions for future research.",1
"Intrinsically motivated goal exploration algorithms enable machines to discover repertoires of policies that produce a diversity of effects in complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous state and action spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered feature space, limiting their autonomy. In this work, we propose to use deep representation learning algorithms to learn an adequate goal space. This is a developmental 2-stage approach: first, in a perceptual learning stage, deep learning algorithms use passive raw sensor observations of world changes to learn a corresponding latent space; then goal exploration happens in a second stage by sampling goals in this latent space. We present experiments where a simulated robot arm interacts with an object, and we show that exploration algorithms using such learned representations can match the performance obtained using engineered representations.",0
"Recent advances have demonstrated that artificial agents can learn complex tasks by interacting with their environments without explicit guidance from human demonstrations. However, these agents often require large amounts of data and computational power to converge on good solutions. Furthermore, these methods may fail altogether if the learned behaviors diverge significantly from those desired by humans. We present a novel method called Unsupervised Learning of Goal Spaces (ULS) which enables agents to learn goals directly from high-level descriptions of tasks specified by humans. ULS uses intrinsic motivation techniques such as curiosity drives exploration to encourage learning beyond immediate task rewards. We evaluate our approach using experiments based on real world robotics scenarios where agents learn several manipulation skills solely from written instructions with zero reward shaping. The results demonstrate that our algorithm enables agents to learn new skills quickly while efficiently selecting actions towards achieving complex goal states. Our work opens up possibilities for agents to better serve human users in unstructured settings by understanding abstract objectives provided by natural language prompts.",1
"This work studies the problem of learning appropriate low dimensional image representations. We propose a generic algorithmic framework, which leverages two classic representation learning paradigms, i.e., sparse representation and the trace quotient criterion. The former is a well-known powerful tool to identify underlying self-explanatory factors of data, while the latter is known for disentangling underlying low dimensional discriminative factors in data. Our developed solutions disentangle sparse representations of images by employing the trace quotient criterion. We construct a unified cost function, coined as the SPARse LOW dimensional representation (SparLow) function, for jointly learning both a sparsifying dictionary and a dimensionality reduction transformation. The SparLow function is widely applicable for developing various algorithms in three classic machine learning scenarios, namely, unsupervised, supervised, and semi-supervised learning. In order to develop efficient joint learning algorithms for maximizing the SparLow function, we deploy a framework of sparse coding with appropriate convex priors to ensure the sparse representations to be locally differentiable. Moreover, we develop an efficient geometric conjugate gradient algorithm to maximize the SparLow function on its underlying Riemannian manifold. Performance of the proposed SparLow algorithmic framework is investigated on several image processing tasks, such as 3D data visualization, face/digit recognition, and object/scene categorization.",0
"In recent years, there has been significant interest in developing methods that can effectively extract low dimensional image representations from large datasets. One promising approach is trace quotient with sparsity priors (TQSP), which combines ideas from linear algebra and machine learning to learn compact and informative image representations. This work presents an analysis of TQSP and demonstrates its effectiveness on several challenging tasks, including object recognition and image retrieval. Our experimental results show that TQSP outperforms state-of-the-art methods in terms of accuracy and efficiency, making it a valuable tool for researchers working in computer vision and pattern recognition. By providing a comprehensive evaluation of TQSP, we hope to inspire further advancements in the field and promote the use of this powerful methodology in real-world applications.",1
"Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN's architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et. al. (2018).",0
"This paper investigates how deep neural network models can produce unintended outputs that are difficult for humans to interpret and make sense of. The authors argue that existing methods used by deep learning practitioners lack sensitivity to parameter values, leading to incomplete explanations of model decisions. They demonstrate through case studies on real datasets that the most commonly applied explanation techniques are only able to capture a limited subset of features present in the inputs, which leads to suboptimal feature attribution maps. Furthermore, their study shows that these methodologies exhibit low stability, often generating substantially different maps each time they are run on the same input data. Finally, they conclude by suggesting that further research into local interpretation of DNNs should aim to develop tools more sensitive to parameter choices within networks.",1
"Deep neural networks, including recurrent networks, have been successfully applied to human activity recognition. Unfortunately, the final representation learned by recurrent networks might encode some noise (irrelevant signal components, unimportant sensor modalities, etc.). Besides, it is difficult to interpret the recurrent networks to gain insight into the models' behavior. To address these issues, we propose two attention models for human activity recognition: temporal attention and sensor attention. These two mechanisms adaptively focus on important signals and sensor modalities. To further improve the understandability and mean F1 score, we add continuity constraints, considering that continuous sensor signals are more robust than discrete ones. We evaluate the approaches on three datasets and obtain state-of-the-art results. Furthermore, qualitative analysis shows that the attention learned by the models agree well with human intuition.",0
"This abstract discusses research related to understanding how recurrent networks can improve human activity recognition (HAR). Research has shown that incorporating continuous attention mechanisms into these models can lead to significant improvements in their ability to accurately classify human activities. However, there is still room for improvement in terms of both model accuracy and computational efficiency. This study aims to address some of those shortcomings by introducing novel architectures that allow for better attention control over time series data, as well as improved training procedures. Experiments conducted on two benchmark datasets demonstrate the effectiveness of our proposed approaches compared to state-of-the-art methods, achieving higher classification accuracies while requiring fewer parameters and training steps. Overall, this work represents a step forward towards more efficient and accurate HAR systems using deep learning techniques.",1
"In this work, we propose an ensemble of classifiers to distinguish between various degrees of abnormalities of the heart using Phonocardiogram (PCG) signals acquired using digital stethoscopes in a clinical setting, for the INTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats SubChallenge. Our primary classification framework constitutes a convolutional neural network with 1D-CNN time-convolution (tConv) layers, which uses features transferred from a model trained on the 2016 Physionet Heart Sound Database. We also employ a Representation Learning (RL) approach to generate features in an unsupervised manner using Deep Recurrent Autoencoders and use Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we utilize an SVM classifier on a high-dimensional segment-level feature extracted using various functionals on short-term acoustic features, i.e., Low-Level Descriptors (LLD). An ensemble of the three different approaches provides a relative improvement of 11.13% compared to our best single sub-system in terms of the Unweighted Average Recall (UAR) performance metric on the evaluation dataset.",0
"In this work we present a novel approach to pathological heart sound classification by combining multiple machine learning methods including transfer learning, semi-supervised learning, and supervised learning. Our method leverages pre-trained convolutional neural networks (CNN) as a feature extractor on small datasets. We use three datasets from different hospitals across two countries with varying class distributions and sample sizes. Our results show that our ensemble model outperforms standalone models trained using each type of data, achieving average accuracy scores above 97%. Moreover, by comparing our performance against state-of-the art methods reported in recent literature, our proposed method surpasses them by significant margins of up to 26%. This study contributes towards reducing diagnostic errors caused by subjective assessments of heart sounds by medical professionals and provides valuable insights into the efficacy of multi-modal machine learning approaches towards developing efficient healthcare decision support systems.",1
"Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recognizing faces in the wild.",0
"This paper presents a novel approach to age-invariant face recognition that leverages disentangled representation learning and photorealistic cross-age face synthesis. We first introduce a new deep convolutional neural network architecture that learns highly interpretable representations, which can then be used to generate diverse, high quality synthetic faces at different ages from the same individual. By training our model on large scale datasets, we demonstrate significant improvements over traditional age-aware face recognition methods, especially in cases where the true identity is difficult to verify based solely on appearance. Our approach achieves state-of-the-art performance across multiple benchmarks, highlighting its effectiveness for both accuracy and generalization. Additionally, by visualizing the learned representations, we provide insights into how these models capture important features that enable more accurate identification and verification of individuals regardless of their facial expressions, poses, lighting conditions, and other variations typically found in real world environments. Overall, our method offers a promising direction for future research towards building intelligent systems capable of handling complex tasks beyond human capabilities.",1
"This paper presents an automatic network adaptation method that finds a ConvNet structure well-suited to a given target task, e.g., image classification, for efficiency as well as accuracy in transfer learning. We call the concept target-aware transfer learning. Given only small-scale labeled data, and starting from an ImageNet pre-trained network, we exploit a scheme of removing its potential redundancy for the target task through iterative operations of filter-wise pruning and network optimization. The basic motivation is that compact networks are on one hand more efficient and should also be more tolerant, being less complex, against the risk of overfitting which would hinder the generalization of learned representations in the context of transfer learning. Further, unlike existing methods involving network simplification, we also let the scheme identify redundant portions across the entire network, which automatically results in a network structure adapted to the task at hand. We achieve this with a few novel ideas: (i) cumulative sum of activation statistics for each layer, and (ii) a priority evaluation of pruning across multiple layers. Experimental results by the method on five datasets (Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable accuracies over the related state-of-the-art techniques while enhancing the computational and storage efficiency of the transferred model.",0
"In recent years, deep learning has revolutionized many fields by enabling powerful representations of data through complex neural networks. However, these networks can require substantial computational resources, which limits their deployment on resource-constrained devices such as smartphones and embedded systems. Therefore, there is a growing interest in designing efficient architectures that can learn high-quality representations while minimizing computation and memory usage.  One promising approach is target aware network adaptation (TANA), where the model adapts its architecture to the specific task at hand rather than using a fixed structure. This allows the network to focus its parameters on important features and reduce redundancy, resulting in more compact models without compromising accuracy.  In this work, we propose a new algorithmic framework for TANA based on meta-learning and attention mechanisms. Our method learns how to prune the architecture dynamically during training, so that only relevant neurons and layers remain active for each task. We demonstrate the effectiveness of our approach on several benchmark datasets across different domains, including image classification, object detection, and language understanding tasks. Experimental results show that our proposed algorithm significantly outperforms state-of-the-art methods in terms of efficiency, scalability, and generalization ability.  Overall, our work represents a significant step towards achieving efficient representation learning under constraints on compute and storage. By combining principles from meta-learning, neuroscience, and computer vision, our framework provides a flexible and effective solution for real-world deployments on edge devices and other platforms.",1
"Abnormal event detection is one of the important objectives in research and practical applications of video surveillance. However, there are still three challenging problems for most anomaly detection systems in practical setting: limited labeled data, ambiguous definition of ""abnormal"" and expensive feature engineering steps. This paper introduces a unified detection framework to handle these challenges using energy-based models, which are powerful tools for unsupervised representation learning. Our proposed models are firstly trained on unlabeled raw pixels of image frames from an input video rather than hand-crafted visual features; and then identify the locations of abnormal objects based on the errors between the input video and its reconstruction produced by the models. To handle video stream, we develop an online version of our framework, wherein the model parameters are updated incrementally with the image frames arriving on the fly. Our experiments show that our detectors, using Restricted Boltzmann Machines (RBMs) and Deep Boltzmann Machines (DBMs) as core modules, achieve superior anomaly detection performance to unsupervised baselines and obtain accuracy comparable with the state-of-the-art approaches when evaluating at the pixel-level. More importantly, we discover that our system trained with DBMs is able to simultaneously perform scene clustering and scene reconstruction. This capacity not only distinguishes our method from other existing detectors but also offers a unique tool to investigate and understand how the model works.",0
"This paper presents a novel method for detecting unknown anomalies in streaming videos using generative energy-based Boltzmann models (GEBMs). GEBMs are powerful mathematical tools that can learn complex patterns from data by modeling the probability distributions over possible outcomes. By leveraging these models, our approach can identify unusual events that deviate significantly from normal behavior without relying on explicit labels or prior knowledge. We evaluate our method using real-world video datasets, demonstrating that it achieves high accuracy while maintaining low computational cost. Our findings showcase the potential of GEBMs as effective anomaly detection solutions, especially in applications where labeled training data may not always be available or easy to obtain.",1
"Multi-view frame reconstruction is an important problem particularly when multiple frames are missing and past and future frames within the camera are far apart from the missing ones. Realistic coherent frames can still be reconstructed using corresponding frames from other overlapping cameras. We propose an adversarial approach to learn the spatio-temporal representation of the missing frame using conditional Generative Adversarial Network (cGAN). The conditional input to each cGAN is the preceding or following frames within the camera or the corresponding frames in other overlapping cameras, all of which are merged together using a weighted average. Representations learned from frames within the camera are given more weight compared to the ones learned from other cameras when they are close to the missing frames and vice versa. Experiments on two challenging datasets demonstrate that our framework produces comparable results with the state-of-the-art reconstruction method in a single camera and achieves promising performance in multi-camera scenario.",0
"Reconstruction with conditional Generative Adversarial Networks (cGAN) has shown great potential for reconstructing frames from unordered collections of views that may have occlusions, missing data points, and other types of uncertainty. In order to fully exploit this potential, we propose using multiple sources of conditioning to improve reconstruction accuracy even further. These additional sources can provide more detailed context regarding scene structure, object geometry, camera motion, texture detail, light direction, etc., thereby helping the generator create higher quality outputs by synthesizing new viewpoints based on the available evidence. To validate our approach, experiments were conducted using two different datasets: one containing real imagery and another consisting of simulated data designed specifically for benchmarking. Results indicate significant improvements over existing methods across both sets of test cases. Our contributions herein include algorithmic innovations as well as demonstration of clear benefits associated with utilizing multi-view conditioned cGANs for frame reconstruction tasks.",1
"In information theory, Fisher information and Shannon information (entropy) are respectively used to quantify the uncertainty associated with the distribution modeling and the uncertainty in specifying the outcome of given variables. These two quantities are complementary and are jointly applied to information behavior analysis in most cases. The uncertainty property in information asserts a fundamental trade-off between Fisher information and Shannon information, which enlightens us the relationship between the encoder and the decoder in variational auto-encoders (VAEs). In this paper, we investigate VAEs in the Fisher-Shannon plane and demonstrate that the representation learning and the log-likelihood estimation are intrinsically related to these two information quantities. Through extensive qualitative and quantitative experiments, we provide with a better comprehension of VAEs in tasks such as high-resolution reconstruction, and representation learning in the perspective of Fisher information and Shannon information. We further propose a variant of VAEs, termed as Fisher auto-encoder (FAE), for practical needs to balance Fisher information and Shannon information. Our experimental results have demonstrated its promise in improving the reconstruction accuracy and avoiding the non-informative latent code as occurred in previous works.",0
"This paper examines Variational Autoencoders (VAEs) from the perspective of information theory and specifically within the framework of the Fisher-Shannon plane. We show that VAEs can be understood as generative models operating near the edge of chaos, where they balance reconstruction accuracy and model complexity. Our analysis demonstrates that this allows VAEs to capture both relevant features and noise present in the data, making them effective at tasks such as density estimation and anomaly detection. Furthermore, we demonstrate how VAEs can be used to optimize an information measure, which we term ""Fisher Information Regularization"", that balances reconstruction error and latent space capacity. Finally, our work contributes towards better understanding the role of regularizers like weight decay, dropout, and early stopping in deep learning by relating them to optimal encoding and decoding on the Fisher-Shannon plane.",1
"While enormous progress has been made to Variational Autoencoder (VAE) in recent years, similar to other deep networks, VAE with deep networks suffers from the problem of degeneration, which seriously weakens the correlation between the input and the corresponding latent codes, deviating from the goal of the representation learning. To investigate how degeneration affects VAE from a theoretical perspective, we illustrate the information transmission in VAE and analyze the intermediate layers of the encoders/decoders. Specifically, we propose a Fisher Information measure for the layer-wise analysis. With such measure, we demonstrate that information loss is ineluctable in feed-forward networks and causes the degeneration in VAE. We show that skip connections in VAE enable the preservation of information without changing the model architecture. We call this class of VAE equipped with skip connections as SCVAE and perform a range of experiments to show its advantages in information preservation and degeneration mitigation.",0
"This is a paper on generative models like Variational Autoencoders (VAEs) that suffer from ""degeneracy"", meaning they become unable to generate plausible data even if given lots of time. We analyze these failures by calculating the amount of information lost through variational inference, especially using Jensen-Shannon divergence combined with cross entropy. Our results show clear correlations, which allows us to predict when generators might degenerate and how to adjust parameters so as to prevent that. Finally, we look at some possible uses of such knowledge towards better image generation quality for visual analysis applications.",1
"This paper investigates conditional generative adversarial networks (cGANs) to overcome a fundamental limitation of using geotagged media for geographic discovery, namely its sparse and uneven spatial distribution. We train a cGAN to generate ground-level views of a location given overhead imagery. We show the ""fake"" ground-level images are natural looking and are structurally similar to the real images. More significantly, we show the generated images are representative of the locations and that the representations learned by the cGANs are informative. In particular, we show that dense feature maps generated using our framework are more effective for land-cover classification than approaches which spatially interpolate features extracted from sparse ground-level images. To our knowledge, ours is the first work to use cGANs to generate ground-level views given overhead imagery and to explore the benefits of the learned representations.",0
"""Generating high-quality ground-level views from overhead imagery is challenging due to occlusions, distortions, and varying viewpoints. In this work, we propose using conditional generative adversarial networks (cGANs) to generate dense ground-level representations from aerial images. Our method leverages recent advances in feature learning and image generation to produce sharp, detailed synthetic ground-level views that accurately reflect their corresponding real-world scenes. We show through extensive evaluation that our approach outperforms traditional methods by generating more accurate and diverse ground-level features while requiring less manual effort. These results have implications for applications such as virtual reality, robotics, and environmental monitoring.""",1
"Talent search and recommendation systems at LinkedIn strive to match the potential candidates to the hiring needs of a recruiter or a hiring manager expressed in terms of a search query or a job posting. Recent work in this domain has mainly focused on linear models, which do not take complex relationships between features into account, as well as ensemble tree models, which introduce non-linearity but are still insufficient for exploring all the potential feature interactions, and strictly separate feature generation from modeling. In this paper, we present the results of our application of deep and representation learning models on LinkedIn Recruiter. Our key contributions include: (i) Learning semantic representations of sparse entities within the talent search domain, such as recruiter ids, candidate ids, and skill entity ids, for which we utilize neural network models that take advantage of LinkedIn Economic Graph, and (ii) Deep models for learning recruiter engagement and candidate response in talent search applications. We also explore learning to rank approaches applied to deep models, and show the benefits for the talent search use case. Finally, we present offline and online evaluation results for LinkedIn talent search and recommendation systems, and discuss potential challenges along the path to a fully deep model architecture. The challenges and approaches discussed generalize to any multi-faceted search engine.",0
"This paper presents work on deep learning techniques that can aid in identifying and ranking potential job candidates using data from social media networks such as LinkedIn. The authors propose models that combine representation learning, which extracts meaningful features from raw data, with convolutional neural nets (CNNs), a type of deep learning algorithm commonly used for image processing tasks but adapted here for text data. These combined methods allow the system to identify relevant information from candidate profiles and rank them according to their suitability for specific roles. Experiments show promising results for these approaches in identifying high quality matches, even surpassing traditional keyword search systems in certain scenarios. Future research directions could focus on expanding the feature extraction component to encompass additional sources of information beyond social media, and further refining the model selection process. Overall, the proposed framework has the potential to greatly improve talent search processes by leveraging modern machine learning techniques.",1
"Scattering networks are a class of designed Convolutional Neural Networks (CNNs) with fixed weights. We argue they can serve as generic representations for modelling images. In particular, by working in scattering space, we achieve competitive results both for supervised and unsupervised learning tasks, while making progress towards constructing more interpretable CNNs. For supervised learning, we demonstrate that the early layers of CNNs do not necessarily need to be learned, and can be replaced with a scattering network instead. Indeed, using hybrid architectures, we achieve the best results with predefined representations to-date, while being competitive with end-to-end learned CNNs. Specifically, even applying a shallow cascade of small-windowed scattering coefficients followed by 1$\times$1-convolutions results in AlexNet accuracy on the ILSVRC2012 classification task. Moreover, by combining scattering networks with deep residual networks, we achieve a single-crop top-5 error of 11.4% on ILSVRC2012. Also, we show they can yield excellent performance in the small sample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. For unsupervised learning, scattering coefficients can be a competitive representation that permits image recovery. We use this fact to train hybrid GANs to generate images. Finally, we empirically analyze several properties related to stability and reconstruction of images from scattering coefficients.",0
"Here are some example summaries of papers titled ""Scattering Networks for Hybrid Representation Learning"". I hope they give you some ideas: 1) In the computer vision field, data augmentation techniques such as rotation, scaling, flipping, cropping etc., have been shown to improve generalization performance by increasing size and complexity of training datasets without incurring additional labeling costs [9]. These techniques generate new samples by applying transformations randomly to existing ones. However, traditional methods are limited to discrete values making them unsuitable to generate high quality representations from continuous valued data such as images and audio signals [22][4] [5]. Data scattering transforms each point into the domain of densely distributed sample points which can then be resampled according to desired distribution[26][8]. This allows generation of arbitrary real value inputs that closely match any arbitrary probability density function [7] [33], allowing synthesis of image like representations even if no actual images exist [6]. Scattering networks take advantage of these properties to learn hybrid representations directly on raw input space rather than hand engineering features first before feeding them through deep neural nets [17][28][29][12][10][13][19]. Such models outperform pure convolutional neural networks (CNN’s) while having fewer parameters [20] making the approach attractive to fields where processing speed and memory usage constraints need consideration[19][22]. In addition since these architectures don’t assume explicit locality in data like CNN’s their parameter sharing nature lends itself well towards parallel computation for further gains in time efficiency.[22][4] Finally because they apply equally well to one shot learning tasks as it does for few/many shot we argue they form a promising alternative to popular metric based algorithms used previously in few/one shot classification problems[29][11]. We demonstrate our claims via experiments performed on both image & audio tasks. Our contributions include :",1
"How to economically cluster large-scale multi-view images is a long-standing problem in computer vision. To tackle this challenge, we introduce a novel approach named Highly-economized Scalable Image Clustering (HSIC) that radically surpasses conventional image clustering methods via binary compression. We intuitively unify the binary representation learning and efficient binary cluster structure learning into a joint framework. In particular, common binary representations are learned by exploiting both sharable and individual information across multiple views to capture their underlying correlations. Meanwhile, cluster assignment with robust binary centroids is also performed via effective discrete optimization under L21-norm constraint. By this means, heavy continuous-valued Euclidean distance computations can be successfully reduced by efficient binary XOR operations during the clustering procedure. To our best knowledge, HSIC is the first binary clustering work specifically designed for scalable multi-view image clustering. Extensive experimental results on four large-scale image datasets show that HSIC consistently outperforms the state-of-the-art approaches, whilst significantly reducing computational time and memory footprint.",0
"In recent years, image clustering has become increasingly important due to the proliferation of images on the internet. However, traditional clustering techniques such as hierarchical clustering algorithms suffer from high computational complexity and scalability issues, especially when dealing with large datasets containing millions or even billions of images. This makes it difficult to accurately cluster all of the images efficiently. To address these limitations, we propose highly-economized multi-view binary compression (HEBC), which effectively reduces the storage and transmission cost by combining multiple views into a single view representation. We then use bitwise operations to compress each image into a compact binary code that can be used to facilitate efficient image retrieval, ranking, annotation, and grouping applications. Our proposed method achieves up to 92% reduction in file size while maintaining low distortion and high accuracy compared to other state-of-the-art methods. Furthermore, HEBC outperforms existing approaches across diverse metrics and performs favorably against popular compression standards including JPEG, BPG, and WebP. Overall, our approach paves the way towards a more economical yet effective solution for handling big multimedia data while preserving quality and reducing computational requirements.",1
"Building behavior profiles of Android applications (apps) with holistic, rich and multi-view information (e.g., incorporating several semantic views of an app such as API sequences, system calls, etc.) would help catering downstream analytics tasks such as app categorization, recommendation and malware analysis significantly better. Towards this goal, we design a semi-supervised Representation Learning (RL) framework named apk2vec to automatically generate a compact representation (aka profile/embedding) for a given app. More specifically, apk2vec has the three following unique characteristics which make it an excellent choice for largescale app profiling: (1) it encompasses information from multiple semantic views such as API sequences, permissions, etc., (2) being a semi-supervised embedding technique, it can make use of labels associated with apps (e.g., malware family or app category labels) to build high quality app profiles, and (3) it combines RL and feature hashing which allows it to efficiently build profiles of apps that stream over time (i.e., online learning). The resulting semi-supervised multi-view hash embeddings of apps could then be used for a wide variety of downstream tasks such as the ones mentioned above. Our extensive evaluations with more than 42,000 apps demonstrate that apk2vec's app profiles could significantly outperform state-of-the-art techniques in four app analytics tasks namely, malware detection, familial clustering, app clone detection and app recommendation.",0
This should not appear in any search engines,1
"The ability to anticipate the future is essential when making real time critical decisions, provides valuable information to understand dynamic natural scenes, and can help unsupervised video representation learning. State-of-art video prediction is based on LSTM recursive networks and/or generative adversarial network learning. These are complex architectures that need to learn large numbers of parameters, are potentially hard to train, slow to run, and may produce blurry predictions. In this paper, we introduce DYAN, a novel network with very few parameters and easy to train, which produces accurate, high quality frame predictions, significantly faster than previous approaches. DYAN owes its good qualities to its encoder and decoder, which are designed following concepts from systems identification theory and exploit the dynamics-based invariants of the data. Extensive experiments using several standard video datasets show that DYAN is superior generating frames and that it generalizes well across domains.",0
This should give me enough context to write the abstract but I may have questions so feel free to ask them before you begin. If there is any part of my request that is unclear please don't hesitate to reach out to clarify your needs as well. You can also share with me the actual text from the paper if that would make your job easier; just let me know how you intend to proceed once we confirm the details. Thank you!,1
"Deep learning algorithms excel at extracting patterns from raw data, and with large datasets, they have been very successful in computer vision and natural language applications. However, in other domains, large datasets on which to learn representations from may not exist. In this work, we develop a novel multimodal CNN-MLP neural network architecture that utilizes both domain-specific feature engineering as well as learned representations from raw data. We illustrate the effectiveness of such network designs in the chemical sciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP network is more accurate than either standalone network designs, and achieves an error classification rate of 0.125 that is 27% lower than the current state-of-the-art. Thus, our work indicates that combining traditional feature engineering with representation learning can be effective, particularly in situations where labeled data is limited.",0
"In recent years, deep neural networks have been widely used in predictive modeling tasks due to their ability to learn complex representations from raw data. However, engineered features can often provide additional context that helps improve model performance. This study proposes a multimodal deep neural network approach combining both learned representations (i.e., convolutional layers) and engineered features (i.e., molecular descriptors) for biodegradability prediction. We evaluated our proposed method on two publicly available datasets containing chemical compounds and showed promising results compared to previous state-of-the-art methods relying solely on either learned representations or engineered features. Our findings suggest that incorporating both types of representations can enhance the accuracy of models for biodegradability predictions. Overall, this work demonstrates the potential benefits of combining different representation modalities to develop more accurate models in computational chemistry applications.",1
"Deep neural networks (DNN) excel at extracting patterns. Through representation learning and automated feature engineering on large datasets, such models have been highly successful in computer vision and natural language applications. Designing optimal network architectures from a principled or rational approach however has been less than successful, with the best successful approaches utilizing an additional machine learning algorithm to tune the network hyperparameters. However, in many technical fields, there exist established domain knowledge and understanding about the subject matter. In this work, we develop a novel furcated neural network architecture that utilizes domain knowledge as high-level design principles of the network. We demonstrate proof-of-concept by developing IL-Net, a furcated network for predicting the properties of ionic liquids, which is a class of complex multi-chemicals entities. Compared to existing state-of-the-art approaches, we show that furcated networks can improve model accuracy by approximately 20-35%, without using additional labeled data. Lastly, we distill two key design principles for furcated networks that can be adapted to other domains.",0
"This paper addresses the problem of designing neural networks that can solve complex problems such as natural language understanding or image recognition by combining multiple specialized subtasks into one system. Our approach builds on insights from artificial intelligence researchers who have proposed methods for integrating knowledge into machine learning algorithms to improve performance. We describe a method called InformationLearning Net (IL-Net) which takes advantage of expert knowledge to guide the design of furcated neural network systems composed of interconnected modules, each addressing a specific task component. Unlike prior work which primarily focused on data integration methods, we instead use expert knowledge directly during model construction in order to achieve better overall performance across the tasks. We show that our framework significantly outperforms strong baselines on several benchmark datasets including CLEF eHealth and SemEval, demonstrating the effectiveness of using domain knowledge during model training for enhanced results on complex challenges where human understanding remains critical for top performance.",1
"The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images.",0
"In recent years there has been a lot of progress made on tasks such as classification with deep neural networks (DNNs). DNNs learn representations that capture abstract concepts without any supervision, thus making them appealing for computer vision applications like image classification. However, most approaches rely on rectified linear unit (ReLU) activation functions which can make training slower than using e.g. sigmoidal activations. We introduce a new activation function for convolutional layers called ""Rot"", which rotates input features during training through multiple angles, before applying the standard ReLU nonlinearity. While the idea of feature augmentation through rotation is not novel, our contribution lies in integrating such augmentation into the forward pass itself. This allows for direct optimization over transformed inputs, removing the need to store multiple versions of the same data point. Since the transformation is performed elementwise we only have to perform one gradient update step per channel to end up at the correct solution -- regardless of how many neurons share weights or even have different batch norm parameters. Our method improves upon baseline performance in terms of accuracy, parameter count and computation speed. On Imagenet the models manage to reach Top-1/Top-5 accuracies of \sim76.2\%/\sim94.1\% after training for merely 1 epoch with significantly fewer FLOPs compared to competing methods. Furthermore, in most cases ""Rot"" outperforms other state-of-the art techniques while training faster and having less parameters.",1
"Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion.",0
"This should highlight the key contributions of your work without summarizing the entire contents. Please use 3rd person point of view throughout the abstract (e.g., instead of ""we show"", write ""the authors show"").  *Authors: Xiangyu Chen, Yirui Wang, Jian Cheng, Zhibin Liang, Mingli Song*  The purpose of our work was to develop a system that could accurately recognize facial expressions while preserving privacy by blurring certain features of the face. We used Variational Generative Adversarial Networks (VGAN) as the basis for our model because they have been shown to effectively generate high quality images. Our approach involves training two separate VGAN models, one for generating new faces with randomized expressions, and another for removing specific parts of the face such as eyes or mouth. These two models were then combined into a single framework where we fine tuned them on a dataset of labeled facial expressions. We evaluated our system using standard metrics in computer vision and found that our method achieved state-of-the-art performance while simultaneously ensuring privacy through feature masking. Overall, our results demonstrate the effectiveness of VGAN-based image representation learning for privacy-preserving facial expression recognition.",1
"Network embedding algorithms are able to learn latent feature representations of nodes, transforming networks into lower dimensional vector representations. Typical key applications, which have effectively been addressed using network embeddings, include link prediction, multilabel classification and community detection. In this paper, we propose BiasedWalk, a scalable, unsupervised feature learning algorithm that is based on biased random walks to sample context information about each node in the network. Our random-walk based sampling can behave as Breath-First-Search (BFS) and Depth-First-Search (DFS) samplings with the goal to capture homophily and role equivalence between the nodes in the network. We have performed a detailed experimental evaluation comparing the performance of the proposed algorithm against various baseline methods, on several datasets and learning tasks. The experiment results show that the proposed method outperforms the baseline ones in most of the tasks and datasets.",0
"Biased Walk has been proposed as a novel approach for representation learning on graphs that combines random walks and biases learned from downstream tasks. By using a biased sampling strategy based on task labels, Biased Walk can capture important substructures while preserving global context. We evaluate our method on several benchmark datasets, including link prediction, node classification, and recommendation systems. Our results show that BiasedWalk outperforms state-of-the art methods for representation learning, such as Node2Vec and DeepWalk, across all tasks and datasets considered. Additionally, we provide an analysis of how different hyperparameters affect performance and demonstrate that our model provides more interpretable representations than competing models by visualizing learned embeddings. Overall, Biased Walk represents a promising new direction for representation learning on graphs.",1
"The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.",0
"Untrained deep neural networks have been observed to produce powerful representations that capture structures in their inputs at multiple levels of abstraction. In contrast, little progress has been made on training convolutional neural networks (CNNs) without task supervision, i.e., fully unsupervised learning. Here we present results that suggest that CNNs trained under weakly-specified conditions can nonetheless learn view invariant mid level visual representations; our method adapts models pre-trained to predict ImageNet class labels by fine tuning them on a new action recognition dataset and by enforcing temporal smoothness constraints during inference. We show that these adapted architectures significantly outperform other approaches in terms of accuracy and generalization across all evaluation metrics including different datasets and network architectures. Our findings provide evidence that the architecture of modern CNNs contains intrinsic mechanisms that enable effective learning from raw image data, even when operating well outside its original design goals and training protocols.",1
"Multi-omic data provides multiple views of the same patients. Integrative analysis of multi-omic data is crucial to elucidate the molecular underpinning of disease etiology. However, multi-omic data has the ""big p, small N"" problem (the number of features is large, but the number of samples is small), it is challenging to train a complicated machine learning model from the multi-omic data alone and make it generalize well. Here we propose a framework termed Multi-view Factorization AutoEncoder with network constraints to integrate multi-omic data with domain knowledge (biological interactions networks). Our framework employs deep representation learning to learn feature embeddings and patient embeddings simultaneously, enabling us to integrate feature interaction network and patient view similarity network constraints into the training objective. The whole framework is end-to-end differentiable. We applied our approach to the TCGA Pan-cancer dataset and achieved satisfactory results to predict disease progression-free interval (PFI) and patient overall survival (OS) events. Code will be made publicly available.",0
"In recent years, there has been significant interest in using integrative analysis methods that incorporate multiple types of data, known as ""multi-omics,"" in order to better understand complex biological systems. One approach to multi-omics integration is through the use of autoencoders, which can learn low-dimensional representations of high-dimensional datasets by reconstructing inputs from latent features. However, traditional autoencoder models may suffer from issues such as poor interpretability, instability during training, and difficulty handling large datasets.  To address these challenges, we propose a new method called the Multi-view Factorization Autoencoder with Network Constraints (MFA). Our model uses factorized representation learning to improve interpretability and stability, and adds network constraints to capture important relationships between variables across different layers of the autoencoder. We apply MFA to several real-world multi-omic datasets and show that our method outperforms state-of-the-art alternatives on metrics such as reconstruction accuracy and clustering performance. Additionally, we demonstrate the utility of MFA for identifying meaningful patterns and subtypes within the data.  Overall, our work represents an important contribution towards improving the scalability and effectiveness of integrative multi-omic analysis methods using machine learning techniques. By combining factorized representation learning and network constraints into a single framework, we have developed a powerful tool that enables researchers to gain insights into complex biological processes at an unprecedented level of detail.",1
"The success of graph embeddings or node representation learning in a variety of downstream tasks, such as node classification, link prediction, and recommendation systems, has led to their popularity in recent years. Representation learning algorithms aim to preserve local and global network structure by identifying node neighborhood notions. However, many existing algorithms generate embeddings that fail to properly preserve the network structure, or lead to unstable representations due to random processes (e.g., random walks to generate context) and, thus, cannot generate to multi-graph problems. In this paper, we propose RECS, a novel, stable graph embedding algorithmic framework. RECS learns graph representations using connection subgraphs by employing the analogy of graphs with electrical circuits. It preserves both local and global connectivity patterns, and addresses the issue of high-degree nodes. Further, it exploits the strength of weak ties and meta-data that have been neglected by baselines. The experiments show that RECS outperforms state-of-the-art algorithms by up to 36.85% on multi-label classification problem. Further, in contrast to baselines, RECS, being deterministic, is completely stable.",0
"In this paper, we propose RECS (Robust Graph Embedding Using Connection Subgraphs), a novel approach that enables more accurate representation of graphs through robust graph embedding techniques based on connection subgraphs. We begin by introducing connection subgraphs, which are small subsets of edges chosen from the original graph at random. These subsets can preserve important connectivity patterns while reducing the complexity of the full graph. Next, we present our three main contributions: a modified algorithm for graph partitioning called Iterative Edge Sampling (IES), a method for connecting high-dimensional node embeddings across multiple partitions using cross-partitions mapping, and a framework for applying adversarial attacks under different perturbation budgets. Our comprehensive evaluation demonstrates RECS consistently outperforms state-of-the-art methods in terms of clustering accuracy, visualization quality, and resistance against various attack scenarios. Overall, our work enhances the understanding of how to effectively utilize random edge sampling for improved graph embedding performance, especially during critical applications where resilience is crucial.",1
"We present compositional nearest neighbors (CompNN), a simple approach to visually interpreting distributed representations learned by a convolutional neural network (CNN) for pixel-level tasks (e.g., image synthesis and segmentation). It does so by reconstructing both a CNN's input and output image by copy-pasting corresponding patches from the training set with similar feature embeddings. To do so efficiently, it makes of a patch-match-based algorithm that exploits the fact that the patch representations learned by a CNN for pixel level tasks vary smoothly. Finally, we show that CompNN can be used to establish semantic correspondences between two images and control properties of the output image by modifying the images contained in the training set. We present qualitative and quantitative experiments for semantic segmentation and image-to-image translation that demonstrate that CompNN is a good tool for interpreting the embeddings learned by pixel-level CNNs.",0
"Deep neural networks have revolutionized computer vision by producing state-of-the-art results across many tasks. While these models often perform well on benchmark datasets, their predictions can remain mysterious due to the lack of explainability and interpretability. In particular, convolutional neural networks (CNNs) used for image classification rely heavily on learn feature representations that are opaque to human understanding. This study proposes Patch Correspondence Analysis (PCA), a methodology for interpreting pixel-level decisions made by deep CNNs trained on image data. Specifically, we introduce a new technique called patch correspondences that allows us to compare individual network activations to ground truth images at unparalleled resolution. Our approach relies on recent advances in visualizing feature maps as heatmaps that highlight which pixels contribute most strongly to each neuron’s activation. By analyzing both localization accuracy and decision boundaries using our proposed PCA framework, we demonstrate how fine-grained knowledge of what specific regions drive classification predictions can aid understanding and potentially improve model robustness against adversarial attacks. We evaluate our PCA method through comprehensive experiments over several popular CNN architectures trained on large-scale image classification datasets such as ImageNet and CIFAR-10/100. Our findings suggest that by carefully studying the spatial interactions within deep CNNs, one can better understand their behavior and make informed design choices toward more transparent machine learning systems.",1
"We investigate the potential of a restricted Boltzmann Machine (RBM) for discriminative representation learning. By imposing the class information preservation constraints on the hidden layer of the RBM, we propose a Signed Laplacian Restricted Boltzmann Machine (SLRBM) for supervised discriminative representation learning. The model utilizes the label information and preserves the global data locality of data points simultaneously. Experimental results on the benchmark data set show the effectiveness of our method.",0
"This paper presents a new approach for learning discriminative representation using signed Laplacian restricted Boltzmann machines (SLRBMs). We propose to learn features that can distinguish different classes by enforcing constraints on the signed connectivity patterns of RBMs. Our method is based on the idea that the graph structure underlying the data plays an important role in feature selection. By incorporating prior knowledge into the model through signed connections, we show that SLRBMs can outperform traditional unrestricted models. Our experiments demonstrate that the proposed approach achieves significant improvements over baseline methods across multiple benchmark datasets. In addition, our analysis reveals insights into the relationships between graph structures and classification performance. Overall, this work provides a novel perspective on learning discriminative representations in deep networks, opening up promising research directions.",1
"Sum-Product Networks (SPNs) are recently introduced deep tractable probabilistic models by which several kinds of inference queries can be answered exactly and in a tractable time. Up to now, they have been largely used as black box density estimators, assessed only by comparing their likelihood scores only. In this paper we explore and exploit the inner representations learned by SPNs. We do this with a threefold aim: first we want to get a better understanding of the inner workings of SPNs; secondly, we seek additional ways to evaluate one SPN model and compare it against other probabilistic models, providing diagnostic tools to practitioners; lastly, we want to empirically evaluate how good and meaningful the extracted representations are, as in a classic Representation Learning framework. In order to do so we revise their interpretation as deep neural networks and we propose to exploit several visualization techniques on their node activations and network outputs under different types of inference queries. To investigate these models as feature extractors, we plug some SPNs, learned in a greedy unsupervised fashion on image datasets, in supervised classification learning tasks. We extract several embedding types from node activations by filtering nodes by their type, by their associated feature abstraction level and by their scope. In a thorough empirical comparison we prove them to be competitive against those generated from popular feature extractors as Restricted Boltzmann Machines. Finally, we investigate embeddings generated from random probabilistic marginal queries as means to compare other tractable probabilistic models on a common ground, extending our experiments to Mixtures of Trees.",0
"Sum-product networks have gained recent attention due to their ability to model complex relationships between variables in large datasets. However, visualization techniques for sum-product networks can struggle to effectively represent these intricate connections. In this paper, we propose several novel methods for depicting sum-product networks using dynamic force-directed layout algorithms that highlight important features while minimizing clutter. We demonstrate how our approach outperforms existing techniques by applying them to real-world data sets such as social network graphs and biochemical reaction pathways. Furthermore, we provide comprehensive experimental results that showcase both the effectiveness and efficiency of our methods. Our work provides valuable insights into the understanding and interpretation of sum-product networks, paving the way for future research in this area.",1
"Network embedding methodologies, which learn a distributed vector representation for each vertex in a network, have attracted considerable interest in recent years. Existing works have demonstrated that vertex representation learned through an embedding method provides superior performance in many real-world applications, such as node classification, link prediction, and community detection. However, most of the existing methods for network embedding only utilize topological information of a vertex, ignoring a rich set of nodal attributes (such as, user profiles of an online social network, or textual contents of a citation network), which is abundant in all real-life networks. A joint network embedding that takes into account both attributional and relational information entails a complete network information and could further enrich the learned vector representations. In this work, we present Neural-Brane, a novel Neural Bayesian Personalized Ranking based Attributed Network Embedding. For a given network, Neural-Brane extracts latent feature representation of its vertices using a designed neural network model that unifies network topological information and nodal attributes; Besides, it utilizes Bayesian personalized ranking objective, which exploits the proximity ordering between a similar node-pair and a dissimilar node-pair. We evaluate the quality of vertex embedding produced by Neural-Brane by solving the node classification and clustering tasks on four real-world datasets. Experimental results demonstrate the superiority of our proposed method over the state-of-the-art existing methods.",0
"Here is a possible abstract:  Networks have become ubiquitous throughout our daily lives, whether we realize it or not. One common use case for networks is as representations of relationships between entities - such as friendships, connections on social media platforms, professional contacts, etc. This type of representation allows for analysis on aggregate network level phenomena such as community detection, centrality measures, degree distribution, etc. However these aggregations are limited in their ability to capture the variability that may exist amongst nodes within a single graph. In particular, attributes associated with each node can influence how they relate to one another, potentially creating unique subgraph structures. These nuances cannot be captured by typical global embeddings of attributed graphs. To better model these types of networks, an approach called attributed network embedding has been proposed which maps each node of an attributed graph into continuous low dimensional space while preserving important pairwise relationship distinctions.  The goal of this work is to improve upon current state of the art attributed network embedding techniques. Specifically, we propose a novel neural network architecture termed ""Neural-Brane"" inspired by traditional diffusion kernels used in graph theory. Our contributions lie in two major areas: firstly, we incorporate deep learning principles via neural activation functions to enable feature extraction during message passing thus improving over limitations present in prior works; secondly, to address scalability concerns faced in current personalised ranking approaches (e.g. DeepWalk), we integrate a probabilistic interpretation via Monte Carlo sampling thus making our method more computationally efficient without losing accuracy. Extensive experiments performed validate the effectiveness of our approach by showing improved performance against several benchmark datasets across varying metrics. We hope this research can serve as foundation for developing better techniques capable of handling complex high dimensi…",1
"Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.",0
"This should provide the reader some insight into the key points of your paper without reading the whole thing. The concept of ""disentanglement"" has been central to recent advances in representation learning for machine intelligence, but there remain significant challenges in scaling disentangling representations across multiple domains of knowledge. In our work, we address these issues by introducing a novel approach called cross-domain latent homology, which allows us to learn highly expressive, interpretable, and transferable representations that can effectively capture complex relationships between features and classes across different domains. Our method leverages the observation that many natural phenomena exhibit deep symmetries, or patterns of latent structure that repeat themselves at multiple scales of abstraction. By exploiting these hidden similarities using powerful mathematical tools from algebraic topology, we develop a new framework for lifelong learning based on unified foundations of data geometry and domain ontology. Our experiments demonstrate the effectiveness and versatility of our system in real-world scenarios such as object recognition, scene understanding, visual question answering, and fine-grained feature matching under extreme conditions. Overall, our findings point towards exciting future directions for pushing the limits of human-AI collaboration through synergies between homotopy theory, scientific discovery, and large-scale machine learning.",1
"Object categories inherently form a hierarchy with different levels of concept abstraction, especially for fine-grained categories. For example, birds (Aves) can be categorized according to a four-level hierarchy of order, family, genus, and species. This hierarchy encodes rich correlations among various categories across different levels, which can effectively regularize the semantic space and thus make prediction less ambiguous. However, previous studies of fine-grained image recognition primarily focus on categories of one certain level and usually overlook this correlation information. In this work, we investigate simultaneously predicting categories of different levels in the hierarchy and integrating this structured correlation information into the deep neural network by developing a novel Hierarchical Semantic Embedding (HSE) framework. Specifically, the HSE framework sequentially predicts the category score vector of each level in the hierarchy, from highest to lowest. At each level, it incorporates the predicted score vector of the higher level as prior knowledge to learn finer-grained feature representation. During training, the predicted score vector of the higher level is also employed to regularize label prediction by using it as soft targets of corresponding sub-categories. To evaluate the proposed framework, we organize the 200 bird species of the Caltech-UCSD birds dataset with the four-level category hierarchy and construct a large-scale butterfly dataset that also covers four level categories. Extensive experiments on these two and the newly-released VegFru datasets demonstrate the superiority of our HSE framework over the baseline methods and existing competitors.",0
"In order to build intelligent systems that can interact with their environment effectively, understanding contextual relationships among entities and actions is crucial. Many methods have been proposed to model hierarchical structures among concepts, but they mostly focus on semantic classification tasks rather than fine-grained recognition. This work addresses these limitations by exploiting fine-grained representation learning and recognition using hierarchical embedding techniques. We show how our method can accurately identify subtle distinctions among similar entities and perform reasoning over them based on natural language prompts, making it possible to train agents that can adapt to dynamic environments and interact with users more naturally. Our experiments demonstrate the effectiveness of our approach compared to state-of-the-art alternatives. By enabling more advanced capabilities such as question answering and zero-shot inference within a task domain, we hope our research will contribute towards developing truly human-like intelligence models. -----",1
"While machine learning approaches to visual emotion recognition offer great promise, current methods consider training and testing models on small scale datasets covering limited visual emotion concepts. Our analysis identifies an important but long overlooked issue of existing visual emotion benchmarks in the form of dataset biases. We design a series of tests to show and measure how such dataset biases obstruct learning a generalizable emotion recognition model. Based on our analysis, we propose a webly supervised approach by leveraging a large quantity of stock image data. Our approach uses a simple yet effective curriculum guided training strategy for learning discriminative emotion features. We discover that the models learned using our large scale stock image dataset exhibit significantly better generalization ability than the existing datasets without the manual collection of even a single label. Moreover, visual representation learned using our approach holds a lot of promise across a variety of tasks on different image and video datasets.",0
"This paper proposes that one must first overcome dataset bias before they can truly study emotions through visuals such as art. In today’s world, many individuals rely on datasets to make important decisions, but those biases can cloud their judgement when studying images. Without understanding how our brains process emotional components, we cannot fully grasp the meaning behind an image, which is why this topic should be discussed further in literature. By unraveling these mysteries, researchers may develop new ways to examine and classify emotion based on visual stimuli, allowing them to expand upon current methods. Furthermore, these insights could potentially lead to better decision making regarding our personal lives and society at large by promoting critical thinking skills in order to overcome dataset bias. Ultimately, this examination seeks to provide readers with insight into how emotional processing affects both scientific progress and everyday life.",1
"The increasing availability of electrocardiogram (ECG) data has motivated the use of data-driven models for automating various clinical tasks based on ECG data. The development of subject-specific models are limited by the cost and difficulty of obtaining sufficient training data for each individual. The alternative of population model, however, faces challenges caused by the significant inter-subject variations within the ECG data. We address this challenge by investigating for the first time the problem of learning representations for clinically-informative variables while disentangling other factors of variations within the ECG data. In this work, we present a conditional variational autoencoder (VAE) to extract the subject-specific adjustment to the ECG data, conditioned on task-specific representations learned from a deterministic encoder. To encourage the representation for inter-subject variations to be independent from the task-specific representation, maximum mean discrepancy is used to match all the moments between the distributions learned by the VAE conditioning on the code from the deterministic encoder. The learning of the task-specific representation is regularized by a weak supervision in the form of contrastive regularization. We apply the proposed method to a novel yet important clinical task of classifying the origin of ventricular tachycardia (VT) into pre-defined segments, demonstrating the efficacy of the proposed method against the standard VAE.",0
"This is a research paper that investigates how learning can be used to create more accurate and effective representations of data. In particular, we focus on a type of data called 12-lead electrograms, which are often used in cardiology to diagnose heart conditions such as ventricular tachycardia. We propose using deep learning techniques to learn representations of these signals that capture important underlying patterns, while separating out irrelevant noise. Our approach involves training a neural network to generate novel examples based on existing ones, allowing us to evaluate its ability to capture meaningful features without overfitting. We then apply our learned representation to a specific task: identifying the region responsible for producing VT episodes. Through extensive experiments and real world testing, we demonstrate significant improvements in accuracy compared to standard approaches. Overall, our work shows great promise for enabling better analysis of physiological signals, improving healthcare applications, particularly in areas where data quality is limited.",1
"Instance-level human parsing towards real-world human analysis scenarios is still under-explored due to the absence of sufficient data resources and technical difficulty in parsing multiple instances in a single pass. Several related works all follow the ""parsing-by-detection"" pipeline that heavily relies on separately trained detection models to localize instances and then performs human parsing for each instance sequentially. Nonetheless, two discrepant optimization targets of detection and parsing lead to suboptimal representation learning and error accumulation for final results. In this work, we make the first attempt to explore a detection-free Part Grouping Network (PGN) for efficiently parsing multiple people in an image in a single pass. Our PGN reformulates instance-level human parsing as two twinned sub-tasks that can be jointly learned and mutually refined via a unified network: 1) semantic part segmentation for assigning each pixel as a human part (e.g., face, arms); 2) instance-aware edge detection to group semantic parts into distinct person instances. Thus the shared intermediate representation would be endowed with capabilities in both characterizing fine-grained parts and inferring instance belongings of each part. Finally, a simple instance partition process is employed to get final results during inference. We conducted experiments on PASCAL-Person-Part dataset and our PGN outperforms all state-of-the-art methods. Furthermore, we show its superiority on a newly collected multi-person parsing dataset (CIHP) including 38,280 diverse images, which is the largest dataset so far and can facilitate more advanced human analysis. The CIHP benchmark and our source code are available at http://sysu-hcp.net/lip/.",0
"This paper presents a novel approach to instance-level human parsing using part grouping networks (PGNs). PGNs have been shown to be effective for fine-grained image understanding tasks such as object detection and segmentation. In our method, we extend PGNs to perform high quality instance-level human parsing by leveraging both bottom-up and top-down features. Our approach achieves state-of-the-art results on challenging benchmark datasets. We believe that the success of our method highlights the power of PGNs for solving complex computer vision problems at the intersection of detection, segmentation, and recognition. Additionally, our work opens up new directions for future research on combining different types of deep network representations within a single model.",1
"Many loss functions in representation learning are invariant under a continuous symmetry transformation. For example, the loss function of word embeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate all word and context embedding vectors. We show that representation learning models for time series possess an approximate continuous symmetry that leads to slow convergence of gradient descent. We propose a new optimization algorithm that speeds up convergence using ideas from gauge theory in physics. Our algorithm leads to orders of magnitude faster convergence and to more interpretable representations, as we show for dynamic extensions of matrix factorization and word embedding models. We further present an example application of our proposed algorithm that translates modern words into their historic equivalents.",0
"Recent advances in optimization methods have enabled improved accuracy and efficiency in solving models that exhibit symmetry breaking. This has important applications in physics and materials science. In particular, we discuss improvements to the ""continuation method"", which allows one to find solutions that depend on a continuous parameter. The approach leverages modern numerical techniques such as automatic differentiation and trust region optimization. Results demonstrate significantly faster convergence compared to traditional discretization schemes. Applications to problems arising from condensed matter systems are considered, including magnetic phase transitions and lattice dynamics. Implications for future research directions are discussed. Overall, our work presents novel tools for addressing complex models that involve symmetry breaking.",1
"Denoising autoencoders (DAEs) have proven useful for unsupervised representation learning, but a thorough theoretical understanding is still lacking of how the input noise influences learning. Here we develop theory for how noise influences learning in DAEs. By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics. We verify our theoretical predictions with simulations as well as experiments on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inputs while learning to reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics. We also show that our theoretical predictions approximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs.",0
"This is an excellent opportunity for you to learn how to write a scientific abstract for academic papers. Here is my advice:  Firstly, the most important thing is that your abstract should accurately reflect the content and findings of the paper. So make sure you have read the full text carefully and highlighted/noted down any key points before attempting to summarize them.  Secondly, keep the language formal but simple - avoid overcomplicating things! Don't use technical jargon unless necessary (e.g. explain briefly what autoencoders are if they aren't well known outside specialized circles). Try to think broadly about the implications beyond just the specifics of your research question - could your findings apply more widely? How might someone outside your field react if they read only the abstract? If there's something exciting or counterintuitive here then emphasize it to pique interest.  Lastly, follow standard formatting guidelines e.g. maximum length = ~250 words, brief summary paragraph followed by list of keywords describing the contents - look up examples online first as different fields may vary slightly on some details. Good luck!  Best regards",1
"We examine two fundamental tasks associated with graph representation learning: link prediction and semi-supervised node classification. We present a novel autoencoder architecture capable of learning a joint representation of both local graph structure and available node features for the multi-task learning of link prediction and node classification. Our autoencoder architecture is efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification, whereas previous related methods require multiple training steps that are difficult to optimize. We provide a comprehensive empirical evaluation of our models on nine benchmark graph-structured datasets and demonstrate significant improvement over related methods for graph representation learning. Reference code and data are available at https://github.com/vuptran/graph-representation-learning",0
"This paper presents a new approach to making predictions on graphs using autoencoders. We propose a novel graph embedding technique that utilizes an autoencoder network to capture the underlying structure of the data. Our method is able to effectively learn representations of complex graphs while still achieving state-of-the-art prediction accuracy. Experimental results demonstrate the effectiveness of our proposed framework across several real-world datasets. Our work offers a promising solution for predictive modeling tasks involving large-scale networks, where traditional methods may fall short.",1
"Deeper convolutional neural networks provide more capacity to approximate complex mapping functions. However, increasing network depth imposes difficulties on training and increases model complexity. This paper presents a new nonlinear computational layer of considerably high capacity to the deep convolutional neural network architectures. This layer performs a set of comprehensive convolution operations that mimics the overall function of the human visual system (HVS) via focusing on learning structural information in its input. The core of its computations is evaluating the components of the structural similarity metric (SSIM) in a setting that allows the kernels to learn to match structural information. The proposed SSIMLayer is inherently nonlinear and hence, it does not require subsequent nonlinear transformations. Experiments conducted on CIFAR-10 benchmark demonstrates that the SSIMLayer provides better convergence than the traditional convolutional layer, bypasses the need for nonlinear transformations and shows more robustness against noise perturbations and adversarial attacks.",0
"SSIM (Structural similarity index) as a non-linear block similarity metric has been used successfully to guide deep convolutional neural networks (CNNs), resulting in improvements over popular linear metrics such as L2. However, training these models remains challenging due to vanishing gradients and sensitivity to hyperparameters. In our work, we propose SSIMLayer, which integrates robust batch normalization into a novel architecture that efficiently approximates local spatially adaptive weights using a compact set of parameters within each layer. Our approach addresses both issues by providing more accurate gradient estimates during backpropagation while allowing for efficient computation. Experimental results show that the proposed method consistently outperforms existing approaches across datasets.",1
"Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level semantic features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).",0
"This paper explores how artificial intelligence can be used to learn spatio-temporal features from video data while also making speed vs accuracy trade offs that optimize performance based on application requirements. We use a deep learning approach called recurrent convolutional neural networks (RNN) which allows us to model both spatial and temporal dependencies in videos. In addition, we propose two novel approaches for efficient feature learning, namely ""Learning to Accelerate"" which accelerates the inference process through hardware optimization techniques such as GPU acceleration and ""Early Exit Networks"" which uses multiple subnetworks to make speed vs accuracy decisions at runtime. Our experiments show that our methods significantly improve prediction accuracy compared to state-of-the-art models while still achieving fast inference speeds. Finally, we demonstrate the effectiveness of our methodology by applying it to challenging real world datasets. Overall, this work presents new insights into spatiotemporal feature learning in AI and provides practical solutions for optimizing performance in computer vision applications.",1
"We propose a new clustering method based on optimal transportation. We solve optimal transportation with variational principles, and investigate the use of power diagrams as transportation plans for aggregating arbitrary domains into a fixed number of clusters. We iteratively drive centroids through target domains while maintaining the minimum clustering energy by adjusting the power diagrams. Thus, we simultaneously pursue clustering and the Wasserstein distances between the centroids and the target domains, resulting in a measure-preserving mapping. We demonstrate the use of our method in domain adaptation, remeshing, and representation learning on synthetic and real data.",0
"In recent years, clustering techniques have become increasingly popular due to their ability to group similar data points together while reducing noise and irrelevant features. However, traditional clustering methods suffer from limitations such as sensitivity to initialization, nonconvexity, and difficulty in choosing appropriate parameters. One approach that addresses these issues is variational clustering, which has been shown to produce good results on several real-world datasets. Here we propose a new method called Variational Wasserstein Clustering (VWC) that combines variational inference with optimal transport theory in order to overcome some of these challenges. Specifically, VWC optimizes a cost function based on both the Kullback-Leibler divergence and the Earth Mover’s Distance, allowing us to learn a low dimensional representation of our data while preserving salient clusters. We demonstrate through experiments on benchmark datasets that VWC outperforms state-of-the-art alternatives across multiple evaluation metrics. Our work contributes to the growing literature on effective clustering algorithms by introducing a novel optimization framework that strikes a balance between efficiency and accuracy. Overall, VWC represents a promising direction for future research in unsupervised learning.",1
"In this report we propose a classification technique for skin lesion images as a part of our submission for ISIC 2018 Challenge in Skin Lesion Analysis Towards Melanoma Detection. Our data was extracted from the ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge datasets. The features are extracted through a Convolutional Neural Network, in our case ResNet50 and then using these features we train a DeepForest, having cascading layers, to classify our skin lesion images. We know that Convolutional Neural Networks are a state-of-the-art technique in representation learning for images, with the convolutional filters learning to detect features from images through backpropagation. These features are then usually fed to a classifier like a softmax layer or other such classifiers for classification tasks. In our case we do not use the traditional backpropagation method and train a softmax layer for classification. Instead, we use Deep Forest, a novel decision tree ensemble approach with performance highly competitive to deep neural networks in a broad range of tasks. Thus we use a ResNet50 to extract the features from skin lesion images and then use the Deep Forest to classify these images. This method has been used because Deep Forest has been found to be hugely efficient in areas where there are only small-scale training data available. Also as the Deep Forest network decides its complexity by itself, it also caters to the problem of dataset imbalance we faced in this problem.",0
"In this research study, we propose using disease classification within dermascopic images using deep learning techniques. We aimed at extracting relevant features from these medical images via pretrained models such as ResNet50 before training our deep forest model on labeled data sets. Our goal was to establish an accurate methodology that can potentially improve diagnosis accuracy among patients suspected of having skin diseases. This research has significant implications given that skin conditions affect billions worldwide annually, making early detection crucial. With this framework, healthcare practitioners could benefit significantly through improved decision support systems equipped with artificial intelligence capabilities. Furthermore, there exist few comparative studies conducted thus far on classifying melanoma which is another reason why we were motivated to address this problem. Through experimentation, we obtained promising results, demonstrating proof of concept. Nonetheless, future work remains necessary, particularly exploring different feature extraction techniques along with hyperparameter tuning to improve overall performance metrics even further. Overall, this research constitutes a fundamental step towards integrating machine learning into modern medicine which could lead to more precise diagnostics and better patient outcomes.",1
"Recently, image-to-image translation has been made much progress owing to the success of conditional Generative Adversarial Networks (cGANs). And some unpaired methods based on cycle consistency loss such as DualGAN, CycleGAN and DiscoGAN are really popular. However, it's still very challenging for translation tasks with the requirement of high-level visual information conversion, such as photo-to-caricature translation that requires satire, exaggeration, lifelikeness and artistry. We present an approach for learning to translate faces in the wild from the source photo domain to the target caricature domain with different styles, which can also be used for other high-level image-to-image translation tasks. In order to capture global structure with local statistics while translation, we design a dual pathway model with one coarse discriminator and one fine discriminator. For generator, we provide one extra perceptual loss in association with adversarial loss and cycle consistency loss to achieve representation learning for two different domains. Also the style can be learned by the auxiliary noise input. Experiments on photo-to-caricature translation of faces in the wild show considerable performance gain of our proposed method over state-of-the-art translation methods as well as its potential real applications.",0
"In recent years, there has been significant progress in computer vision research, particularly in the field of unpaired photo-to-caricature translation (PCT). This technique involves transforming realistic images into caricatures while preserving their identities and emotions. Despite advances made by previous works, PCT methods still face challenges in dealing with faces in the wild due to variations in lighting conditions, poses, and facial expressions that make the task more complex.  This study presents a novel method for tackling the problem of translating photos of faces in the wild into corresponding caricatures. Our approach adopts a two-stage strategy where we first extract the identity features from real photographs using a pretrained face recognition model. Then, we generate stylized caricature drawings based on the extracted features using a deep convolutional neural network (CNN) trained on paired data. Finally, we apply our method to create caricatures of celebrities taken from Google Images under varying conditions and compare them against manually created caricatures from professional artists. Experimental results demonstrate the effectiveness of our proposed method, outperforming existing state-of-the-art techniques in terms of similarity and visual quality.  Our work represents a significant step towards automating the process of creating high-quality caricatures from photos of faces in the wild. With future improvements in training data collection and CNN architectures, we envision even greater accuracy and creativity in the world of digital art generation.",1
"In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans -- a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.",0
"This paper investigates how to learn plannable representations by using GANs to generate data that contains causality. We use a new architecture called InfoGAN which can capture information about objects and their relationships without relying on object labels. We then apply methods from inverse reinforcement learning (IRL) to infer action policies directly from these high-level representations. Our experiments show that InfoGAN outperforms other state-of-the art methods when used as a policy-inference engine, achieving up to 2x improvement over existing approaches. Furthermore, we demonstrate that our learned representations generalize across tasks and domains. Overall, we believe that our work presents a major step towards solving the problem of learning plannable representations.",1
"Visual question answering (VQA) models respond to open-ended natural language questions about images. While VQA is an increasingly popular area of research, it is unclear to what extent current VQA architectures learn key semantic distinctions between visually-similar images. To investigate this question, we explore a reformulation of the VQA task that challenges models to identify counterexamples: images that result in a different answer to the original question. We introduce two methods for evaluating existing VQA models against a supervised counterexample prediction task, VQA-CX. While our models surpass existing benchmarks on VQA-CX, we find that the multimodal representations learned by an existing state-of-the-art VQA model do not meaningfully contribute to performance on this task. These results call into question the assumption that successful performance on the VQA benchmark is indicative of general visual-semantic reasoning abilities.",0
"Title: ""On the Other Hand"" Authors: [Your Name Here], [Second Author], etc. Abstract This work examines visual question answering (VQA) systems, specifically looking at how they deal with negative examples or counterarguments. While VQAs have shown impressive performance on benchmarks like VQAv2, their accuracy can suffer when presented with questions that challenge their underlying assumptions. By analyzing these types of questions, we can gain insight into which aspects of VQA models struggle most with negativity. We then provide recommendations for future improvements, such as incorporating more diverse training data and developing methods to identify negative cases during inference. Finally, we discuss potential applications where counterexamples may play a particularly important role, like legal reasoning or detecting biases. Keywords: visual question answering; counterexample detection; machine learning Title: ""On the Other Side"" Authors: [Your Name Here], [Second Author], etc. Abstract This study explores visual question answering (VQA) algorithms, focusing on how they handle exceptional instances and negative scenarios. Although VQA platforms have exhibited remarkable results on trials such as VQAv2, their precision decreases considerably when faced with queries that dispute their fundamental suppositions. Through evaluating these categories of questions, we can uncover which components of VQA architectures fail to cope well with unfavourable situations. Subsequently, our analysis presents proposals for enhancements, including augmenting heterogeneous training sets and formulating techniques that can distinguish adverse circumstances during evaluation. Ultimately, we delve into several uses where opposites could constitute critical elements, among them judicial reasoning and identifying prejudices. Key terms: visual query addressing; opposite instance spotting; artificial intellect.",1
"Knowledge graphs have emerged as an important model for studying complex multi-relational data. This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources. An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications. To this end, we propose LinkNBed, a deep relational learning framework that learns entity and relationship representations across multiple graphs. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure. Experiments on link prediction and entity linkage demonstrate substantial improvements over the state-of-the-art relational learning approaches.",0
"This paper presents LinkNBed, an approach that learns from large scale structured knowledge graphs by leveraging multi-graph representation learning techniques. The proposed method enables deep model training on large knowledge sources such as Freebase or Wikipedia, which contain many entities without explicit connections or edges. By constructing multiple interconnected graphs based on contextual similarity among entities, we can effectively expand the knowledge graph size and leverage their strengths via entity linkage. We evaluate our system using standard benchmark datasets and achieve state-of-the-art results for several tasks including node classification, relation extraction, and question answering. Our findings highlight the potential benefits of incorporating multi-graph structures into representation learning methods for knowledge intensive NLP tasks. Overall, our work represents a promising step towards harnessing the power of large knowledge bases through advanced machine learning algorithms.",1
"We demonstrate an approach to face attribute detection that retains or improves attribute detection accuracy across gender and race subgroups by learning demographic information prior to learning the attribute detection task. The system, which we call InclusiveFaceNet, detects face attributes by transferring race and gender representations learned from a held-out dataset of public race and gender identities. Leveraging learned demographic representations while withholding demographic inference from the downstream face attribute detection task preserves potential users' demographic privacy while resulting in some of the best reported numbers to date on attribute detection in the Faces of the World and CelebA datasets.",0
"""In today's society, technology has become increasingly prevalent in our daily lives, often times playing crucial roles such as facial recognition software used by law enforcement agencies for identifying individuals caught on camera. However, most of these systems have been developed based on datasets that mostly comprise images of Caucasian faces, limiting their effectiveness in detecting non-Caucasians. This research seeks to address this issue by introducing a new approach called 'InclusiveFaceNet', which improves face attribute detection through increased diversity in race and gender representation. Our method involves training a deep neural network using large scale data augmentation techniques on diverse image datasets consisting of faces from multiple races and genders. Results show significant improvement in accuracy across all demographics compared to state-of-the-art systems, ultimately paving the way towards more inclusive technologies."" This study aimed at developing an improved face attribute detection system, particularly focusing on enhancing its performance for faces belonging to different racial groups and both male and female populations. Our proposed solution, named ""InclusiveFaceNet"", addresses the limitation posed by existing facial recognition software, which primarily consists of Caucasian faces only. We trained the model using a deep neural network coupled with massive data augmentations from multiracial and multicultural image databases. Extensive evaluation demonstrated that InclusiveFaceNet outperformed prevailing methods in accurately determining attributes for people from varied ethnic backgrounds and genders. These promising results indicate the potential for more equitable facial analysis applications empowered by highly inclusive artificial intelligence (AI) systems like our novel approach.",1
"Neural network models and deep models are one of the leading and state of the art models in machine learning. Most successful deep neural models are the ones with many layers which highly increases their number of parameters. Training such models requires a large number of training samples which is not always available. One of the fundamental issues in neural networks is overfitting which is the issue tackled in this thesis. Such problem often occurs when the training of large models is performed using few training samples. Many approaches have been proposed to prevent the network from overfitting and improve its generalization performance such as data augmentation, early stopping, parameters sharing, unsupervised learning, dropout, batch normalization, etc.   In this thesis, we tackle the neural network overfitting issue from a representation learning perspective by considering the situation where few training samples are available which is the case of many real world applications. We propose three contributions. The first one presented in chapter 2 is dedicated to dealing with structured output problems to perform multivariate regression when the output variable y contains structural dependencies between its components. The second contribution described in chapter 3 deals with the classification task where we propose to exploit prior knowledge about the internal representation of the hidden layers in neural networks. Our last contribution presented in chapter 4 showed the interest of transfer learning in applications where only few samples are available. In this contribution, we provide an automatic system based on such learning scheme with an application to medical domain. In this application, the task consists in localizing the third lumbar vertebra in a 3D CT scan. This work has been done in collaboration with the clinic Rouen Henri Becquerel Center who provided us with data.",0
"In recent years, neural networks have become increasingly popular due to their ability to learn complex patterns from data. However, overfitting remains one of the biggest challenges faced by these models. Overfitting occurs when a model fits the training data so closely that it fails to generalize well on unseen data. To address this issue, regularization techniques have been developed which impose constraints on the network parameters during training. One such technique is representation learning, which involves adding a penalty term to the loss function based on the representations learned by the model. This approach encourages the model to learn more meaningful and interpretable features while reducing complexity and preventing overfitting. This paper presents a comprehensive study of representation learning as a regularization method for neural networks. We evaluate several different formulations of the penalty term and compare their performance across multiple benchmark datasets. Our results show that representation learning can effectively reduce test error and improve model interpretability without significantly impacting accuracy on the training set. Furthermore, we demonstrate that our proposed method achieves better or comparable performance compared to other state-of-the-art regularization methods. These findings provide new insights into the use of representation learning for neural network regularization and open up opportunities for future research in this area.",1
"Face recognition technology has demonstrated tremendous progress over the past few years, primarily due to advances in representation learning. As we witness the widespread adoption of these systems, it is imperative to consider the security of face representations. In this paper, we explore the practicality of using a fully homomorphic encryption based framework to secure a database of face templates. This framework is designed to preserve the privacy of users and prevent information leakage from the templates, while maintaining their utility through template matching directly in the encrypted domain. Additionally, we also explore a batching and dimensionality reduction scheme to trade-off face matching accuracy and computational complexity. Experiments on benchmark face datasets (LFW, IJB-A, IJB-B, CASIA) indicate that secure face matching can be practically feasible (16 KB template size and 0.01 sec per match pair for 512-dimensional features from SphereFace) while exhibiting minimal loss in matching performance.",0
"In recent years, face recognition technology has become increasingly important for security purposes such as border control, surveillance systems, and law enforcement. However, processing large amounts of data can pose a significant challenge, particularly when it comes to privacy concerns and maintaining the confidentiality of sensitive biometric images. This study proposes a novel approach to secure face matching using fully homomorphic encryption (FHE), which allows computations to be performed directly on encrypted data without compromising its integrity.  The proposed method utilizes state-of-the-art FHE schemes and machine learning algorithms for efficient and accurate face recognition under challenging lighting conditions. Our experimental results demonstrate that our system achieves high accuracy while preserving data privacy. Moreover, we evaluate different parameter settings of the proposed framework and discuss tradeoffs in terms of computational efficiency, storage requirements, and communication overhead.  This research represents an advancement in the field of computer vision by addressing pressing issues related to data privacy and security. Our work offers new possibilities for seamless integration of advanced facial recognition technologies into existing infrastructure, enabling safe and effective monitoring across various domains. We believe that our contributions have the potential to positively impact society by promoting secure biometrics applications, ultimately resulting in safer communities worldwide.",1
"In this paper, we propose a novel ranking framework for collaborative filtering with the overall aim of learning user preferences over items by minimizing a pairwise ranking loss. We show the minimization problem involves dependent random variables and provide a theoretical analysis by proving the consistency of the empirical risk minimization in the worst case where all users choose a minimal number of positive and negative items. We further derive a Neural-Network model that jointly learns a new representation of users and items in an embedded space as well as the preference relation of users over the pairs of items. The learning objective is based on three scenarios of ranking losses that control the ability of the model to maintain the ordering over the items induced from the users' preferences, as well as, the capacity of the dot-product defined in the learned embedded space to produce the ordering. The proposed model is by nature suitable for implicit feedback and involves the estimation of only very few parameters. Through extensive experiments on several real-world benchmarks on implicit data, we show the interest of learning the preference and the embedding simultaneously when compared to learning those separately. We also demonstrate that our approach is very competitive with the best state-of-the-art collaborative filtering techniques proposed for implicit feedback.",0
"Inference rules are a well studied topic in cognitive psychology, yet they still pose several challenges for recommendation systems that rely on user interactions rather than explicit ratings. This paper tackles one particular aspect of these problems: how can we effectively use implicit feedback data like clicks or purchases to make recommendations? We find that traditional methods based on latent factor models cannot cope with the high variance of pairwise preferences which leads to poor performance on ranking tasks. Instead, we propose using representation learning techniques combined with pairwise ranking losses to model such fine-grained differences. By training on large scale click logs from real users, our approach outperforms strong baselines across a variety of evaluation metrics commonly used to measure recommendation quality.",1
"Computer-aided diagnosis (CAD) techniques for lung field segmentation from chest radiographs (CXR) have been proposed for adult cohorts, but rarely for pediatric subjects. Statistical shape models (SSMs), the workhorse of most state-of-the-art CXR-based lung field segmentation methods, do not efficiently accommodate shape variation of the lung field during the pediatric developmental stages. The main contributions of our work are: (1) a generic lung field segmentation framework from CXR accommodating large shape variation for adult and pediatric cohorts; (2) a deep representation learning detection mechanism, \emph{ensemble space learning}, for robust object localization; and (3) \emph{marginal shape deep learning} for the shape deformation parameter estimation. Unlike the iterative approach of conventional SSMs, the proposed shape learning mechanism transforms the parameter space into marginal subspaces that are solvable efficiently using the recursive representation learning mechanism. Furthermore, our method is the first to include the challenging retro-cardiac region in the CXR-based lung segmentation for accurate lung capacity estimation. The framework is evaluated on 668 CXRs of patients between 3 month to 89 year of age. We obtain a mean Dice similarity coefficient of $0.96\pm0.03$ (including the retro-cardiac region). For a given accuracy, the proposed approach is also found to be faster than conventional SSM-based iterative segmentation methods. The computational simplicity of the proposed generic framework could be similarly applied to the fast segmentation of other deformable objects.",0
"In recent years, advances in deep learning have revolutionized the field of medical image analysis, leading to improved accuracy in tasks such as diagnosis and treatment planning. However, despite these promising developments, few studies have attempted to address the specific challenge of lung field segmentation on chest radiographs (CXR). To address this gap in research, our study proposes a novel approach that combines two powerful techniques: deep space learning and shape learning. This combination allows us to identify and segment the lung regions in CXR images more accurately than existing methods. We evaluate our method using both qualitative and quantitative measures, demonstrating superior performance compared to other state-of-the-art algorithms. Our work represents an important step towards enabling automated interpretation of CXR images for clinical applications. The paper presents extensive experimental evaluations comparing our approach with several competing methods, revealing the advantages and effectiveness of our proposed methodology. Finally, we discuss possible directions for future improvement and extension of our system. Overall, our findings contribute valuable insights into the challenges of automated medical image analysis and highlight the potential impact of advanced artificial intelligence techniques on healthcare. By presenting accurate results, efficient implementation and reliable performance, our solution can assist experts and enhance patient outcomes by reducing errors and speeding up decision making in diagnostic imaging.",1
"Audio-visual representation learning is an important task from the perspective of designing machines with the ability to understand complex events. To this end, we propose a novel multimodal framework that instantiates multiple instance learning. We show that the learnt representations are useful for classifying events and localizing their characteristic audio-visual elements. The system is trained using only video-level event labels without any timing information. An important feature of our method is its capacity to learn from unsynchronized audio-visual events. We achieve state-of-the-art results on a large-scale dataset of weakly-labeled audio event videos. Visualizations of localized visual regions and audio segments substantiate our system's efficacy, especially when dealing with noisy situations where modality-specific cues appear asynchronously.",0
"This paper proposes a novel approach to learning representations using weak supervision from unsynchronized audio and visual data streams. Previous work has often relied on strong synchronization assumptions between these modalities, but our method can learn meaningful features even if they are out of sync by just a few seconds. Our method first uses a pretrained model to extract initial feature maps for both audio and video frames, then employs a carefully designed loss function that encourages alignment between corresponding events detected in each modality. By leveraging large amounts of unlabeled data along with this new regularizer, we achieve state-of-the-art results on several challenging tasks involving joint embedding and cross-modal retrieval. Moreover, we demonstrate the utility of learned representations for downstream applications like activity recognition and speech enhancement. Overall, our framework provides a flexible and scalable solution for representation learning in complex multimodal environments.",1
"Many methods have been proposed to solve the domain adaptation problem recently. However, the success of them implicitly funds on the assumption that the information of domains are fully transferrable. If the assumption is not satisfied, the effect of negative transfer may degrade domain adaptation. In this paper, a better learning network has been proposed by considering three tasks - domain adaptation, disentangled representation, and style transfer simultaneously. Firstly, the learned features are disentangled into common parts and specific parts. The common parts represent the transferrable features, which are suitable for domain adaptation with less negative transfer. Conversely, the specific parts characterize the unique style of each individual domain. Based on this, the new concept of feature exchange across domains, which can not only enhance the transferability of common features but also be useful for image style transfer, is introduced. These designs allow us to introduce five types of training objectives to realize the three challenging tasks at the same time. The experimental results show that our architecture can be adaptive well to full transfer learning and partial transfer learning upon a well-learned disentangled representation. Besides, the trained network also demonstrates high potential to generate style-transferred images.",0
"Here is an abstract for a paper titled ""Domain Adaptation Meets Disentangled Representation Learning and Style Transfer"":  In recent years, domain adaptation has emerged as a promising approach for addressing the problem of distribution shift, where a machine learning model trained on one dataset performs poorly when tested on another related but different dataset. One popular method for overcoming this challenge is by utilizing disentanglement techniques which aim to separate the underlying factors of variation in the data into distinct components, making it easier to adapt to new domains. In addition, style transfer methods have gained attention in computer vision tasks by manipulating the appearance of images in a semantically meaningful manner while preserving their content. In this paper, we propose a novel framework that combines these two concepts, leveraging disentangled representation learning and style transfer to improve domain adaptation performance. Our results show significant improvements over state-of-the-art methods across several benchmark datasets, demonstrating the effectiveness of our proposed approach. Additionally, we provide qualitative analysis and visualizations to support our findings. This work represents a valuable contribution to the field of domain adaptation research and opens up possibilities for future exploration at the intersection of disentangled representation learning and style transfer.",1
"The visual attributes of cells, such as the nuclear morphology and chromatin openness, are critical for histopathology image analysis. By learning cell-level visual representation, we can obtain a rich mix of features that are highly reusable for various tasks, such as cell-level classification, nuclei segmentation, and cell counting. In this paper, we propose a unified generative adversarial networks architecture with a new formulation of loss to perform robust cell-level visual representation learning in an unsupervised setting. Our model is not only label-free and easily trained but also capable of cell-level unsupervised classification with interpretable visualization, which achieves promising results in the unsupervised classification of bone marrow cellular components. Based on the proposed cell-level visual representation learning, we further develop a pipeline that exploits the varieties of cellular elements to perform histopathology image classification, the advantages of which are demonstrated on bone marrow datasets.",0
"Abstract: Here we describe methods for unsupervised learning from histopathology images using generative adversarial networks (GANs) to generate cell level visual representations that capture key characteristics of disease. We use these representations as input features into machine learning models to achieve high accuracy classification without direct supervision. Our method significantly reduces human annotation time while improving model performance compared to state-of-the-art approaches relying on manual feature engineering. This work demonstrates the power of GANs in generating realistic data and opens up opportunities for automating pathological analysis through deep neural networks. Keywords: Unsupervised learning, generative adversarial network (GAN), cell representation, histopathology image processing, high accuracy classification, automatic pathological analysis.",1
"Recently deep neural networks have been widely and successfully applied in computer vision tasks and attracted growing interests in medical imaging. One barrier for the application of deep neural networks to medical imaging is the need of large amounts of prior training pairs, which is not always feasible in clinical practice. In this work we propose a personalized representation learning framework where no prior training pairs are needed, but only the patient's own prior images. The representation is expressed using a deep neural network with the patient's prior images as network input. We then applied this novel image representation to inverse problems in medical imaging in which the original inverse problem was formulated as a constraint optimization problem and solved using the alternating direction method of multipliers (ADMM) algorithm. Anatomically guided brain positron emission tomography (PET) image reconstruction and image denoising were employed as examples to demonstrate the effectiveness of the proposed framework. Quantification results based on simulation and real datasets show that the proposed personalized representation framework outperform other widely adopted methods.",0
"Abstract: This study proposes a deep learning framework for image reconstruction using artificial neural networks (ANN) that learns personalized representations from data acquired by multiple imaging modalities. By incorporating prior knowledge extracted via pretraining on natural images, we achieve improved generalization performance. Our approach leverages adversarial training to encourage disentanglement between the learned representation space and irrelevant attributes such as modality or noise type. Experiments demonstrate improvements over state-of-the-art methods across several medical imaging tasks while achieving competitive results on benchmark datasets. Additionally, we showcase real-world applications of our model including low-dose CT scans and brain MRIs. Finally, qualitative assessments reveal more consistent artifact reduction and enhanced visual fidelity compared to alternative techniques. As this technology advances, it has significant potential for broader integration within healthcare settings. Keywords: inverse problems; deep learning; convolutional neural networks (CNN); generative models; image synthesis",1
"Humans can naturally understand an image in depth with the aid of rich knowledge accumulated from daily lives or professions. For example, to achieve fine-grained image recognition (e.g., categorizing hundreds of subordinate categories of birds) usually requires a comprehensive visual concept organization including category labels and part-level attributes. In this work, we investigate how to unify rich professional knowledge with deep neural network architectures and propose a Knowledge-Embedded Representation Learning (KERL) framework for handling the problem of fine-grained image recognition. Specifically, we organize the rich visual concepts in the form of knowledge graph and employ a Gated Graph Neural Network to propagate node message through the graph for generating the knowledge representation. By introducing a novel gated mechanism, our KERL framework incorporates this knowledge representation into the discriminative image feature learning, i.e., implicitly associating the specific attributes with the feature maps. Compared with existing methods of fine-grained image classification, our KERL framework has several appealing properties: i) The embedded high-level knowledge enhances the feature representation, thus facilitating distinguishing the subtle differences among subordinate categories. ii) Our framework can learn feature maps with a meaningful configuration that the highlighted regions finely accord with the nodes (specific attributes) of the knowledge graph. Extensive experiments on the widely used Caltech-UCSD bird dataset demonstrate the superiority of our KERL framework over existing state-of-the-art methods.",0
"An abstract is written without using ""this"" as a pronoun since there is no article that has been published yet at the time the abstract was written. Furthermore, one must assume good faith that every author would naturally fulfill these conditions. Therefore we have no reason to doubt the truthfulness of the statement that this particular abstract is indeed about a specific fine-grained image recognition model. Thus I feel confident starting my response by saying: This abstract presents a knowledge-embedded representation learning methodology for achieving state-of-the art results in fine-gained image recognition tasks. Unlike traditional computer vision models which learn directly from raw pixel values, our approach leverages structured external data such as class hierarchy, attributes and relationships among classes. Our framework builds on previous work in embedding learning but introduces new techniques specifically tailored to handle multi-label problems common in object detection challenges where objects can belong to multiple categories simultaneously. We evaluate our methods across several benchmark datasets demonstrating their effectiveness compared to existing approaches even those based on more powerful backbone networks. In conclusion, we believe this submission effectively addresses key limitations present in current vision models while providing practical performance gains necessary to bring real world applications closer towards human levels of understanding.",1
"Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently-proposed Wasserstein autoencoder (WAE) which formalizes the adversarial autoencoder (AAE) as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. This adversarially regularized autoencoder (ARAE) allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic/human evaluation compared to existing methods.",0
"Advances in deep learning have led to significant improvements in tasks such as image generation, dimensionality reduction, anomaly detection, data clustering, compression, and more. However, these models often struggle to generate high-quality results due to their reliance on noisy input data, lack of regularization techniques, and overfitting issues. In this work, we propose adversarially regularized autoencoders (ARAs) as a solution to address these problems. Our approach leverages adversarial training methods to improve the performance and stability of traditional autoencoder architectures by encouraging them to learn more expressive representations that can better generalize across different inputs. We showcase our method using both qualitative and quantitative evaluations on several benchmark datasets and demonstrate its effectiveness in producing higher quality outputs compared to state-of-the-art alternatives. Additionally, ARAs are found to be robust against variations in hyperparameters and changes in dataset sizes. This study paves the way towards better understanding and utilizing deep learning approaches to solve real-world challenges while improving the interpretability and explainability of generated results.",1
"Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.",0
"The development of deep neural networks has yielded powerful tools that have achieved state-of-the-art results across many application domains. One key factor contributing to these successes is the ability of deep models to learn complex representations through training on large amounts of data. These learned representations capture meaningful patterns in the input data, allowing them to generalize well to new tasks and outperform simpler methods such as handcrafted features. However, little is understood regarding how such highdimensional and seemingly uninterpretable representations relate to human brain representations, which typically show specificity, disentanglement, and other desirable properties. By analyzing neural activity patterns at different stages of processing within both biological and artificial systems, we aim to shed light on some of the computational principles underlying these emerging regularities, including: hierarchical processing (“what and where”), object recognition “templates,” and mid-level visual representations supporting action prediction and guidance. By identifying sources and consequences of discontinuous changes in neural representation along dimensions of interest, we hope to provide insights into the nature of invariant recognition and its relationship to attention and behavior. By understanding better how these different types of processing support behavior in real world settings versus more restricted laboratory conditions, we can design more cognitively relevant models and advance our understanding of human brain function. This work therefore links the latest advances from machine learning with classic topics in neuroscience—such as the neuroanatomy and physiology of object recognition pathways—in order to make progress on bridging scales from single neurons up towards animal and ultimately human behavior.",1
"Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of ""neighboring"" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.",0
"This paper presents a new approach to representation learning on graphs called “jumping knowledge networks” (JKN). JKN combines traditional graph convolutional networks (GCN) with random jumps to explore different parts of the input graph, effectively augmenting the training data set. By using random jumping actions, we introduce diversity into the network’s search space during training and improve robustness. Our experimental results show that our method achieves state-of-the-art performance on several benchmark datasets while reducing computational complexity compared to other GCN methods. We hope that JKN can serve as an alternative toolkit for researchers working on representation learning problems in the field of computer vision, natural language processing, or recommender systems. Furthermore, we aim at promoting more discussion within the community regarding appropriate evaluation metrics, task definitions, or the interpretability of learned representations from graphs.",1
"This paper describes InfoCatVAE, an extension of the variational autoencoder that enables unsupervised disentangled representation learning. InfoCatVAE uses multimodal distributions for the prior and the inference network and then maximizes the evidence lower bound objective (ELBO). We connect the new ELBO derived for our model with a natural soft clustering objective which explains the robustness of our approach. We then adapt the InfoGANs method to our setting in order to maximize the mutual information between the categorical code and the generated inputs and obtain an improved model.",0
"This paper presents InfoCatVAE, a novel approach to representation learning using categorical variational autoencoders (VAEs). VAEs have become popular for their ability to learn complex representations by maximizing the evidence lower bound (ELBO) of log likelihood. However, existing work on discrete latent variables is limited by difficulties in estimating gradients with respect to continuous parameters. To address these challenges, we propose Categorical VAEs, which use discrete latent variables that make exact inference possible while still enabling gradient estimation through reparameterization tricks. We then extend Categorical VAEs to handle multi-modal distributions and uncertainty using Monte Carlo sampling techniques. Experimental results demonstrate the effectiveness of our method across multiple benchmark datasets, outperforming state-of-the-art baselines. Our findings highlight the promise of combining probabilistic modeling and deep learning, offering new opportunities for representing and manipulating high-dimensional data.",1
"It has been shown that for automated PAP-smear image classification, nucleus features can be very informative. Therefore, the primary step for automated screening can be cell-nuclei detection followed by segmentation of nuclei in the resulting single cell PAP-smear images. We propose a patch based approach using CNN for segmentation of nuclei in single cell images. We then pose the question of ion of segmentation for classification using representation learning with CNN, and whether low-level CNN features may be useful for classification. We suggest a CNN-based feature level analysis and a transfer learning based approach for classification using both segmented as well full single cell images. We also propose a decision-tree based approach for classification. Experimental results demonstrate the effectiveness of the proposed algorithms individually (with low-level CNN features), and simultaneously proving the sufficiency of cell-nuclei detection (rather than accurate segmentation) for classification. Thus, we propose a system for analysis of multi-cell PAP-smear images consisting of a simple nuclei detection algorithm followed by classification using transfer learning.",0
"Title: ""Considerations for Developing PAP Smear Image Analysis Systems using Convolutional Neural Networks (CNN)"" In this paper we discuss some key considerations for developing computerized systems that analyze Pap smear images to detect cervical cancer. We focus on one methodological approach for these systems which uses convolutional neural networks (CNN) for feature extraction from the raw image data. These features can then be used by machine learning algorithms to classify cells as normal or abnormal (precancerous). There have been many such systems developed in recent years and they have shown promise for improving accuracy over traditional manual analysis methods. However there remain several challenges to overcome including optimizing network architectures, selecting high quality training data sets, handling variability due to differences in scanning equipment, staining protocols and other sources of noise in the images. Furthermore, performance must be validated against gold standards established through consensus among pathologists to show true clinical utility. Despite these hurdles, we believe that with careful attention to these issues, intelligent systems based upon CNN models hold great potential for improving the efficiency and effectiveness of PAP smear screening programs worldwide, particularly in underserved communities where access to medical professionals trained in this area may be limited.",1
"A cognitive model of human learning provides information about skills a learner must acquire to perform accurately in a task domain. Cognitive models of learning are not only of scientific interest, but are also valuable in adaptive online tutoring systems. A more accurate model yields more effective tutoring through better instructional decisions. Prior methods of automated cognitive model discovery have typically focused on well-structured domains, relied on student performance data or involved substantial human knowledge engineering. In this paper, we propose Cognitive Representation Learner (CogRL), a novel framework to learn accurate cognitive models in ill-structured domains with no data and little to no human knowledge engineering. Our contribution is two-fold: firstly, we show that representations learnt using CogRL can be used for accurate automatic cognitive model discovery without using any student performance data in several ill-structured domains: Rumble Blocks, Chinese Character, and Article Selection. This is especially effective and useful in domains where an accurate human-authored cognitive model is unavailable or authoring a cognitive model is difficult. Secondly, for domains where a cognitive model is available, we show that representations learned through CogRL can be used to get accurate estimates of skill difficulty and learning rate parameters without using any student performance data. These estimates are shown to highly correlate with estimates using student performance data on an Article Selection dataset.",0
"Deep learning has shown remarkable results on a variety of tasks that involve complex input data. At the same time, many deep learning methods still lack an understanding of how their hidden representations encode real world concepts. This discrepancy limits the utility and interpretability of these models. In recent years, cognitive modeling frameworks have been developed to describe human behavior, often constraining predictions by prior knowledge of task constraints and properties of the external environment. Here we propose learning cognitive models directly from neural network weights trained via backpropagation. We show that it is possible to learn cognitive parameters that can make accurate qualitative predictions without relying on symbolic reasoning. By leveraging recent advances in representation analysis, including attention visualization and activation maximization, we gain insight into what the learned cognitive parameters capture. Ultimately, our approach opens up new possibilities for endowing deep learning systems with interpretable internal states and knowledge structures comparable to those found in traditional cognitive architectures.",1
"We present a self-supervised approach using spatio-temporal signals between video frames for action recognition. A two-stream architecture is leveraged to tangle spatial and temporal representation learning. Our task is formulated as both a sequence verification and spatio-temporal alignment tasks. The former task requires motion temporal structure understanding while the latter couples the learned motion with the spatial representation. The self-supervised pre-trained weights effectiveness is validated on the action recognition task. Quantitative evaluation shows the self-supervised approach competence on three datasets: HMDB51, UCF101, and Honda driving dataset (HDD). Further investigations to boost performance and generalize validity are still required.",0
"This paper presents two stream self-supervised learning method for action recognition that leverages temporal convolutional network (TCN) as the backbone model. TCN models have shown success in tasks requiring sequential data processing due to their efficient design of parallel computations across time steps. However, these models can struggle under unpaired, undersampled settings where action classes may be sparsely populated with examples. To overcome these challenges, we propose a novel framework utilizing both appearance and motion streams to perform supervision on existing datasets without additional human annotation labor. We demonstrate our methods outperform prior work in unpaired and paired settings for video classification by a significant margin, achieving state-of-the-art performance. Finally, we evaluate our approach on multiple benchmarks, including UCF101, HMDB51, and Something-Something V2, verifying its effectiveness over diverse domains. Our results suggest that exploiting the relationship between videos from both spatial dimensions and temporal evolution leads to improved action understanding.",1
"Purpose: This paper focuses on an automated analysis of surgical motion profiles for objective skill assessment and task recognition in robot-assisted surgery. Existing techniques heavily rely on conventional statistic measures or shallow modelings based on hand-engineered features and gesture segmentation. Such developments require significant expert knowledge, are prone to errors, and are less efficient in online adaptive training systems. Methods: In this work, we present an efficient analytic framework with a parallel deep learning architecture, SATR-DL, to assess trainee expertise and recognize surgical training activity. Through an end-to-end learning technique, abstract information of spatial representations and temporal dynamics is jointly obtained directly from raw motion sequences. Results: By leveraging a shared high-level representation learning, the resulting model is successful in the recognition of trainee skills and surgical tasks, suturing, needle-passing, and knot-tying. Meanwhile, we explore the use of ensemble in classification at the trial level, where the SATR-DL outperforms state-of-the-art performance by achieving accuracies of 0.960 and 1.000 in skill assessment and task recognition, respectively. Conclusion: This study highlights the potential of SATR-DL to provide improvements for an efficient data-driven assessment in intelligent robotic surgery.",0
"In recent years, robot-assisted surgery has become increasingly popular due to its potential benefits such as reduced trauma, improved precision, and shorter recovery times. However, assessing the performance of surgeons during these procedures remains challenging due to the lack of standardized evaluation methods that can accurately measure their skill levels. To address this issue, we propose a novel approach called SATR-DL (Surgical Assessment and Task Recognition using deep learning) which utilizes deep neural networks to analyze the motion data generated by robots during surgical procedures. Our method is designed to recognize different tasks performed by surgeons during robotic surgery and provide real-time feedback on their proficiency level. Additionally, our system can identify unusual patterns and anomalies in motion trajectories to detect any errors or complications during the procedure. We evaluate our model using extensive experiments conducted on a large dataset consisting of diverse surgical scenarios and demonstrate its effectiveness in improving surgical skill assessment and task recognition in robot-assisted surgery. Our results show that SATR-DL outperforms existing approaches with significant improvements in accuracy and robustness. Overall, our work provides a promising solution towards enhancing patient safety and surgeon training in robotic surgery.",1
"Learning expressive low-dimensional representations of ultrahigh-dimensional data, e.g., data with thousands/millions of features, has been a major way to enable learning methods to address the curse of dimensionality. However, existing unsupervised representation learning methods mainly focus on preserving the data regularity information and learning the representations independently of subsequent outlier detection methods, which can result in suboptimal and unstable performance of detecting irregularities (i.e., outliers).   This paper introduces a ranking model-based framework, called RAMODO, to address this issue. RAMODO unifies representation learning and outlier detection to learn low-dimensional representations that are tailored for a state-of-the-art outlier detection approach - the random distance-based approach. This customized learning yields more optimal and stable representations for the targeted outlier detectors. Additionally, RAMODO can leverage little labeled data as prior knowledge to learn more expressive and application-relevant representations. We instantiate RAMODO to an efficient method called REPEN to demonstrate the performance of RAMODO.   Extensive empirical results on eight real-world ultrahigh dimensional data sets show that REPEN (i) enables a random distance-based detector to obtain significantly better AUC performance and two orders of magnitude speedup; (ii) performs substantially better and more stably than four state-of-the-art representation learning methods; and (iii) leverages less than 1% labeled data to achieve up to 32% AUC improvement.",0
"This paper focuses on developing a system that can effectively detect outliers in ultrahigh-dimensional data sets using a novel representation learning approach. With the increasing availability of large datasets and advancements in sensor technology, there has been a growing need for efficient methods for identifying anomalous observations. Existing distance-based outlier detection algorithms typically suffer from high computational complexity due to the high dimensionality of modern data, resulting in limited scalability and effectiveness. We propose a methodology to learn representations of high-dimensional data by employing deep neural networks. Our algorithm leverages existing knowledge distilled from pre-trained models and adapts them to learn effective feature embeddings specifically tailored towards random projection-based distance metrics commonly used for outlier detection. Our framework achieves superior performance compared to state-of-the-art approaches across multiple application domains while reducing computational cost significantly, making our solution suitable for real-world deployment scenarios involving massive datasets. The paper concludes with discussions on future research directions aimed at expanding the capabilities of our proposed system. Overall, we demonstrate the viability of our novel representation learning approach as a powerful tool for addressing key challenges associated with outlier detection in today's era of big data.",1
"Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.",0
"This work presents a deep learning method that can learn representations and generate realistic synthetic 3D point clouds. Our method builds on top of recent advances in generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). We introduce several new components designed specifically for processing 3D data: a novel local feature encoding technique, which enables more efficient exploration of the latent space; a spatial attention module, allowing selective fusion of local features to capture contextual dependencies; and a U-Net based decoder, explicitly modeling the occupancy structure within generated samples. These contributions enable our approach to significantly improve over baselines across multiple tasks including object recognition, completion and texture transfer.",1
"The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.",0
"In this paper we explore different ways that shapes can be represented for single view 3D object shape prediction tasks. We use three main types of representation; pixels, which represent objects as images, voxels, which represent objects in a 3D grid of cubes, and point clouds, which represent objects as a collection of points. Each method has its own strengths and weaknesses, so we compare their performance on several benchmark datasets using metrics such as accuracy and precision. Our results show that while pixel based methods perform well overall, there may be cases where other representation methods could lead to more accurate predictions. We conclude by discussing future directions for research in this area, including exploring alternative representations and developing novel training techniques for improving model performance.",1
"Embedding representation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods. We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98X and 478X search speedup respectively over exhaustive linear search. The source code is available at https://github.com/maestrojeong/Deep-Hash-Table-ICML18",0
"End the abstract by including ""The authors would like to thank..."" Do not list any funding agencies (NSF, NIH).",1
"Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for fine-grained recognition essentially enhance the mid-level learning capability of CNNs. Previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classification network, or a sophisticated feature encoding method to capture higher order feature statistics. We show that mid-level representation learning can be enhanced within the CNN framework, by learning a bank of convolutional filters that capture class-specific discriminative patches without extra part or bounding box annotations. Such a filter bank is well structured, properly initialized and discriminatively learned through a novel asymmetric multi-stream architecture with convolutional filter supervision and a non-random layer initialization. Experimental results show that our approach achieves state-of-the-art on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and visualizations are provided to understand our approach.",0
"This paper presents a novel approach for fine-grained recognition using convolutional neural networks (CNNs). We propose learning a discriminative filter bank within each layer of the network that can extract relevant features from images. Our method addresses two main challenges: designing filters that capture subtle differences between classes while generalizing well across different tasks, and incorporating prior knowledge into the model without losing the ability to learn new representations. Experimental results demonstrate that our approach outperforms state-of-the-art methods on several benchmark datasets. The key contributions of our work are as follows: we introduce a new architecture called the discriminative filter bank module; we use a multi-task learning framework that allows sharing of feature extraction among multiple fine-grained classification tasks; and finally, we show that our method achieves competitive performance in challenging recognition tasks where precise distinctions must be made between similar categories. By improving CNN's representational capacity, our system has potential applications in image and video analysis, including object detection, semantic segmentation, action recognition, and image generation.",1
"In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.",0
"This paper develops the concept of self-consistency for trajectories and applies that to reinforcement learning (RL). In particular, we use the concept to extend traditional RL by introducing a notion of self-consistency constraints which enforce some smoothness assumptions on the space of possible state transitions over time. We show how the resulting hierarchical optimization problem can be reformulated as a variational inference problem using an autoencoder architecture. Finally, we demonstrate the effectiveness of our approach through experiments showing improved results compared to baseline methods in challenging environments where both exploration and exploitation are important considerations.",1
"Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).",0
"This paper presents DARLA, a novel approach to improving zero-shot transfer (ZST) performance in reinforcement learning tasks. ZST refers to the ability of an agent to learn from one task and transfer that knowledge to new, unseen tasks without any additional training. While there have been several attempts to improve ZST through methods such as pretraining on related tasks or using more expressive representations, these approaches often require large amounts of data or computation resources. In contrast, DARLA uses a lightweight method that makes minimal assumptions about the environment, resulting in improved ZST performance even with limited data. Our experiments demonstrate the effectiveness of our approach across a range of environments, including those commonly used in RL benchmarks. Overall, DARLA represents a significant step forward in enabling agents to quickly adapt to new situations by leveraging their prior experiences.",1
"Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, ""Would this patient have lower blood sugar had she received a different medication?"". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.",0
"This paper presents a method for learning representations that can be used for counterfactual inference. The proposed approach involves training a neural network on a combination of factual and counterfactual data, allowing the model to learn how to predict both what happened and what could have happened under different conditions.  The key innovation of our method lies in its use of adversarial training to encourage the learned representations to capture both causal relationships and alternative possibilities. By minimizing the distance between predicted factual outcomes and ground truth, while maximizing the distance between predicted counterfactual outcomes and their corresponding factual baselines, we induce the representation space to encode counterfactual reasoning.  We evaluate our method on several benchmark datasets and demonstrate its effectiveness by comparing against strong baseline models from the literature. Our experiments show that the learned representations significantly improve performance on a range of tasks related to counterfactual inference, including outcome prediction, explanation generation, and sensitivity analysis.  Overall, this work advances our understanding of how deep learning models can be trained to reason about what might have been, and opens up new opportunities for using artificial intelligence to support decision making in complex systems.",1
"Conditional density estimation is a general framework for solving various problems in machine learning. Among existing methods, non-parametric and/or kernel-based methods are often difficult to use on large datasets, while methods based on neural networks usually make restrictive parametric assumptions on the probability densities. Here, we propose a novel method for estimating the conditional density based on score matching. In contrast to existing methods, we employ scalable neural networks, but do not make explicit parametric assumptions on densities. The key challenge in applying score matching to neural networks is computation of the first- and second-order derivatives of a model for the log-density. We tackle this challenge by developing a new neural-kernelized approach, which can be applied on large datasets with stochastic gradient descent, while the reproducing kernels allow for easy computation of the derivatives needed in score matching. We show that the neural-kernelized function approximator has universal approximation capability and that our method is consistent in conditional density estimation. We numerically demonstrate that our method is useful in high-dimensional conditional density estimation, and compares favourably with existing methods. Finally, we prove that the proposed method has interesting connections to two probabilistically principled frameworks of representation learning: Nonlinear sufficient dimension reduction and nonlinear independent component analysis.",0
"In recent years, conditional density estimation has emerged as a powerful technique in machine learning and statistics for modeling complex distributions that depend on one or more input variables. This task becomes particularly challenging when dealing with high-dimensional data such as images, time series, and other structured signals where traditional nonparametric methods struggle due to their limited capacity to capture intricate relationships. To address these limitations, we propose Neural Kernelized Conditional Density Estimation (NKCDE), which combines advanced neural network architectures with kernel methods to achieve state-of-the-art performance across diverse applications. Our approach leverages both deep neural networks and kernels by representing the generator as a multilayer perceptron (MLP) and employing a corresponding Mercer kernel. We introduce a novel objective function based on minimizing negative log-likelihood, ensuring greater generalization ability compared to earlier MLP-based approximations. Experiments conducted on benchmark datasets demonstrate significant improvements over existing methods in terms of accuracy and efficiency, establishing NKCDE as an effective tool for real-world applications requiring accurate uncertainty quantification and efficient inference. By enhancing our understanding of conditional density estimation under structured inputs and noise contamination, this work paves the way for further advancements in machine learning research using rich representations and scalable frameworks tailored to large-scale problems.",1
"Graph embedding is a central problem in social network analysis and many other applications, aiming to learn the vector representation for each node. While most existing approaches need to specify the neighborhood and the dependence form to the neighborhood, which may significantly degrades the flexibility of representation, we propose a novel graph node embedding method (namely GESF) via the set function technique. Our method can 1) learn an arbitrary form of representation function from neighborhood, 2) automatically decide the significance of neighbors at different distances, and 3) be applied to heterogeneous graph embedding, which may contain multiple types of nodes. Theoretical guarantee for the representation capability of our method has been proved for general homogeneous and heterogeneous graphs and evaluation results on benchmark data sets show that the proposed GESF outperforms the state-of-the-art approaches on producing node vectors for classification tasks.",0
"In this paper we describe a method for learning graph representations using a universal discriminative mapping mechanism called GESF (Generalized Edge Sampling Function). This approach can be applied to any graph representation problem without modification, making it highly versatile and applicable across multiple domains. We demonstrate that our method achieves state-of-the-art performance on several benchmark datasets, outperforming existing methods that require domain specific modifications. Our experimental results show that GESF significantly improves over traditional node embedding techniques such as DeepWalk and Node2Vec, while maintaining high efficiency. By leveraging edge sampling, GESF is able to capture the unique characteristics of different graphs and learn meaningful representations that can be used for downstream tasks such as classification and regression. Overall, our work introduces a simple yet effective method for learning high quality graph representations that has wide ranging applications in computer science and beyond.",1
"Representation learning is at the heart of what makes deep learning effective. In this work, we introduce a new framework for representation learning that we call ""Holographic Neural Architectures"" (HNAs). In the same way that an observer can experience the 3D structure of a holographed object by looking at its hologram from several angles, HNAs derive Holographic Representations from the training set. These representations can then be explored by moving along a continuous bounded single dimension. We show that HNAs can be used to make generative networks, state-of-the-art regression models and that they are inherently highly resistant to noise. Finally, we argue that because of their denoising abilities and their capacity to generalize well from very few examples, models based upon HNAs are particularly well suited for biological applications where training examples are rare or noisy.",0
"Artificial intelligence (AI) has made significant advances over recent years, largely due to the development of powerful deep learning models that can learn complex patterns from large amounts of data. However, these models often require large computational resources, which limits their application on resource-constrained devices such as smartphones or embedded systems.  Holographic neural architectures represent one possible solution to this problem by exploiting the spatial structure of data while minimizing computation and memory usage. These models take advantage of the locality properties of convolutional filters and use holographic representations of data to reduce redundancy and increase efficiency. In this paper, we introduce a new type of holographic architecture called Lightweight Convolutional Networks (LCN), which further improve upon existing methods by using simplified operations and reducing parameter count without compromising performance.  We evaluate LCNs on several benchmark datasets and compare them against state-of-the art baseline models. Our results show that LCNs achieve comparable accuracy to these baselines while requiring significantly fewer parameters and computations. This makes them well suited for deployment on resource-constrained devices, where power consumption and latency are crucial factors. Overall, our findings demonstrate the effectiveness of holographic neural architectures for efficient model design, opening up new possibilities for AI applications across diverse domains.",1
"Current deep learning based text classification methods are limited by their ability to achieve fast learning and generalization when the data is scarce. We address this problem by integrating a meta-learning procedure that uses the knowledge learned across many tasks as an inductive bias towards better natural language understanding. Based on the Model-Agnostic Meta-Learning framework (MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification. The essential difference between MAML and ATAML is in the separation of task-agnostic representation learning and task-specific attentive adaptation. The proposed ATAML is designed to encourage task-agnostic representation learning by way of task-agnostic parameterization and facilitate task-specific adaptation via attention mechanisms. We provide evidence to show that the attention mechanism in ATAML has a synergistic effect on learning performance. In comparisons with models trained from random initialization, pretrained models and meta trained MAML, our proposed ATAML method generalizes better on single-label and multi-label classification tasks in miniRCV1 and miniReuters-21578 datasets.",0
"In recent years, few-shot learning has emerged as a promising approach for tackling real-world machine learning problems where labeled data is scarce. Despite significant progress in the field, many methods still rely on meta-learning techniques that focus solely on optimizing models at each step, without considering how attention mechanisms can enhance their performance. This paper investigates the impact of incorporating attention into meta-learning algorithms for few-shot text classification tasks, particularly those based on gradient boosting decision trees (GBDTs). We show through extensive experiments that our proposed method leads to better accuracy compared to existing state-of-the-art approaches. Moreover, we demonstrate that using self-attention enables the model to capture essential relationships within classes and improve generalization across various datasets. Overall, our findings highlight the importance of attention mechanisms in enhancing few-shot learning systems via meta-learning techniques.",1
"While deep learning methods are increasingly being applied to tasks such as computer-aided diagnosis, these models are difficult to interpret, do not incorporate prior domain knowledge, and are often considered as a ""black-box."" The lack of model interpretability hinders them from being fully understood by target users such as radiologists. In this paper, we present a novel interpretable deep hierarchical semantic convolutional neural network (HSCNN) to predict whether a given pulmonary nodule observed on a computed tomography (CT) scan is malignant. Our network provides two levels of output: 1) low-level radiologist semantic features, and 2) a high-level malignancy prediction score. The low-level semantic outputs quantify the diagnostic features used by radiologists and serve to explain how the model interprets the images in an expert-driven manner. The information from these low-level tasks, along with the representations learned by the convolutional layers, are then combined and used to infer the high-level task of predicting nodule malignancy. This unified architecture is trained by optimizing a global loss function including both low- and high-level tasks, thereby learning all the parameters within a joint framework. Our experimental results using the Lung Image Database Consortium (LIDC) show that the proposed method not only produces interpretable lung cancer predictions but also achieves significantly better results compared to common 3D CNN approaches.",0
"This paper presents an interpretable deep hierarchical semantic convolutional neural network (DHSNet) for lung nodule malignancy classification. We propose a novel DHS architecture that incorporates both local contextual information from raw images and global image representations learned by high level concepts in each layer. To achieve interpretability we introduce a new method called Progressive Channel Attention Module (P-CAM), which explains the decision making process of our model at different levels of abstraction. Our approach outperforms traditional methods such as random forest and support vector machines on two public datasets, achieving higher accuracy with better area under curve (AUC). We conduct comprehensive experiments to analyze the effects of different components, including feature representation learning, channel attention, and multilevel interpretation. The code and models will be made available online upon publication.",1
"Multi-layered representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments and visualizations confirmed the effectiveness of the model in terms of performance and representation learning ability.",0
"Gradient boosting decision trees have become very popular over recent years due to their ability to accurately model complex real world data. However, many users find that they cannot fit gradient boosting decision trees using publicly available libraries such as XGBoost on larger datasets because these methods suffer from low scalability, both computationally and memory wise, limiting their applicability on big data problems. To overcome these difficulties, we propose multi-layered gradient boosting decision trees where each layer independently models a subset of features. This allows us to scale up the use of gradientboostingdecisiontrees to large datasets by reducing computational requirements through parallelization across multiple nodes. We demonstrate the effectiveness of our method through experiments on real life datasets containing millions of observations. Our results show that multi-layered gradient boosting decision trees outperform traditional gradient boosting decision trees in terms of prediction accuracy while requiring significantly less computing resources, making them suitable for handling big data challenges.",1
"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",0
"Abstract: Artificial neural networks have been successful at solving many complex problems by learning continuous representations from raw inputs. However, these models often require large amounts of data and computational resources, which can limit their applicability in real-world scenarios where resource constraints may exist. In order to address this challenge, researchers have begun exploring methods that allow neural networks to learn discrete representations that can capture important features without requiring as much data or computation. This work presents a new approach called ""Neural Discrete Representation Learning,"" which leverages recent advances in deep learning to efficiently encode input data into discrete codebooks. We demonstrate through experiments on several benchmark datasets that our method achieves competitive performance compared to state-of-the-art continuous representation learning algorithms while significantly reducing resource requirements. Our findings show promise for enabling more efficient and scalable artificial intelligence systems in practice. Keywords: artificial neural network; deep learning; discrete representation learning; representation discretization; computer vision; natural language processing",1
"Geospatial analysis lacks methods like the word vector representations and pre-trained networks that significantly boost performance across a wide range of natural language and computer vision tasks. To fill this gap, we introduce Tile2Vec, an unsupervised representation learning algorithm that extends the distributional hypothesis from natural language -- words appearing in similar contexts tend to have similar meanings -- to spatially distributed data. We demonstrate empirically that Tile2Vec learns semantically meaningful representations on three datasets. Our learned representations significantly improve performance in downstream classification tasks and, similar to word vectors, visual analogies can be obtained via simple arithmetic in the latent space.",0
"Spatially distributed data is ubiquitous across many domains including geology, biology, astronomy, and computer vision. Learning representations from these datasets that capture their inherent structures remains a challenging task, particularly as most methods require extensive labeled data and human annotations, which can be expensive or even impossible to obtain. In this study, we present Tile2Vec, an unsupervised approach based on message passing neural networks (MPNNs) to learn meaningful representations directly from raw spatial data without requiring any explicit labels or prior knowledge. Our method iteratively passes messages between neighboring units or tiles, capturing local interactions and higher level geometric relationships. We evaluate our approach on diverse synthetic and real world benchmarks and demonstrate significant performance gains compared to several state-of-the-art baselines while using considerably less computational resources. By enabling unsupervised learning from large volumes of unlabeled spatially distributed data, we believe Tile2Vec paves the way towards more efficient and effective exploration and analysis of complex scientific phenomena.",1
"Multimodal sensory data resembles the form of information perceived by humans for learning, and are easy to obtain in large quantities. Compared to unimodal data, synchronization of concepts between modalities in such data provides supervision for disentangling the underlying explanatory factors of each modality. Previous work leveraging multimodal data has mainly focused on retaining only the modality-invariant factors while discarding the rest. In this paper, we present a partitioned variational autoencoder (PVAE) and several training objectives to learn disentangled representations, which encode not only the shared factors, but also modality-dependent ones, into separate latent variables. Specifically, PVAE integrates a variational inference framework and a multimodal generative model that partitions the explanatory factors and conditions only on the relevant subset of them for generation. We evaluate our model on two parallel speech/image datasets, and demonstrate its ability to learn disentangled representations by qualitatively exploring within-modality and cross-modality conditional generation with semantics and styles specified by examples. For quantitative analysis, we evaluate the classification accuracy of automatically discovered semantic units. Our PVAE can achieve over 99% accuracy on both modalities.",0
"Title: MultiModal Representations Using Disentangled Partitions Abstract This paper presents a new framework for learning representations from multimodal sensory data that leverages partitioned latent spaces to disentangle complex underlying factors of variation. By using a carefully designed partition structure, we enable efficient computation while capturing intricate relationships across modalities. Our approach enables scalability to high-dimensional and large datasets, enabling applications such as sensor fusion, cross-modal retrieval, and multi-view learning. We evaluate our method on challenging benchmarks, showing state-of-the art results compared to strong baselines. Additionally, through a detailed analysis, we demonstrate how partitioning simplifies interpretation of learned representations, facilitating human understanding of complex relationships present in the original data. Overall, our work advances representation learning research by providing a principled approach for handling multimodal data complexity through well-defined partitions.",1
"Few-shot learning that trains image classifiers over few labeled examples per category is a challenging task. In this paper, we propose to exploit an additional big dataset with different categories to improve the accuracy of few-shot learning over our target dataset. Our approach is based on the observation that images can be decomposed into objects, which may appear in images from both the additional dataset and our target dataset. We use the object-level relation learned from the additional dataset to infer the similarity of images in our target dataset with unseen categories. Nearest neighbor search is applied to do image classification, which is a non-parametric model and thus does not need fine-tuning. We evaluate our algorithm on two popular datasets, namely Omniglot and MiniImagenet. We obtain 8.5\% and 2.7\% absolute improvements for 5-way 1-shot and 5-way 5-shot experiments on MiniImagenet, respectively. Source code will be published upon acceptance.",0
"This study proposes a new approach to few-shot image classification using object-level representation learning. Current methods rely on task-specific meta-learning algorithms that learn to generalize across tasks, which can lead to suboptimal results when applied to real-world scenarios where data is limited. Instead, we propose leveraging large amounts of unlabeled data to pretrain a model on basic visual concepts before fine-tuning it on few-shot tasks. Our method achieves state-of-the-art performance on challenging benchmark datasets while requiring fewer labeled examples than traditional approaches. Additionally, our method enables transfer learning, allowing models trained on one dataset to improve performance on related tasks without further training. Overall, our work demonstrates the potential of pretraining on basic visual representations for enabling few-shot learning in computer vision applications.",1
"This paper proposes the decision tree latent controller generative adversarial network (DTLC-GAN), an extension of a GAN that can learn hierarchically interpretable representations without relying on detailed supervision. To impose a hierarchical inclusion structure on latent variables, we incorporate a new architecture called the DTLC into the generator input. The DTLC has a multiple-layer tree structure in which the ON or OFF of the child node codes is controlled by the parent node codes. By using this architecture hierarchically, we can obtain the latent space in which the lower layer codes are selectively used depending on the higher layer ones. To make the latent codes capture salient semantic features of images in a hierarchically disentangled manner in the DTLC, we also propose a hierarchical conditional mutual information regularization and optimize it with a newly defined curriculum learning method that we propose as well. This makes it possible to discover hierarchically interpretable representations in a layer-by-layer manner on the basis of information gain by only using a single DTLC-GAN model. We evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny ImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn hierarchically interpretable representations with either unsupervised or weakly supervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval tasks and showed its effectiveness in representation learning.",0
"In GANs (Generative Adversarial Networks) two deep neural networks: generator and discriminator have a competitive relationship as they try to fool each other in turn but both cooperate and agree on generating coherence image. Although these models generate images that rival human quality; there still exists space for improvement that can help these models be more intelligent by adding better latent decision making modules, i.e., controller. We propose using decision trees as controllers which enable interpretable decisions at different levels instead of just blindly taking gradients of parameters while backpropagating through generators. Our method also enables controllability over the image generation process where we directly control latents instead of feeding noise into input pipes every time. This way, less noisy data is required, memory usage reduces significantly hence providing greater efficiency without sacrificing performance. Evaluations show how our algorithm outperforms existing state of art methods with higher FID metrics, stronger visual results along with faster inference speeds across multiple datasets such as MNIST, CelebA, LSUN etc.",1
"Kinship verification has a number of applications such as organizing large collections of images and recognizing resemblances among humans. In this research, first, a human study is conducted to understand the capabilities of human mind and to identify the discriminatory areas of a face that facilitate kinship-cues. Utilizing the information obtained from the human study, a hierarchical Kinship Verification via Representation Learning (KVRL) framework is utilized to learn the representation of different face regions in an unsupervised manner. We propose a novel approach for feature representation termed as filtered contractive deep belief networks (fcDBN). The proposed feature representation encodes relational information present in images using filters and contractive regularization penalty. A compact representation of facial images of kin is extracted as an output from the learned model and a multi-layer neural network is utilized to verify the kin accurately. A new WVU Kinship Database is created which consists of multiple images per subject to facilitate kinship verification. The results show that the proposed deep learning framework (KVRL-fcDBN) yields stateof-the-art kinship verification accuracy on the WVU Kinship database and on four existing benchmark datasets. Further, kinship information is used as a soft biometric modality to boost the performance of face verification via product of likelihood ratio and support vector machine based approaches. Using the proposed KVRL-fcDBN framework, an improvement of over 20% is observed in the performance of face verification.",0
"Title: ""Hierarchical Representation Learning for Kinship Verification"" Authors: Shuang Li (shuangli@stanford.edu) Daisy Zhang (dazhang8@gmail.com) Jason Jia (jlia6@gatech.edu) Cheng-Lin Liu (cl2478@columbia.edu) Abstract: This paper presents a novel approach for kinship verification that leverages hierarchical representation learning. Inspired by human cognition, our method first extracts local features from facial regions to capture subtle variations in appearance, and then integrates them into a global descriptor at multiple levels of abstraction. Our framework enables efficient computation and achieves state-of-the-art performance on standard benchmark datasets. Furthermore, we conduct extensive analyses to reveal insights into the learned representations and their impact on the final predictions, demonstrating the effectiveness and generality of our approach for solving related problems beyond kinship verification.",1
"Object detection and semantic segmentation are two main themes in object retrieval from high-resolution remote sensing images, which have recently achieved remarkable performance by surfing the wave of deep learning and, more notably, convolutional neural networks (CNNs). In this paper, we are interested in a novel, more challenging problem of vehicle instance segmentation, which entails identifying, at a pixel-level, where the vehicles appear as well as associating each pixel with a physical instance of a vehicle. In contrast, vehicle detection and semantic segmentation each only concern one of the two. We propose to tackle this problem with a semantic boundary-aware multi-task learning network. More specifically, we utilize the philosophy of residual learning (ResNet) to construct a fully convolutional network that is capable of harnessing multi-level contextual feature representations learned from different residual blocks. We theoretically analyze and discuss why residual networks can produce better probability maps for pixel-wise segmentation tasks. Then, based on this network architecture, we propose a unified multi-task learning network that can simultaneously learn two complementary tasks, namely, segmenting vehicle regions and detecting semantic boundaries. The latter subproblem is helpful for differentiating closely spaced vehicles, which are usually not correctly separated into instances. Currently, datasets with pixel-wise annotation for vehicle extraction are ISPRS dataset and IEEE GRSS DFC2015 dataset over Zeebrugge, which specializes in semantic segmentation. Therefore, we built a new, more challenging dataset for vehicle instance segmentation, called the Busy Parking Lot UAV Video dataset, and we make our dataset available at http://www.sipeo.bgu.tum.de/download so that it can be used to benchmark future vehicle instance segmentation algorithms.",0
"This paper presents a novel approach for vehicle instance segmentation from aerial images and videos using a multi-task learning residual fully convolutional network (RFCN). The proposed method utilizes two different tasks, semantic segmentation and object detection, which work together to accurately identify instances of vehicles within dense urban environments. To achieve this, we design a customized RFCN architecture that incorporates knowledge transfer through encoder layers while ensuring feature propagation across all downsampled resolutions. We apply data augmentation techniques to increase diversity in our training dataset and demonstrate improved performance over baseline approaches on standard benchmark datasets. Overall, our results showcase the effectiveness of multi-task learning as well as the utility of advanced deep neural networks for challenging computer vision tasks such as vehicle instance segmentation from aerial imagery.",1
"In this paper, we introduce an alternative approach, namely GEN (Genetic Evolution Network) Model, to the deep learning models. Instead of building one single deep model, GEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. Significantly different from the wellknown representation learning models with extremely deep structures, the unit models covered in GEN are of a much shallower architecture. In the training process, from each generation, a subset of unit models will be selected based on their performance to evolve and generate the child models in the next generation. GEN has significant advantages compared with existing deep representation learning models in terms of both learning effectiveness, efficiency and interpretability of the learning process and learned results. Extensive experiments have been done on diverse benchmark datasets, and the experimental results have demonstrated the outstanding performance of GEN compared with the state-of-the-art baseline methods in both effectiveness of efficiency.",0
"This paper presents a new deep learning model called the Generate-and-Evaluate (GEN) Model which is designed to provide a different approach to traditional neural network architectures. The authors propose that existing models have significant drawbacks, such as relying on large amounts of data and computational resources, while producing subpar results compared to simpler statistical methods like linear regression. In contrast, they claim that their GEN model can achieve better performance than these state-of-the-art methods by training only one generator per output unit instead of maintaining two generators commonly used in other approaches. They evaluate the proposed method using several benchmark datasets across various domains including image generation, speech synthesis, language translation and question answering tasks. Their experiments demonstrate the effectiveness of the proposed method over many well-known baselines and establish its superiority in terms of accuracy and efficiency. Finally, the paper concludes by discussing future directions and potential applications of the GEN model beyond academia. Overall, this work introduces a promising alternative approach to building high performing machine learning systems for complex real world problems.",1
"Many problems at the intersection of combinatorics and computer science require solving for a permutation that optimally matches, ranks, or sorts some data. These problems usually have a task-specific, often non-differentiable objective function that data-driven algorithms can use as a learning signal. In this paper, we propose the Sinkhorn Policy Gradient (SPG) algorithm for learning policies on permutation matrices. The actor-critic neural network architecture we introduce for SPG uniquely decouples representation learning of the state space from the highly-structured action space of permutations with a temperature-controlled Sinkhorn layer. The Sinkhorn layer produces continuous relaxations of permutation matrices so that the actor-critic architecture can be trained end-to-end. Our empirical results show that agents trained with SPG can perform competitively on sorting, the Euclidean TSP, and matching tasks. We also observe that SPG is significantly more data efficient at the matching task than the baseline methods, which indicates that SPG is conducive to learning representations that are useful for reasoning about permutations.",0
"""Permutation learning has been a topic of interest for artificial intelligence researchers due to its applications in various fields such as computer vision, natural language processing, and robotics. In recent years, deep reinforcement learning algorithms have shown promising results in solving permutation problems. However, one major challenge faced by these algorithms is exploring the vast state space effectively to learn optimal policies. This work proposes a novel algorithm called 'Sinkhorn Policy Gradient' that addresses this challenge by combining policy gradient methods with the Sinkhorn-Knopp algorithm for solving linear programs. Our experimental results on multiple permutation benchmarks demonstrate that our proposed approach outperforms existing state-of-the-art methods by a significant margin while requiring fewer iterations to converge.""  Please note: I assumed some context here regarding RL and permutation learning but if you need further clarification please let me know! Also, please keep in mind that this is just a draft abstract, it might still require revisions based on the feedback from your target audience.",1
"We introduce the task of directly modeling a visually intelligent agent. Computer vision typically focuses on solving various subtasks related to visual intelligence. We depart from this standard approach to computer vision; instead we directly model a visually intelligent agent. Our model takes visual information as input and directly predicts the actions of the agent. Toward this end we introduce DECADE, a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements. Using this data we model how the dog acts and how the dog plans her movements. We show under a variety of metrics that given just visual input we can successfully model this intelligent agent in many situations. Moreover, the representation learned by our model encodes distinct information compared to representations trained on image classification, and our learned representation can generalize to other domains. In particular, we show strong results on the task of walkable surface estimation by using this dog modeling task as representation learning.",0
"This paper proposes a new methodology for understanding and modeling the behavior of dogs based on visual data analysis. By leveraging advances in computer vision algorithms and deep learning techniques, our approach enables accurate identification of different dog behaviors from video footage. This has important implications for animal welfare research, as well as applications such as automated monitoring systems for pet owners and kennels. We describe our system architecture, including preprocessing steps to prepare raw video input and a convolutional neural network that learns to classify eight distinct dog behaviors. Our experiments demonstrate high accuracy across multiple datasets and we discuss future directions for improving performance and expanding applications. Overall, our work represents an important step towards developing intelligent tools for studying animal behavior using modern artificial intelligence technologies.",1
"Person re-identification aims to match a person's identity across multiple camera streams. Deep neural networks have been successfully applied to the challenging person re-identification task. One remarkable bottleneck is that the existing deep models are data hungry and require large amounts of labeled training data. Acquiring manual annotations for pedestrian identity matchings in large-scale surveillance camera installations is a highly cumbersome task. Here, we propose the first semi-supervised approach that performs pseudo-labeling by considering complex relationships between unlabeled and labeled training samples in the feature space. Our approach first approximates the actual data manifold by learning a generative model via adversarial training. Given the trained model, data augmentation can be performed by generating new synthetic data samples which are unlabeled. An open research problem is how to effectively use this additional data for improved feature learning. To this end, this work proposes a novel Feature Affinity based Pseudo-Labeling (FAPL) approach with two possible label encodings under a unified setting. Our approach measures the affinity of unlabeled samples with the underlying clusters of labeled data samples using the intermediate feature representations from deep networks. FAPL trains with the joint supervision of cross-entropy loss together with a center regularization term, which not only ensures discriminative feature representation learning but also simultaneously predicts pseudo-labels for unlabeled data. Our extensive experiments on two standard large-scale datasets, Market-1501 and DukeMTMC-reID, demonstrate significant performance boosts over closely related competitors and outperforms state-of-the-art person re-identification techniques in most cases.",0
"Incorporating unlabeled data has become increasingly important due to difficulties obtaining annotations on large datasets that are necessary for fine tuning deep neural networks (DNNs). This paper presents a feature affinity-based method called pseudo labeling that leverages existing labeled images to generate labels on unseen images by considering their features similarity. Specifically, given two sets of image patches extracted from each person in both labeled and unlabeled images and obtained using a pretrained convolutional network as the feature extractor, we first calculate the distances between corresponding patch pairs across the sets, construct an undirected graph where the edge weight represents the distance between nodes, then assign the same class label to connected components on the graph representing patch clusters as strongly associated with their original person classes. Since DNNs tend to learn discriminative features at earlier layers while retaining more semantic ones towards later layers, we adopt multiple layer outputs in our proposed algorithm so as to capture complementary views. Experimental results show our method improves over several state-of-the-art methods in both semi-supervised setting and supervised only on subsets of fully labeled datasets for person re-identification task, suggesting the effectiveness and generality of our approach. By combining these labeled embeddings and using them to train the model under a contrastive loss framework, this allows us to effectively harness the knowledge gained from fully labeled datasets and leverage the remaining unlabeled dataset to improve performance beyond what was previously possible with just labeled data alone.",1
"While generic object detection has achieved large improvements with rich feature hierarchies from deep nets, detecting small objects with poor visual cues remains challenging. Motion cues from multiple frames may be more informative for detecting such hard-to-distinguish objects in each frame. However, how to encode discriminative motion patterns, such as deformations and pose changes that characterize objects, has remained an open question. To learn them and thereby realize small object detection, we present a neural model called the Recurrent Correlational Network, where detection and tracking are jointly performed over a multi-frame representation learned through a single, trainable, and end-to-end network. A convolutional long short-term memory network is utilized for learning informative appearance change for detection, while learned representation is shared in tracking for enhancing its performance. In experiments with datasets containing images of scenes with small flying objects, such as birds and unmanned aerial vehicles, the proposed method yielded consistent improvements in detection performance over deep single-frame detectors and existing motion-based detectors. Furthermore, our network performs as well as state-of-the-art generic object trackers when it was evaluated as a tracker on the bird dataset.",0
"This paper presents an approach that uses joint detection and tracking to differentiate objects by their motion. The proposed method can accurately detect small flying objects under challenging conditions and track them over time. We evaluate our approach on real-world data from different scenarios, such as UAV inspection videos. Our results demonstrate the effectiveness of our approach in terms of accuracy and robustness, particularly compared to state-of-the-art methods. Furthermore, we provide extensive ablation studies to show the contribution of each component in our system. Overall, our work represents an important step towards autonomous systems that can effectively perceive and interact with dynamic environments. Keywords: object recognition, computer vision, deep learning, tracking, UAV. This study proposes a novel approach for object recognition and differentiation based on their motion patterns. By leveraging advanced techniques in joint detection and tracking of moving objects, the authors aim to improve upon current state-of-the-art methods in computer vision and autonomous systems. Their solution utilizes deep learning algorithms and was evaluated through extensive experiments using real-world data sets, including footage captured during UAV inspections. Results indicate superior performance in terms of accuracy and robustness, setting a new benchmark for object recognition technologies in complex and dynamic environments. Ultimately, these findings have significant implications for applications ranging from surveillance and robotics to self-driving cars and beyond.",1
"Structured representations, such as Bags of Words, VLAD and Fisher Vectors, have proven highly effective to tackle complex visual recognition tasks. As such, they have recently been incorporated into deep architectures. However, while effective, the resulting deep structured representation learning strategies typically aggregate local features from the entire image, ignoring the fact that, in complex recognition tasks, some regions provide much more discriminative information than others.   In this paper, we introduce an attentional structured representation learning framework that incorporates an image-specific attention mechanism within the aggregation process. Our framework learns to predict jointly the image class label and an attention map in an end-to-end fashion and without any other supervision than the target label. As evidenced by our experiments, this consistently outperforms attention-less structured representation learning and yields state-of-the-art results on standard scene recognition and fine-grained categorization benchmarks.",0
Title: Attentional Structured Representation Learning for Image Classification,1
"Scale-space representation has been popular in computer vision community due to its theoretical foundation. The motivation for generating a scale-space representation of a given data set originates from the basic observation that real-world objects are composed of different structures at different scales. Hence, it's reasonable to consider learning features with image pyramids generated by smoothing and down-sampling operations. In this paper we propose Laplacian pyramid auto-encoders, a straightforward modification of the deep convolutional auto-encoder architecture, for unsupervised representation learning. The method uses multiple encoding-decoding sub-networks within a Laplacian pyramid framework to reconstruct the original image and the low pass filtered images. The last layer of each encoding sub-network also connects to an encoding layer of the sub-network in the next level, which aims to reverse the process of Laplacian pyramid generation. Experimental results showed that Laplacian pyramid benefited the classification and reconstruction performance of deep auto-encoder approaches, and batch normalization is critical to get deep auto-encoders approaches to begin learning.",0
"One possible abstract:  Unsupervised representation learning with auto-encoders has recently gained popularity due to their ability to learn meaningful representations from large amounts of unlabeled data. However, traditional auto-encoder architectures may not be able to fully capture high-level abstractions present in complex datasets such as images. In this work, we propose using Laplacian pyramid auto-encoders (LPAE) which enable efficient encoding of multi-scale features by incorporating hierarchical structure into the latent space. Our method achieves state-of-the-art performance on several benchmark tasks while requiring less computational overhead compared to other contemporary methods. We evaluate our approach on standard image classification tasks and demonstrate that LPAEs outperform other encoder-decoder models on three widely used benchmark datasets. Additionally, we conduct ablation studies to investigate the impact of different design choices on model performance. Overall, our results show that LPAEs effectively capture high-level semantic structures and can serve as powerful tools for feature extraction without any supervision.",1
"Identifying potential abuses of human rights through imagery is a novel and challenging task in the field of computer vision, that will enable to expose human rights violations over large-scale data that may otherwise be impossible. While standard databases for object and scene categorisation contain hundreds of different classes, the largest available dataset of human rights violations contains only 4 classes. Here, we introduce the `Human Rights Archive Database' (HRA), a verified-by-experts repository of 3050 human rights violations photographs, labelled with human rights semantic categories, comprising a list of the types of human rights abuses encountered at present. With the HRA dataset and a two-phase transfer learning scheme, we fine-tuned the state-of-the-art deep convolutional neural networks (CNNs) to provide human rights violations classification CNNs (HRA-CNNs). We also present extensive experiments refined to evaluate how well object-centric and scene-centric CNN features can be combined for the task of recognising human rights abuses. With this, we show that HRA database poses a challenge at a higher level for the well studied representation learning methods, and provide a benchmark in the task of human rights violations recognition in visual context. We expect this dataset can help to open up new horizons on creating systems able of recognising rich information about human rights violations. Our dataset, codes and trained models are available online at https://github.com/GKalliatakis/Human-Rights-Archive-CNNs.",0
"This research investigates the effectiveness of using object-centric and scene-centric convolutional neural network (CNN) features for recognizing human rights violations in image data. Object-centric features focus on specific objects within an image that may indicate evidence of abuse, while scene-centric features capture broader contextual information about the environment surrounding these objects. By exploring both types of features, we aim to improve detection accuracy by leveraging their unique strengths and addressing their limitations. Our experiments evaluate the performance of state-of-the-art object detection models equipped with either object-centric or scene-centric CNN features and demonstrate how combining them can significantly enhance the recognition capabilities of such systems. Additionally, our study contributes new insights into the importance of feature choice for effective image analysis tasks related to social impact assessment. Overall, this work underscores the value of multimodal approaches for detecting human rights violations through computer vision techniques.",1
"As digital medical imaging becomes more prevalent and archives increase in size, representation learning exposes an interesting opportunity for enhanced medical decision support systems. On the other hand, medical imaging data is often scarce and short on annotations. In this paper, we present an assessment of unsupervised feature learning approaches for images in the biomedical literature, which can be applied to automatic biomedical concept detection. Six unsupervised representation learning methods were built, including traditional bags of visual words, autoencoders, and generative adversarial networks. Each model was trained, and their respective feature space evaluated using images from the ImageCLEF 2017 concept detection task. We conclude that it is possible to obtain more powerful representations with modern deep learning approaches, in contrast with previously popular computer vision methods. Although generative adversarial networks can provide good results, they are harder to succeed in highly varied data sets. The possibility of semi-supervised learning, as well as their use in medical information retrieval problems, are the next steps to be strongly considered.",0
"Medical image analysis has become increasingly important due to the large amount of data generated by modern imaging techniques. This paper compares different unsupervised learning methods for detecting concepts in medical images. We evaluate each method using three key metrics – precision, recall, and F1 score – on two datasets commonly used in medical image analysis: MNIST and COVID-19 CT scans. Our results show that clustering algorithms achieve higher precision than other methods but at the cost of lower recall and F1 scores. Meanwhile, dimensionality reduction and feature extraction methods have poorer precision but better recall and F1 scores. Based on these findings, we recommend the use of clustering algorithms as a first step for identifying potential regions of interest before fine-tuning the approach with other methods. Overall, our work demonstrates the importance of selecting appropriate evaluation metrics when comparing different unsupervised learning approaches for medical image analysis tasks.",1
"Graph representations have increasingly grown in popularity during the last years. Existing representation learning approaches explicitly encode network structure. Despite their good performance in downstream processes (e.g., node classification, link prediction), there is still room for improvement in different aspects, like efficacy, visualization, and interpretability. In this paper, we propose, t-PINE, a method that addresses these limitations. Contrary to baseline methods, which generally learn explicit graph representations by solely using an adjacency matrix, t-PINE avails a multi-view information graph, the adjacency matrix represents the first view, and a nearest neighbor adjacency, computed over the node features, is the second view, in order to learn explicit and implicit node representations, using the Canonical Polyadic (a.k.a. CP) decomposition. We argue that the implicit and the explicit mapping from a higher-dimensional to a lower-dimensional vector space is the key to learn more useful, highly predictable, and gracefully interpretable representations. Having good interpretable representations provides a good guidance to understand how each view contributes to the representation learning process. In addition, it helps us to exclude unrelated dimensions. Extensive experiments show that t-PINE drastically outperforms baseline methods by up to 158.6% with respect to Micro-F1, in several multi-label classification problems, while it has high visualization and interpretability utility.",0
"In order to apply deep learning to graph data effectively, embedding methods have been applied widely due to their excellent performance on many tasks such as node classification, link prediction, etc. Meanwhile, in recent years there has been increasing attention paid to developing machine learning models that can provide explanations and justifications for their decisions which has led to growing interest in interpretability research in machine learning fields. However, most existing popular node embeddings methods like DeepWalk and LINE do not focus on providing interpretable results because they learn latent features by maximizing random walks likelihoods. This lack of transparency makes these methods difficult to use in real applications where explainability is essential. To address this problem, we propose t-PINE (Tensor-based predictable and interpretable node embeddings) - a novel method based on preserving tensor structures via optimization framework named autoencoder. Specifically, we formulate two kinds of tensor structure constraints (elementwise and hadamard product-based low-rankness) to encourage different property of node representations, making them easy for human interpretation and visualization purposes while maintaining high effectiveness compared to other baselines. Moreover, our approach benefits from a unique advantage where humans are allowed to control what kind properties should be preserved while model training, which further improves the controllability and applicability of learned node embeddings towards diverse downstream task requirements in future works. Experiment results on three benchmark datasets verify that t-PINE achieves superior performances over state-of-the-art competitors under all evaluation metrics and demonstrates better visualizability than any other models with various analysis experiments conducted towar",1
"Machine Learning (ML) is increasingly being used for computer aided diagnosis of brain related disorders based on structural magnetic resonance imaging (MRI) data. Most of such work employs biologically and medically meaningful hand-crafted features calculated from different regions of the brain. The construction of such highly specialized features requires a considerable amount of time, manual oversight and careful quality control to ensure the absence of errors in the computational process. Recent advances in Deep Representation Learning have shown great promise in extracting highly non-linear and information-rich features from data. In this paper, we present a novel large-scale deep unsupervised approach to learn generic feature representations of structural brain MRI scans, which requires no specialized domain knowledge or manual intervention. Our method produces low-dimensional representations of brain structure, which can be used to reconstruct brain images with very low error and exhibit performance comparable to FreeSurfer features on various classification tasks.",0
"In recent years, there has been growing interest in using machine learning algorithms to analyze brain structure data. One key challenge in this field is the lack of labeled training data, which can make it difficult to train accurate models. To address this issue, we present a novel approach for unsupervised deep representation learning that leverages large amounts of raw brain structure data without any manual labeling. Our method uses convolutional autoencoders to learn representations that capture structural features of the brain at multiple scales. We evaluate our model on several benchmark datasets and demonstrate significant improvements over traditional methods in terms of both reconstruction accuracy and feature extraction performance. Our results have important implications for advancing the use of machine learning in neuroscience research and could lead to new insights into how the human brain processes information.",1
"While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.",0
In this paper we present our approach for learning cross-domain disentangled deep representations using adversarial training. Our method involves optimizing a variational autoencoder (VAE) across two domains while encouraging alignment between the encoders from both domains. We achieve this by introducing a domain discriminator that explicitly enforces alignment during training. Experiments on three benchmark datasets show that our approach significantly outperforms prior methods in transferring knowledge gained from one domain to another while preserving relevant features in each task. Our findings contribute towards advancing domain adaptation research and open new opportunities in areas such as computer vision and natural language processing where model performance must adapt to new data distributions.,1
"Person re-identification (re-ID) requires rapid, flexible yet discriminant representations to quickly generalize to unseen observations on-the-fly and recognize the same identity across disjoint camera views. Recent effective methods are developed in a pair-wise similarity learning system to detect a fixed set of features from distinct regions which are mapped to their vector embeddings for the distance measuring. However, the most relevant and crucial parts of each image are detected independently without referring to the dependency conditioned on one and another. Also, these region based methods rely on spatial manipulation to position the local features in comparable similarity measuring. To combat these limitations, in this paper we introduce the Deep Co-attention based Comparators (DCCs) that fuse the co-dependent representations of the paired images so as to focus on the relevant parts of both images and produce their \textit{relative representations}. Given a pair of pedestrian images to be compared, the proposed model mimics the foveation of human eyes to detect distinct regions concurrent on both images, namely co-dependent features, and alternatively attend to relevant regions to fuse them into the similarity learning. Our comparator is capable of producing dynamic representations relative to a particular sample every time, and thus well-suited to the case of re-identifying pedestrians on-the-fly. We perform extensive experiments to provide the insights and demonstrate the effectiveness of the proposed DCCs in person re-ID. Moreover, our approach has achieved the state-of-the-art performance on three benchmark data sets: DukeMTMC-reID \cite{DukeMTMC}, CUHK03 \cite{FPNN}, and Market-1501 \cite{Market1501}.",0
"In recent years, person re-identification has become increasingly important due to its wide range of applications such as video surveillance and personalized services. However, traditional methods suffer from limitations including intra-class variations caused by changing environment conditions or viewpoints. To overcome these limitations, we propose a novel comparator model that utilizes deep co-attention mechanisms to learn relative representations effectively. Our method first generates multiple feature maps using different backbones under varying conditions and then uses attention modules to aggregate relevant features. By doing so, our model can handle complex environments while achieving higher accuracy in both cross-dataset evaluation and online retrieval tasks. Our experimental results demonstrate that the proposed approach outperforms state-of-the-art baseline models significantly on several public benchmark datasets. Overall, our work shows promise in enhancing the performance of person re-identification systems through effective learning of relative representations.",1
"Video representation learning is a vital problem for classification task. Recently, a promising unsupervised paradigm termed self-supervised learning has emerged, which explores inherent supervisory signals implied in massive data for feature learning via solving auxiliary tasks. However, existing methods in this regard suffer from two limitations when extended to video classification. First, they focus only on a single task, whereas ignoring complementarity among different task-specific features and thus resulting in suboptimal video representation. Second, high computational and memory cost hinders their application in real-world scenarios. In this paper, we propose a graph-based distillation framework to address these problems: (1) We propose logits graph and representation graph to transfer knowledge from multiple self-supervised tasks, where the former distills classifier-level knowledge by solving a multi-distribution joint matching problem, and the latter distills internal feature knowledge from pairwise ensembled representations with tackling the challenge of heterogeneity among different features; (2) The proposal that adopts a teacher-student framework can reduce the redundancy of knowledge learnt from teachers dramatically, leading to a lighter student model that solves classification task more efficiently. Experimental results on 3 video datasets validate that our proposal not only helps learn better video representation but also compress model for faster inference.",0
"Successful use cases like YouTube Batch Normalization (YouTubeNet) have shown that self-supervised learning (SSL) can enable transferring knowledge across multiple tasks using pretrained models on smaller datasets. However, SSL often leads to suboptimal solutions due to local minima when training objectives differ across tasks, which limits performance. This paper proposes a novel approach called graph distillation to effectively fuse knowledge from multiple SSL tasks into one model for video classification. Our method first trains a shared backbone network for feature extraction, then generates multiple task graphs to preserve task-specific discriminative features. Next, we integrate these graphs into one graph structure using a fusion mechanism inspired by graph neural networks (GNNs). Finally, our algorithm utilizes entropy minimization to regularize the distribution of node embeddings to ensure smooth optimization during distillation. Experimental results demonstrate that our proposed framework outperforms existing state-of-the-art methods across four benchmark datasets including HMDB51 and UCF101, achieving better generalization ability without increasing computation cost. The code will be made publicly available to facilitate research reproducibility and further advancements in the field.",1
"In this study, we propose the integration of competitive learning into convolutional neural networks (CNNs) to improve the representation learning and efficiency of fine-tuning. Conventional CNNs use back propagation learning, and it enables powerful representation learning by a discrimination task. However, it requires huge amount of labeled data, and acquisition of labeled data is much harder than that of unlabeled data. Thus, efficient use of unlabeled data is getting crucial for DNNs. To address the problem, we introduce unsupervised competitive learning into the convolutional layer, and utilize unlabeled data for effective representation learning. The results of validation experiments using a toy model demonstrated that strong representation learning effectively extracted bases of images into convolutional filters using unlabeled data, and accelerated the speed of the fine-tuning of subsequent supervised back propagation learning. The leverage was more apparent when the number of filters was sufficiently large, and, in such a case, the error rate steeply decreased in the initial phase of fine-tuning. Thus, the proposed method enlarged the number of filters in CNNs, and enabled a more detailed and generalized representation. It could provide a possibility of not only deep but broad neural networks.",0
"This research investigates how competitive learning can enhance the quality of learned representations during pre-training and accelerate fine-tuning performance on image classification tasks using convolutional neural networks (CNNs). Through extensive experiments, we demonstrate that incorporating competitive learning into the training process improves both representation fidelity and generalization ability. Our results show significant gains over traditional methods across multiple datasets, including CIFAR-10/100, ImageNet, and iNaturalist. These improvements translate into more efficient fine-tuning, requiring fewer iterations while achieving equal or better accuracy than state-of-the-art techniques. In summary, our findings highlight the value of integrating competition mechanisms in deep learning models and suggest new directions for designing efficient architectures in computer vision applications.",1
"Learning the disentangled representation of interpretable generative factors of data is one of the foundations to allow artificial intelligence to think like people. In this paper, we propose the analogical training strategy for the unsupervised disentangled representation learning in generative models. The analogy is one of the typical cognitive processes, and our proposed strategy is based on the observation that sample pairs in which one is different from the other in one specific generative factor show the same analogical relation. Thus, the generator is trained to generate sample pairs from which a designed classifier can identify the underlying analogical relation. In addition, we propose a disentanglement metric called the subspace score, which is inspired by subspace learning methods and does not require supervised information. Experiments show that our proposed training strategy allows the generative models to find the disentangled factors, and that our methods can give competitive performances as compared with the state-of-the-art methods.",0
"This paper presents a method for unsupervised learning of disentangled representations using analogies. We formulate representation learning as an inverse problem, where we aim to learn a latent code that predicts the input data distribution up to some entropy regularization. Our key insight is that we can leverage analogy relationships within the dataset to encourage the learning of interpretable and disentangled features. In particular, we introduce a new technique called analogy inference, which finds semantically meaningful directions in feature space by solving linear systems derived from multiple semantic analogies. We apply these directions to perform gradient descent updates to maximize the model evidence log likelihood. Experimental results show that our method significantly outperforms competing baselines on tasks such as image generation and semantic manipulation, demonstrating the effectiveness of incorporating relational knowledge into unsupervised representation learning.",1
"Methods for learning feature representations for Offline Handwritten Signature Verification have been successfully proposed in recent literature, using Deep Convolutional Neural Networks to learn representations from signature pixels. Such methods reported large performance improvements compared to handcrafted feature extractors. However, they also introduced an important constraint: the inputs to the neural networks must have a fixed size, while signatures vary significantly in size between different users. In this paper we propose addressing this issue by learning a fixed-sized representation from variable-sized signatures by modifying the network architecture, using Spatial Pyramid Pooling. We also investigate the impact of the resolution of the images used for training, and the impact of adapting (fine-tuning) the representations to new operating conditions (different acquisition protocols, such as writing instruments and scan resolution). On the GPDS dataset, we achieve results comparable with the state-of-the-art, while removing the constraint of having a maximum size for the signatures to be processed. We also show that using higher resolutions (300 or 600dpi) can improve performance when skilled forgeries from a subset of users are available for feature learning, but lower resolutions (around 100dpi) can be used if only genuine signatures are used. Lastly, we show that fine-tuning can improve performance when the operating conditions change.",0
"Here we present a new methodology for fixed-size representation learning from offline handwriting signatures that varies greatly in size. Our approach employs state-of-the-art techniques from deep learning to generate low-dimensional embeddings which accurately capture the characteristics of these diverse writing samples. We demonstrate the effectiveness of our framework through rigorous experimentation on two benchmark datasets and show that it significantly outperforms traditional methods in terms of classification accuracy. Additionally, we provide qualitative analysis to further validate the robustness and generalizability of our learned representations across different signature types. This work has important implications for applications in secure authentication systems where consistency in performance irrespective of signature size is crucial.",1
"From the frame/clip-level feature learning to the video-level representation building, deep learning methods in action recognition have developed rapidly in recent years. However, current methods suffer from the confusion caused by partial observation training, or without end-to-end learning, or restricted to single temporal scale modeling and so on. In this paper, we build upon two-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling (DTPP), an end-to-end video-level representation learning approach, to address these problems. Specifically, at first, RGB images and optical flow stacks are sparsely sampled across the whole video. Then a temporal pyramid pooling layer is used to aggregate the frame-level features which consist of spatial and temporal cues. Lastly, the trained model has compact video-level representation with multiple temporal scales, which is both global and sequence-aware. Experimental results show that DTPP achieves the state-of-the-art performance on two challenging video action datasets: UCF101 and HMDB51, either by ImageNet pre-training or Kinetics pre-training.",0
"This paper presents a new method for learning video-level representations that can accurately recognize actions performed within those videos. Our approach uses convolutional neural networks (CNNs) to learn features from raw frames, which are then combined into a representation of the entire video. We use these end-to-end learned representations to classify a wide range of human actions, including simple activities like walking, running, and jumping, as well as more complex tasks such as playing sports, cooking, and cleaning. To evaluate our system, we conduct experiments on four public datasets and show that our approach achieves state-of-the-art performance across all metrics, outperforming both handcrafted feature extraction methods and other deep learning approaches. Furthermore, we demonstrate that our learne",1
"Voxelwise classification approaches are popular and effective methods for tissue quantification in brain magnetic resonance imaging (MRI) scans. However, generalization of these approaches is hampered by large differences between sets of MRI scans such as differences in field strength, vendor or acquisition protocols. Due to this acquisition related variation, classifiers trained on data from a specific scanner fail or under-perform when applied to data that was acquired differently. In order to address this lack of generalization, we propose a Siamese neural network (MRAI-net) to learn a representation that minimizes the between-scanner variation, while maintaining the contrast between brain tissues necessary for brain tissue quantification. The proposed MRAI-net was evaluated on both simulated and real MRI data. After learning the MR acquisition invariant representation, any supervised classification model that uses feature vectors can be applied. In this paper, we provide a proof of principle, which shows that a linear classifier applied on the MRAI representation is able to outperform supervised convolutional neural network classifiers for tissue classification when little target training data is available.",0
"Machine learning algorithms are increasingly used to perform tasks such as object recognition, image generation, and machine translation, and there are many open challenges in designing models that can generalize well across different settings and conditions. One key challenge facing these approaches arises due to changes in the acquisition process, which may lead to significant differences in distribution shifts and other forms of variability. This paper proposes a method for learning representations from a collection of images acquired using different protocols, without requiring any explicit supervision or calibration. By leveraging insights from domain adaptation theory, we introduce novel loss functions that regularize the model to produce outputs that remain consistent under shifts in data distributions caused by changing imaging parameters. Our results show state-of-the-art performance on several benchmark datasets, demonstrating the effectiveness of our proposed framework. Overall, our work contributes towards improving the robustness and flexibility of deep neural networks in medical applications where variations in acquisition processes are commonplace.",1
"Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the deep information bottleneck model, identify its shortcomings and propose a model that circumvents them. To this end, we apply a copula transformation which, by restoring the invariance properties of the information bottleneck method, leads to disentanglement of the features in the latent space. Building on that, we show how this transformation translates to sparsity of the latent space in the new model. We evaluate our method on artificial and real data.",0
"In recent years, deep learning has proven extremely effective at solving challenging problems across multiple domains, including computer vision, natural language processing, and speech recognition. However, the success of these methods often comes at the expense of interpretability and explainability, which limits their applicability in certain fields such as healthcare and finance. To address this issue, researchers have proposed a variety of techniques that aim to regularize deep neural networks, encourage sparsity, and promote latent representations with meaningful semantic interpretation. One promising approach is based on the copula function, which allows for joint probability modeling in high dimensions. This work presents an alternative formulation based on the Deep Copula Information Bottleneck (DCIB) principle, which leverages sparse coding and matrix factorization to learn compact, interpretable representations of complex data distributions. We showcase the effectiveness of our method on several real-world tasks, demonstrating state-of-the-art performance while providing explicit guarantees regarding variable importance ranking and feature selection. Our results open up new possibilities for developing robust, transparent artificial intelligence systems applicable to both academic and industrial settings.",1
"This thesis investigates unsupervised time series representation learning for sequence prediction problems, i.e. generating nice-looking input samples given a previous history, for high dimensional input sequences by decoupling the static input representation from the recurrent sequence representation. We introduce three models based on Generative Stochastic Networks (GSN) for unsupervised sequence learning and prediction. Experimental results for these three models are presented on pixels of sequential handwritten digit (MNIST) data, videos of low-resolution bouncing balls, and motion capture data. The main contribution of this thesis is to provide evidence that GSNs are a viable framework to learn useful representations of complex sequential input data, and to suggest a new framework for deep generative models to learn complex sequences by decoupling static input representations from dynamic time dependency representations.",0
"Recent advances in deep learning have allowed researchers to design powerful generative models that can generate realistic sequences of data from scratch, such as text, images, and speech signals. These sequence prediction problems often involve modeling complex dependencies between different elements of a sequence, which makes them challenging tasks. This paper presents a novel approach based on deep generative networks (DGN) that can accurately predict missing values or entire sequences given incomplete input. Our DGN architecture uses a latent variable representation, which allows us to better capture the underlying structure of the problem at hand. We evaluate our method using several benchmark datasets and demonstrate its superior performance compared to state-of-the-art baselines. Additionally, we provide ablation studies to analyze the contributions made by each component of our proposed framework. Overall, our work represents a significant step forward in solving high-dimensional sequence prediction problems using deep neural network architectures, and opens up exciting new possibilities for future research.",1
"Data of different modalities generally convey complimentary but heterogeneous information, and a more discriminative representation is often preferred by combining multiple data modalities like the RGB and infrared features. However in reality, obtaining both data channels is challenging due to many limitations. For example, the RGB surveillance cameras are often restricted from private spaces, which is in conflict with the need of abnormal activity detection for personal security. As a result, using partial data channels to build a full representation of multi-modalities is clearly desired. In this paper, we propose a novel Partial-modal Generative Adversarial Networks (PM-GANs) that learns a full-modal representation using data from only partial modalities. The full representation is achieved by a generated representation in place of the missing data channel. Extensive experiments are conducted to verify the performance of our proposed method on action recognition, compared with four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset for action recognition is introduced, and will be the first publicly available action dataset that contains paired infrared and visible spectrum.",0
"The field of action recognition has seen significant advancements due to deep learning techniques such as Generative Adversarial Networks (GAN). These models have been successful in generating synthetic data that can be used for training more accurate classifiers. However, current GAN approaches use full modalities which may result in less discriminative representations compared to partial-modality representations where only a few important aspects of the actions are considered. This study proposes the usage of Partial Modalities GANs (PM-GAN) that learn better discriminative representation for action recognition by utilizing partial modality information during both generative and discriminative phases of training. Our experiments on benchmark datasets show improved performance over existing state-of-the-art methods using full modality information. Additionally, we provide an analysis of how our proposed approach performs under different levels of partial modality settings thereby demonstrating its robustness across varying scenarios. Overall, this work presents a novel technique towards achieving high quality generation of synthetic data for effective action recognition systems.",1
"Reliably modeling normality and differentiating abnormal appearances from normal cases is a very appealing approach for detecting pathologies in medical images. A plethora of such unsupervised anomaly detection approaches has been made in the medical domain, based on statistical methods, content-based retrieval, clustering and recently also deep learning. Previous approaches towards deep unsupervised anomaly detection model patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned feature space or from large reconstruction errors. In contrast to these patch-based approaches, we show that deep spatial autoencoding models can be efficiently used to capture normal anatomical variability of entire 2D brain MR images. A variety of experiments on real MR data containing MS lesions corroborates our hypothesis that we can detect and even delineate anomalies in brain MR images by simply comparing input images to their reconstruction. Results show that constraints on the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning.",0
"""This paper presents a new method for unsupervised anomaly segmentation in brain MRI images using deep autoencoder models. The proposed approach leverages recent advances in deep learning to learn representations that capture underlying patterns in normal brain structure and function. By training these models on large datasets of healthy subjects, we can identify features that distinguish normal from abnormal tissue appearance, even without any labeled examples of pathology. Our results demonstrate superior performance compared to state-of-the-art methods, including improved sensitivity and specificity in identifying key clinical indicators such as tumors and lesions. This work has important implications for automating the analysis of brain imaging data, enabling earlier detection and more accurate diagnosis of neurological disorders.""",1
"This paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural networks (CNNs) have brought significant advances in image segmentation. However, most of the recent methods rely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for these methods to cope with the growing amount of medical images. This paper proposes a unified approach to unsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two phases. In the first phase, we learn deep feature representations of training patches from a target image using joint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates the CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by utilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep representations from the trained CNN and then project cluster labels to the target image in order to obtain the fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with micro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could further contribute to the pathological examination process. Hence, we aim to automatically divide each image into the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the potential abilities of unsupervised deep representation learning for medical image segmentation.",0
"Abstract: This paper presents a novel unsupervised segmentation method for three-dimensional medical images based on clustering and deep representation learning. The proposed approach first extracts features from each voxel using a convolutional neural network (CNN) to learn high-level representations that capture the intrinsic characteristics of the image data. These representations are then used as input for a k-means clustering algorithm, which groups similar voxels together into distinct clusters. Finally, we apply a postprocessing step to refine the segmentation results by merging neighboring clusters and smoothing boundaries. We evaluate our method on two publicly available datasets, including both magnetic resonance imaging (MRI) and computed tomography (CT) scans. Experimental results demonstrate that our method achieves superior performance compared to traditional unsupervised methods and outperforms several state-of-the-art supervised methods. Our framework has the potential to improve automated segmentation in computer-aided diagnosis and treatment planning applications, enabling more accurate and efficient analysis of complex medical images.",1
"This paper presents a novel method for unsupervised segmentation of pathology images. Staging of lung cancer is a major factor of prognosis. Measuring the maximum dimensions of the invasive component in a pathology images is an essential task. Therefore, image segmentation methods for visualizing the extent of invasive and noninvasive components on pathology images could support pathological examination. However, it is challenging for most of the recent segmentation methods that rely on supervised learning to cope with unlabeled pathology images. In this paper, we propose a unified approach to unsupervised representation learning and clustering for pathology image segmentation. Our method consists of two phases. In the first phase, we learn feature representations of training patches from a target image using the spherical k-means. The purpose of this phase is to obtain cluster centroids which could be used as filters for feature extraction. In the second phase, we apply conventional k-means to the representations extracted by the centroids and then project cluster labels to the target images. We evaluated our methods on pathology images of lung cancer specimen. Our experiments showed that the proposed method outperforms traditional k-means segmentation and the multithreshold Otsu method both quantitatively and qualitatively with an improved normalized mutual information (NMI) score of 0.626 compared to 0.168 and 0.167, respectively. Furthermore, we found that the centroids can be applied to the segmentation of other slices from the same sample.",0
"Medical image analysis has recently become more prominent due to technological advancements in imaging modalities such as CT and MRI scanners, which produce large amounts of complex data that need to be analyzed effectively by computers before they can be used for diagnosis purposes. One important task in medical image analysis is pathology image segmentation, which involves extracting relevant features from images of tissue samples to assist physicians in their diagnoses. This paper presents a new method for unsupervised pathology image segmentation using representation learning with spherical k-means clustering. Our approach uses deep convolutional neural networks (CNNs) pretrained on large public datasets to learn representations of tissue images. We then apply a spherical k-means algorithm to cluster these learned representations into multiple regions that correspond to different histopathologic features commonly seen in cancerous tissues, such as tumor epithelium, stroma, and lymphocytes. Experimental results show that our method outperforms previous state-of-the-art methods for unsupervised pathology image segmentation in terms of both accuracy and efficiency. Additionally, we demonstrate how our approach can be applied to a real clinical dataset to aid pathologists in identifying critical structures in breast biopsy specimens. Overall, our work represents a significant step forward in developing automated tools for improving the speed and quality of pathology diagnostics.",1
"In this paper, we introduce a method for adapting the step-sizes of temporal difference (TD) learning. The performance of TD methods often depends on well chosen step-sizes, yet few algorithms have been developed for setting the step-size automatically for TD learning. An important limitation of current methods is that they adapt a single step-size shared by all the weights of the learning system. A vector step-size enables greater optimization by specifying parameters on a per-feature basis. Furthermore, adapting parameters at different rates has the added benefit of being a simple form of representation learning. We generalize Incremental Delta Bar Delta (IDBD)---a vectorized adaptive step-size method for supervised learning---to TD learning, which we name TIDBD. We demonstrate that TIDBD is able to find appropriate step-sizes in both stationary and non-stationary prediction tasks, outperforming ordinary TD methods and TD methods with scalar step-size adaptation; we demonstrate that it can differentiate between features which are relevant and irrelevant for a given task, performing representation learning; and we show on a real-world robot prediction task that TIDBD is able to outperform ordinary TD methods and TD methods augmented with AlphaBound and RMSprop.",0
"Title: Temporal Difference Learning with Adaptive Step Sizing  Abstract: This paper presents a novel approach to adaptively adjust the step size used in temporal difference (TD) learning algorithms. In contrast to traditional methods that rely on fixed or manually tuned step sizes, our method uses stochastic meta descent to update the step size based on performance feedback from the algorithm. Our proposed framework allows for more efficient exploration and exploitation during reinforcement learning, leading to improved convergence speed and better final policy performance compared to existing approaches. We evaluate our method using both simulation experiments and real world robotic tasks, demonstrating its effectiveness across a range of environments and challenges. Overall, our work provides insights into the role of step size adaptation in reinforcement learning, and opens up new opportunities for developing more efficient and effective RL algorithms.",1
"Being able to predict what may happen in the future requires an in-depth understanding of the physical and causal rules that govern the world. A model that is able to do so has a number of appealing applications, from robotic planning to representation learning. However, learning to predict raw future observations, such as frames in a video, is exceedingly challenging -- the ambiguous nature of the problem can cause a naively designed model to average together possible futures into a single, blurry prediction. Recently, this has been addressed by two distinct approaches: (a) latent variational variable models that explicitly model underlying stochasticity and (b) adversarially-trained models that aim to produce naturalistic images. However, a standard latent variable model can struggle to produce realistic results, and a standard adversarially-trained model underutilizes latent variables and fails to produce diverse predictions. We show that these distinct methods are in fact complementary. Combining the two produces predictions that look more realistic to human raters and better cover the range of possible futures. Our method outperforms prior and concurrent work in these aspects.",0
"Here's the paper abstract:  ""Video prediction has been gaining attention recently as a challenging problem that requires accurate modeling of dynamic scenes over time. In this work, we introduce a new approach to video prediction using stochastic adversarial models. Our method consists of two main components: a generative network that generates future frames given the current context, and a discriminator network that evaluates the realism of each predicted frame. We train these networks together using a novel adversarial loss function that encourages both networks to improve their performance. To overcome the issue of exposure bias commonly seen in other approaches, we further incorporate a stochastic component into our generator that randomizes certain aspects of the generated image at every step. This leads to more diverse predictions and better generalization ability. Experimental results on several benchmark datasets show that our approach outperforms previous state-of-the-art methods significantly while producing qualitatively plausible outputs.""  Is there something specific you want me to focus on or emphasize? I can make adjustments based on your feedback!",1
"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.",0
"In many planning domains there exist universal plans which can achieve goals that depend solely on environmental factors such as temperature, altitude, location, etc. By creating artificial neural networks trained to find these plans we can vastly reduce the computational complexity of finding plans in most domains while still solving all problems quickly. We evaluate our method across six classical control benchmark tasks and one novel problem (MuJoCo Mountain Car) and show its effectiveness at producing high quality solutions faster than competing methods. Our work provides evidence that universal plans often exist in real world applications outside of carefully constructed synthetic tasks like those used by previous works. ------",1
"In this paper, we explore neural network models that learn to associate segments of spoken audio captions with the semantically relevant portions of natural images that they refer to. We demonstrate that these audio-visual associative localizations emerge from network-internal representations learned as a by-product of training to perform an image-audio retrieval task. Our models operate directly on the image pixels and speech waveform, and do not rely on any conventional supervision in the form of labels, segmentations, or alignments between the modalities during training. We perform analysis using the Places 205 and ADE20k datasets demonstrating that our models implicitly learn semantically-coupled object and word detectors.",0
"Our work explores how machines can learn to perceive the world as humans do by jointly discovering both visual objects and spoken words from raw sensory input. We present a new architecture called Visually Grounded Language Learning (VGLL) that integrates vision and language components into a unified framework. VGLL leverages synchronous processing of multi-modal data streams, allowing it to align representations at multiple levels of abstraction. Experimental results show significant improvements over state-of-the-art systems on challenging tasks such as referring expression comprehension and grounded commonsense reasoning. Additionally, we demonstrate VGLL’s capabilities on real-world applications like robotic manipulation and image/text retrieval. Overall, our research paves the way towards building intelligent agents capable of understanding complex human environments and communicating effectively in natural languages.",1
"Learning compact representation is vital and challenging for large scale multimedia data. Cross-view/cross-modal hashing for effective binary representation learning has received significant attention with exponentially growing availability of multimedia content. Most existing cross-view hashing algorithms emphasize the similarities in individual views, which are then connected via cross-view similarities. In this work, we focus on the exploitation of the discriminative information from different views, and propose an end-to-end method to learn semantic-preserving and discriminative binary representation, dubbed Discriminative Cross-View Hashing (DCVH), in light of learning multitasking binary representation for various tasks including cross-view retrieval, image-to-image retrieval, and image annotation/tagging. The proposed DCVH has the following key components. First, it uses convolutional neural network (CNN) based nonlinear hashing functions and multilabel classification for both images and texts simultaneously. Such hashing functions achieve effective continuous relaxation during training without explicit quantization loss by using Direct Binary Embedding (DBE) layers. Second, we propose an effective view alignment via Hamming distance minimization, which is efficiently accomplished by bit-wise XOR operation. Extensive experiments on two image-text benchmark datasets demonstrate that DCVH outperforms state-of-the-art cross-view hashing algorithms as well as single-view image hashing algorithms. In addition, DCVH can provide competitive performance for image annotation/tagging.",0
"In this paper we focus on learning discriminative binary representations (embeddings) that generalize across different views of the same object. This problem arises frequently in practice whenever one wants to compare pairs of objects in some high-dimensional space: One can imagine two pictures of the same person from slightly different angles, or even two very different looking pictures if they represent the same concept. We introduce a novel method called Discriminative Cross-View Binary Representation Learning (DCRL). DCRL consists of three main components, each addressing a specific challenge of cross-view representation learning. First, our method uses data augmentation in conjunction with adversarial training and contrastive loss to learn embeddings that encode viewpoint invariant features. Secondly, we propose using a siamese neural network architecture to simultaneously train multiple instances in order to make use of shared knowledge among related object pairs. Lastly, our approach explicitly models both intra-class similarity as well as inter-class separability during the optimization process. Our experiments show significant improvements over baseline methods on four datasets with both rigid and non-rigid transformations while achieving state-of-the-art performance on most tasks.",1
"Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.",0
"This work presents a novel approach to variational autoencoders (VAEs) that utilizes triplets instead of pairs for training. The proposed method, referred to as TVAE: Triplet-based Variational Autoencoder using Metric Learning, improves upon traditional VAEs by incorporating metric learning into the framework, allowing for better reconstruction accuracy and robustness to noise. We demonstrate the effectiveness of our method on several benchmark datasets, achieving state-of-the-art results in many cases. Our approach has potential applications in fields such as computer vision, natural language processing, and graph embedding, and can serve as a powerful tool for dimensionality reduction and representation learning. Overall, our work makes a significant contribution to the field of deep generative models and has the potential to inspire future research in this area.",1
"Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.",0
"This can be done by using a GPT language model and then writing down all the generated sentences that fit into your requested criteria (you might have to manually edit them afterwards though). Here we go! --->  Dear Assistant, I would like you to write an abstract based on our new paper titled ""Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation"". Please refrain from mentioning the paper title directly within the text. Thank you!",1
"Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually, local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that by employing a \textit{simple} yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 by ~17% in mAP and ~6% in rank-1, CUHK03 by ~4% in rank-1 and DukeMTMC-reID by ~24% in mAP and ~10% in rank-1.",0
"This paper presents human semantic parsing as a key enabler of person re-identification tasks. We introduce human-inspired features such as objectness score maps and bounding boxes generated from visual scenes through human annotation, which improve performance by providing higher quality region proposals compared to traditional computer vision methods that solely rely on heuristics and machine learning models. Our work demonstrates that these features can successfully replace time consuming and costly manual annotations for many applications related to multi-target tracking, face detection and recognition, image search, among others. In summary, our approach paves the way towards more effective and efficient scene understanding algorithms based on human expert knowledge.",1
"Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---indeed, many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks.",0
"In the paper we propose video representation learning using discriminative pooling (VRLDP), an approach that addresses both these problems by jointly training a convolutional neural network on frame-level class labels as well as discriminator networks that predict whether patch pairs belong together or not. Our method has several desirable properties: 1) it learns representations that capture inter-frame dependencies; 2) it is robust to background changes and occlusions; and 3) it can handle videos where important events occur off screen. Extensive experiments demonstrate significant improvements over strong baselines on four challenging action recognition benchmarks, and we show qualitatively that VRLDP captures event-specific dynamics such as interactions among people. Finally, we provide analysis showing that our learned features encode complex spatio-temporal patterns that are highly correlated with human judgments of visual similarity, supporting the effectiveness of our method.",1
"It is of high interest for a company to identify customers expected to bring the largest profit in the upcoming period. Knowing as much as possible about each customer is crucial for such predictions. However, their demographic data, preferences, and other information that might be useful for building loyalty programs is often missing. Additionally, modeling relations among different customers as a network can be beneficial for predictions at an individual level, as similar customers tend to have similar purchasing patterns. We address this problem by proposing a robust framework for structured regression on deficient data in evolving networks with a supervised representation learning based on neural features embedding. The new method is compared to several unstructured and structured alternatives for predicting customer behavior (e.g. purchasing frequency and customer ticket) on user networks generated from customer databases of two companies from different industries. The obtained results show $4\%$ to $130\%$ improvement in accuracy over alternatives when all customer information is known. Additionally, the robustness of our method is demonstrated when up to $80\%$ of demographic information was missing where it was up to several folds more accurate as compared to alternatives that are either ignoring cases with missing values or learn their feature representation in an unsupervised manner.",0
"In recent years, customer engagement has become increasingly important for businesses as they seek to improve their overall customer experience and drive growth. However, measuring customer engagement can be challenging due to the difficulty in collecting complete data on customer behavior. This study proposes a methodology for modeling customer engagement using partial observations obtained through various channels such as social media, website interactions, and customer service inquiries.  The proposed method leverages machine learning techniques to identify patterns and correlations among different types of customer interactions, enabling the creation of predictive models that can estimate levels of engagement based on limited observation data. Experimental results show promising performance gains compared to baseline approaches that rely solely on individual interaction metrics. Furthermore, sensitivity analysis demonstrates robustness against variations in input data quality and quantity.  Overall, this research represents an initial effort towards developing more accurate and reliable methods for tracking customer engagement without requiring comprehensive historical data or direct customer feedback. Future work includes refining the predictive models using additional sources of data, integrating user contextual factors into the analysis pipeline, and exploring applications for personalized marketing campaigns.",1
"Disentangling factors of variation has become a very challenging problem on representation learning. Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, poor quality of generated images from encodings, lack of identity information, etc. In this paper, we propose a supervised learning model called DNA-GAN which tries to disentangle different factors or attributes of images. The latent representations of images are DNA-like, in which each individual piece (of the encoding) represents an independent factor of the variation. By annihilating the recessive piece and swapping a certain piece of one latent representation with that of the other one, we obtain two different representations which could be decoded into two kinds of images with the existence of the corresponding attribute being changed. In order to obtain realistic images and also disentangled representations, we further introduce the discriminator for adversarial training. Experiments on Multi-PIE and CelebA datasets finally demonstrate that our proposed method is effective for factors disentangling and even overcome certain limitations of the existing methods.",0
"This looks like a great example of how artificial intelligence can help us better understand complex biological systems like DNA. By using advanced machine learning techniques such as GANs (Generative Adversarial Networks), researchers were able to generate high quality images that accurately capture the underlying structure of DNA. This technology has significant potential applications across many areas, including medicine, agriculture, and bioengineering. Overall, this study represents a major step forward in our ability to model genetic material, and could have important implications for a wide range of fields.",1
"We present a parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction networks. Using several instantiations of the dataset in which variation is explicitly controlled, we highlight issues in an existing state-of-the-art approach and propose the use of a performance metric with greater semantic meaning to improve experimental interpretability. Our dataset provides canonical test cases that will help the community better understand, and eventually improve, the representations learned by such networks in the future. Code is available at https://github.com/rszeto/moving-symbols .",0
"This paper presents a new dataset that can be used to evaluate video prediction models. The dataset consists of high quality videos captured from real world scenes using specialized cameras. The data includes annotations for objects and their trajectories which makes it suitable for evaluating the representational capabilities of state-of-the art deep learning models. We validate our dataset by comparing predictions generated by several video prediction methods against ground truth sequences. Our results show that the proposed dataset is challenging enough to distinguish between different architectures and variants, while at the same time easy enough for initializing models. Finally we release our dataset publicly so everyone can train on them.",1
"Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .",0
"This paper introduces a new method for unsupervised representation learning using deep neural networks (DNNs). In particular, we focus on DNNs trained on natural image data to predict rotational transformations of input images. We show that such models can learn powerful representations without any explicit supervision or task objectives other than prediction accuracy. Our approach outperforms previous methods on standard benchmark datasets and achieves state-of-the-art results on several challenging tasks including rotation estimation and visual recognition. Furthermore, we provide insights into how these representations capture hierarchical structures in high-dimensional data spaces through a detailed analysis of learned features at different levels of abstraction. Overall, our work highlights the potential benefits of self-supervised learning for building generalizable machine intelligence systems capable of handling complex sensory inputs in real-world environments.",1
"The meteoric rise of deep learning models in computer vision research, having achieved human-level accuracy in image recognition tasks is firm evidence of the impact of representation learning of deep neural networks. In the chemistry domain, recent advances have also led to the development of similar CNN models, such as Chemception, that is trained to predict chemical properties using images of molecular drawings. In this work, we investigate the effects of systematically removing and adding localized domain-specific information to the image channels of the training data. By augmenting images with only 3 additional basic information, and without introducing any architectural changes, we demonstrate that an augmented Chemception (AugChemception) outperforms the original model in the prediction of toxicity, activity, and solvation free energy. Then, by altering the information content in the images, and examining the resulting model's performance, we also identify two distinct learning patterns in predicting toxicity/activity as compared to solvation free energy. These patterns suggest that Chemception is learning about its tasks in the manner that is consistent with established knowledge. Thus, our work demonstrates that advanced chemical knowledge is not a pre-requisite for deep learning models to accurately predict complex chemical properties.",0
"Artificial intelligence (AI) has seen many breakthroughs over recent years, but there still exists fundamental questions that have yet been answered. One of these questions remains regarding how accurate predictions can be made without fully understanding the underlying chemical system at play within deep neural networks (DNN). This paper explores the question by comparing models trained on data sets where either only basic chemistry principles or both basic chemistry principles as well as DFT calculations were used. Our results showed that while adding more chemical knowledge did increase predictive accuracy, there was little difference observed between including DFT calculations and using basic chemistry principles alone. These findings challenge the idea that one must always strive to achieve full theoretical comprehension before making accurate predictions. Ultimately, this work presents evidence that the incorporation of even limited amounts of chemically motivated information into a model can lead to accurate outcomes, regardless of whether or not one uses quantum mechanics.",1
"Despite a lack of theoretical understanding, deep neural networks have achieved unparalleled performance in a wide range of applications. On the other hand, shallow representation learning with component analysis is associated with rich intuition and theory, but smaller capacity often limits its usefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA), an expressive multilayer model formulation that enforces hierarchical structure through constraints on latent variables in each layer. For inference, we propose a differentiable optimization algorithm implemented using recurrent Alternating Direction Neural Networks (ADNNs) that enable parameter learning using standard backpropagation. By interpreting feed-forward networks as single-iteration approximations of inference in our model, we provide both a novel theoretical perspective for understanding them and a practical technique for constraining predictions with prior knowledge. Experimentally, we demonstrate performance improvements on a variety of tasks, including single-image depth prediction with sparse output constraints.",0
"This should summarize the content of your scientific publication. A general summary of your work that allows us to understand what you did. Try to make clear why you think this work makes a significant contribution to science (e.g., because you were able to overcome some previous weaknesses). Use proper language! ---> Ok",1
"How can we effectively encode evolving information over dynamic graphs into low-dimensional representations? In this paper, we propose DyRep, an inductive deep representation learning framework that learns a set of functions to efficiently produce low-dimensional node embeddings that evolves over time. The learned embeddings drive the dynamics of two key processes namely, communication and association between nodes in dynamic graphs. These processes exhibit complex nonlinear dynamics that evolve at different time scales and subsequently contribute to the update of node embeddings. We employ a time-scale dependent multivariate point process model to capture these dynamics. We devise an efficient unsupervised learning procedure and demonstrate that our approach significantly outperforms representative baselines on two real-world datasets for the problem of dynamic link prediction and event time prediction.",0
"""This paper introduces a novel approach to representation learning over dynamic graphs using deep learning techniques. We propose a new model that can learn meaningful representations by capturing the temporal dynamics present in the graph structure. Our method combines state-of-the-art graph convolutional networks (GCN) and recurrent neural networks (RNN), enabling the model to learn both spatial and temporal dependencies from the data. The proposed framework allows us to handle time-varying graphs with different edge densities, making it applicable to many real world applications such as social network analysis, traffic flow prediction, and epidemiology modeling. Experimental results on several benchmark datasets demonstrate significant improvements over baseline methods.""",1
"Rectified linear units, or ReLUs, have become the preferred activation function for artificial neural networks. In this paper we consider two basic learning problems assuming that the underlying data follow a generative model based on a ReLU-network -- a neural network with ReLU activations. As a primarily theoretical study, we limit ourselves to a single-layer network. The first problem we study corresponds to dictionary-learning in the presence of nonlinearity (modeled by the ReLU functions). Given a set of observation vectors $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, we aim to recover $d\times k$ matrix $A$ and the latent vectors $\{\mathbf{c}^i\} \subset \mathbb{R}^k$ under the model $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i +\mathbf{b})$, where $\mathbf{b}\in \mathbb{R}^d$ is a random bias. We show that it is possible to recover the column space of $A$ within an error of $O(d)$ (in Frobenius norm) under certain conditions on the probability distribution of $\mathbf{b}$.   The second problem we consider is that of robust recovery of the signal in the presence of outliers, i.e., large but sparse noise. In this setting we are interested in recovering the latent vector $\mathbf{c}$ from its noisy nonlinear sketches of the form $\mathbf{v} = \mathrm{ReLU}(A\mathbf{c}) + \mathbf{e}+\mathbf{w}$, where $\mathbf{e} \in \mathbb{R}^d$ denotes the outliers with sparsity $s$ and $\mathbf{w} \in \mathbb{R}^d$ denote the dense but small noise. This line of work has recently been studied (Soltanolkotabi, 2017) without the presence of outliers. For this problem, we show that a generalized LASSO algorithm is able to recover the signal $\mathbf{c} \in \mathbb{R}^k$ within an $\ell_2$ error of $O(\sqrt{\frac{(k+s)\log d}{d}})$ when $A$ is a random Gaussian matrix.",0
"Abstract: This paper investigates representation learning and recovery methods in the Rectified Linear Unit (ReLU) model. We explore how different representations impact performance on a variety of tasks, including image classification and language translation. Our experiments show that careful design of network architecture can significantly improve accuracy, even without fine-tuning. Furthermore, we demonstrate that latent feature analysis techniques such as PCA and t-SNE can effectively visualize high-dimensional data sets obtained from deep neural networks. Finally, we present novel algorithms for recovering missing features in deep neural networks. These methods outperform state-of-the-art alternatives and have applications across computer vision, natural language processing, and speech recognition. In conclusion, our work underscores the importance of representation learning in modern machine learning research. By understanding how to most effectively use deep neural networks, we can tackle difficult problems in artificial intelligence that were previously unsolvable.",1
"Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classification. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classification adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches.",0
"In this work, we propose a novel approach to representation learning for domain adaptation that utilizes the concept of Wasserstein distance as a measure of discrepancy between distributions. We demonstrate how incorporating Wasserstein distance into the learning process can lead to more robust representations that better generalize across domains. Our method consists of two main components: a feature extractor network which maps inputs from both source and target domains onto a common high-dimensional space; and a discriminator network which uses the extracted features to compute a Wasserstein distance value reflecting the difference in distribution between the two sets of data. Our system then learns to minimize this distance by adjusting the weights of the feature extractor network until convergence. Experiments on multiple benchmark datasets show that our method outperforms state-of-the-art approaches in terms of accuracy and adaptability. By leveraging the power of deep neural networks together with the strengths of optimal transport theory, we have developed a powerful tool for achieving accurate and reliable cross-domain image classification tasks.",1
"Classification of sequence data is the topic of interest for dynamic Bayesian models and Recurrent Neural Networks (RNNs). While the former can explicitly model the temporal dependencies between class variables, the latter have a capability of learning representations. Several attempts have been made to improve performance by combining these two approaches or increasing the processing capability of the hidden units in RNNs. This often results in complex models with a large number of learning parameters. In this paper, a compact model is proposed which offers both representation learning and temporal inference of class variables by rolling Restricted Boltzmann Machines (RBMs) and class variables over time. We address the key issue of intractability in this variant of RBMs by optimising a conditional distribution, instead of a joint distribution. Experiments reported in the paper on melody modelling and optical character recognition show that the proposed model can outperform the state-of-the-art. Also, the experimental results on optical character recognition, part-of-speech tagging and text chunking demonstrate that our model is comparable to recurrent neural networks with complex memory gates while requiring far fewer parameters.",0
"Title: ""Linear-time sequence classification using restricted boltzman machines"" Authors: [Name(s)] Journal of Machine Learning Research, 21(X), YYYY DOI: XXXXXXXXXXXXX  This paper proposes a novel approach for linear-time sequence classification using restricted boltzmann machines (RBMs). RBMs have shown promising results in many machine learning tasks such as image recognition, natural language processing, and time series analysis. However, training RBMs on sequential data has been computationally challenging due to their inherent serial architecture and dynamic nature of the input sequence. This work addresses these limitations by presenting a linear-time algorithm that trains RBM models directly on raw data without any preprocessing steps.  The proposed method uses a parallel implementation of contrastive divergence to achieve efficient training of the model. Contrastive divergence is an iterative technique used in training deep belief networks (DBNs) which consists of alternating epochs of Gibbs sampling and stochastic gradient descent updates. Our method overcomes some drawbacks associated with conventional DBNs by introducing two key modifications. Firstly, we modify the energy function to consider sequence dependencies explicitly instead of relying exclusively on local reconstruction terms. Secondly, we design a new weight initialization scheme tailored specifically for sequence data, resulting in improved convergence rates compared to traditional techniques like Gaussian initializations.  We evaluate our method using several benchmark datasets including both synthetic and real-world sequences from various domains such as speech recognition and bioinformatics. Experiments show that our approach achieves state-of-the-art performance while maintaining linear computational complexity during training. Furthermore, we demonstrate the effectiveness of our framework by comparing its predictive accuracy against alternative methods based on neural networks, hidden Markov models, and support vector machines.  In summary, this work presents a breakthrough in using RBMs for sequence classification problems where timing is crucial. By leveraging paralleli",1
"Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research.   A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual connections naturally encourage features of residual blocks to move along the negative gradient of loss as we go from one block to the next. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and counterintuitively, overfitting, and we show that simple existing strategies can help alleviating this problem.",0
"In ""Residual Connections Encourage Iterative Inference,"" we explore the role of residual connections in deep neural networks (DNNs). We demonstrate that residual connections promote iterative processing by DNNs, increasing their accuracy on complex tasks. Our approach involves training several variants of popular architectures like ResNet and VGG with and without skip connections. Through extensive evaluation on CIFAR-10 and ImageNet datasets, we show that models with residual connections consistently outperform those without them. Furthermore, we study the effects of varying parameters such as batch size, network depth, and width on these residual connections. We find that our results generalize across multiple hyperparameter settings, suggesting robustness to architectural choices and data availability. Finally, we provide insights into how residual connections affect forward propagation dynamics within the model. Overall, our work contributes new understanding to the design of modern deep learning algorithms, highlighting the importance of residual connections and their potential applications beyond computer vision. Our experimental code has been made publicly available to encourage further research and replication.",1
"Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods, where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories, the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However, most of the existing methods fail when the categories of query sketches have never been seen during training. In this paper, the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built, two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution, respectively. As an important part of ZSIH, we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge, ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works.",0
"A new method for zero-shot sketch-image hashing has been developed that can effectively generate high-quality hashes for images without any training data. This technique uses deep learning algorithms to extract features from sketches and images, and then maps these features to compact hash codes. The resulting hashes have low distortion rates while still preserving important structural information, making them well suited for applications such as image retrieval and digital libraries. Evaluations show that our approach outperforms state-of-the-art methods both quantitatively and qualitatively. Our contributions open up exciting opportunities for researchers working on computer vision, machine learning and data mining who wish to benefit from sketch-based representation of images for effective recognition and classification. We hope future work will build upon our insights into this new family of problem solvers, which promises wide applicability across many diverse areas.",1
"In this paper we address the problem of learning robust cross-domain representations for sketch-based image retrieval (SBIR). While most SBIR approaches focus on extracting low- and mid-level descriptors for direct feature matching, recent works have shown the benefit of learning coupled feature representations to describe data from two related sources. However, cross-domain representation learning methods are typically cast into non-convex minimization problems that are difficult to optimize, leading to unsatisfactory performance. Inspired by self-paced learning, a learning methodology designed to overcome convergence issues related to local optima by exploiting the samples in a meaningful order (i.e. easy to hard), we introduce the cross-paced partial curriculum learning (CPPCL) framework. Compared with existing self-paced learning methods which only consider a single modality and cannot deal with prior knowledge, CPPCL is specifically designed to assess the learning pace by jointly handling data from dual sources and modality-specific prior information provided in the form of partial curricula. Additionally, thanks to the learned dictionaries, we demonstrate that the proposed CPPCL embeds robust coupled representations for SBIR. Our approach is extensively evaluated on four publicly available datasets (i.e. CUFS, Flickr15K, QueenMary SBIR and TU-Berlin Extension datasets), showing superior performance over competing SBIR methods.",0
"This paper presents a novel approach for sketch-based image retrieval that learns representations which can effectively capture both the global structure of images and their local details. Our method leverages partial curriculum learning, which gradually increases the complexity of training data as the model improves over time. We demonstrate that our cross-paced representation learning algorithm outperforms several state-of-the-art methods on multiple benchmark datasets, providing further evidence of the effectiveness of our proposed approach. By exploiting partial curriculums, we show that it is possible to learn powerful image representations from weak supervision signals such as user sketches. As a result, our framework provides new opportunities for developing effective solutions for challenging computer vision tasks where large amounts of annotated data may be difficult or expensive to obtain.",1
"Predictive models that generalize well under distributional shift are often desirable and sometimes crucial to building robust and reliable machine learning applications. We focus on distributional shift that arises in causal inference from observational data and in unsupervised domain adaptation. We pose both of these problems as prediction under a shift in design. Popular methods for overcoming distributional shift make unrealistic assumptions such as having a well-specified model or knowing the policy that gave rise to the observed data. Other methods are hindered by their need for a pre-specified metric for comparing observations, or by poor asymptotic properties. We devise a bound on the generalization error under design shift, incorporating both representation learning and sample re-weighting. Based on the bound, we propose an algorithmic framework that does not require any of the above assumptions and which is asymptotically consistent. We empirically study the new framework using two synthetic datasets, and demonstrate its effectiveness compared to previous methods.",0
"This paper presents a novel method for learning weighted representations that generalize well across different designs. Our approach uses adversarial training to encourage robustness against unseen transformations while incorporating design-specific details using weighting functions. Experimental results show significant improvements over state-of-the-art methods on several benchmark datasets, demonstrating the effectiveness of our proposed technique. The paper concludes by discussing potential applications of our model in fields such as computer vision and natural language processing.",1
"Statistical methods protecting sensitive information or the identity of the data owner have become critical to ensure privacy of individuals as well as of organizations. This paper investigates anonymization methods based on representation learning and deep neural networks, and motivated by novel information theoretical bounds. We introduce a novel training objective for simultaneously training a predictor over target variables of interest (the regular labels) while preventing an intermediate representation to be predictive of the private labels. The architecture is based on three sub-networks: one going from input to representation, one from representation to predicted regular labels, and one from representation to predicted private labels. The training procedure aims at learning representations that preserve the relevant part of the information (about regular labels) while dismissing information about the private labels which correspond to the identity of a person. We demonstrate the success of this approach for two distinct classification versus anonymization tasks (handwritten digits and sentiment analysis).",0
"In recent years there has been growing interest in the use of deep learning models for representation learning tasks such as image classification, object detection, and speech recognition. These approaches rely on large amounts of labeled data and powerful computational resources, which can make them difficult to deploy in practice. To address these limitations, we propose using adversarial neural networks (ANN) to learn robust representations that generalize well across different domains without requiring explicit supervision from labeled examples. Our approach leverages generative adversarial networks (GANs), which consist of two competing neural nets: one generates samples, while the other discriminates between real and generated samples. By optimizing the generator network alone, we can learn high-quality features without relying on class labels or expensive GPU hardware. We evaluate our method on several benchmark datasets and show that it outperforms traditional representation learning techniques by achieving state-of-the-art results while using only small subsets of training data and modest computing resources. This suggests that ANNs may provide a scalable alternative to current methods, enabling widespread deployment of machine learning applications. Learning effective feature representations is essential to many machine learning tasks. Deep learning models have demonstrated impressive performance at representation learning tasks, but they require large quantities of labelled data and significant compute resources. To overcome these limitations, this work proposes employing adversarial neural networks (ANN). Rather than relying on class labels, ANN learns features through competition between two neural networks – one generating samples, another distinguishing real from fake ones. Fine-tuning the generator alone yields high quality features and performs better than non-adversarial counterparts across diverse datasets while demanding less computation and data. This study highlights ANN’s potential as a viable tool for broad adoption of ML applications.",1
"Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available. We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.",0
"We have developed two new algorithms based on our deep successor representation (DSR) that discover meaningful options from raw high-level states. These novel methods extend prior work by scaling to large state spaces using continuous relaxations and enabling both batch and online discovery. In addition we provide theoretical guarantees regarding their optimality. Through extensive evaluation across numerous MuJoCo tasks, we demonstrate that these algorithms yield better solutions than extant alternatives while being more efficient. Finally, we study the real-world impacts of learning improved planners via simulated robotics experiments that show significant performance improvements over today’s state-of-the-art motion planning systems.",1
"Advances in unsupervised learning enable reconstruction and generation of samples from complex distributions, but this success is marred by the inscrutability of the representations learned. We propose an information-theoretic approach to characterizing disentanglement and dependence in representation learning using multivariate mutual information, also called total correlation. The principle of total Cor-relation Ex-planation (CorEx) has motivated successful unsupervised learning applications across a variety of domains, but under some restrictive assumptions. Here we relax those restrictions by introducing a flexible variational lower bound to CorEx. Surprisingly, we find that this lower bound is equivalent to the one in variational autoencoders (VAE) under certain conditions. This information-theoretic view of VAE deepens our understanding of hierarchical VAE and motivates a new algorithm, AnchorVAE, that makes latent codes more interpretable through information maximization and enables generation of richer and more realistic samples.",0
"This study presents a novel approach to explaining machine learning models by encoding total correlation information into latent representations using autoencoders. By preserving total correlation information during training, our method can produce more interpretable and meaningful explanations than traditional methods that only focus on individual feature importance scores. In addition, we show how these latent representations can be used as features for downstream tasks such as anomaly detection and outlier identification. Our experimental results demonstrate the effectiveness of our proposed method compared to baseline approaches across multiple datasets and model architectures. Overall, our work represents a significant advancement in the field of interpretability in machine learning and has important implications for deploying ML systems in real-world applications.",1
"A grand challenge in representation learning is to learn the different explanatory factors of variation behind the high dimen- sional data. Encoder models are often determined to optimize performance on training data when the real objective is to generalize well to unseen data. Although there is enough numerical evidence suggesting that noise injection (during training) at the representation level might improve the generalization ability of encoders, an information-theoretic understanding of this principle remains elusive. This paper presents a sample-dependent bound on the generalization gap of the cross-entropy loss that scales with the information complexity (IC) of the representations, meaning the mutual information between inputs and their representations. The IC is empirically investigated for standard multi-layer neural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the gap and the IC appear to be in direct correlation, suggesting that SGD selects encoders to implicitly minimize the IC. We specialize the IC to study the role of Dropout on the generalization capacity of deep encoders which is shown to be directly related to the encoder capacity, being a measure of the distinguishability among samples from their representations. Our results support some recent regularization methods.",0
"In order to optimize artificial intelligence (AI), one must consider how to represent data accurately within an algorithm. This research focuses on two methods of representation learning that can enhance accuracy: increasing complexity in representation through various means such as layered networks and increasing randomness in the training process. By examining the effectiveness of these techniques through experiments using image classification tasks, we aim to provide insights into their impacts on improving representations. Our results suggest that while incorporating more complex models can lead to better accuracy, there are limitations to relying solely on increased model capacity. On the other hand, introducing appropriate amounts of noise during training helps prevent overfitting, leading to higher accuracy across all tested datasets. Ultimately, our findings indicate that balancing both approaches could offer greater potential for accurate representation learning and improved performance in AI applications.",1
"Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.",0
"This paper proposes a method for fixing a common issue that arises when optimizing variational autoencoders (VAEs) using the evidence lower bound (ELBO). Specifically, we address the problem of vanishing gradients during optimization, which can lead to poor performance and difficulty in training the model. We present a new algorithm that effectively solves this issue by stabilizing the gradient updates and improving convergence speed. Our approach involves modifying the loss function used in VAEs to better align the optimization process with the goal of maximizing the ELBO. Experimental results demonstrate that our method significantly outperforms existing techniques in terms of reconstruction quality, latent space coherency, and efficiency. Overall, our work advances the state of the art in VAE optimization and opens up opportunities for further research in deep learning and generative models.",1
"Recent advances in weakly supervised classification allow us to train a classifier only from positive and unlabeled (PU) data. However, existing PU classification methods typically require an accurate estimate of the class-prior probability, which is a critical bottleneck particularly for high-dimensional data. This problem has been commonly addressed by applying principal component analysis in advance, but such unsupervised dimension reduction can collapse underlying class structure. In this paper, we propose a novel representation learning method from PU data based on the information-maximization principle. Our method does not require class-prior estimation and thus can be used as a preprocessing method for PU classification. Through experiments, we demonstrate that our method combined with deep neural networks highly improves the accuracy of PU class-prior estimation, leading to state-of-the-art PU classification performance.",0
"Title: ""Information-Theoretic Representation Learning for Positive-Unlabeled Classification""",1
"We study the role of latent space dimensionality in Wasserstein auto-encoders (WAEs). Through experimentation on synthetic and real datasets, we argue that random encoders should be preferred over deterministic encoders. We highlight the potential of WAEs for representation learning with promising results on a benchmark disentanglement task.",0
"In recent years, there has been growing interest in using generative models such as autoencoders for unsupervised learning tasks. One popular method for training these models is the adversarial training approach known as the Generative Adversarial Network (GAN). However, GANs can suffer from instability during training, which can make it difficult to obtain meaningful results. An alternative approach that has recently gained attention is the use of energy-based models, particularly those based on the Earth Mover's Distance (EMD) criterion, which have shown promising results in generating high quality samples. In this work, we examine the latent space of Wasserstein Auto-Encoders (WAEs), a type of EMD-based model used for image generation tasks. We investigate how different architectures affect the properties of the latent space, and show that WAEs can generate high quality images while still preserving important features of the data distribution. Our findings provide insights into the design of efficient and effective generative models for unsupervised learning.",1
"This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.",0
"In recent years there has been growing interest in methods that enhance interpretability in deep learning models. With the increasing adoption of deep learning techniques across different application domains comes a pressing need for model transparency and explainability, making interpretability in these models crucial. While many approaches have been proposed, there remains a lack of consensus on a formal definition of interpretability, as well as on how to quantify its importance. Moreover, current visualization techniques mostly focus on exploratory data analysis tasks such as feature visualization, anomaly detection, or clustering, while interpretation methods aim at explaining the inner workings of complex machine learning algorithms. This survey provides an overview of both types of techniques, identifying strengths and weaknesses of each approach, and highlighting open challenges and research opportunities. Furthermore, we provide guidelines for choosing appropriate visualizations based on the nature of the problem domain, discuss ethical considerations surrounding interpretable machine learning models, and identify promising directions for future research. Overall, our goal is to provide a comprehensive review of interpretability techniques for deep learning models, helping readers better understand how to leverage them effectively in real-world applications. ---",1
"In this paper, we explore methods of complicating self-supervised tasks for representation learning. That is, we do severe damage to data and encourage a network to recover them. First, we complicate each of three powerful self-supervised task candidates: jigsaw puzzle, inpainting, and colorization. In addition, we introduce a novel complicated self-supervised task called ""Completing damaged jigsaw puzzles"" which is puzzles with one piece missing and the other pieces without color. We train a convolutional neural network not only to solve the puzzles, but also generate the missing content and colorize the puzzles. The recovery of the aforementioned damage pushes the network to obtain robust and general-purpose representations. We demonstrate that complicating the self-supervised tasks improves their original versions and that our final task learns more robust and transferable representations compared to the previous methods, as well as the simple combination of our candidate tasks. Our approach achieves state-of-the-art performance in transfer learning on PASCAL classification and semantic segmentation.",0
"This paper presents a novel approach to learning image representations by completing damaged jigsaw puzzles. We propose a two-stage pipeline consisting of a localization network followed by a piecing together module. The localization network predicts missing regions of an input image that has been randomly masked using irregular shapes. The predicted region is then passed through the piecing together module which iteratively selects the most similar patch from a large database and fills in the blank space. Our method learns to encode structural relationships between fragments of images which enables efficient retrieval of relevant information. Experiments show promising results on several benchmark datasets including CelebA, Places, and COCO demonstrating state-of-the art performance across multiple tasks such as feature matching, scene recognition, and semantic segmentation. Additionally, we conduct comprehensive analysis to validate our approach and provide insights into how the modellearns robust features. Finally,we conclude thatourapproachprovidesacompetitivebaselineforfuture workinthisareaandopensupnewavenuessforlearningrepresentationsthroughinteractivehumanfeedbackorautomaticonline tuning.",1
"Basing on the analysis by revealing the equivalence of modern networks, we find that both ResNet and DenseNet are essentially derived from the same ""dense topology"", yet they only differ in the form of connection -- addition (dubbed ""inner link"") vs. concatenation (dubbed ""outer link""). However, both two forms of connections have the superiority and insufficiency. To combine their advantages and avoid certain limitations on representation learning, we present a highly efficient and modularized Mixed Link Network (MixNet) which is equipped with flexible inner link and outer link modules. Consequently, ResNet, DenseNet and Dual Path Network (DPN) can be regarded as a special case of MixNet, respectively. Furthermore, we demonstrate that MixNets can achieve superior efficiency in parameter over the state-of-the-art architectures on many competitive datasets like CIFAR-10/100, SVHN and ImageNet.",0
"Here is a possible abstract:  Network science is concerned with understanding complex systems made up of many interacting components. A particularly important class of networks is called ""mixed links"", meaning that some connections have more than one edge connecting two nodes. For example, if Alice knows Bob as well as Charlie, then there would be three edges linking these three nodes together. In contrast, most studies on complex network analysis have focused on single link graphs, where each connection has only one associated edge. These mixed link networks can provide new insights into the behavior of real world phenomena such as the spread of disease, diffusion of innovations, opinion formation, traffic flow, language evolution, and many others. This work contributes novel mathematical models, simulation experiments, analytical results and data analyses from large scale datasets which allow researchers to better grasp the intricacies and dynamics present in these types of complex interactions. Our findings demonstrate how mixed link structures give rise to richer behaviors and provide deeper insight into social processes compared to single link frameworks and thus should serve as a starting point for broader application within the interdisciplinary field of complexity science.",1
"Recently the deep learning techniques have achieved success in multi-label classification due to its automatic representation learning ability and the end-to-end learning framework. Existing deep neural networks in multi-label classification can be divided into two kinds: binary relevance neural network (BRNN) and threshold dependent neural network (TDNN). However, the former needs to train a set of isolate binary networks which ignore dependencies between labels and have heavy computational load, while the latter needs an additional threshold function mechanism to transform the multi-class probabilities to multi-label outputs. In this paper, we propose a joint binary neural network (JBNN), to address these shortcomings. In JBNN, the representation of the text is fed to a set of logistic functions instead of a softmax function, and the multiple binary classifications are carried out synchronously in one neural network framework. Moreover, the relations between labels are captured via training on a joint binary cross entropy (JBCE) loss. To better meet multi-label emotion classification, we further proposed to incorporate the prior label relations into the JBCE loss. The experimental results on the benchmark dataset show that our model performs significantly better than the state-of-the-art multi-label emotion classification methods, in both classification performance and computational efficiency.",0
"This paper presents a new method for multi-label learning using binary neural networks. We introduce a joint binary neural network (JBNN) model that uses shared weights for all tasks while allowing individual biases for each task. Our approach outperforms traditional multi-label methods such as one-vs-rest, random kernels, and label power-law methods on six benchmark datasets. To demonstrate the applicability of our method, we apply it to the challenging problem of emotion classification from text data, achieving state-of-the-art performance on two benchmark datasets.",1
"Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.",0
"Title: ""Deep Hyperspherical Learning""  This paper presents a new approach to deep learning that utilizes hyperspheres instead of traditional Euclidean geometry. This shift to hyperspheres allows for more efficient and effective processing of high-dimensional data by taking advantage of the unique properties of these geometric shapes.  The authors propose a novel neural network architecture called HypeNet that incorporates hypersphere geometry into each layer. HypeNets use radial basis functions on the unit sphere to implement nonlinear activation functions, which have been shown to improve performance over traditional linear activations. Additionally, HypeNets employ distance preserving embeddings that map high-dimensional input data onto the surface of a unit hypersphere, reducing dimensionality while retaining important structural information.  To demonstrate the effectiveness of their method, the authors compare HypeNet models against several state-of-the-art baseline models using six benchmark datasets from diverse domains such as image classification, speech recognition, and natural language understanding. Results show that HypeNets consistently outperform baseline models, achieving higher accuracy across all tasks and datasets.  Furthermore, the authors analyze the behavior of HypeNets at different layers and under different conditions, providing insights into how hyperspherical learning can improve model robustness and interpretability. Overall, this work represents a significant advancement in deep learning research, paving the way for future applications in areas where high-dimensional data analysis is crucial.",1
"Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.",0
"In recent years, deep learning has shown remarkable success across a wide range of applications, from computer vision to natural language processing. However, one major challenge facing these systems is their lack of interpretability and explainability. Many popular techniques, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), rely on complex mathematical operations that make it difficult to understand how they arrive at their decisions. This opacity can lead to issues related to safety and trustworthiness, especially in high-stakes domains like healthcare and finance. To address this problem, researchers have proposed new methods aimed at improving transparency and controlling specific aspects of a model's behavior. One promising approach is adversarial feature learning, which involves identifying key features used by a classifier and manipulating them to achieve desired outcomes. By doing so, we can gain insight into what a model looks at while making predictions and use that knowledge to enhance performance or mitigate unintended consequences. While existing work explores the feasibility of this methodology, there remain important questions regarding effectiveness and scalability. Our study seeks to advance the field by investigating these challenges and demonstrating the benefits of controllable invariant representations learned via adversarial training. Through extensive experiments across multiple tasks, we showcase improvements in robustness, stability, and interpretability compared to state-of-the-art models. These results highlight the potential impact of our contributions, paving the way for future research into controlled, explainable artificial intelligence. Overall, our work represents a significant step forward in understanding how deep learning systems perceive and reason about data.",1
"Time series data constitutes a distinct and growing problem in machine learning. As the corpus of time series data grows larger, deep models that simultaneously learn features and classify with these features can be intractable or suboptimal. In this paper, we present feature learning via long short term memory (LSTM) networks and prediction via gradient boosting trees (XGB). Focusing on the consequential setting of electronic health record data, we predict the occurrence of hypoxemia five minutes into the future based on past features. We make two observations: 1) long short term memory networks are effective at capturing long term dependencies based on a single feature and 2) gradient boosting trees are capable of tractably combining a large number of features including static features like height and weight. With these observations in mind, we generate features by performing ""supervised"" representation learning with LSTM networks. Augmenting the original XGB model with these features gives significantly better performance than either individual method.",0
"This research focuses on developing models that can accurately forecast operating room (OR) data by combining hybrid gradient boosting trees and neural networks. OR scheduling plays a crucial role in healthcare institutions, as efficient use of resources leads to better patient outcomes while reducing costs. Existing studies have used statistical and machine learning algorithms separately to model OR performance. However, their results remain unsatisfactory due to inaccurate predictions and poor generalization ability across different datasets.  To address these issues, we propose a novel framework that integrates gradient boosting trees and neural networks to improve predictive accuracy. Our approach takes advantage of both models’ strengths – the robustness and interpretability of decision trees combined with the powerful feature extraction capabilities of neural networks. We train multiple instances of each algorithm individually, then combine their predictions through ensemble methods to produce final forecasts. By evaluating our method using publicly available data from two hospitals, we show improved prediction performance over baseline models. Additionally, we provide insights into which features play significant roles in OR operation planning, such as surgical specialty and day of the week. These findings contribute to understanding how complex interactions shape hospital operations.",1
"Automatic emotion recognition has become a trending research topic in the past decade. While works based on facial expressions or speech abound, recognizing affect from body gestures remains a less explored topic. We present a new comprehensive survey hoping to boost research in the field. We first introduce emotional body gestures as a component of what is commonly known as ""body language"" and comment general aspects as gender differences and culture dependence. We then define a complete framework for automatic emotional body gesture recognition. We introduce person detection and comment static and dynamic body pose estimation methods both in RGB and 3D. We then comment the recent literature related to representation learning and emotion recognition from images of emotionally expressive gestures. We also discuss multi-modal approaches that combine speech or face with body gestures for improved emotion recognition. While pre-processing methodologies (e.g. human detection and pose estimation) are nowadays mature technologies fully developed for robust large scale analysis, we show that for emotion recognition the quantity of labelled data is scarce, there is no agreement on clearly defined output spaces and the representations are shallow and largely based on naive geometrical representations.",0
"This would be challenging because I don't have access to the full text and context of your paper yet but here you go:  Emotional body gesture recognition (EBGR) has gained increasing attention as a means of understanding nonverbal communication cues in human interactions. EBGR systems analyze movements such as postures, gestures, facial expressions, and other physiological signals to extract emotional states from individuals. These systems can provide insights into how individuals process and express their emotions in social situations. In this survey, we present a comprehensive review of current state-of-the-art methods used in EBGR research. We examine different approaches including feature extraction techniques, machine learning algorithms, and evaluation metrics commonly utilized in the field. Our aim is to provide readers with an overview of recent advancements in the development of EBGR systems while highlighting existing limitations and potential future directions for research in this area. Overall, our findings suggest that there is still much work to be done to improve the accuracy and robustness of EBGR systems. Nevertheless, continued progress in developing these technologies may lead to significant improvements in our ability to understand human behavior and enhance interpersonal communications.",1
"Deep Neural Networks (DNNs) often struggle with one-shot learning where we have only one or a few labeled training examples per category. In this paper, we argue that by using side information, we may compensate the missing information across classes. We introduce two statistical approaches for fusing side information into data representation learning to improve one-shot learning. First, we propose to enforce the statistical dependency between data representations and multiple types of side information. Second, we introduce an attention mechanism to efficiently treat examples belonging to the 'lots-of-examples' classes as quasi-samples (additional training samples) for 'one-example' classes. We empirically show that our learning architecture improves over traditional softmax regression networks as well as state-of-the-art attentional regression networks on one-shot recognition tasks.",0
"Include at least two key ideas from the paper. Do not describe any figures or tables. Use past tense throughout. Write as if you were submitting the manuscript directly to an academic journal (i.e., use formal language). The idea behind one-shot learning has been to improve artificial intelligence systems’ ability to adapt to new situations by training them on a single example instead of a large number of examples. However, this approach can suffer from limited performance due to lack of contextual understanding. We propose using side information, such as text, images, or videos related to the task or environment, to enhance the performance of one-shot learning algorithms. Our method involves fusing the information provided by both modalities, which results in improved accuracy compared to traditional methods that rely solely on image data. Through rigorous experiments conducted on challenging benchmark datasets, we demonstrate that our approach significantly outperforms state-of-the-art models across various metrics. This work showcases how integrating multiple sources of knowledge can greatly benefit machine learning systems, particularly in real-world scenarios where prior knowledge might be available but limited labeled examples may exist.",1
"Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly consider- ing multi-scale feature maps obtained from the inner layers of a primary CNN architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.",0
"This paper presents a novel approach for learning deep structured multi-scale features using attention-gated conditional random fields (CRFs) for contour prediction. Our method learns to predict contours at different scales by jointly modeling shape priors and local features extracted from input images. We introduce a network architecture that captures multi-scale representations through dilated convolutions and attentional gating mechanisms, which selectively focus on informative regions. The learned feature representation is further regularized by incorporating pairwise relationships among pixels via the CRF model. Experiments show our approach outperforms state-of-the-art methods across several benchmark datasets for contour detection tasks, demonstrating its effectiveness in accurately predicting object boundaries. The proposed framework has potential applications in computer vision problems such as image segmentation, instance recognition, and object tracking.",1
"Fabric image retrieval is beneficial to many applications including clothing searching, online shopping and cloth modeling. Learning pairwise image similarity is of great importance to an image retrieval task. With the resurgence of Convolutional Neural Networks (CNNs), recent works have achieved significant progresses via deep representation learning with metric embedding, which drives similar examples close to each other in a feature space, and dissimilar ones apart from each other. In this paper, we propose a novel embedding method termed focus ranking that can be easily unified into a CNN for jointly learning image representations and metrics in the context of fine-grained fabric image retrieval. Focus ranking aims to rank similar examples higher than all dissimilar ones by penalizing ranking disorders via the minimization of the overall cost attributed to similar samples being ranked below dissimilar ones. At the training stage, training samples are organized into focus ranking units for efficient optimization. We build a large-scale fabric image retrieval dataset (FIRD) with about 25,000 images of 4,300 fabrics, and test the proposed model on the FIRD dataset. Experimental results show the superiority of the proposed model over existing metric embedding models.",0
"Title: Deep Similarity Modeling with Focus Ranking for Efficient Fabric Image Retrieval  This paper presents a novel approach to deep similarity modeling using focus ranking techniques for fabric image retrieval applications. With the increasing popularity of e-commerce platforms, there is a growing need for efficient content search engines that can accurately retrieve relevant images from vast databases based on user input queries. In this context, we propose a methodology that leverages recent advances in convolutional neural network architectures (CNNs) trained on large datasets to learn feature embeddings capturing high-level semantics of fashion items. Our framework employs focus ranking to selectively sample discriminative regions within a product image and aggregate their features into compact representations suitable for comparing against other images in the database. Through extensive experiments conducted on publicly available datasets, our results demonstrate significant improvements over state-of-the-art methods, achieving substantial reductions in error rates while maintaining competitive inference speeds. This work provides valuable insights into bridging the gap between visual analysis and machine learning domains, opening up exciting possibilities for future research directions in digital content management systems.",1
"Physical activity and sleep play a major role in the prevention and management of many chronic conditions. It is not a trivial task to understand their impact on chronic conditions. Currently, data from electronic health records (EHRs), sleep lab studies, and activity/sleep logs are used. The rapid increase in the popularity of wearable health devices provides a significant new data source, making it possible to track the user's lifestyle real-time through web interfaces, both to consumer as well as their healthcare provider, potentially. However, at present there is a gap between lifestyle data (e.g., sleep, physical activity) and clinical outcomes normally captured in EHRs. This is a critical barrier for the use of this new source of signal for healthcare decision making. Applying deep learning to wearables data provides a new opportunity to overcome this barrier.   To address the problem of the unavailability of clinical data from a major fraction of subjects and unrepresentative subject populations, we propose a novel unsupervised (task-agnostic) time-series representation learning technique called act2vec. act2vec learns useful features by taking into account the co-occurrence of activity levels along with periodicity of human activity patterns. The learned representations are then exploited to boost the performance of disorder-specific supervised learning models. Furthermore, since many disorders are often related to each other, a phenomenon referred to as co-morbidity, we use a multi-task learning framework for exploiting the shared structure of disorder inducing life-style choices partially captured in the wearables data. Empirical evaluation using actigraphy data from 4,124 subjects shows that our proposed method performs and generalizes substantially better than the conventional time-series symbolic representational methods and task-specific deep learning models.",0
"This is an interesting paper that explores co-morbidity using wearable activity data and unsupervised pre-training methods. The authors use multi-task learning techniques to improve their results. They evaluate their approach by comparing it to existing state-of-the-art methods and show promising results. Overall, this research provides valuable insights into how wearable technology can be used to detect and monitor co-morbidities.",1
"Training deep neural networks is known to require a large number of training samples. However, in many applications only few training samples are available. In this work, we tackle the issue of training neural networks for classification task when few training samples are available. We attempt to solve this issue by proposing a new regularization term that constrains the hidden layers of a network to learn class-wise invariant representations. In our regularization framework, learning invariant representations is generalized to the class membership where samples with the same class should have the same representation. Numerical experiments over MNIST and its variants showed that our proposal helps improving the generalization of neural network particularly when trained with few samples. We provide the source code of our framework https://github.com/sbelharbi/learning-class-invariant-features .",0
"Abstract: ""Neural networks are powerful machine learning models that can achieve state-of-the-art performance on many tasks, but they suffer from overfitting due to their large number of parameters and high capacity for memorizing training data. One approach to addressing this problem is regularization, which introduces additional constraints during training to prevent overfitting and improve generalization performance.""",1
"Recently there has been a dramatic increase in the performance of recognition systems due to the introduction of deep architectures for representation learning and classification. However, the mathematical reasons for this success remain elusive. This tutorial will review recent work that aims to provide a mathematical justification for several properties of deep networks, such as global optimality, geometric stability, and invariance of the learned representations.",0
"This paper provides an overview of the mathematical foundations that underpin deep learning methods, including linear algebra, optimization techniques, and probability theory. Deep learning algorithms have revolutionized many fields by enabling automatic feature detection from raw data, which has led to state-of-the-art performance on tasks such as image classification, speech recognition, language translation, and game playing. We explain how these advances arise from the mathematics underlying deep learning models, particularly focusing on matrix operations, convex optimization, gradient descent, backpropagation, neural network activation functions, autoencoders, variational inference, Monte Carlo integration, Markov Chain Monte Carlo (MCMC), Bayesian neural networks, Gaussian processes, reproducing kernel Hilbert spaces (RKHS), support vector machines (SVMs), radial basis function (RBF) kernels, regularization, stochastic gradient Langevin dynamics (SGLD), Hamiltonians, and Lyapunov functions. In particular, we show why these technical tools lead to robust generalization ability beyond the training set, universal approximation theorems, equivariant representation learning via group convolutional neural networks (CNNs), adversarial examples, energy-based modeling through autoencoder bottlenecks or denoising autoencoders, probabilistic interpretation of dropout, Bayesian treatment of uncertainty by predictive distributions, latent variable modularity through stochastic exponential family inference, and nonparametric flexibility with RKHS kernels and SVMs. By understanding these connections, practitioners can build more effective models based on fundamental principles rather than ad hoc modifications. Overall, this exposition clarifies how cutting-edge artificial intelligence applications emerge from rigorous theoretical foundations and thus enhances collaboration among researchers working across different communities within science and engineering.",1
"The unsupervised Pretraining method has been widely used in aiding human action recognition. However, existing methods focus on reconstructing the already present frames rather than generating frames which happen in future.In this paper, We propose an improved Variantial Autoencoder model to extract the features with a high connection to the coming scenarios, also known as Predictive Learning. Our framework lists as following: two steam 3D-convolution neural networks are used to extract both spatial and temporal information as latent variables. Then a resample method is introduced to create new normal distribution probabilistic latent variables and finally, the deconvolution neural network will use these latent variables generate next frames. Through this possess, we train the model to focus more on how to generate the future and thus it will extract the future high connected features. In the experiment stage, A large number of experiments on UT and UCF101 datasets reveal that future generation aids Prediction does improve the performance. Moreover, the Future Representation Learning Network reach a higher score than other methods when in half observation. This means that Future Representation Learning is better than the traditional Representation Learning and other state- of-the-art methods in solving the human action prediction problems to some extends.",0
"Predicting human actions is an important task in fields such as computer vision and robotics. Existing approaches rely on manually engineered features and lack generalization across different scenarios and subjects. In this work, we propose using a novel machine learning model called the ""Future Representation Learning Variational Autoencoder"" (FRLVA) that leverages future representations to learn more robust features for action prediction. We compare our approach against state-of-the-art methods on multiple datasets and show significant improvement in accuracy and generalization ability. Our results demonstrate the effectiveness of FRLVA for predicting human actions and open up new possibilities for applications where real-time predictions can improve safety and efficiency.",1
"This paper introduces a probabilistic framework for k-shot image classification. The goal is to generalise from an initial large-scale classification task to a separate task comprising new classes and small numbers of examples. The new approach not only leverages the feature-based representation learned by a neural network from the initial task (representational transfer), but also information about the classes (concept transfer). The concept information is encapsulated in a probabilistic model for the final layer weights of the neural network which acts as a prior for probabilistic k-shot learning. We show that even a simple probabilistic model achieves state-of-the-art on a standard k-shot learning dataset by a large margin. Moreover, it is able to accurately model uncertainty, leading to well calibrated classifiers, and is easily extensible and flexible, unlike many recent approaches to k-shot learning.",0
"This paper presents a new approach to few-shot learning that combines discrimination and model uncertainty through the use of probabilistic models. We propose a method that learns from small amounts of labeled data by representing each class as a mixture distribution over features, rather than just a single prototype. By doing so, we can capture both the uncertainty inherent in limited training data and the diversity within each class, leading to better generalization on unseen examples. Our approach leverages recent advances in variational inference and feature embedding techniques, resulting in state-of-the-art performance on several benchmark datasets for image classification and reinforcement learning tasks.",1
"Audio events are quite often overlapping in nature, and more prone to noise than visual signals. There has been increasing evidence for the superior performance of representations learned using sparse dictionaries for applications like audio denoising and speech enhancement. This paper concentrates on modifying the traditional reconstructive dictionary learning algorithms, by incorporating a discriminative term into the objective function in order to learn class-specific adversarial dictionaries that are good at representing samples of their own class at the same time poor at representing samples belonging to any other class. We quantitatively demonstrate the effectiveness of our learned dictionaries as a stand-alone solution for both binary as well as multi-class audio classification problems.",0
"This study develops a novel approach to audio classification by using sparse adversarial dictionaries (SAD). We apply deep learning techniques to achieve state-of-the art performance on benchmark datasets. The proposed method involves training two competing neural networks - one that generates features from the input data and another that attempts to discriminate between the generated features from different classes. Our experiments show that the learned dictionaries outperform several other popular methods including dictionary learning and sparse coding algorithms. The achieved results demonstrate the effectiveness of our approach for multi-class audio classification tasks. Overall, we make significant contributions to the field by providing insights into how sparse representations can improve the accuracy of multi-class audio classification models.",1
"Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods, since no intensive labeling work is involved. However, existing unsupervised methods learn hashing functions by preserving inter and intra correlations, while ignoring the underlying manifold structure across different modalities, which is extremely helpful to capture meaningful nearest neighbors of different modalities for cross-modal retrieval. To address the above problem, in this paper we propose an Unsupervised Generative Adversarial Cross-modal Hashing approach (UGACH), which makes full use of GAN's ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows: (1) We propose a generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed UGACH, given a data of one modality, the generative model tries to fit the distribution over the manifold structure, and select informative data of another modality to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. (2) We propose a correlation graph based approach to capture the underlying manifold structure across different modalities, so that data of different modalities but within the same manifold can have smaller Hamming distance and promote retrieval accuracy. Extensive experiments compared with 6 state-of-the-art methods verify the effectiveness of our proposed approach.",0
"Unsupervised generative adversarial cross-modal hashing (UGAH) is a new method that enables efficient retrieval and matching tasks across multiple modalities by learning jointly a linear projection onto a latent space shared across different domains. In other words, UGAH allows us to create compact representations of data from images, videos, audio files, etc., which can then be used for fast search and comparison without losing important details. The key innovation behind UGAH lies in integrating both generative models (GANs) and autoencoders into a unified framework. This integration enables the model to leverage both the strengths of GANs, such as generating realistic synthetic samples, and those of autoencoders, such as reconstructing inputs faithfully. As a result, UGAH achieves better performance on downstream applications than either type of model alone. To demonstrate the effectiveness of UGAH, we conducted extensive experiments using several benchmark datasets, including MNIST, FashionMNIST, CIFAR-10, YouTube-Birds, and MusicNet. Our results show that UGAH significantly outperforms existing state-of-the-art methods on these diverse tasks, making it a promising tool for future research and applications. In summary, UGAH provides a powerful solution for bridging gaps between modality-specific hash functions and deep metric learning approaches. Its success opens up exciting opportunities for cross-modal retrieval and recommendation systems, where efficiency meets accuracy while preserving task-related characteristics.",1
"Generative Adversarial Networks (GANs) represent a promising class of generative networks that combine neural networks with game theory. From generating realistic images and videos to assisting musical creation, GANs are transforming many fields of arts and sciences. However, their application to healthcare has not been fully realized, more specifically in generating electronic health records (EHR) data. In this paper, we propose a framework for exploring the value of GANs in the context of continuous laboratory time series data. We devise an unsupervised evaluation method that measures the predictive power of synthetic laboratory test time series. Further, we show that when it comes to predicting the impact of drug exposure on laboratory test data, incorporating representation learning of the training cohorts prior to training GAN models is beneficial.",0
"Title: Using Generative Adversarial Networks to Analyze Drug-Induced Changes in Laboratory Test Results Drug administration can cause changes in laboratory test results which may be critical in predicting potential adverse events and monitoring treatment efficacy. Currently, there lacks a reliable method to effectively analyze these drug-induced changes in laboratory test results. Here we propose using generative adversarial networks (GAN) as a framework for exploring and evaluating methods that predict drug-induced trajectories in electronic health records (EHR). GAN consist of two neural networks; one generates new data such as synthetic EHR samples while another discriminates real from fake data. Training both together helps to develop more accurate models capable of generating high quality data. In our study, we utilize this framework to simulate different scenarios based on patient characteristics to evaluate the performance of multiple machine learning algorithms for predictive modeling. We demonstrate that GANs improve the accuracy of the predictions obtained by applying traditional methods alone. Our findings suggest that GANs could serve as valuable tools for analyzing complex data patterns in the EHR space and ultimately lead to better informed clinical decisions. This research contributes to efforts aimed at improving patient outcomes through enhanced analysis of medication effects on human physiology. Further work is warranted to investigate the applicability of GANs in other domains within biomedical informatics including genomics, imaging and disease progression prediction.",1
"In this paper, we study the problem of using representation learning to assist information diffusion prediction on graphs. In particular, we aim at estimating the probability of an inactive node to be activated next in a cascade. Despite the success of recent deep learning methods for diffusion, we find that they often underexplore the cascade structure. We consider a cascade as not merely a sequence of nodes ordered by their activation time stamps; instead, it has a richer structure indicating the diffusion process over the data graph. As a result, we introduce a new data model, namely diffusion topologies, to fully describe the cascade structure. We find it challenging to model diffusion topologies, which are dynamic directed acyclic graphs (DAGs), with the existing neural networks. Therefore, we propose a novel topological recurrent neural network, namely Topo-LSTM, for modeling dynamic DAGs. We customize Topo-LSTM for the diffusion prediction task, and show it improves the state-of-the-art baselines, by 20.1%--56.6% (MAP) relatively, across multiple real-world data sets. Our code and data sets are available online at https://github.com/vwz/topolstm.",0
"Artificial neural networks have been widely used for processing high-dimensional data such as images, videos, audio signals, etc. One of their advantages over traditional statistical models is that they can learn complex nonlinear relationships from raw data without the need for explicit feature engineering. In recent years, there has been growing interest in applying deep learning techniques to time series problems. This includes predicting future states from partially observed past trajectories (diffusion prediction), which is relevant for applications like video compression, anomaly detection, or robotics among others. Most existing works on recurrent neural networks for diffusion prediction rely on sequentiality assumptions of the underlying processes. This means that some form of Markov property needs to hold true at each spatial point separately, allowing one to propagate uncertainty only along the temporal dimension. However, many real world phenomena exhibit more complex dependencies across multiple dimensions simultaneously. Here we present a new model architecture called topological recurrent neural network (TropRNN) that lifts these strong assumptions and learns to process tensor data while preserving certain geometric properties inspired by algebraic topology. We show how our TropRNN leads to efficient learning dynamics through novel gradient flows and demonstrates state-of-the-art performance on several challenging benchmark tasks involving diverse types of data. Our work suggests possible improvements upon existing deep generative models of high-dimensional data using similar principles. Keywords: deep learning, diffusion prediction, algebraic topology, persistent homology, convolutional neural networks, graphical models, variational inference, latent variable models, energybased models, kernel methods, message passing algorithms, Gromov–Hausdorff convergence.",1
"One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.",0
"This paper proposes a new hybrid reward architecture for reinforcement learning (RL) that combines both intrinsic motivation and extrinsic rewards to more effectively guide agent behavior. Intrinsic motivation drives the agent towards exploration and novelty seeking, while extrinsic rewards provide direct feedback on task performance. Our approach uses a mixture model to combine these two sources of motivation, allowing the agent to adaptively balance between them based on the current state of the environment. We demonstrate the effectiveness of our method through experiments across a range of challenging benchmark tasks, showing improved performance compared to existing methods that rely solely on either intrinsic or extrinsic motivation. Overall, our work represents a step forward in developing intelligent agents capable of robust and flexible decision making under uncertainty.",1
"Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https://github.com/eracah/hur-detect.",0
"The paper presents ExtremeWeather, a comprehensive dataset designed for the purpose of detecting, localizing, and understanding extreme weather events at a global scale through a combination of both labeled and unlabeled data. This dataset consists of multi-modal features such as satellite imagery, cloud motion vectors, radar reflectivity, infrared temperature, and more. By leveraging these diverse inputs along with expert human annotations, we aim to enable machine learning algorithms that can accurately identify extreme weather patterns and enhance their interpretation, providing valuable insights into natural disasters and helping communities prepare better for potential risks. Our evaluation shows promising results on several established benchmarks, highlighting the effectiveness of our approach in addressing this crucial problem space.",1
"In representation learning (RL), how to make the learned representations easy to interpret and less overfitted to training data are two important but challenging issues. To address these problems, we study a new type of regulariza- tion approach that encourages the supports of weight vectors in RL models to have small overlap, by simultaneously promoting near-orthogonality among vectors and sparsity of each vector. We apply the proposed regularizer to two models: neural networks (NNs) and sparse coding (SC), and develop an efficient ADMM-based algorithm for regu- larized SC. Experiments on various datasets demonstrate that weight vectors learned under our regularizer are more interpretable and have better generalization performance.",0
"""Abstract: In natural language processing (NLP), representing words as fixed-length vectors allows them to be used in machine learning algorithms such as deep neural networks (DNNs). Traditionally, dense distributed representations like Word2Vec have been widely adopted due to their effectiveness in capturing semantic meaning through the use of neighboring words. However, these dense representations suffer from high levels of redundancy which can make them less effective at capturing subtle differences between similar concepts. In this work, we propose the use of sparse distributed representations based on the concept of minimal spanning trees which allow for more distinctive word embeddings. We experimentally show that our proposed method outperforms other popular methods in several NLP benchmark datasets.""",1
"Graphs (networks) are ubiquitous and allow us to model entities (nodes) and the dependencies (edges) between them. Learning a useful feature representation from graph data lies at the heart and success of many machine learning tasks such as classification, anomaly detection, link prediction, among many others. Many existing techniques use random walks as a basis for learning features or estimating the parameters of a graph model for a downstream prediction task. Examples include recent node embedding methods such as DeepWalk, node2vec, as well as graph-based deep learning algorithms. However, the simple random walk used by these methods is fundamentally tied to the identity of the node. This has three main disadvantages. First, these approaches are inherently transductive and do not generalize to unseen nodes and other graphs. Second, they are not space-efficient as a feature vector is learned for each node which is impractical for large graphs. Third, most of these approaches lack support for attributed graphs.   To make these methods more generally applicable, we propose a framework for inductive network representation learning based on the notion of attributed random walk that is not tied to node identity and is instead based on learning a function $\Phi : \mathrm{\rm \bf x} \rightarrow w$ that maps a node attribute vector $\mathrm{\rm \bf x}$ to a type $w$. This framework serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many other previous methods that leverage traditional random walks.",0
"In recent years, graph data has become increasingly prevalent due to advances in large attributed networks such as social media platforms and biological data repositories. As a result, there has been growing interest in developing machine learning algorithms that can effectively represent complex patterns in these graphs. One approach to representation learning in graphs involves constructing low-dimensional representations (embeddings) of nodes based on their structural relationships with other nodes in the network. This allows for efficient computation of node similarities, which is particularly important in tasks such as node classification and link prediction. This paper introduces a novel algorithm called inductive representation learning (IRen), which extends traditional embedding methods by incorporating both attribute and structural features into the learned embeddings. The proposed framework leverages ideas from non-negative matrix factorization and neural networks, enabling it to capture both linear and nonlinear relationship patterns in the data. Experiments conducted on multiple real-world datasets demonstrate IRen's ability to produce high-quality node embeddings compared to state-of-the-art baseline methods, improving accuracy across several downstream task benchmarks including clustering, classification, and recommendation systems. Overall, our work contributes new insights and techniques towards scalable, flexible, and accurate inductive modeling of large attributed networks through end-to-end optimization of embedding parameters.",1
"The goal of graph representation learning is to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in the graph, and discriminative models that predict the probability of edge existence between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying above two classes of methods, in which the generative model and discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces ""fake"" samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, when considering the implementation of generative model, we propose a novel graph softmax to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including link prediction, node classification, and recommendation, over state-of-the-art baselines.",0
"Introduction Graphs have become increasingly important for representing complex data relationships and structures across many domains. However, traditional graph representation methods suffer from limitations such as high computational cost and difficulty in handling large graphs. In this work, we propose a new framework called GraphGAN that utilizes generative adversarial networks (GAN) to learn meaningful graph representations from raw data. Our method overcomes existing challenges by learning low-dimensional node embeddings through minimax competition between generator and discriminator models. We demonstrate the effectiveness of our approach on several benchmark datasets, showing improvements over state-of-the-art methods in node classification tasks. Finally, we discuss potential future directions and applications of GraphGAN. Overall, our contributions provide a novel approach to graph representation learning with promising results.",1
"Learning low-dimensional representations of networks has proved effective in a variety of tasks such as node classification, link prediction and network visualization. Existing methods can effectively encode different structural properties into the representations, such as neighborhood connectivity patterns, global structural role similarities and other high-order proximities. However, except for objectives to capture network structural properties, most of them suffer from lack of additional constraints for enhancing the robustness of representations. In this paper, we aim to exploit the strengths of generative adversarial networks in capturing latent features, and investigate its contribution in learning stable and robust graph representations. Specifically, we propose an Adversarial Network Embedding (ANE) framework, which leverages the adversarial learning principle to regularize the representation learning. It consists of two components, i.e., a structure preserving component and an adversarial learning component. The former component aims to capture network structural properties, while the latter contributes to learning robust representations by matching the posterior distribution of the latent representations to given priors. As shown by the empirical results, our method is competitive with or superior to state-of-the-art approaches on benchmark network embedding tasks.",0
"Incorporate one interesting quote from the paper. Use a citation manager like Mendeley, EndNote or Zotero to organise references. The field of computer vision has seen significant advances in recent years due to the development of deep learning methods such as convolutional neural networks (CNNs). However, these models can still suffer from issues related to overfitting, which can lead to poor generalization performance on unseen data. One approach to addressing this problem is adversarial training, where a model is trained alongside an adversary that attempts to fool it into making mistakes. This process results in more robust and accurate models that perform well even under challenging conditions. This work investigates the use of adversarial network embedding, where two CNNs are used jointly, each having access only to a subset of input features. By using an adversarial loss term during optimization, we show that our method improves the quality of embeddings produced by both networks. Our experimental evaluations demonstrate substantial improvements in classification accuracy compared to traditional single-model approaches. ""In order to build intelligent systems that truly understanding their environments, we must first learn how to make them see."" - David Hua, Google Brain. The reference list should include all sources mentioned in the text, including books, journal articles, and conference proceedings. For example: Reference List Book Bair et al. (2019) Introduction to Computer Vision: Methods, Models, and Applications, Cambridge University Press. Journal Article Goodfellow et al. (2014) Explaining and Harnessing Adversarial Examples, arXiv preprint arXiv:1412.6071. Conference Proceeding Krizhevsky et al. (2012) ImageNet Classification with Deep Convolutional Neural Networks, Advance",1
"With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks (CNNs). However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model $G$ and a discriminative model $D$. We treat $D$ as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. $G$ can produce numerous images that are similar to the training data; therefore, $D$ can learn better representations of remotely sensed images using the training data provided by $G$. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.",0
"In recent years, unsupervised representation learning has become increasingly important due to the scarcity of labeled data in many applications such as remote sensing image classification. One popular method for achieving unsupervised representation learning is Generative Adversarial Networks (GANs). While previous works have utilized GANs for feature generation in supervised settings, our work focuses on applying these methods to unlabeled images without any human intervention. We introduce a new variant of GANs called Multi-Agent Transfer Adversarial Training GANs (MARTA GANs) that allow multiple agents to collaborate towards achieving better representations. Through experimentations using diverse real-world datasets from remote sensing applications, we demonstrate the effectiveness of MARTA GANs for generating discriminative features that outperform traditional preprocessing techniques while reducing reliance on large amounts of annotated training samples. Our results highlight the potential of multi-agent adversarial training methods for improving representation learning capabilities in resource-constrained environments where access to labeled data may be limited. This research offers valuable insights into advancing machine intelligence systems for high-impact domains including environmental monitoring and climate change mitigation efforts.",1
"A novel 3D shape classification scheme, based on collaborative representation learning, is investigated in this work. A data-driven feature-extraction procedure, taking the form of a simple projection operator, is in the core of our methodology. Provided a shape database, a graph encapsulating the structural relationships among all the available shapes, is first constructed and then employed in defining low-dimensional sparse projections. The recently introduced method of CRPs (collaborative representation based projections), which is based on L2-Graph, is the first variant that is included towards this end. A second algorithm, that particularizes the CRPs to shape descriptors that are inherently nonnegative, is also introduced as potential alternative. In both cases, the weights in the graph reflecting the database structure are calculated so as to approximate each shape as a sparse linear combination of the remaining dataset objects. By way of solving a generalized eigenanalysis problem, a linear matrix operator is designed that will act as the feature extractor. Two popular, inherently high dimensional descriptors, namely ShapeDNA and Global Point Signature (GPS), are employed in our experimentations with SHREC10, SHREC11 and SCHREC 15 datasets, where shape recognition is cast as a multi-class classification problem that is tackled by means of an SVM (support vector machine) acting within the reduced dimensional space of the crafted projections. The results are very promising and outperform state of the art methods, providing evidence about the highly discriminative nature of the introduced 3D shape representations.",0
"In recent years, there has been growing interest in using deep learning techniques for image classification tasks such as 3D shape classification. One challenge in applying these methods to complex 3D shapes is finding effective ways to represent their geometry and topology for input into neural networks.  Collaborative representation based projections (CRPs) have emerged as a powerful method for representing high-dimensional data by projecting them onto lower-dimensional spaces while preserving essential features and properties of the original objects. In this work, we explore the use of CRPs for 3D shape classification using convolutional neural networks (CNNs). We propose a novel architecture that incorporates both local and global features from the shape model to enhance discriminability and robustness.  Experimental results on two benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art accuracy compared to other popular methods. Our analysis shows that the proposed architecture can effectively capture relevant shape characteristics for accurate classification, providing insights into how collaboration between different representations contributes to improved performance. These findings pave the way for future research into developing more advanced architectures leveraging collaborative representations for complex vision problems.",1
"Learning discriminative representations for unseen person images is critical for person Re-Identification (ReID). Most of current approaches learn deep representations in classification tasks, which essentially minimize the empirical classification risk on the training set. As shown in our experiments, such representations commonly focus on several body parts discriminative to the training set, rather than the entire human body. Inspired by the structural risk minimization principle in SVM, we revise the traditional deep representation learning procedure to minimize both the empirical classification risk and the representation learning risk. The representation learning risk is evaluated by the proposed part loss, which automatically generates several parts for an image, and computes the person classification loss on each part separately. Compared with traditional global classification loss, simultaneously considering multiple part loss enforces the deep network to focus on the entire human body and learn discriminative representations for different parts. Experimental results on three datasets, i.e., Market1501, CUHK03, VIPeR, show that our representation outperforms the existing deep representations.",0
"In recent years, person re-identification has emerged as a critical task in computer vision, with numerous applications in security and surveillance systems. This paper presents a novel deep learning approach called Part Loss (PL) for person re-identification that significantly improves identification accuracy compared to traditional methods. PL focuses on representing and extracting discriminative features from pedestrians by explicitly modeling part-level features using multi-scale convolutional neural networks. By doing so, our method addresses common issues such as pose variations and background clutter that hinder accurate detection and retrieval of individuals across non-overlapping camera views. Extensive experiments conducted on several publicly available datasets validate the effectiveness and superiority of our proposed method against state-of-the-art approaches. Overall, our work demonstrates the potential of deep representation learning techniques in solving real-world problems related to personal recognition and tracking under complex environments and scenarios.",1
"For the past 5 years, the ILSVRC competition and the ImageNet dataset have attracted a lot of interest from the Computer Vision community, allowing for state-of-the-art accuracy to grow tremendously. This should be credited to the use of deep artificial neural network designs. As these became more complex, the storage, bandwidth, and compute requirements increased. This means that with a non-distributed approach, even when using the most high-density server available, the training process may take weeks, making it prohibitive. Furthermore, as datasets grow, the representation learning potential of deep networks grows as well by using more complex models. This synchronicity triggers a sharp increase in the computational requirements and motivates us to explore the scaling behaviour on petaflop scale supercomputers. In this paper we will describe the challenges and novel solutions needed in order to train ResNet-50 in this large scale environment. We demonstrate above 90\% scaling efficiency and a training time of 28 minutes using up to 104K x86 cores. This is supported by software tools from Intel's ecosystem. Moreover, we show that with regular 90 - 120 epoch train runs we can achieve a top-1 accuracy as high as 77\% for the unmodified ResNet-50 topology. We also introduce the novel Collapsed Ensemble (CE) technique that allows us to obtain a 77.5\% top-1 accuracy, similar to that of a ResNet-152, while training a unmodified ResNet-50 topology for the same fixed training budget. All ResNet-50 models as well as the scripts needed to replicate them will be posted shortly.",0
"This paper presents a novel approach to scaling up the batch size used in stochastic gradient descent (SGD) training of residual networks on the ImageNet dataset, resulting in increased accuracy and faster convergence times. By carefully tuning hyperparameters such as learning rate and weight decay, we show that it is possible to achieve state-of-the-art performance without resorting to complex architectures or ensemble methods. Our experiments demonstrate that our method significantly reduces the wall clock time required to reach high levels of accuracy compared to previous approaches, while still producing competitive results in terms of top-1 validation error. Overall, these findings have important implications for the field of deep learning, suggesting that there may be further room for improvement through simple yet effective optimization techniques.",1
"Deep learning techniques have been successfully used in learning a common representation for multi-view data, wherein the different modalities are projected onto a common subspace. In a broader perspective, the techniques used to investigate common representation learning falls under the categories of canonical correlation-based approaches and autoencoder based approaches. In this paper, we investigate the performance of deep autoencoder based methods on multi-view data. We propose a novel step-based correlation multi-modal CNN (CorrMCNN) which reconstructs one view of the data given the other while increasing the interaction between the representations at each hidden layer or every intermediate step. Finally, we evaluate the performance of the proposed model on two benchmark datasets - MNIST and XRMB. Through extensive experiments, we find that the proposed model achieves better performance than the current state-of-the-art techniques on joint common representation learning and transfer learning tasks.",0
"This work presents an extension of a recently introduced method called STEpwise COrrelation guided RLearning (STROCORAL) that learns to predict correlations between different modalities. In particular, we consider here multi-modal deep learning models which learn to classify images by using both visual features from Convolutional Neural Networks (CNNs), as well as other external modality inputs such as audio data. Our goal is to design a generic algorithm that can leverage pretrained models for each individual task while improving on their performance through correlation guidance across modalities. We evaluate our approach on several benchmark datasets, comparing against strong baselines that have been previously proposed to achieve state-of-the-art results. Experiments show that STROCORAL significantly outperforms these approaches, validating the effectiveness and generality of our framework. This research has implications for applications where multimodal input integration is important, including but not limited to robotics, human computer interaction, and autonomous driving.",1
"This paper shows that a simple baseline based on a Bag-of-Words (BoW) representation learns surprisingly good knowledge graph embeddings. By casting knowledge base completion and question answering as supervised classification problems, we observe that modeling co-occurences of entities and relations leads to state-of-the-art performance with a training time of a few minutes using the open sourced library fastText.",0
"Incorporating knowledge graph (KG) embeddings into natural language processing models has proven effective in improving performance on various tasks such as question answering and text generation. However, the computation time required by some state-of-the-art KG embedding methods can become prohibitive when dealing with large datasets, limiting their applicability in practice. We present FLAME, a fast linear model for KG embeddings, which significantly reduces the computational cost while still providing comparable results to the current state-of-the-art. Our method leverages efficient matrix operations and sparse representations to achieve faster training and inference times than existing methods. Through extensive experiments across multiple benchmark datasets, we demonstrate that our approach achieves competitive results compared to other baseline methods, including those based on pre-trained transformers like BERT and GPT-2. Additionally, we showcase the potential utility of our model for downstream NLP applications, outperforming existing approaches on two challenging KG-based question answering benchmarks. Overall, our work offers a viable alternative for users seeking to incorporate KG embeddings into their applications without incurring excessive computing resources and costs.",1
"Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, eg pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results.",0
"This paper presents a new approach for person re-identification that utilizes deep learning techniques to learn contextually-aware features over body parts and latent representations. Traditional methods rely on handcrafted features which can struggle to capture subtle variations in appearance caused by changes in lighting, pose, and occlusion. In contrast, our proposed method leverages convolutional neural networks (CNNs) to automatically extract high-level feature maps that encode both spatial and temporal information. These feature maps are then fed into a novel part-aligned pooling layer that learns discriminative features for different body regions such as head, torso, legs, etc. Finally, we propose a latent transformation network that projects these learned features onto a lower dimensional space while preserving their discriminatory power. Experimental results demonstrate that our proposed method significantly outperforms state-of-the-art approaches on several benchmark datasets, showcasing the effectiveness of learning deep contextualized features for person re-identification tasks.",1
"Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.",0
"In recent years, there has been increasing interest in developing models that can accurately predict labels in large scale image datasets. One popular approach to solving this problem is by training machine learning algorithms on labeled subsets of data known as ""training sets"" and then using these trained models to make predictions on new images. However, obtaining high quality labeled data can be time consuming and expensive, especially for large scale datasets. Label distribution learning (LDL) provides a way to reduce the amount of labeled data required to train accurate prediction models. LDL works by modeling the probability distribution over all possible label assignments for each example and optimizing the model parameters such that they maximize the average log likelihood of the true labels given the observed features. By leveraging the implicit structure within the label space itself and exploiting correlations across different labels, LDL enables improved generalization even when only very few examples per class are present. This allows researchers to build highly effective models without requiring huge amounts of manually annotated training data.",1
"Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this application requires substantial implementation effort. Thus, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon.   NiftyNet provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D and 3D images and computational graphs by default.   We present 3 illustrative medical image analysis applications built using NiftyNet: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses.   NiftyNet enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.",0
"Deep learning has emerged as a powerful tool for processing and analyzing medical images, including X-rays, CT scans, MRI scans, and mammograms. However, existing deep learning platforms have several limitations, such as poor scalability, limited customization options, and high computational requirements. To address these challenges, we present NiftyNet, a new open source deep learning platform designed specifically for medical image analysis. Our system is based on lightweight models that can run efficiently on commodity hardware, while still providing state-of-the-art accuracy. We provide extensive documentation, code examples, pre-trained models, and benchmark datasets to help users quickly adopt our approach. Furthermore, NiftyNet offers easy integration with widely used medical image viewers and workstations through well-defined application programming interfaces (API). In summary, NiftyNet provides researchers and practitioners with a flexible and efficient platform to develop novel computer vision algorithms for a variety of medical image analysis tasks.",1
"This paper presents a general graph representation learning framework called DeepGL for learning deep node and edge representations from large (attributed) graphs. In particular, DeepGL begins by deriving a set of base features (e.g., graphlet features) and automatically learns a multi-layered hierarchical graph representation where each successive layer leverages the output from the previous layer to learn features of a higher-order. Contrary to previous work, DeepGL learns relational functions (each representing a feature) that generalize across-networks and therefore useful for graph-based transfer learning tasks. Moreover, DeepGL naturally supports attributed graphs, learns interpretable features, and is space-efficient (by learning sparse feature vectors). In addition, DeepGL is expressive, flexible with many interchangeable components, efficient with a time complexity of $\mathcal{O}(|E|)$, and scalable for large networks via an efficient parallel implementation. Compared with the state-of-the-art method, DeepGL is (1) effective for across-network transfer learning tasks and attributed graph representation learning, (2) space-efficient requiring up to 6x less memory, (3) fast with up to 182x speedup in runtime performance, and (4) accurate with an average improvement of 20% or more on many learning tasks.",0
"This paper presents a novel approach to deep feature learning on graphs using convolutional neural networks (CNN). We propose a method that takes advantage of both spectral graph theory and spatial CNN architectures to learn high-level features from large datasets of graph signals. Our method leverages the strengths of both domains by incorporating global information through eigendecomposition of the Laplacian matrix, while capturing local patterns with a conventional CNN architecture. We showcase our model’s performance on several benchmark datasets including MNIST-based problems like Shallow MNIST, Kuzushiji-MNIST, Sparse MNIST, and CIFAR-10. Compared with traditional methods such as autoencoders, we achieve superior results. Furthermore, ablation studies demonstrate how different components contribute to overall improvements in accuracy. Additionally, visualizations illustrate the learned representations, highlighting their importance for data analysis tasks. Our work shows promise towards generalizable deep feature learning techniques suitable for graph structured data types where node neighborhood proximity should inform node feature prediction. In particular, this can greatly benefit fields involving irregularly sampled measurements, which would otherwise fail to gain representation under standard image-centric approaches. Potential applications span far beyond digit recognition, e.g., biomedical imaging, sensor readings, and more. To summarize, we present a unique contribution within deep learning literature: combining spectral graph theory and CNNs for improved performance on challenging graph signal classification tasks while maintaining computational efficiency necessary for large scale implementation. Significant improvements over existing state-of-the-art provide confidence in exploring future research directions.",1
"We have witnessed the discovery of many techniques for network representation learning in recent years, ranging from encoding the context in random walks to embedding the lower order connections, to finding latent space representations with auto-encoders. However, existing techniques are looking mostly into the local structures in a network, while higher-level properties such as global community structures are often neglected. We propose a novel network representations learning model framework called RUM (network Representation learning throUgh Multi-level structural information preservation). In RUM, we incorporate three essential aspects of a node that capture a network's characteristics in multiple levels: a node's affiliated local triads, its neighborhood relationships, and its global community affiliations. Therefore the framework explicitly and comprehensively preserves the structural information of a network, extending the encoding process both to the local end of the structural information spectrum and to the global end. The framework is also flexible enough to take various community discovery algorithms as its preprocessor. Empirical results show that the representations learned by RUM have demonstrated substantial performance advantages in real-life tasks.",0
"Here is a possible abstract for a research paper on network representation learning through multi-level structural information preservation:  In recent years, there has been increasing interest in developing techniques for representation learning over networks, as these approaches have shown promise in a wide range of applications such as recommender systems, social media analysis, and image processing. One important challenge in this area is how to effectively capture and preserve the complex structure underlying real-world networks, which can involve multiple levels of hierarchy, uncertainty, and dynamics. In this work, we propose a new methodology called RUM (Representation Learning through Multi-level Structural Information Preservation) that addresses this issue by leveraging a combination of graph neural networks and attention mechanisms to adaptively model different aspects of network structures at varying scales. We evaluate our approach using several benchmark datasets from diverse domains, demonstrating significant improvements compared to state-of-the-art baselines across different metrics related to node classification, link prediction, and clustering validation. Overall, our findings suggest that RUM provides a powerful framework for capturing and exploiting rich network structures in various data mining tasks, promising further advances in AI and machine learning research.",1
"Sketch portrait generation benefits a wide range of applications such as digital entertainment and law enforcement. Although plenty of efforts have been dedicated to this task, several issues still remain unsolved for generating vivid and detail-preserving personal sketch portraits. For example, quite a few artifacts may exist in synthesizing hairpins and glasses, and textural details may be lost in the regions of hair or mustache. Moreover, the generalization ability of current systems is somewhat limited since they usually require elaborately collecting a dictionary of examples or carefully tuning features/components. In this paper, we present a novel representation learning framework that generates an end-to-end photo-sketch mapping through structure and texture decomposition. In the training stage, we first decompose the input face photo into different components according to their representational contents (i.e., structural and textural parts) by using a pre-trained Convolutional Neural Network (CNN). Then, we utilize a Branched Fully Convolutional Neural Network (BFCN) for learning structural and textural representations, respectively. In addition, we design a Sorted Matching Mean Square Error (SM-MSE) metric to measure texture patterns in the loss function. In the stage of sketch rendering, our approach automatically generates structural and textural representations for the input photo and produces the final result via a probabilistic fusion scheme. Extensive experiments on several challenging benchmarks suggest that our approach outperforms example-based synthesis algorithms in terms of both perceptual and objective metrics. In addition, the proposed method also has better generalization ability across dataset without additional training.",0
"In recent years, advances in machine learning have enabled computers to generate images that closely match human artwork [8]. However, most existing methods require a large amount of training data and high computational cost, which limits their application in practice. Meanwhile, there is still a significant gap between synthesized images generated by machines and those created by humans, especially for portraits. To address these issues, we propose a novel framework called Content-Adaptive Sketch Portrait Generation (CASPG) using decompositional representation learning. Our method effectively captures the content and style disentanglement from the input image and then transfers them into skeleton contents as well as texture appearance features of sketch portrait generation. We demonstrate the superiority of our approach over state-of-the-art techniques on several benchmark datasets, both quantitatively and qualitatively, through extensive experiments involving expert evaluations and user studies. Compared with other generative models, CASPG presents competitive visual quality with much less training data and lower computation costs, thus showing promising potentials for real-world applications in fine arts, illustrations, fashion designing and online shopping, education, and so forth. Keywords: Generative Adversarial Networks; Style Disentanglement; Decompositional Representation Learning; Sketch Portrait Synthesis; Fine Arts Applications",1
"Face attribute estimation has many potential applications in video surveillance, face retrieval, and social media. While a number of methods have been proposed for face attribute estimation, most of them did not explicitly consider the attribute correlation and heterogeneity (e.g., ordinal vs. nominal and holistic vs. local) during feature representation learning. In this paper, we present a Deep Multi-Task Learning (DMTL) approach to jointly estimate multiple heterogeneous attributes from a single face image. In DMTL, we tackle attribute correlation and heterogeneity with convolutional neural networks (CNNs) consisting of shared feature learning for all the attributes, and category-specific feature learning for heterogeneous attributes. We also introduce an unconstrained face database (LFW+), an extension of public-domain LFW, with heterogeneous demographic attributes (age, gender, and race) obtained via crowdsourcing. Experimental results on benchmarks with multiple face attributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposed approach has superior performance compared to state of the art. Finally, evaluations on a public-domain face database (LAP) with a single attribute show that the proposed approach has excellent generalization ability.",0
"Title: Heterogeneous Face Attribute Estimation using Multi-task Learning  Face attribute estimation has gained increasing interest in recent years due to its numerous applications in computer vision, such as face recognition, image retrieval, and automated surveillance systems. Accurately estimating multiple attributes such as age, gender, ethnicity, and emotional state from facial images remains a challenging task, particularly in real world scenarios where data may have inherent variability and noise. In this work, we propose a novel multi-task learning approach that leverages deep neural networks to simultaneously estimate multiple face attributes with high accuracy across heterogeneous datasets. Our proposed method utilizes convolutional neural networks (CNNs) combined with recurrent layers to learn representations that capture both spatial and temporal features present in facial images. We evaluate our framework on two publicly available datasets, the Cross-Dataset Aging Challenge dataset (CDAC), and the EmotioNet benchmark database, showing improved performance over existing state-of-the-art methods. Results demonstrate the effectiveness of our method in accurately predicting multiple face attributes in a variety of contexts, making it suitable for deployment in real-world settings. Overall, our study presents a new approach towards solving the problem of heterogeneous face attribute estimation by leveraging multi-task learning and deep neural networks.",1
"Random walks are at the heart of many existing deep learning algorithms for graph data. However, such algorithms have many limitations that arise from the use of random walks, e.g., the features resulting from these methods are unable to transfer to new nodes and graphs as they are tied to node identity. In this work, we introduce the notion of attributed random walks which serves as a basis for generalizing existing methods such as DeepWalk, node2vec, and many others that leverage random walks. Our proposed framework enables these methods to be more widely applicable for both transductive and inductive learning as well as for use on graphs with attributes (if available). This is achieved by learning functions that generalize to new nodes and graphs. We show that our proposed framework is effective with an average AUC improvement of 16.1% while requiring on average 853 times less space than existing methods on a variety of graphs from several domains.",0
"In recent years, graph-based representation learning methods have emerged as powerful tools for understanding complex data structures such as networks, graphs, and knowledge bases. These methods use graph-theoretic techniques to capture relationships between objects within the data set, allowing them to learn meaningful representations that can be used for downstream tasks. However, existing graph-based representation learning methods suffer from several limitations, including their reliance on specific problem formulations, lack of generalization across domains, and the difficulty in incorporating additional contextual information into the model. To address these issues, we propose a framework for generalizing graph-based representation learning methods. Our framework introduces new components that allow models to adapt to different types of problems by dynamically adjusting their behavior based on the underlying structure of the input graph. We evaluate our approach using three different datasets, demonstrating that our method outperforms state-of-the-art baselines in terms of both accuracy and interpretability. Our work represents a significant step towards developing more flexible and robust graph-based representation learning algorithms, and has important implications for real-world applications such as recommender systems, fraud detection, and biological network analysis.",1
"We propose a new method for embedding graphs while preserving directed edge information. Learning such continuous-space vector representations (or embeddings) of nodes in a graph is an important first step for using network information (from social networks, user-item graphs, knowledge bases, etc.) in many machine learning tasks.   Unlike previous work, we (1) explicitly model an edge as a function of node embeddings, and we (2) propose a novel objective, the ""graph likelihood"", which contrasts information from sampled random walks with non-existent edges. Individually, both of these contributions improve the learned representations, especially when there are memory constraints on the total size of the embeddings. When combined, our contributions enable us to significantly improve the state-of-the-art by learning more concise representations that better preserve the graph structure.   We evaluate our method on a variety of link-prediction task including social networks, collaboration networks, and protein interactions, showing that our proposed method learn representations with error reductions of up to 76% and 55%, on directed and undirected graphs. In addition, we show that the representations learned by our method are quite space efficient, producing embeddings which have higher structure-preserving accuracy but are 10 times smaller.",0
"This paper proposes a new method for learning edge representations via low-rank asymmetric projections (LARAP). We define LARAP as a projection from higher dimensions onto lower dimensions which preserves most variations on one side while removes redundant and noisy ones on another side. By casting the problem into the lens of optimization on Grassmannian manifolds, we present two optimization methods for solving our proposed model: Stochastic Gradient Descent Manifold Ranking (Sgdr) and Adam Manifold Ranking (Amr). Moreover, extensive experiments demonstrate that LARAP achieves state-of-the-art results on various downstream tasks including image classification, object detection, and semantic segmentation. Our work highlights the potential of explicitly representing edges, and shows competitive performance against recent deep learning models trained end-to-end without bells and whistles like multi-scale training or data augmentation. This simplicity should encourage practitioners to return to elegant ideas from the past, now refined through modern convex relaxations and efficient numerical solvers. ------------------  The paper presents a novel approach for learning edge representations by utilizing low-rank asymmetric projections (LARAP). These projections aim at preserving significant variations on one side and removing redundant and irrelevant components on the other side. To achieve this goal, the authors cast the problem within the framework of optimization on Grassmannian manifolds and introduce two algorithms named Stochochastic Gradient Descent Manifold Ranking (Sgdr) and Adam Manifold Ranking (Amr). Extensive experimental evaluation demonstrates that the proposed method outperforms current approaches across different tasks such as image classification, object detection, and semantic segmen",1
"Sparse representation learning has recently gained a great success in signal and image processing, thanks to recent advances in dictionary learning. To this end, the $\ell_0$-norm is often used to control the sparsity level. Nevertheless, optimization problems based on the $\ell_0$-norm are non-convex and NP-hard. For these reasons, relaxation techniques have been attracting much attention of researchers, by priorly targeting approximation solutions (e.g. $\ell_1$-norm, pursuit strategies). On the contrary, this paper considers the exact $\ell_0$-norm optimization problem and proves that it can be solved effectively, despite of its complexity. The proposed method reformulates the problem as a Mixed-Integer Quadratic Program (MIQP) and gets the global optimal solution by applying existing optimization software. Because the main difficulty of this approach is its computational time, two techniques are introduced that improve the computational speed. Finally, our method is applied to image denoising which shows its feasibility and relevance compared to the state-of-the-art.",0
"This abstract describes a new approach for solving the problem of dictionary learning using the $l_0$ norm, which has been shown to produce sparse representations that can improve performance on downstream tasks. The proposed method leverages recent advances in optimization techniques to find a solution that minimizes both the reconstruction error and the $l_0$ regularization term, which encourages sparsity. Experimental results demonstrate the effectiveness of our approach compared to other methods, including those based on traditional smooth surrogates such as the $l_1$ norm. Our work has important implications for applications in computer vision, signal processing, and machine learning more broadly.",1
"Deep Neural Networks trained on large datasets can be easily transferred to new domains with far fewer labeled examples by a process called fine-tuning. This has the advantage that representations learned in the large source domain can be exploited on smaller target domains. However, networks designed to be optimal for the source task are often prohibitively large for the target task. In this work we address the compression of networks after domain transfer.   We focus on compression algorithms based on low-rank matrix decomposition. Existing methods base compression solely on learned network weights and ignore the statistics of network activations. We show that domain transfer leads to large shifts in network activations and that it is desirable to take this into account when compressing. We demonstrate that considering activation statistics when compressing weights leads to a rank-constrained regression problem with a closed-form solution. Because our method takes into account the target domain, it can more optimally remove the redundancy in the weights. Experiments show that our Domain Adaptive Low Rank (DALR) method significantly outperforms existing low-rank compression techniques. With our approach, the fc6 layer of VGG19 can be compressed more than 4x more than using truncated SVD alone -- with only a minor or no loss in accuracy. When applied to domain-transferred networks it allows for compression down to only 5-20% of the original number of parameters with only a minor drop in performance.",0
"Recently, there has been growing interest in compressing neural networks to improve their efficiency and scalability. Many methods have focused on removing redundancy within individual layers by pruning connections or weights. In contrast, we propose a novel approach that operates directly on the convolutional filters themselves, adapting them for each domain independently while still preserving high accuracy. Our method leverages recent advances in meta learning to train a set of lightweight learners that can quickly specialize to new domains using only a few examples. We evaluate our technique across five diverse image classification benchmarks and show consistent improvement compared to stateof-the art models and pruning methods while maintaining similar levels of accuracy. These results demonstrate the potential of filter adaptation as a promising direction for efficient deep neural network design.",1
"This paper proposes a multi-level feature learning framework for human action recognition using a single body-worn inertial sensor. The framework consists of three phases, respectively designed to analyze signal-based (low-level), components (mid-level) and semantic (high-level) information. Low-level features capture the time and frequency domain property while mid-level representations learn the composition of the action. The Max-margin Latent Pattern Learning (MLPL) method is proposed to learn high-level semantic descriptions of latent action patterns as the output of our framework. The proposed method achieves the state-of-the-art performances, 88.7%, 98.8% and 72.6% (weighted F1 score) respectively, on Skoda, WISDM and OPP datasets.",0
"An important challenge in computer vision is recognizing human actions from sensor data such as video footage or depth maps captured by Microsoft Kinect cameras. We present a framework for learning features that represent both local and global aspects of human motion for recognition tasks like action classification and skeleton joint estimation. This approach uses two convolutional neural networks (CNNs), each specialized for either local feature extraction or encoding temporal dependencies over whole clips using recurrent layers. These modules respectively output fixed length representations, which are then fused into a final descriptor for downstream applications. We evaluate our method on standard benchmarks Nuit et Brune and PHYRE2K. Using multiple regression, we achieve state-of-the-art results, outperforming top methods tested in recent years across all metrics including accuracy, precision and recall. On a subset of these datasets where only RGB information rather than 3D camera reconstructed meshes is available, our architecture still achieves competitive performance, proving effectiveness of our framework against baseline approaches which operate exclusively in image space without use of optical flow. In summary, our work introduces a novel deep learning model able to learn rich spatio-temporal representation suitable for many challenges arising in human behavior analysis, opening up possibilities for even higher performance through fine tuning, extension of training sets and more advanced network architectures.",1
"We study the problem of acoustic feature learning in the setting where we have access to another (non-acoustic) modality for feature learning but not at test time. We use deep variational canonical correlation analysis (VCCA), a recently proposed deep generative method for multi-view representation learning. We also extend VCCA with improved latent variable priors and with adversarial learning. Compared to other techniques for multi-view feature learning, VCCA's advantages include an intuitive latent variable interpretation and a variational lower bound objective that can be trained end-to-end efficiently. We compare VCCA and its extensions with previous feature learning methods on the University of Wisconsin X-ray Microbeam Database, and show that VCCA-based feature learning improves over previous methods for speaker-independent phonetic recognition.",0
"This sounds interesting! I would suggest starting the abstract by explaining that many applications in signal processing involve learning representations of signals using deep neural networks. However, finding meaningful representations can often lead to computational challenges due to large datasets and high dimensionality. One approach that has been proposed is deep variational canonical correlation analysis (DVCCA), which jointly learns a representation of two sets of data while maximizing their mutual agreement. In the new paper, the authors aim to extend DVCCA to acoustic feature learning by introducing a novel objective function based on the cross entropy loss. They evaluate the performance of their method on several benchmark datasets and show promising results compared to state-of-the-art baseline models. Overall, the paper presents a valuable contribution to the field of acoustic feature learning and could potentially have important implications for areas such as speech recognition and music classification.",1
"The recently developed variational autoencoders (VAEs) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However, most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena. In this work, we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, to enable infinite flexibility of the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations.",0
"One of the key challenges in deep learning is understanding how to use neural networks to learn hierarchically structured representations that can effectively capture complex patterns in data. In this work, we address this challenge by introducing nonparametric variational auto-encoders (NVAE), a new model architecture that combines advances from both variational auto-encoding (VAE) and generative adversarial imitation learning (GAIL). NVAEs enable efficient, scalable training on large datasets while still allowing for flexible, interpretable, and expressive modeling. We evaluate our approach using several different benchmark tasks across diverse domains, demonstrating that NVAEs achieve state-of-the-art performance while requiring significantly fewer parameters and computational resources than alternative methods. Our results highlight the promise of nonparametric models for enabling more effective representation learning in deep neural networks.",1
"We introduce a novel method for representation learning that uses an artificial supervision signal based on counting visual primitives. This supervision signal is obtained from an equivariance relation, which does not require any manual annotation. We relate transformations of images to transformations of the representations. More specifically, we look for the representation that satisfies such relation rather than the transformations that match a given representation. In this paper, we use two image transformations in the context of counting: scaling and tiling. The first transformation exploits the fact that the number of visual primitives should be invariant to scale. The second transformation allows us to equate the total number of visual primitives in each tile to that in the whole image. These two transformations are combined in one constraint and used to train a neural network with a contrastive loss. The proposed task produces representations that perform on par or exceed the state of the art in transfer learning benchmarks.",0
"Learning to count has been shown to be a powerful tool for representation learning in machine learning models. In recent years, there have been several advances in counting methods that leverage deep neural networks to estimate counts. These approaches can improve over traditional statistical methods such as importance sampling, particle filtering, and histogram estimation. One popular method called Neural Radiance Estimation (NRE) was proposed in 2020 and showed promising results on object counting tasks. This work proposes a new algorithm based on NRE that improves upon state-of-the-art performance on several benchmark datasets including Cityscapes and KITTI. We analyze the effectiveness of our approach compared to baseline methods, discuss the impact of network architecture and training data, and present ablation studies to validate our design choices. Our findings suggest that using deep neural networks for counting offers significant benefits in terms of accuracy and robustness, making it a valuable technique for future research in computer vision and related fields.",1
"In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.",0
"Here we present a new method for training neural network models to learn visual representations without supervision, based on solving jigsaw puzzles. We find that this leads to a dramatic improvement over previous approaches, allowing us to solve many real world problems such as image generation, classification, and reinforcement learning. In particular, our model is able to generate images which capture a wide range of details at high resolution, rivaling the quality of supervised methods while using no labels at all. Additionally, the learned representations transfer well across tasks, outperforming previous unsupervised methods. We believe these results have important implications for computer vision and artificial intelligence more broadly.",1
"The main contribution of this paper is a simple semi-supervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market-1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/Person-reID_GAN.",0
"This study examines the use of unlabeled samples generated by generative adversarial networks (GANs) to improve person re-identification performance in vitro. Research has shown that data augmentation techniques such as labeling can increase accuracy. However, adding labels can be time consuming and costly. Here we explore the potential benefits of using synthetic samples created by GANs instead. Our results suggest that integrating these generated samples into the training set improved the model's ability to identify individuals across different cameras, outperforming traditional methods without additional labeled data. We conclude that GANs provide an alternative approach to increasing sample size and improving recognition rates. Further research on how to optimize both the generator network and the discriminator network could potentially enhance the effectiveness of our proposed methodology. Overall, this work provides valuable insight into how artificial intelligence can assist in addressing real-world challenges related to public safety, surveillance monitoring systems, computer vision, image processing, and machine learning.",1
"The success of deep learning in computer vision is rooted in the ability of deep networks to scale up model complexity as demanded by challenging visual tasks. As complexity is increased, so is the need for large amounts of labeled data to train the model. This is associated with a costly human annotation effort. To address this concern, with the long-term goal of leveraging the abundance of cheap unlabeled data, we explore methods of unsupervised ""pre-training."" In particular, we propose to use self-supervised automatic image colorization.   We show that traditional methods for unsupervised learning, such as layer-wise clustering or autoencoders, remain inferior to supervised pre-training. In search for an alternative, we develop a fully automatic image colorization method. Our method sets a new state-of-the-art in revitalizing old black-and-white photography, without requiring human effort or expertise. Additionally, it gives us a method for self-supervised representation learning. In order for the model to appropriately re-color a grayscale object, it must first be able to identify it. This ability, learned entirely self-supervised, can be used to improve other visual tasks, such as classification and semantic segmentation. As a future direction for self-supervision, we investigate if multiple proxy tasks can be combined to improve generalization. This turns out to be a challenging open problem. We hope that our contributions to this endeavor will provide a foundation for future efforts in making self-supervision compete with supervised pre-training.",0
"This study investigates how to uncover visual semantics using unsupervised and self-supervised representation learning methods. By analyzing large amounts of image data, our approach can identify patterns and relationships within images that reveal important underlying concepts. We demonstrate how these techniques can effectively learn representations without any explicit guidance or labels, as well as how they can leverage limited labeled data to improve performance. Our results show that this method leads to significant improvements in tasks such as object recognition and semantic segmentation. This work has implications for advancing the field of computer vision and enabling more intelligent machines.",1
"The success of any machine learning system depends critically on effective representations of data. In many cases, it is desirable that a representation scheme uncovers the parts-based, additive nature of the data. Of current representation learning schemes, restricted Boltzmann machines (RBMs) have proved to be highly effective in unsupervised settings. However, when it comes to parts-based discovery, RBMs do not usually produce satisfactory results. We enhance such capacity of RBMs by introducing nonnegativity into the model weights, resulting in a variant called nonnegative restricted Boltzmann machine (NRBM). The NRBM produces not only controllable decomposition of data into interpretable parts but also offers a way to estimate the intrinsic nonlinear dimensionality of data, and helps to stabilize linear predictive models. We demonstrate the capacity of our model on applications such as handwritten digit recognition, face recognition, document classification and patient readmission prognosis. The decomposition quality on images is comparable with or better than what produced by the nonnegative matrix factorization (NMF), and the thematic features uncovered from text are qualitatively interpretable in a similar manner to that of the latent Dirichlet allocation (LDA). The stability performance of feature selection on medical data is better than RBM and competitive with NMF. The learned features, when used for classification, are more discriminative than those discovered by both NMF and LDA and comparable with those by RBM.",0
"In order to build accurate predictive models from data that contain noise, overfitting issues must often addressed by penalizing large weights during learning. This commonly leads to suboptimal solutions which may miss important relationships within the data. We propose using nonnegative restricted Boltzmann machines (NRBM) as a solution because they allow parts-based representations discovery while still preserving their advantages over alternative methods. NRBM have previously been successful at capturing high-level concepts in visual scenes but we show how these ideas can be applied to other types of data such as gene expression microarray data. Our experiments on three public datasets demonstrate stable performance with smaller standard errors than compared techniques. NRBM successfully discover relevant features from low-dimensional representations which makes them ideal for problems where computation cost is critical. Aspects related to interpretability and model selection are discussed.",1
"The analysis of mixed data has been raising challenges in statistics and machine learning. One of two most prominent challenges is to develop new statistical techniques and methodologies to effectively handle mixed data by making the data less heterogeneous with minimum loss of information. The other challenge is that such methods must be able to apply in large-scale tasks when dealing with huge amount of mixed data. To tackle these challenges, we introduce parameter sharing and balancing extensions to our recent model, the mixed-variate restricted Boltzmann machine (MV.RBM) which can transform heterogeneous data into homogeneous representation. We also integrate structured sparsity and distance metric learning into RBM-based models. Our proposed methods are applied in various applications including latent patient profile modelling in medical data analysis and representation learning for image retrieval. The experimental results demonstrate the models perform better than baseline methods in medical data and outperform state-of-the-art rivals in image dataset.",0
"""In recent years, there has been significant interest in mixed data models (MDMs) due to their ability to handle multiple types of data in a unified framework. MDMs can provide valuable insights into complex relationships that exist between different types of data, which can lead to improved decision making and better outcomes across various domains. Despite the potential benefits of MDMs, they have several challenges such as high computational complexity, difficulty in model selection, and lack of interpretability. To address these challenges, we propose a statistical latent space approach (SLS) for MDMing which provides efficient estimation methods and allows for easy interpretation and communication of results. We apply SLS to three diverse application areas: finance, healthcare, and environmental science.""",1
"Learning visual representations with self-supervised learning has become popular in computer vision. The idea is to design auxiliary tasks where labels are free to obtain. Most of these tasks end up providing data to learn specific kinds of invariance useful for recognition. In this paper, we propose to exploit different self-supervised approaches to learn representations invariant to (i) inter-instance variations (two objects in the same class should have similar features) and (ii) intra-instance variations (viewpoint, pose, deformations, illumination, etc). Instead of combining two approaches with multi-task learning, we argue to organize and reason the data with multiple variations. Specifically, we propose to generate a graph with millions of objects mined from hundreds of thousands of videos. The objects are connected by two types of edges which correspond to two types of invariance: ""different instances but a similar viewpoint and category"" and ""different viewpoints of the same instance"". By applying simple transitivity on the graph with these edges, we can obtain pairs of images exhibiting richer visual invariance. We use this data to train a Triplet-Siamese network with VGG16 as the base architecture and apply the learned representations to different recognition tasks. For object detection, we achieve 63.2% mAP on PASCAL VOC 2007 using Fast R-CNN (compare to 67.3% with ImageNet pre-training). For the challenging COCO dataset, our method is surprisingly close (23.5%) to the ImageNet-supervised counterpart (24.4%) using the Faster R-CNN framework. We also show that our network can perform significantly better than the ImageNet network in the surface normal estimation task.",0
"A major challenge in training machine learning models is obtaining large amounts of labeled data. To address this issue, self-supervised learning has gained popularity as a methodology that enables algorithms to learn from unlabeled datasets by utilizing naturally occurring supervision signals. These methods typically rely on designing pretext tasks whose solutions provide implicit guidance to deep networks, allowing them to encode meaningful representations into their weights. One key property these pretext tasks often exhibit is transitive invariance: the solution to the task should remain consistent under certain changes to the input data, such as translations or crops. However, little attention has been paid to investigating whether enforcing transitive invariance during training can actually improve representation quality. We aim to fill this gap by evaluating several formulations of transitive invariance across multiple vision benchmarks, finding evidence that incorporating such constraints indeed leads to improved self-supervised learning performance.",1
"With the rapid growth of online fashion market, demand for effective fashion recommendation systems has never been greater. In fashion recommendation, the ability to find items that goes well with a few other items based on style is more important than picking a single item based on the user's entire purchase history. Since the same user may have purchased dress suits in one month and casual denims in another, it is impossible to learn the latent style features of those items using only the user ratings. If we were able to represent the style features of fashion items in a reasonable way, we will be able to recommend new items that conform to some small subset of pre-purchased items that make up a coherent style set. We propose Style2Vec, a vector representation model for fashion items. Based on the intuition of distributional semantics used in word embeddings, Style2Vec learns the representation of a fashion item using other items in matching outfits as context. Two different convolutional neural networks are trained to maximize the probability of item co-occurrences. For evaluation, a fashion analogy test is conducted to show that the resulting representation connotes diverse fashion related semantics like shapes, colors, patterns and even latent styles. We also perform style classification using Style2Vec features and show that our method outperforms other baselines.",0
"In recent years, fashion industry has seen rapid growth due to e-commerce platforms that provide personalized shopping experiences based on user preferences, including their style sense. However, capturing users’ preferred styles accurately remains challenging as existing methods rely on keyword-based representation which often fails to fully capture the subtleties of human judgment. To address these issues, we propose Style2Vec, a novel approach to learn vector representations of fashion items from large collections of styled outfits using neural networks. Our model allows us to compute continuous item embeddings by learning from aggregated user feedback (e.g., clicks) instead of relying solely on textual descriptions. We evaluate our method against alternative state-of-the-art embedding techniques and demonstrate improved performance on downstream tasks such as clustering, classification, and recommendation. Overall, Style2Vec provides an effective solution for creating accurate vectorial representations of fashion items and enables new possibilities for building intelligent systems within the fashion domain.",1
"We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.",0
"Recent advances in deep learning have enabled significant progress in image colorization tasks using convolutional neural networks (CNNs). However, current state-of-the-art methods still face challenges related to hallucination artifacts, unrealistic colors, and poor generalization performance on out-of-domain data. In order to address these issues, we propose a novel framework that utilizes attention mechanisms and adversarial training to learn more robust representations. Our approach can effectively improve both quantitative metrics and visual quality compared to previous approaches. We demonstrate our method's effectiveness through experiments on multiple benchmark datasets and provide qualitative results showcasing its ability to produce visually pleasing colorizations while minimizing common mistakes made by other techniques. Overall, our work represents a step forward in the field of automatic image colorization and sets new standards for future research in this area.",1
"Given a large unlabeled set of images, how to efficiently and effectively group them into clusters based on extracted visual representations remains a challenging problem. To address this problem, we propose a convolutional neural network (CNN) to jointly solve clustering and representation learning in an iterative manner. In the proposed method, given an input image set, we first randomly pick k samples and extract their features as initial cluster centroids using the proposed CNN with an initial model pre-trained from the ImageNet dataset. Mini-batch k-means is then performed to assign cluster labels to individual input samples for a mini-batch of images randomly sampled from the input image set until all images are processed. Subsequently, the proposed CNN simultaneously updates the parameters of the proposed CNN and the centroids of image clusters iteratively based on stochastic gradient descent. We also proposed a feature drift compensation scheme to mitigate the drift error caused by feature mismatch in representation learning. Experimental results demonstrate the proposed method outperforms start-of-the-art clustering schemes in terms of accuracy and storage complexity on large-scale image sets containing millions of images.",0
"This paper presents a novel approach for joint clustering and representation learning from large-scale image data that incorporates feature drift compensation using convolutional neural networks (CNNs). The proposed method addresses the issue of changing distribution of features over time by compensating for the feature drift and maintaining consistent performance across different epochs. The method first learns a deep embedding model from the training data and then uses it as a regularizer during online mini-batch k-means clustering. Experiments on several benchmark datasets show that our method outperforms state-of-the-art methods in terms of accuracy, robustness, and scalability. Our approach has significant applications in areas such as object detection, classification, and image retrieval where handling temporal changes in distributions is essential.",1
"Face parsing is an important problem in computer vision that finds numerous applications including recognition and editing. Recently, deep convolutional neural networks (CNNs) have been applied to image parsing and segmentation with the state-of-the-art performance. In this paper, we propose a face parsing algorithm that combines hierarchical representations learned by a CNN, and accurate label propagations achieved by a spatially variant recurrent neural network (RNN). The RNN-based propagation approach enables efficient inference over a global space with the guidance of semantic edges generated by a local convolutional model. Since the convolutional architecture can be shallow and the spatial RNN can have few parameters, the framework is much faster and more light-weighted than the state-of-the-art CNNs for the same task. We apply the proposed model to coarse-grained and fine-grained face parsing. For fine-grained face parsing, we develop a two-stage approach by first identifying the main regions and then segmenting the detail components, which achieves better performance in terms of accuracy and efficiency. With a single GPU, the proposed algorithm parses face images accurately at 300 frames per second, which facilitates real-time applications.",0
"This paper presents a novel approach to face parsing using recurrent propagation, which leverages both local and global context to achieve highly accurate predictions. By combining convolutional neural networks (CNNs) with long short-term memory (LSTM) cells, we create a deep learning model that effectively parses faces at multiple scales while capturing temporal dependencies among pixels. Experimental results on benchmark datasets demonstrate that our method significantly outperforms state-of-the-art methods in terms of accuracy and robustness. Overall, our work advances the field of computer vision by enabling more precise and reliable face analysis, with potential applications in areas such as security, surveillance, and biometrics.",1
"In this paper, we suggest a framework to make use of mutual information as a regularization criterion to train Auto-Encoders (AEs). In the proposed framework, AEs are regularized by minimization of the mutual information between input and encoding variables of AEs during the training phase. In order to estimate the entropy of the encoding variables and the mutual information, we propose a non-parametric method. We also give an information theoretic view of Variational AEs (VAEs), which suggests that VAEs can be considered as parametric methods that estimate entropy. Experimental results show that the proposed non-parametric models have more degree of freedom in terms of representation learning of features drawn from complex distributions such as Mixture of Gaussians, compared to methods which estimate entropy using parametric approaches, such as Variational AEs.",0
"This paper introduces a new method for training deep neural networks called Information Potential Auto-encoders (IPAEs). IPAEs are designed to maximize the mutual information between input data and reconstructed output by minimizing the reconstruction error subject to a constraint on the maximum amount of information that can be transmitted through the network. By using a tractable lower bound on mutual information as the objective function, we can train deep generative models that retain more structured information about the underlying data than traditional auto-encoder architectures. We demonstrate the effectiveness of our approach on several benchmark datasets across multiple domains including image generation, speech synthesis, and natural language processing. Our results show that IPAEs outperform previous methods while providing interpretable representations and improved generalization performance. Additionally, we provide analysis of how different hyperparameters affect the behavior of the model, highlighting some key design choices for future work. Overall, IPAEs offer a promising direction towards better understanding deep learning, improving generalizability, and enabling interpretation.",1
"This paper proposes the SVDNet for retrieval problems, with focus on the application of person re-identification (re-ID). We view each weight vector within a fully connected (FC) layer in a convolutional neuron network (CNN) as a projection basis. It is observed that the weight vectors are usually highly correlated. This problem leads to correlations among entries of the FC descriptor, and compromises the retrieval performance based on the Euclidean distance. To address the problem, this paper proposes to optimize the deep representation learning process with Singular Vector Decomposition (SVD). Specifically, with the restraint and relaxation iteration (RRI) training scheme, we are able to iteratively integrate the orthogonality constraint in CNN training, yielding the so-called SVDNet. We conduct experiments on the Market-1501, CUHK03, and Duke datasets, and show that RRI effectively reduces the correlation among the projection vectors, produces more discriminative FC descriptors, and significantly improves the re-ID accuracy. On the Market-1501 dataset, for instance, rank-1 accuracy is improved from 55.3% to 80.5% for CaffeNet, and from 73.8% to 82.3% for ResNet-50.",0
"Effective pedestrian retrieval is a fundamental challenge in computer vision research, requiring high precision in identifying individuals across varying backgrounds and lighting conditions. In recent years, advancements have been made using deep learning techniques such as convolutional neural networks (CNNs) that leverage features extracted from image data. However, these methods suffer from limitations such as slow inference speeds and lack of interpretability.  To address these issues, we propose a new method: Singular Value Decomposition Networks (SVDNet). Our approach uses matrix factorization to transform feature vectors into a lower dimensional space while preserving relevant discriminative information. We demonstrate improved accuracy over state-of-the-art approaches on multiple benchmark datasets while providing faster inference times. Additionally, our method provides a unique advantage by allowing for interpretable results through visualizations of singular values.  Our work highlights the potential benefits of combining traditional linear algebra methods with modern machine learning techniques, opening up possibilities for future exploration in the field of computer vision. Overall, our study presents a significant contribution towards real-world applications of person re-identification, particularly in areas where speed and accuracy are critical, such as surveillance systems and biometric identification.",1
"Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by an expert. These models can be used e.g. for system control by generalizing the expert demonstrations to previously unencountered situations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g. they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert's controls, which limits their practical use as part of a general system identification framework. In this work, we consider the LfD problem in a more general setting where we allow for arbitrary stochastic expert policies, without reasoning about the optimality of the demonstrations. Following a Bayesian methodology, we model the full posterior distribution of possible expert controllers that explain the provided demonstration data. Moreover, we show that our methodology can be applied in a nonparametric context to infer the complexity of the state representation used by the expert, and to learn task-appropriate partitionings of the system state space.",0
"In recent years, policy recognition has become a fundamental problem in artificial intelligence. In practice, policy representation learning can improve decision making, planning, control, and diagnosis processes significantly. For successful realization, robust approaches that are able to handle incomplete and noisy observations are essential. This study proposes a novel approach based on probabilistic inference methods from Bayesian statistics. We extend particle filtering by incorporating hierarchical dependency structures between consecutive state variables to estimate policies more accurately and efficiently. Our main contributions are: (a) exploiting latent variable models as generative models enabling efficient simulation; (b) deriving variational approximations allowing scalable inference computation, even for partially observed nonlinear systems; (c) integrating these approximations into an iterative expectation maximization algorithm, balancing accuracy and efficiency. Our evaluation shows significant benefits over standard particle filter solutions using two challenging benchmarks illustrating diverse use cases and demonstrates competitiveness regarding alternative machine learning methods. In summary, our proposed framework advances research towards general and robust policy recognition from real-world applications in complex scenarios. This work paves the way for future developments beyond simplified settings, and offers practitioners new tools with tangible performance improvements.",1
"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",0
"Title: ""Revisiting Unreasonable Effectiveness of Data in Deep Learning Era""",1
"We present an unsupervised representation learning approach using videos without semantic labels. We leverage the temporal coherence as a supervisory signal by formulating representation learning as a sequence sorting task. We take temporally shuffled frames (i.e., in non-chronological order) as inputs and train a convolutional neural network to sort the shuffled sequences. Similar to comparison-based sorting algorithms, we propose to extract features from all frame pairs and aggregate them to predict the correct order. As sorting shuffled image sequence requires an understanding of the statistical temporal structure of images, training with such a proxy task allows us to learn rich and generalizable visual representation. We validate the effectiveness of the learned representation using our method as pre-training on high-level recognition problems. The experimental results show that our method compares favorably against state-of-the-art methods on action recognition, image classification and object detection tasks.",0
"Title: ""Unsupervised Representation Learning by Sorting Sequences""  This study proposes a new approach to unsupervised representation learning using sorting sequences. By applying stochastic gradient descent on a loss function measuring the difference between sorted and shuffled sequence distributions, the model learns to sort sequences into meaningful groupings that encode relevant features of the underlying data distribution. Results show improved performance over traditional clustering methods across multiple datasets, demonstrating the effectiveness of the proposed method for discovering hidden structure in unlabeled data. Additionally, results from ablation studies suggest that our approach benefits from incorporating both local and global information about the sequences being sorted. Our findings contribute to the growing field of unsupervised representation learning and have important implications for machine learning applications where large amounts of unlabelled data are available.",1
"Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However, existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. In this paper, we develop a dual motion Generative Adversarial Net (GAN) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction. To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows. Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows. Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning.",0
"Here is my attempt at writing an informative yet concise summary of a research paper on video prediction using Generative Adversarial Networks (GAN). If you have any feedback or suggestions please provide them!:  This work presents a novel approach to future frame interpolation using dual motion generative adversarial networks (Dual Motion GAN) by incorporating temporal guidance into both generator and discriminator networks. The proposed method effectively captures spatial and temporal coherency, allowing for accurate rendering of future frames embedded within the current video sequence. By utilizing a dual network design which generates predictions that can then refine the overall output, we demonstrate superior performance compared to state-of-the-art methods. This system offers improved visual quality and efficiency, making it well suited for real world applications such as virtual reality systems. We evaluate our model using commonly accepted metrics for evaluating image and video generation tasks. These results further validate the effectiveness of our framework. Ultimately, this work serves as a step forward towards developing intelligent video processing techniques driven by machine learning models that operate in real time.",1
"Unsupervised learning techniques in computer vision often require learning latent representations, such as low-dimensional linear and non-linear subspaces. Noise and outliers in the data can frustrate these approaches by obscuring the latent spaces.   Our main goal is deeper understanding and new development of robust approaches for representation learning. We provide a new interpretation for existing robust approaches and present two specific contributions: a new robust PCA approach, which can separate foreground features from dynamic background, and a novel robust spectral clustering method, that can cluster facial images with high accuracy. Both contributions show superior performance to standard methods on real-world test sets.",0
"This paper presents a novel approach for learning robust representations for computer vision tasks using deep convolutional neural networks (CNNs). Despite their success, CNNs can suffer from overfitting due to limited training data or noisy labels, which results in poor generalization performance on new datasets. To address these issues, we propose a framework that uses adversarial training and spectral normalization techniques to regularize and stabilize the learned representations. By doing so, our method produces more generalized and robust features that better capture the underlying structures and patterns present in image data. We evaluate our method on several benchmarking datasets and show consistent improvement across a variety of tasks compared to state-of-the-art methods. Our work demonstrates the effectiveness of combining adversarial training and spectral normalization in enhancing model stability and achieving better generalization performance for computer vision applications.",1
"Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the ill-posed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This work presents HashNet, a novel deep architecture for deep learning to hash by continuation method with convergence guarantees, which learns exactly binary hash codes from imbalanced similarity data. The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, deep network with the sign activation function. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art multimedia retrieval performance on standard benchmarks.",0
"In this paper, we introduce a new deep learning method called HashNet that uses continuation techniques to learn binary hash codes directly from data without any quantization steps. Our approach overcomes several limitations of traditional hashing methods such as low precision due to limited bit budgets, sensitivity to initializations, and slow convergence during training. By using continuous relaxations and warm starts, our algorithm can find high quality solutions more efficiently while providing flexible control over different aspects of the final hash codes. We demonstrate HashNet's effectiveness on a range of tasks including image retrieval, face verification, and recommendation systems, showing that it consistently outperforms state-of-the-art baselines and achieves competitive results even against methods optimized at test time.",1
"Nonnegative matrix factorization is a powerful technique to realize dimension reduction and pattern recognition through single-layer data representation learning. Deep learning, however, with its carefully designed hierarchical structure, is able to combine hidden features to form more representative features for pattern recognition. In this paper, we proposed sparse deep nonnegative matrix factorization models to analyze complex data for more accurate classification and better feature interpretation. Such models are designed to learn localized features or generate more discriminative representations for samples in distinct classes by imposing $L_1$-norm penalty on the columns of certain factors. By extending one-layer model into multi-layer one with sparsity, we provided a hierarchical way to analyze big data and extract hidden features intuitively due to nonnegativity. We adopted the Nesterov's accelerated gradient algorithm to accelerate the computing process with the convergence rate of $O(1/k^2)$ after $k$ steps iteration. We also analyzed the computing complexity of our framework to demonstrate their efficiency. To improve the performance of dealing with linearly inseparable data, we also considered to incorporate popular nonlinear functions into this framework and explored their performance. We applied our models onto two benchmarking image datasets, demonstrating our models can achieve competitive or better classification performance and produce intuitive interpretations compared with the typical NMF and competing multi-layer models.",0
"Non-negative matrix factorization (NMF) has emerged as a popular technique for analyzing multiway data due to its ability to capture latent relationships between multiple matrices. However, traditional NMF algorithms often encounter challenges when dealing with high dimensional datasets where many components have very few observations. To address these issues, sparse deep non-negative matrix factorization (SpaDeepNMF) is proposed as a novel method for jointly performing NMF on multiple matrices while enforcing sparsity constraints on the factors and promoting interpretability through the use of meaningful clusters. SpaDeepNMF demonstrates improved performance compared to traditional methods in both synthetic experiments and real-world applications such as collaborative filtering recommendation systems. This approach represents a significant advance in the field of multivariate analysis and holds promise for future research into efficient and interpretable NMF models.",1
"Convolutional Neural Network (CNN) models have become the state-of-the-art for most computer vision tasks with natural images. However, these are not best suited for multi-gigapixel resolution Whole Slide Images (WSIs) of histology slides due to large size of these images. Current approaches construct smaller patches from WSIs which results in the loss of contextual information. We propose to capture the spatial context using novel Representation-Aggregation Network (RAN) for segmentation purposes, wherein the first network learns patch-level representation and the second network aggregates context from a grid of neighbouring patches. We can use any CNN for representation learning, and can utilize CNN or 2D-Long Short Term Memory (2D-LSTM) for context-aggregation. Our method significantly outperformed conventional patch-based CNN approaches on segmentation of tumour in WSIs of breast cancer tissue sections.",0
"This abstract should include all sections: Introduction, Methods & Materials, Results, Conclusion & Future Work Introduction: Histopathological examination of tissue samples plays an important role in diagnosis and treatment planning for multiple diseases. Automatic segmentation of tissues from histology images can significantly reduce time and manual effort required by pathologists while improving consistency and precision. Although great progress has been made in developing deep learning based methods for image analysis, high throughput histopathological imaging generates multi-gigapixel (> billion pixels) images that are computationally challenging to process and require specialized architectures. In this work we present Representation-Aggregation Networks (RAN), a novel method for efficiently processing large scale histology images while maintaining superior segmentation performance. Methods & Materials: We designed RAN as a modular network architecture composed of two main components: feature extraction and feature aggregation. Our proposed feature extraction module leverages dilated convolutions to capture fine details across scales while remaining efficient and fast. To cope with increased memory demands due to high resolution input images, our feature aggregation component processes data in regions instead of individual pixel patches which allows us to use compact self attention mechanisms. We trained our model on a large dataset of over one thousand annotated WSIs encompassing a variety of normal and cancerous conditions across different organs and institutions. Results: Our experiments demonstrate the effectiveness of RAN against state-of-the-art methods in terms of quantitative metrics such as Dice coefficient score and Jaccard index on multiple datasets including publicly available sets and private institutional set. More importantly, qualitatively we observe consistent improvements in segmentations especially along complex boundaries where many existing methods fail. Lastl",1
"We report on CMU Informedia Lab's system used in Google's YouTube 8 Million Video Understanding Challenge. In this multi-label video classification task, our pipeline achieved 84.675% and 84.662% GAP on our evaluation split and the official test set. We attribute the good performance to three components: 1) Refined video representation learning with residual links and hypercolumns 2) Latent concept mining which captures interactions among concepts. 3) Learning with temporal segments and weighted multi-model ensemble. We conduct experiments to validate and analyze the contribution of our models. We also share some unsuccessful trials leveraging conventional approaches such as recurrent neural networks for video representation learning for this large-scale video dataset. All the codes to reproduce our results are publicly available at https://github.com/Martini09/informedia-yt8m-release.",0
"This could involve summarizing each main section of your paper and discussing how the sections work together to support your thesis statement. If you were going to write this, how would you proceed? I am not familiar enough with the content of the full paper but here is some guidance on writing an abstract: * Start by reading the entire paper carefully to identify the key points that need to go into the abstract. Often there may already be draft text from early versions that can form part of the final product. Some journals want structured abstracts divided up into Background/Objectives, Methods, Results, etc., while others prefer unstructured ones without subheadings; typically background material is at least implicitly conveyed through highlighting novelty. You should know which type yours requires. Your choice needs to ensure adequate flow and balance, covering all necessary ground without oversimplifying things that deserve nuance due to complexity, nor making it too complicated or dense for readers at different levels of knowledge to follow. Avoid overwhelming the reader with minutiae or digression, stick closely to the topic if it branches out unexpectedly; don’t introduce new matter or results not in the body (or only very briefly). For length reasons there won’t usually be space for figures; it may suffice simply to state that important supporting data appear within. Try using similar tone, register, verb tense, person, point of view throughout. Don’t give away everything – leave something memorable or thoughtful so they come back after the conference/publication, curious for more! But as abstract is the first thing many read, you must make them engaged and interested now rather than frustrating their expectations by omitting vital context. At the same time, it remains essentially advertisement copy meant to attract potential customers (readers), not explain every detail once they commit to purchase (study the entire article); this means balancing punchiness versus clarity and completeness; depending on target audience there may vary tradeoff between brevity/excitement vs longer attention spans requiring details. For these reasons I believe no single optimal solution exists; feel free to consult me again for revisions until we reach consensus. Good luck!",1
"In this work, we address the problem of improvement of robustness of feature representations learned using convolutional neural networks (CNNs) to image deformation. We argue that higher moment statistics of feature distributions could be shifted due to image deformations, and the shift leads to degrade of performance and cannot be reduced by ordinary normalization methods as observed in experimental analyses. In order to attenuate this effect, we apply additional non-linearity in CNNs by combining power functions with learnable parameters into convolution operation. In the experiments, we observe that CNNs which employ the proposed method obtain remarkable boost in both the generalization performance and the robustness under various types of deformations using large scale benchmark datasets. For instance, a model equipped with the proposed method obtains 3.3\% performance boost in mAP on Pascal Voc object detection task using deformed images, compared to the reference model, while both models provide the same performance using original images. To the best of our knowledge, this is the first work that studies robustness of deep features learned using CNNs to a wide range of deformations for object recognition and detection.",0
"This paper presents a method for improving the robustness of feature representations in convolutional neural networks (CNN) to image deformations such as scaling, rotation, translation, and distortion. The proposed method uses powered convolution, which applies different weights to each pixel based on their distance from the center of the image, rather than uniform convolution. We demonstrate that powered convolution can improve the performance of CNNs on benchmark datasets even without any additional training data, indicating that it helps the network learn more generalizable features. Furthermore, we show how powered convolution can be used to develop new architectures that outperform existing state-of-the-art methods on several challenging tasks related to object recognition, semantic segmentation, and keypoint detection. Finally, we provide extensive ablation studies and visualization techniques to gain insights into the effectiveness of our approach and validate its benefits over baseline models and other recent advances in computer vision.",1
"In this paper, we propose an end-to-end group-wise deep co-saliency detection approach to address the co-salient object discovery problem based on the fully convolutional network (FCN) with group input and group output. The proposed approach captures the group-wise interaction information for group images by learning a semantics-aware image representation based on a convolutional neural network, which adaptively learns the group-wise features for co-saliency detection. Furthermore, the proposed approach discovers the collaborative and interactive relationships between group-wise feature representation and single-image individual feature representation, and model this in a collaborative learning framework. Finally, we set up a unified end-to-end deep learning scheme to jointly optimize the process of group-wise feature representation learning and the collaborative learning, leading to more reliable and robust co-saliency detection results. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.",0
"In computer vision tasks, salient object detection has become increasingly important due to the need for understanding the content of images and videos. However, existing methods often focus on individual objects rather than groups of objects that may interact with each other in certain scenes. This paper presents a group-wise deep co-saliency detection approach that jointly models interdependent regions within a group using multi-instance learning. Our method leverages a convolutional neural network architecture trained on region proposals and utilizes a pairwise similarity matrix between proposal pairs within the same group to capture their relationships. Experimental results demonstrate significant improvements over state-of-the-art methods on four challenging datasets, highlighting the effectiveness of our proposed approach.",1
"Deep learning owes its success to three key factors: scale of data, enhanced models to learn representations from data, and scale of computation. This book chapter presented the importance of the data-driven approach to learn good representations from both big data and small data. In terms of big data, it has been widely accepted in the research community that the more data the better for both representation and classification improvement. The question is then how to learn representations from big data, and how to perform representation learning when data is scarce. We addressed the first question by presenting CNN model enhancements in the aspects of representation, optimization, and generalization. To address the small data challenge, we showed transfer representation learning to be effective. Transfer representation learning transfers the learned representation from a source domain where abundant training data is available to a target domain where training data is scarce. Transfer representation learning gave the OM and melanoma diagnosis modules of our XPRIZE Tricorder device (which finished $2^{nd}$ out of $310$ competing teams) a significant boost in diagnosis accuracy.",0
"This study explores the impact of data size on representation learning algorithms. We examine how different models perform across varying amounts of training data and evaluate their effectiveness at capturing meaningful representations. Our results show that larger datasets tend to lead to better performance, but smaller datasets can still achieve acceptable results if the model has enough capacity. Additionally, we find that models trained on small datasets may struggle to generalize well to new tasks, highlighting the importance of considering both accuracy and generalizability when selecting a model. Overall, our research contributes valuable insights into the relationship between dataset size and representation quality, which could inform future work in the field.",1
"Delineation of line patterns in images is a basic step required in various applications such as blood vessel detection in medical images, segmentation of rivers or roads in aerial images, detection of cracks in walls or pavements, etc. In this paper we present trainable B-COSFIRE filters, which are a model of some neurons in area V1 of the primary visual cortex, and apply it to the delineation of line patterns in different kinds of images. B-COSFIRE filters are trainable as their selectivity is determined in an automatic configuration process given a prototype pattern of interest. They are configurable to detect any preferred line structure (e.g. segments, corners, cross-overs, etc.), so usable for automatic data representation learning. We carried out experiments on two data sets, namely a line-network data set from INRIA and a data set of retinal fundus images named IOSTAR. The results that we achieved confirm the robustness of the proposed approach and its effectiveness in the delineation of line structures in different kinds of images.",0
"This project presents a methodology for image processing called Delineation of Line Patterns. By applying an algorithm known as B-Cosfire Filters to digital imagery, we can extract high-contrast lines, delicate structures, and fine details. These features often go unnoticed by other filtering techniques, making them ideal for applications such as biological tissue analysis and industrial inspection. To test our approach, we utilized several datasets encompassing diverse fields including electron microscopy, material science, and medical research. Our experiments yielded promising results that demonstrate the competence of B-COSIRE Filters in separating essential elements from background noise while maintaining structural integrity. We hope this work contributes to the advancement of computer vision and serves as a foundation for future exploration into the use of B-COSFIRE Filters for image enhancement purposes.",1
"Traffic scene recognition is an important and challenging issue in Intelligent Transportation Systems (ITS). Recently, Convolutional Neural Network (CNN) models have achieved great success in many applications, including scene classification. The remarkable representational learning capability of CNN remains to be further explored for solving real-world problems. Vector of Locally Aggregated Descriptors (VLAD) encoding has also proved to be a powerful method in catching global contextual information. In this paper, we attempted to solve the traffic scene recognition problem by combining the features representational capabilities of CNN with the VLAD encoding scheme. More specifically, the CNN features of image patches generated by a region proposal algorithm are encoded by applying VLAD, which subsequently represent an image in a compact representation. To catch the spatial information, spatial pyramids are exploited to encode CNN features. We experimented with a dataset of 10 categories of traffic scenes, with satisfactory categorization performances.",0
"This paper presents a method for traffic scene recognition using convolutional neural networks (CNNs) and visual descriptors obtained from spatial pyramids. We use a combination of VGGNet features extracted at different spatial scales to capture both fine details and global contextual information. These features are then fed into a deep CNN model trained to classify eight common traffic scenes including empty road, one vehicle, two vehicles, three vehicles, four vehicles, busy street, construction zone, accident scene, and road work zone. Our approach achieves state-of-the-art performance on a publicly available dataset, demonstrating the effectiveness of our proposed method in accurately recognizing complex traffic scenes. Additionally, we provide ablation studies to validate the contribution of each component in our system and discuss potential future improvements. Overall, our method provides a powerful tool for automotive applications such as driver assistance systems and autonomous driving.",1
"Video Question Answering is a challenging problem in visual information retrieval, which provides the answer to the referenced video content according to the question. However, the existing visual question answering approaches mainly tackle the problem of static image question, which may be ineffectively for video question answering due to the insufficiency of modeling the temporal dynamics of video contents. In this paper, we study the problem of video question answering by modeling its temporal dynamics with frame-level attention mechanism. We propose the attribute-augmented attention network learning framework that enables the joint frame-level attribute detection and unified video representation learning for video question answering. We then incorporate the multi-step reasoning process for our proposed attention network to further improve the performance. We construct a large-scale video question answering dataset. We conduct the experiments on both multiple-choice and open-ended video question answering tasks to show the effectiveness of the proposed method.",0
"In the era of large language models like GPT-4 there seems to be a consensus that simply feeding text through transformer architectures is enough for most NLP tasks. However this approach may not always yield optimal results on more complex problems such as Video Question Answering (VQA) which require joint reasoning across modalities. The authors present Attribute Augmented Attention network learning which proposes several key components designed to improve VQA performance. Firstly they introduce an attribute projection layer which embeds questions into a continuous space allowing them to interact directly with video frames. Secondly they use two attention mechanisms one over questions another over video segments both operating independently from each other and then combined at inference time using element wise multiplication. Thirdly they use cross attention blocks to encode interactions between question specific features and object representations. Fourthly they propose spatial attention as a technique to selectively focus decoder attentions to regions relevant to answering the query. Finally they apply feature recalibration by modulating learned weights of each attention block based on prediction errors during training. In summary the authors show that their method improves accuracy on VQA benchmark datasets by addressing challenges associated with multi modal representation and interaction, and model design choices leading to better generalization compared to prior art. This work has implications beyond VQA including possible applications to areas such as multi media retrieval or even language translation where explicit alignment of multiple modalities needs to be solved.",1
"Deep learning for human action recognition in videos is making significant progress, but is slowed down by its dependency on expensive manual labeling of large video collections. In this work, we investigate the generation of synthetic training data for action recognition, as it has recently shown promising results for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation and other computer graphics techniques of modern game engines. We generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for ""Procedural Human Action Videos"". It contains a total of 39,982 videos, with more than 1,000 examples for each action of 35 categories. Our approach is not limited to existing motion capture sequences, and we procedurally define 14 synthetic actions. We introduce a deep multi-task representation learning architecture to mix synthetic and real videos, even if the action categories differ. Our experiments on the UCF101 and HMDB51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance, significantly outperforming fine-tuning state-of-the-art unsupervised generative models of videos.",0
"Here is a sample abstract which summarizes some key points from your paper:  Procedural generation has emerged as a promising technique to generate large amounts of training data for deep learning models. We present a novel approach that uses procedural generation to create videos of human actions. Our method can synthesize diverse sequences by sampling parameterized action modules conditioned on contextual cues such as object affordances and scene layout. We demonstrate through extensive experiments that our generated videos significantly improve recognition performance compared to using images alone, achieving state-of-the-art results on popular benchmark datasets. These results highlight the potential of our method for scaling up video analysis applications and enabling new capabilities across industries.",1
"Auto-Encoders are unsupervised models that aim to learn patterns from observed data by minimizing a reconstruction cost. The useful representations learned are often found to be sparse and distributed. On the other hand, compressed sensing and sparse coding assume a data generating process, where the observed data is generated from some true latent signal source, and try to recover the corresponding signal from measurements. Looking at auto-encoders from this \textit{signal recovery perspective} enables us to have a more coherent view of these techniques. In this paper, in particular, we show that the \textit{true} hidden representation can be approximately recovered if the weight matrices are highly incoherent with unit $ \ell^{2} $ row length and the bias vectors takes the value (approximately) equal to the negative of the data mean. The recovery also becomes more and more accurate as the sparsity in hidden signals increases. Additionally, we empirically demonstrate that auto-encoders are capable of recovering the data generating dictionary when only data samples are given.",0
"Deep learning has revolutionized many fields by enabling the development of highly accurate models through deep neural networks (DNNs). However, due to their large capacity and complexity, DNNs can suffer from overfitting and underfitting, which leads to poor generalization performance on unseen data. As such, techniques like regularization have been developed to prevent these problems while still allowing the model to capture complex patterns in the data. One popular method is auto-encoders, which learn compressed representations that can be used as inputs for other machine learning algorithms. In this work, we focus on optimality conditions for auto-encoder signal recovery to enable efficient and effective use of deep neural network representations. Our results show that optimizing the loss function alone may not always lead to optimal solutions, and additional constraints need to be imposed to ensure accurate representation learning. We propose two novel criteria based on mutual information principles that provide strong guarantees on the quality of learned representations. Experiments using synthetic and real datasets demonstrate that our approach outperforms baseline methods in terms of both reconstruction error and feature utility metrics. Overall, our findings contribute towards developing better auto-encoders for signal recovery and improving performance in downstream applications.",1
"Class imbalance is a challenging issue in practical classification problems for deep learning models as well as traditional models. Traditionally successful countermeasures such as synthetic over-sampling have had limited success with complex, structured data handled by deep learning models. In this paper, we propose Deep Over-sampling (DOS), a framework for extending the synthetic over-sampling method to exploit the deep feature space acquired by a convolutional neural network (CNN). Its key feature is an explicit, supervised representation learning, for which the training data presents each raw input sample with a synthetic embedding target in the deep feature space, which is sampled from the linear subspace of in-class neighbors. We implement an iterative process of training the CNN and updating the targets, which induces smaller in-class variance among the embeddings, to increase the discriminative power of the deep representation. We present an empirical study using public benchmarks, which shows that the DOS framework not only counteracts class imbalance better than the existing method, but also improves the performance of the CNN in the standard, balanced settings.",0
"In this paper, we introduce a novel approach called the deep over-sampling framework (DoF) that addresses the problem of classifying imbalanced data sets. Our proposed method combines multiple techniques including synthetic minority oversampling technique (SMOTE), decision tree pruning, and k-nearest neighbors sampling, and works by progressively increasing the size of the underrepresented class until both classes have equal sizes. We evaluated our method on several benchmark datasets and found that it outperformed other state-of-the-art methods across all metrics used. Additionally, we conducted extensive experiments to demonstrate the effectiveness of each component of our framework and showed that their combination leads to significant improvements. Finally, we provided insights into why SMOTE alone may not always lead to better performance than random guessing, highlighting the limitations of relying solely on resampling to balance class sizes. This work provides a comprehensive study that advances our understanding of how to address class imbalance problems and has important implications for applications such as medical diagnosis, fraud detection, and object recognition systems.",1
"Existing block-diagonal representation researches mainly focuses on casting block-diagonal regularization on training data, while only little attention is dedicated to concurrently learning both block-diagonal representations of training and test data. In this paper, we propose a discriminative block-diagonal low-rank representation (BDLRR) method for recognition. In particular, the elaborate BDLRR is formulated as a joint optimization problem of shrinking the unfavorable representation from off-block-diagonal elements and strengthening the compact block-diagonal representation under the semi-supervised framework of low-rank representation. To this end, we first impose penalty constraints on the negative representation to eliminate the correlation between different classes such that the incoherence criterion of the extra-class representation is boosted. Moreover, a constructed subspace model is developed to enhance the self-expressive power of training samples and further build the representation bridge between the training and test samples, such that the coherence of the learned intra-class representation is consistently heightened. Finally, the resulting optimization problem is solved elegantly by employing an alternative optimization strategy, and a simple recognition algorithm on the learned representation is utilized for final prediction. Extensive experimental results demonstrate that the proposed method achieves superb recognition results on four face image datasets, three character datasets, and the fifteen scene multi-categories dataset. It not only shows superior potential on image recognition but also outperforms state-of-the-art methods.",0
"In recent years, deep learning has emerged as a powerful tool for image recognition tasks, owing largely to advancements in convolutional neural network (CNN) architectures. However, training these models can be computationally expensive and time consuming, particularly on large datasets. Additionally, many existing CNNs struggle to capture complex nonlinear relationships within images, leading to subpar performance in certain applications. This work proposes a novel approach to improve both efficiency and accuracy in image recognition: discriminative block-diagonal representation learning (DBDRL). DBDRL leverages key insights from linear discriminant analysis (LDA) and sparse coding to learn highly efficient representations that maximize inter-class variation while minimizing intra-class variance. We evaluate our method using several benchmark datasets and compare against state-of-the-art approaches, demonstrating superior performance across all metrics. Our results indicate that DBDRL represents a significant advance in the field of computer vision and highlight the potential of combining classical statistical methods with modern machine learning techniques. Overall, this research paves the way for more effective, efficient, and accurate solutions in image recognition and related fields.",1
"How can we learn a classifier that is ""fair"" for a protected or sensitive group, when we do not know if the input to the classifier belongs to the protected group? How can we train such a classifier when data on the protected group is difficult to attain? In many settings, finding out the sensitive input attribute can be prohibitively expensive even during model training, and sometimes impossible during model serving. For example, in recommender systems, if we want to predict if a user will click on a given recommendation, we often do not know many attributes of the user, e.g., race or age, and many attributes of the content are hard to determine, e.g., the language or topic. Thus, it is not feasible to use a different classifier calibrated based on knowledge of the sensitive attribute.   Here, we use an adversarial training procedure to remove information about the sensitive attribute from the latent representation learned by a neural network. In particular, we study how the choice of data for the adversarial training effects the resulting fairness properties. We find two interesting results: a small amount of data is needed to train these adversarial models, and the data distribution empirically drives the adversary's notion of fairness.",0
"As artificial intelligence continues to become more integrated into society, concerns over fairness and bias in machine learning models have come to the forefront of research and discussion. In response, adversarial training methods have been developed as a means of improving model fairness by explicitly considering protected characteristics such as race, gender, or age during the learning process. However, these approaches often face significant theoretical and practical challenges that must be carefully considered before they can be effectively deployed. This paper aims to address some of these issues through an exploration of data decisions and their implications for fair representation learning. We begin by discussing the problem of unintended biases in machine learning algorithms and how adversarial techniques can potentially mitigate them. Next, we examine key aspects of adversarial representation learning, including algorithmic stability, interpretability, and robustness, and argue that there exists a trade-off among these competing goals. Finally, we propose a framework for evaluating different adversarial representation learning strategies based on their ability to achieve desirable outcomes and offer specific recommendations for future work in this area. Overall, our findings highlight the importance of careful consideration of both empirical evidence and theoretical considerations in developing effective and ethically sound AI systems. By doing so, we hope to contribute towards building a more equitable and inclusive digital world.",1
"Indoor scene understanding is central to applications such as robot navigation and human companion assistance. Over the last years, data-driven deep neural networks have outperformed many traditional approaches thanks to their representation learning capabilities. One of the bottlenecks in training for better representations is the amount of available per-pixel ground truth data that is required for core scene understanding tasks such as semantic segmentation, normal prediction, and object edge detection. To address this problem, a number of works proposed using synthetic data. However, a systematic study of how such synthetic data is generated is missing. In this work, we introduce a large-scale synthetic dataset with 400K physically-based rendered images from 45K realistic 3D indoor scenes. We study the effects of rendering methods and scene lighting on training for three computer vision tasks: surface normal prediction, semantic segmentation, and object boundary detection. This study provides insights into the best practices for training with synthetic data (more realistic rendering is worth it) and shows that pretraining with our new synthetic dataset can improve results beyond the current state of the art on all three tasks.",0
"In order to accurately capture real world indoor scenes using computer graphics technology, physically based rendering (PBR) has become increasingly important. PBR allows for more accurate simulation of light and material interactions within a virtual scene, allowing objects in that scene to appear as they would in the real world under similar conditions. This research seeks to improve upon current methods of PBR by utilizing convolutional neural networks, which can better process image data and allow for even greater accuracy than traditional algorithms alone. By comparing simulated images generated using traditional methods with those produced by a deep learning model trained on real world indoor environments, we demonstrate improved accuracy in both visual fidelity and color constancy. Our findings suggest that convolutional neural networks may be used effectively to enhance existing computer graphics techniques such as physically based rendering for improved indoor scene understanding.",1
"Objective: To transform heterogeneous clinical data from electronic health records into clinically meaningful constructed features using data driven method that rely, in part, on temporal relations among data. Materials and Methods: The clinically meaningful representations of medical concepts and patients are the key for health analytic applications. Most of existing approaches directly construct features mapped to raw data (e.g., ICD or CPT codes), or utilize some ontology mapping such as SNOMED codes. However, none of the existing approaches leverage EHR data directly for learning such concept representation. We propose a new way to represent heterogeneous medical concepts (e.g., diagnoses, medications and procedures) based on co-occurrence patterns in longitudinal electronic health records. The intuition behind the method is to map medical concepts that are co-occuring closely in time to similar concept vectors so that their distance will be small. We also derive a simple method to construct patient vectors from the related medical concept vectors. Results: For qualitative evaluation, we study similar medical concepts across diagnosis, medication and procedure. In quantitative evaluation, our proposed representation significantly improves the predictive modeling performance for onset of heart failure (HF), where classification methods (e.g. logistic regression, neural network, support vector machine and K-nearest neighbors) achieve up to 23% improvement in area under the ROC curve (AUC) using this proposed representation. Conclusion: We proposed an effective method for patient and medical concept representation learning. The resulting representation can map relevant concepts together and also improves predictive modeling performance.",0
"Machine learning algorithms have been widely used to analyze electronic health records (EHRs) for predictive analytics, risk stratification and diagnosis decision support systems. In recent years, deep neural networks (DNNs), particularly convolutional neural networks (CNNs), have achieved state-of-the art performance in image classification tasks due to their ability to learn hierarchical feature representations by automatically identifying salient features directly from data without human intervention. Motivated by these advances, we propose to use CNNs to extract high level medical concept representations from EHRs for disease prediction tasks. We test our approach using data collected from the Physionet database consisting of MIMIC II Waveform Database and demonstrate that incorporating these learned concepts improves the area under the receiver operating characteristic curve (AUC ROC) of several well known machine learning models for heart failure (HF) prediction compared to traditional bag-of-words (BoW) approaches which were the previous state-of-the-art method. Furthermore, our qualitative analysis of the most important clinical concepts identified by our model could provide new insights into HF pathology and inform future research directions. Our work provides evidence towards using deep learning methods for representation learning from unstructured EHR data which may lead to improved accuracy, transparency and generalizability across different patient populations as compared to traditional rule based and statistical methods.",1
"Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment's rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.",0
"One possible abstract is: ""In reinforcement learning (RL), the problem of option discovery involves identifying and extracting high-level actions that can achieve complex goals from low-level sensorimotor interactions. In this paper, we propose a novel approach called Laplace Options, which combines the expressiveness of deep neural networks with the interpretability of graphical models. Our framework learns both continuous action options and discrete state abstractions using a variational inference procedure based on a conditional Gaussian prior over continuous variables and a spatiotemporal Markov random field prior over binary variables. To demonstrate the effectiveness of our method, we apply it to a variety of benchmark control tasks including Acrobot, Reacher, LunarLander, and Hopper. Experimental results show that Laplace Options significantly outperforms previous methods for option discovery, achieving higher success rates while discovering more compact representations.""",1
"We present Deep Generalized Canonical Correlation Analysis (DGCCA) -- a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.",0
"In statistics, generalized canonical correlation analysis (GCCA) extends classical CCA by allowing multiple sets of variables to simultaneously capture correlations among datasets. However, GCCA assumes that both datasets have equal dimensions which may result in limitations if one dataset has more variables than another. To overcome these limitations, deep learning methods can be employed. This paper presents a novel framework called Deep Generalized Canonical Correlation Analysis (DGCCA), capable of handling large scale data as well as imbalanced datasets. Our method leverages recent advances in deep neural networks, which allows us to learn effective mappings from input spaces into a shared latent space for two different matrices, even when they possess different sizes. Extensive experiments on publicly available benchmark datasets demonstrate improved performance compared with other state-of-the-art techniques. Furthermore, our model achieves competitive results against established linear models while retaining scalability to larger problems.",1
"There is general consensus that learning representations is useful for a variety of reasons, e.g. efficient use of labeled data (semi-supervised learning), transfer learning and understanding hidden structure of data. Popular techniques for representation learning include clustering, manifold learning, kernel-learning, autoencoders, Boltzmann machines, etc.   To study the relative merits of these techniques, it's essential to formalize the definition and goals of representation learning, so that they are all become instances of the same definition. This paper introduces such a formal framework that also formalizes the utility of learning the representation. It is related to previous Bayesian notions, but with some new twists. We show the usefulness of our framework by exhibiting simple and natural settings -- linear mixture models and loglinear models, where the power of representation learning can be formally shown. In these examples, representation learning can be performed provably and efficiently under plausible assumptions (despite being NP-hard), and furthermore: (i) it greatly reduces the need for labeled data (semi-supervised learning) and (ii) it allows solving classification tasks when simpler approaches like nearest neighbors require too much data (iii) it is more powerful than manifold learning methods.",0
"Representation learning has emerged as a powerful technique in artificial intelligence and machine learning, allowing algorithms to learn complex representations of data that capture underlying structures and patterns. In recent years, there has been growing interest in understanding the benefits of using representation learning in different domains and applications. This paper presents a comprehensive analysis of the provable benefits of representation learning. We first provide a detailed review of existing literature on representation learning and highlight key challenges and limitations in current approaches. Then, we propose a framework for evaluating the effectiveness of representation learning methods and demonstrate how our approach can lead to more efficient and accurate models. Our experimental results show that by leveraging representation learning techniques, we can significantly improve performance on a range of tasks across diverse datasets. Furthermore, we discuss potential future directions and opportunities for further advancing the field of representation learning through new research questions and methodologies. Overall, this work provides important insights into the value of representation learning and contributes to the broader community interested in developing intelligent systems capable of handling complex real-world problems.",1
"Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide feedback on these intermediate steps. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit specification of sub-goals. The resulting reward functions can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also show that our method can be used to learn a real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task. Supplementary material and data are available at https://sermanet.github.io/rewards",0
"In recent years, imitation learning has emerged as a powerful tool for training agents to perform complex tasks by observing and replicating human demonstrations. However, designing effective reward functions that guide the agent towards successful task completion remains a challenge. Most existing approaches rely on handcrafted rewards or require supervision from expert demonstrators, which can be time-consuming and difficult to scale.  This work proposes a novel approach for training agents using unsupervised perceptual rewards. Our method learns a reward function directly from raw sensor inputs such as images or videos, without any explicit guidance or labels. We use deep reinforcement learning algorithms to maximize these perceptual rewards during policy optimization, enabling the agent to learn policies that effectively imitate human behaviors.  We evaluate our approach across several benchmark domains, including robot manipulation, locomotion, and navigation tasks. Results show that our method outperforms state-of-the-art imitation learning techniques and achieves comparable performance to methods that rely on expert feedback. Furthermore, we demonstrate the robustness of our approach by applying it to real-world environments where acquiring expert annotations would be impractical or expensive.  Overall, our work addresses the gap between high-level task specifications and low-level control policies, providing a scalable and efficient solution for training agents via unsupervised perceptual rewards. By leveraging deep neural networks to learn meaningful objectives directly from sensory data, we pave the way for developing intelligent systems capable of versatile and adaptive behavior.",1
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",0
Infer a suitable title from the content below. This is a paper that presents a new method called DKT (Dynamic Kernel Tuning) which can learn continuous semantic representations of symbolic expressions using deep neural networks.,1
"Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks, asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (1) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems.",0
"Transfer learning has emerged as an effective technique for training deep neural networks using pre-trained models that have been trained on large datasets. This approach reduces the amount of labeled data required and enables more efficient use of resources by allowing reuse of existing knowledge learned from other tasks. In recent years, progressive neural network (PNN) architectures have gained popularity due to their ability to achieve good performance without relying heavily on strong regularization techniques such as dropout and weight decay. PNNs can generate diverse solutions which can then be exploited towards transfer learning objectives. Despite the growing interest in both transfer learning and PNNs, little attention has been paid to combining these two approaches within the same framework. In this paper, we present a novel architecture called ProgressNet which leverages the advantages of PNNs and transfers knowledge obtained through pretraining to improve the accuracy of emotional recognition tasks. Experimental results demonstrate the effectiveness of our proposed methodology compared to state-of-the-art alternatives, achieving better results in several challenging benchmark datasets. Our work opens up new possibilities for developing more robust architectures capable of generalizing well across domains while ensuring efficient computational requirements during deployment.",1
"With a simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings. Additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for a word with multiple meanings can be created based on the word's local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task.",0
"Word Embedding models like word2vec have become very popular and widely used due to their ability to learn vector representations that capture semantic meaning. In recent years, research has focused on creating more advanced methods that can further improve performance and incorporate additional contextual information such as dependencies, hierarchies, and syntax. One such method is the use of ""Context Encoder"" which extends traditional word embeddings by taking into account the context in which each word appears. This allows the model to better capture relationships between words and their meanings. By utilizing contextual information, Context Encoders have been shown to outperform previous state-of-the-art embedding techniques while remaining relatively easy to implement and train. This work presents an overview of Context Encoder algorithms, their strengths and limitations, and compares them against other approaches on several benchmark tasks. We conclude that Context Encoders offer a simple yet effective means to enhance word representation learning. They are particularly promising given their simplicity, scalability, and effectiveness across different domains and applications.",1
"Learning binary representation is essential to large-scale computer vision tasks. Most existing algorithms require a separate quantization constraint to learn effective hashing functions. In this work, we present Direct Binary Embedding (DBE), a simple yet very effective algorithm to learn binary representation in an end-to-end fashion. By appending an ingeniously designed DBE layer to the deep convolutional neural network (DCNN), DBE learns binary code directly from the continuous DBE layer activation without quantization error. By employing the deep residual network (ResNet) as DCNN component, DBE captures rich semantics from images. Furthermore, in the effort of handling multilabel images, we design a joint cross entropy loss that includes both softmax cross entropy and weighted binary cross entropy in consideration of the correlation and independence of labels, respectively. Extensive experiments demonstrate the significant superiority of DBE over state-of-the-art methods on tasks of natural object recognition, image retrieval and image annotation.",0
"This paper presents an approach for learning binary representations directly from raw data using a novel end-to-end trainable neural network architecture called DIRECT (Direct Introducing Real Constraints To Enhance Learnability). Our method leverages recent advances in deep learning architectures and optimization techniques to learn compact binary codes that capture key features of the input data while still remaining easy to work with. We demonstrate the effectiveness of our method on several benchmark datasets across different domains, showing consistent improvement over state-of-the-art baselines. Additionally, we provide a detailed analysis of how our model learns representations by studying the activation patterns and gradients of the learned weights during training. Finally, we showcase the versatility of our approach by applying it to two real-world applications: image retrieval and recommender systems. Overall, our work makes significant progress towards the goal of efficient, high-quality representation learning for large-scale machine learning tasks.",1
"We capitalize on large amounts of readily-available, synchronous data to learn a deep discriminative representations shared across three major natural modalities: vision, sound and language. By leveraging over a year of sound from video and millions of sentences paired with images, we jointly train a deep convolutional network for aligned representation learning. Our experiments suggest that this representation is useful for several tasks, such as cross-modal retrieval or transferring classifiers between modalities. Moreover, although our network is only trained with image+text and image+sound pairs, it can transfer between text and sound as well, a transfer the network never observed during training. Visualizations of our representation reveal many hidden units which automatically emerge to detect concepts, independent of the modality.",0
"This paper presents a novel approach to learning multimodal representations that can capture meaningful relationships across different modalities such as vision and language. Our method uses deep alignment techniques to learn a joint embedding space where both visual and textual features can be represented effectively. To achieve this, we propose a two-step pipeline where we first train separate feature extractors for each modality using self-supervised pretext tasks, and then use these features to align the embeddings from the two domains into a unified representation space. We evaluate our approach on several benchmark datasets and show consistent improvements over baseline methods across multiple task types including image classification, object detection, question answering, and machine translation. Our results demonstrate the effectiveness of our proposed approach for capturing cross-modal correlations and achieving state-of-the-art performance on challenging multimodal tasks.",1
"End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.",0
"In the field of speech recognition, neural network models have become increasingly popular due to their impressive results. However, these models can require large amounts of data and computational resources that many organizations may not have access to. As a result, transfer learning has emerged as a promising approach to improve model performance without requiring vast amounts of expensive compute. This study investigates how well established pre-trained language models perform on low resource languages like Vietnamese compared to fine-tuning on smaller datasets. We find that even without significant tuning pre-trained models outperform traditional methods by a wide margin, despite the domain mismatch between English (the pre-training source) and Vietnamese (low resourced target). Moreover, our fine-grained analysis shows a correlation between pre-training data scale and corresponding improvements over non pre-trained models: larger models trained on more data performs better than smaller ones even if pre-trained on less amount of data. We conclude that our results are both encouraging and informative for those who wish to apply modern machine learning techniques but only possess modest means. They suggest there could still exist gaps between human level proficiency using existing state of art pre-trained LLMs, which may pave the path for future research exploring possible solutions.",1
"Nowadays, distributed smart cameras are deployed for a wide set of tasks in several application scenarios, ranging from object recognition, image retrieval, and forensic applications. Due to limited bandwidth in distributed systems, efficient coding of local visual features has in fact been an active topic of research. In this paper, we propose a novel approach to obtain a compact representation of high-dimensional visual data using sensor fusion techniques. We convert the problem of visual analysis in resource-limited scenarios to a multi-view representation learning, and we show that the key to finding properly compressed representation is to exploit the position of cameras with respect to each other as a norm-based regularization in the particular signal representation of sparse coding. Learning the representation of each camera is viewed as an individual task and a multi-task learning with joint sparsity for all nodes is employed. The proposed representation learning scheme is referred to as the multi-view task-driven learning for visual sensor network (MT-VSN). We demonstrate that MT-VSN outperforms state-of-the-art in various surveillance recognition tasks.",0
"In recent years, there has been growing interest in developing methods for recognizing objects in visual sensor networks (VSN) using multiple views of the same object. This task can be challenging due to occlusions, varying lighting conditions, and other factors that may affect the quality and reliability of the image data.  In this paper, we propose a multi-view task-driven recognition approach that utilizes multiple cameras to capture images from different angles and perspectives. Our method incorporates a deep learning-based framework that leverages advanced features such as convolutional neural networks (CNNs) and attention mechanisms. We also address the problem of occlusion by integrating temporal information into our model through the use of recurrent layers.  We evaluate our proposed method on two public datasets: the Washington RGB dataset and the Oxford RobotCar Dataset. Experimental results show significant improvements over existing single-image based approaches, demonstrating the effectiveness of our approach for VSN tasks. Additionally, we provide ablation studies to analyze the impact of each component in our system.  Overall, this work represents an important step forward in improving the performance of object recognition in VSN environments, paving the way for more reliable and efficient applications in areas such as surveillance, robotics, and autonomous vehicles.",1
"Softmax loss is widely used in deep neural networks for multi-class classification, where each class is represented by a weight vector, a sample is represented as a feature vector, and the feature vector has the largest projection on the weight vector of the correct category when the model correctly classifies a sample. To ensure generalization, weight decay that shrinks the weight norm is often used as regularizer. Different from traditional learning algorithms where features are fixed and only weights are tunable, features are also tunable as representation learning in deep learning. Thus, we propose feature incay to also regularize representation learning, which favors feature vectors with large norm when the samples can be correctly classified. With the feature incay, feature vectors are further pushed away from the origin along the direction of their corresponding weight vectors, which achieves better inter-class separability. In addition, the proposed feature incay encourages intra-class compactness along the directions of weight vectors by increasing the small feature norm faster than the large ones. Empirical results on MNIST, CIFAR10 and CIFAR100 demonstrate feature incay can improve the generalization ability.",0
"Title: ""Representation regularization using feature incay""  This paper presents a novel approach to representation regularization called feature incay (FI). FI introduces two key components that promote learning more meaningful representations: intra-feature correlation control (IFCC) and inter-layer dependency promotion (ILDP). IFCC encourages independence among features by penalizing high correlations within each layer while preserving discriminative power. ILDP fosters dependencies between different layers by promoting connectivity patterns across them. Experiments on image classification benchmarks demonstrate improvements over vanilla models as well as popular baselines, indicating the effectiveness of our proposed methodology. Moreover, we investigate the impact of hyperparameters in both components through ablation studies. This work provides insights into efficient use of regularization techniques for enhanced visual understanding tasks.",1
"In this paper, we present a novel method of no-reference image quality assessment (NR-IQA), which is to predict the perceptual quality score of a given image without using any reference image. The proposed method harnesses three functions (i) the visual attention mechanism, which affects many aspects of visual perception including image quality assessment, however, is overlooked in the NR-IQA literature. The method assumes that the fixation areas on an image contain key information to the process of IQA. (ii) the robust averaging strategy, which is a means \--- supported by psychology studies \--- to integrating multiple/step-wise evidence to make a final perceptual judgment. (iii) the multi-task learning, which is believed to be an effectual means to shape representation learning and could result in a more generalized model.   To exploit the synergy of the three, we consider the NR-IQA as a dynamic perception process, in which the model samples a sequence of ""informative"" areas and aggregates the information to learn a representation for the tasks of jointly predicting the image quality score and the distortion type.   The model learning is implemented by a reinforcement strategy, in which the rewards of both tasks guide the learning of the optimal sampling policy to acquire the ""task-informative"" image regions so that the predictions can be made accurately and efficiently (in terms of the sampling steps). The reinforcement learning is realized by a deep network with the policy gradient method and trained through back-propagation.   In experiments, the model is tested on the TID2008 dataset and it outperforms several state-of-the-art methods. Furthermore, the model is very efficient in the sense that a small number of fixations are used in NR-IQA.",0
"In summary: This approach is based on deep learning models called Generative Adversarial Networks (GAN) which aim at synthesizing natural images from random noise as realistically as possible. GAN have proven capable of producing high resolution results comparable to professional renderings. Our research shows that human visual attention modeling can provide additional discriminability for image quality assessments by helping detect important regions that human observers focus their judgements upon. For instance, if we analyze the images humans would look at longest, we find these show higher visual fidelity. We thus propose training our no reference IQA model using such attention maps derived from other state-of-the art saliency models. The evaluation section uses multiple datasets like CIDEr, MS COCO, MOSI, KONVOLKA64x64 etc. against different methods and compares our performance favorably with current SOTA approaches providing higher correlation with mean opinion scores provided by human subjects. By improving both efficiency and accuracy, this approach has potential for adoption across industries ranging from entertainment, security, medicine, remote sensing etc. where automated visual inspection is desired without relying on manual human labor whenever feasible.",1
"Real-world robotics problems often occur in domains that differ significantly from the robot's prior training environment. For many robotic control tasks, real world experience is expensive to obtain, but data is easy to collect in either an instrumented environment or in simulation. We propose a novel domain adaptation approach for robot perception that adapts visual representations learned on a large easy-to-obtain source dataset (e.g. synthetic images) to a target real-world domain, without requiring expensive manual data annotation of real world data before policy search. Supervised domain adaptation methods minimize cross-domain differences using pairs of aligned images that contain the same object or scene in both the source and target domains, thus learning a domain-invariant representation. However, they require manual alignment of such image pairs. Fully unsupervised adaptation methods rely on minimizing the discrepancy between the feature distributions across domains. We propose a novel, more powerful combination of both distribution and pairwise image alignment, and remove the requirement for expensive annotation by using weakly aligned pairs of images in the source and target domains. Focusing on adapting from simulation to real world data using a PR2 robot, we evaluate our approach on a manipulation task and show that by using weakly paired images, our method compensates for domain shift more effectively than previous techniques, enabling better robot performance in the real world.",0
"Visual servo control is a method by which a robot can modify its actions based on visual feedback from its environment. Traditional methods use dense depth maps to build detailed representations of objects in real time, but these methods require large amounts of compute power and memory resources. In our proposed model, we instead learn deep visuomotor representations using weak pairwise constraints derived from sparse object cues, such as edges or corners. These representations allow the system to adapt quickly to changes in lighting conditions, camera positions, or object poses without sacrificing accuracy. Our results show that our method outperforms traditional models while requiring significantly less computational resources. This work has important implications for applications where robots need to operate in unpredictable environments with minimal supervision.",1
"Mammography screening for early detection of breast lesions currently suffers from high amounts of false positive findings, which result in unnecessary invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many of these false-positive findings prior to biopsy. Current approaches estimate tissue properties by means of quantitative parameters taken from generative, biophysical models fit to the q-space encoded signal under certain assumptions regarding noise and spatial homogeneity. This process is prone to fitting instability and partial information loss due to model simplicity. We reveal unexplored potentials of the signal by integrating all data processing components into a convolutional neural network (CNN) architecture that is designed to propagate clinical target information down to the raw input images. This approach enables simultaneous and target-specific optimization of image normalization, signal exploitation, global representation learning and classification. Using a multicentric data set of 222 patients, we demonstrate that our approach significantly improves clinical decision making with respect to the current state of the art.",0
"An innovative technique has been developed to further analyze breast cancer data by utilizing a previously unexplored aspect of magnetic resonance imaging (MRI) known as the q-space signal. This approach allows us to identify hidden potentials within the data that could potentially lead to improved diagnosis and treatment outcomes. By examining the relationship between the q-space signal and clinical variables such as age, menopausal status, breast density, and BMI, we have identified significant correlations that were previously undiscovered. Our findings suggest that there may be new opportunities for developing more accurate and efficient breast cancer screening methods using MRI technology. Overall, this research demonstrates the importance of continuously exploring novel ways to analyze medical data in order to drive progress towards better healthcare solutions.",1
"Entity images could provide significant visual information for knowledge representation learning. Most conventional methods learn knowledge representations merely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model (IKRL), where knowledge representations are learned with both triple facts and images. More specifically, we first construct representations for all images of an entity with a neural image encoder. These image representations are then integrated into an aggregated image-based representation via an attention-based method. We evaluate our IKRL models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.",0
"Title: ""Image-Embodied Knowledge Representation Learning"" This paper explores how images can be used as representations of knowledge in machine learning models. We propose that by incorporating image features into traditional language processing techniques, we can create more effective and efficient models. Through extensive experiments on multiple tasks, our results show that these methods lead to significant improvements over baseline models. Our work has important implications for areas such as computer vision, natural language understanding, and even creative applications like art generation. By using images to represent knowledge, we open up new possibilities for advancing AI systems.",1
"Multiresolution analysis and matrix factorization are foundational tools in computer vision. In this work, we study the interface between these two distinct topics and obtain techniques to uncover hierarchical block structure in symmetric matrices -- an important aspect in the success of many vision problems. Our new algorithm, the incremental multiresolution matrix factorization, uncovers such structure one feature at a time, and hence scales well to large matrices. We describe how this multiscale analysis goes much farther than what a direct global factorization of the data can identify. We evaluate the efficacy of the resulting factorizations for relative leveraging within regression tasks using medical imaging data. We also use the factorization on representations learned by popular deep networks, providing evidence of their ability to infer semantic relationships even when they are not explicitly trained to do so. We show that this algorithm can be used as an exploratory tool to improve the network architecture, and within numerous other settings in vision.",0
"An incremental algorithm for computing multiresolution matrix factorizations (MRMFs) given by multiple low-rank approximations at different scales based on a fixed number of desired factors is introduced. Different from existing methods, our proposed method iteratively builds each MRMF approximation level-wise and uses warm starts via previously computed lower resolution versions rather than recomputing all previous levels from scratch each time. Extensive experiments demonstrate that the new algorithm is faster and requires less memory than nonincremental alternatives while delivering comparable accuracy for applications involving real datasets such as image compression, video synthesis, and feature extraction tasks. The primary contributions of this paper consist of introducing a novel incremental approach for constructing adaptive MRMFs with improved efficiency, which benefits several important use cases ranging from data analysis and representation learning tasks to scientific visualization problems",1
"The empirical fact that classifiers, trained on given data collections, perform poorly when tested on data acquired in different settings is theoretically explained in domain adaptation through a shift among distributions of the source and target domains. Alleviating the domain shift problem, especially in the challenging setting where no labeled data are available for the target domain, is paramount for having visual recognition systems working in the wild. As the problem stems from a shift among distributions, intuitively one should try to align them. In the literature, this has resulted in a stream of works attempting to align the feature representations learned from the source and target domains. Here we take a different route. Rather than introducing regularization terms aiming to promote the alignment of the two representations, we act at the distribution level through the introduction of \emph{DomaIn Alignment Layers} (\DIAL), able to match the observed source and target data distributions to a reference one. Thorough experiments on three different public benchmarks we confirm the power of our approach.",0
This would need to read well on its own as a summary of your work. Start by clearly stating the main idea you are trying to convey here,1
"End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning system is specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all ""peripheral"" modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex.   In this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of scaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.",0
"This is an abstract for a research paper discussing the limitations of end-to-end learning systems. The paper examines how these types of artificial intelligence models have revolutionized many fields by enabling automatic programming through data and training deep neural networks without human intervention. However, there remain significant challenges that must be addressed before we can fully harness their potential. These issues primarily revolve around the interpretability problem, which arises from the opacity and unpredictability of black box models, as well as scalability concerns related to both computational resources required and sample complexity constraints limiting the range of problems that current architectures are capable of solving reliably. By shedding light on these barriers, our work hopes to stimulate further research in addressing them so that future generations of AI systems may overcome these hurdles. Ultimately, we believe this research has important implications for advancing machine learning beyond benchmark tasks and into practical real-world applications across all domains where automation holds great promise but remains elusive today due to these outstanding challenges. Our analysis is grounded in rigorous empirical experiments conducted over state-of-the-art systems as well as contributions drawn from theory and insights gleaned from practitioners working at the cutting edge of machine learning development and deployment. Together, this synthesis offers new perspectives on the opportunities ahead while highlighting obstacles confronting us along the path towards achieving transformative impacts via intelligent machines.",1
"We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.",0
"Deep learning has recently emerged as one of the most powerful tools for image classification, surpassing traditional handcrafted features such as HoG [6] and SIFT [8]. However, deep neural networks typically require large amounts of labeled data for training, making them difficult to apply to problems where only small datasets exist. We present split-brain autoencoders (SBAE), which address this challenge by performing unsupervised representation learning through cross-channel prediction. In SBAE, two different sets of convolutional layers predict each other’s output channels within a single minibatch, leading to representations that capture meaningful patterns from raw pixel input without explicit supervision. Through experiments on several benchmark datasets, we show that our model achieves competitive results compared to state-of-the-art methods while requiring significantly less data and computational resources during training. Our method demonstrates the effectiveness of self-prediction losses for unsupervised feature extraction, opening up opportunities for new applications in areas limited by data availability. This paper presents split-brain autoencoders (SBAE) for unsupervised representation learning through cross-channel prediction. Traditional deep neural networks often require large amounts of labeled data for training, but SBAEs overcome this limitation by utilizing two separate sets of convolutional layers that predict each others' outputs within a minibatch. Experiments demonstrate that SBAE outperforms other methods under constrained settings with minimal data availability. This research holds great potential for expanding the scope of deep learning into fields currently hindered by insufficient data collections.",1
"The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.",0
"Representation learning has become increasingly important in many fields, including computer vision, where it has been used to improve object detection, image classification, and other tasks. In particular, correlation filter (CF)-based tracking is a popular approach that uses learned representations to predict target motion and appearance changes over time. However, existing methods often rely on handcrafted features, which can limit their performance and make them difficult to adapt to new situations.  This work presents end-to-end representation learning for CF-based tracking, which allows the model to learn more efficient and effective representations directly from raw pixel data. By training a deep neural network to regress target position and scale changes, we show that the resulting features outperform traditional methods in terms of accuracy, robustness, and speed. Our approach can effectively handle complex scenarios such as occlusions, deformations, illumination variations, and background clutter without requiring any manual feature engineering or parameter tuning.  We evaluate our method using challenging benchmark datasets and demonstrate state-of-the-art results under different evaluation metrics. Furthermore, we provide detailed ablation studies and visualizations to analyze the behavior of our model and compare it against alternative architectures and techniques. Finally, we discuss potential applications of our approach beyond tracking and highlight promising directions for future research. Overall, our work advances the field of representation learning by demonstrating the effectiveness of end-to-end trained models in real-world scenarios while maintaining computational efficiency.",1
"Although stochastic approximation learning methods have been widely used in the machine learning literature for over 50 years, formal theoretical analyses of specific machine learning algorithms are less common because stochastic approximation theorems typically possess assumptions which are difficult to communicate and verify. This paper presents a new stochastic approximation theorem for state-dependent noise with easily verifiable assumptions applicable to the analysis and design of important deep learning algorithms including: adaptive learning, contrastive divergence learning, stochastic descent expectation maximization, and active learning.",0
"This paper presents a novel approach for analyzing representation learning algorithms using stochastic descent analysis (SDA). Traditional methods for evaluating the performance of these algorithms rely on batch gradient descent optimization, which can be computationally expensive and may not accurately capture their behavior during training. SDA offers a more efficient alternative by modeling the evolution of representations as random processes that capture the interactions among different layers in deep neural networks. By studying the properties of these processes, we gain insights into how representations change over time and how they impact the final model's accuracy and robustness. Our experimental results demonstrate the effectiveness of SDA as a tool for understanding representation learning algorithms and provide new directions for future research in this field.",1
"A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.",0
"In incremental learning the learner receives sequentially new observations that can be related to previous knowledge, but can differ from them drastically in their nature (like moving from static images to videos). This work presents the first algorithmic solution called iCaRL (Incremental Classifier and Representation Learning) that addresses both problems together using deep neural networks as classifiers. We demonstrate experimentally on several public datasets including MNIST, CIFAR-10/100, and ImageNet-Subset that our approach achieves better error rates compared to standard fine-tuning and online learning approaches across all three tasks while reducing computational requirements.  To address catastrophic forgetting we propose two different methods to retrain only parts of previously learned models: selective archiving which stores specific neurons of each layer at the end of training before unfreezing, and gradient thresholding during backpropagation through time that prevents large changes in weights of well performing neurons. To reduce computational cost we introduce incremental batch renormalization that propagates running averages of mean and variance estimates from the main model backwards to the current snapshot. Our experiments show that using these techniques iCaRL outperforms previous state-of-the-art incremental learning algorithms significantly by up to 48% on ImageNet-Subset. Moreover, iCaRL matches or surpasses performance of non-incremental models trained on complete data despite limited access to new incoming observations. Finally, we investigate the behavior of iCaRL under more challenging settings such as varying task orders and smaller batch sizes finding that it still remains competitive.",1
"Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.",0
"In this paper we present deep neural networks as models capable of solving high dimensional, nonlinear pattern recognition problems. We show that these systems can learn representations from raw data that make them suitable for a wide range of tasks such as object detection, human action classification, pose estimation etc., achieving state-of-the art results on standard benchmark datasets. Our experiments demonstrate the importance of choosing appropriate architectures, and designing careful loss functions. They highlight the trade-offs involved in picking model size, training time etc. Our methodology could enable new applications across many areas including robotics, video analysis, gaming, self driving cars, personalized medicine, education etc. This work opens up exciting opportunities but raises important questions about privacy, security, safety, law which need urgent attention.",1
"Designed as extremely deep architectures, deep residual networks which provide a rich visual representation and offer robust convergence behaviors have recently achieved exceptional performance in numerous computer vision problems. Being directly applied to a scene labeling problem, however, they were limited to capture long-range contextual dependence, which is a critical aspect. To address this issue, we propose a novel approach, Contextual Recurrent Residual Networks (CRRN) which is able to simultaneously handle rich visual representation learning and long-range context modeling within a fully end-to-end deep network. Furthermore, our proposed end-to-end CRRN is completely trained from scratch, without using any pre-trained models in contrast to most existing methods usually fine-tuned from the state-of-the-art pre-trained models, e.g. VGG-16, ResNet, etc. The experiments are conducted on four challenging scene labeling datasets, i.e. SiftFlow, CamVid, Stanford background and SUN datasets, and compared against various state-of-the-art scene labeling methods.",0
"Title: Deep Contextual Recurrent Residual Networks for Scene Labeling Abstract Recent advances in deep learning have led to significant improvements in computer vision tasks such as scene labeling. While Convolutional Neural Networks (CNN) architectures have shown great success in these tasks, they suffer from limited context modeling capabilities which can lead to inferior performance compared to models that incorporate recurrence or memory mechanisms. We propose a novel architecture called Deep Contextual Recurrent Residual Networks (DCARN), which combines residual connections with recurrent connections to improve context modeling capacity while maintaining efficient computation. Our approach introduces gating mechanisms at multiple levels within each block of the network, enabling selective propagation of gradients through nested blocks of convolutional layers, allowing for more control over the flow of information throughout the network. Experiments on popular benchmark datasets show that our DCARN outperforms state-of-the-art methods by achieving higher accuracy with fewer parameters, demonstrating the effectiveness of our proposed method.",1
"We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos. Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions. To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality. We use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows. We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations. We demonstrate the effectiveness of our learned temporal representations on activity classification across multiple modalities and datasets such as NTU RGB+D and MSR Daily Activity 3D. Our framework is generic to any input modality, i.e., RGB, Depth, and RGB-D videos.",0
"In recent years, there has been significant interest in using deep learning techniques to model complex video dynamics. One key challenge in doing so is accounting for long-term motion patterns that can span multiple time scales. For example, humans typically exhibit cyclical movements over the course of minutes and hours, while objects like cars may move more gradually over the course of seconds and minutes. To address these types of long-term motions, we present an unsupervised method that models both short- and long-term video dynamics simultaneously. We demonstrate through experiments on two challenging datasets (one from YouTube videos, one synthetic) that our approach effectively learns meaningful representations and outperforms competing methods on downstream action recognition tasks. Our work shows promise for enabling the study of longer term human behavior, as well as improved automation and monitoring applications.",1
"In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications. When the available modalities consist of time series data such as video, audio and sensor signals, it becomes imperative to consider their temporal structure during the fusion process. In this paper, we propose the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model for fusing multiple input modalities that are inherently temporal in nature. Key features of our proposed model include: (i) simultaneous learning of the joint representation and temporal dependencies between modalities, (ii) use of multiple loss terms in the objective function, including a maximum correlation loss term to enhance learning of cross-modal information, and (iii) the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation. We validate our model via experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition. We empirically analyze the contributions of different components of the proposed CorrRNN model, and demonstrate its robustness, effectiveness and state-of-the-art performance on multiple datasets.",0
"This paper presents a novel framework for deep multimodal representation learning from temporal data, which enables representations to capture complex relationships across different modalities over time. Our approach leverages advances in neural network architectures and optimization techniques, allowing us to model the full spatiotemporal dynamics underlying visual, auditory, and other sensor input streams.  We evaluate our method on several challenging benchmark datasets, showing that it outperforms state-of-the-art baselines by significant margins. In addition, we demonstrate that our learned representations generalize well to new tasks, enabling robust and efficient zero-shot transfer learning under realistic conditions. Furthermore, through rigorous ablation studies and sensitivity analyses, we gain insights into the design choices and tradeoffs involved in creating effective multimodal models, highlighting promising directions for future research.  Overall, our work represents a major step forward towards building intelligent systems capable of processing diverse forms of human communication with high fidelity, paving the way for exciting applications in areas such as virtual reality, social robotics, and human-computer interaction.",1
"This workshop explores the interface between cognitive neuroscience and recent advances in AI fields that aim to reproduce human performance such as natural language processing and computer vision, and specifically deep learning approaches to such problems.   When studying the cognitive capabilities of the brain, scientists follow a system identification approach in which they present different stimuli to the subjects and try to model the response that different brain areas have of that stimulus. The goal is to understand the brain by trying to find the function that expresses the activity of brain areas in terms of different properties of the stimulus. Experimental stimuli are becoming increasingly complex with more and more people being interested in studying real life phenomena such as the perception of natural images or natural sentences. There is therefore a need for a rich and adequate vector representation of the properties of the stimulus, that we can obtain using advances in machine learning.   In parallel, new ML approaches, many of which in deep learning, are inspired to a certain extent by human behavior or biological principles. Neural networks for example were originally inspired by biological neurons. More recently, processes such as attention are being used which have are inspired by human behavior. However, the large bulk of these methods are independent of findings about brain function, and it is unclear whether it is at all beneficial for machine learning to try to emulate brain function in order to achieve the same tasks that the brain achieves.",0
"Title: ""Representation learning in artificial neural networks""  Artificial neural networks have gained significant attention due to their ability to learn complex representations from large amounts of data. This has led to impressive advances across many domains, including computer vision, natural language processing, and speech recognition. However, our understanding of how these models learn representations remains limited. In particular, there is still no general consensus on how to design architectures that promote good representation learning, or which specific network features lead to effective learning. This workshop aimed to address these questions by bringing together researchers from both the machine learning and neuroscience communities to discuss recent advances in the field.",1
"We present a principled approach to uncover the structure of visual data by solving a novel deep learning task coined visual permutation learning. The goal of this task is to find the permutation that recovers the structure of data from shuffled versions of it. In the case of natural images, this task boils down to recovering the original image from patches shuffled by an unknown permutation matrix. Unfortunately, permutation matrices are discrete, thereby posing difficulties for gradient-based methods. To this end, we resort to a continuous approximation of these matrices using doubly-stochastic matrices which we generate from standard CNN predictions using Sinkhorn iterations. Unrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet, an end-to-end CNN model for this task. The utility of DeepPermNet is demonstrated on two challenging computer vision problems, namely, (i) relative attributes learning and (ii) self-supervised representation learning. Our results show state-of-the-art performance on the Public Figures and OSR benchmarks for (i) and on the classification and segmentation tasks on the PASCAL VOC dataset for (ii).",0
"DeepPermNet: Visual Permutation Learning focuses on developing novel deep neural networks architectures that can learn from visual permutations. By utilizing permutation learning techniques combined with Convolutional Neural Networks (CNN), we aim to improve image classification accuracy while reducing computational complexity compared to existing state-of-the-art methods. Our methodology leverages concepts such as attention mechanisms and group equivariance to enhance feature extraction capabilities within CNN models. Experimental results demonstrate significant improvements across several benchmark datasets including CIFAR-10 and ImageNet, validating our approach’s effectiveness in achieving high performance in computer vision tasks. Overall, the work presented in DeepPermNet contributes new insights into the role of permutation learning in enhancing CNN performance and advancing artificial intelligence research.",1
Triplet networks are widely used models that are characterized by good performance in classification and retrieval tasks. In this work we propose to train a triplet network by putting it as the discriminator in Generative Adversarial Nets (GANs). We make use of the good capability of representation learning of the discriminator to increase the predictive quality of the model. We evaluated our approach on Cifar10 and MNIST datasets and observed significant improvement on the classification performance using the simple k-nn method.,0
"Title: Improving Object Detection by Generative Adversarial Networks (GAN)  Training triplets networks have been shown to improve object detection through data augmentation but may lead to overfitting due to their reliance on a fixed set of anchor boxes. We propose using generative adversarial networks (GAN) as an alternative method of data augmentation that can prevent overfitting while improving detection accuracy. Our approach generates synthetic images from real annotated objects in order to create new training examples based on existing annotations without relying on fixed anchors. Experiments conducted on two public datasets show that our method outperforms traditional triplet network training in terms of both recall and precision metrics. Furthermore, we demonstrate that our generated images remain plausible even under scrutiny of human evaluators, indicating that they capture important features of real world objects. In conclusion, our work suggests that using GANs for data augmentation has significant potential to improve object detection models, particularly in domains where annotating large amounts of data is expensive or time consuming.",1
"We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.",0
"This paper presents a novel approach to generating reviews and discovering sentiment using deep learning techniques. We use large amounts of data from online review platforms such as Yelp and Amazon to train our model on the characteristics of positive vs negative reviews. Our model then uses this knowledge to generate new reviews that capture the sentiment expressed by existing ones while maintaining grammatical coherence and fluency. We evaluate our method through several experiments and show that it outperforms previous state-of-the-art models. Furthermore, we demonstrate the effectiveness of our method in real-world applications such as detecting product fraud, recommending items based on customer feedback, and summarizing large collections of textual data. Overall, our work has significant implications for improving user experiences across various industries.",1
"We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called ""odd-one-out learning"". In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream convolutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition.   On action classification, our method obtains 60.3\% on the UCF101 dataset using only UCF101 data for training which is approximately 10% better than current state-of-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-of-the art methods by 12.7% on action classification task.",0
"In recent years, self-supervised learning has emerged as a powerful technique for training artificial intelligence models on large amounts of data without relying on explicit labels or annotations. One particularly promising approach to self-supervised learning is based on the odd-one-out task, which involves identifying the one item in a group that differs from all other items in some aspect (such as shape, color, texture, etc.). This type of representation learning has been successfully applied to still images and natural language text, but less work has been done using video data.  This paper presents a novel method for self-supervised video representation learning using odd-one-out networks. We first describe how we generate large quantities of automatically generated video frames along with their corresponding odd-one-outs, which serve as the target outputs for our machine learning algorithm. We then introduce our deep neural network architecture, which processes each frame of video and generates a compact vector representation that captures important features such as object shape, motion, texture, and contextual relationships between objects. Our model learns these representations by minimizing the difference between the predictions made for each frame and its corresponding odd-one-out output.  We evaluate our method using several benchmark datasets commonly used in computer vision research, including UCF-101, HMDB-51, and Something-Something V2. On all three datasets, our method outperforms state-of-the-art alternatives both quantitatively (using standard evaluation metrics) and qualitatively (through human judgment studies). Furthermore, we demonstrate the generalizability of our learned representations by fine-tuning them on specific downstream tasks and achieving strong results on those tasks as well. Finally, we conclude by discussing potential future directions for improving video representation learning through self-supervision.",1
"Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: -Data insufficiency:Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. -Interpretation:The representations learned by deep learning methods should align with medical knowledge. To address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.",0
"Title: ""Graph-Based Attention Models for Healthcare Representation Learning""  Abstract: This study proposes a novel graph-based attention model (GRAM) for healthcare representation learning, which leverages the power of graphs to capture complex relationships among entities within medical data. In traditional natural language processing models, attention mechanisms are applied at each layer independently, which can limit their ability to capture global dependencies in textual data. However, by using graph structures, GRAM captures these relationships more effectively through message passing operations that weigh the importance of different connections based on context. Additionally, GRAM uses meta paths, predefined subgraph patterns in RDF datasets, as shortcuts to encode long-range relations between entities. Our approach was validated using two benchmark datasets, outperforming baseline methods in both settings. Furthermore, the learned embeddings achieved state-of-the-art performance in downstream tasks such as entity linking and relation extraction. These results demonstrate the effectiveness of our graph-based attention model in healthcare representation learning.",1
GPU activity prediction is an important and complex problem. This is due to the high level of contention among thousands of parallel threads. This problem was mostly addressed using heuristics. We propose a representation learning approach to address this problem. We model any performance metric as a temporal function of the executed instructions with the intuition that the flow of instructions can be identified as distinct activities of the code. Our experiments show high accuracy and non-trivial predictive power of representation learning on a benchmark.,0
"This paper presents a method for predicting future activities on computer systems utilizing graphical processing units (GPUs) by leveraging representation learning techniques. We aim to accurately forecast the performance demands placed on these devices to optimize resource allocation and improve system efficiency. To achieve this goal, we gather data from running applications and train machine learning models that extract meaningful features representing the behavior of these programs. By analyzing the activity patterns of the GPU, our approach can provide accurate predictions of future workload, enabling proactive management decisions that enhance overall system performance. Our experiments demonstrate the effectiveness of our model compared to existing methods, showcasing its potential as a valuable tool for managers and administrators operating GPU-based systems.",1
"Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.",0
"In recent years, there has been significant interest in developing visual-semantic embeddings, which aim to represent images and their corresponding textual descriptions as compact, vectorial representations that capture their semantic relationships. However, these models can suffer from instability during training due to the sensitivity of deep neural networks to small perturbations in input data. To address this issue, we propose a novel framework called Learning Robust Visual-Semantic Embeddings (LRVSE). Our method leverages adversarial training and regularization techniques to learn robust visual-semantic embeddings that generalize well across different domains and noise conditions. We evaluate our approach on two benchmark datasets, showing that LRVSE significantly outperforms state-of-the-art methods in terms of accuracy and robustness to noise. This work paves the way for more reliable and efficient applications of visual-semantic embeddings in various computer vision tasks such as image retrieval, zero-shot learning, and question answering.",1
"A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon's mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.",0
This paper presents an information-theoretic framework for fast and robust unsupervised learning through neural population infomax. We develop a novel variational inference method that leverages population dynamics to solve the challenges posed by high-dimensional data distributions. Our approach maximizes informational diversity across multiple scales while minimizing memory usage and computational overhead. Empirical results show significant improvements over state-of-the-art methods on benchmark datasets in terms of both speed and accuracy. This work opens up new possibilities for designing efficient architectures for deep learning models and enables real-time applications such as anomaly detection in large-scale video surveillance systems.,1
"Reinforcement learning optimizes policies for expected cumulative reward. Need the supervision be so narrow? Reward is delayed and sparse for many tasks, making it a difficult and impoverished signal for end-to-end optimization. To augment reward, we consider a range of self-supervised tasks that incorporate states, actions, and successors to provide auxiliary losses. These losses offer ubiquitous and instantaneous supervision for representation learning even in the absence of reward. While current results show that learning from reward alone is feasible, pure reinforcement learning methods are constrained by computational and data efficiency issues that can be remedied by auxiliary losses. Self-supervised pre-training and joint optimization improve the data efficiency and policy returns of end-to-end reinforcement learning.",0
"In most reinforcement learning (RL) agents, a reward function explicitly defines success by assigning positive rewards for desirable states and actions. However, manually defining these rewards can be difficult and time-consuming, limiting our ability to scale up RL methods. By contrast, self-supervised learning has emerged as an alternative approach to training machine learning models without explicit supervision from humans; instead, models learn through pretext tasks that provide a self-defined intrinsic motivation. We propose using self-supervised loss functions for RL, where the agent learns a loss objective that drives exploration and improves performance automatically, without requiring any additional labels beyond the environment dynamics themselves. This approach aligns well with how human players implicitly develop their understanding of good outcomes and effective strategies within games. Our experiments on OpenAI Gym benchmark domains show significant improvements over state-of-the-art model-free RL algorithms. While we cannot guarantee human-level play, our work provides evidence that unsupervised approaches have real potential in solving highdimensional RL problems.",1
"The goal of unsupervised representation learning is to extract a new representation of data, such that solving many different tasks becomes easier. Existing methods typically focus on vectorized data and offer little support for relational data, which additionally describe relationships among instances. In this work we introduce an approach for relational unsupervised representation learning. Viewing a relational dataset as a hypergraph, new features are obtained by clustering vertices and hyperedges. To find a representation suited for many relational learning tasks, a wide range of similarities between relational objects is considered, e.g. feature and structural similarities. We experimentally evaluate the proposed approach and show that models learned on such latent representations perform better, have lower complexity, and outperform the existing approaches on classification tasks.",0
"In this paper we present Clustering-Based Relational Unsupervised Representation Learning (CRURL), an unsupervised learning algorithm that learns an explicit distributed representation of data by exploiting clustering structure and relationships among examples. CRURL leverages advances in efficient large-scale optimization and implicit regularization techniques from recent research in deep learning. We show on several benchmark datasets that CRURL produces state-of-the-art unsupervised representations while offering several advantages over prior work: it can learn meaningful representations without requiring latent semantic analysis as a preprocessing step; its clusters exhibit interpretable grouping structures; and its learned representations generalize well to downstream tasks. Notably, our method makes few assumptions beyond the availability of input data and computation resources--unlike previous relational methods which rely heavily on handengineered features or specific architectures. These results suggest that combining insights from machine learning and computer vision may enable a new generation of algorithms capable of discovering and making use of complex relational structures within big datasets. This could have profound impacts across many applications such as personalized medicine, autonomous vehicles, recommendation systems, and education technology, where extracting knowledge from vast amounts of noisy, high-dimensional data remains challenging despite substantial progress made in supervised learning during the past decade. Overall, our contributions here both push forward technical frontiers in unsupervised learning and motivate continued interdisciplinary collaboration towards solving some of society's most pressing problems involving massive collections of data. Further details appear in the main text, supplementary material, and source code available online at [restricted access].",1
"Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems. Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning, and due to the utility of good representations for transfer learning, representation learning has become as an important and distinct task from supervised learning. At present, this distinction is inconsequential, as supervised methods are state-of-the-art in learning transferable representations. But recent work has shown that generative models can also be powerful agents of representation learning. Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors? In this work, we argue in the affirmative, that from an information theoretic perspective, generative models have greater potential for representation learning. Based on several experimentally validated assumptions, we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models, such as Generative Adversarial Networks (GANs) are not. We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning.",0
"This paper explores the limitations of learning representations using label-based supervision. In recent years, there has been significant progress in developing deep neural networks that can learn representations without explicit guidance from humans. However, these methods still rely heavily on large amounts of labeled data, which can be expensive and time-consuming to obtain. Therefore, researchers have proposed alternative approaches that use less supervision, such as self-supervised learning and transfer learning. These methods often involve training models on multiple tasks simultaneously, allowing them to generalize better across different domains. Despite their promise, however, it remains unclear how well these models perform compared to more fully-supervised approaches. This paper seeks to address this gap by investigating the limits of learning representations under various levels of supervision, including full supervision, weakly-supervised learning, semi-supervised learning, and unsupervised learning. Our findings suggest that while minimizing supervision may improve robustness to domain shifts, it comes at the cost of worse accuracy overall. We provide insights into why certain architectures excel at learning representations with little supervision, and highlight open challenges for future work.",1
"Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only ""autoencodes"" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution $p(z)$ and decoding distribution $p(x|z)$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.",0
"Variational autoencoders (VAEs) have become a popular model for unsupervised learning due to their ability to learn deep generative models of complex data distributions. However, these models can suffer from high computational costs and difficulty in balancing reconstruction error and latent code regularization terms, leading to suboptimal results. To address these issues, we propose a novel variant called Variational Lossy Autoencoder (VLAE). VLAE modifies the standard VAE architecture by introducing a lossy compression step after the encoder network, which reduces the dimensionality of the bottleneck representation before passing it through the decoder network. This modification allows us to simplify the optimization problem without sacrificing performance, making VLAE computationally more efficient while providing improved Latent Space Learning capabilities compared to traditional VAEs. Our experiments on benchmark datasets demonstrate that VLAE outperforms competing methods, both quantitatively and qualitatively, demonstrating the effectiveness of our approach. Overall, VLAE represents an important contribution towards enhancing the performance and efficiency of variational autoencoder models.",1
"We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static. We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchi- cal predictor. Our approach optimizes an objec- tive function which favors balanced and easily- separable multi-way node partitions. We theoret- ically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We empirically validate both variants of the al- gorithm on text classification and language mod- eling, respectively, and show that they compare favorably to common baselines in terms of accu- racy and running time.",0
In this paper we tackle two important problems for machine learning: classification and density estimation. For these tasks we propose a novel framework that learns both trees and representations at once. We show on several benchmark datasets that our method outperforms previous state-of-the-art approaches.,1
"Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However, they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformations. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets.",0
"Introduction: The ability to learn meaningful representations from complex data sets has become increasingly important in fields such as computer vision, natural language processing, and robotics. One approach to learning these representations is through graph-based methods, which use the structure of the data set to identify patterns and relationships that can be used to represent the underlying information. However, traditional graph-based methods often struggle to capture isometric transformations, such as scaling and rotations, which can significantly affect the appearance and interpretation of the data.  Problem statement: In this paper, we propose a novel method for learning graph-based representations that are invariant to isometric transformations. Our method uses a combination of graph convolutional networks (GCNs) and autoencoders to learn a low-dimensional representation of the data that preserves the structural relationships captured by the original graph while being robust to changes in scale and rotation. We evaluate our method on several benchmark datasets and demonstrate its effectiveness at capturing meaningful features even under significant isometric distortions.  Methodology: Our proposed method consists of three main components: an encoder network, a decoder network, and a regularization term based on GCNs. The encoder network maps the input graph onto a lower-dimensional latent space using GCNs, while the decoder network maps samples from the latent space back onto graphs. Both networks use multi-layer perceptrons (MLPs) as their building blocks. The regularization term ensures that the learned representation is equivariant to a specific group of isometries, making it more resilient to changes in scale and orientation. We train our model end-to-end using an unsupervised loss function that minimizes the difference between reconstructed and original graphs.  Results: We evaluate our method on four different benchmark datasets – MNIST, CIFAR-10, NCI1, and Protein. On all datasets, our method outperforms state-of-the-art baseline models and demonstrates i",1
"While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (""PredNet"") architecture that is inspired by the concept of ""predictive coding"" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",0
This should summarize your main results and their implications.,1
"Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.",0
"In recent years deep learning algorithms have achieved state-of-the-art performance on many challenging tasks such as image classification. However, these models remain vulnerable to so called adversarial examples which are inputs designed specifically to cause errors. Given that real world systems deployed by industry might face malicious attacks from adversaries that can craft such inputs, robustness of classifiers becomes essential. This manuscript provides a novel theoretical framework capable of characterizing robustness to adversarial examples in deep neural networks with ReLU activation functions. We analyze robustness along three orthogonal axes: adversary strength, distance based vs certified defenses, and local vs global interpretations of robustness. Based on these components we introduce a set of metrics capturing different aspects of adversarial behavior of the model under analysis while highlighting key tradeoffs involved in designing more resilient architectures. Empirically validating our approach across popular benchmark datasets we showcase substantial gaps between existing methods for improving robustness leaving room for future improvement towards securing classifier predictions. Additionally we investigate the limitations of current heuristics providing insights into how human intuition could fall short in explaining the behavior exhibited by high performing models.",1
"Visual relations, such as ""person ride bike"" and ""bike next to car"", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate $\approx$ object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-to-end relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language priors.",0
"In recent years, there has been increasing interest in developing methods that can accurately detect visual relations between objects in images. One approach to tackling this problem is through the use of embedding networks, which map image features into a high-dimensional space where they can be compared and contrasted with other object representations. In this work, we present a novel method called Visual Translation Embedding Network (VTEN) for visual relation detection. VTEN combines translation operations with an attention mechanism to generate a set of object translations that capture different spatial arrangements between objects. These translated embeddings are then used to train a model that predicts whether two objects are related by one of several predefined visual relationships such as ""on"", ""in"" or ""near"". Experimental results on benchmark datasets demonstrate that our proposed method outperforms state-of-the-art approaches, providing strong evidence of its effectiveness at detecting complex visual relationships. Overall, VTEN represents an important step towards improving our understanding of how computers can be trained to interpret and reason about visual content.",1
"In application domains such as healthcare, we want accurate predictive models that are also causally interpretable. In pursuit of such models, we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties. In a large-scale analysis of Electronic Health Records (EHR), our causally-regularized model outperforms its L1-regularized counterpart in causal accuracy and is competitive in predictive performance. We perform non-linear causality analysis by causally regularizing a special neural network architecture. We also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20% improvement over multilayer perceptron in detecting multivariate causation, a situation common in healthcare, where many causal factors should occur simultaneously to have an effect on the target variable.",0
"Abstract:  The issue of overfitting has been a persistent challenge in machine learning research. One approach to addressing this problem is through regularization techniques, which aim to prevent models from becoming excessively complex by adding constraints during training. Causality is a key concept that can provide valuable insights into how different features contribute to predictions made by a model. This paper proposes causal regularization as a novel technique to improve generalizability of neural networks by incorporating prior knowledge of causal relationships among input variables into the regularization process. By minimizing the impact of non-causal features on model outputs while preserving their predictive power, we demonstrate improved performance across multiple datasets compared to existing regularizers such as L1/L2 norms, dropout, and weight decay. Our findings highlight the potential benefits of leveraging causal reasoning to enhance model interpretability and robustness. We conclude by discussing future directions for refining and extending this framework to accommodate more complex domains and use cases.",1
"In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the neuronal-birth is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.",0
"In today's fast-paced world, online data changes rapidly due to many reasons such as technological advancements, social dynamics, new user behavior patterns, etc. As a result, learning models must adapt quickly to these changes to maintain their relevance and accuracy. This paper proposes ""Neurogenesis-Inspired Dictionary Learning,"" a methodology that addresses the challenge of online model adaptation by imitating the brain's ability to learn continuously through neurogenesis - the process of generating new neurons. Our approach uses dictionary learning to effectively map high-dimensional data into low dimensions without losing essential information while evolving the dictionaries over time according to changes in the underlying data distribution. We showcase promising results on real-world datasets across different domains such as image classification and natural language processing tasks and demonstrate our approach outperforms state-of-the-art baselines under evolving scenarios. By utilizing the proposed technique, machines can better keep up with the pace of change in the digital environment, which has significant implications for artificial intelligence applications in numerous fields.",1
"Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data.",0
"This paper focuses on dataset augmentation techniques for machine learning models in feature space. Data augmentation can improve model performance by increasing the size and diversity of training data. In this work, we investigate different strategies for generating new samples in feature space that preserve the distribution of original data while introducing variations. We propose several methods based on mathematical transformations, random projections, generative models, and domain knowledge. Our evaluation shows that these methods lead to significant improvements over baseline approaches across multiple datasets and task settings. Furthermore, we demonstrate how our approach can generalize well across different architectures and domains, making it a versatile tool for practitioners. Overall, our work highlights the importance of exploring feature space as a means of data augmentation and provides insights into designing effective augmentation schemes.",1
"Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.",0
"This paper proposes a novel approach to deep multi-task representation learning using tensor factorization techniques. We introduce two new methods that aim to learn representations in neural networks that are well suited for multiple tasks simultaneously. Our first method involves performing joint factorized training of the shared layers across all tasks, while our second method uses task-specific attention mechanisms to allow each task to selectively attend to different parts of the shared latent space. Both approaches were evaluated on several benchmark datasets and showed promising results compared to baseline models trained separately on each individual task. Additionally, we performed ablation studies to demonstrate the importance of both components of our proposed model. Overall, our work demonstrates that tensor factorization can effectively improve performance in the challenging setting of multi-task learning.",1
"The cross-entropy loss commonly used in deep learning is closely related to the defining properties of optimal representations, but does not enforce some of the key properties. We show that this can be solved by adding a regularization term, which is in turn related to injecting multiplicative noise in the activations of a Deep Neural Network, a special case of which is the common practice of dropout. We show that our regularized loss function can be efficiently minimized using Information Dropout, a generalization of dropout rooted in information theoretic principles that automatically adapts to the data and can better exploit architectures of limited capacity. When the task is the reconstruction of the input, we show that our loss function yields a Variational Autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Finally, we prove that we can promote the creation of disentangled representations simply by enforcing a factorized prior, a fact that has been observed empirically in recent work. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.",0
"This research investigates how computational models can learn optimal representations by incorporating noisy computation into their training process. By simulating real-world scenarios where incomplete or corrupted data is present, we demonstrate that such a technique leads to more robust and generalizable representations. Our approach introduces ""information dropout"" during model training, which randomly deletes portions of input data or conceals them with noise. Contrary to traditional methods relying on regularization techniques, our method is shown to achieve better results while maintaining computational efficiency. We evaluate the performance of our algorithm across multiple tasks and datasets, including image classification, language processing, and reinforcement learning, demonstrating consistent improvement over baseline models. Our findings suggest that explicit handling of noisy computations during training allows machine learning algorithms to develop resilient representations adaptive to uncertain environments.",1
"In this paper we introduce the deep kernelized autoencoder, a neural network model that allows an explicit approximation of (i) the mapping from an input space to an arbitrary, user-specified kernel space and (ii) the back-projection from such a kernel space to input space. The proposed method is based on traditional autoencoders and is trained through a new unsupervised loss function. During training, we optimize both the reconstruction accuracy of input samples and the alignment between a kernel matrix given as prior and the inner products of the hidden representations computed by the autoencoder. Kernel alignment provides control over the hidden representation learned by the autoencoder. Experiments have been performed to evaluate both reconstruction and kernel alignment performance. Additionally, we applied our method to emulate kPCA on a denoising task obtaining promising results.",0
"In order to meet these requirements, I have provided you with this example: This research explores the application of deep kernelized autoencoders (DKAs) as a novel methodology for image representation learning. DKAs use convolutional neural networks (CNNs) for encoding and decoding data while incorporating traditional nonlinear kernels such as Gaussian processes into their architecture. By introducing kernels, DKAs improve upon standard variational autoencoders by enabling them to capture complex dependencies between input variables more efficiently. To evaluate our approach, we test DKAs on several benchmark datasets commonly used for image classification tasks and compare their performance against other methods, including state-of-the-art deep learning techniques. Our results show that DKAs outperform competing models across all experiments due to their ability to handle high-dimensional spaces with fewer parameters and achieve better generalization capabilities even with smaller training sizes. These findings support the effectiveness of using DKAs for unsupervised feature extraction in computer vision applications. With this work, we aim to provide both theoretical insights and practical guidance for future studies focused on improving automated image analysis systems.",1
"Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding learned using triplet probability constraints to solve the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs comparably or better than the state of the art methods in verification and identification metrics, while requiring much less training data and training time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to extreme pose variation. Furthermore, we demonstrate the robustness of the deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets.",0
"Include only one reference to any previous work done by you or another author in the field of face recognition using deep learning. The performance of face verification and clustering techniques has been greatly improved due to the introduction of triplet probabilistic embedding (TPE). This method leverages recent advancements in generative adversarial networks (GANs) and achieves state-of-the-art results on several benchmark datasets. By training on large scale databases such as YFCC100M+ImageNet, TPE generates high quality embeddings that can be used to accurately identify and group faces with high confidence levels. Additionally, the use of GANs allows us to sample from these distributions, further improving clustering accuracy. We believe our approach holds great promise for future research into more advanced applications, such as facial expression analysis and age estimation. Finally, we note that while there have been many attempts at developing new deep learning architectures for face recognition, none have yet matched the performance of TPE.",1
"Limited annotated data available for the recognition of facial expression and action units embarrasses the training of deep networks, which can learn disentangled invariant features. However, a linear model with just several parameters normally is not demanding in terms of training data. In this paper, we propose an elegant linear model to untangle confounding factors in challenging realistic multichannel signals such as 2D face videos. The simple yet powerful model does not rely on huge training data and is natural for recognizing facial actions without explicitly disentangling the identity. Base on well-understood intuitive linear models such as Sparse Representation based Classification (SRC), previous attempts require a prepossessing of explicit decoupling which is practically inexact. Instead, we exploit the low-rank property across frames to subtract the underlying neutral faces which are modeled jointly with sparse representation on the action components with group sparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot automatic method on raw face videos performs as competitive as SRC applied on manually prepared action components and performs even better than SRC in terms of true positive rate. We apply the model to the even more challenging task of facial action unit recognition, verified on the MPI Face Video Database (MPI-VDB) achieving a decent performance. All the programs and data have been made publicly available.",0
"This paper proposes a method called Linear Disentangled Representation (LDR) learning that can effectively represent facial actions as linear combinations of underlying factors. LDR achieves this by minimizing reconstruction error subject to disentanglement constraints via gradient ascent on the learned representation parameters. Experimental results show that LDR outperforms baseline methods in terms of recognition accuracy while requiring fewer model parameters. LDR also enables interpretability and visualization of latent representations through Pareto fronts of the disentanglement constraint. Overall, this work introduces a novel framework for representation learning of facial actions and contributes to the broader field of video understanding.",1
"In this paper, we study learning generalized driving style representations from automobile GPS trip data. We propose a novel Autoencoder Regularized deep neural Network (ARNet) and a trip encoding framework trip2vec to learn drivers' driving styles directly from GPS records, by combining supervised and unsupervised feature learning in a unified architecture. Experiments on a challenging driver number estimation problem and the driver identification problem show that ARNet can learn a good generalized driving style representation: It significantly outperforms existing methods and alternative architectures by reaching the least estimation error on average (0.68, less than one driver) and the highest identification accuracy (by at least 3% improvement) compared with traditional supervised learning methods.",0
"Automotive sensors can provide a wealth of data about how drivers interact with their vehicles, but analyzing these data requires robust methods that effectively capture patterns and unique characteristics of driving behavior. In this work, we propose an autoencoder regularized neural network (AEA) for learning compact representations of driver styles from sensor measurements. Our method addresses challenges faced by traditional unsupervised approaches such as anomaly detection and clustering through the use of automatic encoders combined with a sparsity promoting loss function. We demonstrate AEA’s effectiveness on real world datasets, showing improved performance over state-of-the-art algorithms and visualizing latent features, which may have important implications for transportation safety and personalization.",1
"Obtaining common representations from different modalities is important in that they are interchangeable with each other in a classification problem. For example, we can train a classifier on image features in the common representations and apply it to the testing of the text features in the representations. Existing multi-modal representation learning methods mainly aim to extract rich information from paired samples and train a classifier by the corresponding labels; however, collecting paired samples and their labels simultaneously involves high labor costs. Addressing paired modal samples without their labels and single modal data with their labels independently is much easier than addressing labeled multi-modal data. To obtain the common representations under such a situation, we propose to make the distributions over different modalities similar in the learned representations, namely modality-invariant representations. In particular, we propose a novel algorithm for modality-invariant representation learning, named Deep Modality Invariant Adversarial Network (DeMIAN), which utilizes the idea of Domain Adaptation (DA). Using the modality-invariant representations learned by DeMIAN, we achieved better classification accuracy than with the state-of-the-art methods, especially for some benchmark datasets of zero-shot learning.",0
"This is a good prompt that could potentially lead to a high quality paper. Some suggestions to improve the prompt would be:  * Clarify if you want me to write a scientific abstract like one typically found at the beginning of research papers (often referred to as just ""the abstract""), which is meant to provide a concise summary of the paper. Or do you want something more like an article summary? * Provide some details on specific aspects of the topic you find most interesting or important, so I can focus my attention therein. If you have any references already, providing them would also make writing easier by helping me know what has been established prior. If you do not feel comfortable sharing sources related to your work, please inform me in advance so we may discuss alternative approaches to improving the prompt.",1
"Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers which is heavily desired in real-world applications concerning surveillance and security. Although much research effort has been devoted to developing face de-occlusion methods, most of them can only work well under constrained conditions, such as all the faces are from a pre-defined closed set. In this paper, we propose a robust LSTM-Autoencoders (RLA) model to effectively restore partially occluded faces even in the wild. The RLA model consists of two LSTM components, which aims at occlusion-robust face encoding and recurrent occlusion removal respectively. The first one, named multi-scale spatial LSTM encoder, reads facial patches of various scales sequentially to output a latent representation, and occlusion-robustness is achieved owing to the fact that the influence of occlusion is only upon some of the patches. Receiving the representation learned by the encoder, the LSTM decoder with a dual channel architecture reconstructs the overall face and detects occlusion simultaneously, and by feat of LSTM, the decoder breaks down the task of face de-occlusion into restoring the occluded part step by step. Moreover, to minimize identify information loss and guarantee face recognition accuracy over recovered faces, we introduce an identity-preserving adversarial training scheme to further improve RLA. Extensive experiments on both synthetic and real datasets of faces with occlusion clearly demonstrate the effectiveness of our proposed RLA in removing different types of facial occlusion at various locations. The proposed method also provides significantly larger performance gain than other de-occlusion methods in promoting recognition performance over partially-occluded faces.",0
"This should summarize the main idea behind your model. For details see my previous questions on this project from April. I am only interested in results that support our paper - e.g., improvements over competing models, etc. No need to explain how we implemented the model unless you want to talk about architectural choices that were key. Finally, assume your audience has strong computer vision background but no prior knowledge about your work specifically (so they don’t know your loss function or data processing pipeline). Assume further that they read papers like ours regularly so have some familiarity with face de-occlusion already. Please write something engaging! Thanks again. ​  Face occlusions occur frequently in real-world images and videos, making it challenging for computer vision algorithms to effectively recognize faces under such conditions. In recent years, deep learning methods based on autoencoders have shown promising results in addressing this problem. However, these methods can still suffer from issues related to robustness, particularly when dealing with severe occlusions or variations in lighting and pose. To overcome these limitations, we propose the use of Robust Long Short-Term Memory Autoencoders (LSTM-AEs) for face de-occusion in unconstrained environments. Our approach leverages both spatial and temporal features extracted from raw image inputs to generate high-quality reconstructions of occluded regions. Compared to existing state-of-the-art methods, including both traditional techniques as well as deep learning approaches, our method achieves significant improvements across multiple metrics relevant to face recognition tasks. These benefits arise due to the unique properties of LSTMs which allow our model to encode memory and attention mechanisms. Our contributions provide valuable insights into the design and implementation of effective face de-occultation systems capable of operating in challenging scenarios. We hope that our findings encourage future research efforts aimed at enhancing the reliability and generalizability of automated face recognition technology.",1
"Many success stories involving deep neural networks are instances of supervised learning, where available labels power gradient-based learning methods. Creating such labels, however, can be expensive and thus there is increasing interest in weak labels which only provide coarse information, with uncertainty regarding time, location or value. Using such labels often leads to considerable challenges for the learning process. Current methods for weak-label training often employ standard supervised approaches that additionally reassign or prune labels during the learning process. The information gain, however, is often limited as only the importance of labels where the network already yields reasonable results is boosted. We propose treating weak-label training as an unsupervised problem and use the labels to guide the representation learning to induce structure. To this end, we propose two autoencoder extensions: class activity penalties and structured dropout. We demonstrate the capabilities of our approach in the context of score-informed source separation of music.",0
"This is an interesting paper that presents a novel approach to dropout training called structured dropout which can improve predictions in weak label scenarios as well as multi-instance learning problems. In addition, the authors show how their method can be applied to score-informed source separation, demonstrating significant improvements over other methods. Overall, this work advances our understanding of deep learning techniques for challenging machine learning tasks and highlights promising future directions for researchers working on these areas.",1
"We present novel method for image-text multi-modal representation learning. In our knowledge, this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature. We only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding. In this paper, we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information. And we show our multi-modal feature has universal semantic information, even though it was trained for category prediction. Our model is end-to-end backpropagation, intuitive and easily extended to other multi-modal learning work.",0
"Here is one possible abstract: Image-text pairs provide rich multi-modal representations of complex concepts that can benefit many natural language processing tasks, including image recognition, text generation, machine translation, question answering, etc. In this paper we propose a novel approach to learn jointly from both image and text modalities using adversarial training. Our method consists of two main components: an encoder network, which takes as input the concatenation of the two modalities (image + corresponding text) and produces two separate latent representations (one for each modality), and a discriminator network that estimates whether the provided modality pair (image, text) is real or generated by our model. We show through extensive experiments on several challenging benchmark datasets that our proposed method significantly outperforms state-of-the-art methods across multiple NLP tasks while achieving competitive results on vision tasks. Additionally, we demonstrate the effectiveness of our method in zero shot learning setting where no paired data is used during fine tuning and only unpaired image - text sets are available.",1
"Hashing aims at generating highly compact similarity preserving code words which are well suited for large-scale image retrieval tasks.   Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper, for the first time, we propose a deep architecture for supervised hashing through residual learning, termed Deep Residual Hashing (DRH), for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements: (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods.",0
"This article presents a novel approach to deep residual hashing that significantly improves upon existing methods by incorporating advanced techniques from both traditional hash learning and recent advances in neural networks. We first provide a comprehensive review of the state-of-the-art residual hashing techniques, highlighting their strengths and weaknesses. Next, we propose our proposed method which addresses these limitations by introducing two key innovations: a deep embedding network that learns a high dimensional feature space; and a robust loss function based on both binary cross entropy and Euclidean distance. Our extensive experiments show that our new method outperforms all previous methods across multiple benchmark datasets, delivering substantially higher precision and recall scores while achieving comparably low computational complexity. Finally, we discuss future research directions inspired by our findings and conclude by summarizing the significant contributions of this work towards addressing challenges in the field of computer vision using hashing techniques.",1
"With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning.   This article surveys various constraints that encourage a learning algorithm to discover factorial representations. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. Supervised inductive biases are constraints on the representations based on additional information connected to observations. Supervisory labels come in variety of types, which vary in how strongly they constrain the representation, how many factors are labeled, how many observations are labeled, and whether or not we know the associations between the constraints and the factors they are related to.   This survey brings together a wide variety of models that all touch on the problem of learning factorial representations and lays out a framework for comparing these models based on the strengths of the underlying supervised and unsupervised inductive biases.",0
"This survey provides an overview of inductive biases that have been used in deep learning models for factorial representation learning. We first introduce the concept of factorial representations and discuss their role in various domains such as image generation, speech recognition, and language translation. Then, we present several types of inductive biases commonly employed in factorial representation-learning models, including nonlinearity constraints, sparsity regularization, tensor decomposition-based approaches, equivariance constraints, attention mechanisms, and others. For each type of bias, we describe how it operates, highlight key papers that use them, discuss implementation details, and analyze strengths and weaknesses. Finally, we conclude by summarizing current trends and future research directions in using these inductive biases for building effective factorial representation-learning models.",1
"In this work, we address the challenging video scene parsing problem by developing effective representation learning methods given limited parsing annotations. In particular, we contribute two novel methods that constitute a unified parsing framework. (1) \textbf{Predictive feature learning}} from nearly unlimited unlabeled video data. Different from existing methods learning features from single frame parsing, we learn spatiotemporal discriminative features by enforcing a parsing network to predict future frames and their parsing maps (if available) given only historical frames. In this way, the network can effectively learn to capture video dynamics and temporal context, which are critical clues for video scene parsing, without requiring extra manual annotations. (2) \textbf{Prediction steering parsing}} architecture that effectively adapts the learned spatiotemporal features to scene parsing tasks and provides strong guidance for any off-the-shelf parsing model to achieve better video scene parsing performance. Extensive experiments over two challenging datasets, Cityscapes and Camvid, have demonstrated the effectiveness of our methods by showing significant improvement over well-established baselines.",0
"In today’s world, video scene parsing has become increasingly important due to growing demands from applications like autonomous driving, robotics, and surveillance systems that rely heavily on accurate object detection in videos. Most existing methods rely heavily on handcrafted features which can lead to limited performance as they cannot generalize well across domains or tasks. Recent advances have shown that learning predictive feature representations using neural networks can significantly improve accuracy in many computer vision problems including image classification and semantic segmentation. Motivated by these results, we present an approach to video scene parsing that learns to extract highly informative features at runtime through a self-supervised pretext task that uses only raw pixels as input. Our method achieves state-of-the-art performance on two benchmark datasets while running in real-time, demonstrating significant improvement over prior works that relied solely on human engineering. Furthermore, our method outperforms existing end-to-end models trained from scratch showing the effectiveness of the learned representations for downstream tasks. We believe this work opens up new possibilities for improving performance in other high level video understanding tasks such as action recognition and spatio-temporal activity localization.",1
"In this paper, we consider the problem of event classification with multi-variate time series data consisting of heterogeneous (continuous and categorical) variables. The complex temporal dependencies between the variables combined with sparsity of the data makes the event classification problem particularly challenging. Most state-of-art approaches address this either by designing hand-engineered features or breaking up the problem over homogeneous variates. In this work, we propose and compare three representation learning algorithms over symbolized sequences which enables classification of heterogeneous time-series data using a deep architecture. The proposed representations are trained jointly along with the rest of the network architecture in an end-to-end fashion that makes the learned features discriminative for the given task. Experiments on three real-world datasets demonstrate the effectiveness of the proposed approaches.",0
"This paper presents a deep learning model for heterogeneous time series classification problems using symbolic representations that capture the patterns and relationships among different types of data streams. Our approach leverages transformers equipped with attention mechanisms to learn representations from raw numerical features as well as discrete symbols extracted from textual descriptions. By training these models on large datasets of multimodal signals and events, we demonstrate their effectiveness in predicting future outcomes across multiple domains. We evaluate our method against state-of-the art techniques using metrics such as accuracy, F1 score, and Matthews correlation coefficient (MCC) on both synthetic and real-world datasets. Results show significant improvements over baseline methods, making our framework a promising toolkit for handling complex time series prediction tasks involving various modalities of input data. In addition, we provide analysis and insights into the behavior of our system through visualization and sensitivity experiments to aid further research on explainability and interpretability issues associated with deep learning solutions applied to temporal sequence analysis. Overall, this work contributes to advancing artificial intelligence technologies for dealing with multifaceted sequential data in an increasingly connected world where diverse sources of data need to be integrated seamlessly.",1
"Supervised (pre-)training currently yields state-of-the-art performance for representation learning for visual recognition, yet it comes at the cost of (1) intensive manual annotations and (2) an inherent restriction in the scope of data relevant for learning. In this work, we explore unsupervised feature learning from unlabeled video. We introduce a novel object-centric approach to temporal coherence that encourages similar representations to be learned for object-like regions segmented from nearby frames. Our framework relies on a Siamese-triplet network to train a deep convolutional neural network (CNN) representation. Compared to existing temporal coherence methods, our idea has the advantage of lightweight preprocessing of the unlabeled video (no tracking required) while still being able to extract object-level regions from which to learn invariances. Furthermore, as we show in results on several standard datasets, our method typically achieves substantial accuracy gains over competing unsupervised methods for image classification and retrieval tasks.",0
"""Object Centric representations have become increasingly important in computer vision tasks such as video summarization, action recognition and image retrieval due to their ability to capture spatial relationships between objects in images. Recently, deep learning techniques using Convolutional Neural Networks (CNN) have been used successfully for object detection, segmentation and feature extraction. However, unsupervised representation learning from unlabeled videos remains challenging because we lack effective methods that can learn meaningful visual features by directly processing raw pixels without any prior annotations, object detectors or human supervision. In this work, we propose a method to jointly learn video representation and object detection on large scale datasets through a two-stream network architecture, consisting of convolutional autoencoders trained on patches extracted from cropped frames and stacked RGB flows obtained by warping optical flow fields inferred from a pretrained flow model across consecutive frame pairs. We show experimentally that our approach achieves significantly better performance than previous state-of-the-art methods in terms of both quantitative measures of reconstruction fidelity and qualitative evaluation of the learned features.""",1
"We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. Discretization based on Symbolic Aggregate Approximation is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. We further build the Markov matrix from the discretized representation from the deconvolution to visualize the time series as complex networks, which show more class-specific statistical properties and clear structures with respect to different labels.",0
"This paper presents a novel approach to multivariate time series classification that utilizes deconvolution layers. These layers allow us to learn representations from raw data without imposing any specific assumptions on the shape of the underlying function. Our method first applies linear convolutions to extract local features at each time point, which are then passed through a stack of deconvolutional layers to recover global patterns. By learning these patterns, our algorithm can capture complex relationships across multiple variables over time. Finally, we use recurrent neural networks (RNNs) to encode temporal dependencies and make predictions. To evaluate our approach, we applied it to both simulated data and real world benchmark datasets, obtaining promising results in terms of accuracy and interpretability. Overall, our method has significant potential applications in fields such as finance, healthcare, and environmental science where understanding multivariate time series data is crucial.",1
"Since about 100 years ago, to learn the intrinsic structure of data, many representation learning approaches have been proposed, including both linear ones and nonlinear ones, supervised ones and unsupervised ones. Particularly, deep architectures are widely applied for representation learning in recent years, and have delivered top results in many tasks, such as image classification, object detection and speech recognition. In this paper, we review the development of data representation learning methods. Specifically, we investigate both traditional feature learning algorithms and state-of-the-art deep learning models. The history of data representation learning is introduced, while available resources (e.g. online course, tutorial and book information) and toolboxes are provided. Finally, we conclude this paper with remarks and some interesting research directions on data representation learning.",0
"Title should be ""Data Representation Learning"" Please create an abstract that is concise (no more than 180 words), informative and engaging. Keywords can come later but make sure you incorporate all relevant keywords related to data representation learning such as feature learning, deep learning, autoencoders, etc. You may cite papers from any source as they are meant to support your work.",1
"Person re-identification (Re-ID) poses a unique challenge to deep learning: how to learn a deep model with millions of parameters on a small training set of few or no labels. In this paper, a number of deep transfer learning models are proposed to address the data sparsity problem. First, a deep network architecture is designed which differs from existing deep Re-ID models in that (a) it is more suitable for transferring representations learned from large image classification datasets, and (b) classification loss and verification loss are combined, each of which adopts a different dropout strategy. Second, a two-stepped fine-tuning strategy is developed to transfer knowledge from auxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel unsupervised deep transfer learning model is developed based on co-training. The proposed models outperform the state-of-the-art deep Re-ID models by large margins: we achieve Rank-1 accuracy of 85.4\%, 83.7\% and 56.3\% on CUHK03, Market1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model (45.1\%) beats most supervised models.",0
"In recent years, person re-identification has emerged as a critical problem in computer vision, where the goal is to identify individuals across different cameras within non-overlapping fields of view. While there have been numerous attempts at solving this problem using traditional methods such as hand-crafted features and convolutional neural networks (CNNs), deep transfer learning techniques have shown promising results. This paper presents a comprehensive survey of state-of-the-art deep transfer learning approaches for person re-identification. We begin by providing an overview of the fundamentals of person re-identification and discussing some common challenges faced by existing solutions. Next, we explore several deep transfer learning techniques that have been proposed in the literature, including fine-tuning pre-trained CNN models, adversarial domain adaptation, generative adversarial imitation learning, cycle consistency loss, and multi-task learning. For each technique, we describe the underlying principles, key insights, pros, and cons. Finally, we provide concluding remarks on the current status of research in this field and highlight future directions for improving person re-identification using deep transfer learning. Overall, this survey provides readers with a comprehensive understanding of recent advances in deep transfer learning for person re-identification and serves as a valuable resource for both researchers and practitioners alike.",1
"The CNN-encoding of features from entire videos for the representation of human actions has rarely been addressed. Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences. We present a new video representation, called temporal linear encoding (TLE) and embedded inside of CNNs as a new layer, which captures the appearance and motion throughout entire videos. It encodes this aggregated information into a robust video feature representation, via end-to-end learning. Advantages of TLEs are: (a) they encode the entire video into a compact feature representation, learning the semantics and a discriminative feature space; (b) they are applicable to all kinds of networks like 2D and 3D CNNs for video classification; and (c) they model feature interactions in a more expressive way and without loss of information. We conduct experiments on two challenging human action datasets: HMDB51 and UCF101. The experiments show that TLE outperforms current state-of-the-art methods on both datasets.",0
"Recent advances in deep learning have led to significant improvements in image understanding tasks such as object recognition, segmentation, and caption generation. In contrast, video analysis has lagged behind due to the lack of effective temporal modeling techniques. Existing approaches either rely on handcrafted features that ignore spatial context or use recurrent neural networks (RNNs) that suffer from vanishing gradients and exploding activations. To address these limitations, we propose Deep Temporal Linear Encoding Networks (DTLE), which combine linear models with deep architectures to capture both short-term dynamics and complex spatial relationships across frames. Our approach exploits the complementary strengths of temporal convolutions and recurrent layers by introducing a novel spatio-temporal bottleneck layer that maps high-dimensional visual representations into low-dimensional latent space vectors. These vectors can then be fed through any number of fully connected layers without losing information. We evaluate our method on two challenging benchmark datasets: action recognition on UCF101 and video caption generation on MSVD. DTLE outperforms state-of-the-art methods while achieving faster inference speed due to its efficient architecture and online training capability. Our results demonstrate the effectiveness of combining linear models and deep architectures for robust temporal modeling in videos.",1
"Learning representations of data, and in particular learning features for a subsequent prediction task, has been a fruitful area of research delivering impressive empirical results in recent years. However, relatively little is understood about what makes a representation `good'. We propose the idea of a risk gap induced by representation learning for a given prediction context, which measures the difference in the risk of some learner using the learned features as compared to the original inputs. We describe a set of sufficient conditions for unsupervised representation learning to provide a benefit, as measured by this risk gap. These conditions decompose the problem of when representation learning works into its constituent parts, which can be separately evaluated using an unlabeled sample, suitable domain-specific assumptions about the joint distribution, and analysis of the feature learner and subsequent supervised learner. We provide two examples of such conditions in the context of specific properties of the unlabeled distribution, namely when the data lies close to a low-dimensional manifold and when it forms clusters. We compare our approach to a recently proposed analysis of semi-supervised learning.",0
"In recent years, feature learning has become increasingly important as a means of obtaining high performance on challenging machine learning tasks. However, despite significant progress in this area, many current techniques suffer from limitations that make them difficult to use effectively across different domains and applications. To address these issues, we propose a modular theory of feature learning that provides a flexible framework for designing and implementing effective feature representations. Our approach emphasizes the importance of decomposing complex problems into simpler subtasks that can be solved using well-understood algorithms and heuristics, while ensuring that the overall solution remains coherent and robust. We demonstrate the effectiveness of our method through experiments on several benchmark datasets and show that it outperforms state-of-the-art methods in terms of both accuracy and computational efficiency. Our work represents a step towards more generalizable solutions for feature learning, and sets the stage for future advances in artificial intelligence.",1
"Latent representation learned from multi-layered neural networks via hierarchical feature abstraction enables recent success of deep learning. Under the deep learning framework, generalization performance highly depends on the learned latent representation which is obtained from an appropriate training scenario with a task-specific objective on a designed network model. In this work, we propose a novel latent space modeling method to learn better latent representation. We designed a neural network model based on the assumption that good base representation can be attained by maximizing the total correlation between the input, latent, and output variables. From the base model, we introduce a semantic noise modeling method which enables class-conditional perturbation on latent space to enhance the representational power of learned latent feature. During training, latent vector representation can be stochastically perturbed by a modeled class-conditional additive noise while maintaining its original semantic feature. It implicitly brings the effect of semantic augmentation on the latent space. The proposed model can be easily learned by back-propagation with common gradient-based optimization algorithms. Experimental results show that the proposed method helps to achieve performance benefits against various previous approaches. We also provide the empirical analyses for the proposed class-conditional perturbation process including t-SNE visualization.",0
"This work proposes a novel approach to modeling semantic noise in natural language processing tasks, aimed at improving the quality of learned representations. We introduce the concept of ""semantic noise"" as a measure of ambiguity, vagueness, and inconsistency in linguistic input, which can have a significant impact on representation learning. Our method involves identifying and analyzing sources of semantic noise, such as synonymy, metaphorical expressions, and contextual dependencies, and incorporating them into the training process using appropriate regularization techniques. Experimental results demonstrate that our approach leads to improved performance on downstream NLP tasks, providing evidence that better handling of semantic noise can lead to more effective representation learning. Overall, we believe that our work represents a valuable contribution to the field of NLP, contributing new insights into how semantic uncertainty can be modeled and leveraged to improve representation learning.",1
"We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.",0
"This paper proposes an end-to-end system for generating videos that includes physical interactions with objects within scenes by predicting future scene dynamics. Our method takes as input only video frames captured from movies, games and other real applications to generate new, plausible frames without using any external libraries, datasets or even human annotations. We train our model through adversarial training with self-supervision to maximize stability and coherence while minimizing diversity, which leads to sharpness at low resolutions. By conditioning on novel events predicted by our trained generator, we can now capture new scene dynamics for better quality video generation. We then compare the results of unconditioned video prediction against those generated under a dynamic event condition. To evaluate quantitatively, we use two measures based on perceptual quality metrics, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), followed by a qualitative comparison on diverse action classes in both static and moving images. Overall, the proposed approach provides higher image fidelity and preserves semantic correspondences across different scenarios compared to previous methods. For example, we observe more accurate object interaction predictions for both moving and rigid objects.",1
"Task engagement is defined as loadings on energetic arousal (affect), task motivation, and concentration (cognition). It is usually challenging and expensive to label cognitive state data, and traditional computational models trained with limited label information for engagement assessment do not perform well because of overfitting. In this paper, we proposed two deep models (i.e., a deep classifier and a deep autoencoder) for engagement assessment with scarce label information. We recruited 15 pilots to conduct a 4-h flight simulation from Seattle to Chicago and recorded their electroencephalograph (EEG) signals during the simulation. Experts carefully examined the EEG signals and labeled 20 min of the EEG data for each pilot. The EEG signals were preprocessed and power spectral features were extracted. The deep models were pretrained by the unlabeled data and were fine-tuned by a different proportion of the labeled data (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for engagement assessment. The models were then tested on the remaining labeled data. We compared performances of the new data representations with the original EEG features for engagement assessment. Experimental results show that the representations learned by the deep models yielded better accuracies for the six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based on different proportions of the labeled data for training, as compared with the corresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%) achieved by the original EEG features. Deep models are effective for engagement assessment especially when less label information was used for training.",0
"In recent years, deep learning models have demonstrated outstanding performance in many computer vision tasks, including image classification, object detection, segmentation, etc. However, training such models typically requires large amounts of labeled data which may not always be available. This makes engagement assessment particularly challenging since obtaining annotations can be costly and time-consuming. In this work, we propose a novel approach using deep learning techniques to address these limitations. Our model leverages scarce label information by jointly optimizing multiple metrics that measure the quality of generated samples as well as their relevance to given tasks. We validate our method on several benchmark datasets commonly used in engagement assessment and demonstrate significant improvements over state-of-the-art methods. Additionally, we showcase our framework's generalization capabilities through experiments across different domains and tasks. Overall, our study presents a promising solution towards enabling effective engagement assessment with limited supervision.",1
"Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.",0
"This paper presents a new approach to leveraging large pretrained language models (LLMs) in natural language processing tasks, specifically relation extraction. Traditional approaches involve fine-tuning the entire model on a small amount of task data, resulting in high computational cost and limited transferability across tasks. Instead, we propose using lifted rule injection (LRI), which injects human-provided rules into the embedding space learned by the LLB to improve accuracy on the given task. We show that LRI outperforms strong baselines on several benchmark datasets, achieving state-of-the-art results without the need for expensive fine-tuning or extensive training data. Our method provides an effective alternative for incorporating prior knowledge into the LLM, demonstrating the potential of enhancing NLP systems through interactive collaboration between humans and machines.",1
"This work advocates Eulerian motion representation learning over the current standard Lagrangian optical flow model. Eulerian motion is well captured by using phase, as obtained by decomposing the image through a complex-steerable pyramid. We discuss the gain of Eulerian motion in a set of practical use cases: (i) action recognition, (ii) motion prediction in static images, (iii) motion transfer in static images and, (iv) motion transfer in video. For each task we motivate the phase-based direction and provide a possible approach.",0
"This would work as an introduction: ""This paper argues that phase has an important role to play in learning motion representations."" How can I assist you?",1
"Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.",0
"In this work we study how adversarial training can improve sketch retrieval accuracy on both synthetic (e.g., drawn by humans) data as well as natural data such as human drawn stick figures and photos of objects from real world scenes. Specifically we train generative models to fool sketch based retrieval systems and then use these attacks to fine tune their parameters to increase robustness against them while maintaining good generalization performance on clean test sets. We demonstrate significant improvement over prior arts’ state of art methods using standard benchmarks. Our framework could potentially provide competitive results for tasks like photo search which suffer from large scale variability. While our focus is on image retrieval, the idea may also find uses in other applications that require understanding what makes certain inputs difficult in order to make them more robust. This includes but is not limited to object detection, semantic segmentation and pose estimation. We conclude with future directions such as applying the methodology beyond Euclidean distance metrics used for matching in this work.",1
"With increasing demand for efficient image and video analysis, test-time cost of scene parsing becomes critical for many large-scale or time-sensitive vision applications. We propose a dynamic hierarchical model for anytime scene labeling that allows us to achieve flexible trade-offs between efficiency and accuracy in pixel-level prediction. In particular, our approach incorporates the cost of feature computation and model inference, and optimizes the model performance for any given test-time budget by learning a sequence of image-adaptive hierarchical models. We formulate this anytime representation learning as a Markov Decision Process with a discrete-continuous state-action space. A high-quality policy of feature and model selection is learned based on an approximate policy iteration method with action proposal mechanism. We demonstrate the advantages of our dynamic non-myopic anytime scene parsing on three semantic segmentation datasets, which achieves $90\%$ of the state-of-the-art performances by using $15\%$ of their overall costs.",0
"This can be challenging because you have to accurately summarize the key contributions of your work without referring to its specific details. To write an effective abstract that achieves this goal, follow these steps: 1) Identify the main problem your research addresses. What challenge exists in the field that your work seeks to overcome? 2) Explain how your approach improves upon existing methods, highlighting any new techniques or innovations introduced by your work. 3) Summarise your findings and their implications within the broader context of the problem addressed. What impact could your research potentially have on related fields and future work? 4) End with a brief statement emphasizing the significance of your work. Why should readers be interested in learning more about your research?  Please use academic language while writing the abstract. Also please add relevant keywords at the end. For example: Computer Vision, Deep Learning, Convolutional Neural Networks, Natural Language Processing etc. Thank You!",1
"Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.",0
"While deep learning has proven highly effective at solving complex problems, there still exists a significant gap between human brain performance on similar tasks and that of current neural networks. We hypothesize that this discrepancy may originate from two fundamental differences: i) the ability of neurons to store and accumulate energy intracellularly through metabolic pathways which enable sustained activity; ii) active communication between neuronal populations facilitated by chemical synapses allowing them to engage in distributed computation. By bridging these gaps we aim towards creating bio plausible models capable of replicating observed biological behaviour across multiple domains in a single system. Our approach involves building multi layered systems comprising of interconnected spiking neurons endowed with plasticity enabled both locally through modulation of ion channel conductance, and distantly via chemical neurotransmitters. Through extensive simulation studies using large datasets collected under diverse recording conditions, we establish the effectiveness of our proposed framework towards realizing such models. An interesting outcome of our simulations emerges from understanding collective dynamics over larger time scales - suggesting previously unknown roles played by different cells implicated in specific cortical regions involved in memory formation during sleep. Ultimately, this work contributes towards addressing broader questions related to consciousness, intelligence and their origins, by focusing on how microscopic processes involving individual spines could give rise to macroscopic phenomena we observe today.",1
"Probabilistic models learned as density estimators can be exploited in representation learning beside being toolboxes used to answer inference queries only. However, how to extract useful representations highly depends on the particular model involved. We argue that tractable inference, i.e. inference that can be computed in polynomial time, can enable general schemes to extract features from black box models. We plan to investigate how Tractable Probabilistic Models (TPMs) can be exploited to generate embeddings by random query evaluations. We devise two experimental designs to assess and compare different TPMs as feature extractors in an unsupervised representation learning framework. We show some experimental results on standard image datasets by applying such a method to Sum-Product Networks and Mixture of Trees as tractable models generating embeddings.",0
"In recent years, there has been significant interest in using deep learning models to represent complex data distributions, especially in domains where traditional modeling techniques have struggled to capture underlying patterns and relationships. However, many deep learning methods suffer from a lack of interpretability, making it difficult to reason about their behavior and potential limitations. To address these challenges, we propose the use of tractable probabilistic models that can achieve state-of-the-art performance on several benchmark datasets while providing insight into the learned representations through explicit modeling of uncertainty and latent structure. Our approach combines advances in variational inference and deep learning to enable efficient posterior sampling and optimization that results in more meaningful representations. We evaluate our method across diverse tasks including image generation, anomaly detection, and reinforcement learning, demonstrating improved performance compared to competitive baselines. Finally, we discuss future directions for incorporating richer structural priors and exploring applications beyond visual perception problems.",1
"Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reaching or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representations learned by these networks and human psychological representations recovered from similarity judgments. We find that deep features learned in service of object classification account for a significant amount of the variance in human similarity judgments for a set of animal images. However, these features do not capture some qualitative distinctions that are a key part of human representations. To remedy this, we develop a method for adapting deep features to align with human similarity judgments, resulting in image representations that can potentially be used to extend the scope of psychological experiments.",0
"As deep learning techniques have become increasingly prevalent in computer vision tasks, their application has expanded into fields such as psychology where they can provide powerful representations of perceptual phenomena. In our work, we explore how these deep network features capture psychological representations by adapting them to better align with established theories of human cognition. We evaluate their ability to accurately predict human behavior on various experiments across different modalities (e.g., visual, auditory) using transfer learning methods that fine-tune networks pretrained on large datasets to new target domains. Our results show that these adapted models significantly improve performance over baseline models, indicating that they effectively encode meaningful psychological concepts related to attention, memory, and decision making. These findings offer promising evidence towards the use of neural networks as tools for capturing complex mental processes, which may lead to further advancements in both psychology research and applied areas like education and therapy.",1
"In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach extends traditional topic models by learning a factorized latent variable representation. The structured representation leads to a model that marries benefits traditionally associated with a discriminative approach, such as feature selection, with those of a generative model, such as principled regularization and ability to handle missing data. The factorization is provided by representing data in terms of aligned pairs of observations as different views. This provides means for selecting a representation that separately models topics that exist in both views from the topics that are unique to a single view. This structured consolidation allows for efficient and robust inference and provides a compact and efficient representation. Learning is performed in a Bayesian fashion by maximizing a rigorous bound on the log-likelihood. Firstly, we illustrate the benefits of the model on a synthetic dataset,. The model is then evaluated in both uni- and multi-modality settings on two different classification tasks with off-the-shelf convolutional neural network (CNN) features which generate state-of-the-art results with extremely compact representations.",0
"In this work, we propose a new approach to learning representations that capture relationships between batteries (sets) of data items. Our method leverages recent advances in topic modeling, which have shown promise as a means of capturing latent structures within text corpora. By adapting these methods to operate on sets of images, audio files, video clips, documents, etc., our algorithm learns low-dimensional embeddings that encode the most salient semantic aspects shared across all items belonging to each battery. We evaluate our technique on several benchmark datasets and demonstrate improvements over state-of-the art baselines across a variety of downstream tasks. Overall, our results provide evidence that inter-battery topic representation learning can effectively discover meaningful latent structures present in complex multi-media collections, enabling improved performance on machine learning problems involving those collections.",1
"This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation can leverage unlabeled target information, while domain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learning algorithm that can be applied to both domain adaptation and domain generalization. SCA is based on a simple geometrical measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA finds a representation that trades between maximizing the separability of classes, minimizing the mismatch between domains, and maximizing the separability of data; each of which is quantified through scatter. The optimization problem of SCA can be reduced to a generalized eigenvalue problem, which results in a fast and exact solution. Comprehensive experiments on benchmark cross-domain object recognition datasets verify that SCA performs much faster than several state-of-the-art algorithms and also provides state-of-the-art classification accuracy in both domain adaptation and domain generalization. We also show that scatter can be used to establish a theoretical generalization bound in the case of domain adaptation.",0
"In recent years, domain adaptation and domain generalization have emerged as important research areas in machine learning, with applications ranging from computer vision to natural language processing. Traditional approaches to these problems often rely on handcrafted features and heuristics that may not generalize well across different domains. In this work, we propose a new framework called scatter component analysis (SCA) which addresses both domain adaptation and domain generalization simultaneously by explicitly modeling the variation within and among multiple source domains. SCA decomposes the source data into several interpretable components such as class-specific, task-irrelevant, shared and noise, allowing us to adapt or regularize specific components based on the target domain. We validate our approach on a variety of benchmark datasets and demonstrate state-of-the-art performance compared to existing methods. Our results show that SCA effectively captures the intrinsic relationships among tasks and domains, making it a powerful tool for addressing challenges in transfer learning. Overall, this work provides a unified and principled approach for understanding and solving domain adaptation and domain generalization problems.",1
"What is the right supervisory signal to train visual representations? Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets. However, in case of biological agents, visual representation learning does not require millions of semantic labels. We argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations (images and videos downloaded from web). For example, babies push objects, poke them, put them in their mouth and throw them to learn representations. Towards this goal, we build one of the first systems on a Baxter platform that pushes, pokes, grasps and observes objects in a tabletop environment. It uses four different types of physical interactions to collect more than 130K datapoints, with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations. We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Quantitatively, we evaluate our learned ConvNet on image classification tasks and show improvements compared to learning without external data. Finally, on the task of instance retrieval, our network outperforms the ImageNet network on recall@1 by 3%",0
"This paper proposes a novel methodology that enables robots to learn visual representations of objects from physical interactions, rather than relying solely on pre-programmed knowledge or human guidance. By engaging in activities such as handling objects, moving them around and exploring their properties, the robot can build up a rich understanding of object shape, texture and material composition, which it can then use to identify and manipulate new instances of similar objects. Furthermore, we demonstrate how this approach allows the robot to generalize its learning beyond specific examples seen during training, enabling more robust performance across different environments and scenarios. Our experiments show promising results for several challenging tasks including multi-view object classification, pose estimation, and grasp planning under cluttered conditions. Overall, our work paves the way towards developing intelligent robots capable of lifelong self-learning through autonomous curiosity-driven interactions with their surroundings. The proposed methodology in this paper presents a novel approach for robots to learn visual representations of objects by interacting with them physically. In contrast to traditional methods, which rely heavily on either prior knowledge or explicit human guidance, this approach allows the robot to build up detailed understanding of objects through actions like manipulation, movement, and observation. As a result, the robot can achieve greater accuracy in identifying and manipulating related objects in diverse situations, going well beyond what was observed during initial training. Extensive experimental evaluation validates these claims for demanding tasks, such as classifying views, estimating poses, and performing effective grasps even while facing clutter. Altogether, this research has significant implications for producing intelligent robots able to sustainably improve themselves via natural, curious behavior within their environment.",1
"Recently, machine learning based single image super resolution (SR) approaches focus on jointly learning representations for high-resolution (HR) and low-resolution (LR) image patch pairs to improve the quality of the super-resolved images. However, due to treat all image pixels equally without considering the salient structures, these approaches usually fail to produce visual pleasant images with sharp edges and fine details. To address this issue, in this work we present a new novel SR approach, which replaces the main building blocks of the classical interpolation pipeline by a flexible, content-adaptive deep neural networks. In particular, two well-designed structure-aware components, respectively capturing local- and holistic- image contents, are naturally incorporated into the fully-convolutional representation learning to enhance the image sharpness and naturalness. Extensively evaluations on several standard benchmarks (e.g., Set5, Set14 and BSD200) demonstrate that our approach can achieve superior results, especially on the image with salient structures, over many existing state-of-the-art SR methods under both quantitative and qualitative measures.",0
"This paper proposes a new approach to image super resolution that utilizes deep neural networks to learn joint representations of local and holistic structures within images. Traditional approaches to image super resolution have focused on either preserving local structure or global coherence, but these methods often struggle to capture both aspects simultaneously. Our method addresses this issue by learning separate representations for local and holistic features before fusing them together to produce high-resolution outputs. Experiments demonstrate significant improvements over state-of-the-art methods across multiple benchmark datasets. We also showcase our method's effectiveness in real-world applications such as satellite imagery enhancement and medical imaging reconstruction. Overall, our work represents an important step towards achieving highly accurate image upscaling while maintaining crucial structural information.",1
"The keep-growing content of Web images may be the next important data source to scale up deep neural networks, which recently obtained a great success in the ImageNet classification challenge and related tasks. This prospect, however, has not been validated on convolutional networks (convnet) -- one of best performing deep models -- because of their supervised regime. While unsupervised alternatives are not so good as convnet in generalizing the learned model to new domains, we use convnet to leverage semi-supervised representation learning. Our approach is to use massive amounts of unlabeled and noisy Web images to train convnets as general feature detectors despite challenges coming from data such as high level of mislabeled data, outliers, and data biases. Extensive experiments are conducted at several data scales, different network architectures, and data reranking techniques. The learned representations are evaluated on nine public datasets of various topics. The best results obtained by our convnets, trained on 3.14 million Web images, outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is closing the gap with VGG-16. These prominent results suggest a budget solution to use deep learning in practice and motivate more research in semi-supervised representation learning.",0
"This paper presents a novel approach for learning deep representations from noisy web images using convolutional neural networks (CNNs). The main challenge in learning meaningful representations from such images is the presence of noise caused by variations in image quality, lighting conditions, background clutter, occlusions, and other artifacts that can affect the accuracy and robustness of traditional computer vision algorithms. In our methodology, we use a combination of adversarial training and data augmentation techniques to improve the generalization performance of CNN models on web image datasets. Specifically, we introduce a multi-task learning framework where each task corresponds to one type of noise present in the images. Our experiments demonstrate that the proposed approach outperforms state-of-the-art methods in terms of classification accuracy and visual feature coherence, highlighting the effectiveness of deep representation learning under challenging scenarios. Overall, this research contributes new insights into understanding how CNNs learn complex features from noisy web images and offers promising solutions towards building more reliable and robust computer vision systems.",1
"We propose a reinforcement learning based approach to tackle the cost-sensitive learning problem where each input feature has a specific cost. The acquisition process is handled through a stochastic policy which allows features to be acquired in an adaptive way. The general architecture of our approach relies on representation learning to enable performing prediction on any partially observed sample, whatever the set of its observed features are. The resulting model is an original mix of representation learning and of reinforcement learning ideas. It is learned with policy gradient techniques to minimize a budgeted inference cost. We demonstrate the effectiveness of our proposed method with several experiments on a variety of datasets for the sparse prediction problem where all features have the same cost, but also for some cost-sensitive settings.",0
"One significant challenge faced by modern machine learning algorithms is the problem of selecting relevant features from large datasets while minimizing costs associated with data acquisition. In practice, feature selection can significantly impact model accuracy, but may require substantial computational resources and human expertise. To address these challenges, we propose a novel approach called sequential cost-sensitive feature acquisition (SCSA). Our method combines feature selection techniques based on statistical significance testing with active search strategies that adaptively select new features based on their expected informativeness and cost. We show that our SCSA algorithm outperforms baseline methods both theoretically and empirically across several benchmark datasets and real world applications. These results suggest that SCSA has important implications for efficient feature engineering and decision making under resource constraints in artificial intelligence.",1
"This paper describes an effective and efficient image classification framework nominated distributed deep representation learning model (DDRL). The aim is to strike the balance between the computational intensive deep learning approaches (tuned parameters) which are intended for distributed computing, and the approaches that focused on the designed parameters but often limited by sequential computing and cannot scale up. In the evaluation of our approach, it is shown that DDRL is able to achieve state-of-art classification accuracy efficiently on both medium and large datasets. The result implies that our approach is more efficient than the conventional deep learning approaches, and can be applied to big data that is too complex for parameter designing focused approaches. More specifically, DDRL contains two main components, i.e., feature extraction and selection. A hierarchical distributed deep representation learning algorithm is designed to extract image statistics and a nonlinear mapping algorithm is used to map the inherent statistics into abstract features. Both algorithms are carefully designed to avoid millions of parameters tuning. This leads to a more compact solution for image classification of big data. We note that the proposed approach is designed to be friendly with parallel computing. It is generic and easy to be deployed to different distributed computing resources. In the experiments, the largescale image datasets are classified with a DDRM implementation on Hadoop MapReduce, which shows high scalability and resilience.",0
"In our modern world where vast amounts of data are generated every second, the need to effectively utilize that data has become increasingly important. One area where this is particularly true is in image classification tasks, which have numerous applications ranging from facial recognition systems to satellite imagery analysis. With large datasets come challenges in terms of scalability, computational efficiency, and effective model training. To address these concerns, we propose a distributed deep representation learning (DDRL) model for big image dataset classification. Our DDRL framework leverages existing models and algorithms by distributing computation across multiple nodes and optimizing parameters using AdamW gradient descent optimization method. This enables us to scale up deep neural networks without significantly impacting their accuracy. We evaluate our proposed model on five widely used benchmark datasets and demonstrate improved performance compared to other state-of-the-art methods. Additionally, we perform ablation studies to further examine the effectiveness of different components in our system. These results highlight the potential of our approach as a powerful tool for handling large and complex datasets in image classification tasks. As such, our work contributes new insights into developing efficient and effective solutions for processing big data while simultaneously achieving high levels of accuracy.",1
"With this positional paper we present a representation learning view on predicate invention. The intention of this proposal is to bridge the relational and deep learning communities on the problem of predicate invention. We propose a theory reconstruction approach, a formalism that extends autoencoder approach to representation learning to the relational settings. Our intention is to start a discussion to define a unifying framework for predicate invention and theory revision.",0
"This research study explores how human learning capabilities can inform machine learning models through representing knowledge acquisition as theory construction, and more specifically how machines could perform predicate invention. Through qualitative evaluations using artificial datasets generated from human language data, we analyze how current representations encode different types of meaning and whether they allow for effective predicate discovery. Our results show that while these representations have limitations, they do capture certain aspects of predicates. We then propose new techniques inspired by cognitive science theories that could better enable machines to reason and learn like humans. Finally, we discuss future work to improve model interpretability and evaluate their effects on human understanding.",1
"The way people look in terms of facial attributes (ethnicity, hair color, facial hair, etc.) and the clothes or accessories they wear (sunglasses, hat, hoodies, etc.) is highly dependent on geo-location and weather condition, respectively. This work explores, for the first time, the use of this contextual information, as people with wearable cameras walk across different neighborhoods of a city, in order to learn a rich feature representation for facial attribute classification, without the costly manual annotation required by previous methods. By tracking the faces of casual walkers on more than 40 hours of egocentric video, we are able to cover tens of thousands of different identities and automatically extract nearly 5 million pairs of images connected by or from different face tracks, along with their weather and location context, under pose and lighting variations. These image pairs are then fed into a deep network that preserves similarity of images connected by the same track, in order to capture identity-related attribute features, and optimizes for location and weather prediction to capture additional facial attribute features. Finally, the network is fine-tuned with manually annotated samples. We perform an extensive experimental analysis on wearable data and two standard benchmark datasets based on web images (LFWA and CelebA). Our method outperforms by a large margin a network trained from scratch. Moreover, even without using manually annotated identity labels for pre-training as in previous methods, our approach achieves results that are better than the state of the art.",0
"In this paper we present our work on egocentric video based facial attribute representation learning by using contextual data. Using wearable cameras such as Google Glasses, users can easily capture videos that contain their view of surrounding objects as well as parts of their own face, which allows us to exploit rich visual cues including identity, pose, expression and lighting conditions. We use these videos along with contextual data such as speech transcripts from meetings and other social interactions as additional sources of supervision for jointly modeling both the high level task of facial attribute prediction and low level vision problems such as object detection and face alignment. Our method significantly improves accuracy over strong baselines in cross-view analysis tasks and user studies have shown promising results where real users were able to interact comfortably with the system while walking through streets and offices.",1
"In this paper, we propose a recurrent framework for Joint Unsupervised LEarning (JULE) of deep representations and image clusters. In our framework, successive operations in a clustering algorithm are expressed as steps in a recurrent process, stacked on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward pass. Our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single model with a unified weighted triplet loss and optimizing it end-to-end, we can obtain not only more powerful representations, but also more precise image clusters. Extensive experiments show that our method outperforms the state-of-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks.",0
"In recent years, unsupervised learning has emerged as a promising approach for learning deep representations from large amounts of image data without the need for labeled training examples. However, many existing methods still suffer from drawbacks such as suboptimal clustering assignments and poor discriminative ability of learned features. To address these limitations, we propose a novel framework that jointly learns deep representations and image clusters in an unsupervised manner. Our method adopts an adversarial setting where a deep network is trained to predict cluster assignments of images by maximizing their mutual information with respect to the latent codes. Simultaneously, an autoencoder architecture reconstructs input images based on the predicted cluster assignments, which serves as regularization for both representation learning and cluster discovery. Through extensive experiments on several benchmark datasets, our method consistently outperforms state-of-the-art approaches in terms of visual quality, quantitative evaluation metrics, and interpretability of learned representations. This work demonstrates the effectiveness of jointly optimizing deep representations and image clusters towards improved performance in unsupervised learning.",1
"Several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space. The resulting eigenvectors encode the embedding coordinates for the training samples only, and so the embedding of novel data samples requires further costly computation. In this paper, we present a method for the out-of-sample extension of graph embeddings using deep neural networks (DNN) to parametrically approximate these nonlinear maps. Compared with traditional nonparametric out-of-sample extension methods, we demonstrate that the DNNs can generalize with equal or better fidelity and require orders of magnitude less computation at test time. Moreover, we find that unsupervised pretraining of the DNNs improves optimization for larger network sizes, thus removing sensitivity to model selection.",0
"Recent advances have been made towards creating graph embeddings that capture structured relational information among objects represented as nodes within graphs. However, these methods often rely on pre-training on large datasets and do not always scale well across different domains, tasks, or data sizes due to their dependence on specific architectures and loss functions. In addition, many approaches cannot handle out-of-sample extension where new unseen instances need to be embedded without relying on any training data other than the existing knowledge base. This can be challenging since these methods are usually trained to minimize reconstruction error using known pairs or similarities, rather than predicting novel data points accurately from scratch. We propose a novel framework called GOLEM (Generative Out-Of-Sample Learned Embedding Model) which addresses both scalability and generalizability concerns by applying deep generative models such as Diffusion Probabilistic Matrix Factorization (DPMF). Our results show that our model achieves state-of-the-art performance on standard benchmark datasets while enabling fast adaptation across diverse domains and scaling efficiently under limited resources. Furthermore, we demonstrate the effectiveness of our approach by comparing against several strong baselines including Node2Vec, GCNII and others. Our methodology can potentially facilitate various downstream applications like anomaly detection, clustering, classification etc. with improved accuracy and speedup capabilities over existing techniques.",1
"This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.",0
"Advances in deep learning have revolutionized many application domains such as computer vision, natural language processing, and speech recognition. However, most successful models rely on deep neural networks that remain hard to interpret and analyze due to their intrinsic complexity and nonlinear nature. In this context, generative adversarial networks (GANs) represent a promising research direction that enables training deep generative models capable of producing realistic data samples while enforcing certain constraints imposed by discriminators designed to distinguish generated from genuine data samples. The state-of-the-art GAN frameworks still face substantial challenges related to stability, mode collapse, vanishing gradients, and other issues that hinder reliable and efficient training convergence. To address these problems, we present Infogan, which learns interpretable representations through information maximization principle leveraging two coupled GAN architectures working simultaneously towards achieving complementary objectives: firstly, optimizing mutual information between latent codes and generated observations; secondly, ensuring accurate generation quality through conventional GAN loss functions that push each other in opposite directions until they find a Nash equilibrium corresponding to optimal performance trade-off under given computational constraints. Through comprehensive experimental evaluations across multiple benchmark datasets, Infogan demonstrates significant improvements over existing baselines in terms of sample fidelity, latent space coherence, semantic integrity, robustness to hyperparameters settings, and scalability to larger model sizes. Our contributions provide insights into understanding how deep generative models can learn meaningful representations through maximizing information content without explicit supervision. Potentially, thi",1
"Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs and even achieve state-of-the-art results on various tasks, including density estimation, image generation, and missing value imputation.",0
"""Memory"" can refer either to the human ability to store facts and experiences, or the artificial intelligence (AI) ability to use previously seen data during learning. This paper is about improving the latter through deep neural networks (DNNs). We propose several novel training methods that leverage memory in DNNs to improve their performance on a variety of tasks such as image classification, language translation, and question answering. Our first method trains each layer independently using small minibatches from previous layers memories, which we call ""layerwise"". We find that layerwise helps generalization by increasing diversity among subnetworks, which are trained without crosstalk using our new crossover operation. Our second method combines gradient updates from different time steps in a sequence by minimizing prediction difference between them, which we call ""timewarp"". Timewarp encourages internal representations to maintain their coherence over longer periods of sequential inputs. Both layerwise and timewarp improve test accuracy significantly across all tasks compared to baseline models without any external memory access. To further boost performance, we combine both methods into one model called LTM-All, which outperforms existing state of the art on most benchmark datasets.",1
"We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.",0
"Advances in machine learning have made it possible for neural networks (NN) to perform complex tasks such as image classification, speech recognition, natural language processing and more. In recent years there has been growing interest in developing NN models that can handle various kinds of data modalities such as textual, graphical, and multimodal data. These systems are called multi-modal learning frameworks, but they come at the cost of increased computational resources and often lack interpretability which hinders their adoption in several fields. To address these challenges we propose Domain Adversarial training of Neural Networks (DAT). DAT helps by enforcing the model to learn domain-specific features through adversarial loss while allowing for a single model to work across multiple domains, therefore reducing the need for fine tuning for each new task. Our experimentation shows improved accuracy over standard transfer learning methods used today. Additionally our method exhibits greater robustness to input perturbations such as noise or translation. Finally we showcase how interpreting the attention mechanism along with DAT improves on prior state of art results for Natural Language Processing tasks showing promise in understanding the internal functioning of large scale deep learning models. This research paves the way towards enabling human trustworthiness of black box decision making system. Keywords: Multi-Modal Learning, Transfer Learning, Attention Mechanisms, Robustness, Explainable Artificial Intelligence",1
"Cross-domain visual data matching is one of the fundamental problems in many real-world vision tasks, e.g., matching persons across ID photos and surveillance videos. Conventional approaches to this problem usually involves two steps: i) projecting samples from different domains into a common space, and ii) computing (dis-)similarity in this space based on a certain distance. In this paper, we present a novel pairwise similarity measure that advances existing models by i) expanding traditional linear projections into affine transformations and ii) fusing affine Mahalanobis distance and Cosine similarity by a data-driven combination. Moreover, we unify our similarity measure with feature representation learning via deep convolutional neural networks. Specifically, we incorporate the similarity measure matrix into the deep architecture, enabling an end-to-end way of model optimization. We extensively evaluate our generalized similarity model in several challenging cross-domain matching tasks: person re-identification under different views and face verification over different modalities (i.e., faces from still images and videos, older and younger faces, and sketch and photo portraits). The experimental results demonstrate superior performance of our model over other state-of-the-art methods.",0
"This abstract describes the method used in ""Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning"". The authors propose a novel approach that leverages both similarity measure learning and feature representation learning to address cross-domain visual matching. The proposed method first extracts deep features from images across multiple domains using convolutional neural networks (CNN), then learns a generalized similarity measurement by maximizing inter-class separation while minimizing intra-class variation within each domain. Finally, they apply metric learning techniques to learn a transferable feature space where instances can be matched accurately across different domains without relying on individual models trained on specific tasks. Evaluation results demonstrate the effectiveness of their approach compared to state-of-the-art methods in several benchmark datasets. Overall, this work presents a unified framework for tackling challenges faced in real applications such as image search and retrieval, object recognition, and other computer vision problems involving heterogeneous data sources.",1
"We consider the statistical problem of learning common source of variability in data which are synchronously captured by multiple sensors, and demonstrate that Siamese neural networks can be naturally applied to this problem. This approach is useful in particular in exploratory, data-driven applications, where neither a model nor label information is available. In recent years, many researchers have successfully applied Siamese neural networks to obtain an embedding of data which corresponds to a ""semantic similarity"". We present an interpretation of this ""semantic similarity"" as learning of equivalence classes. We discuss properties of the embedding obtained by Siamese networks and provide empirical results that demonstrate the ability of Siamese networks to learn common variability.",0
"This paper presents a novel neural network architecture that combines common variable learning (CVL) and invariant representation learning. CVL has been shown to improve generalization performance by encouraging sharing of features among multiple tasks while preserving task specificity. However, most existing CVL methods focus on transferring knowledge from simpler models to more complex ones which limits their use in deep learning settings where pretraining may already provide good feature extractors. To address this limitation, we introduce Siamese networks into CVL framework such that both models learn representations jointly via contrastive loss functions. We show through experiments on several benchmark datasets that our proposed method achieves state of the art results on various transfer learning scenarios such as domain adaptation and multi-task learning. Furthermore, our model significantly reduces computational cost compared to other competitive approaches since only a single model needs training without maintaining multiple versions as required in ensemble based techniques. Our study highlights the advantages of integrating CVL and Siameses networks for effective transfer learning under varying data distributions.",1
We explore unsupervised representation learning of radio communication signals in raw sampled time series representation. We demonstrate that we can learn modulation basis functions using convolutional autoencoders and visually recognize their relationship to the analytic bases used in digital communications. We also propose and evaluate quantitative met- rics for quality of encoding using domain relevant performance metrics.,0
"In recent years, radio communication signals have gained significant attention from both academia and industry due to their diverse applications such as radar sensing for autonomous vehicles, telecommunications systems, and medical imaging. However, processing these signals requires specialized techniques that rely on domain knowledge which can limit further advances in the field. To overcome these limitations, researchers propose unsupervised representation learning methods for structured radio communication signals that allow for efficient data analysis without relying on prior assumptions about the nature of the underlying signals. This approach uses a deep neural network architecture called Siamese networks which take into account the complex nature of radio communication signals while preserving structure within them. By training on large amounts of raw waveform data collected by real-world radio receivers, the proposed method learns representations of structured signals that outperform traditional approaches in tasks such as anomaly detection, classification, and clustering. Overall, the results demonstrate the potential of using unsupervised representation learning methods to process complex and nuanced data types such as those encountered in structured radio communication signals.",1
"Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random mappings is learned during inference, enabling the transformed outputs to adapt to varying complexity. We prove a universal approximation theorem for the VGP, demonstrating its representative power for learning any model. For inference we present a variational objective inspired by auto-encoders and perform black box inference over a wide class of models. The VGP achieves new state-of-the-art results for unsupervised learning, inferring models such as the deep latent Gaussian model and the recently proposed DRAW.",0
"One of the key goals in machine learning (ML) is developing models that can accurately predict outcomes based on data inputs. Many popular ML techniques rely on explicit model assumptions and prior knowledge, but these may limit their performance in real-world applications where complex relationships exist within the underlying data. In recent years, Bayesian inference has become increasingly popular as a flexible alternative approach. As a result, several models have emerged that incorporate a probabilistic nature into traditional nonlinear regression techniques such as linear and polynomial regression models. These new models attempt to capture complex functional patterns while imposing regularization to prevent overfitting. This study investigates one specific type of Bayesian technique called variational Gaussian processes, which extends classical kernel methods to work directly with latent features instead of raw input data points. By employing a stochastic optimization scheme using Markov Chain Monte Carlo sampling, the method aims to provide accurate predictions at scale by exploiting the representational power of Gaussian process models. Our findings demonstrate significant improvements in both accuracy and efficiency compared to existing kernel methods like support vector machines, decision trees, and random forests across diverse datasets. Moreover, we introduce novel extensions that allow our method to achieve state-of-the-art results for challenging tasks involving missing values and noisy labels. Ultimately, variational Gaussian processes serve as a powerful tool for capturing complex relationships within large datasets without requiring any explicit prior domain knowledge or hand engineering.",1
"The past decade has witnessed the rapid development of feature representation learning and distance metric learning, whereas the two steps are often discussed separately. To explore their interaction, this work proposes an end-to-end learning framework called DARI, i.e. Distance metric And Representation Integration, and validates the effectiveness of DARI in the challenging task of person verification. Given the training images annotated with the labels, we first produce a large number of triplet units, and each one contains three images, i.e. one person and the matched/mismatch references. For each triplet unit, the distance disparity between the matched pair and the mismatched pair tends to be maximized. We solve this objective by building a deep architecture of convolutional neural networks. In particular, the Mahalanobis distance matrix is naturally factorized as one top fully-connected layer that is seamlessly integrated with other bottom layers representing the image feature. The image feature and the distance metric can be thus simultaneously optimized via the one-shot backward propagation. On several public datasets, DARI shows very promising performance on re-identifying individuals cross cameras against various challenges, and outperforms other state-of-the-art approaches.",0
"In recent years, person verification has become increasingly important due to security concerns and privacy issues. However, designing effective person verification systems remains challenging, particularly in unconstrained environments where factors such as illumination variation, pose change, age progression, disguise usage, occlusions, etc., can affect recognition accuracy. To address these challenges, we propose a novel approach that combines distance metrics with representation integration (DARI).  Our method begins by extracting features from images using a deep convolutional neural network (CNN) architecture. These features are then transformed into a joint embedding space where distance metrics are calculated to measure similarity between pairs of samples. Next, we integrate multiple representations of each sample into a compact form by aggregating their distances within a learned linear space. This integrated representation is used to perform verification tasks by comparing the distances between query and gallery samples. Our experiments demonstrate that our method outperforms state-of-the-art methods on several benchmark datasets under varying conditions. We believe that our work represents a significant contribution towards enhancing person verification performance.",1
"Methods from convex optimization such as accelerated gradient descent are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since neural networks are not convex and standard guarantees do not apply. This paper develops the first rigorous link between online convex optimization and error backpropagation on convolutional networks. The first step is to introduce circadian games, a mild generalization of convex games with similar convergence properties. The main result is that error backpropagation on a convolutional network is equivalent to playing out a circadian game. It follows immediately that the waking-regret of players in the game (the units in the neural network) controls the overall rate of convergence of the network. Finally, we explore some implications of the results: (i) we describe the representations learned by a neural network game-theoretically, (ii) propose a learning setting at the level of individual units that can be plugged into deep architectures, and (iii) propose a new approach to adaptive model selection by applying bandit algorithms to choose which players to wake on each round.",0
"We propose ""Deep Online Convex Optimization (DOCO)"" that ensures both the i) online convex optimization framework wherein updates can be incremental on the data as they arrive and ii) deep neural networks for high nonlinear capacity within the framework. In existing literature, these objectives often conflict due to excessive computations of propagating gradients through deep models and vanishing/exploding gradients, which require either small learning rates or special architectures. DOCO solves this issue by utilizing forecasting machines called FPMCs. Since propagation-based regularizations have been introduced in many studies, we clarify how our novel methodology works: given N samples at time t, it trains the model using the first K-2N+K samples, predicts the remaining Delta= N-K samples, uses those predictions as new sample sets to minimize loss functions while excluding old true labels during backpropagation from previous steps, then repeats training step by step until all N samples complete without seeing their actual labels again. With this strategy, FPMCs replace propagated gradients via predictions, enabling efficient online updating in deep learning. For convex risk functions commonly used in practice, our algorithm enjoys faster convergence speed than state-of-the-art algorithms thanks to its adaptively refined batch sizes based on current risks. Experiments covering image classification, natural language processing tasks verify effectiveness and efficiency gains over competitors. This research fills technical gaps and opens up opportunities toward deploying large-scale distributed systems where real-time inference, streaming data ingestion, resource constraints are imposed. However, challenges remain regarding generalizability across different types of convex problems that we leave open for future work. Our implementation will be made publicly available upon acceptance to encourage further development.",1
"Methods from convex optimization are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since modern convolutional networks (convnets), incorporating rectifier units and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. This paper provides the first convergence rates for gradient descent on rectifier convnets. The proof utilizes the particular structure of rectifier networks which consists in binary active/inactive gates applied on top of an underlying linear network. The approach generalizes to max-pooling, dropout and maxout. In other words, to precisely the neural networks that perform best empirically. The key step is to introduce gated games, an extension of convex games with similar convergence properties that capture the gating function of rectifiers. The main result is that rectifier convnets converge to a critical point at a rate controlled by the gated-regret of the units in the network. Corollaries of the main result include: (i) a game-theoretic description of the representations learned by a neural network; (ii) a logarithmic-regret algorithm for training neural nets; and (iii) a formal setting for analyzing conditional computation in neural nets that can be applied to recently developed models of attention.",0
"This is a complex and detailed subject area that may require multiple passes and many revisions before you can come up with something both concise and complete. Don't take it personally if I don't think your first pass meets these criteria; it takes time for ideas to become clear. Instead, try to use my feedback constructively by asking followup questions to clarify specific points. Ultimately, only *you* can decide whether or not your latest version adequately captures key aspects of your research problem! Herein lies the real value of engaging in research... we learn how little control we have over our work, even though we put heart into every single word selection. If nothing else, maybe you can appreciate the irony here. -----  The topic of online convex optimization has garnered significant attention due to its potential applications in various fields such as machine learning, finance, and control systems. In recent years, gated games have emerged as a powerful tool for analyzing online decision making problems involving constraints and incomplete information. However, existing literature on online convex optimization with gated games primarily focuses on static settings where players make decisions simultaneously or one after another without any interdependency. To address this gap in the literature, we develop novel algorithms for deep online convex optimization with gated games, which allow players to update their strategies iteratively based on the history of past actions and observations. Our approach relies on a two-timescale stochastic approximation method combined with the regularized Newton-type updates. We establish new theoretical results characterizing convergence rates under different conditions, including time-varying costs, nonlinear coupling effects, and dynamic feasibility constraints. Extensive numerical experiments across diverse game scenarios demonstrate the effectiveness and efficiency of our proposed methods compared with state-of-the-art approaches. Our findings contribute to the development of robust and efficient solutions for large-scale online convex optimization problems with coupled dynamics and uncertainty.",1
"Quantitatively assessing relationships between latent variables and observed variables is important for understanding and developing generative models and representation learning. In this paper, we propose latent-observed dissimilarity (LOD) to evaluate the dissimilarity between the probabilistic characteristics of latent and observed variables. We also define four essential types of generative models with different independence/conditional independence configurations. Experiments using tractable real-world data show that LOD can effectively capture the differences between models and reflect the capability for higher layer learning. They also show that the conditional independence of latent variables given observed variables contributes to improving the transmission of information and characteristics from lower layers to higher layers.",0
"This paper introduces Latent Dissimilarity (LD), a novel method based on variational autoencoders that measures how different two data points are based on their hidden representations. We evaluate LD across a range of tasks and find that it consistently outperforms several state-of-the-art methods including Triplets loss, Contrastive loss, and Deep InfoMax. Our results show that LD improves robustness to noise, reduces sensitivity to hyperparameters and enhances stability during training by mitigating mode collapse. Additionally, we demonstrate the efficacy of LD across diverse domains: image classification, anomaly detection, and reinforcement learning policy evaluation. These findings suggest that LD offers significant advantages over existing techniques, thus making it well suited for applications where measuring dissimilarities between observed inputs is crucial. By proposing Latent Dissimilarity as an alternative for evaluating differences between observations from complex distributions, such as deep generative models trained using unsupervised objectives, our work contributes towards filling an important gap in understanding latent space discrepancies within the field of machine learning research.",1
"We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.",0
"Multitask representation learning has gained significant attention in recent years due to its ability to improve performance across multiple tasks while reducing model size and computational costs. In this work, we explore the benefits of multitask representation learning through extensive experimentation on real-world datasets. Our results show that multitask representation learning consistently outperforms single task learning in terms of accuracy, robustness, and interpretability. Furthermore, our analysis reveals that the learned representations generalize well across different domains and can lead to better feature discovery. These findings have important implications for natural language processing and computer vision applications where data is scarce and task variations are abundant. Overall, this work demonstrates the efficacy and potential of multitask representation learning as a powerful tool in artificial intelligence research.",1
"This paper proposes a novel approach to person re-identification, a fundamental task in distributed multi-camera surveillance systems. Although a variety of powerful algorithms have been presented in the past few years, most of them usually focus on designing hand-crafted features and learning metrics either individually or sequentially. Different from previous works, we formulate a unified deep ranking framework that jointly tackles both of these key components to maximize their strengths. We start from the principle that the correct match of the probe image should be positioned in the top rank within the whole gallery set. An effective learning-to-rank algorithm is proposed to minimize the cost corresponding to the ranking disorders of the gallery. The ranking model is solved with a deep convolutional neural network (CNN) that builds the relation between input image pairs and their similarity scores through joint representation learning directly from raw image pixels. The proposed framework allows us to get rid of feature engineering and does not rely on any assumption. An extensive comparative evaluation is given, demonstrating that our approach significantly outperforms all state-of-the-art approaches, including both traditional and CNN-based methods on the challenging VIPeR, CUHK-01 and CAVIAR4REID datasets. Additionally, our approach has better ability to generalize across datasets without fine-tuning.",0
"This research proposes a novel method called deep ranking for person re-identification via joint representation learning. By optimizing multi-class distance loss function with a siamese network architecture, our model can learn discriminative features that capture identity-specific patterns from both image sets and human annotations. Experimental results show significant improvements over state-of-the art methods on several benchmark datasets. Our approach provides new insights into person re-identification by explicitly considering within-dataset and across-dataset comparisons through joint optimization. Overall, these contributions make our work highly relevant for advancing computer vision applications such as video surveillance and automated pedestrian tracking.  Paper Title: Deep Ranking for Person Re-identification via Joint Representation Learning  Abstract: This study presents a novel technique called deep ranking for person re-identification using a joint representation learning strategy. In contrast to existing methods based solely on feature extraction or metric learning, we propose a holistic framework that integrates image data, labeled examples, and pairwise relationships derived from similarity functions. To achieve this goal, we adapt a Siamese neural network architecture trained under the guidance of a multi-class softmax cross entropy loss function. Empirical evaluation demonstrates substantial performance gains over current approaches across multiple publicly available datasets. Beyond empirical findings, we provide theoretical interpretations of how our rank pooling scheme mitigates domain discrepancy issues caused by varying environments, lighting conditions, or camera viewpoints. These insights pave the way towards enhancing privacy protection mechanisms and improving overall system reliability in surveillance systems. Given the ubiquity of cameras in today’s society, further improvements in person re-identification may also yield valuable benefits for object tracking and behavior analysis tasks throughout diverse domains.",1
"Most recent work focused on affect from facial expressions, and not as much on body. This work focuses on body affect analysis. Affect does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect classification as well as generation. For this paper we choose Conditional Restricted Boltzmann Machines to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component to become Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art, as well as the generative abilities of our model.",0
"This paper presents a novel approach to action-affect classification and morphing using multi-task representation learning. We propose a deep convolutional neural network architecture that learns representations from multiple tasks simultaneously, including both action recognition and affect estimation. Our method leverages multi-task representation learning by sharing weights across different layers within our model, allowing each task to benefit from knowledge learned during training on other related tasks. In addition, we introduce a new dataset consisting of videos depicting diverse actions performed with varied emotional expressions, providing researchers with a valuable resource for studying these important phenomena. Experimental results demonstrate significant improvements over baseline models trained without shared representations, as well as strong performance compared to state-of-the-art methods on standard benchmark datasets. Furthermore, we showcase how our framework can be used effectively for action-affect morphing, enabling smooth transitions between different facial performances based on specified affective labels. Overall, our work represents a major step forward in the development of robust, high-performance systems for action-affect classification and morphing.",1
"This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.",0
"This paper presents a comparative study of three types of models used in computer vision: decision trees (DT), randomized decision forests (RDF), and convolutional neural networks (CNN). These models were compared using publicly available benchmark datasets and evaluation metrics commonly employed by researchers. Our results show that while CNNs generally outperformed DTs and RDFs on most tasks, there can still be situations where one or both of these methods are preferable due to their ease of interpretability or superiority on certain subtasks. We conclude that further investigation into hybrid models which incorporate the strengths of multiple approaches may lead to improved performance across a wider range of problems.",1
"Unsupervised methods have proven effective for discriminative tasks in a single-modality scenario. In this paper, we present a multimodal framework for learning sparse representations that can capture semantic correlation between modalities. The framework can model relationships at a higher level by forcing the shared sparse representation. In particular, we propose the use of joint dictionary learning technique for sparse coding and formulate the joint representation for concision, cross-modal representations (in case of a missing modality), and union of the cross-modal representations. Given the accelerated growth of multimodal data posted on the Web such as YouTube, Wikipedia, and Twitter, learning good multimodal features is becoming increasingly important. We show that the shared representations enabled by our framework substantially improve the classification performance under both unimodal and multimodal settings. We further show how deep architectures built on the proposed framework are effective for the case of highly nonlinear correlations between modalities. The effectiveness of our approach is demonstrated experimentally in image denoising, multimedia event detection and retrieval on the TRECVID dataset (audio-video), category classification on the Wikipedia dataset (image-text), and sentiment classification on PhotoTweet (image-text).",0
"In recent years, multimodal data analysis has become increasingly important due to advances in sensor technology and the rise of multimedia content on the internet. However, analyzing and processing large amounts of multimodal data remains challenging due to issues such as high computational cost, storage requirements, and lack of scalability. To address these concerns, we propose a novel approach based on sparse representation learning that leverages both collaborative and discriminant information across different modalities. Our method effectively compresses the original high-dimensional data into a low-rank latent space while preserving essential features for downstream tasks. We evaluate our approach using several benchmark datasets, demonstrating improved performance over state-of-the-art methods in terms of efficiency, accuracy, and robustness. Further, we showcase the applicability of our framework by exploring real-world use cases in areas such as image search, surveillance, and healthcare. Overall, our work provides an effective solution for handling multimodal data, paving the way for further research in this rapidly evolving field.",1
"Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, \emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine \emph{generative unsupervised feature learning} with a \emph{probabilistic treatment of oracle information like triplets} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables.",0
"Title: Bayesian Representation Learning under Oracle Constraints  Bayesian methods have been shown to be effective for modeling complex dependencies in data, but they can suffer from computational challenges due to intractable posterior distributions. In this work, we consider a novel approach to Bayesian inference that involves using oracle constraints to improve tractability while preserving desirable aspects of the original problem. We develop a framework that allows us to derive closed form solutions for posterior distributions, even in high dimensions and complicated models, by incorporating prior knowledge into the model through oracle constraints. Our framework provides a principled way to balance the complexity of the model against available data, allowing for flexible and efficient statistical inference. We demonstrate the effectiveness of our method on several challenging problems including image classification, natural language processing, and time series prediction. Overall, our approach paves the way towards more scalable and interpretable machine learning models based on sound probabilistic principles.",1
"Kernel canonical correlation analysis (KCCA) is a nonlinear multi-view representation learning technique with broad applicability in statistics and machine learning. Although there is a closed-form solution for the KCCA objective, it involves solving an $N\times N$ eigenvalue system where $N$ is the training set size, making its computational requirements in both memory and time prohibitive for large-scale problems. Various approximation techniques have been developed for KCCA. A commonly used approach is to first transform the original inputs to an $M$-dimensional random feature space so that inner products in the feature space approximate kernel evaluations, and then apply linear CCA to the transformed inputs. In many applications, however, the dimensionality $M$ of the random feature space may need to be very large in order to obtain a sufficiently good approximation; it then becomes challenging to perform the linear CCA step on the resulting very high-dimensional data matrices. We show how to use a stochastic optimization algorithm, recently proposed for linear CCA and its neural-network extension, to further alleviate the computation requirements of approximate KCCA. This approach allows us to run approximate KCCA on a speech dataset with $1.4$ million training samples and a random feature space of dimensionality $M=100000$ on a typical workstation.",0
"In this work we present a new algorithm for large scale approximate kernel canonical correlation analysis (KCCA). We introduce the kernel trick which allows us to perform all computations in a high dimensional feature space while only storing and operating on very few parameters that define these features in each task’s domain. Our approach enables efficient computation at scale by reducing memory usage as well as computational complexity. We demonstrate our results on several real world datasets, showing that our method produces solutions competitive with exact methods while requiring less than one percent of the time and memory.",1
"Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.",0
"Neural network researchers want to know if they can converge on similar results using different models? We conducted tests to find out how well different models generalize their knowledge. Some common methods were compared to see if they produced comparable answers, or if some approaches performed better than others. Our findings show that while many methods did produce usably accurate results, there was still room for improvement across all models tested. Future studies could look into why these differences occur, allowing researchers to develop more effective training algorithms. -----  This study aimed to investigate whether neural networks trained on the same data set would achieve similar performance, regardless of their architecture and hyperparameters (i.e., convergence). To explore this question, we conducted experiments comparing several commonly used methods for training neural networks and evaluated their ability to generalize to unseen data. Overall, our results showed that while most methods yielded acceptable accuracy, there remained significant variation in performance among the different networks. These results highlight the importance of continued exploration into factors affecting neural network convergence, as refining training procedures has the potential to improve overall model effectiveness. Ultimately, further investigation may lead to the development of more efficient and robust learning algorithms.",1
"This paper strives for video event detection using a representation learned from deep convolutional neural networks. Different from the leading approaches, who all learn from the 1,000 classes defined in the ImageNet Large Scale Visual Recognition Challenge, we investigate how to leverage the complete ImageNet hierarchy for pre-training deep networks. To deal with the problems of over-specific classes and classes with few images, we introduce a bottom-up and top-down approach for reorganization of the ImageNet hierarchy based on all its 21,814 classes and more than 14 million images. Experiments on the TRECVID Multimedia Event Detection 2013 and 2015 datasets show that video representations derived from the layers of a deep neural network pre-trained with our reorganized hierarchy i) improves over standard pre-training, ii) is complementary among different reorganizations, iii) maintains the benefits of fusion with other modalities, and iv) leads to state-of-the-art event detection results. The reorganized hierarchies and their derived Caffe models are publicly available at http://tinyurl.com/imagenetshuffle.",0
"Title: Improving Video Event Detection Using ImageNet Shuffled Convolutions Abstract: Effective video event detection requires efficient processing of visual content which can be computationally expensive and data hungry. To address these limitations, we propose using ImageNet shuffled convolutional filters during pre-training. By rearranging the weights within each filter, we increase randomness while maintaining spatial relationship information. Our method allows for better generalization on challenging datasets and outperforms other state-of-the-art methods by significant margins without the need for additional computational resources. Results demonstrate that our approach effectively captures discriminative features necessary for accurate event recognition even under low lighting conditions and occlusions. This study has important implications for computer vision applications, including surveillance and autonomous driving, where real-time event detection is crucial. Keywords: video event detection, image classification, ImageNet, convolutional neural networks, feature extraction, transfer learning.",1
"Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation.",0
"In medical imaging analysis, understanding disease patterns from large amounts of data plays a critical role in improving diagnostic accuracy and treatment planning. Recently, advancements have been made in computer vision algorithms designed specifically to analyze images obtained through radiological modalities such as Magnetic Resonance Imaging (MRI) and Computed Tomography (CT). This study proposes using a multi-layer representation learning approach combining both clinical knowledge and deep learning techniques for more effective disease detection and diagnosis. Using three separate datasets, each containing MRI scans of patients diagnosed with mild Traumatic Brain Injury (mTBI), Alzheimer’s Disease (AD), or Multiple Sclerosis (MS), we demonstrate that our method can successfully identify patterns associated with these conditions by training on large datasets and evaluating performance against existing baseline methods. By fusing prior domain knowledge into deep neural networks, our results show improved classification accuracy compared to traditional single layer architectures alone. Furthermore, we discuss potential applications of multi-layer representation learning approaches beyond brain diseases, including tumor segmentation, joint disease identification, and neurodevelopmental disorders like Autism Spectrum Disorder (ASD). Overall, our findings suggest promise in incorporating richer representations of contextual features into deep models for enhanced medical image analysis outcomes across multiple domains. ---",1
"We explore the question of whether the representations learned by classifiers can be used to enhance the quality of generative models. Our conjecture is that labels correspond to characteristics of natural data which are most salient to humans: identity in faces, objects in images, and utterances in speech. We propose to take advantage of this by using the representations from discriminative classifiers to augment the objective function corresponding to a generative model. In particular we enhance the objective function of the variational autoencoder, a popular generative model, with a discriminative regularization term. We show that enhancing the objective function in this way leads to samples that are clearer and have higher visual quality than the samples from the standard variational autoencoders.",0
"Abstract: Recent advancements in generative models have shown great promise in generating realistic images, text, and other types of data. However, these methods often struggle with generating coherent outputs that match their inputs well. This problem can be alleviated by using discriminators, which aim to distinguish generated samples from real ones. In this work, we propose a new regularization method called discriminator-guided adversarial training (DGAT) that uses multiple discriminators trained on different tasks. We demonstrate that DGAT improves the quality of generated samples by making them more consistent with their corresponding inputs. Our experiments show significant improvements over baseline methods across several benchmark datasets including image synthesis, language generation, and reinforcement learning problems. By imposing constraints on the generator via discriminators, our framework provides a simple yet effective solution towards better alignment between generated outputs and input distributions. Keywords: Discrmininators; Adversarial Training; Generative models",1
"Canonical correlation analysis (CCA) is a classical representation learning technique for finding correlated variables in multi-view data. Several nonlinear extensions of the original linear CCA have been proposed, including kernel and deep neural network methods. These approaches seek maximally correlated projections among families of functions, which the user specifies (by choosing a kernel or neural network structure), and are computationally demanding. Interestingly, the theory of nonlinear CCA, without functional restrictions, had been studied in the population setting by Lancaster already in the 1950s, but these results have not inspired practical algorithms. We revisit Lancaster's theory to devise a practical algorithm for nonparametric CCA (NCCA). Specifically, we show that the solution can be expressed in terms of the singular value decomposition of a certain operator associated with the joint density of the views. Thus, by estimating the population density from data, NCCA reduces to solving an eigenvalue system, superficially like kernel CCA but, importantly, without requiring the inversion of any kernel matrix. We also derive a partially linear CCA (PLCCA) variant in which one of the views undergoes a linear projection while the other is nonparametric. Using a kernel density estimate based on a small number of nearest neighbors, our NCCA and PLCCA algorithms are memory-efficient, often run much faster, and perform better than kernel CCA and comparable to deep CCA.",0
"One approach for analyzing relationships between two multivariate datasets is canonical correlation analysis (CCA). In CCA, linear combinations of variables from each dataset are constructed so that their cross-covariance matrix has as large a value as possible. This procedure results in a set of canonical variates or canonical coefficients which are used to predict one dataset based on the other. However, the traditional formulation of CCA requires that both matrices have the same number of observations, which may not always be the case. To handle this situation, nonparametric techniques such as kernel density estimation can be employed. By using these methods, we can construct smooth approximations of the joint distribution of the data, allowing us to compute a kernelized version of the cross-covariance matrix. This leads naturally to the development of a novel type of correspondence analysis called nonparametric canonical correlation analysis. We provide theoretical guarantees showing the consistency of our estimators and illustrate the method through applications to synthetic and real data sets. Our simulations demonstrate that the proposed method provides accurate estimates under a wide variety of scenarios and outperforms competitors in some cases where the underlying densities exhibit distinct characteristics. Finally, we discuss several potential extensions to enhance the flexibility and power of the new approach.",1
"Representation learning is the foundation for the recent success of neural network models. However, the distributed representations generated by neural networks are far from ideal. Due to their highly entangled nature, they are di cult to reuse and interpret, and they do a poor job of capturing the sparsity which is present in real- world transformations. In this paper, I describe methods for learning disentangled representations in the two domains of graphics and computation. These methods allow neural methods to learn representations which are easy to interpret and reuse, yet they incur little or no penalty to performance. In the Graphics section, I demonstrate the ability of these methods to infer the generating parameters of images and rerender those images under novel conditions. In the Computation section, I describe a model which is able to factorize a multitask learning problem into subtasks and which experiences no catastrophic forgetting. Together these techniques provide the tools to design a wide range of models that learn disentangled representations and better model the factors of variation in the real world.",0
"In recent years, there has been significant progress in developing machine learning algorithms that can learn from large datasets and make accurate predictions on new data. However, many of these models still struggle with understanding complex relationships between variables and generating interpretable representations of data. To address this challenge, researchers have explored using disentangled representations in neural models. These representations aim to capture different aspects of the underlying structure of the data in a transparent and meaningful manner, allowing for better interpretability and generalization performance. This paper proposes a novel method for creating disentangled representations in deep generative models by integrating insights from probabilistic programming and variational inference into the training process. We evaluate our approach on several benchmark datasets and demonstrate significantly improved performance compared to state-of-the-art methods. Our results show the potential benefits of incorporating disentanglement criteria into the model design process, paving the way for more explainable and reliable machine learning systems.",1
"We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for learning while only one view is available for downstream tasks. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a batch-style correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them empirically on image, speech, and text tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE). We also explore a stochastic optimization procedure for minibatch correlation-based objectives and discuss the time/performance trade-offs for kernel-based and neural network-based implementations.",0
"Abstract: Many researchers in the field of deep learning have recently turned their attention towards developing algorithms that can learn from multiple sources of data simultaneously, rather than just relying on one source at a time. This approach has been shown to lead to more robust representations, as well as improved performance on downstream tasks. In this paper, we examine the different objectives and optimization techniques used in multi-view representation learning, and provide both a comprehensive review of existing approaches and some new contributions of our own. We focus particularly on two popular methods, namely contrastive learning and collaborative training, and explore how they can be adapted to work in combination with each other and other common architectures such as Generative Adversarial Networks (GAN) and autoencoders. Through experimental evaluations using three diverse datasets – MNIST, CelebA, and Places – we showcase the benefits of combining these models and demonstrate state-of-the-art results across all three benchmarks, outperforming most previously published works. Our findings suggest that the use of multi-view representation learning could become increasingly important in future machine learning research as the volume and variety of available datasets continues to grow.",1
"Disentangled distributed representations of data are desirable for machine learning, since they are more expressive and can generalize from fewer examples. However, for complex data, the distributed representations of multiple objects present in the same input can interfere and lead to ambiguities, which is commonly referred to as the binding problem. We argue for the importance of the binding problem to the field of representation learning, and develop a probabilistic framework that explicitly models inputs as a composition of multiple objects. We propose an unsupervised algorithm that uses denoising autoencoders to dynamically bind features together in multi-object inputs through an Expectation-Maximization-like clustering process. The effectiveness of this method is demonstrated on artificially generated datasets of binary images, showing that it can even generalize to bind together new objects never seen by the autoencoder during training.",0
"In recent years, deep learning has become increasingly popular due to its ability to achieve state-of-the-art results on many challenging problems in computer vision. One common approach used by these models is convolutional neural networks (CNNs), which have been shown to excel at tasks such as object detection, segmentation, and classification. However, despite their successes, these models can suffer from several limitations, including limited interpretability and lack of robustness against input perturbations.  In order to address these issues, we propose a new method called binding via reconstruction clustering (BRC). Our approach involves training two separate CNNs: one that takes images directly as inputs and produces class probabilities, and another that takes the activation maps generated by the first network as inputs and reconstructs the original image. We then apply spectral clustering to group pixels into regions corresponding to different objects present in the scene. By doing so, BRC allows us to obtain interpretable outputs that are easy to visualize, and can provide insights into how the model makes predictions. Additionally, BRC achieves improved robustness against adversarial examples compared to traditional methods based on regularization techniques alone. Experimental evaluations demonstrate that our framework achieves competitive performance on standard benchmark datasets while providing more interpretable and robust outputs than baseline methods.",1
"This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.",0
"This work presents a novel approach to unsupervised representation learning from visual data that relies on predicting missing contextual information rather than self-supervision mechanisms such as contrastive loss or adversarial training objectives. We show that our method can learn high quality representations that significantly outperform previous approaches on several benchmark datasets across a variety of downstream tasks, including image classification, semantic segmentation, and object detection. Our key insight is that maximizing prediction performance can drive meaningful feature extraction, even without explicit supervision. By formulating the problem as a denoising autoencoder task, we ensure robustness against input perturbations which enables more generalizable learned features. Overall, our results demonstrate significant improvements over state-of-the-art methods, highlighting the effectiveness of our proposed approach for solving challenging vision problems under limited label guidance.",1
"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",0
"This paper presents a novel unsupervised representation learning method using deep convolutional generative adversarial networks (DCGANs). DCGANs have shown promising results in generating realistic images but their use for unsupervised feature learning has been limited due to difficulties in training stability and mode collapse. We propose a new framework that stabilizes the training process and improves generation quality by introducing a novel auxiliary loss function based on autoencoder reconstruction error. Our approach achieves state-of-the-art performance on several benchmark datasets for unsupervised representation learning tasks such as colorization, super resolution, and texture synthesis. Additionally, we demonstrate the effectiveness of our model in downstream semantic segmentation and object detection tasks. Overall, our work highlights the potential of unsupervised representation learning with deep generative models for high-level vision tasks.",1
"Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Knowledge graphs are typically treated as static: A knowledge graph grows more links when more facts become available but the ground truth values associated with links is considered time invariant. In this paper we address the issue of knowledge graphs where triple states depend on time. We assume that changes in the knowledge graph always arrive in form of events, in the sense that the events are the gateway to the knowledge graph. We train an event prediction model which uses both knowledge graph background information and information on recent events. By predicting future events, we also predict likely changes in the knowledge graph and thus obtain a model for the evolution of the knowledge graph as well. Our experiments demonstrate that our approach performs well in a clinical application, a recommendation engine and a sensor network application.",0
"In recent years, there has been significant interest in using knowledge graphs (KGs) as a means of organizing and retrieving structured data from large corpora such as text documents, images, videos and audio files. One key challenge associated with KG construction lies in how to represent events that occur over time within these graph structures. Existing approaches typically either focus on modeling temporal relationships via static edges connecting node pairs, or rely upon additional resources like calendars or external metadata sources to provide event context information. While these methods have proven effective, they may struggle when confronted with complex situations where multiple events co-evolve simultaneously according to interrelated cause-and-effect dependencies, making it difficult to accurately capture all relevant details without compromising efficiency or scalability. This study addresses these shortcomings by proposing a novel solution capable of predicting the mutual evolution of events in realtime across interdependent domains. Our approach involves building dynamic causal models reflecting the underlying physics governing each specific domain, then inferring corresponding parameter settings through deep learning techniques. We demonstrate via numerous experiments that our method outperforms alternative state-of-the art solutions while providing more accurate predictions for diverse applications ranging from weather forecasting to financial market analysis.",1
"Salient object detection increasingly receives attention as an important component or step in several pattern recognition and image processing tasks. Although a variety of powerful saliency models have been intensively proposed, they usually involve heavy feature (or model) engineering based on priors (or assumptions) about the properties of objects and backgrounds. Inspired by the effectiveness of recently developed feature learning, we provide a novel Deep Image Saliency Computing (DISC) framework for fine-grained image saliency computing. In particular, we model the image saliency from both the coarse- and fine-level observations, and utilize the deep convolutional neural network (CNN) to learn the saliency representation in a progressive manner. Specifically, our saliency model is built upon two stacked CNNs. The first CNN generates a coarse-level saliency map by taking the overall image as the input, roughly identifying saliency regions in the global context. Furthermore, we integrate superpixel-based local context information in the first CNN to refine the coarse-level saliency map. Guided by the coarse saliency map, the second CNN focuses on the local context to produce fine-grained and accurate saliency map while preserving object details. For a testing image, the two CNNs collaboratively conduct the saliency computing in one shot. Our DISC framework is capable of uniformly highlighting the objects-of-interest from complex background while preserving well object details. Extensive experiments on several standard benchmarks suggest that DISC outperforms other state-of-the-art methods and it also generalizes well across datasets without additional training. The executable version of DISC is available online: http://vision.sysu.edu.cn/projects/DISC.",0
"Automatic image saliency detection has been widely studied due to its importance in many applications such as image/video compression, object recognition, content-aware image resizing, advertisement selection, and so on. Traditional methods mostly rely on handcrafted features and heuristics which have limited representation capability and suffer from poor generalization ability when facing complex scenes containing diverse objects with clutter backgrounds, multiple occlusions and varying scales/orientations. This paper presents DISC (Deep Image Saliency Computing), a novel deep learning framework that can effectively learn discriminative representations by progressively discovering multi-scale feature hierarchies from the input images. Unlike most existing works that predict dense saliency maps pixel-by-pixel based on their individual feature extraction, our method explores contextual interactions across different spatial locations adaptively to generate more accurate predictions. Our main contributions include: 1) we propose a new network architecture that progressively learns global and local representations at each stage; 2) We develop an attention mechanism to dynamically select informative regions and aggregate them with corresponding weights for high level semantic reasoning; 3) Extensive experiments conducted on six challenging benchmark datasets show superior performance against state-of-the art approaches in terms of prediction accuracy and computational efficiency. In addition, we demonstrate promising results when applied to real-world scenarios like gaze estimation, visual retargeting, and video summarization.",1
"In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. Such relationship is not limited to object of the target category, but also includes surrounding objects of other categories. We show that image regions with multiple contexts and multiple rotations are effective in capturing such relationship during the representation learning process and in handling the semantic and visual variation caused by different window-object configurations. Experimental results show that the representation learned by our approach can improve the object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved by our single model and it is the best among published results. On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute mAP.",0
"Deep learning approaches have demonstrated promising results on generic object detection tasks by jointly learning feature representations and bounding box predictions in an end-to-end fashion. However, directly predicting bounding boxes from high-level semantic features can result in ambiguous localization due to the lack of spatial constraints. In this paper, we present a new framework that leverages the relationship between detected objects and image windows to guide representation learning for more accurate object detections. We propose a novel window-object embedding which explicitly models the contextual correlation between objects and their corresponding image regions. This allows us to effectively integrate both global semantic information and fine-grained spatial cues into our model. Extensive experiments on popular benchmarks such as COCO and Pascal VOC demonstrate that our proposed method outperforms state-of-the-art methods under similar settings while running at real-time speeds. Our work highlights the importance of considering object-window relationships for robust object detection in complex scenarios.",1
"We introduce a framework for analyzing transductive combination of Gaussian process (GP) experts, where independently trained GP experts are combined in a way that depends on test point location, in order to scale GPs to big data. The framework provides some theoretical justification for the generalized product of GP experts (gPoE-GP) which was previously shown to work well in practice but lacks theoretical basis. Based on the proposed framework, an improvement over gPoE-GP is introduced and empirically validated.",0
"A new method has been proposed for aggregating expert opinions on complex problems using Gaussian processes (GPs). The approach utilizes transduction and log opining techniques to improve GP accuracy by incorporating domain knowledge from experts with varying levels of training data availability. This novel combination allows for better predictions even when only small amounts of training data are available. The proposed method outperforms other state-of-the-art methods across multiple datasets and can have significant impact in fields such as medicine and finance where accurate prediction models could greatly benefit society. Overall, the results showcase the effectiveness of combining transduction and log opinion pooling of Gaussian process experts for improved performance over existing approaches.",1
"Complex-valued neural networks (CVNNs) are an emerging field of research in neural networks due to their potential representational properties for audio, image, and physiological signals. It is common in signal processing to transform sequences of real values to the complex domain via a set of complex basis functions, such as the Fourier transform. We show how CVNNs can be used to learn complex representations of real valued time-series data. We present methods and results using a framework that can compose holomorphic and non-holomorphic functions in a multi-layer network using a theoretical result called the Wirtinger derivative. We test our methods on a representation learning task for real-valued signals, recurrent complex-valued networks and their real-valued counterparts. Our results show that recurrent complex-valued networks can perform as well as their real-valued counterparts while learning filters that are representative of the domain of the data.",0
"Abstract: This work investigates the use of complex-valued neural networks (CVNNs) for representation learning tasks. CVNNs have been shown to possess certain advantages over their real-valued counterparts, such as improved numerical stability and better performance in some cases. However, there has been limited research on using CVNNs for representation learning tasks, which can involve high-dimensional data sets and nonlinear relationships. We propose several architectures that incorporate complex convolutional layers and demonstrate their effectiveness through experimentation on benchmark datasets. Our results show that CVNNs are capable of achieving comparable or even superior performance compared to traditional methods, particularly in situations where the dataset exhibits characteristics well suited to complex arithmetic. This study contributes new insights into the use of CVNNs for representation learning and provides guidance for future research in this area.",1
"This paper presents Pinterest Related Pins, an item-to-item recommendation system that combines collaborative filtering with content-based ranking. We demonstrate that signals derived from user curation, the activity of users organizing content, are highly effective when used in conjunction with content-based ranking. This paper also demonstrates the effectiveness of visual features, such as image or object representations learned from convnets, in improving the user engagement rate of our item-to-item recommendation system.",0
"Abstract: With the rise of social media platforms like Pinterest, item-to-item recommendations have become increasingly important to provide users with personalized content that meets their interests. Traditionally, these recommendations were generated using collaborative filtering techniques such as matrix factorization, which rely heavily on user ratings and explicit feedback data. Recently, deep learning methods, especially convolutional neural networks (ConvNets), have gained significant attention due to their ability to capture nonlinear patterns from raw image data. In this study, we explore how human curation can complement ConvNet features to enhance item-to-item recommendation performance on Pinterest. We introduce a novel hybrid approach by combining three sets of features: (i) visual features learned by a pretrained ConvNet model, (ii) latent factors extracted from user behavior logs via a collaborative filtering algorithm, and (iii) manual annotations provided by expert curators who label items according to specific themes. Our experimental results demonstrate that incorporating human curation leads to better overall performance compared to solely relying on either computer vision or collaborative filtering approaches. Moreover, our analysis shows that different types of features contribute differently to recommendation accuracy under varying conditions, suggesting that an appropriate weighting mechanism should be designed based on various contextual factors. Overall, our findings highlight the importance of balancing automation and human intervention in designing efficient recommendation systems on Pinterest and other similar platforms.",1
"Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.",0
"""Texture synthesis has been a challenging task in computer graphics due to the complexity involved in generating high-quality textures that match existing ones. In recent years, convolutional neural networks (CNN) have emerged as powerful tools for texture synthesis owing to their ability to learn complex patterns from large datasets. This research presents a novel approach to texture synthesis using CNNs, which produces state-of-the-art results in terms of visual fidelity and quality. Our methodology leverages a two-stage process involving pre-training followed by fine-tuning on a specific dataset. During pre-training, we use the images generated during training to create new sets of data based on the differences between real images and the generated outputs. This allows us to refine our model before moving into the second stage, where we fine-tune the network using the target dataset. Through extensive experiments on several benchmark datasets, we demonstrate that our approach outperforms other methods for texture synthesis based on visual metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and perceptual loss measures like LPIPS. Furthermore, we showcase some applications of our proposed method in image generation tasks like single image super-resolution (SISR), video frame interpolation (VFI), and style transfer.""",1
"Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.",0
"This should be an informative abstract that can be used as either a standalone summary for those interested in the general idea behind your paper or as preliminary text for introducing the paper at a conference or seminar, so focus on conveying the main results and methodology rather than details about the authors. You should provide enough detail about your work without going into excessive technical depth or jargon. If you have any questions please feel free to ask me, I would love some more specific guidance on how to approach writing an effective scientific abstract!",1
"We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoid drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units.",0
"Artificial intelligence (AI) has been used in many fields, from image recognition and natural language processing to robotics and computer vision. One area where AI can make a significant impact is human dynamics modeling, which involves understanding how individuals interact with each other and their environment over time. Recurrent neural networks (RNNs) have proven to be effective models for this task because they allow for sequence data input and output, making them well suited for human behavior analysis. In our study, we propose using RNN models as a framework to predict future actions based on past interactions within social groups. We evaluate the accuracy of these models by testing them against real-world datasets and comparing their predictions to actual outcomes. Our findings demonstrate that RNN models can provide valuable insights into human dynamics and improve decision-making in areas such as sociology, economics, psychology, and public policy. Overall, our research contributes to the growing field of AI applications for studying complex systems, providing new opportunities for advancing knowledge in various domains.",1
"Several recent approaches showed how the representations learned by Convolutional Neural Networks can be repurposed for novel tasks. Most commonly it has been shown that the activation features of the last fully connected layers (fc7 or fc6) of the network, followed by a linear classifier outperform the state-of-the-art on several recognition challenge datasets. Instead of recognition, this paper focuses on the image retrieval problem and proposes a examines alternative pooling strategies derived for CNN features. The presented scheme uses the features maps from an earlier layer 5 of the CNN architecture, which has been shown to preserve coarse spatial information and is semantically meaningful. We examine several pooling strategies and demonstrate superior performance on the image retrieval task (INRIA Holidays) at the fraction of the computational cost, while using a relatively small memory requirements. In addition to retrieval, we see similar efficiency gains on the SUN397 scene categorization dataset, demonstrating wide applicability of this simple strategy. We also introduce and evaluate a novel GeoPlaces5K dataset from different geographical locations in the world for image retrieval that stresses more dramatic changes in appearance and viewpoint.",0
"This task requires a deep understanding of convolutional neural networks (CNNs) for image retrieval, as well as experience with object recognition and scene classification tasks. In addition, familiarity with relevant literature and research related to CNN architectures such as VGGNet, ResNet, and MobileNets is important. The ability to analyze and interpret results from these systems is essential. If you need assistance writing your paper, please keep reading!",1
"In this paper we propose a strategy for semi-supervised image classification that leverages unsupervised representation learning and co-training. The strategy, that is called CURL from Co-trained Unsupervised Representation Learning, iteratively builds two classifiers on two different views of the data. The two views correspond to different representations learned from both labeled and unlabeled data and differ in the fusion scheme used to combine the image features. To assess the performance of our proposal, we conducted several experiments on widely used data sets for scene and object recognition. We considered three scenarios (inductive, transductive and self-taught learning) that differ in the strategy followed to exploit the unlabeled data. As image features we considered a combination of GIST, PHOG, and LBP as well as features extracted from a Convolutional Neural Network. Moreover, two embodiments of CURL are investigated: one using Ensemble Projection as unsupervised representation learning coupled with Logistic Regression, and one based on LapSVM. The results show that CURL clearly outperforms other supervised and semi-supervised learning methods in the state of the art.",0
"This paper presents a novel approach to unsupervised representation learning called CURL (Co-trained Unsupervised Representation Learning). Traditional approaches to unsupervised representation learning involve training models on large amounts of data without any supervision, which can be difficult due to high computational costs and slow convergence rates. In contrast, CURL employs co-training techniques that leverage multiple views of the same dataset to learn robust representations more efficiently. Through extensive experiments across several benchmark datasets, we show that CURL outperforms state-of-the-art baselines in terms of accuracy and efficiency. We believe that our work represents an important step forward in advancing the field of unsupervised representation learning, enabling new applications in computer vision and other domains.",1
"Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for both linear SVMs and the nonlinear extension with latent representation learning. For linear SVMs, to deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights are analytically updated. For nonlinear latent SVMs, we consider learning one layer of latent representations in SVMs and extend the data augmentation technique in conjunction with first-order Taylor-expansion to deal with the intractable expected non-smooth hinge loss and the nonlinearity of latent representations. Finally, we apply the similar data augmentation ideas to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions, and we further develop a non-linear extension of logistic regression by incorporating one layer of latent representations. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of both linear and nonlinear SVMs. In addition, the nonlinear SVMs further improve the prediction performance on several image datasets.",0
"Title: Dropout Training for Support Vector Machines with Data Augmentation Abstract Machine learning models have become increasingly popular due to their ability to handle large amounts of data and make accurate predictions. However, training these models can be computationally expensive and time consuming. One approach that has been used to improve model accuracy while reducing computational cost is dropout training. In this work, we investigate how combining dropout training with data augmentation can further improve model performance on image classification tasks using support vector machines (SVM). We show that our proposed method outperforms standard SVMs as well as other state-of-the-art methods on several benchmark datasets. Furthermore, we demonstrate the efficacy of our method by applying it to real-world applications such as medical imaging and facial recognition. Our results highlight the potential benefits of incorporating regularization techniques like dropout into machine learning pipelines to achieve high levels of accuracy at reduced computational costs. Keywords:Dropout, Support Vector Machines, Data Augmentation, Image Classification",1
"Evidence is mounting that Convolutional Networks (ConvNets) are the most effective representation learning method for visual recognition tasks. In the common scenario, a ConvNet is trained on a large labeled dataset (source) and the feed-forward units activation of the trained network, at a certain layer of the network, is used as a generic representation of an input image for a task with relatively smaller training set (target). Recent studies have shown this form of representation transfer to be suitable for a wide range of target visual recognition tasks. This paper introduces and investigates several factors affecting the transferability of such representations. It includes parameters for training of the source ConvNet such as its architecture, distribution of the training data, etc. and also the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. Then, by optimizing these factors, we show that significant improvements can be achieved on various (17) visual recognition tasks. We further show that these visual recognition tasks can be categorically ordered based on their distance from the source task such that a correlation between the performance of tasks and their distance from the source task w.r.t. the proposed factors is observed.",0
"This paper explores the transferability of generic convolutional neural network (ConvNet) representations across different tasks and datasets. While recent advances have shown that pre-trained ConvNets can improve performance on downstream tasks when fine-tuned, little is known about which factors contribute to successful transfer learning. By conducting experiments on image classification, object detection, and semantic segmentation problems, we aim to identify key properties of the training data, architecture design choices, and task characteristics that affect the translatability of ConvNets. Our findings provide insights into how practitioners should approach building and using these models and highlight promising directions for future research.",1
"We address the problem of communicating domain knowledge from a user to the designer of a clustering algorithm. We propose a protocol in which the user provides a clustering of a relatively small random sample of a data set. The algorithm designer then uses that sample to come up with a data representation under which $k$-means clustering results in a clustering (of the full data set) that is aligned with the user's clustering. We provide a formal statistical model for analyzing the sample complexity of learning a clustering representation with this paradigm. We then introduce a notion of capacity of a class of possible representations, in the spirit of the VC-dimension, showing that classes of representations that have finite such dimension can be successfully learned with sample size error bounds, and end our discussion with an analysis of that dimension for classes of representations induced by linear embeddings.",0
"This paper presents a statistical framework for clustering based on representation learning. We propose that clustering algorithms can benefit from explicitly modeling the distribution of data features as well as their shared patterns. To achieve this, we develop novel formulations for density estimation and clustering that incorporate prior knowledge about the relationship between these factors. Our framework uses probabilistic models which are trained jointly with deep neural networks, enabling us to capture complex representations while maintaining interpretability. Experimental results demonstrate the effectiveness of our approach across diverse benchmark datasets in comparison to state-of-the-art methods. Furthermore, our method provides insights into how the learned clusters correspond to meaningful groupings of underlying variables, allowing researchers to gain more insight into their data.",1
"Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art.",0
"In recent years, deep learning has emerged as a powerful tool for building recommender systems that can accurately predict user preferences and provide personalized recommendations. However, these models often rely on large amounts of data and computational resources, making them difficult to deploy and maintain. This paper presents a collaborative approach to deep learning for recommender systems, where multiple institutions work together to share their datasets and jointly train a model that benefits all parties involved. By pooling resources and knowledge, we demonstrate how such collaborative efforts can lead to more efficient use of computing resources and better performance on benchmark tasks. We propose a novel framework based on federated learning, which allows each institution to retain control over their own data while still contributing to the collective goal of improving recommendation quality. Our experiments show that our method outperforms standard training approaches in terms of both accuracy and efficiency, demonstrating the potential value of collaboration in machine learning research. Overall, this work represents an important step towards building scalable and effective recommendation systems using deep learning techniques.",1
"Auto-encoders are perhaps the best-known non-probabilistic methods for representation learning. They are conceptually simple and easy to train. Recent theoretical work has shed light on their ability to capture manifold structure, and drawn connections to density modelling. This has motivated researchers to seek ways of auto-encoder scoring, which has furthered their use in classification. Gated auto-encoders (GAEs) are an interesting and flexible extension of auto-encoders which can learn transformations among different images or pixel covariances within images. However, they have been much less studied, theoretically or empirically. In this work, we apply a dynamical systems view to GAEs, deriving a scoring function, and drawing connections to Restricted Boltzmann Machines. On a set of deep learning benchmarks, we also demonstrate their effectiveness for single and multi-label classification.",0
"""This""paper presents a novel method for scoring and classifying data using gated auto-encoders. Gated auto-encoders are neural networks that have been trained to reconstruct their inputs while imposing a bottleneck layer that compresses the input into a lower dimensional representation. We propose using these compressed representations as features for downstream classification tasks. Our experiments on several benchmark datasets demonstrate that our approach outperforms state-of-the-art feature extraction methods by achieving higher accuracy on challenging classification problems. Additionally, we investigate the effectiveness of different attention mechanisms in improving performance and show that they can significantly boost classification accuracy, especially on imbalanced datasets where the positive examples are rare. Overall, our results highlight the potential of gated auto-encoders as powerful feature extractors for scoring and classifying complex data.",1
"Sketch-based face recognition is an interesting task in vision and multimedia research, yet it is quite challenging due to the great difference between face photos and sketches. In this paper, we propose a novel approach for photo-sketch generation, aiming to automatically transform face photos into detail-preserving personal sketches. Unlike the traditional models synthesizing sketches based on a dictionary of exemplars, we develop a fully convolutional network to learn the end-to-end photo-sketch mapping. Our approach takes whole face photos as inputs and directly generates the corresponding sketch images with efficient inference and learning, in which the architecture are stacked by only convolutional kernels of very small sizes. To well capture the person identity during the photo-sketch transformation, we define our optimization objective in the form of joint generative-discriminative minimization. In particular, a discriminative regularization term is incorporated into the photo-sketch generation, enhancing the discriminability of the generated person sketches against other individuals. Extensive experiments on several standard benchmarks suggest that our approach outperforms other state-of-the-art methods in both photo-sketch generation and face sketch verification.",0
"In recent years there has been increasing interest in developing techniques that can generate images based on text descriptions. One approach to doing so involves training deep neural networks on pairs of text descriptions and corresponding photos, such as sketches or paintings that attempt to capture the essence of each description. Examples might include generating a photo of a car from a description like ""red sedan,"" or creating a drawing of a person holding a guitar given the phrase ""a woman playing music."" These systems typically involve two stages: first converting text into a set of semantic concepts and then using those concepts to synthesize a new image. In this work we propose an alternative end-to-end framework for photo-sketch generation that directly maps textual input to a visual representation, without any intermediate step involving concept identification. Our method is based on fully convolutional representation learning, which allows us to train our network on patch-level supervision rather than relying on whole images at once. We demonstrate through experiments that our system is able to produce high quality results across a range of tasks, including both sketching and painting styles, and outperform previous methods in terms of both quantitative metrics and human evaluation. Overall, our approach offers a significant improvement over existing techniques for generating images from text descriptions, opening up exciting possibilities for applications in areas such as creative design, art production, and communication.",1
"We present a representation learning method that learns features at multiple different levels of scale. Working within the unsupervised framework of denoising autoencoders, we observe that when the input is heavily corrupted during training, the network tends to learn coarse-grained features, whereas when the input is only slightly corrupted, the network tends to learn fine-grained features. This motivates the scheduled denoising autoencoder, which starts with a high level of noise that lowers as training progresses. We find that the resulting representation yields a significant boost on a later supervised task compared to the original input, or to a standard denoising autoencoder trained at a single noise level. After supervised fine-tuning our best model achieves the lowest ever reported error on the CIFAR-10 data set among permutation-invariant methods.",0
"This paper presents a novel approach for training neural networks to generate high quality images by utilizing scheduled denoising autoencoders (SDAs). Traditional autoencoder architectures aim to minimize reconstruction error as a means of learning a robust representation of input data. However, these models often suffer from poor generalization performance due to their reliance on noise-free inputs during training. SDAs address this limitation by periodically corrupting the inputs with noise during training, forcing the model to learn a more resilient encoding that can tolerate missing or erroneous information. Our experiments demonstrate that SDAs outperform state-of-the-art autoencoder baselines across a variety of image generation tasks, achieving higher fidelity outputs while requiring fewer computational resources. Additionally, we showcase the versatility of our method by applying it to several different architectures, including variational autoencoders (VAEs) and generative adversarial networks (GANs), further validating its effectiveness across a range of deep learning frameworks. Overall, the results presented in this work highlight the significant potential of SDAs for improving the performance of unsupervised representation learning algorithms in computer vision and other domains.",1
"Representation learning is currently a very hot topic in modern machine learning, mostly due to the great success of the deep learning methods. In particular low-dimensional representation which discriminates classes can not only enhance the classification procedure, but also make it faster, while contrary to the high-dimensional embeddings can be efficiently used for visual based exploratory data analysis.   In this paper we propose Maximum Entropy Linear Manifold (MELM), a multidimensional generalization of Multithreshold Entropy Linear Classifier model which is able to find a low-dimensional linear data projection maximizing discriminativeness of projected classes. As a result we obtain a linear embedding which can be used for classification, class aware dimensionality reduction and data visualization. MELM provides highly discriminative 2D projections of the data which can be used as a method for constructing robust classifiers.   We provide both empirical evaluation as well as some interesting theoretical properties of our objective function such us scale and affine transformation invariance, connections with PCA and bounding of the expected balanced accuracy error.",0
"In order to create effective machine learning models, we need to first identify suitable features that capture relevant characteristics of our data. This process can be difficult due to both high dimensionality and large scale of many datasets. However, linear methods have been shown to effectively reduce dimensions while preserving important relationships within the data, such as sparsity, nonnegativity, and low rank structures. We propose using maximum entropy linear manifold (MELM) techniques to learn discriminative low-dimensional representations of complex datasets. MELM is derived from latent semantic analysis by replacing the singular value decomposition with a matrix logarithm operation. By optimizing MELM objectives, we seek to maximize the separability across classes and preserve the intrinsic geometric structure of original input space. Our experiments show that MELM outperforms other state-of-the-art feature extraction techniques on several benchmark datasets for classification tasks. Furthermore, the learned representations lead to better performance on downstream tasks such as image clustering and retrieval. These results demonstrate the potential effectiveness of utilizing MELM for building robust machine learning models.",1
"Efficient and accurate joint representation of a collection of images, that belong to the same class, is a major research challenge for practical image set classification. Existing methods either make prior assumptions about the data structure, or perform heavy computations to learn structure from the data itself. In this paper, we propose an efficient image set representation that does not make any prior assumptions about the structure of the underlying data. We learn the non-linear structure of image sets with Deep Extreme Learning Machines (DELM) that are very efficient and generalize well even on a limited number of training samples. Extensive experiments on a broad range of public datasets for image set classification (Honda/UCSD, CMU Mobo, YouTube Celebrities, Celebrity-1000, ETH-80) show that the proposed algorithm consistently outperforms state-of-the-art image set classification methods both in terms of speed and accuracy.",0
"This research explores the use of deep extreme learning machines (DELMs) as a means of representation learning for image set classification. We demonstrate that DELMs can effectively learn representations from sets of images which outperform traditional hand engineered features and other state-of-the-art deep learning models such as convolutional neural networks (CNNs). Our experiments show that using these learned representations leads to significant improvements in image set classification accuracy across various benchmark datasets, including MNIST, CIFAR-10, SVHN and NORB. Furthermore, we introduce two new regularization techniques to reduce overfitting in training: dropout for sparsity and batch renormalization to prevent mode collapse. In conclusion, our work shows that DELMs offer an efficient alternative for high performance image set classification through effective representation learning without requiring large amounts of data, computational resources or architectural complexity compared to existing methods.",1
"While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.",0
"Artificial neural networks have become increasingly popular due to their ability to achieve state-of-the-art results on a wide range of tasks. However, training these deep neural networks can be computationally expensive and time consuming. In order to reduce the computational cost while still achieving high accuracy, researchers have proposed several techniques such as pruning, quantization, and knowledge distillation. One recent technique that has gained attention is the use of thin deep neural networks (TDNNs). TDNNs aim to maintain accuracy while reducing model size by using fewer neurons than traditional models. Despite the success of TDNNs, they require careful design choices regarding network architecture and hyperparameters. This paper provides insights into how to build effective TDNNs through a set of guidelines or ""hints"". Specifically, we provide guidance on choosing architectures, setting hyperparameters, and optimizing activation functions. Our experiments show that following these hints leads to improved performance across multiple benchmark datasets compared to prior work. These guidelines should prove valuable to researchers looking to adopt TDNNs for their own applications.",1
"We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory -- a recent theory of feedforward processing in sensory cortex.",0
"Title: On Invariance and Selectivity in Representation Learning  In this work, we explore the interplay between invariance and selectivity in representation learning tasks. We begin by discussing the importance of both concepts in representing complex data distributions while maintaining desirable properties such as robustness to noise and generalization performance. Next, we present recent advances in the field that demonstrate how these objectives can be achieved through careful design choices, including architectural modifications and regularization techniques. Finally, we provide experimental evidence on a range of benchmark datasets that highlights the benefits of incorporating both invariance and selectivity into learned representations. Our results suggest potential directions for future research aimed at achieving even more effective and efficient representation learning models. Overall, our goal is to stimulate further investigation into the balance between invariance and selectivity required for successful representation learning across different domains.",1
"Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these ""deep learning"" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.",0
"Deep learning has become one of the most popular fields within machine learning and artificial intelligence due to its ability to achieve state-of-the-art results on a variety of tasks. Despite this success, many researchers have struggled to gain insight into how deep learning models actually learn their representations, as these systems can contain millions of parameters and operate over multiple layers of abstraction. One approach that has been proposed to address this issue is the use of linear support vector machines (SVMs) instead of complex nonlinear functions such as convolutional neural networks. In this paper, we demonstrate how to modify existing architectures such as ResNets so they can still work with linear SVMs. We show that by doing so, we can gain deeper understanding into the behavior of the model while maintaining competitive performance on challenging image classification benchmarks. Our method achieves equivalent accuracy to prior art with dramatically reduced computation time compared to the original implementation of ResNet. Additionally, our method allows us to leverage techniques from traditional machine learning approaches, such as regularization, kernel tricks, and duality theory. Overall, we believe our work demonstrates the potential of combining ideas from both deep learning and classical SVM-based methods to develop more interpretable and efficient machine learning algorithms.",1
"We introduce a new representation learning algorithm suited to the context of domain adaptation, in which data at training and test time come from similar but different distributions. Our algorithm is directly inspired by theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on a data representation that cannot discriminate between the training (source) and test (target) domains. We propose a training objective that implements this idea in the context of a neural network, whose hidden layer is trained to be predictive of the classification task, but uninformative as to the domain of the input. Our experiments on a sentiment analysis classification benchmark, where the target domain data available at training time is unlabeled, show that our neural network for domain adaption algorithm has better performance than either a standard neural network or an SVM, even if trained on input features extracted with the state-of-the-art marginalized stacked denoising autoencoders of Chen et al. (2012).",0
"Artificial neural networks (ANN) have become popular for image generation due to their ability to generate high quality images from random noise input. These models such as DALL-E and Stable Diffusion were trained using adversarial loss which leads to better generated images but at the cost of slow convergence and increased computational complexity. In recent years, domain-adversarial training has gained attention as a method to stabilize generative modeling by incorporating unlabeled data into generator training without compromising on sample fidelity. We introduce and evaluate state-of-the-art unsupervised adversarial training techniques applied to Variational Autoencoders (VAEs) trained with unpaired real/fake discriminators across multiple datasets and architectures to demonstrate their effectiveness over current methods, improving results by significant margins while reducing computational overhead. Our findings establish a strong benchmark against which future work can be compared.",1
"Recently proposed budding tree is a decision tree algorithm in which every node is part internal node and part leaf. This allows representing every decision tree in a continuous parameter space, and therefore a budding tree can be jointly trained with backpropagation, like a neural network. Even though this continuity allows it to be used in hierarchical representation learning, the learned representations are local: Activation makes a soft selection among all root-to-leaf paths in a tree. In this work we extend the budding tree and propose the distributed tree where the children use different and independent splits and hence multiple paths in a tree can be traversed at the same time. This ability to combine multiple paths gives the power of a distributed representation, as in a traditional perceptron layer. We show that distributed trees perform comparably or better than budding and traditional hard trees on classification and regression tasks.",0
"Distributed decision trees (DDTs) have become increasingly popular due to their ability to efficiently handle large datasets that cannot fit into memory on a single machine. They offer several advantages over traditional decision tree algorithms such as reduced communication costs and improved scalability. In this paper, we propose a novel algorithm for distributed decision trees based on random projections. We show how to generate compressed versions of the original data using dimensionality reduction techniques, which can then be used to train decision trees in parallel across multiple machines. Our experiments demonstrate that our approach significantly improves both accuracy and training time compared to state-of-the-art methods for distributed decision trees. Additionally, we provide theoretical analysis showing that our method reduces both computational and communicational overhead while maintaining high levels of model accuracy. Overall, this work advances the field of distributed learning by providing an effective solution for building decision trees on massive datasets.",1
"A key element in transfer learning is representation learning; if representations can be developed that expose the relevant factors underlying the data, then new tasks and domains can be learned readily based on mappings of these salient factors. We propose that an important aim for these representations are to be unbiased. Different forms of representation learning can be derived from alternative definitions of unwanted bias, e.g., bias to particular tasks, domains, or irrelevant underlying data dimensions. One very useful approach to estimating the amount of bias in a representation comes from maximum mean discrepancy (MMD) [5], a measure of distance between probability distributions. We are not the first to suggest that MMD can be a useful criterion in developing representations that apply across multiple domains or tasks [1]. However, in this paper we describe a number of novel applications of this criterion that we have devised, all based on the idea of developing unbiased representations. These formulations include: a standard domain adaptation framework; a method of learning invariant representations; an approach based on noise-insensitive autoencoders; and a novel form of generative model.",0
"Title: ""Learning Unbiased Features"" (Note that I removed the title from your request.)  Abstract: Despite significant advances in machine learning over recent decades, the performance of many algorithms remains subject to unwanted biases in the data they learn from. In order to address this problem, researchers have proposed approaches aimed at detecting and mitigating these biases, but few methods have focused on adaptively updating model parameters during training to explicitly minimize bias in learned representations. Here we propose a new method called Adaptive Bias Correction (ABC), which learns unbiased feature representations by adaptively adjusting weights in deep neural networks based on estimates of distribution skewness. Our approach has been applied successfully across several benchmark datasets and demonstrated consistent improvements compared to state-of-the art models, particularly in terms of fairness metrics such as demographic parity, equal opportunity, and statistical parity. We hope ABC can serve as a step towards creating more inclusive artificial intelligence systems while improving overall prediction accuracy.",1
"Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting.",0
"In recent years, convolutional neural networks (CNN) have become increasingly popular for image classification tasks due to their ability to learn hierarchical representations from raw input data. However, traditional CNN architectures often struggle with scale variations in images, which can lead to poor performance on certain datasets. To address this issue, we propose a novel approach called locally scale-invariant CNNs that use spatial pyramid pooling to capture multi-scale features within each layer of the network. Our method is inspired by human vision, where neurons at different levels of abstraction detect objects at different scales. We evaluate our proposed model on several benchmark datasets such as CIFAR-10 and SVHN, and show that it outperforms state-of-the-art models while maintaining competitive computational efficiency. Additionally, we provide detailed ablation studies to demonstrate the contribution of each component of our model towards improving image classification accuracy. Overall, our work advances the field of computer vision and deep learning research by introducing a simple yet effective solution to handle scale variations in images.",1
"We develop methods for detector learning which exploit joint training over both weak and strong labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels.",0
"In recent years, there has been significant interest in developing automated object detection algorithms that can accurately identify objects in images without manual labeling. One approach to achieving this goal is through multiple instance learning (MIL), which allows models to learn from instances rather than individual pixels. However, current methods have limitations in terms of scalability and computational efficiency.  To address these challenges, we propose a novel framework for joint MIL and representation learning (JL-R) that enables efficient training on large datasets while maintaining high accuracy. Our method utilizes discriminative local features and an adaptive pooling strategy to enhance feature extraction and reduce redundancy. This results in improved detection performance compared to previous state-of-the-art approaches.  We evaluate our model using standard benchmarks for object detection, demonstrating superior performance across all metrics. We also conduct an ablation study to analyze the impact of each component in our proposed architecture.  Our work contributes to the field by introducing a new algorithm capable of efficiently identifying objects in complex scenes without relying on costly data annotations. Our findings have implications for a wide range of applications, including computer vision, robotics, and autonomous systems.",1
"After intensive research, heterogenous face recognition is still a challenging problem. The main difficulties are owing to the complex relationship between heterogenous face image spaces. The heterogeneity is always tightly coupled with other variations, which makes the relationship of heterogenous face images highly nonlinear. Many excellent methods have been proposed to model the nonlinear relationship, but they apt to overfit to the training set, due to limited samples. Inspired by the unsupervised algorithms in deep learning, this paper proposes an novel framework for heterogeneous face recognition. We first extract Gabor features at some localized facial points, and then use Restricted Boltzmann Machines (RBMs) to learn a shared representation locally to remove the heterogeneity around each facial point. Finally, the shared representations of local RBMs are connected together and processed by PCA. Two problems (Sketch-Photo and NIR-VIS) and three databases are selected to evaluate the proposed method. For Sketch-Photo problem, we obtain perfect results on the CUFS database. For NIR-VIS problem, we produce new state-of-the-art performance on the CASIA HFB and NIR-VIS 2.0 databases.",0
"In today’s world, face recognition technology has become increasingly popular due to its importance in surveillance systems, access control, biometric authentication, photo management applications, and more. However, current heterogeneous face recognition techniques encounter several difficulties caused by variations such as illumination conditions, pose, age progression, disguise, expression, resolution, blur, etc., which can result in decreased accuracy rates. To overcome these issues, our research focuses on exploring shared representation learning methods that aim at simultaneously optimizing discriminative ability for multiple tasks while reducing sensitivity to intra-class variation. Our approach combines two well-known models: deep convolutional neural networks (CNN) and sparse representations (SR). By training both of them jointly using pairs from different domains, we obtain powerful feature extractors that lead to increased performance levels compared to traditional single model approaches. Furthermore, we propose a novel regularization method based on the margin maximization theory that allows us to learn a robust yet generalizable mapping function between heterogeneous features and their corresponding labels. Extensive experiments conducted on six public benchmark datasets validate our proposed scheme, achieving superior results over state-of-the-art competitors under difficult matching scenarios. Overall, our work presents a promising direction towards efficient and reliable face recognition across diverse environments.",1
"We revisit a pioneer unsupervised learning technique called archetypal analysis, which is related to successful data analysis methods such as sparse coding and non-negative matrix factorization. Since it was proposed, archetypal analysis did not gain a lot of popularity even though it produces more interpretable models than other alternatives. Because no efficient implementation has ever been made publicly available, its application to important scientific problems may have been severely limited. Our goal is to bring back into favour archetypal analysis. We propose a fast optimization scheme using an active-set strategy, and provide an efficient open-source implementation interfaced with Matlab, R, and Python. Then, we demonstrate the usefulness of archetypal analysis for computer vision tasks, such as codebook learning, signal classification, and large image collection visualization.",0
"Archetypal analysis has emerged as a powerful tool for representation learning by leveraging structured patterns present in data. However, existing archetypal analysis methods suffer from high computational cost and sensitivity to outliers, which impedes their applicability to large datasets. To address these limitations, we propose a fast and robust method for archetypal analysis that significantly reduces computation time while ensuring greater stability and accuracy. Our approach utilizes randomized techniques to efficiently identify archetypes and introduces a novel regularization term that minimizes the influence of outliers. Extensive experiments on various benchmark datasets demonstrate the superior performance of our proposed method over state-of-the-art approaches in terms of efficiency, effectiveness, and robustness. This work provides researchers and practitioners with a more efficient and reliable solution for uncovering meaningful structure within complex datasets through archetypal analysis.",1
"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.",0
"Abstract: Artificial intelligence (AI) has made significant progress over the past several decades, thanks in large part to advances in representation learning. This technique involves training machines to extract features from raw data that capture essential information about a problem domain while preserving relevant characteristics for subsequent analysis and decision making. In this review, we provide a comprehensive examination of recent developments in representation learning, discussing both traditional methods and emerging trends. We highlight key applications across different domains and explore open challenges facing researchers as they push the limits of current knowledge. Our goal is to inform readers about state-of-the-art techniques and motivate further inquiry into new perspectives on representation learning. Keywords: artificial intelligence, machine learning, feature extraction, deep learning, computer vision, natural language processing, robotics, reinforcement learning",1
"We consider the problem of image representation for the tasks of unsupervised learning and semi-supervised learning. In those learning tasks, the raw image vectors may not provide enough representation for their intrinsic structures due to their highly dense feature space. To overcome this problem, the raw image vectors should be mapped to a proper representation space which can capture the latent structure of the original data and represent the data explicitly for further learning tasks such as clustering.   Inspired by the recent research works on deep neural network and representation learning, in this paper, we introduce the multiple-layer auto-encoder into image representation, we also apply the locally invariant ideal to our image representation with auto-encoders and propose a novel method, called Graph regularized Auto-Encoder (GAE). GAE can provide a compact representation which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure.   Extensive experiments on image clustering show encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-word cases.",0
"Deep neural networks have revolutionized image representation learning by providing powerful feature extractors capable of capturing complex patterns from large datasets. However, these models often struggle to generalize well on smaller datasets and may suffer from issues such as overfitting or poor interpretability. To address these limitations, we propose using graph regularized autoencoders (GRAs) to learn robust representations of images. Our method leverages both spatial and semantic relationships among pixels within an image, effectively enhancing the ability of GRAs to capture global structure and reduce reconstruction error. We demonstrate that our approach outperforms state-of-the-art methods across multiple benchmarks, achieving significant gains in accuracy while providing interpretable representations suitable for downstream tasks such as object detection and segmentation. Importantly, our framework can operate at scale and effectively learns from small amounts of data, making it ideal for real-world applications where labeled examples are scarce. By bridging the gap between deep learning and traditional computer vision techniques, we establish a foundation for building more effective image processing systems that excel under diverse operating conditions.",1
"Most representation learning algorithms for language and image processing are local, in that they identify features for a data point based on surrounding points. Yet in language processing, the correct meaning of a word often depends on its global context. As a step toward incorporating global context into representation learning, we develop a representation learning algorithm that incorporates joint prediction into its technique for producing features for a word. We develop efficient variational methods for learning Factorial Hidden Markov Models from large texts, and use variational distributions to produce features for each word that are sensitive to the entire input sequence, not just to a local context window. Experiments on part-of-speech tagging and chunking indicate that the features are competitive with or better than existing state-of-the-art representation learning methods.",0
"Abstract: This paper proposes a new model for learning representations of natural language using factorial hidden Markov models (FHMMs). Traditional methods for encoding text into fixed-length vectors, such as bag-of-words (BoW) and TF-IDF approaches, can struggle with capturing complex relationships between words, sentences, and documents. In contrast, FHMMs enable more flexible, high-order interactions by representing sequences of observations using a set of underlying states that capture common patterns across multiple dimensions simultaneously. Our approach builds upon recent advances in applying neural networks to process raw text input, leveraging deep neural network architectures and attention mechanisms to learn state transition probabilities over time while handling sequential data efficiently. Empirical results on several benchmark datasets demonstrate significant improvement in performance compared to baseline BoW and TF-IDF models, highlighting the potential of our proposed methodology in a variety of NLP tasks including document classification, sentiment analysis, machine translation, and question answering. Overall, we contribute towards enabling more expressive natural language processing techniques through the development and evaluation of novel probabilistic models based on hidden Markov processes.",1
"When deep learning is applied to visual object recognition, data augmentation is often used to generate additional training data without extra labeling cost. It helps to reduce overfitting and increase the performance of the algorithm. In this paper we investigate if it is possible to use data augmentation as the main component of an unsupervised feature learning architecture. To that end we sample a set of random image patches and declare each of them to be a separate single-image surrogate class. We then extend these trivial one-element classes by applying a variety of transformations to the initial 'seed' patches. Finally we train a convolutional neural network to discriminate between these surrogate classes. The feature representation learned by the network can then be used in various vision tasks. We find that this simple feature learning algorithm is surprisingly successful, achieving competitive classification results on several popular vision datasets (STL-10, CIFAR-10, Caltech-101).",0
"Artificial intelligence has made significant progress in recent years due to advances in unsupervised feature learning. Traditional supervised machine learning methods require large amounts of labeled training data, which can be time-consuming and expensive to obtain. Unsupervised learning methods seek to learn representations that capture important features of the data without explicit guidance from human annotators. In this work, we propose a novel method for unsupervised feature learning using image augmentation techniques. Our approach leverages popular image transformation operations such as cropping, flipping, rotating, and color distortion to generate new versions of the input image. We show that these transformed images contain valuable information that can improve the performance of existing state-of-the-art models on benchmark datasets. Furthermore, our experiments demonstrate that our model is able to learn meaningful representations even when trained on relatively small datasets. Overall, our work contributes to the growing field of unsupervised learning and shows promise for improving performance in computer vision tasks without relying solely on vast quantities of labeled data.",1
"Representation learning and unsupervised learning are two central topics of machine learning and signal processing. Deep learning is one of the most effective unsupervised representation learning approach. The main contributions of this paper to the topics are as follows. (i) We propose to view the representative deep learning approaches as special cases of the knowledge reuse framework of clustering ensemble. (ii) We propose to view sparse coding when used as a feature encoder as the consensus function of clustering ensemble, and view dictionary learning as the training process of the base clusterings of clustering ensemble. (ii) Based on the above two views, we propose a very simple deep learning algorithm, named deep random model ensemble (DRME). It is a stack of random model ensembles. Each random model ensemble is a special k-means ensemble that discards the expectation-maximization optimization of each base k-means but only preserves the default initialization method of the base k-means. (iv) We propose to select the most powerful representation among the layers by applying DRME to clustering where the single-linkage is used as the clustering algorithm. Moreover, the DRME based clustering can also detect the number of the natural clusters accurately. Extensive experimental comparisons with 5 representation learning methods on 19 benchmark data sets demonstrate the effectiveness of DRME.",0
"In recent years, there has been significant interest in developing deep learning models that can generate realistic images, audio, and video. However, training these models often requires large amounts of data and computational resources, which may not be feasible for many researchers and organizations. To address this issue, we propose a new method called Simple Deep Random Model Ensemble (SDRME) that combines multiple pre-trained deep learning models to create more accurate and diverse outputs. Our approach involves randomly selecting several models from a pool of available options and then averaging their predictions. We show through extensive experiments that our method outperforms existing ensemble techniques across a range of tasks including image generation, semantic segmentation, object detection, and speech recognition. Furthermore, our approach is simple to implement and requires significantly fewer resources than training custom models from scratch. Overall, SDRME represents a promising new direction for deep learning research, enabling researchers to achieve state-of-the-art results without requiring access to vast amounts of data and computation.",1
"Unsupervised deep learning is one of the most powerful representation learning techniques. Restricted Boltzman machine, sparse coding, regularized auto-encoders, and convolutional neural networks are pioneering building blocks of deep learning. In this paper, we propose a new building block -- distributed random models. The proposed method is a special full implementation of the product of experts: (i) each expert owns multiple hidden units and different experts have different numbers of hidden units; (ii) the model of each expert is a k-center clustering, whose k-centers are only uniformly sampled examples, and whose output (i.e. the hidden units) is a sparse code that only the similarity values from a few nearest neighbors are reserved. The relationship between the pioneering building blocks, several notable research branches and the proposed method is analyzed. Experimental results show that the proposed deep model can learn better representations than deep belief networks and meanwhile can train a much larger network with much less time than deep belief networks.",0
"This should summarize the content and main points of the article including methods used and results obtained. This should be written using formal language as if you were submitting this into a journal publication and may vary slightly from informal writing styles such as blogs. Use past tense throughout. Please note that I am not providing any input other than a possible output style which would likely need revision by someone familiar with the topic. Abstract: Learning deep representations without parameter inference has been shown to improve nonlinear dimensionality reduction techniques. By incorporating explicit regularization constraints, the resulting models are able to generalize better across data domains. Our method, titled ""Deep Latent Autoencoder"", employs a novel autoencoder architecture based on denoising autoencoders, where the latent variables learned are continuous but constrained by a discrete prior distribution (such as the Gaussian mixture model). The approach enforces the local linear structure in the encoded space, thus making the representations more robust against noise and outliers. We evaluate our algorithm on several benchmark datasets, including MNIST, NORB, CIFAR10 and SVHN, demonstrating significant improvements over state-of-the-art algorithms.",1
"We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.",0
"This paper investigates the impact of prior knowledge on optimization tasks. Previous studies have shown that incorporating domain expertise into optimization algorithms can significantly improve their performance. In particular, we focus on how different types of prior information such as statistical models, problem structure, and learning from demonstration can enhance computational efficiency and solution quality. Our results show that these forms of prior information are essential for solving complex real-world problems where standard optimization methods may struggle due to high dimensions, nonlinearity, or incomplete data. We provide concrete examples from several application domains including control systems design, image processing, and autonomous driving, which demonstrate the benefits of using prior information for optimization. Overall, our findings highlight the importance of integrating prior knowledge into optimizers to achieve better solutions faster.",1
"This paper describes our solution to the multi-modal learning challenge of ICML. This solution comprises constructing three-level representations in three consecutive stages and choosing correct tag words with a data-specific strategy. Firstly, we use typical methods to obtain level-1 representations. Each image is represented using MPEG-7 and gist descriptors with additional features released by the contest organizers. And the corresponding word tags are represented by bag-of-words model with a dictionary of 4000 words. Secondly, we learn the level-2 representations using two stacked RBMs for each modality. Thirdly, we propose a bimodal auto-encoder to learn the similarities/dissimilarities between the pairwise image-tags as level-3 representations. Finally, during the test phase, based on one observation of the dataset, we come up with a data-specific strategy to choose the correct tag words leading to a leap of an improved overall performance. Our final average accuracy on the private test set is 100%, which ranks the first place in this challenge.",0
"This paper presents a novel approach for constructing hierarchical image-tag bimodal representations for word tags alternative choice. The proposed method leverages deep learning techniques to learn jointly from both textual and visual modalities, resulting in more accurate tag predictions. Experimental results demonstrate that our model outperforms state-of-the-art methods on several benchmark datasets. Our contributions can pave the way towards advancing multimodal representation learning research and improving downstream applications such as image retrieval, classification, and question answering.",1
"The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.",0
"This paper presents an analysis of three recent machine learning contests and their respective challenges in representation learning. We examine the specific tasks that were addressed by each contest, as well as the approaches taken by participants in developing solutions. Our findings indicate that while there have been significant advances in representation learning over the past few years, there remain several key areas where further progress can be made. These challenges relate to issues such as data quality and availability, model interpretability, scalability, generalization across different domains, and the need for more fine-grained evaluation metrics. By discussing these challenges, we hope to encourage future research in representation learning that addresses these important problems.",1
"Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks. In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.",0
This paper presents novel techniques which leverage deep representations learnt by multiple neural networks and combine them using horizontal and vertical ensembles. These methods overcome issues present in existing ensemble techniques such as bagging and boosting. Experiments demonstrate significant improvement over strong baselines on both facial recognition tasks and natural language processing problems.,1
"We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches.",0
"Effective learning of domain-invariant image representations requires solving challenging problems related to differences in distribution and discrepancies across features. In this work, we propose an approach that effectively bridges the gap between these two domains using discriminator-based adversarial training combined with feature augmentation techniques such as cutout and CutMix. We evaluate our model on several datasets and demonstrate significant improvements over state-of-the-art methods in terms of both accuracy and robustness. Furthermore, our method achieves better generalization performance compared to other approaches while reducing computational overheads. Our contributions provide insights into improving domain adaptation for vision tasks, particularly those dealing with object recognition and segmentation.",1
"Copulas are a powerful tool for modeling multivariate distributions as they allow to separately estimate the univariate marginal distributions and the joint dependency structure. However, known parametric copulas offer limited flexibility especially in high dimensions, while commonly used non-parametric methods suffer from the curse of dimensionality. A popular remedy is to construct a tree-based hierarchy of conditional bivariate copulas. In this paper, we propose a flexible, yet conceptually simple alternative based on implicit generative neural networks. The key challenge is to ensure marginal uniformity of the estimated copula distribution. We achieve this by learning a multivariate latent distribution with unspecified marginals but the desired dependency structure. By applying the probability integral transform, we can then obtain samples from the high-dimensional copula distribution without relying on parametric assumptions or the need to find a suitable tree structure. Experiments on synthetic and real data from finance, physics, and image generation demonstrate the performance of this approach.",0
"This paper introduces implicit generative copula models (IGCM), which provide a powerful framework to model multivariate dependencies by learning explicit representations of joint probability densities from data. While traditional copula approaches reconstruct the dependence structure without access to likelihood estimates for conditional distributions, IGCMs capture both marginal and dependency relationships within a unified probabilistic architecture. As demonstrated through extensive experiments on real world datasets, these methods lead to improved predictive performance over alternative state-of-the art approaches across a range of domains including finance, insurance and climate science. Moreover, the inherent interpretability offered by our approach allows us to gain novel insights into complex high-dimensional problems which would otherwise remain hidden using standard techniques such as principal component analysis or correlation matrices. Overall, we believe that IGCM represents a significant advance towards building flexible and reliable statistical models tailored to specific use cases and can have broad impact ranging from quantitative risk management to causal inference problems where understanding the interplay between multiple variables plays a crucial role. Implicit generative copula models (IGCM) represent a new methodology for modeling multivariate dependencies. Unlike traditional copula-based approaches that rely only on the relationship between marginal and dependence structures, IGCM integrates both pieces of information within a single probabilistic model. This results in improved accuracy and interpretability relative to alternative approaches on diverse applications such as financial markets and climatological systems. Furthermore, the learned representations facilitate the discovery of previously unknown relationships among multiple factors in complex scenarios, providing substantial benefits for decision making and scientific inquiry. Ultimately, IGCM holds great promise as a general tool for effectively addressing challenges pertaining to statistical modeling and prediction involving many dimensions.",1
"We propose Generative Probabilistic Image Colorization, a diffusion-based generative process that trains a sequence of probabilistic models to reverse each step of noise corruption. Given a line-drawing image as input, our method suggests multiple candidate colorized images. Therefore, our method accounts for the ill-posed nature of the colorization problem. We conducted comprehensive experiments investigating the colorization of line-drawing images, report the influence of a score-based MCMC approach that corrects the marginal distribution of estimated samples, and further compare different combinations of models and the similarity of their generated images. Despite using only a relatively small training dataset, we experimentally develop a method to generate multiple diverse colorization candidates which avoids mode collapse and does not require any additional constraints, losses, or re-training with alternative training conditions. Our proposed approach performed well not only on color-conditional image generation tasks using biased initial values, but also on some practical image completion and inpainting tasks.",0
"Automatic image colorization is an intriguing topic that has captured researchers’ attention due to its potential applications such as historical photo restoration, art creation, enhancing user experience, etc. Various approaches have been proposed in recent years including optimization methods based on L2 norms [6, 8], local linear regression [9] and most recently deep learning methods that leverage convolutional neural networks (CNN) [4]. These existing techniques have their own pros and cons regarding robustness, efficiency and flexibility. In this work, we present a generative probabilistic model which outperforms current state-of-the-art algorithms while addressing some of their drawbacks at once.",1
"In this paper, we propose a method for keypoint discovery from a 2D image using image-level supervision. Recent works on unsupervised keypoint discovery reliably discover keypoints of aligned instances. However, when the target instances have high viewpoint or appearance variation, the discovered keypoints do not match the semantic correspondences over different images. Our work aims to discover keypoints even when the target instances have high viewpoint and appearance variation by using image-level supervision. Motivated by the weakly-supervised learning approach, our method exploits image-level supervision to identify discriminative parts and infer the viewpoint of the target instance. To discover diverse parts, we adopt a conditional image generation approach using a pair of images with structural deformation. Finally, we enforce a viewpoint-based equivariance constraint using the keypoints from the image-level supervision to resolve the spatial correlation problem that consistently appears in the images taken from various viewpoints. Our approach achieves state-of-the-art performance for the task of keypoint estimation on the limited supervision scenarios. Furthermore, the discovered keypoints are directly applicable to downstream tasks without requiring any keypoint labels.",0
"Title: Weakly Supervised Keypoint Discovery Abstract: This work presents a novel weakly supervised approach for keypoint discovery that leverages both image-level labels and unannotated images to learn discriminative features and model object representations at scale. Our method adapts a deep convolutional neural network (CNN) architecture trained on large amounts of unlabeled data, followed by a self-supervised pretext task which enables learning of high level features that distinguish between different objects even without full annotations. By using these learned feature maps as inputs, we then train another CNN module specifically designed to predict keypoints locations directly from the feature activations generated by the first stage. During training, we employ two complementary loss functions - the standard keypoint error over annotated images, and a new uncertainty based regularization term enforcing low confidence predictions where ground truth is missing. Results show significant improvements over existing methods relying solely on fully supervised settings, demonstrating the effectiveness and generalization ability of our proposed framework across multiple benchmark datasets and scenarios. Title: Weakly Supervised Keypoint Discovery  Abstract: This research proposes a weakly supervised technique for identifying keypoints in images by utilizing both labeled images and unlabeled images. To accomplish this goal, we leverage a deep convolutional neural network (CNN) pre-trained on vast quantities of unlabeled data, followed by a self-supervised pretext task that permits the model to learn high-level features capable of distinguishing among diverse objects, irrespective of complete annotations. Subsequently, we use these acquired feature maps to train yet another CNN component expressly configured for detecting keypoints through their activation outputs. While training, we employ two complimentary loss functions – the conventional keypoint discrepancy on labeled pictures, along wit",1
"In this work we propose a new task: artistic visualization of classical Chinese poems, where the goal is to generatepaintings of a certain artistic style for classical Chinese poems. For this purpose, we construct a new dataset called Paint4Poem. Thefirst part of Paint4Poem consists of 301 high-quality poem-painting pairs collected manually from an influential modern Chinese artistFeng Zikai. As its small scale poses challenges for effectively training poem-to-painting generation models, we introduce the secondpart of Paint4Poem, which consists of 3,648 caption-painting pairs collected manually from Feng Zikai's paintings and 89,204 poem-painting pairs collected automatically from the web. We expect the former to help learning the artist painting style as it containshis most paintings, and the latter to help learning the semantic relevance between poems and paintings. Further, we analyze Paint4Poem regarding poem diversity, painting style, and the semantic relevance between poems and paintings. We create abenchmark for Paint4Poem: we train two representative text-to-image generation models: AttnGAN and MirrorGAN, and evaluate theirperformance regarding painting pictorial quality, painting stylistic relevance, and semantic relevance between poems and paintings.The results indicate that the models are able to generate paintings that have good pictorial quality and mimic Feng Zikai's style, but thereflection of poem semantics is limited. The dataset also poses many interesting research directions on this task, including transferlearning, few-shot learning, text-to-image generation for low-resource data etc. The dataset is publicly available.(https://github.com/paint4poem/paint4poem)",0
"This paper describes the development of Paint4Poem, a dataset consisting of classical Chinese poems paired with artistic visualizations intended to capture the emotional essence of each poem. We introduce our data collection methodology, which involved gathering over 2,800 poem-image pairs from social media platforms, as well as conducting user studies to evaluate the quality of these pairs and inform refinements to our process. Our results suggest that the final set of image prompts can indeed elicit appropriate associations with their corresponding poems, demonstrating promise for creative applications such as literary reflection or education, and offering insights into crossmodal correspondences between Chinese poetry and natural scenes.",1
"Recently, there has been an increasing interest in image editing methods that employ pre-trained unconditional image generators (e.g., StyleGAN). However, applying these methods to translate images to multiple visual domains remains challenging. Existing works do not often preserve the domain-invariant part of the image (e.g., the identity in human face translations), they do not usually handle multiple domains, or do not allow for multi-modal translations. This work proposes an implicit style function (ISF) to straightforwardly achieve multi-modal and multi-domain image-to-image translation from pre-trained unconditional generators. The ISF manipulates the semantics of an input latent code to make the image generated from it lying in the desired visual domain. Our results in human face and animal manipulations show significantly improved results over the baselines. Our model enables cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. The code and data are available at: \url{https://github.com/yhlleo/stylegan-mmuit}.",0
"In recent years, image-to-image translation has become an increasingly important task in computer vision. One popular approach to addressing this problem is through the use of generative adversarial networks (GANs). However, most existing GAN architectures have limitations that make them difficult to use for high-resolution image translation tasks. To overcome these limitations, we propose a new method called ISF-GAN which incorporates an implicit style function into the generator network. This function allows the model to better capture both content and style information from input images and produce more accurate translations. Our results show that our proposed method outperforms previous state-of-the-art approaches on several benchmark datasets, demonstrating its effectiveness in generating high-quality, high-resolution image translations. Overall, the development of ISF-GAN represents a significant advance in the field of image-to-image translation and holds great promise for future applications in computer vision.",1
"We propose a novel loss for generative models, dubbed as GRaWD (Generative Random Walk Deviation), to improve learning representations of unexplored visual spaces. Quality learning representation of unseen classes (or styles) is critical to facilitate novel image generation and better generative understanding of unseen visual classes, i.e., zero-shot learning (ZSL). By generating representations of unseen classes based on their semantic descriptions, e.g., attributes or text, generative ZSL attempts to differentiate unseen from seen categories. The proposed GRaWD loss is defined by constructing a dynamic graph that includes the seen class/style centers and generated samples in the current minibatch. Our loss initiates a random walk probability from each center through visual generations produced from hallucinated unseen classes. As a deviation signal, we encourage the random walk to eventually land after t steps in a feature representation that is difficult to classify as any of the seen classes. We demonstrate that the proposed loss can improve unseen class representation quality inductively on text-based ZSL benchmarks on CUB and NABirds datasets and attribute-based ZSL benchmarks on AWA2, SUN, and aPY datasets. In addition, we investigate the ability of the proposed loss to generate meaningful novel visual art on the WikiArt dataset. The results of experiments and human evaluations demonstrate that the proposed GRaWD loss can improve StyleGAN1 and StyleGAN2 generation quality and create novel art that is significantly more preferable. Our code is made publicly available at https://github.com/Vision-CAIR/GRaWD.",0
"Abstract: Deep learning has revolutionized the field of computer vision by enabling the development of powerful models that can perform complex tasks such as object recognition and image generation. However, these models often struggle with generalizing to new domains and tasks, especially when confronted with unseen examples during inference time. In this work, we propose a novel method based on generative random walks deviation loss (GRWDL) that helps improve representations learned from deep neural networks for better transferability across datasets and models. By generating synthetic training data using GRWDL and fine-tuning on these augmentations, our model significantly outperforms state-of-the-art methods across several benchmarks, demonstrating the efficacy of our approach in boosting unseen learning representation performance.",1
"Generating fine-grained, realistic images from text has many applications in the visual and semantic realm. Considering that, we propose Bangla Attentional Generative Adversarial Network (AttnGAN) that allows intensified, multi-stage processing for high-resolution Bangla text-to-image generation. Our model can integrate the most specific details at different sub-regions of the image. We distinctively concentrate on the relevant words in the natural language description. This framework has achieved a better inception score on the CUB dataset. For the first time, a fine-grained image is generated from Bangla text using attentional GAN. Bangla has achieved 7th position among 100 most spoken languages. This inspires us to explicitly focus on this language, which will ensure the inevitable need of many people. Moreover, Bangla has a more complex syntactic structure and less natural language processing resource that validates our work more.",0
"This paper presents a method for fine-grained image generation from text descriptions written in Bangla language, one of the most widely spoken languages in South Asia. We use attentional generative adversarial networks (AttentionGAN) to generate images conditioned on Bangla text inputs. Our approach models the relationships between different parts of the generated image by explicitly incorporating spatial attention mechanisms that allow the network to focus on specific regions in the output as guided by the input text description. Our results show that our method can effectively generate high-quality, detailed images that capture the characteristics described in the input text, even for complex scenes and objects. We compare our method with state-of-the-art approaches for image generation from text and demonstrate significant improvements in terms of visual fidelity and alignment with the input text descriptions. Overall, our work highlights the potential of combining attentional models with GANs for fine-grained image synthesis, paving the way for future research in this area.",1
"Variational autoencoders (VAEs), as an important aspect of generative models, have received a lot of research interests and reached many successful applications. However, it is always a challenge to achieve the consistency between the learned latent distribution and the prior latent distribution when optimizing the evidence lower bound (ELBO), and finally leads to an unsatisfactory performance in data generation. In this paper, we propose a latent distribution consistency approach to avoid such substantial inconsistency between the posterior and prior latent distributions in ELBO optimizing. We name our method as latent distribution consistency VAE (LDC-VAE). We achieve this purpose by assuming the real posterior distribution in latent space as a Gibbs form, and approximating it by using our encoder. However, there is no analytical solution for such Gibbs posterior in approximation, and traditional approximation ways are time consuming, such as using the iterative sampling-based MCMC. To address this problem, we use the Stein Variational Gradient Descent (SVGD) to approximate the Gibbs posterior. Meanwhile, we use the SVGD to train a sampler net which can obtain efficient samples from the Gibbs posterior. Comparative studies on the popular image generation datasets show that our method has achieved comparable or even better performance than several powerful improvements of VAEs.",0
"This paper presents LDC-VAE, a novel method that combines latent distribution consistency with variational autoencoders (VAEs) to improve their performance on data generation tasks. Traditional VAEs suffer from difficulties in modeling high dimensional and complex distributions, which leads to poor sample quality and difficulty in training. To address these issues, we propose using latent distribution consistency constraints during training to encourage VAEs to learn more consistent embeddings across different layers and sampling steps, resulting in improved generative capabilities. Our approach outperforms state-of-the-art VAE models on several benchmark datasets, demonstrating its effectiveness in generating high-quality samples with better coherence and fewer artifacts. We conclude by discussing potential applications of our method in fields such as computer vision, natural language processing, and robotics.",1
"Recent advances in generative adversarial networks (GANs) have shown remarkable progress in generating high-quality images. However, this gain in performance depends on the availability of a large amount of training data. In limited data regimes, training typically diverges, and therefore the generated samples are of low quality and lack diversity. Previous works have addressed training in low data setting by leveraging transfer learning and data augmentation techniques. We propose a novel transfer learning method for GANs in the limited data domain by leveraging informative data prior derived from self-supervised/supervised pre-trained networks trained on a diverse source domain. We perform experiments on several standard vision datasets using various GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the proposed method effectively transfers knowledge to domains with few target images, outperforming existing state-of-the-art techniques in terms of image quality and diversity. We also show the utility of data instance prior in large-scale unconditional image generation.",0
"A generative adversarial network (GAN) consists of two sub-networks: a generator that produces data such as images, audio, video frames etc. and a discriminator that evaluates whether the generated data looks real or fake. Training GANs remains challenging due to difficulties in optimizing both networks simultaneously. One approach to improve stability and performance is through regularization techniques that constrain the optimization process by adding additional terms to the loss function. Data instance prior (DIP), introduced in (Wang et al., 2018), provides another regularization technique that has been shown to produce state-of-the-art results on several benchmark datasets across different application domains. DIP works by enforcing consistency between pairs of instances sampled from the same class and pairs coming from different classes. This ensures that intra-class variance is minimized while inter-class variance is maximized which leads to better separation between distinct clusters in the feature space. In other words, DIP helps enforce class separability without relying on explicit labels. This paper presents an extension of DIP for use within GANs, called Data Instance Prior Plus (DIPP). By incorporating DIP into the training process of a conditional GAN, we show that we can achieve improved results compared to traditional methods such as cycle consistency alone. We evaluate our proposed method on several benchmark datasets and demonstrate that it outperforms previous methods based on standard metrics of quality such as Fréchet Inception Distance (FID) and Intra-Class Variance (ICV). Our findings suggest that regularization with DIPP can effectively stabilize GAN training and lead to more visually pleasing synthesized outputs. With these advances, applications of GANs could see greater adoption in industry, as they become more reliable tools for generating high quality data instances quickly and efficiently.",1
"With the recent progress in Generative Adversarial Networks (GANs), it is imperative for media and visual forensics to develop detectors which can identify and attribute images to the model generating them. Existing works have shown to attribute images to their corresponding GAN sources with high accuracy. However, these works are limited to a closed set scenario, failing to generalize to GANs unseen during train time and are therefore, not scalable with a steady influx of new GANs. We present an iterative algorithm for discovering images generated from previously unseen GANs by exploiting the fact that all GANs leave distinct fingerprints on their generated images. Our algorithm consists of multiple components including network training, out-of-distribution detection, clustering, merge and refine steps. Through extensive experiments, we show that our algorithm discovers unseen GANs with high accuracy and also generalizes to GANs trained on unseen real datasets. We additionally apply our algorithm to attribution and discovery of GANs in an online fashion as well as to the more standard task of real/fake detection. Our experiments demonstrate the effectiveness of our approach to discover new GANs and can be used in an open-world setup.",0
"In recent years, generative adversarial networks (GANs) have become increasingly popular due to their ability to generate high quality images that are difficult to distinguish from real ones. However, identifying which images are generated by GANs and attributing them to specific models can be challenging, particularly in open-world settings where new GAN models may emerge over time. This paper presents a novel approach towards discovering and attributing open-world GAN generated images. Our method utilizes a combination of domain adaptation techniques and feature analysis to identify GAN generated images among a large collection of natural images. We evaluate our approach on several benchmark datasets and demonstrate its effectiveness in accurately detecting and attributing open-world GAN generated images while minimizing false positives. Overall, our work represents a step forward in understanding the capabilities and limitations of current image generation methods and has important implications for security applications such as image authentication and forgery detection.",1
"A novel explainable AI method called CLEAR Image is introduced in this paper. CLEAR Image is based on the view that a satisfactory explanation should be contrastive, counterfactual and measurable. CLEAR Image explains an image's classification probability by contrasting the image with a corresponding image generated automatically via adversarial learning. This enables both salient segmentation and perturbations that faithfully determine each segment's importance. CLEAR Image was successfully applied to a medical imaging case study where it outperformed methods such as Grad-CAM and LIME by an average of 27% using a novel pointing game metric. CLEAR Image excels in identifying cases of ""causal overdetermination"" where there are multiple patches in an image, any one of which is sufficient by itself to cause the classification probability to be close to one.",0
"In recent years, there has been growing interest in developing computational methods for generating visual explanations that can effectively communicate complex concepts and insights to human audiences. One approach to generating such explanations is through the use of counterfactual examples, which involve showing how changes to certain elements of a situation can lead to different outcomes. However, traditional approaches to generating counterfactual examples can suffer from several limitations, including their reliance on oversimplified scenarios that may not accurately reflect real-world situations and a lack of consideration for overdetermined relationships, where multiple factors may contribute to the same outcome. To address these challenges, we propose a new methodology called contrastive counterfactual visual explanations (CCVE). Our approach involves identifying key differences between two similar yet distinct scenarios and using these differences to generate intuitive and informative visual representations of complex relationships and interactions. We demonstrate the effectiveness of our proposed methodology through experiments involving both synthetic data sets and real-world applications, showing that CCVEs can improve understanding and retention of information while providing more accurate representations of causal relationships than traditional counterfactual examples alone. Ultimately, our work represents a significant contribution to the field of explainable artificial intelligence, as well as to other areas of study that seek to leverage computer graphics and interactive visualization techniques for effective communication of complex ideas and phenomena.",1
"We propose a novel approach, MUSE, to illustrate textual attributes visually via portrait generation. MUSE takes a set of attributes written in text, in addition to facial features extracted from a photo of the subject as input. We propose 11 attribute types to represent inspirations from a subject's profile, emotion, story, and environment. We propose a novel stacked neural network architecture by extending an image-to-image generative model to accept textual attributes. Experiments show that our approach significantly outperforms several state-of-the-art methods without using textual attributes, with Inception Score score increased by 6% and Fr\'echet Inception Distance (FID) score decreased by 11%, respectively. We also propose a new attribute reconstruction metric to evaluate whether the generated portraits preserve the subject's attributes. Experiments show that our approach can accurately illustrate 78% textual attributes, which also help MUSE capture the subject in a more creative and expressive way.",0
"This paper presents a new method for generating portrait paintings that combines traditional manual painting techniques with machine learning algorithms. By using textual attributes as input, our system can generate realistic portraits tailored to specific individuals, capturing both their likeness and their personality traits. Our approach leverages recent advances in deep generative models such as Generative Adversarial Networks (GANs) to synthesize images directly from text descriptions. We use a discriminator network trained on real photographs to guide the generator towards more accurate and detailed results. To further enhance the quality and fidelity of the generated portraits, we introduce novel techniques such as spatial attention, which allows the model to focus on salient features in the input description, and stochastic noise injection, which improves robustness and reduces artifacts. Extensive experiments demonstrate the effectiveness and superior performance of our method compared to existing methods in terms of visual coherence, identity preservation, and interpretability.",1
"Drawing and annotating comic illustrations is a complex and difficult process. No existing machine learning algorithms have been developed to create comic illustrations based on descriptions of illustrations, or the dialogue in comics. Moreover, it is not known if a generative adversarial network (GAN) can generate original comics that correspond to the dialogue and/or descriptions. GANs are successful in producing photo-realistic images, but this technology does not necessarily translate to generation of flawless comics. What is more, comic evaluation is a prominent challenge as common metrics such as Inception Score will not perform comparably, as they are designed to work on photos. In this paper: 1. We implement ComicGAN, a novel text-to-comic pipeline based on a text-to-image GAN that synthesizes comics according to text descriptions. 2. We describe an in-depth empirical study of the technical difficulties of comic generation using GAN's. ComicGAN has two novel features: (i) text description creation from labels via permutation and augmentation, and (ii) custom image encoding with Convolutional Neural Networks. We extensively evaluate the proposed ComicGAN in two scenarios, namely image generation from descriptions, and image generation from dialogue. Our results on 1000 Dilbert comic panels and 6000 descriptions show synthetic comic panels from text inputs resemble original Dilbert panels. Novel methods for text description creation and custom image encoding brought improvements to Frechet Inception Distance, detail, and overall image quality over baseline algorithms. Generating illustrations from descriptions provided clear comics including characters and colours that were specified in the descriptions.",0
"Abstract. We propose ComicGAN - a new approach that combines text description of comics as input to generate images directly from human level descriptions (texts). In contrast to most existing work on image generation which either use other images as prior knowledge (e.g., cycleGAN) or employ retrieval systems (e.g., READ), our model accepts pure natural language as inputs. This allows users to create novel instances by simply describing them using languages, without any need to retrieve similar examples from external databases or search engines. Furthermore, we show how adversarial training can enhance visual quality even further without increasing computational complexity. Extensive user study shows favorable results over baselines both qualitatively and quantitatively.",1
"Applications based on Deep Neural Networks (DNNs) have grown exponentially in the past decade. To match their increasing computational needs, several Non-Volatile Memory (NVM) crossbar-based accelerators have been proposed. Apart from improved energy efficiency and performance, these approximate hardware also possess intrinsic robustness for defense against Adversarial Attacks, which is an important security concern for DNNs. Prior works have focused on quantifying this intrinsic robustness for vanilla networks, that is DNNs trained on unperturbed inputs. However, adversarial training of DNNs is the benchmark technique for robustness, and sole reliance on intrinsic robustness of the hardware may not be sufficient. In this work, we explore the design of robust DNNs through the amalgamation of adversarial training and the intrinsic robustness offered by NVM crossbar-based analog hardware. First, we study the noise stability of such networks on unperturbed inputs and observe that internal activations of adversarially trained networks have lower Signal-to-Noise Ratio (SNR), and are sensitive to noise than vanilla networks. As a result, they suffer significantly higher performance degradation due to the non-ideal computations; on an average 2x accuracy drop. On the other hand, for adversarial images generated using Projected-Gradient-Descent (PGD) White-Box attacks, ResNet-10/20 adversarially trained on CIFAR-10/100 display a 5-10% gain in robust accuracy due to the underlying NVM crossbar when the attack epsilon ($\epsilon_{attack}$, the degree of input perturbations) is greater than the epsilon of the adversarial training ($\epsilon_{train}$). Our results indicate that implementing adversarially trained networks on analog hardware requires careful calibration between hardware non-idealities and $\epsilon_{train}$ to achieve optimum robustness and performance.",0
"This paper investigates the effectiveness of adversarial training techniques on non-volatile memory (NVM) crossbar architectures that store weights using resistance values. We study the tradeoff between noise stability and robustness under different levels of applied voltage and temperature variation. Our results show that adversarial training can significantly improve the resilience of these systems even under high variability conditions. Moreover, we observe that certain configurations offer higher accuracy at lower power costs than traditional models. These findings have important implications for the design of energy efficient hardware neural networks that require minimal retraining due to environmental fluctuations. Overall, our work contributes towards understanding how to optimize adversary robustness in resistive processing units, paving the pathway towards more robust, reliable, and scalable artificial intelligence systems.",1
"Recently, the Vision Transformer (ViT) has shown impressive performance on high-level and low-level vision tasks. In this paper, we propose a new ViT architecture, named Hybrid Local-Global Vision Transformer (HyLoG-ViT), for single image dehazing. The HyLoG-ViT block consists of two paths, the local ViT path and the global ViT path, which are used to capture local and global dependencies. The hybrid features are fused via convolution layers. As a result, the HyLoG-ViT reduces the computational complexity and introduces locality in the networks. Then, the HyLoG-ViT blocks are incorporated within our dehazing networks, which jointly learn the intrinsic image decomposition and image dehazing. Specifically, the network consists of one shared encoder and three decoders for reflectance prediction, shading prediction, and haze-free image generation. The tasks of reflectance and shading prediction can produce meaningful intermediate features that can serve as complementary features for haze-free image generation. To effectively aggregate the complementary features, we propose a complementary features selection module (CFSM) to select the useful ones for image dehazing. Extensive experiments on homogeneous, non-homogeneous, and nighttime dehazing tasks reveal that our proposed Transformer-based dehazing network can achieve comparable or even better performance than CNNs-based dehazing models.",0
"In this work, we present a novel hybrid local-global transformer architecture for image dehazing that leverages both local and global contextual information to effectively remove haze from foggy images while preserving intricate details. Our approach combines multi-scale dilated convolutions and self-attention mechanisms within a unified framework, enabling efficient and adaptive feature representation across different spatial scales.  Our model is trained end-to-end on a large dataset of real-world hazy images, using adversarial training and perceptual loss functions to ensure high visual quality and fidelity compared to ground truth references. Extensive experiments demonstrate significant improvements over state-of-the-art methods in terms of objective metrics such as PSNR, SSIM, LPIPS, and subjective evaluations by human raters. Our method produces visually stunning results across a wide range of complex scenes, including natural landscapes, urban environments, and indoor settings, illustrating its robustness and versatility under varying levels of haze severity and weather conditions.  Overall, our research makes two main contributions: (i) a new hybrid local-global transformer network tailored specifically for image dehazing tasks, bridging the gap between low-level vision and deep learning techniques; (ii) extensive experimental validation corroborating the effectiveness and efficiency of our proposed approach against existing benchmarks in computer vision and graphics communities. We believe our work opens up promising avenues towards more advanced applications of deep learning models for challenging imaging problems affected by atmospheric impairments, where accurate scene understanding and perception are crucial. \--\",1
"Generating portrait images by controlling the motions of existing faces is an important task of great consequence to social media industries. For easy use and intuitive control, semantically meaningful and fully disentangled parameters should be used as modifications. However, many existing techniques do not provide such fine-grained controls or use indirect editing methods i.e. mimic motions of other individuals. In this paper, a Portrait Image Neural Renderer (PIRenderer) is proposed to control the face motions with the parameters of three-dimensional morphable face models (3DMMs). The proposed model can generate photo-realistic portrait images with accurate movements according to intuitive modifications. Experiments on both direct and indirect editing tasks demonstrate the superiority of this model. Meanwhile, we further extend this model to tackle the audio-driven facial reenactment task by extracting sequential motions from audio inputs. We show that our model can generate coherent videos with convincing movements from only a single reference image and a driving audio stream. Our source code is available at https://github.com/RenYurui/PIRender.",0
"In recent years, generative models have revolutionized computer graphics by synthesizing high quality images through data driven methods. Motivated by these advances in image generation, we introduce PIRenderer – an application that allows users to control portrait image generation. Our system utilizes semantic neural rendering (SNR), which combines textual descriptions and facial features to generate realistic portraits. By controlling SNR directly using sliders and widgets, users can interactively explore different styles and attributes, including skin color, hair style, gender identity, lighting conditions, etc., without sacrificing photo-realism. Additionally, we propose a novel optimization algorithm based on Glow that stabilizes training and enables efficient fine tuning of pretrained models for improved fidelity and visual coherence across diverse attribute spaces. Extensive experiments demonstrate both qualitatively and quantitatively that our method achieves superior performance over competing baselines while providing more meaningful user controls throughout the pipeline.",1
"Recent advance in diffusion models incorporates the Stochastic Differential Equation (SDE), which brings the state-of-the art performance on image generation tasks. This paper improves such diffusion models by analyzing the model at the zero diffusion time. In real datasets, the score function diverges as the diffusion time ($t$) decreases to zero, and this observation leads an argument that the score estimation fails at $t=0$ with any neural network structure. Subsequently, we introduce Unbounded Diffusion Model (UDM) that resolves the score diverging problem with an easily applicable modification to any diffusion models. Additionally, we introduce a new SDE that overcomes the theoretic and practical limitations of Variance Exploding SDE. On top of that, the introduced Soft Truncation method improves the sample quality by mitigating the loss scale issue that happens at $t=0$. We further provide a theoretic result of the proposed method to uncover the behind mechanism of the diffusion models.",0
"Scoring models are widely used in natural language processing tasks to evaluate text quality, such as code generation and machine translation. These scores are often computed using handcrafted features that rely on domain knowledge and can require significant time and resources to develop. However, these approaches may not generalize well across different datasets and domains due to their reliance on specific feature sets and parameters. In contrast, unsupervised score matching methods learn a function directly from data without explicit labels, which has been shown to improve performance compared to traditional supervised learning techniques.  We propose a new score matching model for evaluating unbounded data scores based on the concept of ""fuzzy alignment"" between predicted outputs and reference translations. Our method leverages advances in generative pretraining, attention mechanisms, and self-attention networks to capture more complex relationships between inputs and references. We conduct experiments on several benchmark datasets, including both machine translation and code generation tasks, demonstrating that our approach outperforms state-of-the-art supervised baselines as well as other recent unsupervised scoring models. Furthermore, we show that our model is capable of producing interpretable alignments that can be utilized for fine-grained analysis and error diagnosis. Overall, our work shows promise for developing scalable and effective evaluation metrics that do not rely solely on manual feature engineering and labeling.",1
"We develop a rigorous and general framework for constructing information-theoretic divergences that subsume both $f$-divergences and integral probability metrics (IPMs), such as the $1$-Wasserstein distance. We prove under which assumptions these divergences, hereafter referred to as $(f,\Gamma)$-divergences, provide a notion of `distance' between probability measures and show that they can be expressed as a two-stage mass-redistribution/mass-transport process. The $(f,\Gamma)$-divergences inherit features from IPMs, such as the ability to compare distributions which are not absolutely continuous, as well as from $f$-divergences, namely the strict concavity of their variational representations and the ability to control heavy-tailed distributions for particular choices of $f$. When combined, these features establish a divergence with improved properties for estimation, statistical learning, and uncertainty quantification applications. Using statistical learning as an example, we demonstrate their advantage in training generative adversarial networks (GANs) for heavy-tailed, not-absolutely continuous sample distributions. We also show improved performance and stability over gradient-penalized Wasserstein GAN in image generation.",0
"This paper introduces and explores $(f, \Gamma)$ divergences as a flexible and expressive framework that can interpolate between popular families of divergences including f-divergences, integral probability metrics (IPMs), and Bregman–Lagrangian Divergence (BLD). We establish connections among these well-known distances under the umbrella of $\left(f,\Gamma\right)$ framework by finding out equivalent representations via Fenchel conjugates, Legendre transforms, etc., which makes our study self contained. Our investigations open new vistas regarding different types of regularization such as $H$—regularized estimation along with IPM-type regularization; besides, we explore some new links involving Bregman Lagrangian Divergence (e.g. Neyman–Pearson lemma type results) and connections related to statistical models like Hellinger distance. Finally, we provide some examples and simulations that illustrate the applicability of our findings within areas like semi-parametric density ratio modeling, two-sample testing, density difference quantification, etc. Our work offers many fresh opportunities for future research on this topic, as it provides a broader context for studying several classical and modern divergence measures used widely across applied sciences, including data science, machine learning, statistics, econometrics, and signal processing, etc.",1
"Denoising diffusion probabilistic models (DDPM) have shown remarkable performance in unconditional image generation. However, due to the stochasticity of the generative process in DDPM, it is challenging to generate images with the desired semantics. In this work, we propose Iterative Latent Variable Refinement (ILVR), a method to guide the generative process in DDPM to generate high-quality images based on a given reference image. Here, the refinement of the generative process in DDPM enables a single DDPM to sample images from various sets directed by the reference image. The proposed ILVR method generates high-quality images while controlling the generation. The controllability of our method allows adaptation of a single DDPM without any additional learning in various image generation tasks, such as generation from various downsampling factors, multi-domain image translation, paint-to-image, and editing with scribbles.",0
"Title: Conditioning Methods for Improved Noise Reduction in Probabilistic Models  This paper presents a novel approach for denoising diffusion probabilistic models using conditional methods. We propose conditioning on an external latent variable that encodes additional knowledge related to the data generation process, such as texture synthesis constraints or scene priors. Our method improves noise reduction by enhancing local coherency within the model while preserving global structure. Experiments on several benchmark datasets demonstrate that our proposed technique outperforms previous approaches in terms of visual fidelity and objective quality metrics. This work provides new insights into the application of probabilistic models for image processing tasks and paves the way for future research in this area. Overall, we believe this paper makes a valuable contribution to the field of computer vision and machine learning.",1
"Recently, some works found an interesting phenomenon that adversarially robust classifiers can generate good images comparable to generative models. We investigate this phenomenon from an energy perspective and provide a novel explanation. We reformulate adversarial example generation, adversarial training, and image generation in terms of an energy function. We find that adversarial training contributes to obtaining an energy function that is flat and has low energy around the real data, which is the key for generative capability. Based on our new understanding, we further propose a better adversarial training method, Joint Energy Adversarial Training (JEAT), which can generate high-quality images and achieve new state-of-the-art robustness under a wide range of attacks. The Inception Score of the images (CIFAR-10) generated by JEAT is 8.80, much better than original robust classifiers (7.50).",0
"Title: Exploring the Potential of Adversarially Robust Machine Learning Models for Image Classification Abstract Artificial intelligence (AI) has revolutionized many fields by enabling automation through data analysis techniques. One major application area where these advancements have been particularly significant lies within image classification tasks - including applications such as facial recognition systems used by governments and large corporations. While there exists a great deal of research into deep learning methods utilizing Convolutional Neural Networks (CNNs), current models may still yield results susceptible to adversarial attacks due to imperfect training regimes. This study explores how recent innovations in generative modeling techniques can enable improved robustness for machine learning classifiers. By incorporating Generative Adversarial Network (GAN) components into existing architectures we investigate improvements to both standard accuracy and immunity against adversarial examples. We present quantitative comparisons based on experiments using datasets widely used across related domains – CIFAR-10 and Styled GAN – revealing higher overall performance compared to established baseline CNN models. Our work opens up new potential avenues for addressing current limitations of state-of-the-art CNN image classification methods.",1
"The vanilla GAN (Goodfellow et al. 2014) suffers from mode collapse deeply, which usually manifests as that the images generated by generators tend to have a high similarity amongst them, even though their corresponding latent vectors have been very different. In this paper, we introduce a pluggable diversity penalty module (DPM) to alleviate mode collapse of GANs. It reduces the similarity of image pairs in feature space, i.e., if two latent vectors are different, then we enforce the generator to generate two images with different features. The normalized Gram matrix is used to measure the similarity. We compare the proposed method with Unrolled GAN (Metz et al. 2016), BourGAN (Xiao, Zhong, and Zheng 2018), PacGAN (Lin et al. 2018), VEEGAN (Srivastava et al. 2017) and ALI (Dumoulin et al. 2016) on 2D synthetic dataset, and results show that the diversity penalty module can help GAN capture much more modes of the data distribution. Further, in classification tasks, we apply this method as image data augmentation on MNIST, Fashion- MNIST and CIFAR-10, and the classification testing accuracy is improved by 0.24%, 1.34% and 0.52% compared with WGAN GP (Gulrajani et al. 2017), respectively. In domain translation, diversity penalty module can help StarGAN (Choi et al. 2018) generate more accurate attention masks and accelarate the convergence process. Finally, we quantitatively evaluate the proposed method with IS and FID on CelebA, CIFAR-10, MNIST and Fashion-MNIST, and the results suggest GAN with diversity penalty module gets much higher IS and lower FID compared with some SOTA GAN architectures.",0
"Introduction: Generative Adversarial Networks (GAN) have emerged as powerful tools in computer vision tasks such as image generation and super-resolution. However, mode collapse, where one or more modes of the generator distribution fail to receive adequate probability mass from the discriminator distribution, remains a challenge in training stable GAN models. Various approaches exist to address this issue, but few techniques efficiently penalize the diversity loss without affecting realism during inference time. In this work, we propose the Diversity Penalty Module (DPM), which enhances the stability of GAN by introducing additional constraints on the latent space while preserving the original content. Methodology: Our approach leverages a novel regularization technique that imposes constraints on the latent representation, preventing degeneration into modes seen earlier. The proposed module encourages exploration by minimizing the distance between noise samples drawn uniformly from a sphere centered at zero and any two points sampled along the manifold of latent codes. We additively combine our regularizer with other adversarial losses using hyperparameters alpha_DPM and beta_DPM. Results: Experimental evaluations show that our DPM consistently improves the quality of generated images and reduces the likelihood of mode collapse across different datasets, architectures, and training settings. For instance, we observe improved results for both CIFAR-10 and ImageNet data sets, achieving better FID scores compared to existing state-of-the-art methods. Moreover, the visualizations depict superior outputs with higher fidelity and less blurriness in high-resolution images synthesis problems like ImageNet. Conclusion: By incorporating the DPM, we alleviate mode collapse in GANs during training, thereby resulting in increased stability and reduced likelihood of convergence to suboptimal solutions. This simple yet effective method shows promise as a generalizable solution for stabilizing GANs across various domains. Future directions might explore t",1
"We propose a conditional generative adversarial network (GAN) model for zero-shot video generation. In this study, we have explored zero-shot conditional generation setting. In other words, we generate unseen videos from training samples with missing classes. The task is an extension of conditional data generation. The key idea is to learn disentangled representations in the latent space of a GAN. To realize this objective, we base our model on the motion and content decomposed GAN and conditional GAN for image generation. We build the model to find better-disentangled representations and to generate good-quality videos. We demonstrate the effectiveness of our proposed model through experiments on the Weizmann action database and the MUG facial expression database.",0
"""In recent years, generative models have made significant advances in image synthesis tasks such as inpainting and super resolution. However, video generation has remained largely unexplored due to its high computational requirements and complexity. In this work, we present a novel approach for zero-shot video generation using conditional Moving Diffusion Probability (MoDPro) models. Our proposed method leverages conditional modularity to generate videos at multiple scales and frame rates while maintaining coherence and consistency throughout the sequence. We demonstrate that our model outperforms state-of-the-art methods in terms of visual fidelity, diversity, and speed on challenging benchmark datasets. This research paves the way for exciting future applications such as virtual reality content creation, special effects for movies, and interactive gaming.""",1
"Deep learning is a technique for machine learning using multi-layer neural networks. It has been used for image synthesis and image recognition, but in recent years, it has also been used for various social detection and social labeling. In this analysis, we compared (1) the number of Iterations per minute between the GPU and CPU when using the VGG model and the NIN model, and (2) the number of Iterations per minute by the number of pixels when using the VGG model, using an image with 128 pixels. When the number of pixels was 64 or 128, the processing time was almost the same when using the GPU, but when the number of pixels was changed to 256, the number of iterations per minute decreased and the processing time increased by about three times. In this case study, since the number of pixels becomes core dumping when the number of pixels is 512 or more, we can consider that we should consider improvement in the vector calculation part. If we aim to achieve 8K highly saturated computer graphics using neural networks, we will need to consider an environment that allows computation even when the size of the image becomes even more highly saturated and massive, and parallel computation when performing image recognition and tuning.",0
"When using GPUs in devising image generation methods using deep learning, state-of-the art results can be achieved through a variety of techniques and approaches. In recent years, advances in computer graphics have allowed for more efficient use of hardware resources such as GPUs, leading to significant improvements in image quality and processing time. Additionally, novel architectures like Generative Adversarial Networks (GANs) have been used extensively in generating images by leveraging the power of both unsupervised learning and supervised fine tuning. Furthermore, there has been significant progress in developing new loss functions and regularization techniques that improve stability during training, making it possible to achieve even better generations at higher resolutions. This research will provide readers with a comprehensive overview of cutting-edge developments in the field of using GPUs for image generation with deep learning, helping them stay abreast of latest trends, challenges, and opportunities. By studying these innovations, practitioners in industries ranging from entertainment to healthcare stand poised to benefit immensely from the potential applications of this technology, from automating content creation workflows to improving medical diagnostics accuracy. As such, this report serves as an indispensable resource to those who want to harness the full power of deep learning on GPUs for image generation purposes.",1
"This paper investigates an open research task of text-to-image synthesis for automatically generating or manipulating images from text descriptions. Prevailing methods mainly use the text as conditions for GAN generation, and train different models for the text-guided image generation and manipulation tasks. In this paper, we propose a novel unified framework of Cycle-consistent Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image manipulation tasks. Specifically, we first train a GAN model without text input, aiming to generate images with high diversity and quality. Then we learn a GAN inversion model to convert the images back to the GAN latent space and obtain the inverted latent codes for each image, where we introduce the cycle-consistency training to learn more robust and consistent inverted latent codes. We further uncover the latent space semantics of the trained GAN model, by learning a similarity model between text representations and the latent codes. In the text-guided optimization module, we generate images with the desired semantic attributes by optimizing the inverted latent codes. Extensive experiments on the Recipe1M and CUB datasets validate the efficacy of our proposed framework.",0
"We present a novel approach for text-to-image synthesis using inverse generative adversarial networks (GANs). Our method employs cycle consistency in both the generator network and discriminator network to improve stability and efficiency during training. By imposing constraints on the generator and discriminator separately, we can ensure that each component is optimized effectively without interfering with one another. Extensive experiments demonstrate the effectiveness of our approach compared to state-of-the-art methods, yielding improved visual fidelity and coherence in generated images while maintaining accurate alignment with input text descriptions.",1
"Generative diffusion models have emerged as leading models in speech and image generation. However, in order to perform well with a small number of denoising steps, a costly tuning of the set of noise parameters is needed. In this work, we present a simple and versatile learning scheme that can step-by-step adjust those noise parameters, for any given number of steps, while the previous work needs to retune for each number separately. Furthermore, without modifying the weights of the diffusion model, we are able to significantly improve the synthesis results, for a small number of steps. Our approach comes at a negligible computation cost.",0
"In this paper we develop a method for estimating noise in generative diffusion models (GDMs) that significantly outperforms previous methods based on heuristics. Our approach involves identifying the optimal parameter settings for GDM training and inference using variational autoencoders (VAEs). We show that our new method produces more accurate estimates of noise than state-of-the-art alternatives across three different benchmark datasets: MNIST, CelebA, and ImageNet. Furthermore, we demonstrate the effectiveness of our method by applying it to a wide range of real world tasks including image generation, denoising, and super resolution. We conclude that our work represents an important step towards improving the performance of GDMs in general, and lays the foundation for future research into these promising models.",1
"As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition (MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank recovery problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants.",0
"One possible abstract for a research paper evaluating different models for natural language processing could read:  Abstract: This paper compares two popular techniques for natural language processing (NLP): attention mechanisms and matrix decomposition methods such as singular value decomposition (SVD) and principal component analysis (PCA). We evaluate these approaches on several NLP tasks, including text classification, sentiment analysis, named entity recognition, and question answering. Our results show that while both attention and matrix decomposition can produce strong performance across all tasks, attention generally outperforms matrix decomposition by a significant margin. We attribute this advantage to the ability of attention to capture important relationships within and between sentences, allowing for more precise representation learning and better generalization to unseen data. Despite some limitations and potential computational constraints, we argue that attention deserves serious consideration as a powerful tool in modern NLP.  In summary, our work demonstrates that attention is superior to matrix decomposition in terms of achieving high accuracy on common benchmark datasets for NLP applications. However, the choice of model ultimately depends on specific task requirements and tradeoffs between efficiency and effectiveness. Further exploration into the benefits and drawbacks of alternative representations remains crucial for progress in NLP and artificial intelligence more broadly.",1
"In this study, we introduce a measure for machine perception, inspired by the concept of Just Noticeable Difference (JND) of human perception. Based on this measure, we suggest an adversarial image generation algorithm, which iteratively distorts an image by an additive noise until the model detects the change in the image by outputting a false label. The noise added to the original image is defined as the gradient of the cost function of the model. A novel cost function is defined to explicitly minimize the amount of perturbation applied to the input image while enforcing the perceptual similarity between the adversarial and input images. For this purpose, the cost function is regularized by the well-known total variation and bounded range terms to meet the natural appearance of the adversarial image. We evaluate the adversarial images generated by our algorithm both qualitatively and quantitatively on CIFAR10, ImageNet, and MS COCO datasets. Our experiments on image classification and object detection tasks show that adversarial images generated by our JND method are both more successful in deceiving the recognition/detection models and less perturbed compared to the images generated by the state-of-the-art methods, namely, FGV, FSGM, and DeepFool methods.",0
"This research focuses on understanding the perceptual limitations of machine learning models trained on regular images by generating adversarial images that minimize perturbations while still fooling the model. We use a technique called minimal adversarial examples generation which creates small modifications to the input image that cause the model output to change significantly but remain visually imperceptible from the original image. Our experiments show that even with minimal distortions, current state-of-the art deep convolutional neural networks can make mistakes on seemingly simple tasks such as classification of MNIST digits, CIFAR-10 and SVHN datasets. These results demonstrate the need for further research into developing more robust machine learning systems capable of detecting subtle changes in data inputs. Additionally, our findings shed light on the human ability to perceive minor differences between related objects and their representation in different mediums which has implications beyond computer vision research.",1
"Generating images according to natural language descriptions is a challenging task. Prior research has mainly focused to enhance the quality of generation by investigating the use of spatial attention and/or textual attention thereby neglecting the relationship between channels. In this work, we propose the Combined Attention Generative Adversarial Network (CAGAN) to generate photo-realistic images according to textual descriptions. The proposed CAGAN utilises two attention models: word attention to draw different sub-regions conditioned on related words; and squeeze-and-excitation attention to capture non-linear interaction among channels. With spectral normalisation to stabilise training, our proposed CAGAN improves the state of the art on the IS and FID on the CUB dataset and the FID on the more challenging COCO dataset. Furthermore, we demonstrate that judging a model by a single evaluation metric can be misleading by developing an additional model adding local self-attention which scores a higher IS, outperforming the state of the art on the CUB dataset, but generates unrealistic images through feature repetition.",0
"Title: CAGAN: Text-to-Image Generation with Combined Attention GANs Abstract: Image generation from textual descriptions has become increasingly popular in recent years due to advances in deep learning techniques. Many existing methods use Generative Adversarial Networks (GAN) to generate images, but these models often suffer from stability issues, lack of attention mechanisms, and limited diversity in generated outputs. In our work, we propose using combined attention mechanisms within GAN architectures to improve image quality, reduce instability, and increase output diversity. Our method, called CAGAN, utilizes two novel attention modules which focus on different aspects of the input texts - semantic content and spatial relationships between words. These attention modules are used alongside traditional adversarial loss functions in the training process. Experimental results show that CAGAN significantly outperforms other state-of-the-art approaches in terms of visual fidelity, stability, and diverse generation capabilities. This paper provides valuable insights into using attention mechanisms in GANs for generating high-quality images from textual descriptions.",1
"Empirically multidimensional discriminator (critic) output can be advantageous, while a solid explanation for it has not been discussed. In this paper, (i) we rigorously prove that high-dimensional critic output has advantage on distinguishing real and fake distributions; (ii) we also introduce an square-root velocity transformation (SRVT) block which further magnifies this advantage. The proof is based on our proposed maximal p-centrality discrepancy which is bounded above by p-Wasserstein distance and perfectly fits the Wasserstein GAN framework with high-dimensional critic output n. We have also showed when n = 1, the proposed discrepancy is equivalent to 1-Wasserstein distance. The SRVT block is applied to break the symmetric structure of high-dimensional critic output and improve the generalization capability of the discriminator network. In terms of implementation, the proposed framework does not require additional hyper-parameter tuning, which largely facilitates its usage. Experiments on image generation tasks show performance improvement on benchmark datasets.",0
"Advances in deep learning have enabled the development of powerful generative models that can generate realistic images, videos, audio, and text. However, training these models requires careful tuning and manual effort by domain experts. To address this challenge, we propose a novel framework called Autonomous Weakly Guided Attention (AWGAN), which combines high-dimensional discriminator output with traditional attention mechanisms to automatically focus on meaningful features during training. Our approach improves upon existing methods by incorporating learned priors from unlabeled data into the GAN loss function, allowing the model to learn more effectively without human supervision. Experimental results show that our method outperforms state-of-the-art GAN variants across multiple domains, demonstrating the effectiveness of autonomously guiding attention in generative adversarial networks. Overall, AWGAN provides a promising new direction for developing flexible, robust, and automatic systems capable of generating diverse, high-quality outputs across different modalities.",1
"Deep neural networks have become the default choice for many applications like image and video recognition, segmentation and other image and video related tasks.However, a critical challenge with these models is the lack of explainability.This requirement of generating explainable predictions has motivated the research community to perform various analysis on trained models.In this study, we analyze the learned feature maps of trained models using MNIST images for achieving more explainable predictions.Our study is focused on deriving a set of primitive elements, here called visual concepts, that can be used to generate any arbitrary sample from the data generating distribution.We derive the primitive elements from the feature maps learned by the model.We illustrate the idea by generating visual concepts from a Variational Autoencoder trained using MNIST images.We augment the training data of MNIST dataset by adding about 60,000 new images generated with visual concepts chosen at random.With this we were able to reduce the reconstruction loss (mean square error) from an initial value of 120 without augmentation to 60 with augmentation.Our approach is a first step towards the final goal of achieving trained deep neural network models whose predictions, features in hidden layers and the learned filters can be well explained.Such a model when deployed in production can easily be modified to adapt to new data, whereas existing deep learning models need a re training or fine tuning. This process again needs a huge number of data samples that are not easy to generate unless the model has good explainability.",0
"This paper presents a novel approach to learning a vocabulary of visual concepts and operators that can be used to perform complex tasks on images. Our method leverages deep neural networks to learn both high level concept representations and low level operator representations from raw image data. We demonstrate that our model is able to generate valid operator sequences for a range of visual tasks including image manipulation, scene understanding, and object detection. Additionally, we show that our learned operators generalize well across different datasets and task domains, allowing them to be applied to new scenarios without retraining. Overall, our work represents a significant step towards enabling machines to interpret and manipulate visual content at human levels of performance.",1
"Generative Adversarial Networks (GANs) have become the most used networks towards solving the problem of image generation. Self-supervised GANs are later proposed to avoid the catastrophic forgetting of the discriminator and to improve the image generation quality without needing the class labels. However, the generalizability of the self-supervision tasks on different GAN architectures is not studied before. To that end, we extensively analyze the contribution of a previously proposed self-supervision task, deshuffling of the DeshuffleGANs in the generalizability context. We assign the deshuffling task to two different GAN discriminators and study the effects of the task on both architectures. We extend the evaluations compared to the previously proposed DeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the best FID results for several datasets compared to the other self-supervised GANs. Furthermore, we compare the deshuffling with the rotation prediction that is firstly deployed to the GAN training and demonstrate that its contribution exceeds the rotation prediction. We design the conditional DeshuffleGAN called cDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we show the contribution of the self-supervision tasks to the GAN training on the loss landscape and present that the effects of these tasks may not be cooperative to the adversarial training in some settings. Our code can be found at https://github.com/gulcinbaykal/DeshuffleGAN.",0
"Artificial intelligence (AI) has revolutionized many areas of science and technology by providing automation of complex tasks such as image generation, speech recognition, natural language processing, and many others. Despite recent advances, there remains a fundamental challenge in designing algorithms that can generate high quality outputs without requiring large amounts of labeled data. In recent years, generative adversarial networks (GANs) have emerged as a powerful tool for generating images and other forms of data, but they still suffer from several drawbacks including mode collapse, poor performance on unseen domains, lack of controllability, and instability during training. Recently, deshuffling GANs were introduced to address some of these issues by using a novel regularization term called deshuffling penalty which encourages the generator network to use all possible combinations of feature vectors to produce diverse output samples. This study explores the effectiveness of DeshuffleGANs in improving image generation performance compared to traditional GANs trained under supervision and self-supervision schemes. Our experimental results show that DeshuffleGANs outperform state-of-the-art methods across multiple evaluation metrics and demonstrate better generalizability over unseen datasets. These findings contribute towards achieving greater efficiency in developing robust AI systems capable of producing high quality outputs without requiring substantial amounts of training data.",1
"We proposes a flexible person generation framework called Dressing in Order (DiOr), which supports 2D pose transfer, virtual try-on, and several fashion editing tasks. The key to DiOr is a novel recurrent generation pipeline to sequentially put garments on a person, so that trying on the same garments in different orders will result in different looks. Our system can produce dressing effects not achievable by existing work, including different interactions of garments (e.g., wearing a top tucked into the bottom or over it), as well as layering of multiple garments of the same type (e.g., jacket over shirt over t-shirt). DiOr explicitly encodes the shape and texture of each garment, enabling these elements to be edited separately. Joint training on pose transfer and inpainting helps with detail preservation and coherence of generated garments. Extensive evaluations show that DiOr outperforms other recent methods like ADGAN in terms of output quality, and handles a wide range of editing functions for which there is no direct supervision.",0
"In recent years, there has been significant progress in computer vision research focused on developing methods that can generate realistic images of clothing items and outfits. One particular challenge in generating these images lies in accurately representing the pose of the person wearing the garments, as different poses can greatly affect how an item appears. This paper presents a novel approach to addressing this issue by leveraging recurrent neural networks (RNNs) in combination with convolutional neural networks (CNNs). Our method allows us to transfer the pose from one image to another while maintaining high fidelity, creating virtual try-on scenarios where users can see themselves in new clothes from multiple angles. Additionally, our method supports editing operations like replacing textures, adding patterns, changing colors, removing accessories, and many more. We evaluate our model through extensive experimentation and demonstrate state-of-the-art performance across several metrics. Our results showcase the effectiveness of using RNNs for pose transfer and provide exciting opportunities for enhancing online shopping experiences through virtual reality technologies. Overall, our work represents an important step forward in advancing computer vision techniques applied to fashion and retail industries.",1
"This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (eg, class labels); conditioning on regression labels is mathematically distinct and raises two fundamental problems:(P1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (aka empirical cGAN losses) often fails in practice;(P2) Since regression labels are scalar and infinitely many, conventional label input methods are not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a naive label input (NLI) method and an improved label input (ILI) method to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. The error bounds of a discriminator trained with HVDL and SVDL are derived under mild assumptions in this work. Two new benchmark datasets (RC-49 and Cell-200) and a novel evaluation metric (Sliding Fr\'echet Inception Distance) are also proposed for this continuous scenario. Our experiments on the Circular 2-D Gaussians, RC-49, UTKFace, Cell-200, and Steering Angle datasets show that CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label. Moreover, in these experiments, CcGAN substantially outperforms cGAN both visually and quantitatively.",0
"This paper presents a novel approach for training continuous conditional generative adversarial networks (CcGANs), which allows them to generate high-quality images that can fool human observers. We introduce two new mechanisms to improve the performance of CcGANs: a new loss function based on perceptual similarity metrics and a label input mechanism that ensures more effective utilization of labeled data during training. Our results show that these innovations lead to significant improvements over state-of-the-art methods across multiple benchmark datasets, demonstrating the effectiveness of our approach.",1
"Training real-world neural network models to achieve high performance and generalizability typically requires a substantial amount of labeled data, spanning a broad range of variation. This data-labeling process can be both labor and cost intensive. To achieve desirable predictive performance, a trained model is typically applied into a domain where the data distribution is similar to the training dataset. However, for many agricultural machine learning problems, training datasets are collected at a specific location, during a specific period in time of the growing season. Since agricultural systems exhibit substantial variability in terms of crop type, cultivar, management, seasonal growth dynamics, lighting condition, sensor type, etc, a model trained from one dataset often does not generalize well across domains. To enable more data efficient and generalizable neural network models in agriculture, we propose a method that generates photorealistic agricultural images from a synthetic 3D crop model domain into real world crop domains. The method uses a semantically constrained GAN (generative adversarial network) to preserve the fruit position and geometry. We observe that a baseline CycleGAN method generates visually realistic target domain images but does not preserve fruit position information while our method maintains fruit positions well. Image generation results in vineyard grape day and night images show the visual outputs of our network are much better compared to a baseline network. Incremental training experiments in vineyard grape detection tasks show that the images generated from our method can significantly speed the domain adaption process, increase performance for a given number of labeled images (i.e. data efficiency), and decrease labeling requirements.",0
"This paper presents a framework that combines 3D crop models with Generative Adversarial Network (GAN) to improve fruit detection efficiency and generalization performance. Incorporating 3D crop knowledge can reduce computational requirements and enable better understanding of visual features required by GANs to produce high quality synthetic data. By enhancing detectors with such more accurate training samples, we expect our approach to outperform previous methods that solely relied on supervised learning from real images. We demonstrate that our system achieves state-of-the-art results under low sample budgets and strong domain shifts across diverse datasets, including apple defect classification and orange region detection. Our work provides valuable insights into the design of more efficient and effective GAN-based generators, and how these advances can translate to important applications in agriculture. This research paves the path towards new possibilities in developing smart robotics systems that automate critical tasks for farmers while improving food sustainability. Ultimately, this investigation opens up opportunities for further exploration into applying AI towards addressing global challenges associated with climate change and population growth.",1
"Current state-of-the-art photorealistic generators are computationally expensive, involve unstable training processes, and have real and synthetic distributions that are dissimilar in higher-dimensional spaces. To solve these issues, we propose a variant of the StackGAN architecture. The new architecture incorporates conditional generators to construct an image in many stages. In our model, we generate grayscale facial images in two different stages: noise to edges (stage one) and edges to grayscale (stage two). Our model is trained with the CelebA facial image dataset and achieved a Fr\'echet Inception Distance (FID) score of 73 for edge images and a score of 59 for grayscale images generated using the synthetic edge images. Although our model achieved subpar results in relation to state-of-the-art models, dropout layers could reduce the overfitting in our conditional mapping. Additionally, since most images can be broken down into important features, improvements to our model can generalize to other datasets. Therefore, our model can potentially serve as a superior alternative to traditional means of generating photorealistic images.",0
"Here is a possible abstract for the paper ""StackGAN: Facial Image Generation Optimizations"":  In recent years, Generative Adversarial Networks (GANs) have emerged as a powerful tool for generating high quality images across a wide range of domains, including natural scenes, objects, and faces. However, training GANs remains a challenging task due to instability issues that arise during optimization. In this paper we present an improved version of the popular DCGAN architecture known as StackGAN which incorporates new techniques aimed at improving stability during training and generating higher quality results. Our contributions include: introducing a novel batch normalization technique specifically tailored for stabilizing GAN training; developing a novel loss function better suited for capturing fine details, such as facial features, and significantly reducing noise present in generated samples; and integrating these improvements into an end-to-end trainable pipeline using PyTorch. We evaluate our model on several benchmark datasets demonstrating state-of-the-art performance for image generation tasks, while providing a more stable training process compared to current architectures.",1
"Over recent years, deep learning-based computer vision systems have been applied to images at an ever-increasing pace, oftentimes representing the only type of consumption for those images. Given the dramatic explosion in the number of images generated per day, a question arises: how much better would an image codec targeting machine-consumption perform against state-of-the-art codecs targeting human-consumption? In this paper, we propose an image codec for machines which is neural network (NN) based and end-to-end learned. In particular, we propose a set of training strategies that address the delicate problem of balancing competing loss functions, such as computer vision task losses, image distortion losses, and rate loss. Our experimental results show that our NN-based codec outperforms the state-of-the-art Versa-tile Video Coding (VVC) standard on the object detection and instance segmentation tasks, achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast thanks to its compact size. To the best of our knowledge, this is the first end-to-end learned machine-targeted image codec.",0
"In recent years there has been increased interest in using machine learning methods such as deep neural networks (DNNs) for image compression. Despite their promising results, existing approaches typically rely on pre-trained models trained on other tasks, which may not necessarily capture all the relevant visual features required for effective coding efficiency. Here we present a novel end-to-end trainable image coder that exploits learned representations by training it from scratch to optimize directly for rate distortion metrics. Our framework introduces two key novelties: an autoregressive model to generate codevectors conditioned on previous layers, and a progressive refinement module to iteratively adapt to new input sequences. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance compared to non-learned coders across a wide range of fidelity measures while meeting desired bitrates for different applications. Code is available at <http://github.com/openai/imagecodingforsmachines>. This work provides valuable insights into developing more efficient image coders via DNN architectures designed specifically for target metric optimization without relying solely on offline fine-tuning. Potential future research directions extend beyond traditional lossy compression towards generative modelling and domain adaptation under limited resources constraints.",1
"While the researches on single image super-resolution (SISR), especially equipped with deep neural networks (DNNs), have achieved tremendous successes recently, they still suffer from two major limitations. Firstly, the real image degradation is usually unknown and highly variant from one to another, making it extremely hard to train a single model to handle the general SISR task. Secondly, most of current methods mainly focus on the downsampling process of the degradation, but ignore or underestimate the inevitable noise contamination. For example, the commonly-used independent and identically distributed (i.i.d.) Gaussian noise distribution always largely deviates from the real image noise (e.g., camera sensor noise), which limits their performance in real scenarios. To address these issues, this paper proposes a model-based unsupervised SISR method to deal with the general SISR task with unknown degradations. Instead of the traditional i.i.d. Gaussian noise assumption, a novel patch-based non-i.i.d. noise modeling method is proposed to fit the complex real noise. Besides, a deep generator parameterized by a DNN is used to map the latent variable to the high-resolution image, and the conventional hyper-Laplacian prior is also elaborately embedded into such generator to further constrain the image gradients. Finally, a Monte Carlo EM algorithm is designed to solve our model, which provides a general inference framework to update the image generator both w.r.t. the latent variable and the network parameters. Comprehensive experiments demonstrate that the proposed method can evidently surpass the current state of the art (SotA) method (about 1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster speed.",0
"This article examines unsupervised single image super-resolution under complex noise. In recent years, deep learning has made significant progress in high-quality super-resolution (SR). However, current state-of-the-art SR methods typically require large amounts of training data, which limits their applicability to many real-world scenarios where only one low-quality version of an image is available. To address this limitation, we propose an approach that leverages self-ensemble techniques to achieve single image SR under real-world conditions of complex noise such as motion blur, JPEG compression artifacts, etc., without requiring any additional training images. We evaluate our method on several public datasets and demonstrate that it outperforms traditional bicubic interpolation and other leading state-of-the-art SR algorithms by a wide margin. Our work represents an important step towards enabling the development of robust SR solutions for real-world imaging applications.",1
"We propose a manifold matching approach to generative models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high-dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and $p$-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks are learned simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in unconditional image generation task, the proposed method obtains competitive results compared with existing generative models; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Experiments and analysis demonstrate the feasibility and effectiveness of the proposed framework.",0
"Manifold matching via deep metric learning (MMDLM) has been recently proposed as a powerful methodology that can handle multiple manifolds for generative modeling tasks such as image generation, style transfer, and superresolution. By using MMDLM, we aim to address limitations inherent in previous works that only used one manifold for each task. In this work, we present an extensive evaluation of the effectiveness and efficiency of our approach, comparing it against several state-of-the-art methods on benchmark datasets. Our results demonstrate significant improvements over existing approaches in terms of quality metrics and visual fidelity. Furthermore, our qualitative evaluations show more coherence and consistency across multiple manifolds in generated outputs. We believe that MMDLM represents a promising direction towards advanced generative modeling and hope that our findings inspire future research efforts in this field.",1
"Conditional image synthesis from layout has recently attracted much interest. Previous approaches condition the generator on object locations as well as class labels but lack fine-grained control over the diverse appearance aspects of individual objects. Gaining control over the image generation process is fundamental to build practical applications with a user-friendly interface. In this paper, we propose a method for attribute controlled image synthesis from layout which allows to specify the appearance of individual objects without affecting the rest of the image. We extend a state-of-the-art approach for layout-to-image generation to additionally condition individual objects on attributes. We create and experiment on a synthetic, as well as the challenging Visual Genome dataset. Our qualitative and quantitative results show that our method can successfully control the fine-grained details of individual objects when modelling complex scenes with multiple objects. Source code, dataset and pre-trained models are publicly available (https://github.com/stanifrolov/AttrLostGAN).",0
"Title: ""Attribute Controlled Image Synthesis""  The process of generating images is no longer limited to manual methods such as painting or photography, but can now be done using artificial intelligence techniques like generative adversarial networks (GANs). In this paper we present a novel approach that allows users to generate new images with specific attributes by controlling the layout and style. This technique, called AttrLostGAN, uses two GANs working together to create images based on user inputted preferences. Our results show that our method outperforms existing state-of-the-art approaches in terms of both visual fidelity and efficiency. We believe that our work opens up new possibilities for image generation and has many potential applications across different fields.",1
"In this paper, we treat the image generation task using an autoencoder, a representative latent model. Unlike many studies regularizing the latent variable's distribution by assuming a manually specified prior, we approach the image generation task using an autoencoder by directly estimating the latent distribution. To this end, we introduce 'latent density estimator' which captures latent distribution explicitly and propose its structure. Through experiments, we show that our generative model generates images with the improved visual quality compared to previous autoencoder-based generative models.",0
This could be used as the first page in a scientific journal article that describes an experiment that trained an unprincipled autoencoder on the MNIST dataset and then generated new images from it by starting with noise vectors sampled uniformly from a unit sphere centered at the origin in high dimensional space.,1
"Radio echo sounding (RES) is a common technique used in subsurface glacial imaging, which provides insight into the underlying rock and ice. However, systematic noise is introduced into the data during collection, complicating interpretation of the results. Researchers most often use a combination of manual interpretation and filtering techniques to denoise data; however, these processes are time intensive and inconsistent. Fully Convolutional Networks have been proposed as an automated alternative to identify layer boundaries in radargrams. However, they require high-quality manually processed training data and struggle to interpolate data in noisy samples (Varshney et al. 2020).   Herein, the authors propose a GAN based model to interpolate layer boundaries through noise and highlight layers in two-dimensional glacial RES data. In real-world noisy images, filtering often results in loss of data such that interpolating layer boundaries is nearly impossible. Furthermore, traditional machine learning approaches are not suited to this task because of the lack of paired data, so we employ an unpaired image-to-image translation model. For this model, we create a synthetic dataset to represent the domain of images with clear, highlighted layers and use an existing real-world RES dataset as our noisy domain.   We implement a CycleGAN trained on these two domains to highlight layers in noisy images that can interpolate effectively without significant loss of structure or fidelity. Though the current implementation is not a perfect solution, the model clearly highlights layers in noisy data and allows researchers to determine layer size and position without mathematical filtering, manual processing, or ground-truth images for training. This is significant because clean images generated by our model enable subsurface researchers to determine glacial layer thickness more efficiently.",0
"In recent years, deep learning has revolutionized the field of image processing and analysis. One challenge that remains is how to effectively process raw radar imagery data (RES) collected by drones and other unmanned aerial vehicles. This type of data can contain large amounts of noise and interference from surrounding objects, making it difficult to extract important features such as buildings, roads, and vegetation. To address this issue, we propose using cycle-consistent adversarial networks (CycleGANs), which have been shown to generate high quality images in other domains. Our approach involves training two generative models on paired noisy/clean image datasets, each equipped with a discriminator network that learns to distinguish real images from generated ones. We then apply these trained models to new sets of noisy input images, generating clean versions that preserve key details while removing unwanted noise. Experimental results demonstrate significant improvements over baseline methods in terms of both visual fidelity and quantitative metrics such as mean squared error and structural similarity index measure (SSIM). Overall, our work shows promise for enabling more accurate and efficient automatic feature extraction from complex RES data.",1
"Generative adversarial networks have been widely used in image synthesis in recent years and the quality of the generated image has been greatly improved. However, the flexibility to control and decouple facial attributes (e.g., eyes, nose, mouth) is still limited. In this paper, we propose a novel approach, called ChildGAN, to generate a child's image according to the images of parents with heredity prior. The main idea is to disentangle the latent space of a pre-trained generation model and precisely control the face attributes of child images with clear semantics. We use distances between face landmarks as pseudo labels to figure out the most influential semantic vectors of the corresponding face attributes by calculating the gradient of latent vectors to pseudo labels. Furthermore, we disentangle the semantic vectors by weighting irrelevant features and orthogonalizing them with Schmidt Orthogonalization. Finally, we fuse the latent vector of the parents by leveraging the disentangled semantic vectors under the guidance of biological genetic laws. Extensive experiments demonstrate that our approach outperforms the existing methods with encouraging results.",0
"In recent years, Generative Adversarial Networks (GAN) have been widely applied to many fields such as computer vision, natural language processing and others due to their remarkable generative performance on complex datasets. Motivated by this success, we develop heredity-aware child face image synthesis with latent space disentanglement through GANs.",1
"In this paper, we propose a novel loss function for training Generative Adversarial Networks (GANs) aiming towards deeper theoretical understanding as well as improved stability and performance for the underlying optimization problem. The new loss function is based on cumulant generating functions giving rise to \emph{Cumulant GAN}. Relying on a recently-derived variational formula, we show that the corresponding optimization problem is equivalent to R{\'e}nyi divergence minimization, thus offering a (partially) unified perspective of GAN losses: the R{\'e}nyi family encompasses Kullback-Leibler divergence (KLD), reverse KLD, Hellinger distance and $\chi^2$-divergence. Wasserstein GAN is also a member of cumulant GAN. In terms of stability, we rigorously prove the linear convergence of cumulant GAN to the Nash equilibrium for a linear discriminator, Gaussian distributions and the standard gradient descent ascent algorithm. Finally, we experimentally demonstrate that image generation is more robust relative to Wasserstein GAN and it is substantially improved in terms of both inception score and Fr\'echet inception distance when both weaker and stronger discriminators are considered.",0
"Artificial intelligence (AI) has made significant progress over recent years due to advances in deep learning techniques such as generative adversarial networks (GANs). However, these models can often suffer from instability and unreliability during training. One approach to overcome this problem is by using cumulative distribution functions (CDFs), which have been used successfully in other fields such as economics and finance. In our work, we propose the use of Cumulant Generative Adversarial Networks (CuGANs) where the generator and discriminator are trained using the cumulative distributions of their inputs instead of the raw data itself. This approach leads to improved stability and reliability during training while still generating high quality images comparable to state-of-the-art GANs. Our results show that CuGANs achieve better performance on several benchmark datasets compared to traditional GANs, demonstrating the effectiveness of our proposed methodology. The implications of our findings extend beyond computer vision applications and could potentially lead to novel uses across multiple domains affected by instabilities during optimization processes involving machine learning algorithms.",1
"Single image generative models perform synthesis and manipulation tasks by capturing the distribution of patches within a single image. The classical (pre Deep Learning) prevailing approaches for these tasks are based on an optimization process that maximizes patch similarity between the input and generated output. Recently, however, Single Image GANs were introduced both as a superior solution for such manipulation tasks, but also for remarkable novel generative tasks. Despite their impressiveness, single image GANs require long training time (usually hours) for each image and each task. They often suffer from artifacts and are prone to optimization issues such as mode collapse. In this paper, we show that all of these tasks can be performed without any training, within several seconds, in a unified, surprisingly simple framework. We revisit and cast the ""good-old"" patch-based methods into a novel optimization-free framework. We start with an initial coarse guess, and then simply refine the details coarse-to-fine using patch-nearest-neighbor search. This allows generating random novel images better and much faster than GANs. We further demonstrate a wide range of applications, such as image editing and reshuffling, retargeting to different sizes, structural analogies, image collage and a newly introduced task of conditional inpainting. Not only is our method faster ($\times 10^3$-$\times 10^4$ than a GAN), it produces superior results (confirmed by quantitative and qualitative evaluation), less artifacts and more realistic global structure than any of the previous approaches (whether GAN-based or classical patch-based).",0
"Title: ""Single Image Generative Models without GANs: The Case for Patches Nearest Neighbors""  In recent years, Generative Adversarial Networks (GANs) have become a popular choice for generating realistic images using deep learning techniques. However, these models can suffer from stability issues during training and require careful hyperparameter tuning to produce high quality results. Additionally, they rely on adversarial training which may not always be well suited for all tasks. This has led some researchers to explore alternative generative models that do not use GANs. One such approach is the use of patch-based nearest neighbors to generate single images. These methods have been shown to produce comparable results to GANs while offering greater simplicity and ease of implementation.  The main advantage of patch-based nearest neighbor models is their ability to leverage pre-trained convolutional neural networks (CNNs). By extracting features from existing image datasets using CNNs, these models can learn local patterns and structures which can then be used to synthesize new images. Furthermore, by using simple nearest neighbor search operations rather than complex adversarial training, patch-based models eliminate many of the instability and optimization challenges associated with GANs.  While patch-based nearest neighbor models have been previously explored in limited contexts, this work demonstrates their effectiveness across a range of diverse tasks including object transfiguration, style transfer, and super resolution. Through extensive empirical evaluation, we show that these models can achieve competitive performance compared to state-of-the-art GAN-based approaches while requiring significantly fewer parameters and computational resources. Overall, our findings suggest that patch-based nearest neighbors could emerge as a promising solution for many computer vision problems where traditional GAN architectures may fall short.",1
"Self-supervised monocular depth estimation has become an appealing solution to the lack of ground truth labels, but its reconstruction loss often produces over-smoothed results across object boundaries and is incapable of handling occlusion explicitly. In this paper, we propose a new approach to leverage pseudo ground truth depth maps of stereo images generated from self-supervised stereo matching methods. The confidence map of the pseudo ground truth depth map is estimated to mitigate performance degeneration by inaccurate pseudo depth maps. To cope with the prediction error of the confidence map itself, we also leverage the threshold network that learns the threshold dynamically conditioned on the pseudo depth maps. The pseudo depth labels filtered out by the thresholded confidence map are used to supervise the monocular depth network. Furthermore, we propose the probabilistic framework that refines the monocular depth map with the help of its uncertainty map through the pixel-adaptive convolution (PAC) layer. Experimental results demonstrate superior performance to state-of-the-art monocular depth estimation methods. Lastly, we exhibit that the proposed threshold learning can also be used to improve the performance of existing confidence estimation approaches.",0
Monocular Depth Estimation (MDE) plays a crucial role in Computer Vision owing to its ability to estimate accurate depth from single images. This study introduces novel adaptive confid,1
"In this paper, we propose a graph-based image-to-image translation framework for generating images. We use rich data collected from the popular creativity platform Artbreeder (http://artbreeder.com), where users interpolate multiple GAN-generated images to create artworks. This unique approach of creating new images leads to a tree-like structure where one can track historical data about the creation of a particular image. Inspired by this structure, we propose a novel graph-to-image translation model called Graph2Pix, which takes a graph and corresponding images as input and generates a single image as output. Our experiments show that Graph2Pix is able to outperform several image-to-image translation frameworks on benchmark metrics, including LPIPS (with a 25% improvement) and human perception studies (n=60), where users preferred the images generated by our method 81.5% of the time. Our source code and dataset are publicly available at https://github.com/catlab-team/graph2pix.",0
"This framework can translate images from one domain into another. For example, you could use Graph2Pix to take an image of a dog on a beach and generate an image showing that same dog in the snowy mountains. The authors developed two models, one trained for unpaired translation (like a cat to a dog) and the other for paired translation (where you have images from both domains). They show results where their system generates high quality outputs on par with those generated by human artists. One thing they note is that the model seems particularly good at preserving some features while hallucinating others. They hope this research opens up new possibilities for computer vision like generating novel views synthesized from multiple examples. Abstract: A new graph-based image to image translation framework, named Graph2Pix, has been proposed as a method for translating images across different domains. By utilizing the power of graphs, this approach enables effective mapping and manipulation of visual data from one context to another. Specifically, the authors introduce two variants of the Graph2Pix model - one designed for unpaired image translation and the other tailored for paired translation tasks. Extensive experiments demonstrate the remarkable ability of these systems to produce highly accurate results on par with images generated by humans. Perhaps most importantly, this technique shows great promise for enhancing traditional methods in areas such as computer vision by allowing for more advanced generation of novel views through synthesis of multiple examples. Overall, the development of Graph2Pix marks an exciting advancement in the field with vast potential applications.",1
"Deep Neural Networks have been very successfully used for many computer vision and pattern recognition applications. While Convolutional Neural Networks(CNNs) have shown the path to state of art image classifications, Generative Adversarial Networks or GANs have provided state of art capabilities in image generation. In this paper we extend the applications of CNNs and GANs to experiment with up-sampling techniques in the domains of security and surveillance. Through this work we evaluate, compare and contrast the state of art techniques in both CNN and GAN based image and video up-sampling in the surveillance domain. As a result of this study we also provide experimental evidence to establish DISTS as a stronger Image Quality Assessment(IQA) metric for comparing GAN Based Image Up-sampling in the surveillance domain.",0
"This might sound tricky but I believe you can do this if you follow these guidelines: • No technical terms (jargon), except common everyday ones that have entered widespread usage • Use everyday language that most educated adults would grasp easily, as well as specialized ideas from fields like art history • Emphasize the research question, methodology, & key findings/implications • Provide some context on why up-sampling may be important in today’s surveillance applications. Here goes my attempt to summarise your paper:  In recent years, advances in generative adversarial networks (GAN) have opened new possibilities for image generation and manipulation. But how effective are these tools at enhancing images beyond their original resolution? Can they provide useful data for security or military purposes, such as identifying faces or detecting objects? In our study we aimed to evaluate the performance of two widely used GAN algorithms - StarGAN and CycleGAN – using a benchmark dataset commonly employed by the computer vision community. We found both models performed reasonably, although more refinement may be required before the technology could offer significant benefits in surveillance scenarios. One critical factor affecting quality was model size; larger models generally outperformed smaller variants even though inference took longer. Despite limitations, future research into upscaling methods using deep learning techniques has potential to improve the capabilities of today's cameras. Ultimately, GANs should complement rather than replace traditional imaging approaches like superresolution.",1
"Conditional Generative Adversarial Networks (cGANs) extend the standard unconditional GAN framework to learning joint data-label distributions from samples, and have been established as powerful generative models capable of generating high-fidelity imagery. A challenge of training such a model lies in properly infusing class information into its generator and discriminator. For the discriminator, class conditioning can be achieved by either (1) directly incorporating labels as input or (2) involving labels in an auxiliary classification loss. In this paper, we show that the former directly aligns the class-conditioned fake-and-real data distributions $P(\text{image}|\text{class})$ ({\em data matching}), while the latter aligns data-conditioned class distributions $P(\text{class}|\text{image})$ ({\em label matching}). Although class separability does not directly translate to sample quality and becomes a burden if classification itself is intrinsically difficult, the discriminator cannot provide useful guidance for the generator if features of distinct classes are mapped to the same point and thus become inseparable. Motivated by this intuition, we propose a Dual Projection GAN (P2GAN) model that learns to balance between {\em data matching} and {\em label matching}. We then propose an improved cGAN model with Auxiliary Classification that directly aligns the fake and real conditionals $P(\text{class}|\text{image})$ by minimizing their $f$-divergence. Experiments on a synthetic Mixture of Gaussian (MoG) dataset and a variety of real-world datasets including CIFAR100, ImageNet, and VGGFace2 demonstrate the efficacy of our proposed models.",0
"Abstract: This research presents a novel approach to conditional image generation using dual projection generative adversarial networks (DPGAN). Existing methods for conditional image generation often suffer from limitations such as mode collapse and poor quality outputs. Our method addresses these issues by introducing two discriminators that operate on different levels of abstraction, allowing the generator to produce more diverse and detailed images while still conforming to the given conditions. We evaluate our model on several benchmark datasets and demonstrate significant improvements over state-of-the-art methods in terms of visual fidelity, diversity, and alignment with conditioning variables. The proposed framework has potential applications in areas such as computer vision, graphics, and art generation, making it a valuable addition to the field of generative adversarial networks. Keywords: generative adversarial networks, conditional image generation, dual projection, mode collapse, variational autoencoders.",1
"Multi-modal generation has been widely explored in recent years. Current research directions involve generating text based on an image or vice versa. In this paper, we propose a new task called CIGLI: Conditional Image Generation from Language and Image. Instead of generating an image based on text as in text-image generation, this task requires the generation of an image from a textual description and an image prompt. We designed a new dataset to ensure that the text description describes information from both images, and that solely analyzing the description is insufficient to generate an image. We then propose a novel language-image fusion model which improves the performance over two established baseline methods, as evaluated by quantitative (automatic) and qualitative (human) evaluations. The code and dataset is available at https://github.com/vincentlux/CIGLI.",0
This can go here as well...,1
"Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.",0
"Image synthesis has been a challenging task in computer vision due to its complexity and high computational requirements. Recent advances have utilized deep neural networks such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), which struggle with generating images that preserve global structure and local details. To address these limitations, we propose a new method called ImageBART, which uses bidirectional context with multinomial diffusion for autoregressive image synthesis. Our approach combines autoregressive models and diffusion methods, enabling efficient generation of detailed images while maintaining their overall coherence. We demonstrate the superiority of our model through extensive experiments on diverse benchmark datasets against state-of-the-art approaches. These results confirm the effectiveness and versatility of our proposed method for complex image synthesis tasks, opening up new opportunities for real-world applications.",1
"This paper considers the problem of generating an HDR image of a scene from its LDR images. Recent studies employ deep learning and solve the problem in an end-to-end fashion, leading to significant performance improvements. However, it is still hard to generate a good quality image from LDR images of a dynamic scene captured by a hand-held camera, e.g., occlusion due to the large motion of foreground objects, causing ghosting artifacts. The key to success relies on how well we can fuse the input images in their feature space, where we wish to remove the factors leading to low-quality image generation while performing the fundamental computations for HDR image generation, e.g., selecting the best-exposed image/region. We propose a novel method that can better fuse the features based on two ideas. One is multi-step feature fusion; our network gradually fuses the features in a stack of blocks having the same structure. The other is the design of the component block that effectively performs two operations essential to the problem, i.e., comparing and selecting appropriate images/regions. Experimental results show that the proposed method outperforms the previous state-of-the-art methods on the standard benchmark tests.",0
"High dynamic range (HDR) imaging has become increasingly important due to its ability to capture scenes with high contrast ratios that cannot be captured by traditional cameras. However, HDR imaging poses many challenges such as ghosting artifacts caused by motion during exposure and limited resolution compared to low dynamic range images. To address these issues, we propose a new method called Progressive and Selective Fusion Network (PSFN) which combines multiple exposures in real-time while preserving details in both bright and dark areas. Our approach utilizes a dual network architecture where one network captures global context while the other focuses on local details. The two networks work together to generate high quality HDR images free from ghosting effects. Experiments show that our proposed PSFN outperforms state-of-the-art methods in terms of visual quality and objective metrics. This technique paves the way for more advanced applications of HDR imaging such as virtual reality and augmented reality.",1
"Image generation has been heavily investigated in computer vision, where one core research challenge is to generate images from arbitrarily complex distributions with little supervision. Generative Adversarial Networks (GANs) as an implicit approach have achieved great successes in this direction and therefore been employed widely. However, GANs are known to suffer from issues such as mode collapse, non-structured latent space, being unable to compute likelihoods, etc. In this paper, we propose a new unsupervised non-parametric method named mixture of infinite conditional GANs or MIC-GANs, to tackle several GAN issues together, aiming for image generation with parsimonious prior knowledge. Through comprehensive evaluations across different datasets, we show that MIC-GANs are effective in structuring the latent space and avoiding mode collapse, and outperform state-of-the-art methods. MICGANs are adaptive, versatile, and robust. They offer a promising solution to several well-known GAN issues. Code available: github.com/yinghdb/MICGANs.",0
"This paper presents a new approach to unsupervised image generation using infinite generative adversarial networks (IGAN). Traditional GANs have struggled with instability and mode collapse, but IGAN addresses these issues by using a theoretically infinite number of discriminator classifiers to ensure that the generator always has a competitive opponent. We show that our method outperforms other state-of-the-art techniques on several benchmark datasets, generating high-quality images with greater variability and less noise. Our results suggest that IGAN may provide a powerful tool for creating realistic synthetic data, helping advance research areas such as computer vision, graphics, and artificial intelligence.",1
"Unsupervised disentanglement learning is a crucial issue for understanding and exploiting deep generative models. Recently, SeFa tries to find latent disentangled directions by performing SVD on the first projection of a pre-trained GAN. However, it is only applied to the first layer and works in a post-processing way. Hessian Penalty minimizes the off-diagonal entries of the output's Hessian matrix to facilitate disentanglement, and can be applied to multi-layers.However, it constrains each entry of output independently, making it not sufficient in disentangling the latent directions (e.g., shape, size, rotation, etc.) of spatially correlated variations. In this paper, we propose a simple Orthogonal Jacobian Regularization (OroJaR) to encourage deep generative model to learn disentangled representations. It simply encourages the variation of output caused by perturbations on different latent dimensions to be orthogonal, and the Jacobian with respect to the input is calculated to represent this variation. We show that our OroJaR also encourages the output's Hessian matrix to be diagonal in an indirect manner. In contrast to the Hessian Penalty, our OroJaR constrains the output in a holistic way, making it very effective in disentangling latent dimensions corresponding to spatially correlated variations. Quantitative and qualitative experimental results show that our method is effective in disentangled and controllable image generation, and performs favorably against the state-of-the-art methods. Our code is available at https://github.com/csyxwei/OroJaR",0
"Abstract: In this work, we present a novel unsupervised method called orthogonal Jacobian regularization (OJR) for disentangling representations learned by deep generative models such as variational autoencoders (VAEs). OJR works by constraining the learning process so that the covariance matrix of the latent representation stays orthonormal during training. This encourages independence between the different factors of variation, making it easier for the model to capture meaningful disentangled features. We demonstrate the effectiveness of our approach on several benchmark datasets, showing improved performance over state-of-the-art methods both quantitatively and qualitatively. Our results indicate that OJR can effectively learn disentangled representations from raw image data without any supervision.  Keywords: VAEs, unsupervised learning, generative models, disentanglement, independence, orthogonality.",1
"Remarkable results have been achieved by DCNN based self-supervised depth estimation approaches. However, most of these approaches can only handle either day-time or night-time images, while their performance degrades for all-day images due to large domain shift and the variation of illumination between day and night images. To relieve these limitations, we propose a domain-separated network for self-supervised depth estimation of all-day images. Specifically, to relieve the negative influence of disturbing terms (illumination, etc.), we partition the information of day and night image pairs into two complementary sub-spaces: private and invariant domains, where the former contains the unique information (illumination, etc.) of day and night images and the latter contains essential shared information (texture, etc.). Meanwhile, to guarantee that the day and night images contain the same information, the domain-separated network takes the day-time images and corresponding night-time images (generated by GAN) as input, and the private and invariant feature extractors are learned by orthogonality and similarity loss, where the domain gap can be alleviated, thus better depth maps can be expected. Meanwhile, the reconstruction and photometric losses are utilized to estimate complementary information and depth maps effectively. Experimental results demonstrate that our approach achieves state-of-the-art depth estimation results for all-day images on the challenging Oxford RobotCar dataset, proving the superiority of our proposed approach.",0
"Here is my suggested abstract for your paper:  ""The task of estimating depth from a single image remains challenging even today due to the complexity involved in understanding scene geometry, lighting conditions, and object properties. In recent years, deep learning approaches have shown promising results by leveraging large amounts of labeled training data to learn complex representations that capture spatiotemporal patterns present in images. However, acquiring such massive annotations can be costly and impractical in many real-world scenarios. To address these limitations, we propose a self-supervised approach based on domain separation that enables efficient use of unlabeled images collected during daily life activities. Our method learns domain-invariant features across different environments while explicitly separating scenes into indoor/outdoor domains, allowing us to handle varying illumination levels, weather conditions, and appearances across diverse scenarios. Experiments conducted on public benchmark datasets show significant improvements over state-of-the-art methods trained under supervised settings, demonstrating the effectiveness of our proposed technique.""",1
"Despite recent advancements in single-domain or single-object image generation, it is still challenging to generate complex scenes containing diverse, multiple objects and their interactions. Scene graphs, composed of nodes as objects and directed-edges as relationships among objects, offer an alternative representation of a scene that is more semantically grounded than images. We hypothesize that a generative model for scene graphs might be able to learn the underlying semantic structure of real-world scenes more effectively than images, and hence, generate realistic novel scenes in the form of scene graphs. In this work, we explore a new task for the unconditional generation of semantic scene graphs. We develop a deep auto-regressive model called SceneGraphGen which can directly learn the probability distribution over labelled and directed graphs using a hierarchical recurrent architecture. The model takes a seed object as input and generates a scene graph in a sequence of steps, each step generating an object node, followed by a sequence of relationship edges connecting to the previous nodes. We show that the scene graphs generated by SceneGraphGen are diverse and follow the semantic patterns of real-world scenes. Additionally, we demonstrate the application of the generated graphs in image synthesis, anomaly detection and scene graph completion.",0
"Automatically generating scene graphs from images has become increasingly important as large amounts of image data becomes available in applications such as autonomous robots and virtual/augmented reality environments. Many existing approaches rely on deep learning techniques that require extensive labeled training data to achieve good performance. In contrast, our approach generates high quality scene graphs using only unlabeled images as input. We formulate the problem of generating scene graphs into two subproblems: first predicting objects in a scene (object detection) and second grouping these predicted objects into semantic relationships (scene graph generation). To solve these tasks we introduce new convolutional neural network architectures with novel attention mechanisms that selectively focus on discriminative features. Our methods outperform state-of-the-art results both quantitatively and qualitatively on three popular benchmark datasets demonstrating their effectiveness in real world scenarios where annotated data may be limited. Overall, our work represents a significant step forward towards building intelligent systems capable of efficiently processing complex visual inputs without relying heavily on manually curated annotations.",1
"Great progress has been made by the advances in Generative Adversarial Networks (GANs) for image generation. However, there lacks enough understanding on how a realistic image can be generated by the deep representations of GANs from a random vector. This chapter will give a summary of recent works on interpreting deep generative models. We will see how the human-understandable concepts that emerge in the learned representation can be identified and used for interactive image generation and editing.",0
This is an important contribution on how Generative Adversarial Networks (GAN) can create images through interactive means. You learn the basics of GANs such as why they were introduced by Ian Goodfellow et al. at NIPS 2014 and their training process. After understanding that you then go into detail about how these models interactively generate images using human feedback via reward signals. We then explain how to properly evaluate these systems so that you know which one performs better than others. At the end we give conclusions based upon our findings. This work has many contributions towards interpreting and evaluating GANs!,1
"The interest of the machine learning community in image synthesis has grown significantly in recent years, with the introduction of a wide range of deep generative models and means for training them. Such machines' ultimate goal is to match the distributions of the given training images and the synthesized ones. In this work, we propose a general model-agnostic technique for improving the image quality and the distribution fidelity of generated images, obtained by any generative model. Our method, termed BIGRoC (boosting image generation via a robust classifier), is based on a post-processing procedure via the guidance of a given robust classifier and without a need for additional training of the generative model. Given a synthesized image, we propose to update it through projected gradient steps over the robust classifier, in an attempt to refine its recognition. We demonstrate this post-processing algorithm on various image synthesis methods and show a significant improvement of the generated images, both quantitatively and qualitatively.",0
Title: Bigroi,1
"Recently, Generative Adversarial Networks (GANs)} have been widely used for portrait image generation. However, in the latent space learned by GANs, different attributes, such as pose, shape, and texture style, are generally entangled, making the explicit control of specific attributes difficult. To address this issue, we propose a SofGAN image generator to decouple the latent space of portraits into two subspaces: a geometry space and a texture space. The latent codes sampled from the two subspaces are fed to two network branches separately, one to generate the 3D geometry of portraits with canonical pose, and the other to generate textures. The aligned 3D geometries also come with semantic part segmentation, encoded as a semantic occupancy field (SOF). The SOF allows the rendering of consistent 2D semantic segmentation maps at arbitrary views, which are then fused with the generated texture maps and stylized to a portrait photo using our semantic instance-wise (SIW) module. Through extensive experiments, we show that our system can generate high quality portrait images with independently controllable geometry and texture attributes. The method also generalizes well in various applications such as appearance-consistent facial animation and dynamic styling.",0
"This research presents a new approach to portrait image generation using GANs (Generative Adversarial Networks) that allows users to select from a wide range of styles and dynamically apply them to any input photograph. The proposed system, called SofGAN, generates high quality images by combining two models - a Style Fusion module that blends different styles onto the input photo, and a novel Texture Synthesis model that generates detailed textures. Experimental results show that SofGAN outperforms state-of-the-art methods on both objective metrics such as PSNR and SSIM, and subjective evaluations conducted via user studies. Additionally, we demonstrate how SofGAN can be used to create diverse and creative artwork that pushes the boundaries of traditional portrait photography. Overall, our work represents an important step forward in the field of computer vision and has many potential applications in fields such as advertising, entertainment, and education.",1
"Electronic Health Records often suffer from missing data, which poses a major problem in clinical practice and clinical studies. A novel approach for dealing with missing data are Generative Adversarial Nets (GANs), which have been generating huge research interest in image generation and transformation. Recently, researchers have attempted to apply GANs to missing data generation and imputation for EHR data: a major challenge here is the categorical nature of the data. State-of-the-art solutions to the GAN-based generation of categorical data involve either reinforcement learning, or learning a bidirectional mapping between the categorical and the real latent feature space, so that the GANs only need to generate real-valued features. However, these methods are designed to generate complete feature vectors instead of imputing only the subsets of missing features. In this paper we propose a simple and yet effective approach that is based on previous work on GANs for data imputation. We first motivate our solution by discussing the reason why adversarial training often fails in case of categorical features. Then we derive a novel way to re-code the categorical features to stabilize the adversarial training. Based on experiments on two real-world EHR data with multiple settings, we show that our imputation approach largely improves the prediction accuracy, compared to more traditional data imputation approaches.",0
"Electronic Health Records (EHR) contain valuable medical information that can improve patient outcomes. However, missing values and incomplete records pose challenges to their utilization. Previous works have tackled these issues by imputing missing values using regression models or clustering algorithms. This paper proposes using generative adversarial networks (GANs), a powerful machine learning technique, to perform categorical EHR imputation. GANs consist of two deep neural networks: a generator and a discriminator. The generator creates synthetic data examples similar to real ones while the discriminator learns to differentiate between them. Our approach employs the trained generator as an imputation model for filling missing values in EHR datasets. The proposed method achieves state-of-the-art results compared to existing imputation techniques on five benchmark EHR datasets across different domains. Additionally, we analyze the impact of varying hyperparameters and discuss future directions towards deploying our framework into clinical practice. Overall, our work demonstrates that GANs offer a promising alternative to traditional imputation methods, potentially transforming how incomplete healthcare data is handled.",1
"Automated inspection and detection of foreign objects on railways is important for rail transportation safety as it helps prevent potential accidents and trains derailment. Most existing vision-based approaches focus on the detection of frontal intrusion objects with prior labels, such as categories and locations of the objects. In reality, foreign objects with unknown categories can appear anytime on railway tracks. In this paper, we develop a semi-supervised convolutional autoencoder based framework that only requires railway track images without prior knowledge on the foreign objects in the training process. It consists of three different modules, a bottleneck feature generator as encoder, a photographic image generator as decoder, and a reconstruction discriminator developed via adversarial learning. In the proposed framework, the problem of detecting the presence, location, and shape of foreign objects is addressed by comparing the input and reconstructed images as well as setting thresholds based on reconstruction errors. The proposed method is evaluated through comprehensive studies under different performance criteria. The results show that the proposed method outperforms some well-known benchmarking methods. The proposed framework is useful for data analytics via the train Internet-of-Things (IoT) systems",0
"In recent years, railway foreign object detection has become increasingly important due to its potential to reduce accidents caused by objects on rail tracks. Traditional methods such as manual inspection and CCTV cameras have limitations, including high labor costs and poor effectiveness at detecting small objects. To address these challenges, we propose a semi-supervised convolutional autoencoder based method that can effectively detect foreign objects on railways using computer vision techniques. Our approach uses unlabelled data and a limited amount of labelled data to improve detection accuracy while reducing computational complexity. We demonstrate through extensive experiments that our proposed method outperforms state-of-the-art supervised learning approaches and achieves high accuracy under various conditions. This research has significant implications for enhancing public safety in railway transportation systems worldwide.",1
"Image-to-image translation has been revolutionized with GAN-based methods. However, existing methods lack the ability to preserve the identity of the source domain. As a result, synthesized images can often over-adapt to the reference domain, losing important structural characteristics and suffering from suboptimal visual quality. To solve these challenges, we propose a novel frequency domain image translation (FDIT) framework, exploiting frequency information for enhancing the image generation process. Our key idea is to decompose the image into low-frequency and high-frequency components, where the high-frequency feature captures object structure akin to the identity. Our training objective facilitates the preservation of frequency information in both pixel space and Fourier spectral space. We broadly evaluate FDIT across five large-scale datasets and multiple tasks including image translation and GAN inversion. Extensive experiments and ablations show that FDIT effectively preserves the identity of the source image, and produces photo-realistic images. FDIT establishes state-of-the-art performance, reducing the average FID score by 5.6% compared to the previous best method.",0
"This work presents a novel approach called frequency domain image translation (FDIT) which translates images from one domain to another while preserving their identity as well as improving their photo-realism. FDIT works by first analyzing the input image in the frequency domain using DCT and then generating corresponding output features by adjusting them based on user specified semantic maps that define correspondences across domains. Finally, we synthesize images in the spatial domain from these features. Our method performs better than previous state-of-the-art methods like CycleGANs, DiscoGANs etc in terms of perceptual metrics as well as human evaluations where users were asked to rate how similar the translated outputs looked compared to the ground truth examples. We believe our technique could potentially revolutionize applications such as virtual/augmented reality, robotics, gaming, entertainment, education, design, marketing and fashion industries among others and have significant societal impact.",1
"We propose a novel universal detector for detecting images generated by using CNNs. In this paper, properties of checkerboard artifacts in CNN-generated images are considered, and the spectrum of images is enhanced in accordance with the properties. Next, a classifier is trained by using the enhanced spectrums to judge a query image to be a CNN-generated ones or not. In addition, an ensemble of the proposed detector with emphasized spectrums and a conventional detector is proposed to improve the performance of these methods. In an experiment, the proposed ensemble is demonstrated to outperform a state-of-the-art method under some conditions.",0
"This paper presents a new method for detecting manipulated images generated by CNN (Convolutional Neural Network) models using the presence of checkerboard artifacts in the frequency domain. The authors propose that these artifacts can serve as a unique signature of images synthesized by certain types of generative models, allowing them to be reliably distinguished from authentic photographs or other types of digitally altered images. To achieve this goal, they develop a novel detector based on two key components: a feature extraction stage which identifies and extracts the checkerboard patterns from the image spectrum, and a classification stage which uses machine learning techniques to distinguish real from fake images based on their pattern characteristics. They demonstrate the effectiveness of their approach through extensive experiments on several benchmark datasets, showing promising results in terms of detection accuracy and robustness against various adversarial attacks designed to evade existing forgery detectors. Overall, the proposed solution represents a significant step forward in protecting digital media from malicious manipulation and helping users make more informed decisions about the trustworthiness of online content.",1
"Furnishing and rendering an indoor scene is a common but tedious task for interior design: an artist needs to observe the space, create a conceptual design, build a 3D model, and perform rendering. In this paper, we introduce a new problem of domain-specific image synthesis using generative modeling, namely neural scene decoration. Given a photograph of an empty indoor space, we aim to synthesize a new image of the same space that is fully furnished and decorated. Neural scene decoration can be applied in practice to efficiently generate conceptual but realistic interior designs, bypassing the traditional multi-step and time-consuming pipeline. Our attempt to neural scene decoration in this paper is a generative adversarial neural network that takes the input photograph and directly produce the image of the desired furnishing and decorations. Our network contains a novel image generator that transforms an initial point-based object layout into a realistic photograph. We demonstrate the performance of our proposed method by showing that it outperforms the baselines built upon previous works on image translations both qualitatively and quantitatively. Our user study further validates the plausibility and aesthetics in the generated designs.",0
"In recent years, advances in computer vision have enabled the creation of virtual environments using real world data as input. One challenge that arises when generating these environments is deciding how to decorate them. Current methods rely on large amounts of manual labor, which can be time consuming and expensive. This paper presents a novel method called ""Neural Scene Decoration,"" which utilizes deep learning techniques to automatically generate additional elements for insertion into a virtual scene based on a single input photograph. Our approach involves training a generative model on a large dataset of images, allowing it to learn the relationships between different objects and scenes. Once trained, we use this model to synthesize new scenes by selecting appropriate features from existing photos, resulting in detailed and accurate additions to our generated environment. We evaluate the quality of these additions through user studies and demonstrate their effectiveness compared to manually created alternatives.",1
"Recent image generation models show remarkable generation performance. However, they mirror strong location preference in datasets, which we call spatial bias. Therefore, generators render poor samples at unseen locations and scales. We argue that the generators rely on their implicit positional encoding to render spatial content. From our observations, the generator's implicit positional encoding is translation-variant, making the generator spatially biased. To address this issue, we propose injecting explicit positional encoding at each scale of the generator. By learning the spatially unbiased generator, we facilitate the robust use of generators in multiple tasks, such as GAN inversion, multi-scale generation, generation of arbitrary sizes and aspect ratios. Furthermore, we show that our method can also be applied to denoising diffusion probabilistic models.",0
"Introduction: Recent advances in computer vision have led to significant progress in generating realistic images using deep generative models such as GANs (Generative Adversarial Networks) and VAEs (Variational Autoencoders). However, these models often suffer from spatial biases that can result in inconsistent image generation across different regions of the input space. In this work, we propose a novel approach to addressing spatial bias in generative models by introducing a new loss function that encourages consistency within local neighborhoods of the input space. Methods: We begin by training a baseline generative model on a large dataset of images. Next, we introduce our proposed local consistency loss which regularizes the generator network during training. This loss ensures that nearby points in the input space lead to similar outputs from the generator. Experimental Results: Our experiments demonstrate that incorporating the local consistency loss significantly improves the quality of generated images and reduces spatial biases observed in previous approaches. Quantitative evaluations show improved performance across multiple metrics including visual fidelity, diversity, and coherence. Conclusion: Our results suggest that our method has significant potential for producing more accurate and consistent images, particularly when dealing with high-resolution inputs where traditional generative models struggle due to their tendency towards spatial biases. Additionally, our method has applications beyond image synthesis, potentially opening up new opportunities for spatially unbiased generative models in other domains.",1
"Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained blindly? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image from those domains. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.",0
"In recent years, generative adversarial networks (GANs) have become increasingly popular due to their ability to generate high-quality images that can fool even human observers. However, domain adaptation remains one of the main challenges faced by GANs, as models trained on one dataset often struggle to generalize well across different domains. To address this issue, we propose StyleGAN-NADA, which combines state-of-the-art image generation techniques with a novel framework for clip-guided domain adaptation. Our approach allows us to synthesize diverse coherent and contextually consistent images from text descriptions by utilizing CLIP features derived from external data sources like OpenImageDataset. We demonstrate through extensive experiments that our method outperforms previous approaches in terms of visual fidelity, diversity, and quantitative evaluation metrics such as Fréchet Inception Distance (FID). Additionally, our model achieves comparable performance to other methods using only half of the training time. Finally, we present an ablation study examining each component in our proposed framework. This work has wide implications for both computer vision and natural language processing researchers interested in developing more advanced and efficient generative models capable of tackling new tasks without retraining.",1
"Machine learning tools are becoming increasingly powerful and widely used. Unfortunately membership attacks, which seek to uncover information from data sets used in machine learning, have the potential to limit data sharing. In this paper we consider an approach to increase the privacy protection of data sets, as applied to face recognition. Using an auxiliary face recognition model, we build on the StyleGAN generative adversarial network and feed it with latent codes combining two distinct sub-codes, one encoding visual identity factors, and, the other, non-identity factors. By independently varying these vectors during image generation, we create a synthetic data set of fictitious face identities. We use this data set to train a face recognition model. The model performance degrades in comparison to the state-of-the-art of face verification. When tested with a simple membership attack our model provides good privacy protection, however the model performance degrades in comparison to the state-of-the-art of face verification. We find that the addition of a small amount of private data greatly improves the performance of our model, which highlights the limitations of using synthetic data to train machine learning models.",0
"This paper proposes a novel method for training face verification models using generated face images rather than real photographs. Despite advances in generative modeling techniques like GANs (Generative Adversarial Networks), generating high quality synthetic images that capture fine details remains challenging. As such, there have been few attempts to use these methods in the field of computer vision applications related to facial recognition. Our approach uses existing deep learning architectures for face classification tasks as well as recent developments in image generation to train models on large datasets of generated faces. We demonstrate through experimentation that our approach achieves comparable accuracy to state-of-the-art face verification systems trained on real-world data, even though we only used randomly generated inputs. These results show promise for future work that could potentially reduce the reliance on human annotation efforts while still allowing for highly accurate performance in applied settings.",1
"Generating photo-realistic images from a text description is a challenging problem in computer vision. Previous works have shown promising performance to generate synthetic images conditional on text by Generative Adversarial Networks (GANs). In this paper, we focus on the category-consistent and relativistic diverse constraints to optimize the diversity of synthetic images. Based on those constraints, a category-consistent and relativistic diverse conditional GAN (CRD-CGAN) is proposed to synthesize $K$ photo-realistic images simultaneously. We use the attention loss and diversity loss to improve the sensitivity of the GAN to word attention and noises. Then, we employ the relativistic conditional loss to estimate the probability of relatively real or fake for synthetic images, which can improve the performance of basic conditional loss. Finally, we introduce a category-consistent loss to alleviate the over-category issues between K synthetic images. We evaluate our approach using the Birds-200-2011, Oxford-102 flower and MSCOCO 2014 datasets, and the extensive experiments demonstrate superiority of the proposed method in comparison with state-of-the-art methods in terms of photorealistic and diversity of the generated synthetic images.",0
"This paper proposes CRD-CGAN (Category-Constrained Generative Adversarial Networks), which combines category-consistent constraints (CCC) and relativistic discriminator (RDC). CCC regularizes the generator output towards the expected categories while RDC enables discrimination against ambiguous regions during training. We conducted extensive experiments on CelebAHQ and LSUN datasets, evaluating quantitatively and qualitatively. The proposed CRD-CGAN generates higher quality images than existing methods under various settings with improved visual fidelity, coherency, and interpretability. Our results demonstrate that the combination of CCC and RDC provides a more efficient solution for image generation tasks.",1
"We conduct a subjective experiment to compare the performance of traditional image coding methods and learning-based image coding methods. HEVC and VVC, the state-of-the-art traditional coding methods, are used as the representative traditional methods. The learning-based methods used contain not only CNN-based methods, but also a GAN-based method, all of which are advanced or typical. Single Stimuli (SS), which is also called Absolute Category Rating (ACR), is adopted as the methodology of the experiment to obtain perceptual quality of images. Additionally, we utilize some typical and frequently used objective quality metrics to evaluate the coding methods in the experiment as comparison. The experiment shows that CNN-based and GAN-based methods can perform better than traditional methods in low bit-rates. In high bit-rates, however, it is hard to verify whether CNN-based methods are superior to traditional methods. Because the GAN method does not provide models with high target bit-rates, we cannot exactly tell the performance of the GAN method in high bit-rates. Furthermore, some popular objective quality metrics have not shown the ability well to measure quality of images generated by learning-based coding methods, especially the GAN-based one.",0
"Title: ""An Evaluation of Traditional and Learning-Based Image Coding Methods""  This study compares the performance of subjective evaluations of traditionally optimized image coders, such as JPEG, with recently developed deep learning based image compression systems. We conducted a user study where participants rated images compressed using both types of coders on several quality metrics including visual fidelity, file size, and perceptual complexity. Our results show that while deep learning based compressors achieve better objective measures of efficiency, human raters preferred JPEG images more often across all tested quality metrics. These findings have important implications for future research directions in image compression, suggesting that more attention should be given to understanding how humans perceive differences in compressed images, rather than relying exclusively on objective performance metrics. Overall, our work demonstrates the value of incorporating subjective evaluation into studies of image coding methods, providing insight into tradeoffs between efficiency and perceived quality that may not be captured by current benchmarking practices.",1
"In this paper, we demonstrated a practical application of realistic river image generation using deep learning. Specifically, we explored a generative adversarial network (GAN) model capable of generating high-resolution and realistic river images that can be used to support modeling and analysis in surface water estimation, river meandering, wetland loss, and other hydrological research studies. First, we have created an extensive repository of overhead river images to be used in training. Second, we incorporated the Progressive Growing GAN (PGGAN), a network architecture that iteratively trains smaller-resolution GANs to gradually build up to a very high resolution to generate high quality (i.e., 1024x1024) synthetic river imagery. With simpler GAN architectures, difficulties arose in terms of exponential increase of training time and vanishing/exploding gradient issues, which the PGGAN implementation seemed to significantly reduce. The results presented in this study show great promise in generating high-quality images and capturing the details of river structure and flow to support hydrological research, which often requires extensive imagery for model performance.",0
"Title: Realistic River Image Synthesis Using Deep Generative Adversarial Networks  This paper presents a novel approach to realistically synthesize river images using deep generative adversarial networks (DGAN). The proposed method involves training two deep convolutional neural networks, one as a generator and the other as a discriminator. The generator network produces new samples that resemble real river images, while the discriminator network distinguishes between generated images and real ones. We use an improved architecture based on Wasserstein GAN (WGAN) which helps achieve better stability during training and higher quality results compared to traditional DGAN architectures.  To ensure high fidelity and coherency in our synthesized river images, we introduce an attention mechanism into our generator network. This allows the model to focus on specific regions of interest and produce more detailed output. Additionally, we employ a multi-scale scheme that utilizes features from different resolution scales, resulting in more diverse textural details and overall image accuracy. Furthermore, we propose a simple yet effective post-processing technique to refine the final output by selecting appropriate colors and intensities for river water, sky, trees, landscapes, etc.  We evaluated our method against previous state-of-the-art approaches and demonstrate its superior performance in terms of visual quality and quantitative metrics such as peak signal-to-noise ratio (PSNR), structured similarity index measure (SSIM), and perceptual evaluation of image quality (PSIM). In conclusion, we believe that our method has significant potential applications in computer graphics, gaming industries, surveillance systems, and virtual reality environments where realistic image rendering is important.",1
"Latest Generative Adversarial Networks (GANs) are gathering outstanding results through a large-scale training, thus employing models composed of millions of parameters requiring extensive computational capabilities. Building such huge models undermines their replicability and increases the training instability. Moreover, multi-channel data, such as images or audio, are usually processed by realvalued convolutional networks that flatten and concatenate the input, often losing intra-channel spatial relations. To address these issues related to complexity and information loss, we propose a family of quaternion-valued generative adversarial networks (QGANs). QGANs exploit the properties of quaternion algebra, e.g., the Hamilton product, that allows to process channels as a single entity and capture internal latent relations, while reducing by a factor of 4 the overall number of parameters. We show how to design QGANs and to extend the proposed approach even to advanced models.We compare the proposed QGANs with real-valued counterparts on several image generation benchmarks. Results show that QGANs are able to obtain better FID scores than real-valued GANs and to generate visually pleasing images. Furthermore, QGANs save up to 75% of the training parameters. We believe these results may pave the way to novel, more accessible, GANs capable of improving performance and saving computational resources.",0
"Deep learning techniques have become increasingly popular in recent years as a powerful tool to generate realistic images, videos, audio, and text using generative models such as Generative Adversarial Networks (GAN). These models consist of two sub-networks: a generator network that generates data samples, and a discriminator network that evaluates whether these generated samples come from the true underlying distribution or from the generator itself. In practice, however, training GANs can be challenging due to issues related to instability, mode collapse, blurriness, and overfitting. One potential solution to address some of these issues is to use quaternions instead of Euclidean coordinates for image generation in GAN architectures. Quaternions represent rotations in four-dimensional space using complex numbers and provide several advantages over traditional approaches including better handling of orientational ambiguity, improved stability, and enhanced numerical robustness. This work explores the application of quaternion representations within GAN models through detailed experiments on various benchmark datasets, demonstrating significant improvements in terms of visual fidelity and quantitative metrics compared to standard Euclidean baselines. Our results showcase the effectiveness of quaternionic image synthesis within generative adversarial networks, opening up new opportunities for advanced computer vision tasks, video creation, graphics programming, and other applications benefiting from high-quality image generation. Title: ""Quaternion Generative Adversarial Networks""",1
"We propose a new method to detect deepfake images using the cue of the source feature inconsistency within the forged images. It is based on the hypothesis that images' distinct source features can be preserved and extracted after going through state-of-the-art deepfake generation processes. We introduce a novel representation learning approach, called pair-wise self-consistency learning (PCL), for training ConvNets to extract these source features and detect deepfake images. It is accompanied by a new image synthesis approach, called inconsistency image generator (I2G), to provide richly annotated training data for PCL. Experimental results on seven popular datasets show that our models improve averaged AUC over the state of the art from 96.45% to 98.05% in the in-dataset evaluation and from 86.03% to 92.18% in the cross-dataset evaluation.",0
"""Deepfakes,"" or manipulated media that has been altered via artificial intelligence (AI) techniques such as Generative Adversarial Networks (GANs), have become increasingly prevalent on social media platforms. These videos often contain fictitious content like someone saying something they never actually did, which poses significant challenges for journalists, policymakers, and citizens who struggle to separate fact from fiction. In recent years, several deep learning approaches have been proposed to detect these manipulated media, but most methods rely on large amounts of labeled data and assume access to high quality features, making them difficult to apply to more general scenarios where only limited training data is available. This paper presents a novel approach called Self-Supervised Learning Forensics (SSLF) that can learn self-consistency models using unlabeled image pairs, even if there exists no ground truth data. SSLF consists of two components: the feature encoder network and the consistency score network, both trained through adversarial self-supervision. Our experiments show that our method outperforms state-of-the-art baselines across different deepfake detection benchmark datasets while requiring minimal supervisory signals, demonstrating the effectiveness and robustness of SSLF under various realistic settings.",1
"In most interactive image generation tasks, given regions of interest (ROI) by users, the generated results are expected to have adequate diversities in appearance while maintaining correct and reasonable structures in original images. Such tasks become more challenging if only limited data is available. Recently proposed generative models complete training based on only one image. They pay much attention to the monolithic feature of the sample while ignoring the actual semantic information of different objects inside the sample. As a result, for ROI-based generation tasks, they may produce inappropriate samples with excessive randomicity and without maintaining the related objects' correct structures. To address this issue, this work introduces a MOrphologic-structure-aware Generative Adversarial Network named MOGAN that produces random samples with diverse appearances and reliable structures based on only one image. For training for ROI, we propose to utilize the data coming from the original image being augmented and bring in a novel module to transform such augmented data into knowledge containing both structures and appearances, thus enhancing the model's comprehension of the sample. To learn the rest areas other than ROI, we employ binary masks to ensure the generation isolated from ROI. Finally, we set parallel and hierarchical branches of the mentioned learning process. Compared with other single image GAN schemes, our approach focuses on internal features including the maintenance of rational structures and variation on appearance. Experiments confirm a better capacity of our model on ROI-based image generation tasks than its competitive peers.",0
"Abstract: In recent years, great progress has been made by generative models in image generation tasks such as StyleGANs and DALL-E 2. These models learn high-fidelity representations that generate realistic images with fine details. However, most state-of-the-art methods lack explicit understanding of structure in the generated objects, resulting in significant quality degradation when producing novel views. We present a new approach called MOGAN (Morphologic-Structure-Aware GAN) that explicitly considers morphology during training, enabling learning from a single image to create multiple structured outputs that capture object shape better than existing baselines. By introducing an attention module based on local feature maps, we align the model’s internal representation with ground truth structures, ensuring consistency across different domains. Experimental results demonstrate significantly improved performance over traditional GAN architectures and other state-of-the-art methods while maintaining comparable visual fidelity. Our approach sets new standards in the field of unconditional image synthesis and paves the way towards generating even higherquality images.",1
"One of the important research topics in image generative models is to disentangle the spatial contents and styles for their separate control. Although StyleGAN can generate content feature vectors from random noises, the resulting spatial content control is primarily intended for minor spatial variations, and the disentanglement of global content and styles is by no means complete. Inspired by a mathematical understanding of normalization and attention, here we present a novel hierarchical adaptive Diagonal spatial ATtention (DAT) layers to separately manipulate the spatial contents from styles in a hierarchical manner. Using DAT and AdaIN, our method enables coarse-to-fine level disentanglement of spatial contents and styles. In addition, our generator can be easily integrated into the GAN inversion framework so that the content and style of translated images from multi-domain image translation tasks can be flexibly controlled. By using various datasets, we confirm that the proposed method not only outperforms the existing models in disentanglement scores, but also provides more flexible control over spatial features in the generated images.",0
"In recent years, Generative Adversarial Networks (GAN) have become increasingly popular for generating realistic images using deep neural networks. However, one major challenge faced by these models is that they often generate blurry or unrecognizable content when translating between different styles. To address this issue, we propose a new method called ""Diagonal Attention and Style-based GAN"" which uses diagonal attention modules to disentangle style from content during image generation. This allows us to translate content into desired styles while preserving key features such as objects and semantics. We evaluate our model on several benchmark datasets and show significant improvements over baseline methods. Our results demonstrate the effectiveness of diagonal attention and style-based GANs in content-style disentanglement tasks, opening up possibilities for applications such as artistic stylization, photo editing, and generative design.",1
"Scene text editing (STE), which converts a text in a scene image into the desired text while preserving an original style, is a challenging task due to a complex intervention between text and style. To address this challenge, we propose a novel representational learning-based STE model, referred to as RewriteNet that employs textual information as well as visual information. We assume that the scene text image can be decomposed into content and style features where the former represents the text information and style represents scene text characteristics such as font, alignment, and background. Under this assumption, we propose a method to separately encode content and style features of the input image by introducing the scene text recognizer that is trained by text information. Then, a text-edited image is generated by combining the style feature from the original image and the content feature from the target text. Unlike previous works that are only able to use synthetic images in the training phase, we also exploit real-world images by proposing a self-supervised training scheme, which bridges the domain gap between synthetic and real data. Our experiments demonstrate that RewriteNet achieves better quantitative and qualitative performance than other comparisons. Moreover, we validate that the use of text information and the self-supervised training scheme improves text switching performance. The implementation and dataset will be publicly available.",0
"This paper presents RewriteNet, a novel deep learning model that generates realistic scene text images by editing text in real world image scenes. Unlike existing methods which rely on pre-designed templates and handcrafted features, our approach leverages natural language instructions and learned representations from image synthesis models to manipulate the appearance of characters and objects within original scenes. Our method first predicts a set of semantic masks, each corresponding to one character class and representing the visibility probability of instances belonging to this class over pixels. Then, these masks are used as attention weights to edit the original image features according to the desired sequence of characters specified through text input. By doing so, we can generate new images where characters and objects have been edited coherently and faithfully, allowing for diverse variations while preserving important details and context from the original image. We evaluate our system using both objective metrics such as FID scores and human judgments, demonstrating improved performance compared to state-of-the-art alternatives. Overall, our work shows promising potential in generating more realistic scene text images for VR/AR applications, product design, and other creative tasks that require fine control over visual outputs.",1
"Generative adversarial networks (GANs) nowadays are capable of producing images of incredible realism. One concern raised is whether the state-of-the-art GAN's learned distribution still suffers from mode collapse, and what to do if so. Existing diversity tests of samples from GANs are usually conducted qualitatively on a small scale, and/or depends on the access to original training data as well as the trained model parameters. This paper explores to diagnose GAN intra-mode collapse and calibrate that, in a novel black-box setting: no access to training data, nor the trained model parameters, is assumed. The new setting is practically demanded, yet rarely explored and significantly more challenging. As a first stab, we devise a set of statistical tools based on sampling, that can visualize, quantify, and rectify intra-mode collapse. We demonstrate the effectiveness of our proposed diagnosis and calibration techniques, via extensive simulations and experiments, on unconditional GAN image generation (e.g., face and vehicle). Our study reveals that the intra-mode collapse is still a prevailing problem in state-of-the-art GANs and the mode collapse is diagnosable and calibratable in black-box settings. Our codes are available at: https://github.com/VITA-Group/BlackBoxGANCollapse.",0
"Despite the rapid progress made in Generative Adversarial Networks (GANs), intrinsic mode collapse remains one of their biggest limitations. Intrinsic mode collapse occurs when the generator loses coherence among modes and struggles to capture crucial details present in real data distributions. This study takes a step towards understanding intra-mode collapse by developing a pilot framework focused on black-box diagnosis and calibration techniques applied to GAN performance. Our research finds that studying generators based only on fidelity metrics like Frechet Inception Distance (FID) can lead to incomplete insights into GAN behavior. Additionally, we show how certain architectures may perform better than others depending on specific use cases. We also investigate whether training schedules impact the quality and stability of generated samples. By examining these factors and more, our work provides valuable insight into intrinsic mode collapse phenomena while opening up new areas for future investigation.",1
"Handwritten text recognition in low resource scenarios, such as manuscripts with rare alphabets, is a challenging problem. The main difficulty comes from the very few annotated data and the limited linguistic information (e.g. dictionaries and language models). Thus, we propose a few-shot learning-based handwriting recognition approach that significantly reduces the human labor annotation process, requiring only few images of each alphabet symbol. First, our model detects all symbols of a given alphabet in a textline image, then a decoding step maps the symbol similarity scores to the final sequence of transcribed symbols. Our model is first pretrained on synthetic line images generated from any alphabet, even though different from the target domain. A second training step is then applied to diminish the gap between the source and target data. Since this retraining would require annotation of thousands of handwritten symbols together with their bounding boxes, we propose to avoid such human effort through an unsupervised progressive learning approach that automatically assigns pseudo-labels to the non-annotated data. The evaluation on different manuscript datasets show that our model can lead to competitive results with a significant reduction in human effort.",0
"In low resource handwriting recognition tasks, datasets can often have limited data, making it difficult to train deep learning models effectively. To address this challenge, we propose a progressive few shot learning approach that leverages transfer learning and active learning techniques to improve model performance on these small datasets. Our method involves training the model on similar but larger datasets, using only a few labeled examples from the target task. We then fine-tune the model using active learning, where the most informative samples are selected for labeling based on their impact on the overall accuracy. Experimental results show significant improvements over baseline methods across multiple languages and writing styles, demonstrating the effectiveness of our proposed approach.",1
"Synthetic data is becoming increasingly common for training computer vision models for a variety of tasks. Notably, such data has been applied in tasks related to humans such as 3D pose estimation where data is either difficult to create or obtain in realistic settings. Comparatively, there has been less work into synthetic animal data and it's uses for training models. Consequently, we introduce a parametric canine model, DynaDog+T, for generating synthetic canine images and data which we use for a common computer vision task, binary segmentation, which would otherwise be difficult due to the lack of available data.",0
"""Our goal was to design a parametric animal model that could generate realistic images of dogs based on user input and training data. We created DynaDog+T by combining state-of-the-art computer graphics techniques with machine learning algorithms. This model can create a wide range of dog breeds and body types, allowing users to customize their virtual pets with ease. Our evaluation shows that DynaDog+T outperforms previous models in terms of visual fidelity and flexibility. This work has implications in computer animation, video games, and virtual reality applications.""",1
"For successful scene text recognition (STR) models, synthetic text image generators have alleviated the lack of annotated text images from the real world. Specifically, they generate multiple text images with diverse backgrounds, font styles, and text shapes and enable STR models to learn visual patterns that might not be accessible from manually annotated data. In this paper, we introduce a new synthetic text image generator, SynthTIGER, by analyzing techniques used for text image synthesis and integrating effective ones under a single algorithm. Moreover, we propose two techniques that alleviate the long-tail problem in length and character distributions of training data. In our experiments, SynthTIGER achieves better STR performance than the combination of synthetic datasets, MJSynth (MJ) and SynthText (ST). Our ablation study demonstrates the benefits of using sub-components of SynthTIGER and the guideline on generating synthetic text images for STR models. Our implementation is publicly available at https://github.com/clovaai/synthtiger.",0
"This paper presents a new dataset called SynthTIGER (Synthetic Text Image Generator) designed to improve text recognition models. Traditional datasets used to train such models often contain limited variations of natural images or synthetic data that lacks realism. In contrast, our dataset offers large scale, high quality synthetic images paired with their corresponding ground truth captions, which enables training more effective models while reducing reliance on human annotation efforts. Furthermore, we demonstrate how training with SynthTIGER can significantly outperform other approaches in terms of performance, without any fine tuning required. Our work sets a baseline in advancing the field of computer vision towards better text recognition models through novel data generation techniques.",1
"We propose an audio-driven talking-head method to generate photo-realistic talking-head videos from a single reference image. In this work, we tackle two key challenges: (i) producing natural head motions that match speech prosody, and (ii) maintaining the appearance of a speaker in a large head motion while stabilizing the non-face regions. We first design a head pose predictor by modeling rigid 6D head movements with a motion-aware recurrent neural network (RNN). In this way, the predicted head poses act as the low-frequency holistic movements of a talking head, thus allowing our latter network to focus on detailed facial movement generation. To depict the entire image motions arising from audio, we exploit a keypoint based dense motion field representation. Then, we develop a motion field generator to produce the dense motion fields from input audio, head poses, and a reference image. As this keypoint based representation models the motions of facial regions, head, and backgrounds integrally, our method can better constrain the spatial and temporal consistency of the generated videos. Finally, an image generation network is employed to render photo-realistic talking-head videos from the estimated keypoint based motion fields and the input reference image. Extensive experiments demonstrate that our method produces videos with plausible head motions, synchronized facial expressions, and stable backgrounds and outperforms the state-of-the-art.",0
"This paper presents Audio2Head (A2H), which can generate high-quality talking heads that follow audio inputs one time by utilizing lightweight deep neural networks. We introduce novel techniques to accurately model head motion without detailed camera parameters or facial landmarks through exploiting physical constraints. In particular, we first devise an efficient method for modeling rigid motions using pre-trained pose estimators and quaternions. Furthermore, to recover nonrigid deformations from monocular video sequences captured in unconstrained environments, we propose an autoencoder that minimizes reconstruction error via adversarial training. Our experiments demonstrate superior performance compared with state-of-the-art methods on two datasets under both objective and subjective evaluation metrics. Finally, our framework generates diverse, plausible results given various types of input audios efficiently thanks to its compact architecture. Our project page contains supplementary materials, models, and code to ensure replicability for researchers. This study presents a new approach for generating realistic talking heads based on audio inputs called Audio2Head (A2H). By leveraging lightweight deep neural networks, A2H can produce high quality animations that accurately capture natural head movements even without precise camera details or facial landmark data. To achieve this, the authors introduce innovative techniques to model head motion using existing technologies like pose estimation algorithms and quaternions. Additionally, the research team proposes an autoencoder network that can learn how to reconstruct face deformations from video clips recorded in informal settings. Extensive testing shows that the resulting animations produced by A2H outperform other systems in terms of visual fidelity and lip sync accuracy assessed through both quantitative and qualitative evaluations. Moreover, due t...",1
"Image-to-image translation models have shown remarkable ability on transferring images among different domains. Most of existing work follows the setting that the source domain and target domain keep the same at training and inference phases, which cannot be generalized to the scenarios for translating an image from an unseen domain to another unseen domain. In this work, we propose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem, which aims to learn a model that can translate samples from image domains that are not observed during training. Accordingly, we propose a framework called ZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model each domain with domain-specific feature distribution that is semantically consistent on vision and attribute modalities. Then the domain-invariant features are disentangled with an shared encoder for image generation. We carry out extensive experiments on CUB and FLO datasets, and the results demonstrate the effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows significant accuracy improvements over state-of-the-art zero-shot learning methods on CUB and FLO.",0
"In recent years, unsupervised image-to-image translation has become an increasingly important area of study due to its potential applications across various domains including art, entertainment, medicine, and engineering. One popular approach towards achieving zero-shot (ZS) image translation involves pretraining on a diverse set of datasets and then fine-tuning the model on small amounts of labeled data from target domains. While these models have achieved impressive results, they often suffer from mode collapse during training which can lead to significant loss of diversity in generated outputs. This paper presents ZSTGAN, an adversarial framework that tackles the problem of mode collapse by incorporating two novel components into existing methods such as CycleGANs: Zero Shot Discriminator (ZSD) for discrimination against poor quality translations and Smart Sampling (SSampler) to prevent overspecialization to the given samples. Our experiments demonstrate substantial improvements over baseline methods across multiple benchmark datasets. Additionally, our method exhibits better generalization ability by generating more plausible outputs for previously unseen objects/classes after fine-tuning on only one sample per class.",1
"Place recognition is indispensable for a drift-free localization system. Due to the variations of the environment, place recognition using single-modality has limitations. In this paper, we propose a bi-modal place recognition method, which can extract a compound global descriptor from the two modalities, vision and LiDAR. Specifically, we first build the elevation image generated from 3D points as a structural representation. Then, we derive the correspondences between 3D points and image pixels that are further used in merging the pixel-wise visual features into the elevation map grids. In this way, we fuse the structural features and visual features in the consistent bird-eye view frame, yielding a semantic representation, namely CORAL. And the whole network is called CORAL-VLAD. Comparisons on the Oxford RobotCar show that CORAL-VLAD has superior performance against other state-of-the-art methods. We also demonstrate that our network can be generalized to other scenes and sensor configurations on cross-city datasets.",0
"Bi-modal place recognition has become increasingly important as technology advances towards integrating multiple sensory data streams into complex systems. In order to effectively process these vast amounts of information, researchers have developed the CORAL system, which employs a colored structural representation approach that utilizes both visual (RGB) and depth (D) modalities. By representing scene structures using edge connectivity and color features, CORAL provides a more accurate model than traditional methods that rely on solely RGB information. This study evaluates the performance of CORAL against several state-of-the art algorithms on two public datasets, demonstrating significant improvement over existing approaches. Furthermore, this methodology can be extended to other sensor types and applications where multi-modality is key. Overall, CORAL represents an important step forward in bi-modal place recognition research, offering a powerful tool for future technological development.",1
"Generating images from scene graphs is a challenging task that attracted substantial interest recently. Prior works have approached this task by generating an intermediate layout description of the target image. However, the representation of each object in the layout was generated independently, which resulted in high overlap, low coverage, and an overall blurry layout. We propose a novel method that alleviates these issues by generating the entire layout description gradually to improve inter-object dependency. We empirically show on the COCO-STUFF dataset that our approach improves the quality of both the intermediate layout and the final image. Our approach improves the layout coverage by almost 20 points and drops object overlap to negligible amounts.",0
"In this work we propose a novel end-to-end framework that transforms scene graphs into photo-realistic images using contextualized object layout refinement. Our approach leverages graph convolutional networks (GCN) to encode semantic relationships between objects represented as nodes in the graph, capturing their geometric arrangements. Subsequently, we generate an initial image by deforming predefined shape features guided by edge connections from the GCN output. To further improve visual fidelity, we integrate additional contextual constraints derived from attention mechanisms to fine-grained adjustments of object positions, scaled shapes and interpenetrating regions. Finally, our framework generates the final result through iterative optimization on pixel-wise differences with respect to real photographs via adversarial training with discriminators. Experiments validate the effectiveness of each component and demonstrate superior image quality compared against state-of-the-art techniques.",1
"Recent developments related to generative models have made it possible to generate diverse high-fidelity images. In particular, layout-to-image generation models have gained significant attention due to their capability to generate realistic complex images containing distinct objects. These models are generally conditioned on either semantic layouts or textual descriptions. However, unlike natural images, providing auxiliary information can be extremely hard in domains such as biomedical imaging and remote sensing. In this work, we propose a multi-object generation framework that can synthesize images with multiple objects without explicitly requiring their contextual information during the generation process. Based on a vector-quantized variational autoencoder (VQ-VAE) backbone, our model learns to preserve spatial coherency within an image as well as semantic coherency between the objects and the background through two powerful autoregressive priors: PixelSNAIL and LayoutPixelSNAIL. While the PixelSNAIL learns the distribution of the latent encodings of the VQ-VAE, the LayoutPixelSNAIL is used to specifically learn the semantic distribution of the objects. An implicit advantage of our approach is that the generated samples are accompanied by object-level annotations. We demonstrate how coherency and fidelity are preserved with our method through experiments on the Multi-MNIST and CLEVR datasets; thereby outperforming state-of-the-art multi-object generative methods. The efficacy of our approach is demonstrated through application on medical imaging datasets, where we show that augmenting the training set with generated samples using our approach improves the performance of existing models.",0
"Automatically generating annotated images containing multiple coherent objects has been a challenging problem in computer vision for decades. Despite significant progress in recent years, creating accurate object detectors remains difficult due to issues such as occlusion and cluttered backgrounds. In our paper, we present a novel method that addresses these problems by combining advanced generative models with powerful object detection algorithms. Our approach generates high-fidelity images using adversarial training techniques and then applies state-of-the-art object detection networks to create detailed annotations. By integrating these components into a single pipeline, we achieve impressive results across a wide range of use cases while maintaining control over object placement, appearance, and scale. We evaluate the effectiveness of our system through extensive experiments on public benchmark datasets and demonstrate its superior performance compared to existing approaches. Overall, our work represents a major step forward in the field and opens up new possibilities for applications ranging from virtual reality to autonomous driving.",1
"Generative Adversarial Networks (GANs) are commonly used for modeling complex distributions of data. Both the generators and discriminators of GANs are often modeled by neural networks, posing a non-transparent optimization problem which is non-convex and non-concave over the generator and discriminator, respectively. Such networks are often heuristically optimized with gradient descent-ascent (GDA), but it is unclear whether the optimization problem contains any saddle points, or whether heuristic methods can find them in practice. In this work, we analyze the training of Wasserstein GANs with two-layer neural network discriminators through the lens of convex duality, and for a variety of generators expose the conditions under which Wasserstein GANs can be solved exactly with convex optimization approaches, or can be represented as convex-concave games. Using this convex duality interpretation, we further demonstrate the impact of different activation functions of the discriminator. Our observations are verified with numerical results demonstrating the power of the convex interpretation, with applications in progressive training of convex architectures corresponding to linear generators and quadratic-activation discriminators for CelebA image generation. The code for our experiments is available at https://github.com/ardasahiner/ProCoGAN.",0
"Inferring meaningful representations from data remains challenging due to intrinsic ambiguities arising from complex patterns or noise, as well as external uncertainties introduced by imperfect models or incomplete knowledge. We demonstrate that hidden convexity can emerge naturally within the deep energy landscape of generative adversarial networks (GANs) under mild regularization. By leveraging this property with closed-form solutions at local optima, we achieve state-of-the-art performance on several benchmark datasets across diverse tasks including generation, classification, clustering, denoising, inpainting, completion, and more - without relying on meta-parameters such as architecture size or training time, nor post-hoc techniques like thresholding or pruning. Our findings open new doors towards efficient and interpretable machine learning based on human cognition principles of perception and abstraction, ultimately benefiting fields ranging from medical imaging to social sciences and beyond.",1
"We present a coupled Variational Auto-Encoder (VAE) method that improves the accuracy and robustness of the probabilistic inferences on represented data. The new method models the dependency between input feature vectors (images) and weighs the outliers with a higher penalty by generalizing the original loss function to the coupled entropy function, using the principles of nonlinear statistical coupling. We evaluate the performance of the coupled VAE model using the MNIST dataset. Compared with the traditional VAE algorithm, the output images generated by the coupled VAE method are clearer and less blurry. The visualization of the input images embedded in 2D latent variable space provides a deeper insight into the structure of new model with coupled loss function: the latent variable has a smaller deviation and a more compact latent space generates the output values. We analyze the histogram of the likelihoods of the input images using the generalized mean, which measures the model's accuracy as a function of the relative risk. The neutral accuracy, which is the geometric mean and is consistent with a measure of the Shannon cross-entropy, is improved. The robust accuracy, measured by the -2/3 generalized mean, is also improved.",0
This sounds like an interesting paper. Can you provide some more context on what a variational autoencoder (VAE) is? I can try my best to write an abstract based on that information.,1
"Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such observation can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). We observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce novel regularization techniques for training GANs with ViTs. Empirically, our approach, named ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2 on CIFAR-10, CelebA, and LSUN bedroom datasets.",0
"Generate an abstract for a research paper on training generative adversarial networks (GANs) using vision transformer architectures. Here is an example: ViTGAN: Training GANs with Vision Transformer Architectures  Generative Adversarial Networks (GANs) have revolutionized the field of computer vision by enabling the synthesis of high-quality images that closely resemble real data. Recently, attention-based vision transformer models such as ViT have achieved state-of-the-art results in image classification tasks. In this work, we explore the use of vision transformers as discriminators in GANs, which we refer to as ViTGANs. Our approach combines the strengths of both worlds - the powerful generation capabilities of GANs and the strong representation learning abilities of vision transformers. We present extensive experiments comparing our proposed architecture to several baseline models across multiple benchmark datasets, demonstrating improved performance in terms of visual fidelity and quantitative metrics. Additionally, we showcase how ViTGANs can generate high-resolution images with finer details, making them well suited for downstream applications such as semantic segmentation and object detection. Overall, our findings highlight the promising potential of integrating transformers into the GAN framework for achieving better image synthesis.",1
"In the present study, we propose to implement a new framework for estimating generative models via an adversarial process to extend an existing GAN framework and develop a white-box controllable image cartoonization, which can generate high-quality cartooned images/videos from real-world photos and videos. The learning purposes of our system are based on three distinct representations: surface representation, structure representation, and texture representation. The surface representation refers to the smooth surface of the images. The structure representation relates to the sparse colour blocks and compresses generic content. The texture representation shows the texture, curves, and features in cartoon images. Generative Adversarial Network (GAN) framework decomposes the images into different representations and learns from them to generate cartoon images. This decomposition makes the framework more controllable and flexible which allows users to make changes based on the required output. This approach overcomes any previous system in terms of maintaining clarity, colours, textures, shapes of images yet showing the characteristics of cartoon images.",0
"Abstract: Many modern image editing techniques require generating realistically looking images, such as digital photos of nonexistent objects, photo retouching, creating videos from textual descriptions, and more. Most recently, cartoonization has become a popular topic due to its potential applications in movies, comics, advertisements, etc., where some scenes need to look like they were drawn by hand. However, current methods either can’t preserve important details present in the original images, suffer from artifacts introduced during processing, or remain computationally expensive to be applied in practice on high resolution inputs. To address these challenges we propose white-box cartoonization using our extended CycleGAN architecture (CycleWB). We improve over standard CycleGAN by introducing new losses focused on better preserving lines within the input that should stay unchanged after the conversion process; introducing a novel edge sharpening approach making edges smoother without increasing noise; a simplified pipeline requiring fewer steps at training time compared to other white box models; improving the speed of inference throughput via hardware optimizations. Our method significantly outperforms previous approaches across multiple metrics including human preference according to user studies (n = 264) where our model achieved a higher accuracy (80%) at distinguishing real photographs from their corresponding cartoonized versions than any baseline considered in this research. In addition to human studies demonstrating improved visual fidelity we provide extensive ablation analysis comparing against the state-of-the art, which shows consistent quality gains across all evaluation tasks. Ultimately, our cartoonization framework provides users with advanced controls over generated outputs allowing them to produce creative and visually stunning content faster and easier compared to traditional animation pipelines often involving hiring professional artists to draw each frame individually. Finally, we share project pages with code, pretrained weights, comparisons between competing approaches and discuss future work possibilities extending beyond simple still ima",1
"In many applications of computer graphics, art and design, it is desirable for a user to provide intuitive non-image input, such as text, sketch, stroke, graph or layout, and have a computer system automatically generate photo-realistic images that adhere to the input content. While classic works that allow such automatic image content generation have followed a framework of image retrieval and composition, recent advances in deep generative models such as generative adversarial networks (GANs), variational autoencoders (VAEs), and flow-based methods have enabled more powerful and versatile image generation tasks. This paper reviews recent works for image synthesis given intuitive user input, covering advances in input versatility, image generation methodology, benchmark datasets, and evaluation metrics. This motivates new perspectives on input representation and interactivity, cross pollination between major image generation paradigms, and evaluation and comparison of generation methods.",0
"This paper provides a comprehensive review of deep image synthesis methods that can generate novel images based on intuitive user input such as text descriptions, sketches, or interactive interfaces. We present key techniques used by these methods including generative adversarial networks (GANs), variational autoencoders (VAEs), and style transfer, which enable the generation of high-fidelity images. Furthermore, we discuss challenges associated with training large models efficiently and generating detailed images with fine-grained structures. Additionally, ethical considerations related to controllable image synthesis systems are addressed. Finally, we suggest promising research directions for advancing deep image synthesis towards more creativity-enabling applications.",1
"Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must either be closer in the representation space, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL) is one such representation learning algorithm that has achieved state-of-the-art results in self-supervised image classification on ImageNet under the linear evaluation protocol. However, the utility of the learnt features of BYOL to perform clustering is not explored. In this work, we study the clustering ability of BYOL and observe that features learnt using BYOL may not be optimal for clustering. We propose a novel consensus clustering based loss function, and train BYOL with the proposed loss in an end-to-end way that improves the clustering ability and outperforms similar clustering based methods on some popular computer vision datasets.",0
"In recent years, unsupervised representation learning has become increasingly popular due to its ability to extract high-quality representations from raw data without any supervision. One application area where these learned representations can prove particularly valuable is clustering, which involves grouping similar objects together into distinct clusters. Many traditional clustering methods rely on handcrafted features that may not capture all relevant characteristics of the underlying dataset. By contrast, consensus clustering with unsupervised representation learning (CURL) leverages learned representations to improve clustering results by identifying consistent cluster assignments across multiple views of the same data. This paper proposes two CURL algorithms: one based on multi-view kernels and another on deep neural networks. Experimental evaluations demonstrate the effectiveness of our approach compared to baseline methods in several benchmark datasets. Our CURL framework shows promise as a tool for exploratory analysis of large datasets, providing meaningful insights into their structure and content.",1
"Attention is a general reasoning mechanism than can flexibly deal with image information, but its memory requirements had made it so far impractical for high resolution image generation. We present Grid Partitioned Attention (GPA), a new approximate attention algorithm that leverages a sparse inductive bias for higher computational and memory efficiency in image domains: queries attend only to few keys, spatially close queries attend to close keys due to correlations. Our paper introduces the new attention layer, analyzes its complexity and how the trade-off between memory usage and model power can be tuned by the hyper-parameters.We will show how such attention enables novel deep learning architectures with copying modules that are especially useful for conditional image generation tasks like pose morphing. Our contributions are (i) algorithm and code1of the novel GPA layer, (ii) a novel deep attention-copying architecture, and (iii) new state-of-the art experimental results in human pose morphing generation benchmarks.",0
"""",1
"The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion models for Imputation (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-70% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines.",0
"Abstract: In this paper we present CSDI (Conditional Score-Based Diffusion Models) as a new methodology for probabilistic time series imputation that can reliably estimate missing values in incomplete data sets. CSDI models learn from conditional score distributions which provide insights into how changes in one variable impact another, enabling the model to make accurate predictions based on temporal patterns learned by conditioning on these scores. Our experimental results show that CSDI outperforms existing state-of-the-art methods for both univariate and multivariate time series prediction tasks across multiple domains. We demonstrate the effectiveness of our approach using detailed error analysis and comparisons to alternative techniques. This work represents an important contribution towards filling a gap in current literature regarding the use of diffusion models in imputation problems.",1
"We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation challenge, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2.",0
"Artificial intelligence has made significant strides in recent years in generating high quality images using deep learning techniques such as generative adversarial networks (GANs) and variational autoencoders (VAEs). One area where current methods fall short is their ability to handle complex tasks such as textual input into these models to create coherent visual representations. In order to address these challenges, we propose cascading diffusion models which integrate multi-scale information that accounts for dependencies across different scales of resolution. We demonstrate the efficacy of our approach on standard datasets commonly used to evaluate image generation quality. Our results show improved fidelity compared to state-of-the-art methods including more detailed depictions of objects, sharper edges, and better overall structure. We conclude by discussing potential future directions for research in this field.",1
"Generative Adversarial Networks (GANs) have become the de-facto standard in image synthesis. However, without considering the foreground-background decomposition, existing GANs tend to capture excessive content correlation between foreground and background, thus constraining the diversity in image generation. This paper presents a novel Foreground-Background Composition GAN (FBC-GAN) that performs image generation by generating foreground objects and background scenes concurrently and independently, followed by composing them with style and geometrical consistency. With this explicit design, FBC-GAN can generate images with foregrounds and backgrounds that are mutually independent in contents, thus lifting the undesirably learned content correlation constraint and achieving superior diversity. It also provides excellent flexibility by allowing the same foreground object with different background scenes, the same background scene with varying foreground objects, or the same foreground object and background scene with different object positions, sizes and poses. It can compose foreground objects and background scenes sampled from different datasets as well. Extensive experiments over multiple datasets show that FBC-GAN achieves competitive visual realism and superior diversity as compared with state-of-the-art methods.",0
"FBC-GAN: Diversifying and Improving Image Generation through Background and Foreground Segmentation The field of image synthesis has seen significant advancements thanks to GANs (Generative Adversarial Network), but there remains a need for improved control over generated images. In this work, we propose FBC-GAN, which stands for “Foreground-background composition.” We use our method to generate diverse high-quality results by segmenting inputs into background and foreground elements that can be controlled independently. Our approach improves both visual quality and diversity compared to previous methods while retaining competitive quantitative performance. FBC-GAN tackles several key challenges faced by existing approaches, such as limited control over generated images, low image fidelity, poor diversity, and instability during training. By using foreground generation to directly control important aspects of an image like object appearance or viewpoint, our method provides users greater creativity and expressiveness. This new level of user control empowers applications across computer graphics and vision domains, from artistic stylization to product configuration tools to virtual reality content creation. Additionally, our architecture offers benefits for stability and speed due to its simpler design, smaller size, and fewer training parameters than other state-of-the-art models. We evaluate our proposed method on five benchmark datasets, each showcasing different characteristics regarding scene complexity or type of variation. Quantitatively, FBC-GAN performs favorably against prior arts, sometimes exceeding their scores by large margins. Qualitatively, subjective evaluations reveal more attractive and coherent results produced by our model, especially in complex scenes where others struggle. User studies further support these observations. In summary, our research makes a signi",1
"Generative adversarial networks (GANs) have achieved great success in image translation and manipulation. However, high-fidelity image generation with faithful style control remains a grand challenge in computer vision. This paper presents a versatile image translation and manipulation framework that achieves accurate semantic and style guidance in image generation by explicitly building a correspondence. To handle the quadratic complexity incurred by building the dense correspondences, we introduce a bi-level feature alignment strategy that adopts a top-$k$ operation to rank block-wise features followed by dense attention between block features which reduces memory cost substantially. As the top-$k$ operation involves index swapping which precludes the gradient propagation, we propose to approximate the non-differentiable top-$k$ operation with a regularized earth mover's problem so that its gradient can be effectively back-propagated. In addition, we design a novel semantic position encoding mechanism that builds up coordinate for each individual semantic region to preserve texture structures while building correspondences. Further, we design a novel confidence feature injection module which mitigates mismatch problem by fusing features adaptively according to the reliability of built correspondences. Extensive experiments show that our method achieves superior performance qualitatively and quantitatively as compared with the state-of-the-art. The code is available at \href{https://github.com/fnzhan/RABIT}{https://github.com/fnzhan/RABIT}.",0
"This paper presents a new method for aligning image features from different domains. Our approach uses a bi-level optimization framework to minimize domain gap and maximizes inter-domain correspondence. We demonstrate the versatility of our model through various image translation tasks such as colorization, super resolution, and semantic segmentation on both synthetic datasets and real world images. Experimental results show that our method outperforms previous state-of-the-art methods by a significant margin while maintaining competitive running time. We believe that feature alignment is an important step towards achieving more advanced image manipulation systems. Future work includes extending our model to handle higher level features and exploring the use of unsupervised learning techniques for automatic domain adaptation.",1
"Despite significant progress on current state-of-the-art image generation models, synthesis of document images containing multiple and complex object layouts is a challenging task. This paper presents a novel approach, called DocSynth, to automatically synthesize document images based on a given layout. In this work, given a spatial layout (bounding boxes with object categories) as a reference by the user, our proposed DocSynth model learns to generate a set of realistic document images consistent with the defined layout. Also, this framework has been adapted to this work as a superior baseline model for creating synthetic document image datasets for augmenting real data during training for document layout analysis tasks. Different sets of learning objectives have been also used to improve the model performance. Quantitatively, we also compare the generated results of our model with real data using standard evaluation metrics. The results highlight that our model can successfully generate realistic and diverse document images with multiple objects. We also present a comprehensive qualitative analysis summary of the different scopes of synthetic image generation tasks. Lastly, to our knowledge this is the first work of its kind.",0
"In this paper, we present DocSynth, a layout guided approach for controllable document image synthesis that enables generating high fidelity images of documents from natural language descriptions. Our method leverages deep neural networks and conditional generation techniques to create coherent layouts of textual content based on user input. We demonstrate how our framework can generate realistic document images such as receipts, certificates, forms, etc., that closely match the desired design specifications. Additionally, we showcase the effectiveness of our system by comparing it against state-of-the-art methods and providing analysis of visual quality metrics. Finally, through several case studies and a usability experiment, we highlight the potential applications of DocSynth in diverse domains including accessibility, data entry automation, and digital art creation. Overall, DocSynth provides a powerful tool for creating authentic looking documents that meets end-user requirements while minimizing manual effort.",1
"This paper studies probability distributions of penultimate activations of classification networks. We show that, when a classification network is trained with the cross-entropy loss, its final classification layer forms a Generative-Discriminative pair with a generative classifier based on a specific distribution of penultimate activations. More importantly, the distribution is parameterized by the weights of the final fully-connected layer, and can be considered as a generative model that synthesizes the penultimate activations without feeding input data. We empirically demonstrate that this generative model enables stable knowledge distillation in the presence of domain shift, and can transfer knowledge from a classifier to variational autoencoders and generative adversarial networks for class-conditional image generation.",0
"This paper investigates the distribution of penultimate activations (i.e., activations before softmax) in classification networks using both theoretical analysis and empirical study on a wide range of benchmark datasets. We find that penultimate activations follow nearly Gaussian distributions under certain conditions, which leads to significant insights into the properties and limitations of deep learning models. In particular, we show how to calculate confidence intervals for predictions based solely on knowledge of the mean and standard deviation of these activation distributions. Furthermore, our analyses reveal interesting patterns in the spreading and decay of signal across layers in neural networks, as well as unexpected robustness properties that arise due to randomization factors such as weight initialization and batch normalization. Our results have important implications for understanding why overparameterized neural networks can still generalize well and provide new directions for future research on calibration methods that could improve their reliability. Overall, this work provides novel tools and perspectives for analyzing deep learning models beyond accuracy metrics alone.",1
"Interactive facial image manipulation attempts to edit single and multiple face attributes using a photo-realistic face and/or semantic mask as input. In the absence of the photo-realistic image (only sketch/mask available), previous methods only retrieve the original face but ignore the potential of aiding model controllability and diversity in the translation process. This paper proposes a sketch-to-image generation framework called S2FGAN, aiming to improve users' ability to interpret and flexibility of face attribute editing from a simple sketch. The proposed framework modifies the constrained latent space semantics trained on Generative Adversarial Networks (GANs). We employ two latent spaces to control the face appearance and adjust the desired attributes of the generated face. Instead of constraining the translation process by using a reference image, the users can command the model to retouch the generated images by involving the semantic information in the generation process. In this way, our method can manipulate single or multiple face attributes by only specifying attributes to be changed. Extensive experimental results on CelebAMask-HQ dataset empirically shows our superior performance and effectiveness on this task. Our method successfully outperforms state-of-the-art methods on attribute manipulation by exploiting greater control of attribute intensity.",0
"Increasingly powerful deep learning techniques have allowed researchers to create high fidelity image generation models that can produce convincing synthetic images from textual descriptions. However, these methods often lack semantic control over key visual aspects of the generated images such as identity or pose, which limits their utility in many applications where users need finer grained control over generated images. Recently, GANs equipped with novel loss functions that enforce consistency between generated outputs and user inputs on a semantically meaningful feature space have shown promising results in improving semantics preservation in image translation tasks. Motivated by these recent advances, we present our approach: S2FGAN (Semantically Aware Interactive Sketch-to-Face Translation), which builds upon these foundational ideas and takes one step further to enable interactive sketching while the model generates the output in real time. Our core innovation lies within formulating the cycle consistent losses on two distinct levels - local features level (e.g., facial landmarks) and global representation level (latent code). By doing so, we achieve high quality, controllable translations across domains without explicit supervision; our method effectively handles pose variation via latent space manipulation even if not provided during training. We showcase the versatility of our framework through extensive experiments on three challenging datasets spanning diverse settings, including faces, human bodies, and caricature illustrations. Comprehensive comparisons with state-of-the-art methods validate the superiority of our proposed framework, highlighting benefits of each component.",1
"While GANs have shown success in realistic image generation, the idea of using GANs for other tasks unrelated to synthesis is underexplored. Do GANs learn meaningful structural parts of objects during their attempt to reproduce those objects? In this work, we test this hypothesis and propose a simple and effective approach based on GANs for semantic part segmentation that requires as few as one label example along with an unlabeled dataset. Our key idea is to leverage a trained GAN to extract pixel-wise representation from the input image and use it as feature vectors for a segmentation network. Our experiments demonstrate that GANs representation is ""readily discriminative"" and produces surprisingly good results that are comparable to those from supervised baselines trained with significantly more labels. We believe this novel repurposing of GANs underlies a new class of unsupervised representation learning that is applicable to many other tasks. More results are available at https://repurposegans.github.io/.",0
"This paper presents a novel method that repurposes Generative Adversarial Networks (GANs) for one-shot semantic part segmentation. Unlike traditional methods that rely on detailed annotations and large amounts of training data, our approach leverages minimal supervision by utilizing transfer learning from similar images. Our proposed method outperforms state-of-the-art techniques both quantitatively and qualitatively while significantly reducing annotation effort. Additionally, we demonstrate the effectiveness of our algorithm across multiple domains including autonomous driving, augmented reality and medical imaging. Overall, this work represents a significant advancement towards real world applications where annotating each object instance within a scene becomes prohibitive.",1
"In this paper, we present a non-parametric structured latent variable model for image generation, called NP-DRAW, which sequentially draws on a latent canvas in a part-by-part fashion and then decodes the image from the canvas. Our key contributions are as follows. 1) We propose a non-parametric prior distribution over the appearance of image parts so that the latent variable ``what-to-draw'' per step becomes a categorical random variable. This improves the expressiveness and greatly eases the learning compared to Gaussians used in the literature. 2) We model the sequential dependency structure of parts via a Transformer, which is more powerful and easier to train compared to RNNs used in the literature. 3) We propose an effective heuristic parsing algorithm to pre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show that our method significantly outperforms previous structured image models like DRAW and AIR and is competitive to other generic generative models. Moreover, we show that our model's inherent compositionality and interpretability bring significant benefits in the low-data learning regime and latent space editing. Code is available at https://github.com/ZENGXH/NPDRAW.",0
"""In computer vision research, developing models that can generate realistic images has become an increasingly important area of study. However, traditional generative modeling methods suffer from limitations such as difficulty in training deep neural networks (DNNs) due to their large size and complexity, sensitivity to initialization parameters, and lack of interpretability. To address these challenges, we propose a new approach called NP-DRAW, which uses a non-parametric structured latent variable model (LVM). This LVM allows us to separate high-dimensional data representations into two parts: a low-dimensional probabilistic part that captures global shape variation and a dense, continuous representation of local features that vary smoothly in space. Our results show that NP-DRAW significantly outperforms state-of-the-art image generation models on several benchmark datasets in terms of visual quality and quantitative evaluation metrics.""  Keywords: Generative Models, Computer Vision, Deep Neural Networks (DNN), Interpretability, Visual Quality, Evaluation Metrics",1
"This work tackles the issue of fairness in the context of generative procedures, such as image super-resolution, which entail different definitions from the standard classification setting. Moreover, while traditional group fairness definitions are typically defined with respect to specified protected groups -- camouflaging the fact that these groupings are artificial and carry historical and political motivations -- we emphasize that there are no ground truth identities. For instance, should South and East Asians be viewed as a single group or separate groups? Should we consider one race as a whole or further split by gender? Choosing which groups are valid and who belongs in them is an impossible dilemma and being ""fair"" with respect to Asians may require being ""unfair"" with respect to South Asians. This motivates the introduction of definitions that allow algorithms to be \emph{oblivious} to the relevant groupings.   We define several intuitive notions of group fairness and study their incompatibilities and trade-offs. We show that the natural extension of demographic parity is strongly dependent on the grouping, and \emph{impossible} to achieve obliviously. On the other hand, the conceptually new definition we introduce, Conditional Proportional Representation, can be achieved obliviously through Posterior Sampling. Our experiments validate our theoretical results and achieve fair image reconstruction using state-of-the-art generative models.",0
"Sure! Here's an example abstract:  ""Image generation techniques using deep learning have been rapidly advancing over recent years, but ensuring fairness in these systems remains challenging due to their reliance on sensitive attributes that can be uncertain or subjective. This work focuses on addressing fairness issues in image generation by presenting two methods that utilize uncertainty estimates from pre-trained models to guide the generation process. Our first method employs aleatoric uncertainty to adjust the likelihood of selecting certain attribute values during the generative process, while our second method uses epistemic uncertainty to weight different possible attribute choices based on confidence levels. We evaluate both approaches on benchmark datasets and demonstrate their effectiveness in producing more diverse images while maintaining high quality. Additionally, we provide ablation studies to show the contribution of each method component.""",1
"Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard softmax classifier can be reinterpreted as an energy-based model (EBM) for the joint distribution p(x,y); the resulting model can be optimized to improve calibration, robustness, and out-of-distribution detection, while generating samples rivaling the quality of recent GAN-based approaches. However, the softmax classifier that JEM exploits is inherently discriminative and its latent feature space is not well formulated as probabilistic distributions, which may hinder its potential for image generation and incur training instability. We hypothesize that generative classifiers, such as Linear Discriminant Analysis (LDA), might be more suitable for image generation since generative classifiers model the data generation process explicitly. This paper therefore investigates an LDA classifier for image classification and generation. In particular, the Max-Mahalanobis Classifier (MMC), a special case of LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be trained discriminatively, generatively, or jointly for image classification and generation. Extensive experiments on multiple datasets show that GMMC achieves state-of-the-art discriminative and generative performances, while outperforming JEM in calibration, adversarial robustness, and out-of-distribution detection by a significant margin. Our source code is available at https://github.com/sndnyang/GMMC.",0
"Abstract: This paper explores new applications of generative max-Mahalanobis classifiers for image classification using deep neural networks. We train classifiers on large datasets of images labeled as belonging to one of several classes, then evaluate their performance on held out test sets. Our experiments show that these models achieve state-of-the-art accuracy, even though they can use simpler architectures than other approaches. Additionally, we propose novel methods for generating synthetic images by sampling from the learned distributions of features corresponding to different categories. Finally, we discuss future directions such as improving sample quality through latent space interpolation and generating dynamic scenes. With its broad range of potential applications, generative max-Mahalanobis classification has the promise to revolutionize how we approach computer vision tasks. By jointly learning to discriminate and generate, our framework opens up exciting possibilities in imaging research and beyond.",1
"We present a novel data generation tool for document processing. The tool focuses on providing a maximal level of visual information in a normal type document, ranging from character position to paragraph-level position. It also enables working with a large dataset on low-resource languages as well as providing a mean of processing thorough full-level information of the documented text. The data generation tools come with a dataset of 320000 Vietnamese synthetic document images and an instruction to generate a dataset of similar size in other languages. The repository can be found at: https://github.com/tson1997/SDL-Document-Image-Generation",0
"Full-level annotated document layout (FDLP) refers to the process of analyzing and structuring large amounts of unstructured text into organized documents that can be easily searched and retrieved. However, generating high-quality FDLP datasets is often challenging due to the complexities involved in annotation and organization. In recent years, several techniques have been proposed to address these issues, including semi-automatic labeling using distant supervision, active learning, transfer learning from pre-trained models, and graph-based methods. Despite their effectiveness, there remain limitations in terms of scalability, accuracy, and interpretability. To overcome these challenges, we propose a novel approach called Structured Document Learning (SDL), which leverages advances in deep learning architectures and probabilistic programming frameworks to generate new data generation tools for improved FDLP performance. Our experiments show that SDL outperforms state-of-the-art baselines across various evaluation metrics and domains, demonstrating its potential as a powerful tool for creating high-quality annotated document layouts. This work paves the way for future research in developing more efficient and effective strategies for generating and utilizing FDLP datasets in real-world applications.",1
"In most existing learning systems, images are typically viewed as 2D pixel arrays. However, in another paradigm gaining popularity, a 2D image is represented as an implicit neural representation (INR) - an MLP that predicts an RGB pixel value given its (x,y) coordinate. In this paper, we propose two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and use them to build a state-of-the-art continuous image GAN. Previous attempts to adapt INRs for image generation were limited to MNIST-like datasets and do not scale to complex real-world data. Our proposed INR-GAN architecture improves the performance of continuous image generators by several times, greatly reducing the gap between continuous image GANs and pixel-based ones. Apart from that, we explore several exciting properties of the INR-based decoders, like out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior. The project page is located at https://universome.github.io/inr-gan.",0
"Artificial intelligence has been making rapid progress in image generation. One important area of research is adversarially trained generative models (ATGM), which aim to improve the quality and realism of generated images by adding noise that deceives a discriminator network during training. Despite their success, there exist several limitations to current ATGMs such as difficulty in generating high resolution images, limited diversity, and sensitivity to hyperparameters. To address these issues we propose a new framework called Progressive Adversarial Generative Networks (PAGNet). PAGNets achieve continuous image generation through the use of multi-scale pyramidal encoding and adversarial learning under different spatial scales. We demonstrate the effectiveness of our approach on three datasets: CIFAR-10, STL-10, and ImageNet. Experimental results show that PAGNet outperforms state-of-the-art methods on both quantitative measures and human evaluations. Our work highlights the potential for advancing image synthesis using innovative architectures designed for large scale data.",1
"Advances in technology have led to the development of methods that can create desired visual multimedia. In particular, image generation using deep learning has been extensively studied across diverse fields. In comparison, video generation, especially on conditional inputs, remains a challenging and less explored area. To narrow this gap, we aim to train our model to produce a video corresponding to a given text description. We propose a novel training framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN), which evolves frame-by-frame and finally produces a full-length video. In the first phase, we focus on creating a high-quality single video frame while learning the relationship between the text and an image. As the steps proceed, our model is trained gradually on more number of consecutive frames.This step-by-step learning process helps stabilize the training and enables the creation of high-resolution video based on conditional text descriptions. Qualitative and quantitative experimental results on various datasets demonstrate the effectiveness of the proposed method.",0
"Title: Text to Image to Video Generation with Step-by-Step Evolutionary Generator (TiVGAN)  Artificial intelligence has made significant advancements in recent years, enabling machines to perform complex tasks such as image and video generation from textual descriptions. While these models have achieved impressive results, there remains a need for improvement in terms of both efficiency and visual coherence. In particular, existing methods often struggle with generating high-quality images that align with the given text description step by step.  To address these limitations, we propose TiVGAN, a novel approach that generates high-resolution videos frame-by-frame while preserving temporal consistency and coherence. Our method is based on an evolutionary algorithm that iteratively evolves each individual frame towards better alignment with the text description. By leveraging multi-objective optimization, our model balances fidelity to the input text with visual quality, resulting in more accurate and visually compelling outputs.  We evaluate our approach using standard benchmark datasets and demonstrate the effectiveness of TiVGAN in comparison with state-of-the-art alternatives. Our experiments show that our method produces superior results across multiple metrics, including similarity to ground truth, visual fidelity, and diversity. Furthermore, we provide qualitative analysis to illustrate the benefits of our proposed system.  Our work represents a significant advance in the field of text-to-image-to-video synthesis. We believe that TiVGAN's ability to generate highly coherent, visually appealing videos holds great potential for numerous applications ranging from entertainment to education and beyond. With future improvements in computational resources, we expect even greater strides to be made in this exciting area of research.",1
"In this paper, we propose a novel encoder, called ShapeEditor, for high-resolution, realistic and high-fidelity face exchange. First of all, in order to ensure sufficient clarity and authenticity, our key idea is to use an advanced pretrained high-quality random face image generator, i.e. StyleGAN, as backbone. Secondly, we design ShapeEditor, a two-step encoder, to make the swapped face integrate the identity and attribute of the input faces. In the first step, we extract the identity vector of the source image and the attribute vector of the target image respectively; in the second step, we map the concatenation of identity vector and attribute vector into the $\mathcal{W+}$ potential space. In addition, for learning to map into the latent space of StyleGAN, we propose a set of self-supervised loss functions with which the training data do not need to be labeled manually. Extensive experiments on the test dataset show that the results of our method not only have a great advantage in clarity and authenticity than other state-of-the-art methods, but also reflect the sufficient integration of identity and attribute.",0
"In recent years, deep learning has made significant strides in generating realistic images using Generative Adversarial Networks (GANs). One particular architecture that has gained popularity is StyleGAN, which can generate high resolution and detailed images across various domains such as faces, objects, and scenes. In this work, we propose a novel approach for face swapping based on the pretrained StyleGAN2 encoder. Our method, called ""ShapeEditor"", enables intuitive control over the target identity by manipulating the latent code generated from the input image, resulting in natural looking transitions between the source and target identities. We evaluate our approach on both qualitative metrics and human studies, demonstrating improved quality and coherence compared to previous methods while maintaining efficiency through minimal training data requirements. With its ease of use and effectiveness, ShapeEditor offers a new tool for computer graphics researchers, artists, and practitioners alike.",1
"Since human-labeled samples are free for the target set, unsupervised person re-identification (Re-ID) has attracted much attention in recent years, by additionally exploiting the source set. However, due to the differences on camera styles, illumination and backgrounds, there exists a large gap between source domain and target domain, introducing a great challenge on cross-domain matching. To tackle this problem, in this paper we propose a novel method named Dual-stream Reciprocal Disentanglement Learning (DRDL), which is quite efficient in learning domain-invariant features. In DRDL, two encoders are first constructed for id-related and id-unrelated feature extractions, which are respectively measured by their associated classifiers. Furthermore, followed by an adversarial learning strategy, both streams reciprocally and positively effect each other, so that the id-related features and id-unrelated features are completely disentangled from a given image, allowing the encoder to be powerful enough to obtain the discriminative but domain-invariant features. In contrast to existing approaches, our proposed method is free from image generation, which not only reduces the computational complexity remarkably, but also removes redundant information from id-related features. Extensive experiments substantiate the superiority of our proposed method compared with the state-of-the-arts. The source code has been released in https://github.com/lhf12278/DRDL.",0
"In recent years, person re-identification (ReID) has become increasingly important as a tool for tracking individuals across different camera networks. However, achieving high accuracy remains challenging due to variations in lighting conditions, pose changes, and background clutter. One method used to overcome these issues is domain adaptation, which involves training models on multiple datasets that cover more diverse scenarios than those seen in standard benchmarks like Market1501 and DukeMTMC. In this paper, we propose a new approach called dual-stream reciprocal disentanglement learning (DSRDL), which combines two well-known techniques: feature-level disentanglement and adversarial training using cycle consistency loss. By doing so, our algorithm can effectively learn discriminative features while reducing cross-domain discrepancies, improving overall performance compared to state-of-the-art methods. We evaluate the effectiveness of DSRDL using four widely used datasets and demonstrate its superiority over other algorithms through extensive experiments. Overall, this research presents a significant advancement in the field of ReID by introducing a novel and effective technique for enhancing domain adaptation and achieving improved results.",1
"Pseudo-normality synthesis, which computationally generates a pseudo-normal image from an abnormal one (e.g., with lesions), is critical in many perspectives, from lesion detection, data augmentation to clinical surgery suggestion. However, it is challenging to generate high-quality pseudo-normal images in the absence of the lesion information. Thus, expensive lesion segmentation data have been introduced to provide lesion information for the generative models and improve the quality of the synthetic images. In this paper, we aim to alleviate the need of a large amount of lesion segmentation data when generating pseudo-normal images. We propose a Semi-supervised Medical Image generative LEarning network (SMILE) which not only utilizes limited medical images with segmentation masks, but also leverages massive medical images without segmentation masks to generate realistic pseudo-normal images. Extensive experiments show that our model outperforms the best state-of-the-art model by up to 6% for data augmentation task and 3% in generating high-quality images. Moreover, the proposed semi-supervised learning achieves comparable medical image synthesis quality with supervised learning model, using only 50 of segmentation data.",0
"This paper presents a novel approach to identifying diseases using semi-supervised pseudo-normalization and normalcy synthesis techniques on medical images. Traditionally, the detection of diseases requires large amounts of labeled training data, which can be difficult to obtain. To overcome this limitation, we propose a method that leverages unlabeled images to learn how the appearance of diseased tissue differs from healthy tissue.  Our approach starts by generating pseudo-normals for each pixel in an input image. These pseudo-normals represent the distribution of intensities observed at that location across all pixels within normal regions identified in our dataset. We then use these pseudo-normals as a prior to generate new samples representing ""abnormal"" appearances of different diseases. Finally, we train a classifier to predict whether any given sample comes from the original input image, one of the generated pseudo-normal distributions, or one of the abnormal diseases synthesized during training.  We evaluate our method on three publicly available datasets: NCAT-br96 breast MRI scans, LUNA lung CT scans, and DeepMind-HF chest X-rays. Our results demonstrate that our method achieves state-of-the-art performance compared to existing supervised and weakly supervised methods, while requiring significantly less annotated data for training. Additionally, we show through qualitative analysis that our model generates realistic pseudo-normals and convincing examples of several common diseases.  Overall, our work demonstrates the potential of combining semi-supervised learning and normalcy synthesis to improve the accuracy of disease diagnosis in medical imaging applications. By utilizing both unannotated and partially annotated data sources, our method allows for more efficient and effective training of machine learning models, with significant implications for improving healthcare outcomes worldwide.",1
"Generative adversarial networks (GANs) have gained considerable attention owing to their ability to reproduce images. However, they can recreate training images faithfully despite image degradation in the form of blur, noise, and compression, generating similarly degraded images. To solve this problem, the recently proposed noise robust GAN (NR-GAN) provides a partial solution by demonstrating the ability to learn a clean image generator directly from noisy images using a two-generator model comprising image and noise generators. However, its application is limited to noise, which is relatively easy to decompose owing to its additive and reversible characteristics, and its application to irreversible image degradation, in the form of blur, compression, and combination of all, remains a challenge. To address these problems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that can learn a clean image generator directly from degraded images without knowledge of degradation parameters (e.g., blur kernel types, noise amounts, or quality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator model composed of image, blur-kernel, noise, and quality-factor generators. However, in contrast to NR-GAN, to address irreversible characteristics, we introduce masking architectures adjusting degradation strength values in a data-driven manner using bypasses before and after degradation. Furthermore, to suppress uncertainty caused by the combination of blur, noise, and compression, we introduce adaptive consistency losses imposing consistency between irreversible degradation processes according to the degradation strengths. We demonstrate the effectiveness of BNCR-GAN through large-scale comparative studies on CIFAR-10 and a generality analysis on FFHQ. In addition, we demonstrate the applicability of BNCR-GAN in image restoration.",0
"Artificial intelligence has revolutionized many aspects of our lives. From smartphones to self-driving cars, from virtual assistants to medical diagnosis systems, artificial intelligence algorithms have improved our technological landscape. Despite significant advances over recent years, some AI approaches still struggle to reliably respond with suitable answers due to the complexity of tasks they face. However, generative adversarial networks (GANs) can significantly increase the robustness of machine learning models by addressing these issues through blur, noise, and compression. Our proposed method uses GANs to improve stability and reliability under adverse conditions such as image distortion or incomplete data sets. This work investigates how modifications in three key areas: blurring, noising, and compression techniques, result in increased resilience to real world scenarios. Ultimately, these improvements lead to better responses to difficult questions that would normally perplex traditional machine learning approaches. With their enhanced accuracy and reduced error rates, GANs offer a more reliable option in both industrial and domestic settings. By strengthening AI capabilities, we move closer towards creating intelligent machines capable of thinking and reacting like humans.",1
"Variational inference (VI) plays an essential role in approximate Bayesian inference due to its computational efficiency and broad applicability. Crucial to the performance of VI is the selection of the associated divergence measure, as VI approximates the intractable distribution by minimizing this divergence. In this paper we propose a meta-learning algorithm to learn the divergence metric suited for the task of interest, automating the design of VI methods. In addition, we learn the initialization of the variational parameters without additional cost when our method is deployed in the few-shot learning scenarios. We demonstrate our approach outperforms standard VI on Gaussian mixture distribution approximation, Bayesian neural network regression, image generation with variational autoencoders and recommender systems with a partial variational autoencoder.",0
"This is a challenging problem that needs special attention and should be considered separately from other forms of learning. Meta-learning divergences aim to model uncertainty over latent variables by exploiting structured assumptions about conditional distributions induced by learned neural networks. Our approach has three stages: meta-learn the parameters of prior distribution using variational inference; learn to select between prior predictive (which models the data) vs posterior predictive densities via training loss minimization on synthetic task ensembles, while maximizing mutual information between model's outputs under different tasks at each iteration step; use these predictions as ground truth signals for next optimization iteration. We evaluated our algorithm on several benchmark datasets including SVHN, CIFAR-10, ImageNet, Omniglot, and YFCC1m. Empirical results indicate significant gains across all metrics compared to standard VI baselines. We visualize uncertainty maps which reveal interesting patterns such as object contours, occlusions, background clutters etc. Extensive ablation studies confirm validity of design choices we made throughout our pipeline implementation and shed light onto interpreting behavior under various hyperparameter regimes. Additional human evaluation provides qualitative assessment of model performances based on ranking criteria such as recognition rates. We compare with recent competitive advances including ensemble methods like Snapshot Ensemble and model averaging, and show improvement over them. To summarize, MLDV establishes itself as powerful alternative within family of generative deep learning architectures paving new directions towards better understanding of Bayesian inference over discrete latents spaces.",1
"Providing a human-understandable explanation of classifiers' decisions has become imperative to generate trust in their use for day-to-day tasks. Although many works have addressed this problem by generating visual explanation maps, they often provide noisy and inaccurate results forcing the use of heuristic regularization unrelated to the classifier in question. In this paper, we propose a new general perspective of the visual explanation problem overcoming these limitations. We show that visual explanation can be produced as the difference between two generated images obtained via two specific conditional generative models. Both generative models are trained using the classifier to explain and a database to enforce the following properties: (i) All images generated by the first generator are classified similarly to the input image, whereas the second generator's outputs are classified oppositely. (ii) Generated images belong to the distribution of real images. (iii) The distances between the input image and the corresponding generated images are minimal so that the difference between the generated elements only reveals relevant information for the studied classifier. Using symmetrical and cyclic constraints, we present two different approximations and implementations of the general formulation. Experimentally, we demonstrate significant improvements w.r.t the state-of-the-art on three different public data sets. In particular, the localization of regions influencing the classifier is consistent with human annotations.",0
"This paper investigates how to use conditional generative models (CGMs) in classifiers. We propose using them as part of a general framework to explain decisions made by those systems. This allows users to make more informed choices based on the reasoning used by their machine learning algorithms. By combining CGMs with other methods of explanation, we can create more complete pictures that highlight both statistical patterns found during training, along with human explanations from experts in related fields. Our approach has promising results across multiple domains tested, including computer vision, natural language processing, and decision tree ensembles. These demonstrate improvements over traditional approaches, which often lacked transparency into why certain predictions were made. Our method opens up new possibilities for debugging classifiers, understanding failures, and enhancing interpretability overall. Additionally, our work can be applied beyond classifiers, potentially becoming a cornerstone in a broader theory of interpreting complex artificial intelligence (AI).",1
"We propose a novel and unified Cycle in Cycle Generative Adversarial Network (C2GAN) for generating human faces, hands, bodies, and natural scenes. Our proposed C2GAN is a cross-modal model exploring the joint exploitation of the input image data and guidance data in an interactive manner. C2GAN contains two different generators, i.e., an image-generation generator and a guidance-generation generator. Both generators are mutually connected and trained in an end-to-end fashion and explicitly form three cycled subnets, i.e., one image generation cycle and two guidance generation cycles. Each cycle aims at reconstructing the input domain and simultaneously produces a useful output involved in the generation of another cycle. In this way, the cycles constrain each other implicitly providing complementary information from both image and guidance modalities and bringing an extra supervision gradient across the cycles, facilitating a more robust optimization of the whole model. Extensive results on four guided image-to-image translation subtasks demonstrate that the proposed C2GAN is effective in generating more realistic images compared with state-of-the-art models. The code is available at https://github.com/Ha0Tang/C2GAN.",0
"In natural language processing (NLP), generative models have emerged as powerful tools for generating new text data that can be used in downstream applications such as machine translation or question answering. However, training generative NLP models can be computationally expensive, limiting their applicability on smaller hardware systems. Here we propose using cycles within generative adversarial networks (GANs) to reduce computational costs while still maintaining high levels of accuracy. Specifically, we show that dividing each cycle into two subcycles improves stability and reduces model memory usage compared to standard single-cycle GANs. This enables our method, called ""Cycle-in-Cycle GAN"", to achieve state-of-the-art performance across several benchmark datasets for image generation tasks including human faces, hands, bodies, and natural scenes. Furthermore, our approach consistently yields lower FID scores than competitive methods over multiple runs on various datasets, demonstrating improved robustness. Our results highlight the effectiveness of applying cycles within cycles in generating high-quality synthetic images at a reduced cost, making generative NLP more accessible to users with limited computing resources. Overall, these findings contribute to advancing the field of computer vision by providing researchers and practitioners with an efficient means to generate realistic images using modern deep learning techniques.",1
"Deep visual models are susceptible to adversarial perturbations to inputs. Although these signals are carefully crafted, they still appear noise-like patterns to humans. This observation has led to the argument that deep visual representation is misaligned with human perception. We counter-argue by providing evidence of human-meaningful patterns in adversarial perturbations. We first propose an attack that fools a network to confuse a whole category of objects (source class) with a target label. Our attack also limits the unintended fooling by samples from non-sources classes, thereby circumscribing human-defined semantic notions for network fooling. We show that the proposed attack not only leads to the emergence of regular geometric patterns in the perturbations, but also reveals insightful information about the decision boundaries of deep models. Exploring this phenomenon further, we alter the `adversarial' objective of our attack to use it as a tool to `explain' deep visual representation. We show that by careful channeling and projection of the perturbations computed by our method, we can visualize a model's understanding of human-defined semantic notions. Finally, we exploit the explanability properties of our perturbations to perform image generation, inpainting and interactive image manipulation by attacking adversarialy robust `classifiers'.In all, our major contribution is a novel pragmatic adversarial attack that is subsequently transformed into a tool to interpret the visual models. The article also makes secondary contributions in terms of establishing the utility of our attack beyond the adversarial objective with multiple interesting applications.",0
"""Abstract: This paper describes methods that can attack deep networks by generating inputs that are confusing to them but easily recognizable as such by human observers. We first show that existing methods for adversarial examples often fool humans just as effectively as they do deep neural nets (DNNs). By contrast our method produces adversarial perturbations that are clearly visible and provoke laughter or disgust from human observers while leaving DNN models completely baffled.""",1
"Although hierarchical structures are popular in recent vision transformers, they require sophisticated designs and massive datasets to work well. In this work, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical manner. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture with minor code changes upon the original vision transformer and obtains improved performance compared to existing methods. Our empirical results show that the proposed method NesT converges faster and requires much less training data to achieve good generalization. For example, a NesT with 68M parameters trained on ImageNet for 100/300 epochs achieves $82.3\%/83.8\%$ accuracy evaluated on $224\times 224$ image size, outperforming previous methods with up to $57\%$ parameter reduction. Training a NesT with 6M parameters from scratch on CIFAR10 achieves $96\%$ accuracy using a single GPU, setting a new state of the art for vision transformers. Beyond image classification, we extend the key idea to image generation and show NesT leads to a strong decoder that is 8$\times$ faster than previous transformer based generators. Furthermore, we also propose a novel method for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer.",0
"This paper presents a new approach for aggregating nested transformers. We propose a novel architecture that utilizes multi-head attention mechanisms to capture interactions between different layers within each module. Our method outperforms previous state-of-the-art methods on a variety of benchmark datasets, demonstrating its effectiveness for natural language processing tasks. Additionally, we analyze the impact of different design choices such as head number and scaling factor size, providing insights into how these factors affect model performance. Overall, our work represents a significant contribution to the field of artificial intelligence.",1
"Interpreting how does deep neural networks (DNNs) make predictions is a vital field in artificial intelligence, which hinders wide applications of DNNs. Visualization of learned representations helps we humans understand the vision of DNNs. In this work, visualized images that can activate the neural network to the target classes are generated by back-propagation method. Here, rotation and scaling operations are applied to introduce the transformation invariance in the image generating process, which we find a significant improvement on visualization effect. Finally, we show some cases that such method can help us to gain insight into neural networks.",0
"This study explores methods for improving the accuracy of computer vision systems by leveraging transformation invariant optimization techniques. We demonstrate how these techniques can help identify key features in images, even when those features undergo changes in appearance due to changes in lighting conditions or other environmental factors. Our approach builds upon prior work in image processing, using machine learning algorithms trained on large datasets of images to develop powerful feature detectors that can distinguish important elements within an image regardless of external transformations. Experimental results show promising improvements over traditional computer vision approaches, particularly in challenging real-world scenarios where variations in lighting or viewpoint can significantly impact performance. By advancing the state of art in transformation invariant optimization for computer vision, our research has potential applications across many fields, including robotics, autonomous vehicles, medical imaging, and more.",1
"Recent work introduced progressive network growing as a promising way to ease the training for large GANs, but the model design and architecture-growing strategy still remain under-explored and needs manual design for different image data. In this paper, we propose a method to dynamically grow a GAN during training, optimizing the network architecture and its parameters together with automation. The method embeds architecture search techniques as an interleaving step with gradient-based training to periodically seek the optimal architecture-growing strategy for the generator and discriminator. It enjoys the benefits of both eased training because of progressive growing and improved performance because of broader architecture design space. Experimental results demonstrate new state-of-the-art of image generation. Observations in the search procedure also provide constructive insights into the GAN model design such as generator-discriminator balance and convolutional layer choices.",0
"This paper presents a new approach to training generative adversarial networks (GANs), which dynamically grows the number of generator and discriminator neurons over time. We show that this method leads to improved stability, better performance on a variety of benchmark datasets, and more interpretable results compared to traditional static architecture GANs. In addition, we introduce two novel architectures specifically designed for image generation tasks: a ""residual"" discriminator network that allows for more efficient learning at early stages of training; and a new generator network based on a denoising autoencoder. Our experiments demonstrate state-of-the-art performance across multiple evaluation metrics on both natural images and clinical MRI scans.",1
"Deep learning approaches have become the standard solution to many problems in computer vision and robotics, but obtaining sufficient training data in high enough quality is challenging, as human labor is error prone, time consuming, and expensive. Solutions based on simulation have become more popular in recent years, but the gap between simulation and reality is still a major issue. In this paper, we introduce a novel method for augmenting synthetic image data through unsupervised image-to-image translation by applying the style of real world images to simulated images with open source frameworks. The generated dataset is combined with conventional augmentation methods and is then applied to a neural network model running in real-time on autonomous soccer robots. Our evaluation shows a significant improvement compared to models trained on images generated entirely in simulation.",0
"In recent years, there has been significant progress in developing virtual reality (VR) systems that can provide users with high levels of realism and immersion. However, one major challenge facing these systems is the gap between simulated reality and actual reality, which can lead to poor user experiences and reduce overall effectiveness. To address this issue, researchers have developed unsupervised image translation algorithms that can generate photorealistic images from simulation data. These methods use machine learning techniques such as generative adversarial networks (GANs) and cycle consistency loss functions to achieve accurate translations between simulated and real environments. This paper presents several promising examples of how these algorithms can be used to close the reality gap in VR systems and improve user experience. Overall, the results demonstrate the potential value of unsupervised sim-to-real image translation in enhancing the performance and efficacy of virtual reality applications.",1
"State-of-the-art (SOTA) Generative Models (GMs) can synthesize photo-realistic images that are hard for humans to distinguish from genuine photos. We propose to perform reverse engineering of GMs to infer the model hyperparameters from the images generated by these models. We define a novel problem, ""model parsing"", as estimating GM network architectures and training loss functions by examining their generated images -- a task seemingly impossible for human beings. To tackle this problem, we propose a framework with two components: a Fingerprint Estimation Network (FEN), which estimates a GM fingerprint from a generated image by training with four constraints to encourage the fingerprint to have desired properties, and a Parsing Network (PN), which predicts network architecture and loss functions from the estimated fingerprints. To evaluate our approach, we collect a fake image dataset with $100$K images generated by $100$ GMs. Extensive experiments show encouraging results in parsing the hyperparameters of the unseen models. Finally, our fingerprint estimation can be leveraged for deepfake detection and image attribution, as we show by reporting SOTA results on both the recent Celeb-DF and image attribution benchmarks.",0
"This is a technical article on reverse engineering generative models. It outlines how hyperparameters can be inferred from generated images. As such, it may require advanced knowledge of machine learning techniques to fully comprehend. If you would like me to write a more simplified summary without jargon then please don’t hesitate to ask!",1
"Human motion retargeting aims to transfer the motion of one person in a ""driving"" video or set of images to another person. Existing efforts leverage a long training video from each target person to train a subject-specific motion transfer model. However, the scalability of such methods is limited, as each model can only generate videos for the given target subject, and such training videos are labor-intensive to acquire and process. Few-shot motion transfer techniques, which only require one or a few images from a target, have recently drawn considerable attention. Methods addressing this task generally use either 2D or explicit 3D representations to transfer motion, and in doing so, sacrifice either accurate geometric modeling or the flexibility of an end-to-end learned representation. Inspired by the Transformable Bottleneck Network, which renders novel views and manipulations of rigid objects, we propose an approach based on an implicit volumetric representation of the image content, which can then be spatially manipulated using volumetric flow fields. We address the challenging question of how to aggregate information across different body poses, learning flow fields that allow for combining content from the appropriate regions of input images of highly non-rigid human subjects performing complex motions into a single implicit volumetric representation. This allows us to learn our 3D representation solely from videos of moving people. Armed with both 3D object understanding and end-to-end learned rendering, this categorically novel representation delivers state-of-the-art image generation quality, as shown by our quantitative and qualitative evaluations.",0
"This abstract should assume that readers have knowledge of deep learning but might not know any more than that so try to explain concepts as if you were teaching them to someone new to the field who has some background in math/computer science. Also describe how your method differs from related work including limitations. Lastly highlight key findings that make these results interesting! Good luck! Title: ""Flow Guided Transformable Bottleneck Networks for Motion Retargeting"" Abstract Deep learning has revolutionized many fields, including computer vision tasks such as motion retargeting. In recent years, we have seen improvements in accuracy and efficiency due to advancements like convolutional neural networks (CNNs). One challenge faced by researchers remains creating high quality output for tasks where input data may vary widely and can cause training issues. Our approach tackles this problem head on using the transformable bottleneck block which acts as the building block of our network architecture. We introduce a novel flow guiding mechanism into the mix, allowing us to adaptively adjust the features generated during processing. By focusing attention only where necessary, computational resources are better allocated for improved performance without sacrificing speed. To showcase the effectiveness of our model we compare against current state of the art methods. Results demonstrate superiority in both quantitative metrics and visual fidelity. Additionally our work runs efficiently enough to run real time on most consumer hardware devices opening up possibilities for future applications in gaming and other interactive environments. Keywords: Motion Retargeting; Flow Guidance; Attention Mechanisms; CNNs; Efficiency",1
"Attention-based models, exemplified by the Transformer, can effectively model long range dependency, but suffer from the quadratic complexity of self-attention operation, making them difficult to be adopted for high-resolution image generation based on Generative Adversarial Networks (GANs). In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efficient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function. To further improve the performance, we introduce an additional self-modulation component based on cross-attention. The resulting model, denoted as HiT, has a linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images. We show in the experiments that the proposed HiT achieves state-of-the-art FID scores of 31.87 and 2.95 on unconditional ImageNet $128 \times 128$ and FFHQ $256 \times 256$, respectively, with a reasonable throughput. We believe the proposed HiT is an important milestone for generators in GANs which are completely free of convolutions.",0
"This paper presents a new method for improving training stability and generating higher quality images from high-resolution generative adversarial networks (GANs). Our approach builds upon recent advances in transformer architectures by applying self attention mechanisms to both the generator and discriminator models within the GAN framework. Experimental results demonstrate that our improved model significantly outperforms previous state-of-the-art methods on a range of benchmark datasets. Furthermore, we show through qualitative and quantitative evaluations that our generated images exhibit greater visual fidelity, coherency, and diversity compared to existing GAN approaches at similar resolutions. Our findings indicate that the application of self attention mechanisms provides a powerful tool for enhancing the performance and capabilities of high-resolution GANs, opening up possibilities for their use in even more demanding image synthesis tasks such as video generation and domain transfer.",1
"Generative modeling has evolved to a notable field of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsupervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable. In this work, we introduce a general framework, called CoPE, that enables a polynomial expansion of two input variables and captures their auto- and cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in five tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that CoPE can be useful for tackling diverse conditional generation tasks.",0
"Artificial neural networks have made significant strides in generating high quality images through deep learning techniques like Generative Adversarial Networks (GANs). However, they still lack interpretability, as most of these models operate as ""black boxes"", making it difficult to analyze their decision-making processes. In order to address these limitations, we propose a novel framework called CoPE (Conditional Pixel-wise Explanations), which generates conditional explanations in real time during inference by encoding local explanatory signals directly into each pixel of generated images. This approach achieves state-of-the-art performance on several benchmark datasets while providing transparency into how decisions are made at each step of the model's computation graph, promising new possibilities for debugging and improving existing GANs. We demonstrate that our framework can provide accurate approximations to any arbitrary model within the family of polynomial expansions and showcase its flexibility across multiple applications such as semantic segmentation, edge detection, object detection, image super-resolution, and image classification. Our experiments indicate that adding explicit global or local conditions may yield even better results than generic GANs counterparts for specific tasks without compromising image fidelity. Ultimately, this work opens up opportunities for developing more efficient, transparent, and customizable conditional generators applicable in many domains beyond computer vision.",1
"Although much progress has been made recently in 3D face reconstruction, most previous work has been devoted to predicting accurate and fine-grained 3D shapes. In contrast, relatively little work has focused on generating high-fidelity face textures. Compared with the prosperity of photo-realistic 2D face image generation, high-fidelity 3D face texture generation has yet to be studied. In this paper, we proposed a novel UV map generation model that predicts the UV map from a single face image. The model consists of a UV sampler and a UV generator. By selectively sampling the input face image's pixels and adjusting their relative locations, the UV sampler generates an incomplete UV map that could faithfully reconstruct the original face. Missing textures in the incomplete UV map are further full-filled by the UV generator. The training is based on pseudo ground truth blended by the 3DMM texture and the input face texture, thus weakly supervised. To deal with the artifacts in the imperfect pseudo UV map, multiple partial UV map discriminators are leveraged.",0
"In this work we present a novel weakly-supervised approach for generating photo-realistic textures for 3D face reconstructions from single images. Our method leverages adversarial training to learn mappings from low resolution geometry and high quality texture maps of real faces onto their corresponding 3D models. This allows us to generate new, detailed photo-realistic textures without requiring any additional annotations beyond those used during initial supervision. Experimental results show that our technique generates highly plausible and visually convincing results while outperforming alternative methods both quantitatively and qualitatively. Moreover, as we demonstrate on several datasets, our framework can even generalize across subjects by utilizing only a few labelled examples, making it particularly suitable for applications where annotating large amounts of data might not be feasible. Overall, our findings pave the way towards enabling robust 3D facial analysis systems through more accurate capture and synthesis of facial appearance.",1
"Recent advances in image inpainting have shown impressive results for generating plausible visual details on rather simple backgrounds. However, for complex scenes, it is still challenging to restore reasonable contents as the contextual information within the missing regions tends to be ambiguous. To tackle this problem, we introduce pretext tasks that are semantically meaningful to estimating the missing contents. In particular, we perform knowledge distillation on pretext models and adapt the features to image inpainting. The learned semantic priors ought to be partially invariant between the high-level pretext task and low-level image inpainting, which not only help to understand the global context but also provide structural guidance for the restoration of local textures. Based on the semantic priors, we further propose a context-aware image inpainting model, which adaptively integrates global semantics and local features in a unified image generator. The semantic learner and the image generator are trained in an end-to-end manner. We name the model SPL to highlight its ability to learn and leverage semantic priors. It achieves the state of the art on Places2, CelebA, and Paris StreetView datasets.",0
"In recent years, image inpainting has emerged as a significant topic within computer vision research. This study presents a novel approach to context-aware image inpainting that leverages learned semantic priors to generate high-quality results. By incorporating contextual information into traditional inpainting techniques, our method can effectively restore missing regions while preserving global coherence and local details. Our system learns these semantic prior models from vast amounts of unlabelled data by explicitly modelling and predicting feature dependencies at multiple scales. We demonstrate the effectiveness of our approach through comprehensive experiments on challenging benchmark datasets, comparing favourably against state-of-the-art methods. Overall, this work represents a significant step forward towards more accurate and realistic image completion using deep learning techniques.",1
"Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.",0
"Recent advances have shown that diffusion models can generate high quality images and text conditioned on few examples. However most state-of-the art approaches suffer from overfitting artifacts caused by their strong regularization. We present a novel model called ""Diffusion Denoising Model"" which is trained explicitly using only unpaired data (i.e., no paired training data), allowing us to easily transfer knowledge across tasks without fine-tuning, resulting in improved performance compared to prior work. Our approach improves upon previous methods in terms of both visual fidelity as well as numerical measures such as Frechet Inception Distance. We perform thorough experiments comparing against other diffusion denoising models and traditional generative adversarial network based systems. Code for reproducibility is available at https://github.com/facebookresearch/ImageNetConvBloom.  ----",1
"Virtual try-on methods aim to generate images of fashion models wearing arbitrary combinations of garments. This is a challenging task because the generated image must appear realistic and accurately display the interaction between garments. Prior works produce images that are filled with artifacts and fail to capture important visual details necessary for commercial applications. We propose Outfit Visualization Net (OVNet) to capture these important details (e.g. buttons, shading, textures, realistic hemlines, and interactions between garments) and produce high quality multiple-garment virtual try-on images. OVNet consists of 1) a semantic layout generator and 2) an image generation pipeline using multiple coordinated warps. We train the warper to output multiple warps using a cascade loss, which refines each successive warp to focus on poorly generated regions of a previous warp and yields consistent improvements in detail. In addition, we introduce a method for matching outfits with the most suitable model and produce significant improvements for both our and other previous try-on methods. Through quantitative and qualitative analysis, we demonstrate our method generates substantially higher-quality studio images compared to prior works for multi-garment outfits. An interactive interface powered by this method has been deployed on fashion e-commerce websites and received overwhelmingly positive feedback.",0
"In recent years, advancements in computer graphics have allowed virtual try-on (VT) technology to become increasingly prevalent. However, current VT systems often struggle with accurately capturing intricate details such as stitching, seams, and wrinkles that can greatly affect the overall appearance of clothing on individuals. This lack of attention to detail can lead to unsatisfactory user experiences and unrealistic product visualizations.  This study addresses these issues by presenting a novel approach to generating highly detailed and accurate outfit visualizations using generative adversarial networks (GANs). Our method takes into account various physical properties of materials such as transparency, glossiness, and roughness, which are then integrated with light transport simulations to achieve more realistic reflections and shadows. We introduce several new techniques including stitching detection, customized blending models, fine-grained geometry transferring, and local distortion estimation to capture subtle differences among fabrics, patterns, and textures.  To evaluate our proposed method, we conducted extensive experiments on both synthetic and real datasets, showing significant improvements over state-of-the-art VT methods in terms of quantitative metrics and subjective user studies. We believe our work represents an important step towards enhancing the realism and accuracy of outfit visualizations, ultimately leading to greater customer satisfaction and increased sales conversions. Additionally, our framework has the potential to impact other domains where precise material rendering and geometric detail preservation are essential, such as product design and architectural visualization. Overall, this research demonstrates the power of combining advanced image generation algorithms with domain knowledge to solve challenges in specific application areas.",1
"Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases,such as image generation, OOD detection, and compositional generation.",0
"Title: ""Improved Contrastive Divergence Training of Energy Based Models""  Constructing graphical models that accurately capture real world phenomena can be a challenging task. In particular, energy based models (EBMs) often struggle to converge during training due to slow mixing rates, high correlations among parameters, and difficulties in exploring the parameter space. One popular approach to address these issues is contrastive divergence (CD), which alternates between Gibbs sampling and Metropolis-Hastings updates. Despite CD’s effectiveness, there remain opportunities for improvement. For instance, existing methods only focus on updating one layer at a time, which may result in suboptimal convergence and poor accuracy. Furthermore, some EBM architectures lack sufficient information to perform effective CD updates, leading to slower performance.  To overcome these limitations, we propose a novel multi-layered version of CD (ML-CD). ML-CD simultaneously updates multiple layers within each iteration, allowing more efficient exploration of the model’s latent variables. We further improve upon standard CD by using different learning rate schedules for each layer, providing additional flexibility in adapting to varying complexity across layers. Our method leverages information from neighboring layers, enabling better gradient estimation and resulting in higher accuracy compared to single-layer approaches.  Empirically, our experiments demonstrate significant improvements over baseline CD on several benchmark datasets, including MNIST, CIFAR-10, and SVHN. Overall, we provide a simple yet powerful extension to CD capable of handling diverse EBM architectures and outperforming current state-of-the-art techniques, opening new possibilities in automating model construction for complex domains. Our code and data are available online, encouraging future research into designing even more effective training schemes for graphical models using insights gained from our work.",1
"Current vision systems are trained on huge datasets, and these datasets come with costs: curation is expensive, they inherit human biases, and there are concerns over privacy and usage rights. To counter these costs, interest has surged in learning from cheaper data sources, such as unlabeled images. In this paper we go a step further and ask if we can do away with real image datasets entirely, instead learning from noise processes. We investigate a suite of image generation models that produce images from simple random processes. These are then used as training data for a visual representation learner with a contrastive loss. We study two types of noise processes, statistical image models and deep generative models under different random initializations. Our findings show that it is important for the noise to capture certain structural properties of real data but that good performance can be achieved even with processes that are far from realistic. We also find that diversity is a key property to learn good representations. Datasets, models, and code are available at https://mbaradad.github.io/learning_with_noise.",0
Learning to see by looking at noise: a study on perceptual learning through exposure to random patterns,1
"Unsupervised domain adaptation for object detection is a challenging problem with many real-world applications. Unfortunately, it has received much less attention than supervised object detection. Models that try to address this task tend to suffer from a shortage of annotated training samples. Moreover, existing methods of feature alignments are not sufficient to learn domain-invariant representations. To address these limitations, we propose a novel augmented feature alignment network (AFAN) which integrates intermediate domain image generation and domain-adversarial training into a unified framework. An intermediate domain image generator is proposed to enhance feature alignments by domain-adversarial training with automatically generated soft domain labels. The synthetic intermediate domain images progressively bridge the domain divergence and augment the annotated source domain training data. A feature pyramid alignment is designed and the corresponding feature discriminator is used to align multi-scale convolutional features of different semantic levels. Last but not least, we introduce a region feature alignment and an instance discriminator to learn domain-invariant features for object proposals. Our approach significantly outperforms the state-of-the-art methods on standard benchmarks for both similar and dissimilar domain adaptations. Further extensive experiments verify the effectiveness of each component and demonstrate that the proposed network can learn domain-invariant representations.",0
"Automatic object detection has become increasingly important as there are many applications in industries like autonomous driving and augmented reality. Current state-of-the art methods use convolutional neural networks (CNNs) which have difficulty detecting objects across different domains due to their high variance compared to humans who are better at generalizing from one domain to another. To address this issue, we propose AFAN: Augmented Feature Alignment Network, where we use generative adversarial networks (GANs) to align features learned by two CNN models trained on disjoint sets. We show that our method can improve performance on cross-domain object detection tasks by generating synthetic data to enhance diversity without manual labeling efforts. Our experiments on six benchmark datasets demonstrate that our approach achieves state-of-the-art results and outperforms previous works by significant margins across all metrics. This study demonstrates that AFAN is a promising tool for improving performance in object detection tasks and advancing technologies in areas such as autonomous driving and augmented reality.",1
"Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple ""views"" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more ""model zoos"" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is released on our project page: https://ali-design.github.io/GenRep/",0
"This paper explores the use of generative models as a data source for multiview representation learning. With the recent advancements in deep learning technology, there has been a surge in interest in developing multiview representations that capture different aspects of complex entities such as images, videos, audio signals, natural language text, etc. In order to achieve this goal, several approaches have been proposed including pretraining on multiple views generated by applying random augmentations/transformations to the original data, self-supervised pretraining using proxy tasks, and multi-task supervision from related but distinct datasets. These methods generally require large amounts of diverse training data which can often be challenging and expensive to acquire.  In contrast, generative models like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs) and others are capable of generating vast quantities of synthetic data that exhibit diverse distributions and characteristics similar to those present in real world data. By leveraging these generative models as a data source, we propose a novel method for multiview representation learning that efficiently captures valuable information from various viewpoints without requiring extensive amounts of labeled data from each individual modality. We evaluate our approach on three common benchmark datasets: CelebA, UTKFace and ImageNet, demonstrating improved performance across all three domains. Our results indicate that utilizing generative models as a data source holds significant potential in multiview representation learning tasks.",1
"The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via multiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an attentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discriminator, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN's output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1 for image-text alignment, compared to three other recent models. XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91.",0
"This paper proposes a novel approach to text-to-image generation that leverages cross-modal contrastive learning. The proposed method uses a siamese network architecture to learn features that capture high-level semantic concepts across both modalities. By enforcing alignment between these features through a contrastive loss function, our model is able to generate images that more accurately represent the underlying semantics of the input text description. We evaluate the performance of our method on several benchmark datasets and demonstrate its effectiveness compared to state-of-the-art approaches. Our results show that cross-modal contrastive learning can improve the quality and fidelity of generated images, particularly for complex and diverse visual scenes. Overall, our work represents an important contribution towards enabling machines to effectively interpret and create visual content from natural language descriptions.",1
"As a generic modeling tool, Convolutional Neural Networks (CNNs) have been widely employed in image generation and translation tasks. However, when fed with a flat input, current CNN models may fail to generate vivid results due to the spatially shared convolution kernels. We call it the flatness degradation of CNNs. Unfortunately, such degradation is the greatest obstacles to generate a spatially-variant output from a flat input, which has been barely discussed in the previous literature. To tackle this problem, we propose a model agnostic solution, i.e. Noise Incentive Block (NIB), which serves as a generic plug-in for any CNN generation model. The key idea is to break the flat input condition while keeping the intactness of the original information. Specifically, the NIB perturbs the input data symmetrically with a noise map and reassembles them in the feature domain as driven by the objective function. Extensive experiments show that existing CNN models equipped with NIB survive from the flatness degradation and are able to generate visually better results with richer details in some specific image generation tasks given flat inputs, e.g. semantic image synthesis, data-hidden image generation, and deep neural dithering.",0
"Artificial neural networks (ANN) have achieved tremendous success in recent years across a wide range of applications such as image recognition and natural language processing. However, they remain prone to overfitting due to their complex architectures and large capacity. Regularization methods like L2 regularization can reduce overfitting but may lead to suboptimal solutions. Dropout has emerged as a powerful tool to address overfitting by randomly dropping out neurons during training and evaluation. Despite its effectiveness, dropout suffers from randomness that varies at each iteration during training, which leads to time-consuming optimization processes for hyperparameter tuning. To tackle these challenges, we propose Noise Incentive Block (NIB), a novel regularization method based on noise injection into convolutional blocks without modifying the network architecture or parameters. NIB explicitly encourages CNNs to learn more robust representations against input noises. By doing so, NIB regularizes the network without hurting accuracy, reducing computational cost compared to existing state-of-the-art methods while improving generalizability. Extensive experiments on popular datasets such as CIFAR-10/100, ImageNet demonstrate NIB’s effectiveness both quantitatively and qualitatively. We believe our work sheds light on using noise to improve deep learning models generally and paves the path towards enhancing modern CNNs for better performance and interpretability.",1
"Generating videos predicting the future of a given sequence has been an area of active research in recent years. However, an essential problem remains unsolved: most of the methods require large computational cost and memory usage for training. In this paper, we propose a novel method for generating future prediction videos with less memory usage than the conventional methods. This is a critical stepping stone in the path towards generating videos with high image quality, similar to that of generated images in the latest works in the field of image generation. We achieve high-efficiency by training our method in two stages: (1) image reconstruction to encode video frames into latent variables, and (2) latent variable prediction to generate the future sequence. Our method achieves an efficient compression of video into low-dimensional latent variables by decomposing each frame according to its hierarchical structure. That is, we consider that video can be separated into background and foreground objects, and that each object holds time-varying and time-independent information independently. Our experiments show that the proposed method can efficiently generate future prediction videos, even for complex datasets that cannot be handled by previous methods.",0
"""Training deep neural networks for video generation remains a challenging task due to their high computational cost and sensitivity to hyperparameters. In this study, we propose a novel approach that addresses these issues by using hierarchically structured latent spaces. Our method enables efficient training by reducing the complexity of the optimization problem through disentanglement, leading to improved stability, faster convergence, and more accurate predictions. We demonstrate our model's effectiveness on several benchmark datasets and compare its performance against state-of-the-art methods. Our findings suggest that our framework provides significant advantages over existing approaches, paving the way towards more efficient and effective training for future video generation.""",1
"We applied Deep Learning algorithm known as Generative Adversarial Networks (GANs) to perform solar image-to-image translation. That is, from Solar Dynamics Observatory (SDO)/Helioseismic and Magnetic Imager(HMI) line of sight magnetogram images to SDO/Atmospheric Imaging Assembly(AIA) 0304-{\AA} images. The Ultraviolet(UV)/Extreme Ultraviolet(EUV) observations like the SDO/AIA0304-{\AA} images were only made available to scientists in the late 1990s even though the magenetic field observations like the SDO/HMI have been available since the 1970s. Therefore by leveraging Deep Learning algorithms like GANs we can give scientists access to complete datasets for analysis. For generating high resolution solar images we use the Pix2PixHD and Pix2Pix algorithms. The Pix2PixHD algorithm was specifically designed for high resolution image generation tasks, and the Pix2Pix algorithm is by far the most widely used image to image translation algorithm. For training and testing we used the data for the year 2012, 2013 and 2014. The results show that our deep learning models are capable of generating high resolution(1024 x 1024 pixels) AIA0304 images from HMI magnetograms. Specifically, the pixel-to-pixel Pearson Correlation Coefficient of the images generated by Pix2PixHD and original images is as high as 0.99. The number is 0.962 if Pix2Pix is used to generate images. The results we get for our Pix2PixHD model is better than the results obtained by previous works done by others to generate AIA0304 images. Thus, we can use these models to generate AIA0304 images when the AIA0304 data is not available which can be used for understanding space weather and giving researchers the capability to predict solar events such as Solar Flares and Coronal Mass Ejections. As far as we know, our work is the first attempt to leverage Pix2PixHD algorithm for SDO/HMI to SDO/AIA0304 image-to-image translation.",0
"This paper presents a new method for generating high resolution solar images using generative adversarial networks (GAN). We train our GAN on a dataset of low resolution solar images and use conditional GAN architecture to generate high resolution counterparts. Our method improves upon existing approaches by introducing a novel discriminator network that better captures the underlying structure present in solar images. Experimental results show that our model outperforms state-of-the art baselines in terms of visual fidelity and overall image quality. Furthermore, we demonstrate the applicability of our approach in real world scenarios where high resolution solar imaging is critical, such as space weather forecasting and satellite monitoring.",1
"Our work focuses on unsupervised and generative methods that address the following goals: (a) learning unsupervised generative representations that discover latent factors controlling image semantic attributes, (b) studying how this ability to control attributes formally relates to the issue of latent factor disentanglement, clarifying related but dissimilar concepts that had been confounded in the past, and (c) developing anomaly detection methods that leverage representations learned in (a). For (a), we propose a network architecture that exploits the combination of multiscale generative models with mutual information (MI) maximization. For (b), we derive an analytical result (Lemma 1) that brings clarity to two related but distinct concepts: the ability of generative networks to control semantic attributes of images they generate, resulting from MI maximization, and the ability to disentangle latent space representations, obtained via total correlation minimization. More specifically, we demonstrate that maximizing semantic attribute control encourages disentanglement of latent factors. Using Lemma 1 and adopting MI in our loss function, we then show empirically that, for image generation tasks, the proposed approach exhibits superior performance as measured in the quality and disentanglement trade space, when compared to other state of the art methods, with quality assessed via the Frechet Inception Distance (FID), and disentanglement via mutual information gap. For (c), we design several systems for anomaly detection exploiting representations learned in (a), and demonstrate their performance benefits when compared to state-of-the-art generative and discriminative algorithms. The above contributions in representation learning have potential applications in addressing other important problems in computer vision, such as bias and privacy in AI.",0
"This paper presents a novel approach to uncovering semantic attributes that explain data behavior, and then using those attributes to control generation, discovery anomalies, disentangle representation spaces, transfer knowledge from other domains, perform realistic synthesis, and improve robustness. Our system achieves state-of-the-art results on numerous benchmarks while running efficiently thanks to efficient computation on modern GPU hardware. In summary, our work advances machine learning capabilities by providing new tools to better interpret black box models as well as make more effective use of large datasets, particularly important now as big data becomes increasingly available. We expect these contributions will have a broad impact across many areas of scientific inquiry.",1
"Recently, AutoRegressive (AR) models for the whole image generation empowered by transformers have achieved comparable or even better performance to Generative Adversarial Networks (GANs). Unfortunately, directly applying such AR models to edit/change local image regions, may suffer from the problems of missing global information, slow inference speed, and information leakage of local guidance. To address these limitations, we propose a novel model -- image Local Autoregressive Transformer (iLAT), to better facilitate the locally guided image synthesis. Our iLAT learns the novel local discrete representations, by the newly proposed local autoregressive (LA) transformer of the attention mask and convolution mechanism. Thus iLAT can efficiently synthesize the local image regions by key guidance information. Our iLAT is evaluated on various locally guided image syntheses, such as pose-guided person image synthesis and face editing. Both the quantitative and qualitative results show the efficacy of our model.",0
"""The development of new technologies has revolutionized many industries, including image processing and computer vision. One such technology that has gained popularity recently is deep learning, which has been used successfully on large datasets to develop powerful models for image recognition tasks. However, despite these advancements, there remains room for improvement in terms of efficiency and accuracy. In this study, we present the Image Local Autoregressive Transformer (ILAT), a novel model based on the autoregressive transformer architecture. Our experiments show that ILAT outperforms existing state-of-the-art models in several benchmarks, demonstrating its effectiveness in image local feature generation. Furthermore, our ablation studies indicate that the combination of autoregressive and attention mechanisms contributes significantly to the performance improvements achieved by ILAT. Overall, this work represents a significant step forward in the field of image processing and computer vision.""",1
"Despite the recent popularity of neural network-based solvers for optimal transport (OT), there is no standard quantitative way to evaluate their performance. In this paper, we address this issue for quadratic-cost transport -- specifically, computation of the Wasserstein-2 distance, a commonly-used formulation of optimal transport in machine learning. To overcome the challenge of computing ground truth transport maps between continuous measures needed to assess these solvers, we use input-convex neural networks (ICNN) to construct pairs of measures whose ground truth OT maps can be obtained analytically. This strategy yields pairs of continuous benchmark measures in high-dimensional spaces such as spaces of images. We thoroughly evaluate existing optimal transport solvers using these benchmark measures. Even though these solvers perform well in downstream tasks, many do not faithfully recover optimal transport maps. To investigate the cause of this discrepancy, we further test the solvers in a setting of image generation. Our study reveals crucial limitations of existing solvers and shows that increased OT accuracy does not necessarily correlate to better results downstream.",0
"In recent years, there has been significant interest in developing numerical methods for solving optimal transport problems. Many different approaches have been proposed, including linear programming algorithms based on entropic regularization, iterative schemes that minimize a discrete approximation of the cost functional, and deep learning techniques such as generative adversarial networks (GANs) and variational autoencoders (VAEs). However, comparatively little attention has been paid to continuous solver",1
"Fooling people with highly realistic fake images generated with Deepfake or GANs brings a great social disturbance to our society. Many methods have been proposed to detect fake images, but they are vulnerable to adversarial perturbations -- intentionally designed noises that can lead to the wrong prediction. Existing methods of attacking fake image detectors usually generate adversarial perturbations to perturb almost the entire image. This is redundant and increases the perceptibility of perturbations. In this paper, we propose a novel method to disrupt the fake image detection by determining key pixels to a fake image detector and attacking only the key pixels, which results in the $L_0$ and the $L_2$ norms of adversarial perturbations much less than those of existing works. Experiments on two public datasets with three fake image detectors indicate that our proposed method achieves state-of-the-art performance in both white-box and black-box attacks.",0
"Adversarial examples are inputs designed to fool machine learning models into making incorrect predictions. We present methods for generating adversarial examples that have low perceptual difference from their original images, but still cause state-of-the-art fake image detectors such as RGB adversarial perturbations. Our approach relies on using feature maps generated by a convolutional neural network trained on classification tasks and then applying mathematical operations to generate imperceptibly different adversarial images. Experiment results show that our method outperforms previous work in terms of the effectiveness of creating adversarial examples while maintaining high visual quality.",1
"Controllable person image generation aims to produce realistic human images with desirable attributes (e.g., the given pose, cloth textures or hair style). However, the large spatial misalignment between the source and target images makes the standard architectures for image-to-image translation not suitable for this task. Most of the state-of-the-art architectures avoid the alignment step during the generation, which causes many artifacts, especially for person images with complex textures. To solve this problem, we introduce a novel Spatially-Adaptive Warped Normalization (SAWN), which integrates a learned flow-field to warp modulation parameters. This allows us to align person spatial-adaptive styles with pose features efficiently. Moreover, we propose a novel self-training part replacement strategy to refine the pretrained model for the texture-transfer task, significantly improving the quality of the generated cloth and the preservation ability of irrelevant regions. Our experimental results on the widely used DeepFashion dataset demonstrate a significant improvement of the proposed method over the state-of-the-art methods on both pose-transfer and texture-transfer tasks. The source code is available at https://github.com/zhangqianhui/Sawn.",0
"Here is my attempt: This paper presents a new approach to controllable person image synthesis that utilizes spatially-adaptive warped normalization. Our method enables the generation of realistic images by aligning feature representations across different domains while preserving local variations. We introduce two key components: global style control for the entire image, which allows for efficient optimization; and spatially adaptive domain alignment that achieves more coherent local features within generated images. Extensive experiments demonstrate the effectiveness of our proposed technique over state-of-the-art methods on several datasets, especially when fine-grained manipulation is desired. Let me know if you want any changes!",1
"In this paper, we propose Multiresolution Graph Networks (MGN) and Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate graphs in a multiresolution and equivariant manner. At each resolution level, MGN employs higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution. MGVAE constructs a hierarchical generative model based on MGN to variationally autoencode the hierarchy of coarsened graphs. Our proposed framework is end-to-end permutation equivariant with respect to node ordering. Our methods have been successful with several generative tasks including link prediction on citation graphs, unsupervised molecular representation learning to predict molecular properties, molecular generation, general graph generation and graph-based image generation.",0
"This paper presents a novel approach for unsupervised learning of graph representations using a multiresolution graph variational autoencoder (GVar). GVar leverages recent advances in convolutional neural networks for graphs by incorporating them into the encoder and decoder components of the VAE framework. By doing so, we aim to capture both global and local patterns present in the input data through the use of hierarchical representations at different resolutions. Our experimental results on several benchmark datasets demonstrate that our method can outperform state-of-the-art methods in terms of reconstruction accuracy and model fitness. Additionally, we showcase that learned representation exhibit desirable characteristics such as disentanglement, interpretability and generalization across tasks. Finally, we believe that our work provides new insights into the application of graph deep learning techniques for representing complex systems in fields such as bioinformatics, social network analysis, and knowledge representation.",1
"The objective of person re-identification (re-ID) is to retrieve a person's images from an image gallery, given a single instance of the person of interest. Despite several advancements, learning discriminative identity-sensitive and viewpoint invariant features for robust Person Re-identification is a major challenge owing to the large pose variation of humans. This paper proposes a re-ID pipeline that utilizes the image generation capability of Generative Adversarial Networks combined with pose clustering and feature fusion to achieve pose invariant feature learning. The objective is to model a given person under different viewpoints and large pose changes and extract the most discriminative features from all the appearances. The pose transformational GAN (pt-GAN) module is trained to generate a person's image in any given pose. In order to identify the most significant poses for discriminative feature extraction, a Pose Clustering module is proposed. The given instance of the person is modelled in varying poses and these features are effectively combined through the Feature Fusion Network. The final re-ID model consisting of these 3 sub-blocks, alleviates the pose dependence in person re-ID. Also, The proposed model is robust to occlusion, scale, rotation and illumination, providing a framework for viewpoint invariant feature learning. The proposed method outperforms the state-of-the-art GAN based models in 4 benchmark datasets. It also surpasses the state-of-the-art models that report higher re-ID accuracy in terms of improvement over baseline.",0
"Effectively identifying individuals across different camera views remains a challenging task in computer vision. Current methods rely on pose estimation techniques that often fail under real world conditions, resulting in low accuracy person re-identification (ReID) systems. To address this challenge, we propose a novel method leveraging generative adversarial networks (GANs). Our approach incorporates both synthetic data generation and robust feature learning, enabling accurate pose invariant person re-identification even without precise pose estimates. Experiments conducted on multiple datasets demonstrate that our proposed framework outperforms state-of-the-art methods by significant margins, paving the way towards more advanced and reliable video surveillance applications.",1
"Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at 67%-74% sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator play a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.",0
"Artificial intelligence has seen significant advancements in recent years, particularly in the field of Generative Adversarial Networks (GANs). These networks have proven to be highly effective at generating realistic images, audio clips, and even text from scratch, making them increasingly popular among researchers and practitioners alike. However, little attention has been paid to their potential applications beyond creative tasks such as image generation and data augmentation. In this paper, we explore the use of GANs for playing lottery tickets - that is, predicting which numbers are most likely to win based on historical patterns. Our approach leverages a combination of deep learning techniques and domain knowledge to develop a system capable of generating high-quality predictions for various lotteries. We evaluate our model using standard performance metrics such as accuracy, precision, recall, and F1 score, demonstrating its effectiveness compared to traditional methods. Overall, our work highlights the versatility of GANs and their ability to perform well across a range of domains.",1
"Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.",0
"This paper proposes a novel approach to visual attention that uses two linear layers instead of self-attention mechanisms found in traditional transformer models. Our proposed method achieves superior performance on challenging benchmark datasets such as ImageNet while maintaining computational efficiency. The main contribution lies in our use of external attention which allows us to focus selectively on specific parts of input features instead of relying solely on global context. We demonstrate through extensive experiments that external attention can improve the accuracy and robustness of deep learning models by explicitly attending to relevant regions. Furthermore, we show that our approach surpasses state-of-the art alternatives, validating the effectiveness of our design choices.",1
"Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView (zero-shot) achieves a new state-of-the-art FID on blurred MS COCO, outperforms previous GAN-based models and a recent similar work DALL-E.",0
"Abstract In recent years, textual descriptions of images have been used to train deep neural networks (DNNs) that can generate highly realistic, pixel-perfect images corresponding to those descriptors. These so-called “Generative Adversarial Network” (GAN) models require large amounts of training data and computational resources, making them challenging to deploy on edge devices or web browsers. In this work, we present CogView, a new method that leverages transformer architectures, which have recently revolutionized natural language processing tasks such as machine translation and question answering, to achieve state-of-the-art image generation from text. Specifically, we show how to pretrain a transformer network using unpaired text-image examples found online, fine-tune it on paired examples derived from object detection datasets, and use it directly for generating novel images based solely on prompted text inputs at test time. We demonstrate the effectiveness of our approach across several benchmark tasks, including scene synthesis, portrait drawing, and image completion, consistently outperforming competitive GAN baselines on both quantitative metrics and human judgments. Our experiments further reveal intriguing connections between human visual imagination processes and learned representations in vision transformers trained under these settings. By pushing the limits of modern text2img generation techniques, our study paves the way towards exciting applications ranging from content creation tools to AI-powered art systems.",1
"We present a Python-based renderer built on NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, designed to generate high-quality synthetic images for research in computer vision and deep learning. Our tool enables the description and manipulation of complex dynamic 3D scenes containing object meshes, materials, textures, lighting, volumetric data (e.g., smoke), and backgrounds. Metadata, such as 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors, can also be generated. In this work, we discuss design goals, architecture, and performance. We demonstrate the use of data generated by path tracing for training an object detector and pose estimator, showing improved performance in sim-to-real transfer in situations that are difficult for traditional raster-based renderers. We offer this tool as an easy-to-use, performant, high-quality renderer for advancing research in synthetic data generation and deep learning.",0
"NViSII (Networked Video Surveillance II) is a software tool that enables users to create photorealistic images from video data using advanced image generation algorithms. The tool features scripting capabilities, allowing users to automate complex workflows and easily build custom imagery pipelines. With support for real-time rendering, NViSII can generate images at high frame rates, making it suitable for use in applications such as robotics and computer vision. Additionally, the software includes built-in machine learning models that allow it to automatically select appropriate algorithms based on input content and output requirements. Overall, NViSII provides researchers and developers with a powerful yet user-friendly platform for generating highly detailed and accurate synthetic visualizations.",1
"Generative adversarial network (GAN) has become one of the most important neural network models for classical unsupervised machine learning. A variety of discriminator loss functions have been developed to train GAN's discriminators and they all have a common structure: a sum of real and fake losses that only depends on the actual and generated data respectively. One challenge associated with an equally weighted sum of two losses is that the training may benefit one loss but harm the other, which we show causes instability and mode collapse. In this paper, we introduce a new family of discriminator loss functions that adopts a weighted sum of real and fake parts, which we call adaptive weighted loss functions or aw-loss functions. Using the gradients of the real and fake parts of the loss, we can adaptively choose weights to train a discriminator in the direction that benefits the GAN's stability. Our method can be potentially applied to any discriminator model with a loss that is a sum of the real and fake parts. Experiments validated the effectiveness of our loss functions on an unconditional image generation task, improving the baseline results by a significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores and FID.",0
"Generative adversarial networks (GANs) have emerged as a powerful tool for generating realistic images, but training them can be challenging due to instability issues such as mode collapse and poor generalization performance. In recent years, many efforts have been made to improve GAN stability by modifying the loss function used during training. One popular approach involves using weighted losses that assign higher weights to underfitted regions and lower weights to overfitted ones. However, these approaches tend to rely on manual selection of hyperparameters and may suffer from slow convergence or suboptimal results. To address these limitations, we propose a novel adaptive weight discriminator method that dynamically adjusts the importance maps at runtime based on the learned features of both generator and discriminator. Our method shows state-of-the-art results across multiple benchmark datasets while significantly reducing the computational cost compared to previous methods. Further analysis reveals that our algorithm leads to more stable and diverse solutions, providing insights into how generators might learn better representations. Overall, this work provides a simple yet effective solution for improving GANs training stability and performance through dynamic adaptation.",1
"Existing vision-based action recognition is susceptible to occlusion and appearance variations, while wearable sensors can alleviate these challenges by capturing human motion with one-dimensional time-series signal. For the same action, the knowledge learned from vision sensors and wearable sensors, may be related and complementary. However, there exists significantly large modality difference between action data captured by wearable-sensor and vision-sensor in data dimension, data distribution and inherent information content. In this paper, we propose a novel framework, named Semantics-aware Adaptive Knowledge Distillation Networks (SAKDN), to enhance action recognition in vision-sensor modality (videos) by adaptively transferring and distilling the knowledge from multiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher modalities and uses RGB videos as student modality. To preserve local temporal relationship and facilitate employing visual deep learning model, we transform one-dimensional time-series signals of wearable sensors to two-dimensional images by designing a gramian angular field based virtual image generation model. Then, we build a novel Similarity-Preserving Adaptive Multi-modal Fusion Module to adaptively fuse intermediate representation knowledge from different teacher networks. Finally, to fully exploit and transfer the knowledge of multiple well-trained teacher networks to the student network, we propose a novel Graph-guided Semantically Discriminative Mapping loss, which utilizes graph-guided ablation analysis to produce a good visual explanation highlighting the important regions across modalities and concurrently preserving the interrelations of original data. Experimental results on Berkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness of our proposed SAKDN.",0
"This paper presents Semantics-Aware Adaptive Knowledge Distillation (SAKD) method that enhances sensor-to-vision action recognition performance by incorporating semantic knowledge from textual descriptions into the process of distilling knowledge across different deep models. We propose an adaptive approach that selectively adopts distilled features according to their relevance based on input modality. Our model effectively integrates heterogeneous knowledge sources while alleviating negative transfer. Extensive experiments show SAKD significantly improves upon baseline methods for both vision-alone and multimodal settings on challenging benchmark datasets, establishing state-of-the-art results.",1
"Deep learning models are widely used for image analysis. While they offer high performance in terms of accuracy, people are concerned about if these models inappropriately make inferences using irrelevant features that are not encoded from the target object in a given image. To address the concern, we propose a metamorphic testing approach that assesses if a given inference is made based on irrelevant features. Specifically, we propose two novel metamorphic relations to detect such inappropriate inferences. We applied our approach to 10 image classification models and 10 object detection models, with three large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the top-5 correct predictions made by the image classification models are subject to inappropriate inferences using irrelevant features. The corresponding rate for the object detection models is over 8.5%. Based on the findings, we further designed a new image generation strategy that can effectively attack existing models. Comparing with a baseline approach, our strategy can double the success rate of attacks.",0
"This paper presents a new methodology for testing deep learning models used for image analysis by utilizing object-relevant metamorphic relations (ORMRs). ORMRs are relationships that capture how objects appear in different conditions, such as changes in lighting, viewpoint, occlusions, etc., while preserving their identities. We propose using these relationships to create synthetic training data that can augment existing datasets, which in turn improves model robustness and generalization capabilities. To demonstrate the effectiveness of our approach, we conducted experiments on popular benchmark datasets and compared results against state-of-the-art methods. Results showed significant improvements in both accuracy and robustness, highlighting the potential benefits of leveraging ORMRs for enhancing deep learning models applied to image analysis tasks. Our work offers a promising direction for further research into model evaluation techniques tailored specifically for object recognition problems.",1
"Malicious application of deepfakes (i.e., technologies can generate target faces or face attributes) has posed a huge threat to our society. The fake multimedia content generated by deepfake models can harm the reputation and even threaten the property of the person who has been impersonated. Fortunately, the adversarial watermark could be used for combating deepfake models, leading them to generate distorted images. The existing methods require an individual training process for every facial image, to generate the adversarial watermark against a specific deepfake model, which are extremely inefficient. To address this problem, we propose a universal adversarial attack method on deepfake models, to generate a Cross-Model Universal Adversarial Watermark (CMUA-Watermark) that can protect thousands of facial images from multiple deepfake models. Specifically, we first propose a cross-model universal attack pipeline by attacking multiple deepfake models and combining gradients from these models iteratively. Then we introduce a batch-based method to alleviate the conflict of adversarial watermarks generated by different facial images. Finally, we design a more reasonable and comprehensive evaluation method for evaluating the effectiveness of the adversarial watermark. Experimental results demonstrate that the proposed CMUA-Watermark can effectively distort the fake facial images generated by deepfake models and successfully protect facial images from deepfakes in real scenes.",0
"Deepfake technology has posed significant challenges for society as it enables individuals to manipulate video content by generating images that are difficult to distinguish from real ones, leading to potential damages ranging from private embarrassment to political instability. In this work, we introduce CMUA-Watermark, a cross-model universal adversarial watermarking approach that embeds robust, imperceptible, and unique fingerprints into deepfake videos generated by different models. We demonstrate the effectiveness of our method on four popular deepfake generation architectures across two distinct tasks - face swapping and talking head replication. Our experiments reveal consistent high detection accuracies, even under strong attacks like fine-grained cropping and compression, proving the efficiency of our method against state-of-the-art attacks. Furthermore, we propose a novel algorithm that generates synthetic training data labeled with automatically inferred ground truth from publicly available deepfake datasets, enabling effective adaptation to new domains. Overall, our findings contribute significantly towards combating malicious usage of deepfake technology while fostering open research through shared resources.",1
"Noise injection has been proved to be one of the key technique advances in generating high-fidelity images. Despite its successful usage in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on the geodesic normal coordinates. Guided by our theories, we find that the existing method is incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.",0
"Artificial intelligence (AI) has been shown to improve performance on a wide range of tasks by injecting noise into generative adversarial networks (GANs). This noise injection can significantly increase stability during training and reduce overfitting. However, there remains limited understanding of how different types of noise affect GAN performance. Here we present experimental results showing that adding Gaussian noise to both generator and discriminator inputs can lead to improved accuracy and robustness compared to other forms of noise. We further demonstrate that our method generalizes across multiple datasets, architectures, and noise distributions. These findings contribute important insights towards improving the reliability of GANs in AI applications where high fidelity data generation is critical. Overall, our work emphasizes the need for careful evaluation of GAN design choices, as well as the importance of considering the properties of specific datasets and use cases.",1
"Generative adversarial networks (GANs) are a class of generative models with two antagonistic neural networks: a generator and a discriminator. These two neural networks compete against each other through an adversarial process that can be modeled as a stochastic Nash equilibrium problem. Since the associated training process is challenging, it is fundamental to design reliable algorithms to compute an equilibrium. In this paper, we propose a stochastic relaxed forward-backward (SRFB) algorithm for GANs and we show convergence to an exact solution when an increasing number of data is available. We also show convergence of an averaged variant of the SRFB algorithm to a neighborhood of the solution when only few samples are available. In both cases, convergence is guaranteed when the pseudogradient mapping of the game is monotone. This assumption is among the weakest known in the literature. Moreover, we apply our algorithm to the image generation problem.",0
"One of the most effective methods for training generative adversarial networks (GAN) involves using the framework of a stochastic Nash game. This approach allows for more stable convergence than traditional GAN methods while still preserving their ability to generate high quality samples. In our work, we propose a novel method for optimizing these stochastic Nash games through the use of a new algorithm called Stochastic Newton Update (SNU). Our experiments demonstrate that SNU outperforms state-of-the-art optimization techniques such as Adam and SGD, resulting in improved performance across multiple benchmark datasets. Overall, our contributions provide valuable insights into the field of training GANs and have important implications for a range of applications including computer vision and natural language processing.",1
"Intrinsic Image Decomposition is an open problem of generating the constituents of an image. Generating reflectance and shading from a single image is a challenging task specifically when there is no ground truth. There is a lack of unsupervised learning approaches for decomposing an image into reflectance and shading using a single image. We propose a neural network architecture capable of this decomposition using physics-based parameters derived from the image. Through experimental results, we show that (a) the proposed methodology outperforms the existing deep learning-based IID techniques and (b) the derived parameters improve the efficacy significantly. We conclude with a closer analysis of the results (numerical and example images) showing several avenues for improvement.",0
"Title: ""Intrinsic Image Decomposition Using an Optics-Inspired Convolutional Neural Network Approach""  The challenging task of separating an object from its background into reflectance and shading components has been addressed using convolutional neural networks (CNNs). Most existing methods use the end-to-end trainable architecture that learns all parameters simultaneously, without taking advantage of prior knowledge on how light interacts with materials. In this work, we propose a novel CNN framework that incorporates an optical model as an integral part of the network to improve the accuracy of intrinsic image decomposition results. We demonstrate that our optics-informed approach outperforms current state-of-the-art methods while requiring fewer parameters and improving interpretability by explicitly enforcing physical properties of surface illumination. Our findings open new opportunities for advanced computer vision applications such as material recognition, visual effects rendering, and robotic manipulation.",1
"Contact pressure between the human body and its surroundings has important implications. For example, it plays a role in comfort, safety, posture, and health. We present a method that infers contact pressure between a human body and a mattress from a depth image. Specifically, we focus on using a depth image from a downward facing camera to infer pressure on a body at rest in bed occluded by bedding, which is directly applicable to the prevention of pressure injuries in healthcare. Our approach involves augmenting a real dataset with synthetic data generated via a soft-body physics simulation of a human body, a mattress, a pressure sensing mat, and a blanket. We introduce a novel deep network that we trained on an augmented dataset and evaluated with real data. The network contains an embedded human body mesh model and uses a white-box model of depth and pressure image generation. Our network successfully infers body pose, outperforming prior work. It also infers contact pressure across a 3D mesh model of the human body, which is a novel capability, and does so in the presence of occlusion from blankets.",0
"This paper presents a novel approach to infer both body pose and contact pressure using only a single depth image as input. Traditionally, these two tasks have been addressed separately and often require additional sensor data such as inertial measurement units (IMUs) or force plates. However, we show that by leveraging recent advances in deep learning, we can perform joint inference on pose and pressure directly from depth images without any additional sensors. Our method utilizes a convolutional neural network architecture designed specifically for this task, which takes raw depth data as input and outputs heatmaps representing contact pressures at each pixel location. We demonstrate the effectiveness of our approach through extensive experiments on synthetic and real world datasets, outperforming state-of-the-art methods in both accuracy and efficiency. With applications ranging from human computer interaction to gait analysis and robotics, our method provides a powerful tool for capturing rich physical interactions in the real world.",1
"There has been a rise in the use of Machine Learning as a Service (MLaaS) Vision APIs as they offer multiple services including pre-built models and algorithms, which otherwise take a huge amount of resources if built from scratch. As these APIs get deployed for high-stakes applications, it's very important that they are robust to different manipulations. Recent works have only focused on typical adversarial attacks when evaluating the robustness of vision APIs. We propose two new aspects of adversarial image generation methods and evaluate them on the robustness of Google Cloud Vision API's optical character recognition service and object detection APIs deployed in real-world settings such as sightengine.com, picpurify.com, Google Cloud Vision API, and Microsoft Azure's Computer Vision API. Specifically, we go beyond the conventional small-noise adversarial attacks and introduce secret embedding and transparent adversarial examples as a simpler way to evaluate robustness. These methods are so straightforward that even non-specialists can craft such attacks. As a result, they pose a serious threat where APIs are used for high-stakes applications. Our transparent adversarial examples successfully evade state-of-the art object detections APIs such as Azure Cloud Vision (attack success rate 52%) and Google Cloud Vision (attack success rate 36%). 90% of the images have a secret embedded text that successfully fools the vision of time-limited humans but is detected by Google Cloud Vision API's optical character recognition. Complementing to current research, our results provide simple but unconventional methods on robustness evaluation.",0
"Adversarial examples have been shown to pose significant challenges for machine learning models, as they can cause these models to make incorrect predictions despite having access to large amounts of training data. In order to better understand and address this problem, researchers have developed techniques that generate adversarial examples in a transparent manner. These methods use simple mathematical operations to create perturbations that cause a model to misclassify images in a controllable and reproducible fashion. By using these approaches, we can gain insights into how different types of inputs affect a model's behavior, and develop strategies for improving their robustness against such attacks. This work has important implications for a wide range of applications where safety and reliability are critical factors, including autonomous vehicles, medical imaging systems, and security cameras. The results presented here provide a foundation for future work aimed at advancing our understanding of adversarial vulnerabilities and developing more secure machine learning algorithms.",1
"The content based image retrieval aims to find the similar images from a large scale dataset against a query image. Generally, the similarity between the representative features of the query image and dataset images is used to rank the images for retrieval. In early days, various hand designed feature descriptors have been investigated based on the visual cues such as color, texture, shape, etc. that represent the images. However, the deep learning has emerged as a dominating alternative of hand-designed feature engineering from a decade. It learns the features automatically from the data. This paper presents a comprehensive survey of deep learning based developments in the past decade for content based image retrieval. The categorization of existing state-of-the-art methods from different perspectives is also performed for greater understanding of the progress. The taxonomy used in this survey covers different supervision, different networks, different descriptor type and different retrieval type. A performance analysis is also performed using the state-of-the-art methods. The insights are also presented for the benefit of the researchers to observe the progress and to make the best choices. The survey presented in this paper will help in further research progress in image retrieval using deep learning.",0
"In the last decade we have witnessed significant advancements in image retrieval techniques due to deep learning methods applied to computer vision problems such as feature extraction from images. Convolutional neural networks (CNNs) have demonstrated state-of-the-art performance for many visual recognition tasks like object detection, image classification, and segmentation. They rely on hierarchical features which capture low level features first then combine them into high level semantic concepts. Many studies focused on enhancing those models' efficiency by using data augmentation and transferring knowledge across similar architectures. Ensemble methods were used in order to obtain better results while reducing computational cost comparing to single large models. Quantization techniques where proposed in order to deploy these large models on resource constrained devices without losing accuracy. These advances opened doors for new applications like realtime content based search on mobile phones through cloud services. Our study aimed at presenting a survey of this fast evolving research area concentrating on the contributions made over the past ten years in terms of methodologies, databases, evaluation metrics, applications and future directions. We highlight how deep learning played a major role towards solving challenges that remained unsolved before by traditional hand crafted features in addition to improving existing approaches. Finally our work provides recommendations on designing future systems including but not limited to: datasets, evaluation protocols and hardware utilisation.",1
"Generative Adversarial Networks (GANs) have made great success in synthesizing high-quality images. However, how to steer the generation process of a well-trained GAN model and customize the output image is much less explored. It has been recently found that modulating the input latent code used in GANs can reasonably alter some variation factors in the output image, but such manipulation usually presents to change the entire image as a whole. In this work, we propose an effective approach, termed as LoGAN, to support local editing of the output image. Concretely, we introduce two operators, i.e., content modulation and style modulation, together with a priority mask to facilitate the precise control of the intermediate generative features. Taking bedroom synthesis as an instance, we are able to seamlessly remove, insert, shift, and rotate the individual objects inside a room. Furthermore, our method can completely clear out a room and then refurnish it with customized furniture and styles. Experimental results show the great potentials of steering the image generation of pre-trained GANs for versatile image editing.",0
"This paper presents a new method for generating images of bedrooms based on user input using generative adversarial networks (GANs). Our approach allows users to locally control different aspects of the generated image, such as color schemes or furniture arrangements, by adjusting sliders or providing specific requests. We show that our method can generate high-quality images that accurately reflect the desired changes made by the user, while still maintaining coherence within the overall scene. Additionally, we demonstrate the effectiveness of our model through quantitative evaluations and visual comparisons to other state-of-the-art methods for controllable GANs.  This work has applications in fields such as interior design and virtual reality environments, where realistic and customizable graphics are important factors. By enabling users to create personalized scenes based on their own preferences, our method provides a unique tool for creativity and expression, making it relevant to anyone interested in digital art and imagery. Moreover, our contributions towards improving local controls in GAN models have potential impact beyond our current application domain, paving the way for future research into better controllability of deep learning systems in general.",1
"Image-to-image (I2I) translation has matured in recent years and is able to generate high-quality realistic images. However, despite current success, it still faces important challenges when applied to small domains. Existing methods use transfer learning for I2I translation, but they still require the learning of millions of parameters from scratch. This drawback severely limits its application on small domains. In this paper, we propose a new transfer learning for I2I translation (TransferI2I). We decouple our learning process into the image generation step and the I2I translation step. In the first step we propose two novel techniques: source-target initialization and self-initialization of the adaptor layer. The former finetunes the pretrained generative model (e.g., StyleGAN) on source and target data. The latter allows to initialize all non-pretrained network parameters without the need of any data. These techniques provide a better initialization for the I2I translation step. In addition, we introduce an auxiliary GAN that further facilitates the training of deep I2I systems even from small datasets. In extensive experiments on three datasets, (Animal faces, Birds, and Foods), we show that we outperform existing methods and that mFID improves on several datasets with over 25 points.",0
"In this paper we present a method that can take an image (or other data) as input and generate an output that has similar properties but different details than the original. For example if given an image of a cat we could produce one of a dog. We use transfer learning techniques to improve generalization over previous approaches by using large amounts of pre-existing labelled image data which captures objects at many scales and shapes. Our method uses a combination of generative adversarial networks (GANs), cycle consistency loss, identity mappings, VAEs with learned latent priors, and discriminator feedback loops to enable training on small datasets. This allows us to train high quality models with modest computational resources. We compare our approach against several baselines across four challenging tasks such as (semantic, holistic, seasonal and style changes), where our model obtains significantly better results both qualitatively and quantitatively while running orders of magnitude faster during training. We demonstrate wide applicability of our approach via additional visual examples and three user studies.",1
"Image content is a predominant factor in marketing campaigns, websites and banners. Today, marketers and designers spend considerable time and money in generating such professional quality content. We take a step towards simplifying this process using Generative Adversarial Networks (GANs). We propose a simple and novel conditioning strategy which allows generation of images conditioned on given semantic attributes using a generator trained for an unconditional image generation task. Our approach is based on modifying latent vectors, using directional vectors of relevant semantic attributes in latent space. Our method is designed to work with both discrete (binary and multi-class) and continuous image attributes. We show the applicability of our proposed approach, named Directional GAN, on multiple public datasets, with an average accuracy of 86.4% across different attributes.",0
"Advances in generative adversarial networks (GAN) have enabled significant strides in realistically generating images from scratch, however there remains room for improvement in terms of conditioning these models towards specific high level concepts such as object classes, attributes, or textual prompts. In this work we propose a novel conditional loss term that guides the generator towards its intended target concept while maintaining the ability to synthesize new examples by only using a simple classification network. We show through extensive experiments on CIFAR10, LSUN, and ImageNet datasets our method leads to large improvements over previous state of the art methods while simultaneously reducing the computational cost during training. Finally, qualitative results demonstrate how the proposed directional constraint improves control over generated samples, making them more meaningful. We believe this research will open up exciting future directions in controllable image generation.",1
"Blind face restoration (BFR) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (DNN) usually cannot lead to acceptable results. Existing generative adversarial network (GAN) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to ensure that the latent code and noise input to the GAN can be respectively generated from the deep and shallow features of the DNN, controlling the global face structure, local face details and background of the reconstructed image. The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed GPEN achieves significantly superior results to state-of-the-art BFR methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/GPEN.",0
"Artificial intelligence has revolutionized our understanding of many fields through breakthroughs in machine learning techniques such as Generative Adversarial Networks (GAN). In recent years, these methods have been applied to image generation tasks which allow us to restore corrupted images into high quality ones while keeping their original content intact. This study explores one potential application area of this technique: face restoration from images with occlusions like glasses, scarfs, etc. Despite previous efforts in blind face recognition, occlusions can drastically reduce the accuracy of existing algorithms. Our proposed model leverages prior knowledge learned by a pre-trained network on large scale datasets, without additional fine tuning. We demonstrate that such embeddings improve performance over baseline models and achieve state-of-the-art results on standard benchmark datasets. Additionally, we show the generalizability of our method by testing on real world scenarios where subjects wear different accessories compared to those present during training. Overall, this work emphasizes the potential benefits of using already trained GANs to enhance human recognition systems even beyond their original scope.",1
"Face recognition is a popular and well-studied area with wide applications in our society. However, racial bias had been proven to be inherent in most State Of The Art (SOTA) face recognition systems. Many investigative studies on face recognition algorithms have reported higher false positive rates of African subjects cohorts than the other cohorts. Lack of large-scale African face image databases in public domain is one of the main restrictions in studying the racial bias problem of face recognition. To this end, we collect a face image database namely CASIA-Face-Africa which contains 38,546 images of 1,183 African subjects. Multi-spectral cameras are utilized to capture the face images under various illumination settings. Demographic attributes and facial expressions of the subjects are also carefully recorded. For landmark detection, each face image in the database is manually labeled with 68 facial keypoints. A group of evaluation protocols are constructed according to different applications, tasks, partitions and scenarios. The performances of SOTA face recognition algorithms without re-training are reported as baselines. The proposed database along with its face landmark annotations, evaluation protocols and preliminary results form a good benchmark to study the essential aspects of face biometrics for African subjects, especially face image preprocessing, face feature analysis and matching, facial expression recognition, sex/age estimation, ethnic classification, face image generation, etc. The database can be downloaded from our http://www.cripacsir.cn/dataset/",0
"This paper presents CASIA-Face-Africa (CFA), a large-scale African face image database containing over 24,000 images from 689 individuals across six ethnic groups, namely Bamoun, Duala, Krio, Igbo, Wolof, and Yoruba. To address the lack of diversity in existing face databases, CFA was designed to provide a more diverse representation of African faces that can be used by researchers developing facial analysis systems. The dataset includes both constrained and unconstrained images obtained through social media mining, as well as controlled acquisition sessions conducted at the University of Lagos. In addition to demographic attributes such as age, gender, and ethnicity, each subject has been carefully annotated according to their presence in different poses, illuminations, expressions, accessories, occlusions, and disguises. All images have been manually checked for quality control purposes. Experiments on three benchmark datasets demonstrate the effectiveness of CASIA-Face-Africa in improving performance in challenging real-world scenarios. We anticipate that our database will serve as a valuable resource for advancing research into issues related to human appearance, behavior, and interaction in diverse settings.",1
"Generative Adversarial Networks have recently shown promise for video generation, building off of the success of image generation while also addressing a new challenge: time. Although time was analyzed in some early work, the literature has not adequately grown with temporal modeling developments. We study the effects of Neural Differential Equations to model the temporal dynamics of video generation. The paradigm of Neural Differential Equations presents many theoretical strengths including the first continuous representation of time within video generation. In order to address the effects of Neural Differential Equations, we investigate how changes in temporal models affect generated video quality. Our results give support to the usage of Neural Differential Equations as a simple replacement for older temporal generators. While keeping run times similar and decreasing parameter count, we produce a new state-of-the-art model in 64$\times$64 pixel unconditional video generation, with an Inception Score of 15.20.",0
"This is your paper abstract: Latent neural differential equations (LNDEs) provide a powerful framework for modeling high-dimensional systems. They offer a way to represent complex relationships using simple mathematical models that can capture important aspects of time dynamics, including nonlinearities, uncertainties, noise, memory effects, and feedback loops. By learning these LNDEs from data, one can gain insights into how systems operate, predict their behavior under new conditions, generate novel system trajectories, control them towards desired outcomes, test hypotheses about underlying mechanisms, design interventions for improving performance, and more. Applying these methods to video generation produces detailed and realistic visualizations of dynamic scenes by modeling the motion of objects as latent functions of time rather than frame-by-frame predictions, allowing the model to handle interactions among objects more effectively. Overall, LNDEs have broad applicability across many domains, including science, engineering, economics, psychology, sociology, and beyond.",1
"We propose PD-GAN, a probabilistic diverse GAN for image inpainting. Given an input image with arbitrary hole regions, PD-GAN produces multiple inpainting results with diverse and visually realistic content. Our PD-GAN is built upon a vanilla GAN which generates images based on random noise. During image generation, we modulate deep features of input random noise from coarse-to-fine by injecting an initially restored image and the hole regions in multiple scales. We argue that during hole filling, the pixels near the hole boundary should be more deterministic (i.e., with higher probability trusting the context and initially restored image to create natural inpainting boundary), while those pixels lie in the center of the hole should enjoy more degrees of freedom (i.e., more likely to depend on the random noise for enhancing diversity). To this end, we propose spatially probabilistic diversity normalization (SPDNorm) inside the modulation to model the probability of generating a pixel conditioned on the context information. SPDNorm dynamically balances the realism and diversity inside the hole region, making the generated content more diverse towards the hole center and resemble neighboring image content more towards the hole boundary. Meanwhile, we propose a perceptual diversity loss to further empower PD-GAN for diverse content generation. Experiments on benchmark datasets including CelebA-HQ, Places2 and Paris Street View indicate that PD-GAN is effective for diverse and visually realistic image restoration.",0
"This paper presents a novel approach to image inpainting using Generative Adversarial Networks (GANs). Traditional GANs have shown limitations in generating high quality images due to mode collapse and lack of diversity. To address these issues, we propose a probabilistic diverse GAN (PD-GAN) that incorporates Bayesian inference and a new loss function to encourage diverse and coherent generation across multiple modes. Our model outperforms state-of-the-art methods on several benchmark datasets, achieving better visual fidelity, structural similarity, and perceptual realism. We demonstrate the effectiveness of our method through qualitative and quantitative evaluations. Furthermore, we showcase applications of our approach to other domains such as video inpainting and style transfer. Overall, this work represents a significant advancement in generative image synthesis and has implications for fields ranging from computer graphics to autonomous systems.",1
"Many applications of deep learning for image generation use perceptual losses for either training or fine-tuning of the generator networks. The use of perceptual loss however incurs repeated forward-backward passes in a large image classification network as well as a considerable memory overhead required to store the activations of this network. It is therefore desirable or sometimes even critical to get rid of these overheads.   In this work, we propose a way to train generator networks using approximations of perceptual loss that are computed without forward-backward passes. Instead, we use a simpler perceptual gradient network that directly synthesizes the gradient field of a perceptual loss. We introduce the concept of proxy targets, which stabilize the predicted gradient, meaning that learning with it does not lead to divergence or oscillations. In addition, our method allows interpretation of the predicted gradient, providing insight into the internals of perceptual loss and suggesting potential ways to improve it in future work.",0
"In this work we introduce perceptual gradient networks (PGN) which combine two established approaches: perception distortion models like deep feature perturbation methods that measure perceptual similarity, along with generative adversarial training to make the perturbations realistically correspond to different physical factors affecting image quality (e.g., noise, blur). PGN enable more efficient evaluation of perceptual loss using direct comparison rather than expensive user studies; they provide sharper visualizations of what specific distortions look like; they allow controllably generating novel, higher fidelity examples corresponding to given distortions by conditioning on clean images or features. We demonstrate several applications, including improving the accuracy and interpretability of perceptual similarity metrics; facilitating comparisons across architectures and datasets without requiring access to held-out sets; providing insight into how models encode semantic information versus low-level properties of images; allowing users to explore important factors influencing model performance beyond just their magnitude. On multiple benchmark tasks and architectures we achieve new state-of-the-art results while reducing computational requirements compared to past works. These contributions establish PGN as a flexible toolkit towards advancing research on computer vision and machine learning through evaluating perceptual behavior. By exploring semantically meaningful tradeoffs between high fidelity data generation and efficient use of compute resources, our work opens up intriguing possibilities for practitioners interested in bridging the gap between human perception and machine predictions to better serve diverse downstream objectives such as creating virtual reality content or developing automotive safety systems.",1
"In this paper, we focus on deep clustering and unsupervised representation learning for images. Recent advances in deep clustering and unsupervised representation learning are based on the idea that different views of an input image (generated through data augmentation techniques) must be closer in the representation space (exemplar consistency), and/or similar images have a similar cluster assignment (population consistency). We define an additional notion of consistency, consensus consistency, which ensures that representations are learnt to induce similar partitions for variations in the representation space, different clustering algorithms or different initializations of a clustering algorithm. We define a clustering loss by performing variations in the representation space and seamlessly integrate all three consistencies (consensus, exemplar and population) into an end-to-end learning framework. The proposed algorithm, Consensus Clustering using Unsupervised Representation Learning (ConCURL) improves the clustering performance over state-of-the art methods on four out of five image datasets. Further, we extend the evaluation procedure for clustering to reflect the challenges in real world clustering tasks, such as clustering performance in the case of distribution shift. We also perform a detailed ablation study for a deeper understanding of the algorithm.",0
"Title: ""Representation Learning for Cluster Analysis via Model Ensembling"" Authors: Guanlei Chen (A*STAR Institute for Infocomm Research), Yiming Yang (The Hong Kong University of Science and Technology) This paper presents a novel approach to representation learning for cluster analysis that involves building consensus among multiple base models. In traditional clustering algorithms, a single model is used to represent data points and determine their group membership. However, these methods can suffer from poor performance if the underlying assumptions of the algorithm do not hold true for the given dataset. By contrast, our proposed method utilizes ensembles of base models to learn a more robust and accurate representation of the data. This ensemble is then employed to perform clustering on the resulting representations. Our experimental results demonstrate that our method outperforms state-of-the art alternatives across a wide range of datasets, including image, text, and sensor datasets. These findings suggest that by leveraging the diversity inherent in multiple base models, we can achieve improved clustering accuracy through effective representation learning.",1
"Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD.",0
"Our paper focuses on designing a good image generator for high resolution video synthesis. Current state-of-the art methods use generative models such as GANs (Generative Adversarial Networks) which can generate realistic images but have difficulty preserving important features from input frames, resulting in low quality outputs especially for high-resolution inputs where many complex details need to be retained. In this work we introduce a novel approach that leverages knowledge distillation, which reduces the gap between teacher model (a pretrained Vision Transformer) and student generator by transferring knowledge from the teacher directly into the latent space of the generator at training time. We evaluate our method against SOTA approaches on several benchmark datasets including Kinetics-400, Something-Something-V2, UCF-101, and DAVIS-17 and show significant improvement over baseline models across all metrics. Additionally, using our framework it’s possible to preserve temporal consistency making output videos even more coherent.",1
"Evaluating image generation models such as generative adversarial networks (GANs) is a challenging problem. A common approach is to compare the distributions of the set of ground truth images and the set of generated test images. The Frech\'et Inception distance is one of the most widely used metrics for evaluation of GANs, which assumes that the features from a trained Inception model for a set of images follow a normal distribution. In this paper, we argue that this is an over-simplified assumption, which may lead to unreliable evaluation results, and more accurate density estimation can be achieved using a truncated generalized normal distribution. Based on this, we propose a novel metric for accurate evaluation of GANs, named TREND (TRuncated gEneralized Normal Density estimation of inception embeddings). We demonstrate that our approach significantly reduces errors of density estimation, which consequently eliminates the risk of faulty evaluation results. Furthermore, we show that the proposed metric significantly improves robustness of evaluation results against variation of the number of image samples.",0
"Title: A Study on Generative Adversarial Network (GAN) Evaluation using Truncated Generalized Normal Distributions for Inception Embedding Analysis  Abstract: Artificial intelligence has seen significant advancements in recent years due to deep learning techniques such as generative adversarial networks (GANs). However, evaluating the quality of generated images by these models remains a challenging task. Researchers have proposed various evaluation metrics, including Fréchet Inception Distance (FID), which measures differences between feature distributions extracted from real images and generated ones. This study focuses on improving FID calculations by employing truncated generalized normal density estimation instead of standard kernel density estimation used previously. Our approach leverages the nature of image features captured through convolutional neural networks to effectively model their underlying distribution for improved accuracy in GAN assessments. We provide comprehensive experimental results that showcase our method's effectiveness in generating high-quality images while yielding better performance compared to traditional methods across several benchmark datasets. These findings contribute to the field's understanding of GAN evaluation, ultimately helping researchers develop more reliable and effective artificial intelligence systems.",1
"We propose a novel approach for few-shot talking-head synthesis. While recent works in neural talking heads have produced promising results, they can still produce images that do not preserve the identity of the subject in source images. We posit this is a result of the entangled representation of each subject in a single latent code that models 3D shape information, identity cues, colors, lighting and even background details. In contrast, we propose to factorize the representation of a subject into its spatial and style components. Our method generates a target frame in two steps. First, it predicts a dense spatial layout for the target image. Second, an image generator utilizes the predicted layout for spatial denormalization and synthesizes the target frame. We experimentally show that this disentangled representation leads to a significant improvement over previous methods, both quantitatively and qualitatively.",0
"In this paper we present a novel framework for few-shot talking head synthesis that leverages learned spatial representations encoded by a generative model. This framework consists of two main components: a spatial transformer network (STN) which extracts a spatial representation from text input and warps the corresponding image features accordingly; and a generative adversarial network (GAN) which generates new video frames conditioned on the warped image features as well as the original audio signal. Our approach outperforms previous methods in terms of visual fidelity, naturalness and alignment with the given speech signal. We demonstrate the effectiveness of our method using several subjective evaluations and user studies, where participants rate the quality of generated videos against ground truth data. Additionally, we conduct an ablation study showing the impact each component has on the final result. Finally, we conclude by discussing some limitations of our work and potential future research directions. -----  Learning the complex mapping between spoken language and facial expressions remains a challenging task in computer graphics and multimedia. However, recent advancements in deep learning have led to significant progress in this area, particularly through few-shot talking head synthesis approaches. One such approach, presented in this paper, involves utilizing learned spatial representations encoded by a generative model.  Our proposed framework comprises two core components: a spatial transformer network (STN), responsible for extracting a spatial representation from text input and subsequently manipulating corresponding image features; and a generative adversarial network (GAN), generating new video frames based on both the modified image features and the original audio signal. Experimental results demonstrate that our method surpasses existing techniques in regards to visual accuracy and coherence with provided audio.  To evaluate the efficacy o",1
"Person re-identification (re-ID) concerns the matching of subject images across different camera views in a multi camera surveillance system. One of the major challenges in person re-ID is pose variations across the camera network, which significantly affects the appearance of a person. Existing development data lack adequate pose variations to carry out effective training of person re-ID systems. To solve this issue, in this paper we propose an end-to-end pose-driven attention-guided generative adversarial network, to generate multiple poses of a person. We propose to attentively learn and transfer the subject pose through an attention mechanism. A semantic-consistency loss is proposed to preserve the semantic information of the person during pose transfer. To ensure fine image details are realistic after pose translation, an appearance discriminator is used while a pose discriminator is used to ensure the pose of the transferred images will exactly be the same as the target pose. We show that by incorporating the proposed approach in a person re-identification framework, realistic pose transferred images and state-of-the-art re-identification results can be achieved.",0
"This paper presents a new approach to image generation for person re-identification tasks that utilizes pose-driven attention guidelines. By incorporating these guidelines into the generation process, we can improve the accuracy and quality of generated images, making them more suitable for use in real-world applications. Our method involves training a generative model on a large dataset of annotated images, which allows us to learn the underlying patterns and relationships between different poses and identities. We then apply this knowledge to generate high-resolution images that capture fine details and accurately represent the target identity. To ensure that our generated images meet the desired requirements, we introduce an additional loss function based on feature similarity, which helps to align the generated images with their corresponding class label. Extensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art results on several benchmark datasets. Overall, our work shows promise for advancing the field of person re-identification through improved image generation techniques.",1
"Conditional generative adversarial networks (cGANs) target at synthesizing diverse images given the input conditions and latent codes, but unfortunately, they usually suffer from the issue of mode collapse. To solve this issue, previous works mainly focused on encouraging the correlation between the latent codes and their generated images, while ignoring the relations between images generated from various latent codes. The recent MSGAN tried to encourage the diversity of the generated image but only considers ""negative"" relations between the image pairs. In this paper, we propose a novel DivCo framework to properly constrain both ""positive"" and ""negative"" relations between the generated images specified in the latent space. To the best of our knowledge, this is the first attempt to use contrastive learning for diverse conditional image synthesis. A novel latent-augmented contrastive loss is introduced, which encourages images generated from adjacent latent codes to be similar and those generated from distinct latent codes to be dissimilar. The proposed latent-augmented contrastive loss is well compatible with various cGAN architectures. Extensive experiments demonstrate that the proposed DivCo can produce more diverse images than state-of-the-art methods without sacrificing visual quality in multiple unpaired and paired image generation tasks.",0
"This paper presents a new method for image synthesis called DivCo, which stands for ""Diverse Conditional Image Synthesis."" The problem of diverse conditional image synthesis has been studied extensively in recent years as more applications require realistic images that can capture many different variations under specific conditions. Most existing methods rely on data augmentation, which generates multiple variants of each training sample by applying random transformations. However, these approaches have limitations such as producing only marginal differences from the original samples, lacking control over generated results, and suffering from low diversity due to large margins imposed during training. To address these issues, we propose using a contrastive generative adversarial network (cGAN) framework. In our model, we introduce a novel multi-scale discriminator and a loss function to promote high-quality and diverse output samples. We demonstrate through extensive experiments on three challenging datasets that our approach achieves state-of-the-art performance compared to other baseline models while generating more diverse outputs. Our work expands the possibilities of image generation tasks and could potentially inspire future research directions towards controllable and diverse conditional image synthesis.",1
"A text to image generation (T2I) model aims to generate photo-realistic images which are semantically consistent with the text descriptions. Built upon the recent advances in generative adversarial networks (GANs), existing T2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) The condition batch normalization methods are applied on the whole image feature maps equally, ignoring the local semantics; (2) The text encoder is fixed during training, which should be trained with the image generator jointly to learn better text representations for image generation. To address these limitations, we propose a novel framework Semantic-Spatial Aware GAN, which is trained in an end-to-end fashion so that the text encoder can exploit better text information. Concretely, we introduce a novel Semantic-Spatial Aware Convolution Network, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a mask map in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code is available at https://github.com/wtliao/text2image.",0
"Generating images from text descriptions has been a longstanding challenge in computer vision research. Recent advances have used generative adversarial networks (GANs) that can synthesize high quality images guided by natural language prompts. However, most existing methods still suffer from common limitations such as: lacking spatial awareness, not understanding semantic content, generating unrealistic results, and struggling with fine details. In this work we propose Text to Image Generation with Semantic-Spatial Aware GAN to address these challenges. Our approach includes a novel spatial transformer network module which incorporates both contextual information and spatial attention mechanisms. This allows our model to effectively attend to regions relevant to specific concepts described in the input text. We further introduce a modality-specific discriminator for effective image evaluation. Through extensive experiments on multiple benchmark datasets, we demonstrate that our proposed method consistently produces more accurate results compared with state-of-the-art alternatives across several metrics. These findings suggest that our framework provides significant progress towards enabling machines to efficiently generate images accurately reflecting given texts inputs.",1
"Several regularization methods have recently been introduced which force the latent activations of an autoencoder or deep neural network to conform to either a Gaussian or hyperspherical distribution, or to minimize the implicit rank of the distribution in latent space. In the present work, we introduce a novel regularizing loss function which simulates a pairwise repulsive force between items and an attractive force of each item toward the origin. We show that minimizing this loss function in isolation achieves a hyperspherical distribution. Moreover, when used as a regularizing term, the scaling factor can be adjusted to allow greater flexibility and tolerance of eccentricity, thus allowing the latent variables to be stratified according to their relative importance, while still promoting diversity. We apply this method of Eccentric Regularization to an autoencoder, and demonstrate its effectiveness in image generation, representation learning and downstream classification tasks.",0
"Abstract Eccentric regularization is a technique used to minimize hyperspherical energy. Traditionally, eccentricity has been calculated using vector distances from data points to the origin and normalized by dividing it by the radius of the sphere centered at those points which have the smallest sum of squares norm. This approach requires iterative computation and explicit projection onto the corresponding tangential space. In our work we aim to eliminate these drawbacks by computing eccentricity directly through a closed form solution and minimizing its value using gradient descent. Our experiments show that our method results in accurate estimates and outperforms previous approaches in terms of speed and efficiency. We believe that our contributions could potentially lead to applications in areas such as image processing and computer vision where minimizing eccentricity can enhance performance significantly.",1
"Image quality assessment (IQA) aims to assess the perceptual quality of images. The outputs of the IQA algorithms are expected to be consistent with human subjective perception. In image restoration and enhancement tasks, images generated by generative adversarial networks (GAN) can achieve better visual performance than traditional CNN-generated images, although they have spatial shift and texture noise. Unfortunately, the existing IQA methods have unsatisfactory performance on the GAN-based distortion partially because of their low tolerance to spatial misalignment. To this end, we propose the reference-oriented deformable convolution, which can improve the performance of an IQA network on GAN-based distortion by adaptively considering this misalignment. We further propose a patch-level attention module to enhance the interaction among different patch regions, which are processed independently in previous patch-based methods. The modified residual block is also proposed by applying modifications to the classic residual block to construct a patch-region-based baseline called WResNet. Equipping this baseline with the two proposed modules, we further propose Region-Adaptive Deformable Network (RADN). The experiment results on the NTIRE 2021 Perceptual Image Quality Assessment Challenge dataset show the superior performance of RADN, and the ensemble approach won fourth place in the final testing phase of the challenge. Code is available at https://github.com/IIGROUP/RADN.",0
"This paper presents a novel approach for image quality assessment (IQA) that leverages region-adaptive deformable networks. IQA refers to the process of evaluating the perceived quality of images or videos, which can be challenging due to variations across different regions within an image. To address these variations, we propose using a deformable network architecture, where the structure adapts to local features of each region. Our method consists of three main components: feature extraction, region grouping, and quality prediction. During feature extraction, we extract regional features from each part of the image separately using convolutional neural networks. Next, we group similar regions into distinct clusters based on their characteristics, such as texture, sharpness, and noise levels. Finally, we train separate deformable networks for each cluster to predict perceptual scores representing overall image quality. We evaluate our model on multiple benchmark datasets, achieving state-of-the-art results on most tasks while maintaining high computational efficiency. Overall, our work demonstrates the effectiveness of region-adaptive deformable networks for accurate and efficient IQA.",1
"We consider image completion from the perspective of amortized inference in an image generative model. We leverage recent state of the art variational auto-encoder architectures that have been shown to produce photo-realistic natural images at non-trivial resolutions. Through amortized inference in such a model we can train neural artifacts that produce diverse, realistic image completions even when the vast majority of an image is missing. We demonstrate superior sample quality and diversity compared to prior art on the CIFAR-10 and FFHQ-256 datasets. We conclude by describing and demonstrating an application that requires an in-painting model with the capabilities ours exhibits: the use of Bayesian optimal experimental design to select the most informative sequence of small field of view x-rays for chest pathology detection.",0
"Image Completion via Inference in Deep Generative Models: An Approach Based on Neural Networks In recent years, deep generative models have emerged as powerful tools for image completion tasks. These methods use neural networks to generate new pixels based on incomplete data, with promising results in terms of quality and fidelity. However, current approaches often suffer from instability during training, resulting in poor performance and limited applicability to real-world scenarios. This paper presents an approach that addresses these limitations by using inference techniques to improve stability and accuracy. We demonstrate our method on several benchmark datasets and show significant improvements over state-of-the-art algorithms in both quantitative and qualitative evaluations. Our findings provide insights into the capabilities and limitations of deep generative models for image completion and open up new research directions in computer vision.",1
"The widespread dissemination of forged images generated by Deepfake techniques has posed a serious threat to the trustworthiness of digital information. This demands effective approaches that can detect perceptually convincing Deepfakes generated by advanced manipulation techniques. Most existing approaches combat Deepfakes with deep neural networks by mapping the input image to a binary prediction without capturing the consistency among different pixels. In this paper, we aim to capture the subtle manipulation artifacts at different scales for Deepfake detection. We achieve this with transformer models, which have recently demonstrated superior performance in modeling dependencies between pixels for a variety of recognition tasks in computer vision. In particular, we introduce a Multi-modal Multi-scale TRansformer (M2TR), which uses a multi-scale transformer that operates on patches of different sizes to detect the local inconsistency at different spatial levels. To improve the detection results and enhance the robustness of our method to image compression, M2TR also takes frequency information, which is further combined with RGB features using a cross modality fusion module. Developing and evaluating Deepfake detection methods requires large-scale datasets. However, we observe that samples in existing benchmarks contain severe artifacts and lack diversity. This motivates us to introduce a high-quality Deepfake dataset, SR-DF, which consists of 4,000 DeepFake videos generated by state-of-the-art face swapping and facial reenactment methods. On three Deepfake datasets, we conduct extensive experiments to verify the effectiveness of the proposed method, which outperforms state-of-the-art Deepfake detection methods.",0
"Title your abstract appropriately. Paper Title: M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection  Abstract ----------------------  This work proposes a novel deep learning model called ""M2TR"" that can detect deepfakes by analyzing both audio and video data using multi-scale transformer architecture. Existing methods for deepfake detection have relied on either audio or visual cues alone, which has limited their effectiveness. By leveraging multi-modal information, our model achieves state-of-the-art performance in detecting deepfakes across different scenarios. We evaluate our approach on multiple datasets and demonstrate its superiority over other techniques through extensive experiments. Our results show that the combination of multi-scale features and attention mechanisms significantly improves the robustness and generalization ability of the model. This research contributes to the growing field of deepfake detection, highlighting the potential of multi-modal approaches for addressing emerging challenges in digital media authenticity verification.  Keywords - deepfake detection, multimodal fusion, multi-scale transformers, attention mechanism",1
"Neural networks-based learning of the distribution of non-dispatchable renewable electricity generation from sources such as photovoltaics (PV) and wind as well as load demands has recently gained attention. Normalizing flow density models have performed particularly well in this task due to the training through direct log-likelihood maximization. However, research from the field of image generation has shown that standard normalizing flows can only learn smeared-out versions of manifold distributions and can result in the generation of noisy data. To avoid the generation of time series data with unrealistic noise, we propose a dimensionality-reducing flow layer based on the linear principal component analysis (PCA) that sets up the normalizing flow in a lower-dimensional space. We train the resulting principal component flow (PCF) on data of PV and wind power generation as well as load demand in Germany in the years 2013 to 2015. The results of this investigation show that the PCF preserves critical features of the original distributions, such as the probability density and frequency behavior of the time series. The application of the PCF is, however, not limited to renewable power generation but rather extends to any data set, time series, or otherwise, which can be efficiently reduced using PCA.",0
"Title: Deep Learning Based Stochastic Optimization Method For Solving Linear Programming Problems Authors: Xiaohui Li (Tsinghua University), Chengyang Duan (Cambridge Joint Laboratory on Web Science) Abstract: This paper presents a new deep learning based stochastic optimization method called DeepOptSGO that can solve linear programming problems efficiently and effectively. In our approach, we use a normalizing flow to learn an implicit distribution from noisy samples generated by an initial solver, and then we optimize the parameters using gradient ascent with respect to the negative log likelihood objective function which leads to finding solutions of LP problems. We test our algorithm against well known open source packages including CBC, Gurobi, and SciPy, with instances from SuiteOR library and MIPLIB2 benchmark suite. Experimental results show that DeepOptSGO outperforms other state-of-the-art methods and achieves comparable accuracy compared to commercial software, thus demonstrating its effectiveness and potential to revolutionize the field of mathematical optimization. Our findings have important implications for solving linear programs, especially for large scale real world applications where traditional methods might be slow or infeasible. Moreover, since training models takes less time than generating new solutions, once trained, predictions on new datasets could be made quickly without retraining, enabling fast solution generation for big data analytics scenarios. Future work will focus on extending DeepOptSGO to handle nonlinear constraints and uncertain systems.",1
"Despite all the challenges and limitations, vision-based vehicle speed detection is gaining research interest due to its great potential benefits such as cost reduction, and enhanced additional functions. As stated in a recent survey [1], the use of learning-based approaches to address this problem is still in its infancy. One of the main difficulties is the need for a large amount of data, which must contain the input sequences and, more importantly, the output values corresponding to the actual speed of the vehicles. Data collection in this context requires a complex and costly setup to capture the images from the camera synchronized with a high precision speed sensor to generate the ground truth speed values. In this paper we explore, for the first time, the use of synthetic images generated from a driving simulator (e.g., CARLA) to address vehicle speed detection using a learning-based approach. We simulate a virtual camera placed over a stretch of road, and generate thousands of images with variability corresponding to multiple speeds, different vehicle types and colors, and lighting and weather conditions. Two different approaches to map the sequence of images to an output speed (regression) are studied, including CNN-GRU and 3D-CNN. We present preliminary results that support the high potential of this approach to address vehicle speed detection.",0
"This article presents a methodology that uses machine learning techniques and artificial intelligence algorithms to detect vehicles speeds using imagery data collected through simulation driving scenarios created in synthetic driving environments. By analyzing image frames captured by cameras in different locations within the virtual scenario, our approach can accurately estimate velocity without relying on GPS tracking or predefined road features. Our results show high accuracy in predicting car velocities under varied lighting conditions, weather effects, time of day and camera angles. We discuss how such speed estimates could potentially provide valuable insights into driver behavior, traffic safety analysis, and urban planning among other applications. The goal of the paper is to demonstrate the potential benefits and limitations of data-driven approaches for capturing real-world driving data while highlighting future research directions towards developing more reliable and robust systems. Overall, we believe our work contributes significantly to recent advances in the field of transportation science and technology.",1
"This paper studies the neural architecture search (NAS) problem for developing efficient generator networks. Compared with deep models for visual recognition tasks, generative adversarial network (GAN) are usually designed to conduct various complex image generation. We first discover an intact search space of generator networks including three dimensionalities, i.e., path, operator, channel for fully excavating the network performance. To reduce the huge search cost, we explore a coarse-to-fine search strategy which divides the overall search process into three sub-optimization problems accordingly. In addition, a fair supernet training approach is utilized to ensure that all sub-networks can be updated fairly and stably. Experiments results on benchmarks show that we can provide generator networks with better image quality and lower computational costs over the state-of-the-art methods. For example, with our method, it takes only about 8 GPU hours on the entire edges-to-shoes dataset to get a 2.56 MB model with a 24.13 FID score and 10 GPU hours on the entire Urban100 dataset to get a 1.49 MB model with a 24.94 PSNR score.",0
"Our algorithm speeds up the search process by coarsely searching through large model spaces first before gradually honing in on smaller regions that produce better results. This approach balances computational cost against quality of generated outputs so that we can find models more efficiently without compromising on performance. Through extensive experiments across different domains and task types, we demonstrate the effectiveness and efficiency of our method compared to traditional methods which often require much longer searches. In summary, our work shows how coarse-to-fine searching can be used as an effective tool for generating high quality outputs from generative adversarial networks.",1
"The existing text-guided image synthesis methods can only produce limited quality results with at most \mbox{$\text{256}^2$} resolution and the textual instructions are constrained in a small Corpus. In this work, we propose a unified framework for both face image generation and manipulation that produces diverse and high-quality images with an unprecedented resolution at 1024 from multimodal inputs. More importantly, our method supports open-world scenarios, including both image and text, without any re-training, fine-tuning, or post-processing. To be specific, we propose a brand new paradigm of text-guided image generation and manipulation based on the superior characteristics of a pretrained GAN model. Our proposed paradigm includes two novel strategies. The first strategy is to train a text encoder to obtain latent codes that align with the hierarchically semantic of the aforementioned pretrained GAN model. The second strategy is to directly optimize the latent codes in the latent space of the pretrained GAN model with guidance from a pretrained language model. The latent codes can be randomly sampled from a prior distribution or inverted from a given image, which provides inherent supports for both image generation and manipulation from multi-modal inputs, such as sketches or semantic labels, with textual guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.",0
"In recent years, there has been significant progress in developing generative models that can synthesize novel images based on text descriptions. These models have shown promise in a variety of domains including computer vision, art creation, virtual reality, and video games. However, most current methods rely on close-world assumptions, where the environment is highly constrained and the model has access to complete information about the desired output during training and inference. This approach often leads to poor generalization abilities, lack of interpretability, and limited flexibility in manipulating generated outputs at test time. To address these limitations, we propose a new framework for open-world text guided face image generation and manipulation. Our method uses a combination of textual guidance, facial landmark detection, attribute conditioning, and latent space navigation techniques to generate high quality face images that accurately capture given characteristics, such as gender, age, expression, head pose, and lighting conditions. Additionally, our framework allows users to manipulate the generated faces in real-time through intuitive user interfaces, enabling fine-grained control over identity attributes and detailed facial features. We evaluate our method using both quantitative measures and human evaluations, demonstrating superior performance compared to state-of-the-art baselines across multiple tasks, including face generation from scratch, face editing under changing conditions, and facial attribute transfer. Overall, our work represents a step towards more powerful, flexible, and interpretable generative models capable of handling complex, unconstrained environments, opening up exciting opportunities in diverse fields involving graphics, visual effects, medical imaging, fashion design, entertainment industry, education, and many others.",1
"It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations. This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning. We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images. Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data. Along the way, we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process. Code for our experiments may be found here https://github.com/ppope/dimensions.",0
"This paper presents research into how human brains process images that have inherent dimensionality - i.e., images where there is an intuitive sense of three dimensions, even though they may only appear as flat images on a screen or piece of paper. We show how this intrinsic dimension impacts learning by analyzing neural activity data from MRI scans of participants exposed to images with different levels of intrinsic dimension. Our results indicate that images with high intrinsic dimensionality activate more areas of the brain involved in visual processing than images without such depth cues, leading to increased attention and better retention of learned material. These findings shed new light on how our perception of space influences cognition and suggest potential applications in fields like education and advertising.",1
"This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet's best model achieves $46.4$ $AP$ (and $65.1$ $AP_{50}$), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in other visual detection tasks, namely, video object detection, instance segmentation, 3D object detection and keypoint detection for human pose estimation, and an additional ``labels to photo`` image generation task, where the integration of our voting module consistently improves performance in all cases. Code is available at \url{https://github.com/nerminsamet/houghnet}.",0
"This research presents a new deep learning architecture called HoughNet that improves object detection by integrating both short and long range contextual relationships between objects. Traditional convolutional neural networks (CNNs) rely heavily on local neighborhood information which can lead to poor performance in identifying objects that have spatially diverse features. By incorporating a global attention module based on Hough voting, HoughNet effectively captures long-range relationships between objects and significantly increases accuracy in challenging scenarios such as occlusions, background clutter, and partial visibility. Experiments show that HoughNet outperforms state-of-the-art detectors across multiple benchmark datasets including COCO, VOC2007, and PASCAL-VOC. Additionally, our model achieves real-time inference speed making it practical for deployment in various applications. Overall, this work demonstrates the importance of incorporating both short and long range contextual information for accurate object detection.",1
"Human pose transfer has received great attention due to its wide applications, yet is still a challenging task that is not well solved. Recent works have achieved great success to transfer the person image from the source to the target pose. However, most of them cannot well capture the semantic appearance, resulting in inconsistent and less realistic textures on the reconstructed results. To address this issue, we propose a new two-stage framework to handle the pose and appearance translation. In the first stage, we predict the target semantic parsing maps to eliminate the difficulties of pose transfer and further benefit the latter translation of per-region appearance style. In the second one, with the predicted target semantic maps, we suggest a new person image generation method by incorporating the region-adaptive normalization, in which it takes the per-region styles to guide the target appearance generation. Extensive experiments show that our proposed SPGNet can generate more semantic, consistent, and photo-realistic results and perform favorably against the state of the art methods in terms of quantitative and qualitative evaluation. The source code and model are available at https://github.com/cszy98/SPGNet.git.",0
"This paper presents a new method for generating high quality images of people using semantic segmentation techniques. Our approach uses region-adaptive normalization to improve image generation accuracy. We train our model on a large dataset of annotated person images and use a convolutional neural network to generate new images from scratch. Experimental results show that our method outperforms previous state-of-the-art approaches both quantitatively and qualitatively. We also provide a user study comparing human evaluations of our generated images against real images and demonstrate that our generated images are highly similar and comparable. Overall, we believe that our work represents a significant advancement in the field of computer vision and has important applications in fields such as video surveillance and advertising.",1
"Learning to generate new images for a novel category based on only a few images, named as few-shot image generation, has attracted increasing research interest. Several state-of-the-art works have yielded impressive results, but the diversity is still limited. In this work, we propose a novel Delta Generative Adversarial Network (DeltaGAN), which consists of a reconstruction subnetwork and a generation subnetwork. The reconstruction subnetwork captures intra-category transformation, i.e., ""delta"", between same-category pairs. The generation subnetwork generates sample-specific ""delta"" for an input image, which is combined with this input image to generate a new image within the same category. Besides, an adversarial delta matching loss is designed to link the above two subnetworks together. Extensive experiments on five few-shot image datasets demonstrate the effectiveness of our proposed method.",0
"One possible abstract for a paper about a new approach to few-shot image generation using generative adversarial networks (GANs) could read as follows:  Few-shot image generation is an important but challenging problem that involves generating high-quality images from just a small number of example images. Existing approaches based on GANs have made significant progress in this area, but they can still struggle with diversity and quality. In this work, we present a novel approach called DeltaGAN that addresses these issues by introducing sample-specific deltas into the generation process. Our method leverages the power of pretrained CNNs to extract features from input examples, and uses these features to generate diverse and coherent samples via iterative refinement. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods across several benchmark datasets, achieving significantly higher fidelity and visual variety while maintaining competitive performance in terms of metrics such as Frechet distance and log likelihood. We believe that DeltaGAN represents a step forward towards realizing more advanced forms of few-shot image synthesis, opening up promising directions for future research in this exciting field.",1
"Training generative models, such as GANs, on a target domain containing limited examples (e.g., 10) can easily result in overfitting. In this work, we seek to utilize a large source domain for pretraining and transfer the diversity information from source to target. We propose to preserve the relative similarities and differences between instances in the source via a novel cross-domain distance consistency loss. To further reduce overfitting, we present an anchor-based strategy to encourage different levels of realism over different regions in the latent space. With extensive results in both photorealistic and non-photorealistic domains, we demonstrate qualitatively and quantitatively that our few-shot model automatically discovers correspondences between source and target domains and generates more diverse and realistic images than previous methods.",0
"This paper describes an approach to few-shot image generation that involves identifying cross-domain correspondences between training images and unseen target data. We propose a method based on adversarial networks which can generate high-quality synthetic examples using only a small number of labeled samples from each domain. Our approach utilizes a correspondence network trained on a large dataset of diverse images to map source features into corresponding target features, enabling efficient few-shot learning. Our results demonstrate significant improvements over previous methods in terms of both visual quality and quantitative metrics such as Fréchet Inception Distance (FID). The proposed method has potential applications in domains where limited labeled data is available, including computer vision, medical imaging, and autonomous vehicles.",1
"Generative Adversarial Networks (GANs) have shown satisfactory performance in synthetic image generation by devising complex network structure and adversarial training scheme. Even though GANs are able to synthesize realistic images, there exists a number of generated images with defective visual patterns which are known as artifacts. While most of the recent work tries to fix artifact generations by perturbing latent code, few investigate internal units of a generator to fix them. In this work, we devise a method that automatically identifies the internal units generating various types of artifact images. We further propose the sequential correction algorithm which adjusts the generation flow by modifying the detected artifact units to improve the quality of generation while preserving the original outline. Our method outperforms the baseline method in terms of FID-score and shows satisfactory results with human evaluation.",0
The following section contains a list of statements related to the subject matter of your request which could serve as abstract bullet points for a paper on automatic correction of internal units in generative neural networks: * Generative neural networks (GNN) require large amounts of data to produce accurate results * GNN have problems converging without enough training data resulting in poor performance * Correcting incorrect outputs can lead to improved generalization performance but requires manual intervention and domain expertise * This paper presents a method that automatically corrects internal units in a GNN to improve accuracy through minimizing reconstruction loss with respect to the ground truth output and maximize likelihood of the latent variables using variational inference * Experiments show significant improvements over baseline models both quantitatively and qualitatively * Approach has applications beyond image generation including natural language processing and speech synthesis,1
"Focus based methods have shown promising results for the task of depth estimation. However, most existing focus based depth estimation approaches depend on maximal sharpness of the focal stack. Out of focus information in the focal stack poses challenges for this task. In this paper, we propose a dynamically multi modal learning strategy which incorporates RGB data and the focal stack in our framework. Our goal is to deeply excavate the spatial correlation in the focal stack by designing the spatial correlation perception module and dynamically fuse multi modal information between RGB data and the focal stack in a adaptive way by designing the multi modal dynamic fusion module. The success of our method is demonstrated by achieving the state of the art performance on two datasets. Furthermore, we test our network on a set of different focused images generated by a smart phone camera to prove that the proposed method not only broke the limitation of only using light field data, but also open a path toward practical applications of depth estimation on common consumer level cameras data.",0
"In recent years, depth estimation from light fields has gained significant interest due to its applications in computer vision, such as refocusing after capture, rendering high dynamic range images, and simulating human binocular disparity. However, accurate depth estimation remains challenging due to issues like occlusions, motion blur, and lack of texture details. To address these limitations, we propose a novel approach called Dynamic Fusion Network (DFN) that effectively fuses multi-scale features extracted from light field image pairs using a coarse-to-fine architecture. Our method dynamically selects informative layers during training based on their error gradients, ensuring efficient feature fusion and improved accuracy. Extensive experimental results demonstrate that our DFN significantly outperforms existing methods in terms of both quantitative metrics and visual quality measures. This work shows promise for enabling new capabilities in next generation light field systems, and paves the way for future research in the area of computational imaging.",1
"Event camera is an asynchronous, high frequency vision sensor with low power consumption, which is suitable for human action understanding task. It is vital to encode the spatial-temporal information of event data properly and use standard computer vision tool to learn from the data. In this work, we propose a timestamp image encoding 2D network, which takes the encoded spatial-temporal images with polarity information of the event data as input and output the action label. In addition, we propose a future timestamp image generator to generate futureaction information to aid the model to anticipate the human action when the action is not completed. Experiment results show that our method can achieve the same level of performance as those RGB-based benchmarks on real world action recognition,and also achieve the state of the art (SOTA) result on gesture recognition. Our future timestamp image generating model can effectively improve the prediction accuracy when the action is not completed. We also provide insight discussion on the importance of motion and appearance information in action recognition and anticipation.",0
"In this work, we propose a novel deep neural network architecture called Event-based Timestamp Image Encoding (ETIE) Network that effectively captures both spatial and temporal features from event camera data for human action recognition and anticipation. Our approach utilizes the unique properties of event cameras, which provide high-resolution asynchronous pixel-level sensing of dynamic scenes. Unlike traditional frame-based methods, our ETIE network encodes image sequences into compact high-dimensional representations using a combination of time-stamped binary images and corresponding continuous confidence maps.  Our network consists of several components: an event decoder module, timestamp prediction layer, timestamp refinement module, feature extraction layers, fusion layers, and output layers. The event decoder generates a sequence of synchronized binary frames and corresponding confidence maps based on the input event stream. We then predict timestamps for each event using a shared convolutional neural network (CNN), while introducing a new mask attention mechanism to improve robustness against noise. This allows us to accurately encode events at different locations within a single image frame, capturing fine-grained motion details.  The encoded representation is further processed through a series of feature extraction and fusion layers designed to extract discriminative patterns across space and time dimensions. Finally, fully connected layers are used to produce class probabilities for action classification as well as future action anticipation tasks.  Experimental results demonstrate the effectiveness of our method compared to state-of-the-art approaches on two benchmark datasets, namely N-Elephant and Dexterous Hand Gesture. Furthermore, we showcase promising results for the challenging task of hand gesture anticipation using the proposed model, highlighting its potential applications in robotics and other real-world scenarios where early detection and understanding of human actions can greatly benefit safety, efficiency, and user experience. Overall, this work represents an important step towards harve",1
"We address the task of multi-view image-to-image translation for person image generation. The goal is to synthesize photo-realistic multi-view images with pose-consistency across all views. Our proposed end-to-end framework is based on a joint learning of multiple unpaired image-to-image translation models, one per camera viewpoint. The joint learning is imposed by constraints on the shared 3D human pose in order to encourage the 2D pose projections in all views to be consistent. Experimental results on the CMU-Panoptic dataset demonstrate the effectiveness of the suggested framework in generating photo-realistic images of persons with new poses that are more consistent across all views in comparison to a standard Image-to-Image baseline. The code is available at: https://github.com/sony-si/MultiView-Img2Img",0
"This paper presents a new method for multi-view image-to-image translation using 3D pose as supervision. Our approach combines the strengths of recent image generation methods based on deep neural networks, while addressing their weaknesses. We use an adversarial loss function that encourages our network to generate images that are both realistic and consistent with the input poses. Additionally, we introduce several novel techniques that improve the accuracy and efficiency of our method, such as progressive refinement stages, view synthesis modules, and pose transfer losses. To evaluate our approach, we perform extensive experiments on two challenging datasets and demonstrate significant improvements over state-of-the-art methods. Furthermore, we showcase applications of our system for virtual try-on, motion retargeting, and gaze manipulation tasks. Overall, our work represents a major step towards high-quality, multi-view image-to-image translation, bridging the gap between traditional geometry-based approaches and deep learning methods. Please refer to the full text for actual results.",1
"Restoring the clean background from the superimposed images containing a noisy layer is the common crux of a classical category of tasks on image restoration such as image reflection removal, image deraining and image dehazing. These tasks are typically formulated and tackled individually due to the diverse and complicated appearance patterns of noise layers within the image. In this work we present the Deep-Masking Generative Network (DMGN), which is a unified framework for background restoration from the superimposed images and is able to cope with different types of noise. Our proposed DMGN follows a coarse-to-fine generative process: a coarse background image and a noise image are first generated in parallel, then the noise image is further leveraged to refine the background image to achieve a higher-quality background image. In particular, we design the novel Residual Deep-Masking Cell as the core operating unit for our DMGN to enhance the effective information and suppress the negative information during image generation via learning a gating mask to control the information flow. By iteratively employing this Residual Deep-Masking Cell, our proposed DMGN is able to generate both high-quality background image and noisy image progressively. Furthermore, we propose a two-pronged strategy to effectively leverage the generated noise image as contrasting cues to facilitate the refinement of the background image. Extensive experiments across three typical tasks for image background restoration, including image reflection removal, image rain steak removal and image dehazing, show that our DMGN consistently outperforms state-of-the-art methods specifically designed for each single task.",0
"The restoration of superimposed images has become an increasingly important task in computer vision due to the rise of social media and the prevalence of composite images. In recent years, deep learning models have been applied to tackle this problem with varying degrees of success. This paper proposes a novel framework called ""Deep-Masking Generative Network"" (DMGN) that unifies background extraction and image composition into one end-to-end network.  The DMGN model consists of two components: a feature masker and a generative module. The feature masker identifies the regions of interest (ROIs) containing the objects in the foreground and extracts their features. These ROIs act as conditioning inputs for the generative module, which generates new compositions of the background scenes by manipulating latent vectors learned from a pre-trained generator. Experimental results on public datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance in terms of visual quality, object detection accuracy, and user preference scores. Overall, DMGN provides a powerful tool for automatic background generation in superimposed images while preserving the original content of the scene.",1
"Solar cell electroluminescence (EL) defect segmentation is an interesting and challenging topic. Many methods have been proposed for EL defect detection, but these methods are still unsatisfactory due to the diversity of the defect and background. In this paper, we provide a new idea of using generative adversarial network (GAN) for defect segmentation. Firstly, the GAN-based method removes the defect region in the input defective image to get a defect-free image, while keeping the background almost unchanged. Then, the subtracted image is obtained by making difference between the defective input image with the generated defect-free image. Finally, the defect region can be segmented through thresholding the subtracted image. To keep the background unchanged before and after image generation, we propose a novel strong identity GAN (SIGAN), which adopts a novel strong identity loss to constraint the background consistency. The SIGAN can be used not only for defect segmentation, but also small-samples defective dataset augmentation. Moreover, we release a new solar cell EL image dataset named as EL-2019, which includes three types of images: crack, finger interruption and defect-free. Experiments on EL-2019 dataset show that the proposed method achieves 90.34% F-score, which outperforms many state-of-the-art methods in terms of solar cell defects segmentation results.",0
"In recent years, solar cell defect segmentation has become increasingly important due to the growing demand for efficient renewable energy sources. One approach to improving the efficiency of solar cells involves identifying and quantifying defects on their surface. However, manually inspecting large numbers of images can be time consuming and error prone. In order to address these challenges, we propose a novel image generation method called SIGAN that combines generative adversarial networks (GANs) with attention mechanisms. Our method uses GANs to generate realistic synthetic images of solar cells with varying levels of defect density. We then use an attention mechanism to focus on regions of interest within each generated image, allowing us to extract high quality masks of the defects. These masks can be used for subsequent analysis or augmented onto new images to create additional training data. Experimental results show that our method outperforms existing state-of-the art methods in terms of both segmentation accuracy and visual fidelity. Overall, our work presents a powerful tool for efficiently generating realistic synthetic data that can aid in the development of more efficient solar cell technologies.",1
"Pose-guided person image generation usually involves using paired source-target images to supervise the training, which significantly increases the data preparation effort and limits the application of the models. To deal with this problem, we propose a novel multi-level statistics transfer model, which disentangles and transfers multi-level appearance features from person images and merges them with pose features to reconstruct the source person images themselves. So that the source images can be used as supervision for self-driven person image generation. Specifically, our model extracts multi-level features from the appearance encoder and learns the optimal appearance representation through attention mechanism and attributes statistics. Then we transfer them to a pose-guided generator for re-fusion of appearance and pose. Our approach allows for flexible manipulation of person appearance and pose properties to perform pose transfer and clothes style transfer tasks. Experimental results on the DeepFashion dataset demonstrate our method's superiority compared with state-of-the-art supervised and unsupervised methods. In addition, our approach also performs well in the wild.",0
"Image generation using Generative Adversarial Networks (GAN) has become increasingly popular due to their ability to produce high-quality images that are difficult to distinguish from real photographs. However, current GAN methods often require large amounts of computational resources and can still struggle to generate coherent images when given arbitrary text descriptions as input. In this paper, we present a new approach called MUST-GAN which uses multi-level statistics transfer and self-attention mechanisms to improve image quality and reduce computation requirements. Our method outperforms existing state-of-the-art approaches on multiple benchmark datasets while requiring significantly less time and computing power. We demonstrate the effectiveness of our model through extensive experiments and provide detailed comparisons against other leading methods. This work represents a significant step forward towards efficient and accurate generative models capable of producing highly realistic synthetic images.",1
"We propose a novel transformer-based styled handwritten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global and local writing style patterns. The proposed HWT captures the long and short range relationships within the style examples through a self-attention mechanism, thereby encoding both global and local style patterns. Further, the proposed transformer-based HWT comprises an encoder-decoder attention that enables style-content entanglement by gathering the style representation of each query character. To the best of our knowledge, we are the first to introduce a transformer-based generative network for styled handwritten text generation. Our proposed HWT generates realistic styled handwritten text images and significantly outperforms the state-of-the-art demonstrated through extensive qualitative, quantitative and human-based evaluations. The proposed HWT can handle arbitrary length of text and any desired writing style in a few-shot setting. Further, our HWT generalizes well to the challenging scenario where both words and writing style are unseen during training, generating realistic styled handwritten text images.",0
"Handwritten notes are still widely used in schools, despite advancements in digital technology that have enabled electronic note taking and organization systems. Handwriting is believed to facilitate learning more effectively than typing because it provides sensory feedback and engages multiple regions of the brain involved in memory formation (Bui & Myerson, 2014). This study examines how transformer networks can be applied to handwriting recognition tasks and improve performance compared to traditional methods like Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs). We evaluate our approach using two publicly available datasets: IAM Database and CEDAR database. Our results show that the proposed method outperforms other state-of-the-art approaches on both datasets by achieving higher accuracy rates. These findings suggest that there is great potential for implementing deep learning techniques into handwriting recognition technologies, which could ultimately enhance user experiences across many industries including education, healthcare, finance, and beyond.",1
"This paper studies the problem of learning the conditional distribution of a high-dimensional output given an input, where the output and input may belong to two different domains, e.g., the output is a photo image and the input is a sketch image. We solve this problem by cooperative training of a fast thinking initializer and slow thinking solver. The initializer generates the output directly by a non-linear transformation of the input as well as a noise vector that accounts for latent variability in the output. The slow thinking solver learns an objective function in the form of a conditional energy function, so that the output can be generated by optimizing the objective function, or more rigorously by sampling from the conditional energy-based model. We propose to learn the two models jointly, where the fast thinking initializer serves to initialize the sampling of the slow thinking solver, and the solver refines the initial output by an iterative algorithm. The solver learns from the difference between the refined output and the observed output, while the initializer learns from how the solver refines its initial output. We demonstrate the effectiveness of the proposed method on various conditional learning tasks, e.g., class-to-image generation, image-to-image translation, and image recovery. The advantage of our method over GAN-based methods is that our method is equipped with a slow thinking process that refines the solution guided by a learned objective function.",0
"Abstract: In many real-world applications, conditional learning tasks require both fast thinking initializers and slow thinking solvers to efficiently converge on optimal solutions. Existing methods either focus solely on optimizing one type of network at the expense of the other or use heuristics that may fail to find good solutions. We propose cooperatively training fast thinking initializers and slow thinking solvers through adversarial competition, which jointly optimizes both types of networks by promoting discordance between their predictions while minimizing error. This enables efficient convergence to high quality solutions without relying on additional heuristics such as warm starts or restarting from previously found solutions. Experiments show significant improvements over prior state-of-the-art algorithms across a variety of benchmarks, demonstrating the effectiveness of our proposed approach for solving difficult conditional learning problems.",1
"The Generative Adversarial Network (GAN) is a state-of-the-art technique in the field of deep learning. A number of recent papers address the theory and applications of GANs in various fields of image processing. Fewer studies, however, have directly evaluated GAN outputs. Those that have been conducted focused on using classification performance, e.g., Inception Score (IS) and statistical metrics, e.g., Fr\'echet Inception Distance (FID). Here, we consider a fundamental way to evaluate GANs by directly analyzing the images they generate, instead of using them as inputs to other classifiers. We characterize the performance of a GAN as an image generator according to three aspects: 1) Creativity: non-duplication of the real images. 2) Inheritance: generated images should have the same style, which retains key features of the real images. 3) Diversity: generated images are different from each other. A GAN should not generate a few different images repeatedly. Based on the three aspects of ideal GANs, we have designed the Likeness Score (LS) to evaluate GAN performance, and have applied it to evaluate several typical GANs. We compared our proposed measure with two commonly used GAN evaluation methods: IS and FID, and four additional measures. Furthermore, we discuss how these evaluations could help us deepen our understanding of GANs and improve their performance.",0
"This paper presents a new method for evaluating generative adversarial networks (GANs) that directly assesses the quality of images generated by these models. Existing evaluation methods rely heavily on human judgement or proxy tasks which can be subjective or indirect measures of performance. Our approach leverages recent advances in computer vision and machine learning to compare GAN outputs against ground truth data using quantitative metrics such as peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). By analyzing the visual fidelity of generated images in direct comparison to real-world examples, our method provides more reliable insights into GAN effectiveness than previous approaches. Additionally, we demonstrate how our novel evaluation metric can be used to optimize GAN training parameters, enabling researchers and practitioners to achieve better results faster.",1
"Face verification has come into increasing focus in various applications including the European Entry/Exit System, which integrates face recognition mechanisms. At the same time, the rapid advancement of biometric authentication requires extensive performance tests in order to inhibit the discriminatory treatment of travellers due to their demographic background. However, the use of face images collected as part of border controls is restricted by the European General Data Protection Law to be processed for no other reason than its original purpose. Therefore, this paper investigates the suitability of synthetic face images generated with StyleGAN and StyleGAN2 to compensate for the urgent lack of publicly available large-scale test data. Specifically, two deep learning-based (SER-FIQ, FaceQnet v1) and one standard-based (ISO/IEC TR 29794-5) face image quality assessment algorithm is utilized to compare the applicability of synthetic face images compared to real face images extracted from the FRGC dataset. Finally, based on the analysis of impostor score distributions and utility score distributions, our experiments reveal negligible differences between StyleGAN vs. StyleGAN2, and further also minor discrepancies compared to real face images.",0
"In recent years, there has been significant progress in developing synthetic data techniques to create large amounts of labeled training data for machine learning models. These methods have shown promise in many applications such as natural language processing, computer vision, and speech recognition. However, questions remain regarding their applicability for face recognition tasks. This work explores the feasibility of using synthetic data for face recognition by conducting extensive experiments on different datasets under varying conditions. Our results demonstrate that while synthetic data can provide marginal benefits in some situations, it often falls short compared to real-world datasets, especially when dealing with complex facial features and variations in pose, lighting, expression, etc. We discuss potential reasons why this may occur, including the lack of diversity in existing generative models and differences in how humans perceive faces versus artificial systems. Additionally, we present guidelines for selecting appropriate types and quantities of synthetic data based on application requirements. Overall, our findings indicate that while synthetic data has limitations in improving face recognition performance, it can still serve as valuable supplementary material if used judiciously.",1
"Generative adversarial networks (GANs), e.g., StyleGAN2, play a vital role in various image generation and synthesis tasks, yet their notoriously high computational cost hinders their efficient deployment on edge devices. Directly applying generic compression approaches yields poor results on GANs, which motivates a number of recent GAN compression works. While prior works mainly accelerate conditional GANs, e.g., pix2pix and CycleGAN, compressing state-of-the-art unconditional GANs has rarely been explored and is more challenging. In this paper, we propose novel approaches for unconditional GAN compression. We first introduce effective channel pruning and knowledge distillation schemes specialized for unconditional GANs. We then propose a novel content-aware method to guide the processes of both pruning and distillation. With content-awareness, we can effectively prune channels that are unimportant to the contents of interest, e.g., human faces, and focus our distillation on these regions, which significantly enhances the distillation quality. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the state-of-the-art compression method. Notably, we reduce the FLOPs of StyleGAN2 by 11x with visually negligible image quality loss compared to the full-size model. More interestingly, when applied to various image manipulation tasks, our compressed model forms a smoother and better disentangled latent manifold, making it more effective for image editing.",0
"This paper presents a novel approach for compressing Generative Adversarial Networks (GANs) using content-aware techniques. While GANs have proven effective at generating high quality images, their large model size makes them challenging to deploy on devices with limited resources. Existing compression methods focus mainly on reducing the model parameters without considering the image generation process, resulting in degraded visual quality and increased inference time. In contrast, our proposed method leverages both spatial and frequency domain knowledge during training and generations phases to improve efficiency while maintaining the perceptual fidelity of generated images. We achieve this by introducing two key components: a new loss function that integrates high-frequency information into the generator training; and a lightweight frequency-domain discriminator. Extensive experiments demonstrate the superior performance of our method compared to state-of-the art approaches across multiple benchmark datasets. Our work opens up new possibilities for compressing deep learning models in other domains such as video processing, natural language understanding, etc., where high accuracy with low latency and storage overhead is desirable.",1
"Conditional Generative Adversarial Networks (cGAN) were designed to generate images based on the provided conditions, \eg, class-level distributions. However, existing methods have used the same generating architecture for all classes. This paper presents a novel idea that adopts NAS to find a distinct architecture for each class. The search space contains regular and class-modulated convolutions, where the latter is designed to introduce class-specific information while avoiding the reduction of training data for each class generator. The search algorithm follows a weight-sharing pipeline with mixed-architecture optimization so that the search cost does not grow with the number of classes. To learn the sampling policy, a Markov decision process is embedded into the search algorithm and a moving average is applied for better stability. We evaluate our approach on CIFAR10 and CIFAR100. Besides achieving better image generation quality in terms of FID scores, we discover several insights that are helpful in designing cGAN models. Code is available at https://github.com/PeterouZh/NAS_cGAN.",0
"Artificial intelligence (AI) has revolutionized many industries by enabling computers to perform tasks that require human level understanding of complex data patterns. One such application area where AIs have made significant progress recently is image generation - here, generative models can synthesize novel images from textual descriptions. Recently, conditional methods were introduced into GAN frameworks, which allow generating high quality images conditioned on class labels such as object categories. However, current approaches still face limitations in their applicability; most methods suffer from poor convergence during training due to nonlinearity problems arising from large batch sizes and are thus computationally intensive. In this paper we propose several new techniques aimed at improving the performance of these models, specifically focusing on better stability through improved normalization layers and introducing skip connections during convolutions to improve flow within deep models. We then investigate how these innovations affect adversarial robustness across different classes while ensuring diversity in generated outputs for each task. Our results suggest that our model, dubbed ""Searching toward Class-aware Generators for Conditional Generative Adversarial Networks"", outperforms state-of-the-art models under multiple evaluation metrics both quantitatively and qualitatively demonstrating the effectiveness of our design choices.",1
"This paper presents a new approach for synthesizing a novel street-view panorama given an overhead satellite image. Taking a small satellite image patch as input, our method generates a Google's omnidirectional street-view type panorama, as if it is captured from the same geographical location as the center of the satellite patch. Existing works tackle this task as an image generation problem which adopts generative adversarial networks to implicitly learn the cross-view transformations, while ignoring the domain relevance. In this paper, we propose to explicitly establish the geometric correspondences between the two-view images so as to facilitate the cross-view transformation learning. Specifically, we observe that when a 3D point in the real world is visible in both views, there is a deterministic mapping between the projected points in the two-view images given the height information of this 3D point. Motivated by this, we develop a novel Satellite to Street-view image Projection (S2SP) module which explicitly establishes such geometric correspondences and projects the satellite images to the street viewpoint. With these projected satellite images as network input, we next employ a generator to synthesize realistic street-view panoramas that are geometrically consistent with the satellite images. Our S2SP module is differentiable and the whole framework is trained in an end-to-end manner. Extensive experimental results on two cross-view benchmark datasets demonstrate that our method generates images that better respect the scene geometry than existing approaches.",0
"Geometry-guided street-view panorama synthesis from satellite imagery is a challenging task that involves creating high-quality 360-degree panoramic images from aerial satellite data. This process requires careful consideration of both geometric and photometric factors to ensure accurate alignment and consistent coloration across multiple image sources. In this work, we present a novel methodology for generating street-level panoramas using deep learning techniques specifically designed for use with satellite imagery. Our approach leverages advances in computer vision and machine learning algorithms to generate seamless, high-resolution panoramas that capture the complex geometry of urban environments while preserving important details such as building facades and road networks. By training our model on large datasets of real-world examples, we demonstrate improved performance over traditional methods in terms of both visual quality and computational efficiency. Ultimately, this technology has applications ranging from virtual tourism experiences to urban planning and disaster response efforts. We believe that this research represents a significant step towards more powerful and effective uses of satellite imagery in the modern world.",1
"Generative Adversarial Networks (GANs) have recently advanced image synthesis by learning the underlying distribution of the observed data. However, how the features learned from solving the task of image generation are applicable to other vision tasks remains seldom explored. In this work, we show that learning to synthesize images can bring remarkable hierarchical visual features that are generalizable across a wide range of applications. Specifically, we consider the pre-trained StyleGAN generator as a learned loss function and utilize its layer-wise representation to train a novel hierarchical encoder. The visual feature produced by our encoder, termed as Generative Hierarchical Feature (GH-Feat), has strong transferability to both generative and discriminative tasks, including image editing, image harmonization, image classification, face verification, landmark detection, and layout prediction. Extensive qualitative and quantitative experimental results demonstrate the appealing performance of GH-Feat.",0
"This abstract summarizes our work on using generative hierarchies of features to improve image synthesis methods by explicitly encoding knowledge about object structure and appearance. We use variational autoencoders with attention mechanisms to learn these feature hierarchies and show how they can help guide both local details (e.g., texture) and global shape. By incorporating prior knowledge into the training process, we demonstrate that our approach leads to more realistic images while requiring fewer training examples than previous methods. Our results suggest promising directions for future research at the intersection of vision and language processing. Keywords: Variational Autoencoder; Attention Mechanism; Image Synthesis; Generative Models; Deep Learning",1
"The task of image generation started to receive some attention from artists and designers to inspire them in new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control on them. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user choice by performing several optimization steps to recover optimal parameters from the model's latent space. We tested several exploration methods starting with classic gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so that they can even be used without numerical criterion, without inspirational image, but with only with human preference. Thus, by iterating on one's preferences we could make robust Facial Composite or Fashion Generation algorithms. High resolution of the produced design generations are obtained using progressive growing of GANs. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.",0
"""Image generation has seen tremendous progress through advances in generative models such as GANs (Generative Adversarial Networks). However, generating images that possess a high level of creativity and inspire human emotions remains challenging. This paper presents a novel framework called Inspirational Adversarial Image Generation (IAIG) which focuses on generating highly creative and inspirational images by incorporating two adversarial objectives - content preservation and perceptual fidelity."" \par \hangindent=2em ""The proposed IAIG framework comprises three main components: encoder network, generator network and discriminator network. We train these networks using a multi-task learning approach wherein the image generator generates novel images while preserving salient features from input images and improving their visual quality. Our contributions include evaluations of generated images based on both objective metrics and subjective user studies.""  \par \hangindent=2em  ""In summary, our framework introduces new possibilities for creating visually stunning images that evoke powerful feelings. The use cases include digital art, entertainment media, marketing materials, and more. With future work expanding upon our current research, we envision even greater strides towards developing intelligent systems capable of creating truly inspired works of art and design."" \par Please note that this is an example only. Abstract should have less than 150 words long depending on your target journal/conference. Also you need to cite any related papers or previous works.",1
"The interest of the deep learning community in image synthesis has grown massively in recent years. Nowadays, deep generative methods, and especially Generative Adversarial Networks (GANs), are leading to state-of-the-art performance, capable of synthesizing images that appear realistic. While the efforts for improving the quality of the generated images are extensive, most attempts still consider the generator part as an uncorroborated ""black-box"". In this paper, we aim to provide a better understanding and design of the image generation process. We interpret existing generators as implicitly relying on sparsity-inspired models. More specifically, we show that generators can be viewed as manifestations of the Convolutional Sparse Coding (CSC) and its Multi-Layered version (ML-CSC) synthesis processes. We leverage this observation by explicitly enforcing a sparsifying regularization on appropriately chosen activation layers in the generator, and demonstrate that this leads to improved image synthesis. Furthermore, we show that the same rationale and benefits apply to generators serving inverse problems, demonstrated on the Deep Image Prior (DIP) method.",0
"Improving image generation through sparse modeling techniques has become increasingly important due to their ability to efficiently represent complex data sets. This method utilizes statistical models that capture important patterns within large datasets while ignoring irrelevant details. By using these models, we can improve our understanding of natural phenomena by identifying key features that influence observed outcomes. In turn, this enables more accurate predictions and simulations, enabling new discoveries across many fields. Ultimately, sparse representation offers a powerful toolset for researchers working on data-intensive projects, allowing them to make sense of vast amounts of information quickly and effectively.",1
"The significant progress on Generative Adversarial Networks (GANs) has facilitated realistic single-object image generation based on language input. However, complex-scene generation (with various interactions among multiple objects) still suffers from messy layouts and object distortions, due to diverse configurations in layouts and appearances. Prior methods are mostly object-driven and ignore their inter-relations that play a significant role in complex-scene images. This work explores relationship-aware complex-scene image generation, where multiple objects are inter-related as a scene graph. With the help of relationships, we propose three major updates in the generation framework. First, reasonable spatial layouts are inferred by jointly considering the semantics and relationships among objects. Compared to standard location regression, we show relative scales and distances serve a more reliable target. Second, since the relations between objects significantly influence an object's appearance, we design a relation-guided generator to generate objects reflecting their relationships. Third, a novel scene graph discriminator is proposed to guarantee the consistency between the generated image and the input scene graph. Our method tends to synthesize plausible layouts and objects, respecting the interplay of multiple objects in an image. Experimental results on Visual Genome and HICO-DET datasets show that our proposed method significantly outperforms prior arts in terms of IS and FID metrics. Based on our user study and visual inspection, our method is more effective in generating logical layout and appearance for complex-scenes.",0
"In recent years, generative models such as Generative Adversarial Networks (GAN) have shown remarkable performance in generating high quality images by learning from large amounts of data. However, one major drawback of these methods is that they require enormous computational resources and can take days or even weeks of training time on powerful GPU clusters. In contrast, our approach builds upon recent developments in computer vision which focus on using pretrained convolutional neural networks to learn relationships between two domains, enabling efficient image generation without requiring massive datasets or intensive computation. Our method leverages these relationship learning techniques together with established GAN architectures, allowing us to generate complex scenes far more efficiently than traditional GAN approaches while maintaining comparable levels of visual fidelity. We evaluate our model through extensive experiments, demonstrating significant improvements over existing state-of-the-art methods in terms of both runtime efficiency and generated image quality. We believe our work opens up exciting opportunities for future research into exploiting domain knowledge to further enhance the capabilities of image generation models.",1
"Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. This leads to impressive 3D consistency, but incorporating such a bias comes at a price: the camera needs to be modeled as well. Current approaches assume fixed intrinsics and a predefined prior over camera pose ranges. As a result, parameter tuning is typically required for real-world data, and results degrade if the data distribution is not matched. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.",0
"This work presents Composition Aware Modular Pixar Animation Research (CAMPARI), which utilizes modularity to improve renderer efficiency while maintaining visual quality. CAMPARI introduces three key components that enable efficient rendering while ensuring high visual fidelity: Camera Adaptive Rasterization (CARAF); Multi-Frame Probing Sampler (MFPS); and Material Synthesis Network (MSynth). CARAF enables adaptivity based on view angle and resolution through rasterizing at variable granularity, making rendering more lightweight. MFPS improves sampling by enabling multi-frame rendering during sampling time and reducing sample density. MSynth simplifies material creation by training a neural network model for predictive synthesis of materials from semantic input attributes. Using these components, we achieve physically accurate renders efficiently using only commodity hardware. Our framework enables artists to focus more on composition rather than individual scene elements, allowing them to create better content faster. We demonstrate the effectiveness of our method using multiple examples, including complex scenes such as destruction simulations, subsurface scattering, and refraction effects. Overall, our approach provides new ways to enhance realism without compromising performance.",1
"Generative adversarial networks (GANs) have shown impressive results in both unconditional and conditional image generation. In recent literature, it is shown that pre-trained GANs, on a different dataset, can be transferred to improve the image generation from a small target data. The same, however, has not been well-studied in the case of conditional GANs (cGANs), which provides new opportunities for knowledge transfer compared to unconditional setup. In particular, the new classes may borrow knowledge from the related old classes, or share knowledge among themselves to improve the training. This motivates us to study the problem of efficient conditional GAN transfer with knowledge propagation across classes. To address this problem, we introduce a new GAN transfer method to explicitly propagate the knowledge from the old classes to the new classes. The key idea is to enforce the popularly used conditional batch normalization (BN) to learn the class-specific information of the new classes from that of the old classes, with implicit knowledge sharing among the new ones. This allows for an efficient knowledge propagation from the old classes to the new ones, with the BN parameters increasing linearly with the number of new classes. The extensive evaluation demonstrates the clear superiority of the proposed method over state-of-the-art competitors for efficient conditional GAN transfer tasks. The code is available at: https://github.com/mshahbazi72/cGANTransfer",0
"The advent of deep generative models has opened new possibilities for image generation and manipulation tasks, but the difficulty in training them remains a challenge. In practice, fine-tuning these models requires large amounts of labeled data from similar domains and datasets which may not always be available or feasible to collect. To address this issue, we propose a method that transfers knowledge across classes by efficiently propagating information learned on one dataset to another through conditional Generative Adversarial Networks (cGAN). Our approach introduces a novel feature that allows cGANs to preserve relevant features that have been previously used in other classifiers while still capturing the underlying structure present within each domain. Additionally, our model utilizes unlabeled images as additional training data, further enhancing performance without incurring significant computational costs. We evaluate our model using several benchmark datasets, demonstrating improved performance compared to state-of-the-art methods, including superior classification accuracy and more effective learning capacity under limited supervision settings. This research advances the field of deep generative models by providing a powerful tool for transferring knowledge across classes and significantly reducing reliance on costly human annotation efforts.",1
"Person image synthesis, e.g., pose transfer, is a challenging problem due to large variation and occlusion. Existing methods have difficulties predicting reasonable invisible regions and fail to decouple the shape and style of clothing, which limits their applications on person image editing. In this paper, we propose PISE, a novel two-stage generative model for Person Image Synthesis and Editing, which is able to generate realistic person images with desired poses, textures, or semantic layouts. For human pose transfer, we first synthesize a human parsing map aligned with the target pose to represent the shape of clothing by a parsing generator, and then generate the final image by an image generator. To decouple the shape and style of clothing, we propose joint global and local per-region encoding and normalization to predict the reasonable style of clothing for invisible regions. We also propose spatial-aware normalization to retain the spatial context relationship in the source image. The results of qualitative and quantitative experiments demonstrate the superiority of our model on human pose transfer. Besides, the results of texture transfer and region editing show that our model can be applied to person image editing.",0
"Imagine you have a large dataset containing thousands of images that are labeled as belonging to one of two classes (e.g., ""person"" vs ""not person""). Now imagine you want to create new images that look like they belong to those classes, but contain specific features that were never seen in any image before - perhaps your own likeness, or another celebrity. This can be useful in many ways. For example, you could make memes that combine familiar faces with silly scenarios; you could use them as placeholders for people who haven't provided photos yet; or you could generate novel artwork using techniques inspired by artists such as Warhol. You could even use this technology to build training sets for other machine learning algorithms! Of course, there are also many potential abuses of such capabilities... But we will ignore such concerns here and instead focus on technical details. How might we achieve this synthesis task? We propose a decoupling approach based on Generative Adversarial Networks (GANs). As with most recent advances in generative modeling, we rely heavily on latent variable models - specifically Variational Autoencoders (VAEs) and Normalizing Flows (NFs), which map data points from observation space into a continuous latent distribution. Firstly, we present a method called Personalized Image Synthesis Using Variational Autoencoders (PISAVAE). The key idea is that VAEs learn powerful representations by minimizing reconstruction loss on training data, so directly sampling parameters drawn from the posterior over these learned factors should yield plausible novel images. However, without modifications, such samples would typically lie within the range of variability found during training - e.g., images synthesized by simply ""randomly erasing"" objects in the style of Pix2pixHD. To incorporate desired features/attributes, we need tighter couplings between generator output and specified goals. Here we opt f",1
"Image generation has rapidly evolved in recent years. Modern architectures for adversarial training allow to generate even high resolution images with remarkable quality. At the same time, more and more effort is dedicated towards controlling the content of generated images. In this paper, we take one further step in this direction and propose a conditional generative adversarial network (GAN) that generates images with a defined number of objects from given classes. This entails two fundamental abilities (1) being able to generate high-quality images given a complex constraint and (2) being able to count object instances per class in a given image. Our proposed model modularly extends the successful StyleGAN2 architecture with a count-based conditioning as well as with a regression sub-network to count the number of generated objects per class during training. In experiments on three different datasets, we show that the proposed model learns to generate images according to the given multiple-class count condition even in the presence of complex backgrounds. In particular, we propose a new dataset, CityCount, which is derived from the Cityscapes street scenes dataset, to evaluate our approach in a challenging and practically relevant scenario.",0
"""The problem of multi-class multi-instance count conditioned adversarial image generation has been an important challenge in the field of computer vision, particularly in generating images that meet specific conditions while maintaining their quality. In this work, we propose a novel approach that addresses these issues by using a conditional generative adversarial network (cGAN). Our method involves training two discriminators: one to evaluate the realism of generated images and another to evaluate whether the generated images match specified conditions. Additionally, our algorithm employs a recurrent attention module that allows for better management of objects within images. Experimental results demonstrate that our proposed method outperforms state-of-the-art methods in terms of both visual fidelity and matching desired conditions. We believe that our method holds great potential for applications such as image completion and generating realistic scenes from text descriptions.""",1
"Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fr\'{e}chet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID).",0
"Artificial intelligence (AI) has been on the rise over the past decade, leading researchers and practitioners alike to explore new ways to train deep learning models using Generative Adversarial Networks (GANs). However, training stable and high-quality GANs remains challenging due to issues such as mode collapse, lack of control, unsteady convergence, vanishing gradients, instability, etc. Existing solutions have attempted to address these problems but still fall short. In light of this challenge, we introduce two new losses and architectures that work together seamlessly to improve the generation performance, stability, controllability, and interpretability of GANs, which would enable exciting future applications such as art design tools, generative video editing systems, and realistic data synthesis. Using extensive experiments on benchmark datasets such as CIFAR-10/100 and ImageNet, our method demonstrates improved FID scores, reduced quantitative errors, increased visual quality, more diverse generated images, better localization maps, faster convergence speed, higher robustness, and other desirable properties compared to several state-of-the-art baselines. Our contributions can significantly benefit related research areas, including computer vision, natural language processing, robotics, autonomous vehicles, medical imaging analysis, game creation, virtual/augmented reality, and many others that require image-like data generation or manipulation tasks.",1
"Localized Narratives is a dataset with detailed natural language descriptions of images paired with mouse traces that provide a sparse, fine-grained visual grounding for phrases. We propose TReCS, a sequential model that exploits this grounding to generate images. TReCS uses descriptions to retrieve segmentation masks and predict object labels aligned with mouse traces. These alignments are used to select and position masks to generate a fully covered segmentation canvas; the final image is produced by a segmentation-to-image generator using this canvas. This multi-step, retrieval-based approach outperforms existing direct text-to-image generation models on both automatic metrics and human evaluations: overall, its generated images are more photo-realistic and better match descriptions.",0
"Recent advances in deep learning have enabled the development of powerful text-to-image generation models that can synthesize high quality images from natural language descriptions. However, most existing methods lack fine-grained control over generated outputs, resulting in inconsistent visualizations that may fail to capture specific details specified in the input text. In order to overcome these limitations, we propose a novel framework that integrates fine-grained user attention into the image generation process. Our method enables users to specify which regions or objects they want to focus on during image generation, allowing for more precise manipulation of the output imagery. We demonstrate the effectiveness of our approach through extensive experiments on challenging benchmark datasets, showing improved performance across various metrics compared to state-of-the-art alternatives. By providing both quantitative and qualitative analyses of the results, we highlight the key benefits of incorporating fine-grained attention into text-to-image generation tasks. Overall, our work contributes to the growing field of computer vision research, pushing forward the boundaries of creative applications based on artificial intelligence systems.",1
"The moments (a.k.a., mean and standard deviation) of latent features are often removed as noise when training image recognition models, to increase stability and reduce training time. However, in the field of image generation, the moments play a much more central role. Studies have shown that the moments extracted from instance normalization and positional normalization can roughly capture style and shape information of an image. Instead of being discarded, these moments are instrumental to the generation process. In this paper we propose Moment Exchange, an implicit data augmentation method that encourages the model to utilize the moment information also for recognition models. Specifically, we replace the moments of the learned features of one training image by those of another, and also interpolate the target labels -- forcing the model to extract training signal from the moments in addition to the normalized features. As our approach is fast, operates entirely in feature space, and mixes different signals than prior methods, one can effectively combine it with existing augmentation approaches. We demonstrate its efficacy across several recognition benchmark data sets where it improves the generalization capability of highly competitive baseline networks with remarkable consistency.",0
"In this paper we explore two important problems in machine learning: feature normalization and data augmentation. We first discuss how these techniques can improve model performance by reducing overfitting and increasing training efficiency. Next, we describe recent advances in each area and provide examples from real-world applications. Finally, we offer future research directions that could potentially further enhance these methods and lead to even better results. Overall, our aim is to highlight the critical role that these simple yet powerful tools play in achieving high accuracy and robustness in modern machine learning models.",1
"Face photo-sketch synthesis and recognition has many applications in digital entertainment and law enforcement. Recently, generative adversarial networks (GANs) based methods have significantly improved the quality of image synthesis, but they have not explicitly considered the purpose of recognition. In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model that applies a new perceptual loss to supervise the image generation network. It improves CycleGAN on photo-sketch synthesis by paying more attention to the synthesis of key facial regions, such as eyes and nose, which are important for identity recognition. Furthermore, we develop a mutual optimization procedure between the synthesis model and the recognition model, which iteratively synthesizes better images by IACycleGAN and enhances the recognition model by the triplet loss of the generated and real samples. Extensive experiments are performed on both photo-tosketch and sketch-to-photo tasks using the widely used CUFS and CUFSF databases. The results show that the proposed method performs better than several state-of-the-art methods in terms of both synthetic image quality and photo-sketch recognition accuracy.",0
"This should summarize your research without providing any specific details related to implementation techniques used by you. Be creative in coming up with a description and focus more on the high level ideas behind the work, rather than how you went ahead implementing them.  Identity-aware image synthesis has recently emerged as a promising direction for enhancing face recognition systems. In this study, we propose an identity-aware cycle-consistent adversarial network (cycleGAN) that can generate photo-like images from sketches while preserving the unique characteristics of each individual. Our approach utilizes two discriminators - one for evaluating realness and another for enforcing identity consistency - and employs an attention mechanism that enables our model to selectively focus on identity-salient features during training. We evaluate our method using multiple datasets, demonstrating significant improvements over previous methods for both tasks. Overall, our work represents an important step towards building more robust face recognition systems that can effectively handle variations in appearance.",1
"While Generative Adversarial Networks (GANs) show increasing performance and the level of realism is becoming indistinguishable from natural images, this also comes with high demands on data and computation. We show that state-of-the-art GAN models -- such as they are being publicly released by researchers and industry -- can be used for a range of applications beyond unconditional image generation. We achieve this by an iterative scheme that also allows gaining control over the image generation process despite the highly non-linear latent spaces of the latest GAN models. We demonstrate that this opens up the possibility to re-use state-of-the-art, difficult to train, pre-trained GANs with a high level of control even if only black-box access is granted. Our work also raises concerns and awareness that the use cases of a published GAN model may well reach beyond the creators' intention, which needs to be taken into account before a full public release. Code is available at https://github.com/a514514772/hijackgan.",0
"Recent advances in generative adversarial networks (GANs) have resulted in the development of powerful models capable of generating high quality images that are difficult to distinguish from real ones. However, these pretrained black-box GANs can pose security risks if they fall into the wrong hands. In our research, we demonstrate how hijacking attacks on GANs can result in the creation of malicious content such as deepfakes or synthetic identity generation. We show that by carefully analyzing the training process and leveraging transfer learning techniques, attackers can effectively use GANs for unintended purposes without access to the source code or the underlying data. Our work highlights the urgent need for effective countermeasures against potential threats posed by advanced machine learning algorithms.",1
"The first step toward Seed Phenotyping i.e. the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of pa-rameters that form more complex traits is the identification of seed type. Generally, a plant re-searcher inspects the visual attributes of a seed such as size, shape, area, color and texture to identify the seed type, a process that is tedious and labor-intensive. Advances in the areas of computer vision and deep learning have led to the development of convolutional neural networks (CNN) that aid in classification using images. While they classify efficiently, a key bottleneck is the need for an extensive amount of labelled data to train the CNN before it can be put to the task of classification. The work leverages the concepts of Contrastive Learning and Domain Randomi-zation in order to achieve the same. Briefly, domain randomization is the technique of applying models trained on images containing simulated objects to real-world objects. The use of synthetic images generated from a representational sample crop of real-world images alleviates the need for a large volume of test subjects. As part of the work, synthetic image datasets of five different types of seed images namely, canola, rough rice, sorghum, soy and wheat are applied to three different self-supervised learning frameworks namely, SimCLR, Momentum Contrast (MoCo) and Build Your Own Latent (BYOL) where ResNet-50 is used as the backbone in each of the networks. When the self-supervised models are fine-tuned with only 5% of the labels from the synthetic dataset, results show that MoCo, the model that yields the best performance of the self-supervised learning frameworks in question, achieves an accuracy of 77% on the test dataset which is only ~13% less than the accuracy of 90% achieved by ResNet-50 trained on 100% of the labels.",0
"In recent years, deep learning has become increasingly popular as a tool for image classification tasks due to its impressive accuracy and ability to learn from large amounts of data. However, training such models requires vast datasets and significant computational resources, making them difficult to use in domains where labeled data is limited or expensive to obtain. To overcome these challenges, self-supervised learning (SSL) methods have been proposed that utilize unlabeled data to train models by solving pretext tasks. These tasks are designed to extract meaningful features that can then be used for downstream tasks such as classification. One approach to SSL is domain randomization (DR), which involves adding random noise or perturbations to the environment during training to increase robustness and generalizability of learned representations. In this work, we explore how DR can be applied to seed classification, a common problem in agriculture and food science. We propose a framework that combines DR with SSL to create a model capable of accurately classifying seeds under varying conditions. Our experimental results demonstrate the effectiveness of our method, achieving significantly higher accuracy than other state-of-the-art SSL approaches. Additionally, we showcase the potential application of our method in real world scenarios such as quality control inspections and sorting seeds for breeding programs. Overall, our study contributes to the growing field of SSL and highlights the promise of DR for improving performance in challenging environments where labeled data may be scarce.",1
"Although Generative Adversarial Networks (GANs) are successfully applied to diverse fields, training GANs on synthetic aperture radar (SAR) data is a challenging task mostly due to speckle noise. On the one hands, in a learning perspective of human's perception, it is natural to learn a task by using various information from multiple sources. However, in the previous GAN works on SAR target image generation, the information on target classes has only been used. Due to the backscattering characteristics of SAR image signals, the shapes and structures of SAR target images are strongly dependent on their pose angles. Nevertheless, the pose angle information has not been incorporated into such generative models for SAR target images. In this paper, we firstly propose a novel GAN-based multi-task learning (MTL) method for SAR target image generation, called PeaceGAN that uses both pose angle and target class information, which makes it possible to produce SAR target images of desired target classes at intended pose angles. For this, the PeaceGAN has two additional structures, a pose estimator and an auxiliary classifier, at the side of its discriminator to combine the pose and class information more efficiently. In addition, the PeaceGAN is jointly learned in an end-to-end manner as MTL with both pose angle and target class information, thus enhancing the diversity and quality of generated SAR target images The extensive experiments show that taking an advantage of both pose angle and target class learning by the proposed pose estimator and auxiliary classifier can help the PeaceGAN's generator effectively learn the distributions of SAR target images in the MTL framework, so that it can better generate the SAR target images more flexibly and faithfully at intended pose angles for desired target classes compared to the recent state-of-the-art methods.",0
"Abstract:  In recent years, Synthetic Aperture Radar (SAR) imagery has become increasingly important due to its ability to capture high resolution images even during adverse weather conditions. However, generating realistic SAR target images remains challenging as it requires both good signal processing algorithms and accurate prior knowledge of scene statistics.  To address this challenge, we propose a novel deep learning based method called PeaceGAN that utilizes Generative Adversarial Networks (GANs) to generate highly realistic synthetic SAR target images. Unlike traditional approaches, our approach uses multi-task learning framework which allows the generator network to learn more efficient representations by explicitly enforcing joint supervision from discriminators, feature extractors, pose estimators and auxiliary classifiers. We show that this leads to improved image quality and better generalization performance compared to previous methods.  Additionally, we present a novel pose estimation system which estimates relative azimuth angle and slant range distance of each pixel in the generated image using an encoder network. This is done so that further applications can take advantage of these features such as object detection, segmentation, tracking etc. Lastly, we have implemented an auxiliary binary classification task alongside the pose estimator to improve overall stability of training process and confidence in predictions.  Experimental results on a publicly available dataset demonstrate the effectiveness of our proposed approach achieving state-of-the art accuracy against other benchmarking methods and significantly outperforming them under less number of iterations, parameters and computational resources. Our model shows promising improvements in generating high resolution SAR target images which could potentially have broad impacts across many areas including but not limited to defense, urban planning, disaster management and environmental monitoring. Overall, PeaceGAN provides an effective solution for generating synthetic SAR data which is crucial towards developing next generation radars systems.",1
"Generative Adversarial Networks (GANs) advance face synthesis through learning the underlying distribution of observed data. Despite the high-quality generated faces, some minority groups can be rarely generated from the trained models due to a biased image generation process. To study the issue, we first conduct an empirical study on a pre-trained face synthesis model. We observe that after training the GAN model not only carries the biases in the training data but also amplifies them to some degree in the image generation process. To further improve the fairness of image generation, we propose an interpretable baseline method to balance the output facial attributes without retraining. The proposed method shifts the interpretable semantic distribution in the latent space for a more balanced image generation while preserving the sample diversity. Besides producing more balanced data regarding a particular attribute (e.g., race, gender, etc.), our method is generalizable to handle more than one attribute at a time and synthesize samples of fine-grained subgroups. We further show the positive applicability of the balanced data sampled from GANs to quantify the biases in other face recognition systems, like commercial face attribute classifiers and face super-resolution algorithms.",0
"In recent years, deep generative models have become increasingly popular due to their ability to generate realistic data samples. However, these models can suffer from unfairness issues which may lead to discriminatory outcomes. Retraining is one common approach used to address fairness issues but can be computationally expensive. This paper proposes two methods that improve the fairness of deep generative models without retraining by adding regularization terms to the objective function. The first method adds a disparity measure term which promotes equitable distribution of errors across different groups. The second method uses a novel group-specific feature importance scoring technique to adjust model parameters based on group characteristics such as race or gender. Results show significant improvements in fairness metrics while maintaining high quality generated samples compared to baseline models. These methods provide practitioners with simple yet effective solutions for improving fairness in deep generative models without resorting to costly retraining.",1
"In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module maps real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity learns the text-image matching by mapping the image and text into a common embedding space. The instance-level optimization is for identity preservation in manipulation. Our model can produce diverse and high-quality images with an unprecedented resolution at 1024. Using a control mechanism based on style-mixing, our TediGAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels, with or without instance guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.",0
"""This paper presents TediGAN, a text-guided image generation model capable of producing diverse and visually plausible face images based on natural language input. Unlike existing approaches that generate faces from scratch, our method incorporates a pretrained face recognition network as a guide to ensure high-fidelity results. Furthermore, we introduce several novel techniques such as latent space manipulation and style mixing to increase the diversity and controllability of generated images. Our experiments demonstrate the effectiveness of these components, achieving state-of-the-art performance across multiple metrics including human evaluation.""  TediGAN builds upon recent advances in generative adversarial networks (GANs) by leveraging external knowledge provided via text prompts. In addition to producing realistic synthetic face images, this approach enables fine-grained control over specific facial attributes, enabling applications ranging from virtual try-on to content creation and beyond. Moreover, our work provides valuable insights into understanding the complex relationship between semantic features and visual representations, with potential implications for both computer vision and NLP research communities. Overall, we believe TediGAN represents a significant step towards making high-quality, data-driven models more accessible and interpretable to non-experts.",1
"The conditional generative adversarial network (cGAN) is a powerful tool of generating high-quality images, but existing approaches mostly suffer unsatisfying performance or the risk of mode collapse. This paper presents Omni-GAN, a variant of cGAN that reveals the devil in designing a proper discriminator for training the model. The key is to ensure that the discriminator receives strong supervision to perceive the concepts and moderate regularization to avoid collapse. Omni-GAN is easily implemented and freely integrated with off-the-shelf encoding methods (e.g., implicit neural representation, INR). Experiments validate the superior performance of Omni-GAN and Omni-INR-GAN in a wide range of image generation and restoration tasks. In particular, Omni-INR-GAN sets new records on the ImageNet dataset with impressive Inception scores of 262.85 and 343.22 for the image sizes of 128 and 256, respectively, surpassing the previous records by 100+ points. Moreover, leveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution images to arbitrary resolution, even up to x60+ higher resolution. Code is available.",0
"Title: Uncovering the Mysteries of Generative Adversarial Networks: An Exploration into Omni-GANs and Beyond  Generative adversarial networks (GANs) have revolutionized the field of computer graphics and image synthesis by generating high-quality images that often surpass human capabilities. However, their training remains challenging due to instability issues such as mode collapse, lack of diversity, or oversmoothing artifacts. Recent advancements in GAN research led to conditional GANs (cGANs), which improve stability and enable controllable generation tasks. Yet, these methods still suffer from constraints related to the limited expressivity of standard generators, discriminators, and loss functions used during training. To address these limitations, we propose Omni-GAN - a novel architecture that unifies diverse state-of-the-art techniques to overcome the common pitfalls observed in traditional GAN systems while improving performance on several benchmark datasets. Our work provides an extensive analysis of GAN behavior and identifies specific design principles required to achieve excellent results across different architectures. We hope that our findings contribute to future advances in the development and understanding of powerful GAN models.",1
"This paper proposes a method to extract the position and pose of vehicles in the 3D world from a single traffic camera. Most previous monocular 3D vehicle detection algorithms focused on cameras on vehicles from the perspective of a driver, and assumed known intrinsic and extrinsic calibration. On the contrary, this paper focuses on the same task using uncalibrated monocular traffic cameras. We observe that the homography between the road plane and the image plane is essential to 3D vehicle detection and the data synthesis for this task, and the homography can be estimated without the camera intrinsics and extrinsics. We conduct 3D vehicle detection by estimating the rotated bounding boxes (r-boxes) in the bird's eye view (BEV) images generated from inverse perspective mapping. We propose a new regression target called \textit{tailed~r-box} and a \textit{dual-view} network architecture which boosts the detection accuracy on warped BEV images. Experiments show that the proposed method can generalize to new camera and environment setups despite not seeing imaged from them during training.",0
"This paper presents a method for detecting vehicles using monocular images taken from uncalibrated traffic cameras through homography techniques. We propose a novel approach that combines feature extraction methods such as corner detection and contour analysis to improve vehicle detection performance under challenging lighting conditions and occlusions. Our algorithm utilizes efficient image processing techniques, including adaptive thresholding and morphological operations, to accurately segment cars and estimate their dimensions. Experimental results on real-world datasets demonstrate the effectiveness of our proposed method compared to state-of-the-art approaches, achieving high precision and recall rates even when dealing with large variations in illumination, scale, and pose changes across different scenarios. Furthermore, we present detailed ablation studies to evaluate the contributions of each component in our framework, emphasizing the key factors that contribute to successful monocular 3D vehicle detection using homography. Overall, our work represents an important contribution towards enabling accurate and reliable vehicle detection for autonomous systems operating under diverse environmental constraints.",1
"While generative adversarial networks (GANs) can successfully produce high-quality images, they can be challenging to control. Simplifying GAN-based image generation is critical for their adoption in graphic design and artistic work. This goal has led to significant interest in methods that can intuitively control the appearance of images generated by GANs. In this paper, we present HistoGAN, a color histogram-based method for controlling GAN-generated images' colors. We focus on color histograms as they provide an intuitive way to describe image color while remaining decoupled from domain-specific semantics. Specifically, we introduce an effective modification of the recent StyleGAN architecture to control the colors of GAN-generated images specified by a target color histogram feature. We then describe how to expand HistoGAN to recolor real images. For image recoloring, we jointly train an encoder network along with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised approach trained to encourage the network to keep the original image's content while changing the colors based on the given target histogram. We show that this histogram-based approach offers a better way to control GAN-generated and real images' colors while producing more compelling results compared to existing alternative strategies.",0
"This is a short research paper aimed at describing our recent work on controlling colors of GAN generated images. We propose a new method that allows us to match real image color histogram distribution with fake ones from GAN. Our approach uses simple operations like adding noise, subtracting noise, scaling and shifting values of both RGB channels separately, which can be easily incorporated into existing models. Evaluation shows that we have improved FID score significantly compared to CycleGAN as well as improving visual quality by matching histogram distributions. Furthermore, we showcase an easy way to control colors without changing other aspects of generated images. Finally, we provide detailed analysis of the results showing effectiveness of proposed methods.",1
"Modern neural networks have been successful in many regression-based tasks such as face recognition, facial landmark detection, and image generation. In this work, we investigate an intuitive but understudied characteristic of modern neural networks, namely, the nonsmoothness. The experiments using synthetic data confirm that such operations as ReLU and max pooling in modern neural networks lead to nonsmoothness. We quantify the nonsmoothness using a feature named the sum of the magnitude of peaks (SMP) and model the input-output relationships for building blocks of modern neural networks. Experimental results confirm that our model can accurately predict the statistical behaviors of the nonsmoothness as it propagates through such building blocks as the convolutional layer, the ReLU activation, and the max pooling layer. We envision that the nonsmoothness feature can potentially be used as a forensic tool for regression-based applications of neural networks.",0
"In recent years there has been increasing interest in studying neural networks beyond their local linearity properties. One particularly relevant piece of work, by He, et al., shows that even simple smooth activation functions can lead to nonsmooth network behavior under mild conditions on data geometry (He et al., 2019). This suggests that models which assume differentiability at all points — such as backpropagation and most model-based training techniques — may miss important aspects of how modern neural networks actually behave. Our goal here is to provide additional mathematical insights into why we might expect neural networks trained using popular architectures like ResNet to exhibit nonsmoothness, focusing specifically on properties of gradient descent optimization on overparameterized problems. We argue that several key features commonly found in these systems, including batch normalization layers, skip connections, and rectified nonlinearities, interact in ways that make nonsmoothness more likely both during training and later at deployment time. By characterizing how common components influence optimization dynamics and loss landscapes, we hope our results better equip researchers to reason about what kinds of nonsmooth behaviors may arise in specific settings and design methods accordingly. To study this problem mathematically, we develop new convergence theory for stochastic gradient descent incorporating some of the structured randomness present in modern deep learning frameworks. This analysis yields explicit bounds revealing interesting tradeoffs among hyperparameters controlling batch size and architecture depth; we use those bounds to gain insight into regimes where nonsmooth solutions become generic rather than exceptions. While we focus mainly on continuous-time dynamics due to analytical convenience, careful experiments demonstrate how many of our conclusions carry ove",1
"We present a new method for few-shot human motion transfer that achieves realistic human image generation with only a small number of appearance inputs. Despite recent advances in single person motion transfer, prior methods often require a large number of training images and take long training time. One promising direction is to perform few-shot human motion transfer, which only needs a few of source images for appearance transfer. However, it is particularly challenging to obtain satisfactory transfer results. In this paper, we address this issue by rendering a human texture map to a surface geometry (represented as a UV map), which is personalized to the source person. Our geometry generator combines the shape information from source images, and the pose information from 2D keypoints to synthesize the personalized UV map. A texture generator then generates the texture map conditioned on the texture of source images to fill out invisible parts. Furthermore, we may fine-tune the texture map on the manifold of the texture generator from a few source images at the test time, which improves the quality of the texture map without over-fitting or artifacts. Extensive experiments show the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively. Our code is available at https://github.com/HuangZhiChao95/FewShotMotionTransfer.",0
"In this paper we explore few shot human motion transfer using personalized geometry and texture modeling. We use deep learning techniques to train our system on large datasets containing examples of movements performed by different individuals. Our approach involves pre-training the model on general movement data before fine tuning it using small amounts of specific user data. By doing so, we are able to quickly adapt our model to new users and generate realistic and customised animations from very little input. Through extensive testing, we demonstrate that our method outperforms existing state-of-the-art methods and achieves high levels of accuracy even with only one example per action class. Additionally, we showcase several applications of our system such as virtual reality avatars and gaming characters. Overall, our work presents a novel approach to few shot human motion transfer which has promising potential in many areas of computer graphics and animation.",1
"The recently introduced introspective variational autoencoder (IntroVAE) exhibits outstanding image generations, and allows for amortized inference using an image encoder. The main idea in IntroVAE is to train a VAE adversarially, using the VAE encoder to discriminate between generated and real data samples. However, the original IntroVAE loss function relied on a particular hinge-loss formulation that is very hard to stabilize in practice, and its theoretical convergence analysis ignored important terms in the loss. In this work, we take a step towards better understanding of the IntroVAE model, its practical implementation, and its applications. We propose the Soft-IntroVAE, a modified IntroVAE that replaces the hinge-loss terms with a smooth exponential loss on generated samples. This change significantly improves training stability, and also enables theoretical analysis of the complete algorithm. Interestingly, we show that the IntroVAE converges to a distribution that minimizes a sum of KL distance from the data distribution and an entropy term. We discuss the implications of this result, and demonstrate that it induces competitive image generation and reconstruction. Finally, we describe two applications of Soft-IntroVAE to unsupervised image translation and out-of-distribution detection, and demonstrate compelling results. Code and additional information is available on the project website -- https://taldatech.github.io/soft-intro-vae-web",0
"In recent years, deep generative models have become increasingly popular due to their ability to generate realistic data samples, which can then be used for tasks such as image generation, completion, and augmentation. One family of generative models that has gained significant attention is variational autoencoders (VAEs). These models use an encoder network to map high-dimensional input data into lower-dimensional latent space representations, and then decode those latent spaces back into reconstructed outputs. However, VAEs suffer from some limitations, including difficulties in exploring new modes, limited sample quality, and poor calibration of uncertainty estimates. To address these issues, we propose soft-introspective VAEs (soft-IVAs), which integrate introspection mechanisms into the training process. This allows us to better analyze and improve the behavior of the model during training by directly inspecting the distribution over latent codes at each step. Our approach outperforms standard VIAEs on several benchmark datasets, achieving better reconstruction accuracy and more accurate uncertainty estimation. We demonstrate that our method improves sample quality and allows efficient evaluation of important metrics such as mode coverage and KL divergence. Overall, our work advances the state-of-the-art in generative modelling by providing a novel approach for understanding and optimizing the behavior of VAEs.",1
"Synthesizing high-quality realistic images from text descriptions is a challenging task. Almost all existing text-to-image Generative Adversarial Networks employ stacked architecture as the backbone. They utilize cross-modal attention mechanisms to fuse text and image features, and introduce extra networks to ensure text-image semantic consistency. In this work, we propose a much simpler, but more effective text-to-image model than previous works. Corresponding to the above three limitations, we propose: 1) a novel one-stage text-to-image backbone which is able to synthesize high-quality images directly by one pair of generator and discriminator, 2) a novel fusion module called deep text-image fusion block which deepens the text-image fusion process in generator, 3) a novel target-aware discriminator composed of matching-aware gradient penalty and one-way output which promotes the generator to synthesize more realistic and text-image semantic consistent images without introducing extra networks. Compared with existing text-to-image models, our proposed method (i.e., DF-GAN) is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance. Extensive experiments on both Caltech-UCSD Birds 200 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models.",0
"This paper presents a new approach for text-to-image synthesis using deep generative models called ""Deep Fusion Generative Adversarial Network"" (DF-GAN). We propose several novel techniques that improve upon traditional GAN architectures by introducing high-level guidance through semantic images, ensembling multiple networks trained on different levels of abstraction, and incorporating attention mechanisms to focus on important regions. We demonstrate the effectiveness of our method on two benchmark datasets - COCO and LSUN - achieving state-of-the-art performance on both qualitative metrics such as Inception Scores and Frechet Distance, as well as quantitative measures like mean opinion scores from human evaluations. Our framework provides interpretable results and can generate diverse and coherent images based on natural language descriptions. Overall, we believe that DF-GAN represents a significant advance in text-to-image synthesis research and has promising applications in areas such as computer vision, virtual reality, and art generation.",1
"We consider the task of photo-realistic unconditional image generation (generate high quality, diverse samples that carry the same visual content as the image) on mobile platforms using Generative Adversarial Networks (GANs). In this paper, we propose a novel approach to trade-off image generation accuracy of a GAN for the energy consumed (compute) at run-time called Scale-Energy Tradeoff GAN (SETGAN). GANs usually take a long time to train and consume a huge memory hence making it difficult to run on edge devices. The key idea behind SETGAN for an image generation task is for a given input image, we train a GAN on a remote server and use the trained model on edge devices. We use SinGAN, a single image unconditional generative model, that contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. During the training process, we determine the optimal number of scales for a given input image and the energy constraint from the target edge device. Results show that with SETGAN's unique client-server-based architecture, we were able to achieve a 56% gain in energy for a loss of 3% to 12% SSIM accuracy. Also, with the parallel multi-scale training, we obtain around 4x gain in training time on the server.",0
"This paper presents SETGAN, which stands for Scale and Energy Trade-Off Generative Adversarial Networks. SETGAN combines two key principles that enable efficient inference and deployment of generative models such as those used for images, including trade-offs involving scale. These architectural choices allow SETGAN to achieve superior accuracy at small model sizes and reduced computational costs compared to prior methods. Our contributions showcase how to optimize generative networks, improve performance via efficient training techniques, reduce compute requirements during testing on embedded platforms using mobile devices without sacrificing image quality. We demonstrate the effectiveness of our approach through experiments evaluating generation fidelity against state-of-the-art approaches across multiple datasets, showing improved performance in energy efficiency, latency, scalability and size. In conclusion, we provide further analysis examining how well these systems transfer over to real world use cases with limited computing resources.",1
"A layout to image (L2I) generation model aims to generate a complicated image containing multiple objects (things) against natural background (stuff), conditioned on a given layout. Built upon the recent advances in generative adversarial networks (GANs), existing L2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) the object-to-object as well as object-to-stuff relations are often broken and (2) each object's appearance is typically distorted lacking the key defining characteristics associated with the object class. We argue that these are caused by the lack of context-aware object and stuff feature encoding in their generators, and location-sensitive appearance representation in their discriminators. To address these limitations, two new modules are proposed in this work. First, a context-aware feature transformation module is introduced in the generator to ensure that the generated feature encoding of either object or stuff is aware of other co-existing objects/stuff in the scene. Second, instead of feeding location-insensitive image features to the discriminator, we use the Gram matrix computed from the feature maps of the generated object images to preserve location-sensitive information, resulting in much enhanced object appearance. Extensive experiments show that the proposed method achieves state-of-the-art performance on the COCO-Thing-Stuff and Visual Genome benchmarks.",0
