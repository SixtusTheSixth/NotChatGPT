"Compared to reinforcement learning, imitation learning (IL) is a powerful paradigm for training agents to learn control policies efficiently from expert demonstrations. However, in most cases, obtaining demonstration data is costly and laborious, which poses a significant challenge in some scenarios. A promising alternative is to train agent learning skills via imitation learning without expert demonstrations, which, to some extent, would extremely expand imitation learning areas. To achieve such expectation, in this paper, we propose Hindsight Generative Adversarial Imitation Learning (HGAIL) algorithm, with the aim of achieving imitation learning satisfying no need of demonstrations. Combining hindsight idea with the generative adversarial imitation learning (GAIL) framework, we realize implementing imitation learning successfully in cases of expert demonstration data are not available. Experiments show that the proposed method can train policies showing comparable performance to current imitation learning methods. Further more, HGAIL essentially endows curriculum learning mechanism which is critical for learning policies.",0
"Imitation learning (IL) is a more efficient approach for training agents to learn control policies from expert demonstrations compared to reinforcement learning. However, obtaining demonstration data can be expensive and difficult, which is a significant challenge in some scenarios. To address this issue, we propose the Hindsight Generative Adversarial Imitation Learning (HGAIL) algorithm, which aims to expand imitation learning areas by enabling agent learning without expert demonstrations. The HGAIL algorithm combines the hindsight idea with the generative adversarial imitation learning (GAIL) framework to successfully implement imitation learning in cases where expert demonstration data is not available. Experiments show that the proposed method can train policies with comparable performance to current imitation learning methods and also includes a curriculum learning mechanism that is critical for learning policies.",1
"This report first provides a brief overview of a number of supervised learning algorithms for regression tasks. Among those are neural networks, regression trees, and the recently introduced Nexting. Nexting has been presented in the context of reinforcement learning where it was used to predict a large number of signals at different timescales. In the second half of this report, we apply the algorithms to historical weather data in order to evaluate their suitability to forecast a local weather trend. Our experiments did not identify one clearly preferable method, but rather show that choosing an appropriate algorithm depends on the available side information. For slowly varying signals and a proficient number of training samples, Nexting achieved good results in the studied cases.",0
"To begin with, this report briefly introduces various supervised learning algorithms that can be applied to regression tasks. These algorithms include neural networks, regression trees, and a recently developed method called Nexting. Nexting was initially introduced in reinforcement learning to predict numerous signals at different timescales. In the latter part of this report, we implemented these algorithms on historical weather data to assess their effectiveness in predicting local weather trends. Our experiments did not reveal a distinct preference for any one method; instead, we discovered that the appropriate algorithm choice is reliant on the available supplementary information. In cases where the signals vary slowly and there are ample training samples, Nexting yielded satisfactory outcomes.",1
"In model-based reinforcement learning, the agent interleaves between model learning and planning. These two components are inextricably intertwined. If the model is not able to provide sensible long-term prediction, the executed planner would exploit model flaws, which can yield catastrophic failures. This paper focuses on building a model that reasons about the long-term future and demonstrates how to use this for efficient planning and exploration. To this end, we build a latent-variable autoregressive model by leveraging recent ideas in variational inference. We argue that forcing latent variables to carry future information through an auxiliary task substantially improves long-term predictions. Moreover, by planning in the latent space, the planner's solution is ensured to be within regions where the model is valid. An exploration strategy can be devised by searching for unlikely trajectories under the model. Our method achieves higher reward faster compared to baselines on a variety of tasks and environments in both the imitation learning and model-based reinforcement learning settings.",0
"The process of model-based reinforcement learning involves the agent alternating between learning and planning using a model. The success of this approach is dependent on the quality of the model's ability to predict long-term outcomes. If the model is not accurate, the planner's execution could result in disastrous consequences. The focus of this study is on developing a model that can reason about the future in the long term, which can be used for efficient planning and exploration. The researchers created a latent-variable autoregressive model using variational inference, which they claim improves long-term prediction by forcing latent variables to carry future information through an auxiliary task. Planning in the latent space ensures that the planner's solution falls within areas where the model is reliable, and an exploration strategy can be devised by identifying unlikely trajectories under the model. The method has been tested on various tasks and environments, and it has been found to produce higher rewards faster than the baselines in both the imitation learning and model-based reinforcement learning settings.",1
"We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",0
"THOR, the House of Interactions, is a visual AI research framework accessible at http://ai2thor.allenai.org. The framework features true-to-life 3D indoor environments where AI agents can navigate and execute tasks by interacting with objects. AI2-THOR supports various research areas, such as deep reinforcement learning, planning, unsupervised representation learning, and learning models of cognition, among others. The main objective of AI2-THOR is to support the development of visually intelligent models and advance research in this field.",1
"Deep reinforcement learning (RL) has achieved breakthrough results on many tasks, but agents often fail to generalize beyond the environment they were trained in. As a result, deep RL algorithms that promote generalization are receiving increasing attention. However, works in this area use a wide variety of tasks and experimental setups for evaluation. The literature lacks a controlled assessment of the merits of different generalization schemes. Our aim is to catalyze community-wide progress on generalization in deep RL. To this end, we present a benchmark and experimental protocol, and conduct a systematic empirical study. Our framework contains a diverse set of environments, our methodology covers both in-distribution and out-of-distribution generalization, and our evaluation includes deep RL algorithms that specifically tackle generalization. Our key finding is that `vanilla' deep RL algorithms generalize better than specialized schemes that were proposed specifically to tackle generalization.",0
"Although deep reinforcement learning (RL) has accomplished remarkable results on numerous tasks, agents often struggle to apply their learning to environments beyond their training. As a result, more attention is being given to deep RL algorithms that promote generalization. However, the evaluation of various generalization schemes lacks a controlled assessment in current literature, as works in this area use a wide range of tasks and experimental setups. Our goal is to stimulate community-wide progress on generalization in deep RL by presenting a benchmark and experimental protocol and conducting a systematic empirical study. Our framework consists of a diverse set of environments, and our methodology covers both in-distribution and out-of-distribution generalization, while our evaluation includes deep RL algorithms specifically designed to tackle generalization. Our primary finding is that ""vanilla"" deep RL algorithms perform better in generalization than specialized schemes developed explicitly for this purpose.",1
"The area of building energy management has received a significant amount of interest in recent years. This area is concerned with combining advancements in sensor technologies, communications and advanced control algorithms to optimize energy utilization. Reinforcement learning is one of the most prominent machine learning algorithms used for control problems and has had many successful applications in the area of building energy management. This research gives a comprehensive review of the literature relating to the application of reinforcement learning to developing autonomous building energy management systems. The main direction for future research and challenges in reinforcement learning are also outlined.",0
"In recent times, considerable attention has been directed towards the field of building energy management, which aims to maximize energy usage by combining advancements in sensor technology, communications, and advanced control algorithms. One of the most widely-used machine learning algorithms for control problems is reinforcement learning, which has proven successful in various applications of building energy management. This study provides a thorough analysis of the existing literature on the use of reinforcement learning for creating self-sufficient building energy management systems, along with an overview of future research directions and the challenges involved.",1
"Multiagent reinforcement learning algorithms (MARL) have been demonstrated on complex tasks that require the coordination of a team of multiple agents to complete. Existing works have focused on sharing information between agents via centralized critics to stabilize learning or through communication to increase performance, but do not generally look at how information can be shared between agents to address the curse of dimensionality in MARL. We posit that a multiagent problem can be decomposed into a multi-task problem where each agent explores a subset of the state space instead of exploring the entire state space. This paper introduces a multiagent actor-critic algorithm and method for combining knowledge from homogeneous agents through distillation and value-matching that outperforms policy distillation alone and allows further learning in both discrete and continuous action spaces.",0
"MARL algorithms have been proven effective in tackling complex tasks that require the coordination of multiple agents. While previous research has focused on utilizing centralized critics or communication to enhance learning, little attention has been given to addressing the issue of dimensionality. In this paper, we propose a solution whereby a multiagent problem can be broken down into a multi-task problem, with each agent exploring a specific subset of the state space. Our approach involves a multiagent actor-critic algorithm and a method for combining knowledge from similar agents through distillation and value-matching. This approach yields better results than policy distillation alone and enables learning in both discrete and continuous action spaces.",1
"Deep Reinforcement Learning has enabled the control of increasingly complex and high-dimensional problems. However, the need of vast amounts of data before reasonable performance is attained prevents its widespread application. We employ binary corrective feedback as a general and intuitive manner to incorporate human intuition and domain knowledge in model-free machine learning. The uncertainty in the policy and the corrective feedback is combined directly in the action space as probabilistic conditional exploration. As a result, the greatest part of the otherwise ignorant learning process can be avoided. We demonstrate the proposed method, Predictive Probabilistic Merging of Policies (PPMP), in combination with DDPG. In experiments on continuous control problems of the OpenAI Gym, we achieve drastic improvements in sample efficiency, final performance, and robustness to erroneous feedback, both for human and synthetic feedback. Additionally, we show solutions beyond the demonstrated knowledge.",0
"The utilization of Deep Reinforcement Learning has facilitated the management of intricate and multidimensional predicaments. However, its extensive data requirement for satisfactory results hinders its widespread usage. We have devised a general and comprehensible binary corrective feedback approach to incorporate human insight and expertise in model-free machine learning. This method entails a probabilistic conditional exploration, allowing the combination of policy uncertainty and corrective feedback directly in the action space. Consequently, a significant portion of the otherwise uninformed learning process can be avoided. We have demonstrated this technique, named Predictive Probabilistic Merging of Policies (PPMP), with DDPG. In our experiments on continuous control issues of the OpenAI Gym, we have witnessed a remarkable improvement in sample efficiency, final performance, and resistance to erroneous feedback for both human and synthetic feedback. Furthermore, we have provided solutions beyond the demonstrated knowledge.",1
"Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.",0
"The ability to decipher a person's objective through their actions is a significant issue in AI applications like automated assistants and recommender systems. The most commonly used model for this task is the rational actor model, wherein individuals are assumed to have stable reward functions, discount the future exponentially, and create optimal plans. By utilizing the rational actor assumption, techniques like inverse reinforcement learning (IRL) can be employed to deduce a person's goals based on their actions. However, there is a competing model known as the dual-system model. Here, decision-making is a result of the interplay between two systems- a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We have expanded the dual-system framework to Markov decision problems and demonstrated how to compute optimal plans for dual-system agents. Our research shows that dual-system agents exhibit behaviors that contradict the rational actor assumption. Naive applications of rational-actor IRL to the behavior of dual-system agents can lead to incorrect inferences about their goals and interventions that negatively impact their overall utility. Therefore, we have developed a simple IRL algorithm to accurately deduce the goals of dual-system decision-makers. This enables us to make interventions that aid, rather than obstruct, the dual-system agent's ability to achieve their true goals.",1
"We propose a new low-cost machine-learning-based methodology which assists designers in reducing the gap between the problem and the solution in the design process. Our work applies reinforcement learning (RL) to find the optimal task-oriented design solution through the construction of the design action for each task. For this task-oriented design, the 3D design process in product design is assigned to an action space in Deep RL, and the desired 3D model is obtained by training each design action according to the task. By showing that this method achieves satisfactory design even when applied to a task pursuing multiple goals, we suggest the direction of how machine learning can contribute to the design process. Also, we have validated with product designers that this methodology can assist the creative part in the process of design.",0
"A new approach is suggested in this article that utilizes machine learning to help bridge the gap between design problems and solutions in a cost-effective manner. The proposed methodology employs reinforcement learning (RL) to identify the optimal task-based design solution by constructing design actions for each task. The 3D design process in product design is mapped to an action space in Deep RL for task-oriented design, and each design action is trained based on the task to obtain the desired 3D model. The study demonstrates that this method is effective even when applied to tasks with multiple goals, indicating the potential for machine learning to contribute to the design process. In addition, product designers have confirmed that this methodology can assist with the creative aspect of the design process.",1
"How do we know if communication is emerging in a multi-agent system? The vast majority of recent papers on emergent communication show that adding a communication channel leads to an increase in reward or task success. This is a useful indicator, but provides only a coarse measure of the agent's learned communication abilities. As we move towards more complex environments, it becomes imperative to have a set of finer tools that allow qualitative and quantitative insights into the emergence of communication. This may be especially useful to allow humans to monitor agents' behaviour, whether for fault detection, assessing performance, or even building trust. In this paper, we examine a few intuitive existing metrics for measuring communication, and show that they can be misleading. Specifically, by training deep reinforcement learning agents to play simple matrix games augmented with a communication channel, we find a scenario where agents appear to communicate (their messages provide information about their subsequent action), and yet the messages do not impact the environment or other agent in any way. We explain this phenomenon using ablation studies and by visualizing the representations of the learned policies. We also survey some commonly used metrics for measuring emergent communication, and provide recommendations as to when these metrics should be used.",0
"Determining the emergence of communication in a multi-agent system can be indicated by an increase in task success or reward, as observed in recent studies. However, this provides a limited understanding of the agents' communication abilities. As more complex environments are introduced, it is necessary to have more precise and comprehensive tools for evaluating the emergence of communication to facilitate fault detection, performance assessment, and trust-building. This paper examines various metrics for measuring communication, highlighting their limitations and potential for misleading results. Through training deep reinforcement learning agents in matrix games with communication channels, we demonstrate a scenario where agents seemingly communicate without any impact on the environment or other agents. We explain this phenomenon using ablation studies and visualization of learned policies. We also review commonly used metrics and provide recommendations for their appropriate usage.",1
"Deep Reinforcement Learning (DRL) has been applied to address a variety of cooperative multi-agent problems with either discrete action spaces or continuous action spaces. However, to the best of our knowledge, no previous work has ever succeeded in applying DRL to multi-agent problems with discrete-continuous hybrid (or parameterized) action spaces which is very common in practice. Our work fills this gap by proposing two novel algorithms: Deep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent Hierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized training but decentralized execution paradigm: different levels of communication between different agents are used to facilitate the training process, while each agent executes its policy independently based on local observations during execution. Our empirical results on several challenging tasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN and Deep MAHHQN are effective and significantly outperform existing independent deep parameterized Q-learning method.",0
"DRL has been utilized to tackle cooperative multi-agent problems that have either discrete or continuous action spaces. However, it has never been successful in handling multi-agent problems with hybrid action spaces, which are commonly found in practical scenarios. This gap is addressed in our work by introducing two innovative algorithms: Deep MAPQN and Deep MAHHQN. We adopt the centralized training but decentralized execution approach, using various communication levels between agents during training while allowing each agent to execute its policy independently based on local observations. Our experimental results on challenging tasks, such as simulated RoboCup Soccer and Ghost Story, demonstrate the effectiveness of both Deep MAPQN and Deep MAHHQN, which significantly outperform the existing independent deep parameterized Q-learning method.",1
"We present a deep reinforcement learning method of progressive view inpainting for 3D point scene completion under volume guidance, achieving high-quality scene reconstruction from only a single depth image with severe occlusion. Our approach is end-to-end, consisting of three modules: 3D scene volume reconstruction, 2D depth map inpainting, and multi-view selection for completion. Given a single depth image, our method first goes through the 3D volume branch to obtain a volumetric scene reconstruction as a guide to the next view inpainting step, which attempts to make up the missing information; the third step involves projecting the volume under the same view of the input, concatenating them to complete the current view depth, and integrating all depth into the point cloud. Since the occluded areas are unavailable, we resort to a deep Q-Network to glance around and pick the next best view for large hole completion progressively until a scene is adequately reconstructed while guaranteeing validity. All steps are learned jointly to achieve robust and consistent results. We perform qualitative and quantitative evaluations with extensive experiments on the SUNCG data, obtaining better results than the state of the art.",0
"Our study introduces a deep reinforcement learning approach to progressively complete 3D point scenes through view inpainting, while adhering to volume guidance, resulting in superior scene reconstruction from a single depth image with extensive occlusion. We have devised an end-to-end strategy comprising three modules: 3D scene volume reconstruction, 2D depth map inpainting, and multi-view selection for completion. Our method initially employs the 3D volume module to obtain a volumetric scene reconstruction as guidance for the following view inpainting stage, which endeavors to compensate for the missing information. The third step involves projecting the volume under the same view of the input, concatenating it to complete the current view depth, and integrating all depth into the point cloud. As the occluded areas are not available, we have implemented a deep Q-network to scan the area and select the next best view for progressive large hole completion, while ensuring validity until a scene has satisfactorily reconstructed. All stages are learned together to achieve consistent and robust outcomes. We have conducted qualitative and quantitative assessments with extensive experiments on SUNCG data, obtaining better results than the current state of the art.",1
"Reinforcement learning (RL) is a promising data-driven approach for adaptive traffic signal control (ATSC) in complex urban traffic networks, and deep neural networks further enhance its learning power. However, centralized RL is infeasible for large-scale ATSC due to the extremely high dimension of the joint action space. Multi-agent RL (MARL) overcomes the scalability issue by distributing the global control to each local RL agent, but it introduces new challenges: now the environment becomes partially observable from the viewpoint of each local agent due to limited communication among agents. Most existing studies in MARL focus on designing efficient communication and coordination among traditional Q-learning agents. This paper presents, for the first time, a fully scalable and decentralized MARL algorithm for the state-of-the-art deep RL agent: advantage actor critic (A2C), within the context of ATSC. In particular, two methods are proposed to stabilize the learning procedure, by improving the observability and reducing the learning difficulty of each local agent. The proposed multi-agent A2C is compared against independent A2C and independent Q-learning algorithms, in both a large synthetic traffic grid and a large real-world traffic network of Monaco city, under simulated peak-hour traffic dynamics. Results demonstrate its optimality, robustness, and sample efficiency over other state-of-the-art decentralized MARL algorithms.",0
"Adaptive traffic signal control (ATSC) in complex urban traffic networks can be achieved using reinforcement learning (RL) with the aid of deep neural networks. However, the high dimension of the joint action space makes centralized RL impractical for large-scale ATSC. Multi-agent RL (MARL) is a promising solution that distributes global control to local RL agents, but it presents new challenges such as limited communication among agents and partial observability of the environment. Existing studies in MARL focus on communication and coordination among traditional Q-learning agents. In this paper, we introduce a fully scalable and decentralized MARL algorithm for the deep RL agent A2C, specifically for ATSC. We propose two methods to enhance the learning procedure by improving observability and reducing the learning difficulty of each local agent. The performance of the proposed multi-agent A2C is compared against independent A2C and independent Q-learning algorithms in both a large synthetic traffic grid and a large real-world traffic network of Monaco city under simulated peak-hour traffic dynamics. Results demonstrate the optimality, robustness, and sample efficiency of the proposed algorithm over other state-of-the-art decentralized MARL algorithms.",1
"Machine learning, especially deep neural networks, has been rapidly developed in fields including computer vision, speech recognition and reinforcement learning. Although Mini-batch SGD is one of the most popular stochastic optimization methods in training deep networks, it shows a slow convergence rate due to the large noise in gradient approximation. In this paper, we attempt to remedy this problem by building more efficient batch selection method based on typicality sampling, which reduces the error of gradient estimation in conventional Minibatch SGD. We analyze the convergence rate of the resulting typical batch SGD algorithm and compare convergence properties between Minibatch SGD and the algorithm. Experimental results demonstrate that our batch selection scheme works well and more complex Minibatch SGD variants can benefit from the proposed batch selection strategy.",0
"The development of machine learning, particularly deep neural networks, has advanced rapidly in various fields such as computer vision, speech recognition, and reinforcement learning. Despite being a popular stochastic optimization technique for training deep networks, Mini-batch SGD exhibits a slow rate of convergence due to the considerable noise in gradient approximation. This paper addresses this issue by introducing a more efficient batch selection method based on typicality sampling, which decreases the error of gradient estimation in conventional Minibatch SGD. The convergence rate of the resulting algorithm, typical batch SGD, is analyzed and compared to that of Minibatch SGD. Experimental results demonstrate the effectiveness of our batch selection approach, and more complicated Minibatch SGD variations can benefit from it.",1
"Existing imitation learning approaches often require that the complete demonstration data, including sequences of actions and states, are available. In this paper, we consider a more realistic and difficult scenario where a reinforcement learning agent only has access to the state sequences of an expert, while the expert actions are unobserved. We propose a novel tensor-based model to infer the unobserved actions of the expert state sequences. The policy of the agent is then optimized via a hybrid objective combining reinforcement learning and imitation learning. We evaluated our hybrid approach on an illustrative domain and Atari games. The empirical results show that (1) the agents are able to leverage state expert sequences to learn faster than pure reinforcement learning baselines, (2) our tensor-based action inference model is advantageous compared to standard deep neural networks in inferring expert actions, and (3) the hybrid policy optimization objective is robust against noise in expert state sequences.",0
"Many current methods for imitation learning require complete demonstration data, including both action and state sequences. This paper, however, addresses a more challenging and realistic scenario in which a reinforcement learning agent only has access to the state sequences of an expert, with no information about their actions. To overcome this difficulty, a novel tensor-based model is proposed to infer the unobserved actions of the expert state sequences. The agent's policy is then optimized through a hybrid objective that combines reinforcement learning and imitation learning. The effectiveness of this approach is demonstrated through experiments in an illustrative domain and Atari games, which show that the agents can learn faster than pure reinforcement learning baselines by leveraging expert state sequences. Furthermore, the proposed tensor-based action inference model outperforms standard deep neural networks in inferring expert actions, and the hybrid policy optimization objective is robust against noise in expert state sequences.",1
"Many robotic applications require the agent to perform long-horizon tasks in partially observable environments. In such applications, decision making at any step can depend on observations received far in the past. Hence, being able to properly memorize and utilize the long-term history is crucial. In this work, we propose a novel memory-based policy, named Scene Memory Transformer (SMT). The proposed policy embeds and adds each observation to a memory and uses the attention mechanism to exploit spatio-temporal dependencies. This model is generic and can be efficiently trained with reinforcement learning over long episodes. On a range of visual navigation tasks, SMT demonstrates superior performance to existing reactive and memory-based policies by a margin.",0
"Partially observable environments in robotic applications often require agents to perform long-term tasks where decision making depends on past observations. Properly memorizing and utilizing long-term history is vital. The Scene Memory Transformer (SMT), a novel memory-based policy, is proposed in this study. SMT embeds and adds each observation to a memory and utilizes the attention mechanism to exploit spatio-temporal dependencies. This model is generic and efficiently trained with reinforcement learning over long episodes. SMT outperforms existing reactive and memory-based policies by a margin in various visual navigation tasks.",1
"To improve efficiency and reduce failures in autonomous vehicles, research has focused on developing robust and safe learning methods that take into account disturbances in the environment. Existing literature in robust reinforcement learning poses the learning problem as a two player game between the autonomous system and disturbances. This paper examines two different algorithms to solve the game, Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, and compares performance on an autonomous driving scenario. We extend the game formulation to a semi-competitive setting and demonstrate that the resulting adversary better captures meaningful disturbances that lead to better overall performance. The resulting robust policy exhibits improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional reinforcement learning methods.",0
"Research has been focused on developing reliable and secure learning methods to enhance efficiency and minimize malfunctions in autonomous vehicles. The current body of literature on robust reinforcement learning approaches the learning problem as a two-player game between the autonomous system and disturbances. This study compares the performance of two algorithms, namely Robust Adversarial Reinforcement Learning and Neural Fictitious Self Play, on an autonomous driving scenario. We expand the game formulation to a semi-competitive environment and demonstrate that the resulting adversary takes into account significant disturbances, resulting in better overall performance. The resulting robust policy displays improved driving efficiency while concurrently reducing collision rates compared to conventional reinforcement learning methods' baseline control policies.",1
"Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency when rewards are delayed and sparse. We introduce a solution that enables agents to learn temporally extended actions at multiple levels of abstraction in a sample efficient and automated fashion. Our approach combines universal value functions and hindsight learning, allowing agents to learn policies belonging to different time scales in parallel. We show that our method significantly accelerates learning in a variety of discrete and continuous tasks.",0
"When the rewards are delayed and sparse, Reinforcement Learning (RL) algorithms may not be efficient in terms of sample usage. To address this issue, we propose a solution that enables agents to learn temporally extended actions at different levels of abstraction, in an automated and efficient manner. Our approach incorporates universal value functions and hindsight learning, which facilitates parallel learning of policies belonging to different time scales. Our study demonstrates that our approach accelerates learning significantly in various continuous and discrete tasks.",1
"We introduce a multi-agent meta-modeling game to generate data, knowledge, and models that make predictions on constitutive responses of elasto-plastic materials. We introduce a new concept from graph theory where a modeler agent is tasked with evaluating all the modeling options recast as a directed multigraph and find the optimal path that links the source of the directed graph (e.g. strain history) to the target (e.g. stress) measured by an objective function. Meanwhile, the data agent, which is tasked with generating data from real or virtual experiments (e.g. molecular dynamics, discrete element simulations), interacts with the modeling agent sequentially and uses reinforcement learning to design new experiments to optimize the prediction capacity. Consequently, this treatment enables us to emulate an idealized scientific collaboration as selections of the optimal choices in a decision tree search done automatically via deep reinforcement learning.",0
"To generate predictions on the constitutive responses of elasto-plastic materials, we propose a multi-agent meta-modeling game. Our approach includes a novel concept from graph theory where a modeler agent evaluates modeling options recast as a directed multigraph. The modeler agent identifies the optimal path that connects the source of the directed graph (e.g. strain history) to the target (e.g. stress) using an objective function. In addition, a data agent generates data from real or virtual experiments (e.g. molecular dynamics, discrete element simulations) and uses reinforcement learning to design new experiments to optimize prediction capacity. By automating the decision tree search process via deep reinforcement learning, our approach emulates an ideal scientific collaboration.",1
"The game of Chinese Checkers is a challenging traditional board game of perfect information that differs from other traditional games in two main aspects: first, unlike Chess, all checkers remain indefinitely in the game and hence the branching factor of the search tree does not decrease as the game progresses; second, unlike Go, there are also no upper bounds on the depth of the search tree since repetitions and backward movements are allowed. Therefore, even in a restricted game instance, the state-space of the game can still be unbounded, making it challenging for a computer program to excel. In this work, we present an approach that effectively combines the use of heuristics, Monte Carlo tree search, and deep reinforcement learning for building a Chinese Checkers agent without the use of any human game-play data. Experiment results show that our agent is competent under different scenarios and reaches the level of experienced human players.",0
"Chinese Checkers is a traditional board game that is challenging and requires perfect information. It differs from other traditional games in two main ways. Firstly, unlike Chess, all checkers remain in the game indefinitely, and the branching factor of the search tree does not decrease as the game progresses. Secondly, unlike Go, there are no upper bounds on the depth of the search tree as repetitions and backward movements are allowed. As a result, the state-space of the game can be unbounded, which makes it difficult for computer programs to excel, even in restricted game instances. In this study, we present an approach that combines the use of heuristics, Monte Carlo tree search, and deep reinforcement learning to build a Chinese Checkers agent without human game-play data. The experiment results show that our agent is competent in different scenarios and can reach the level of experienced human players.",1
"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",0
"In what way can we create reinforcement learning agents that are safe and do not cause unnecessary disruptions in their environment? Our research indicates that current methods of penalizing side effects may actually lead to negative incentives, such as a desire to prevent irreversible changes in the environment, even those caused by other agents. To determine the origin of these undesirable incentives, we divide side effects penalties into two parts: a baseline state and a measurement of deviation from this state. We argue that some of these incentives come from the chosen baseline, while others come from the chosen deviation measurement. To address this issue, we introduce a new version of the stepwise inaction baseline and a new deviation measurement based on the relative accessibility of states. By combining these design choices, we are able to avoid the negative incentives mentioned earlier; however, simpler baselines and the unreachability measurement do not accomplish this. Through a series of gridworld experiments intended to demonstrate possible negative incentives, we demonstrate this empirically by comparing different combinations of baseline and deviation measurement choices.",1
"Nonlinear optimal control problems are often solved with numerical methods that require knowledge of system's dynamics which may be difficult to infer, and that carry a large computational cost associated with iterative calculations. We present a novel neurobiologically inspired hierarchical learning framework, Reinforcement Learning Optimal Control, which operates on two levels of abstraction and utilises a reduced number of controllers to solve nonlinear systems with unknown dynamics in continuous state and action spaces. Our approach is inspired by research at two levels of abstraction: first, at the level of limb coordination human behaviour is explained by linear optimal feedback control theory. Second, in cognitive tasks involving learning symbolic level action selection, humans learn such problems using model-free and model-based reinforcement learning algorithms. We propose that combining these two levels of abstraction leads to a fast global solution of nonlinear control problems using reduced number of controllers. Our framework learns the local task dynamics from naive experience and forms locally optimal infinite horizon Linear Quadratic Regulators which produce continuous low-level control. A top-level reinforcement learner uses the controllers as actions and learns how to best combine them in state space while maximising a long-term reward. A single optimal control objective function drives high-level symbolic learning by providing training signals on desirability of each selected controller. We show that a small number of locally optimal linear controllers are able to solve global nonlinear control problems with unknown dynamics when combined with a reinforcement learner in this hierarchical framework. Our algorithm competes in terms of computational cost and solution quality with sophisticated control algorithms and we illustrate this with solutions to benchmark problems.",0
"To solve nonlinear optimal control problems, numerical methods are often used, which require knowledge of the system's dynamics and entail a high computational cost. However, we propose a new approach called Reinforcement Learning Optimal Control, which is inspired by both limb coordination and cognitive learning research. Our approach operates on two levels of abstraction and utilizes a reduced number of controllers to solve nonlinear systems with unknown dynamics in continuous state and action spaces. We learn the local task dynamics from naive experience and form locally optimal infinite horizon Linear Quadratic Regulators, which produce continuous low-level control. A top-level reinforcement learner uses the controllers as actions and learns how to best combine them in state space while maximizing a long-term reward. By providing training signals on the desirability of each selected controller, a single optimal control objective function drives high-level symbolic learning. We show that our algorithm competes in terms of computational cost and solution quality with sophisticated control algorithms. This is demonstrated through solutions to benchmark problems, which prove that a small number of locally optimal linear controllers can solve global nonlinear control problems with unknown dynamics when combined with a reinforcement learner in this hierarchical framework.",1
"Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.",0
"Tasks in the real world are typically well-organized. Researchers have taken an interest in hierarchical reinforcement learning (HRL) as a way to leverage the hierarchical structure of a given task in reinforcement learning (RL). However, it is not an easy task to identify the hierarchical policy structure that improves the performance of RL. This article proposes an HRL method that learns a hidden variable of a hierarchical policy using mutual information maximization. Our method is a means of learning a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. The gating policy selects option policies based on an option-value function, and these option policies are optimized using the deterministic policy gradient method. This framework is derived by drawing a comparison between a monolithic policy in standard RL and a hierarchical policy in HRL, using a deterministic option policy. Experimental results demonstrate that our HRL approach can learn a range of options and that it can improve the performance of RL in continuous control tasks.",1
"Active vision is inherently attention-driven: The agent actively selects views to attend in order to fast achieve the vision task while improving its internal representation of the scene being observed. Inspired by the recent success of attention-based models in 2D vision tasks based on single RGB images, we propose to address the multi-view depth-based active object recognition using attention mechanism, through developing an end-to-end recurrent 3D attentional network. The architecture takes advantage of a recurrent neural network (RNN) to store and update an internal representation. Our model, trained with 3D shape datasets, is able to iteratively attend to the best views targeting an object of interest for recognizing it. To realize 3D view selection, we derive a 3D spatial transformer network which is differentiable for training with backpropagation, achieving much faster convergence than the reinforcement learning employed by most existing attention-based models. Experiments show that our method, with only depth input, achieves state-of-the-art next-best-view performance in time efficiency and recognition accuracy.",0
"Active vision involves the agent deliberately choosing views to focus on in order to quickly complete the vision task and enhance its understanding of the observed scene. Given the success of attention-based models in 2D vision tasks that rely on single RGB images, we propose a solution for multi-view depth-based active object recognition using attention mechanisms. Our approach involves creating a recurrent 3D attentional network that benefits from a recurrent neural network (RNN) to maintain and update an internal representation. Using 3D shape datasets, our model can repeatedly attend to the best views for recognizing an object of interest. We have developed a differentiable 3D spatial transformer network to enable 3D view selection, which facilitates faster convergence during training than reinforcement learning methods used in existing attention-based models. Our experiments demonstrate that our method, which only requires depth input, achieves superior next-best-view performance in terms of time efficiency and recognition accuracy.",1
"Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.",0
"It is challenging to learn understandable and transferable subpolicies and break down complex tasks into simpler ones. Traditional hierarchical reinforcement learning methods enforce top-down decomposition, while meta-learning techniques need a task distribution to learn such breakdowns. This paper introduces a framework that uses various suboptimal world models to divide complex task solutions into more straightforward modular subpolicies. The framework does this automatically, starting from a single source task and concurrently learning the necessary modular subpolicies and a controller to coordinate them. We conduct experiments on high dimensional continuous action control tasks to demonstrate the usefulness of this approach in both complex single task learning and lifelong learning. Finally, we perform ablation studies to comprehend the importance and resilience of different elements in the framework and the limitations of this approach.",1
"This paper investigates whether learning contingency-awareness and controllable aspects of an environment can lead to better exploration in reinforcement learning. To investigate this question, we consider an instantiation of this hypothesis evaluated on the Arcade Learning Element (ALE). In this study, we develop an attentive dynamics model (ADM) that discovers controllable elements of the observations, which are often associated with the location of the character in Atari games. The ADM is trained in a self-supervised fashion to predict the actions taken by the agent. The learned contingency information is used as a part of the state representation for exploration purposes. We demonstrate that combining actor-critic algorithm with count-based exploration using our representation achieves impressive results on a set of notoriously challenging Atari games due to sparse rewards. For example, we report a state-of-the-art score of >11,000 points on Montezuma's Revenge without using expert demonstrations, explicit high-level information (e.g., RAM states), or supervisory data. Our experiments confirm that contingency-awareness is indeed an extremely powerful concept for tackling exploration problems in reinforcement learning and opens up interesting research questions for further investigations.",0
"The aim of this study is to explore if acquiring knowledge of contingencies and controllable features of an environment can improve exploration in reinforcement learning. The study is conducted using the Arcade Learning Element (ALE) as the testing platform. An attentive dynamics model (ADM) is developed to identify controllable elements in the observations, such as the character's location in Atari games. The ADM is trained in a self-supervised manner to predict the agent's actions, and the contingency information obtained is used as part of the state representation to facilitate exploration. The combination of actor-critic algorithm with count-based exploration using the ADM representation achieves remarkable results in challenging Atari games with sparse rewards. For instance, the study reports a new high score of >11,000 points in Montezuma's Revenge without expert demonstrations, high-level information or supervisory data. The findings demonstrate that contingency-awareness is a powerful tool in addressing exploration issues in reinforcement learning and suggest further research opportunities.",1
"Efficiently adapting to new environments and changes in dynamics is critical for agents to successfully operate in the real world. Reinforcement learning (RL) based approaches typically rely on external reward feedback for adaptation. However, in many scenarios this reward signal might not be readily available for the target task, or the difference between the environments can be implicit and only observable from the dynamics. To this end, we introduce a method that allows for self-adaptation of learned policies: No-Reward Meta Learning (NoRML). NoRML extends Model Agnostic Meta Learning (MAML) for RL and uses observable dynamics of the environment instead of an explicit reward function in MAML's finetune step. Our method has a more expressive update step than MAML, while maintaining MAML's gradient based foundation. Additionally, in order to allow more targeted exploration, we implement an extension to MAML that effectively disconnects the meta-policy parameters from the fine-tuned policies' parameters. We first study our method on a number of synthetic control problems and then validate our method on common benchmark environments, showing that NoRML outperforms MAML when the dynamics change between tasks.",0
"Being able to adapt to new environments and changes is crucial for agents to operate effectively in the real world. Reinforcement learning (RL) approaches typically require external reward feedback for adaptation, but this reward signal may not always be available or the difference between environments may be implicit. To address this, we present a method called No-Reward Meta Learning (NoRML) which allows for self-adaptation of learned policies. NoRML builds upon Model Agnostic Meta Learning (MAML) for RL, but uses observable dynamics of the environment instead of an explicit reward function during the finetune step. Our method has a more expressive update step and maintains MAML's gradient based foundation. Additionally, we implement an extension to MAML that disconnects the meta-policy parameters from the fine-tuned policies' parameters to allow for more targeted exploration. We validate our method on synthetic control problems and common benchmark environments, demonstrating that NoRML outperforms MAML when the dynamics change between tasks.",1
"Deep reinforcement learning, applied to vision-based problems like Atari games, maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. By separating the image processing from decision-making, one could better understand the complexity of each task, as well as potentially find smaller policy representations that are easier for humans to understand and may generalize better. To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations as they appear in an open-ended online-learning context; Direct Residuals Sparse Coding encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion. The encoder autonomously selects observations online to train on, in order to maximize code sparsity. As the dictionary size increases, the encoder produces increasingly larger inputs for the neural network: this is addressed by a variation of the Exponential Natural Evolution Strategies algorithm which adapts its probability distribution dimensionality along the run. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game's controls). These are still capable of achieving results comparable---and occasionally superior---to state-of-the-art techniques which use two orders of magnitude more neurons.",0
"The concept of deep reinforcement learning involves using neural networks to process images and make decisions simultaneously. However, separating these processes could lead to a better understanding of the complexity of each task and potentially result in smaller policy representations that are easier for humans to comprehend and may generalize better. Our proposal is a new approach to learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. This involves an encoder that generates state representations based on two innovative algorithms. The encoder can grow its dictionary size over time to address new observations as they appear in an open-ended online-learning context. It also encodes observations by disregarding reconstruction error minimization and aiming for the highest information inclusion. The encoder autonomously selects observations online to train on to maximize code sparsity. Using a variation of the Exponential Natural Evolution Strategies algorithm, the system adapts its probability distribution dimensionality along the run. We tested our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons, which achieved comparable or occasionally superior results compared to state-of-the-art techniques that use two orders of magnitude more neurons.",1
"Deep Deterministic Policy Gradient (DDPG) has been proved to be a successful reinforcement learning (RL) algorithm for continuous control tasks. However, DDPG still suffers from data insufficiency and training inefficiency, especially in computationally complex environments. In this paper, we propose Asynchronous Episodic DDPG (AE-DDPG), as an expansion of DDPG, which can achieve more effective learning with less training time required. First, we design a modified scheme for data collection in an asynchronous fashion. Generally, for asynchronous RL algorithms, sample efficiency or/and training stability diminish as the degree of parallelism increases. We consider this problem from the perspectives of both data generation and data utilization. In detail, we re-design experience replay by introducing the idea of episodic control so that the agent can latch on good trajectories rapidly. In addition, we also inject a new type of noise in action space to enrich the exploration behaviors. Experiments demonstrate that our AE-DDPG achieves higher rewards and requires less time consuming than most popular RL algorithms in Learning to Run task which has a computationally complex environment. Not limited to the control tasks in computationally complex environments, AE-DDPG also achieves higher rewards and 2- to 4-fold improvement in sample efficiency on average compared to other variants of DDPG in MuJoCo environments. Furthermore, we verify the effectiveness of each proposed technique component through abundant ablation study.",0
"Although Deep Deterministic Policy Gradient (DDPG) has proven to be an effective reinforcement learning (RL) algorithm for continuous control tasks, it still faces challenges with data insufficiency and training inefficiency, particularly in computationally complex environments. This paper introduces Asynchronous Episodic DDPG (AE-DDPG), an expansion of DDPG that enhances learning efficiency while requiring less training time. The authors propose a modified scheme for data collection in an asynchronous manner, addressing the issue of sample efficiency and training stability as the degree of parallelism increases. The experience replay is redesigned using episodic control to enable the agent to quickly learn from good trajectories, and a new type of noise is introduced in the action space to enrich exploration behaviors. Experiments demonstrate that AE-DDPG outperforms other popular RL algorithms in Learning to Run task and achieves higher rewards and 2- to 4-fold improvement in sample efficiency on average compared to other variants of DDPG in MuJoCo environments. The effectiveness of each proposed technique component is verified through extensive ablation study.",1
"In this empirical paper, we investigate how learning agents can be arranged in more efficient communication topologies for improved learning. This is an important problem because a common technique to improve speed and robustness of learning in deep reinforcement learning and many other machine learning algorithms is to run multiple learning agents in parallel. The standard communication architecture typically involves all agents intermittently communicating with each other (fully connected topology) or with a centralized server (star topology). Unfortunately, optimizing the topology of communication over the space of all possible graphs is a hard problem, so we borrow results from the networked optimization and collective intelligence literatures which suggest that certain families of network topologies can lead to strong improvements over fully-connected networks. We start by introducing alternative network topologies to DRL benchmark tasks under the Evolution Strategies paradigm which we call Network Evolution Strategies. We explore the relative performance of the four main graph families and observe that one such family (Erdos-Renyi random graphs) empirically outperforms all other families, including the de facto fully-connected communication topologies. Additionally, the use of alternative network topologies has a multiplicative performance effect: we observe that when 1000 learning agents are arranged in a carefully designed communication topology, they can compete with 3000 agents arranged in the de facto fully-connected topology. Overall, our work suggests that distributed machine learning algorithms would learn more efficiently if the communication topology between learning agents was optimized.",0
"The aim of our empirical study is to investigate the potential benefits of arranging learning agents in more efficient communication topologies for improved learning. This is particularly relevant as multiple learning agents are frequently used in parallel to enhance the speed and robustness of deep reinforcement learning and other machine learning algorithms. Typically, communication occurs intermittently between all agents (fully connected topology) or with a centralized server (star topology). However, optimizing the topology of communication is a complex task. To address this, we draw on existing research from the networked optimization and collective intelligence fields, which suggest that certain network topologies can lead to significant improvements over fully connected networks. We introduce alternative network topologies to DRL benchmark tasks using Network Evolution Strategies. We observe that one particular family (Erdos-Renyi random graphs) outperforms all others, including the de facto fully connected communication topologies. Furthermore, using alternative network topologies has a multiplicative effect on performance. Our findings highlight the potential benefits of optimizing the communication topology between learning agents in distributed machine learning algorithms.",1
"Building a good predictive model requires an array of activities such as data imputation, feature transformations, estimator selection, hyper-parameter search and ensemble construction. Given the large, complex and heterogenous space of options, off-the-shelf optimization methods are infeasible for realistic response times. In practice, much of the predictive modeling process is conducted by experienced data scientists, who selectively make use of available tools. Over time, they develop an understanding of the behavior of operators, and perform serial decision making under uncertainty, colloquially referred to as educated guesswork. With an unprecedented demand for application of supervised machine learning, there is a call for solutions that automatically search for a good combination of parameters across these tasks to minimize the modeling error. We introduce a novel system called APRL (Autonomous Predictive modeler via Reinforcement Learning), that uses past experience through reinforcement learning to optimize such sequential decision making from within a set of diverse actions under a time constraint on a previously unseen predictive learning problem. APRL actions are taken to optimize the performance of a final ensemble. This is in contrast to other systems, which maximize individual model accuracy first and create ensembles as a disconnected post-processing step. As a result, APRL is able to reduce up to 71\% of classification error on average over a wide variety of problems.",0
"To build an effective predictive model, various tasks such as data imputation, feature transformations, estimator selection, hyper-parameter search, and ensemble construction are required. However, due to the vast and complex array of options, using off-the-shelf optimization methods is impractical for achieving realistic response times. Experienced data scientists usually carry out most of the predictive modeling process, selectively utilizing the available tools and relying on their knowledge of operator behavior to make educated guesses. With the increasing demand for supervised machine learning, there is a need for solutions that automatically search for an optimal combination of parameters to minimize modeling errors. To address this issue, we introduce APRL (Autonomous Predictive modeler via Reinforcement Learning), a novel system that employs reinforcement learning to optimize sequential decision making under time constraints for various tasks while considering a previously unseen predictive learning problem. APRL takes actions to optimize the performance of a final ensemble, rather than focusing on maximizing individual model accuracy and then creating ensembles as a separate post-processing step. As a result, APRL can reduce classification errors by up to 71% on average across a wide range of problems.",1
"Pedestrian detection is one of the most explored topics in computer vision and robotics. The use of deep learning methods allowed the development of new and highly competitive algorithms. Deep Reinforcement Learning has proved to be within the state-of-the-art in terms of both detection in perspective cameras and robotics applications. However, for detection in omnidirectional cameras, the literature is still scarce, mostly because of their high levels of distortion. This paper presents a novel and efficient technique for robust pedestrian detection in omnidirectional images. The proposed method uses deep Reinforcement Learning that takes advantage of the distortion in the image. By considering the 3D bounding boxes and their distorted projections into the image, our method is able to provide the pedestrian's position in the world, in contrast to the image positions provided by most state-of-the-art methods for perspective cameras. Our method avoids the need of pre-processing steps to remove the distortion, which is computationally expensive. Beyond the novel solution, our method compares favorably with the state-of-the-art methodologies that do not consider the underlying distortion for the detection task.",0
"Computer vision and robotics have extensively explored pedestrian detection, with the development of competitive algorithms facilitated by the use of deep learning methods. Deep Reinforcement Learning is a cutting-edge approach in both perspective camera detection and robotics. However, literature on the detection of pedestrians using omnidirectional cameras remains limited due to high levels of distortion. This study introduces a new and efficient technique that leverages deep Reinforcement Learning to detect pedestrians in omnidirectional images. Our method considers the distortion in the image by analyzing the 3D bounding boxes and their projections, enabling us to provide the pedestrian's position in the world. This approach eliminates the need for computationally expensive pre-processing steps to remove the distortion. Compared to state-of-the-art methodologies that do not account for distortion in the detection task, our method performs favorably.",1
"Point of care ultrasound (POCUS) consists in the use of ultrasound imaging in critical or emergency situations to support clinical decisions by healthcare professionals and first responders. In this setting it is essential to be able to provide means to obtain diagnostic data to potentially inexperienced users who did not receive an extensive medical training. Interpretation and acquisition of ultrasound images is not trivial. First, the user needs to find a suitable sound window which can be used to get a clear image, and then he needs to correctly interpret it to perform a diagnosis. Although many recent approaches focus on developing smart ultrasound devices that add interpretation capabilities to existing systems, our goal in this paper is to present a reinforcement learning (RL) strategy which is capable to guide novice users to the correct sonic window and enable them to obtain clinically relevant pictures of the anatomy of interest. We apply our approach to cardiac images acquired from the parasternal long axis (PLAx) view of the left ventricle of the heart.",0
"The utilization of ultrasound imaging in critical or emergency situations by healthcare professionals and first responders is known as Point of Care Ultrasound (POCUS). In such scenarios, it is crucial to provide diagnostic data to untrained users. However, interpreting and acquiring ultrasound images is not a simple task. The user must first locate a suitable sound window to capture a clear image, and then accurately interpret it to make a diagnosis. Although there are efforts to develop smart ultrasound devices that incorporate interpretation capabilities, this paper focuses on presenting a reinforcement learning (RL) strategy. This strategy can guide novice users to the correct sonic window and enable them to obtain clinically relevant pictures of the heart's anatomy. Specifically, we apply our approach to cardiac images acquired from the parasternal long axis (PLAx) view of the left ventricle.",1
"Humans are capable of attributing latent mental contents such as beliefs or intentions to others. The social skill is critical in daily life for reasoning about the potential consequences of others' behaviors so as to plan ahead. It is known that humans use such reasoning ability recursively by considering what others believe about their own beliefs. In this paper, we start from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policies, to which each agent finds the best response and then improve their own policies. We develop decentralized-training-decentralized-execution algorithms, namely PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenarios when there exists one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.",0
"The ability of humans to attribute latent mental states, such as beliefs and intentions, to others is crucial for daily life as it allows us to reason about potential consequences and plan ahead. Humans use this reasoning ability recursively by considering what others believe about their own beliefs. In this paper, we introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning, starting from level-1 recursion. Our hypothesis is that it is beneficial for each agent to consider how opponents would react to their future behaviors. We adopt variational Bayes methods to approximate the opponents' conditional policies, allowing each agent to find the best response and improve their own policies. Our decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, are proven to converge in self-play scenarios with one Nash equilibrium. We test our methods on matrix and differential games, where common gradient-based methods fail to converge, and find that it is critical to consider opponents' beliefs about what the agent believes. Our work contributes a new idea for modeling opponents in multi-agent reinforcement learning.",1
"We present a framework, which we call Molecule Deep $Q$-Networks (MolDQN), for molecule optimization by combining domain knowledge of chemistry and state-of-the-art reinforcement learning techniques (double $Q$-learning and randomized value functions). We directly define modifications on molecules, thereby ensuring 100\% chemical validity. Further, we operate without pre-training on any dataset to avoid possible bias from the choice of that set. Inspired by problems faced during medicinal chemistry lead optimization, we extend our model with multi-objective reinforcement learning, which maximizes drug-likeness while maintaining similarity to the original molecule. We further show the path through chemical space to achieve optimization for a molecule to understand how the model works.",0
"Our framework, MolDQN, combines chemical expertise with advanced reinforcement learning techniques (double $Q$-learning and randomized value functions) to optimize molecules while guaranteeing their chemical validity. We achieve this by directly modifying the molecules themselves, without pre-training on any particular dataset to avoid bias. We also address the challenges of medicinal chemistry lead optimization by incorporating multi-objective reinforcement learning to maximize drug-likeness while preserving the original molecule's similarity. Additionally, we demonstrate how our model operates by tracing the chemical space pathway to achieve optimal molecule optimization.",1
"Reinforcement learning (RL) tasks are challenging to implement, execute and test due to algorithmic instability, hyper-parameter sensitivity, and heterogeneous distributed communication patterns. We argue for the separation of logical component composition, backend graph definition, and distributed execution. To this end, we introduce RLgraph, a library for designing and executing reinforcement learning tasks in both static graph and define-by-run paradigms. The resulting implementations are robust, incrementally testable, and yield high performance across different deep learning frameworks and distributed backends.",0
"The implementation, execution, and testing of reinforcement learning (RL) tasks pose a challenge due to algorithmic instability, hyper-parameter sensitivity, and heterogeneous distributed communication patterns. Our suggestion is to separate the logical component composition, backend graph definition, and distributed execution. We present RLgraph, a library that enables the design and execution of reinforcement learning tasks in both static graph and define-by-run paradigms. The resulting implementations are robust, can be tested incrementally, and demonstrate high performance across various deep learning frameworks and distributed backends.",1
"Despite the recent progress in deep reinforcement learning field (RL), and, arguably because of it, a large body of work remains to be done in reproducing and carefully comparing different RL algorithms. We present catalyst.RL, an open source framework for RL research with a focus on reproducibility and flexibility. Main features of our library include large-scale asynchronous distributed training, easy-to-use configuration files with the complete list of hyperparameters for the particular experiments, efficient implementations of various RL algorithms and auxiliary tricks, such as frame stacking, n-step returns, value distributions, etc. To vindicate the usefulness of our framework, we evaluate it on a range of benchmarks in a continuous control, as well as on the task of developing a controller to enable a physiologically-based human model with a prosthetic leg to walk and run. The latter task was introduced at NeurIPS 2018 AI for Prosthetics Challenge, where our team took the 3rd place, capitalizing on the ability of catalyst.RL to train high-quality and sample-efficient RL agents.",0
"Despite recent advancements in the deep reinforcement learning field, there is still a significant amount of work to be done in replicating and comparing various RL algorithms. We introduce catalyst.RL, an open source framework designed for RL research with an emphasis on reproducibility and flexibility. Our library offers large-scale asynchronous distributed training, easy-to-use configuration files with a comprehensive list of experiment-specific hyperparameters, and efficient implementations of various RL algorithms and auxiliary techniques, such as frame stacking, n-step returns, and value distributions. To demonstrate the effectiveness of our framework, we tested it on different benchmarks for continuous control and also used it to develop a controller for a human model with a prosthetic leg to walk and run. Our team secured the 3rd place in the NeurIPS 2018 AI for Prosthetics Challenge, showcasing the ability of catalyst.RL to train high-quality and sample-efficient RL agents.",1
"Reinforcement learning (RL) typically defines a discount factor as part of the Markov Decision Process. The discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bellman equation. However, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences. In this work we revisit the fundamentals of discounting in RL and bridge this disconnect by implementing an RL agent that acts via hyperbolic discounting. We demonstrate that a simple approach approximates hyperbolic discount functions while still using familiar temporal-difference learning techniques in RL. Additionally, and independent of hyperbolic discounting, we make a surprising discovery that simultaneously learning value functions over multiple time-horizons is an effective auxiliary task which often improves over a strong value-based RL agent, Rainbow.",0
"The traditional method of reinforcement learning (RL) includes a discount factor in the Markov Decision Process, which calculates the value of future rewards through an exponential system and guarantees theoretical convergence of the Bellman equation. However, research in psychology, economics, and neuroscience indicates that humans and animals have hyperbolic time-preferences. To reconcile this discrepancy, we present an RL agent that uses hyperbolic discounting, which we accomplish by revisiting the fundamentals of discounting in RL. Our approach approximates hyperbolic discount functions using familiar temporal-difference learning techniques in RL. Interestingly, we also discover that simultaneously learning value functions over multiple time-horizons is a helpful auxiliary task that often outperforms a strong value-based RL agent, Rainbow, independent of hyperbolic discounting.",1
"Artificial Neural Networks (ANNs) are currently being used as function approximators in many state-of-the-art Reinforcement Learning (RL) algorithms. Spiking Neural Networks (SNNs) have been shown to drastically reduce the energy consumption of ANNs by encoding information in sparse temporal binary spike streams, hence emulating the communication mechanism of biological neurons. Due to their low energy consumption, SNNs are considered to be important candidates as co-processors to be implemented in mobile devices. In this work, the use of SNNs as stochastic policies is explored under an energy-efficient first-to-spike action rule, whereby the action taken by the RL agent is determined by the occurrence of the first spike among the output neurons. A policy gradient-based algorithm is derived considering a Generalized Linear Model (GLM) for spiking neurons. Experimental results demonstrate the capability of online trained SNNs as stochastic policies to gracefully trade energy consumption, as measured by the number of spikes, and control performance. Significant gains are shown as compared to the standard approach of converting an offline trained ANN into an SNN.",0
"Currently, Artificial Neural Networks (ANNs) are being utilized as function approximators in advanced Reinforcement Learning (RL) algorithms. Spiking Neural Networks (SNNs) have proven to substantially decrease ANNs' energy consumption by encoding information into sparse temporal binary spike streams, resembling the biological neurons' communication mechanism. As a result of their low energy consumption, SNNs are deemed significant candidates as co-processors for mobile devices. This study examines the utilization of SNNs as stochastic policies under an energy-efficient first-to-spike action rule, where the RL agent's action is determined by the initial output neuron spike. A policy gradient-based algorithm is developed, taking into account a Generalized Linear Model (GLM) for spiking neurons. Experimental findings indicate that online trained SNNs as stochastic policies can effectively balance energy consumption, measured by the number of spikes, and control performance. In comparison to the standard practice of converting an offline trained ANN into an SNN, significant gains are demonstrated.",1
"Traditional reinforcement learning agents learn from experience, past or present, gained through interaction with their environment. Our approach synthesizes experience, without requiring an agent to interact with their environment, by asking the policy directly ""Are there situations X, Y, and Z, such that in these situations you would select actions A, B, and C?"" In this paper we present Introspection Learning, an algorithm that allows for the asking of these types of questions of neural network policies. Introspection Learning is reinforcement learning algorithm agnostic and the states returned may be used as an indicator of the health of the policy or to shape the policy in a myriad of ways. We demonstrate the usefulness of this algorithm both in the context of speeding up training and improving robustness with respect to safety constraints.",0
"Traditional reinforcement learning agents acquire knowledge from their present or past experiences through interactions with their surroundings. Our approach involves creating experiences without the need for agents to interact with their environment. Instead, we directly ask the policy whether it would select certain actions in specific situations. This method is called Introspection Learning, and it allows for asking these types of questions to neural network policies. The algorithm is independent of the reinforcement learning method used, and the states obtained can be utilized for various purposes, such as assessing the policy's health or shaping it. We demonstrate the algorithm's usefulness in terms of speeding up training and improving the policy's robustness when dealing with safety constraints.",1
"Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent's learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.",0
"While reinforcement learning techniques can produce impressive results in simulations, there are two significant challenges when applying them to the real world. Firstly, generating samples is costly, and secondly, proficient but specialized policies may fail when faced with unexpected disturbances or novel situations during testing. As it is unfeasible to train separate policies for all possible scenarios, this study suggests learning how to adapt quickly and effectively to new tasks online. To facilitate sample-efficient learning, the authors propose an approach that utilizes meta-learning to train a dynamics model prior, which, when combined with recent data, can be swiftly adapted to the local context. The research demonstrates online adaptation for continuous control tasks on both simulated and real-world agents. Simulated agents exhibit online adaptation to various terrains, dynamic environments, and damaged body parts. The authors also showcase the significance of incorporating online adaptation into autonomous agents that operate in the real world by applying their method to a dynamic legged millirobot. The robot can adapt quickly to various challenges such as missing limbs, novel terrains and slopes, errors in pose estimation, and payload pulling.",1
"In this paper, the distributed edge caching problem in fog radio access networks (F-RANs) is investigated. By considering the unknown spatio-temporal content popularity and user preference, a user request model based on hidden Markov process is proposed to characterize the fluctuant spatio-temporal traffic demands in F-RANs. Then, the Q-learning method based on the reinforcement learning (RL) framework is put forth to seek the optimal caching policy in a distributed manner, which enables fog access points (F-APs) to learn and track the potential dynamic process without extra communications cost. Furthermore, we propose a more efficient Q-learning method with value function approximation (Q-VFA-learning) to reduce complexity and accelerate convergence. Simulation results show that the performance of our proposed method is superior to those of the traditional methods.",0
"This paper examines the issue of distributed edge caching in fog radio access networks (F-RANs). The authors propose a model based on a hidden Markov process to address the unknown spatio-temporal content popularity and user preference, which characterizes the ever-changing spatio-temporal traffic demands in F-RANs. They then introduce the Q-learning method, based on the reinforcement learning (RL) framework, to determine the optimal caching policy in a distributed manner. This allows fog access points (F-APs) to learn and track the potential dynamic process without incurring any additional communication costs. Additionally, the authors suggest a more efficient Q-learning method with value function approximation (Q-VFA-learning) to reduce complexity and speed up convergence. The simulation results demonstrate that the proposed approach outperforms traditional methods.",1
"We consider reinforcement learning in input-driven environments, where an exogenous, stochastic input process affects the dynamics of the system. Input processes arise in many applications, including queuing systems, robotics control with disturbances, and object tracking. Since the state dynamics and rewards depend on the input process, the state alone provides limited information for the expected future returns. Therefore, policy gradient methods with standard state-dependent baselines suffer high variance during training. We derive a bias-free, input-dependent baseline to reduce this variance, and analytically show its benefits over state-dependent baselines. We then propose a meta-learning approach to overcome the complexity of learning a baseline that depends on a long sequence of inputs. Our experimental results show that across environments from queuing systems, computer networks, and MuJoCo robotic locomotion, input-dependent baselines consistently improve training stability and result in better eventual policies.",0
"Our focus is on reinforcement learning within input-driven environments, where the dynamics of the system are influenced by an exogenous and stochastic input process. These input processes are prevalent in various scenarios such as robotics control with disturbances, object tracking, and queuing systems. Given that the state dynamics and rewards are affected by the input process, relying solely on the state for expected future returns is insufficient. Standard state-dependent baselines used in policy gradient methods result in high variance during training. To address this issue, we establish an input-dependent baseline that is free of bias and has been proven to perform better than state-dependent baselines. We also propose a meta-learning approach to facilitate the learning of a baseline that is dependent on a long sequence of inputs. Our experimental results demonstrate that input-dependent baselines enhance training stability and lead to better eventual policies across a range of environments such as computer networks, MuJoCo robotic locomotion, and queuing systems.",1
"Q-learning methods represent a commonly used class of algorithms in reinforcement learning: they are generally efficient and simple, and can be combined readily with function approximators for deep reinforcement learning (RL). However, the behavior of Q-learning methods with function approximation is poorly understood, both theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with modern deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains.",0
"Q-learning methods are widely used in reinforcement learning due to their simplicity and efficiency, and they can be easily combined with function approximators for deep RL. However, there is a lack of understanding about their behavior with function approximation, both theoretically and empirically. To address this issue, we conducted an experimental investigation using a ""unit testing"" framework that employs oracles to identify sources of error. Our focus was on function approximation, sampling error, and nonstationarity, and we checked if the trends found in oracle settings were consistent with modern deep RL methods. Our results show that large neural network architectures can enhance learning stability, compensate for overfitting, and introduce a novel sampling method that accounts for function approximation error, which leads to significant improvement in high-dimensional continuous control domains.",1
"Autonomous driving is a challenging domain that entails multiple aspects: a vehicle should be able to drive to its destination as fast as possible while avoiding collision, obeying traffic rules and ensuring the comfort of passengers. In this paper, we present a deep learning variant of thresholded lexicographic Q-learning for the task of urban driving. Our multi-objective DQN agent learns to drive on multi-lane roads and intersections, yielding and changing lanes according to traffic rules. We also propose an extension for factored Markov Decision Processes to the DQN architecture that provides auxiliary features for the Q function. This is shown to significantly improve data efficiency. We then show that the learned policy is able to zero-shot transfer to a ring road without sacrificing performance.",0
"The field of autonomous driving poses several challenges, including the need for a vehicle to reach its destination quickly without causing accidents, following traffic laws, and ensuring passengers' comfort. This study introduces a deep learning version of thresholded lexicographic Q-learning for urban driving. Our agent, using multi-objective DQN, learns to navigate multi-lane roads and intersections while adhering to traffic rules, changing lanes, and yielding. We also suggest an addition to the DQN architecture with factored Markov Decision Processes, which improves data efficiency. Our findings demonstrate that the policy learned by the agent can successfully transfer to a ring road without any performance loss.",1
"High-level driving behavior decision-making is an open-challenging problem for connected vehicle technology, especially in heterogeneous traffic scenarios. In this paper, a deep reinforcement learning based high-level driving behavior decision-making approach is proposed for connected vehicle in heterogeneous traffic situations. The model is composed of three main parts: a data preprocessor that maps hybrid data into a data format called hyper-grid matrix, a two-stream deep neural network that extracts the hidden features, and a deep reinforcement learning network that learns the optimal policy. Moreover, a simulation environment, which includes different heterogeneous traffic scenarios, is built to train and test the proposed method. The results demonstrate that the model has the capability to learn the optimal high-level driving policy such as driving fast through heterogeneous traffic without unnecessary lane changes. Furthermore, two separate models are used to compare with the proposed model, and the performances are analyzed in detail.",0
"Connected vehicle technology faces a complex issue in making high-level driving behavior decisions, particularly in scenarios involving diverse traffic. This paper proposes a deep reinforcement learning-based approach for making such decisions in heterogeneous traffic situations. The approach has three main components: a data preprocessor that converts hybrid data to hyper-grid matrix format, a two-stream deep neural network that uncovers hidden features, and a deep reinforcement learning network that learns the best policy. A simulation environment with various traffic scenarios is also developed to train and test the approach. Results show that the model can learn the optimal driving policy, enabling a fast drive through heterogeneous traffic without unnecessary lane changes. Additionally, two other models are compared with the proposed model, and their performances are extensively analyzed.",1
"Actor-critic methods can achieve incredible performance on difficult reinforcement learning problems, but they are also prone to instability. This is partly due to the interaction between the actor and critic during learning, e.g., an inaccurate step taken by one of them might adversely affect the other and destabilize the learning. To avoid such issues, we propose to regularize the learning objective of the actor by penalizing the temporal difference (TD) error of the critic. This improves stability by avoiding large steps in the actor update whenever the critic is highly inaccurate. The resulting method, which we call the TD-regularized actor-critic method, is a simple plug-and-play approach to improve stability and overall performance of the actor-critic methods. Evaluations on standard benchmarks confirm this.",0
"Although actor-critic methods can excel at challenging reinforcement learning tasks, they are susceptible to instability, partially due to the interplay between the actor and critic during learning. If one of them takes an imprecise step, it could harm the other and disrupt the learning process. To circumvent these issues, we suggest regulating the actor's learning objective by penalizing the critic's temporal difference (TD) error. This technique enhances stability by preventing significant changes in the actor update when the critic is notably inaccurate. Our resulting method, dubbed the TD-regularized actor-critic method, is a straightforward, adaptable approach to enhancing the stability and overall performance of actor-critic methods. Standard benchmarks have validated this.",1
"Real-time traffic volume inference is key to an intelligent city. It is a challenging task because accurate traffic volumes on the roads can only be measured at certain locations where sensors are installed. Moreover, the traffic evolves over time due to the influences of weather, events, holidays, etc. Existing solutions to the traffic volume inference problem often rely on dense GPS trajectories, which inevitably fail to account for the vehicles which carry no GPS devices or have them turned off. Consequently, the results are biased to taxicabs because they are almost always online for GPS tracking. In this paper, we propose a novel framework for the citywide traffic volume inference using both dense GPS trajectories and incomplete trajectories captured by camera surveillance systems. Our approach employs a high-fidelity traffic simulator and deep reinforcement learning to recover full vehicle movements from the incomplete trajectories. In order to jointly model the recovered trajectories and dense GPS trajectories, we construct spatiotemporal graphs and use multi-view graph embedding to encode the multi-hop correlations between road segments into real-valued vectors. Finally, we infer the citywide traffic volumes by propagating the traffic values of monitored road segments to the unmonitored ones through masked pairwise similarities. Extensive experiments with two big regions in a provincial capital city in China verify the effectiveness of our approach.",0
"The ability to accurately infer real-time traffic volume is crucial for a smart city, but it is a complex task due to the limited availability of sensors and the dynamic nature of traffic influenced by various factors. Traditional methods rely on GPS data, which is biased towards taxis and does not account for vehicles without GPS devices. In this study, we propose a new approach that combines dense GPS trajectories with incomplete trajectories captured by camera surveillance systems. We use a traffic simulator and deep reinforcement learning to recover missing vehicle movements and construct spatiotemporal graphs to model road segments' correlations. Our approach uses multi-view graph embedding to encode these correlations into real-valued vectors and infer citywide traffic volumes through pairwise similarities. Our experiments in two regions of a Chinese provincial capital city demonstrate the effectiveness of our approach.",1
"Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184% improvement on the constrained property optimization task.",0
"Fundamental to research in chemistry, biology, and social science is the generation of new graph structures that optimize given objectives while adhering to underlying rules. In molecular graph generation, for example, the aim is to discover new molecules with desired properties while following physical laws such as chemical valency. However, designing models to achieve this remains challenging due to the complexity of the rules involved. In this paper, we propose the Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model that uses reinforcement learning for goal-directed graph generation. GCPN is trained to optimize domain-specific rewards and adversarial loss through policy gradient and operates in an environment that includes domain-specific rules. Our experimental results demonstrate that GCPN can improve chemical property optimization by 61% over state-of-the-art baselines while resembling known molecules and can achieve a 184% improvement on the constrained property optimization task.",1
"Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is sometimes easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.",0
"For Reinforcement Learning (RL) agents, the difficulty lies in knowing the appropriate action to take when there are multiple options available in each state. The presence of redundant or irrelevant actions exacerbates this challenge. To overcome this, identifying which actions should not be taken is sometimes easier. This study introduces the Action-Elimination Deep Q-Network (AE-DQN) architecture that incorporates an Action Elimination Network (AEN) with a Deep RL algorithm to eliminate sub-optimal actions. The AEN is trained to predict invalid actions using an external elimination signal provided by the environment. Simulations demonstrate that AE-DQN significantly improves both speed and robustness over vanilla DQN in text-based games with over a thousand discrete actions.",1
"Deep hashing methods have received much attention recently, which achieve promising results by taking advantage of the strong representation power of deep networks. However, most existing deep hashing methods learn a whole set of hashing functions independently, while ignore the correlations between different hashing functions that can promote the retrieval accuracy greatly. Inspired by the sequential decision ability of deep reinforcement learning, we propose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH). Our proposed DRLIH approach models the hashing learning problem as a sequential decision process, which learns each hashing function by correcting the errors imposed by previous ones and promotes retrieval accuracy. To the best of our knowledge, this is the first work to address hashing problem from deep reinforcement learning perspective. The main contributions of our proposed DRLIH approach can be summarized as follows: (1) We propose a deep reinforcement learning hashing network. In the proposed network, we utilize recurrent neural network (RNN) as agents to model the hashing functions, which take actions of projecting images into binary codes sequentially, so that the current hashing function learning can take previous hashing functions' error into account. (2) We propose a sequential learning strategy based on proposed DRLIH. We define the state as a tuple of internal features of RNN's hidden layers and image features, which can reflect history decisions made by the agents. We also propose an action group method to enhance the correlation of hash functions in the same group. Experiments on three widely-used datasets demonstrate the effectiveness of our proposed DRLIH approach.",0
"Recent attention has been given to deep hashing methods, which utilize the strong representation power of deep networks to achieve promising results. However, existing methods tend to learn a set of hashing functions independently, disregarding the correlations between functions that can greatly improve retrieval accuracy. To address this issue, we propose a novel approach called Deep Reinforcement Learning for Image Hashing (DRLIH). Our DRLIH approach models the hashing learning problem as a sequential decision process, utilizing deep reinforcement learning to correct errors and promote retrieval accuracy. This is the first work to approach hashing from a deep reinforcement learning perspective. Our proposed approach involves a deep reinforcement learning hashing network, which uses recurrent neural network (RNN) agents to model the hashing functions in a sequential manner. This allows previous hashing functions' errors to be taken into account during current learning. We also propose a sequential learning strategy based on DRLIH, defining the state as a tuple of internal features of RNN's hidden layers and image features to reflect agents' history decisions. Additionally, we introduce an action group method to enhance the correlation of hash functions in the same group. Our experiments demonstrate the effectiveness of our proposed DRLIH approach on three widely-used datasets.",1
"The success of popular algorithms for deep reinforcement learning, such as policy-gradients and Q-learning, relies heavily on the availability of an informative reward signal at each timestep of the sequential decision-making process. When rewards are only sparsely available during an episode, or a rewarding feedback is provided only after episode termination, these algorithms perform sub-optimally due to the difficultly in credit assignment. Alternatively, trajectory-based policy optimization methods, such as cross-entropy method and evolution strategies, do not require per-timestep rewards, but have been found to suffer from high sample complexity by completing forgoing the temporal nature of the problem. Improving the efficiency of RL algorithms in real-world problems with sparse or episodic rewards is therefore a pressing need. In this work, we introduce a self-imitation learning algorithm that exploits and explores well in the sparse and episodic reward settings. We view each policy as a state-action visitation distribution and formulate policy optimization as a divergence minimization problem. We show that with Jensen-Shannon divergence, this divergence minimization problem can be reduced into a policy-gradient algorithm with shaped rewards learned from experience replays. Experimental results indicate that our algorithm works comparable to existing algorithms in environments with dense rewards, and significantly better in environments with sparse and episodic rewards. We then discuss limitations of self-imitation learning, and propose to solve them by using Stein variational policy gradient descent with the Jensen-Shannon kernel to learn multiple diverse policies. We demonstrate its effectiveness on a challenging variant of continuous-control MuJoCo locomotion tasks.",0
"Deep reinforcement learning algorithms, such as policy-gradients and Q-learning, rely heavily on an informative reward signal at each timestep for successful performance. However, when rewards are sparse or only provided after the episode ends, these algorithms struggle due to difficulty in credit assignment. Trajectory-based policy optimization methods, such as cross-entropy and evolution strategies, do not require per-timestep rewards but are limited by high sample complexity. Therefore, there is a need to improve the efficiency of RL algorithms in real-world problems with sparse or episodic rewards. This study introduces a self-imitation learning algorithm that performs well in such settings. The algorithm views each policy as a visitation distribution and optimizes it through divergence minimization, resulting in a policy-gradient algorithm with shaped rewards learned from experience replays. Experimental results show that the algorithm works comparably to existing methods in dense reward environments and significantly better in sparse and episodic reward environments. Limitations of self-imitation learning are discussed, and Stein variational policy gradient descent with the Jensen-Shannon kernel is proposed to overcome them by learning multiple diverse policies. The effectiveness of this method is demonstrated on a challenging variant of continuous-control MuJoCo locomotion tasks.",1
"We consider a problem of learning the reward and policy from expert examples under unknown dynamics. Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal rewards. Our method simultaneously learns empowerment through variational information maximization along with the reward and policy under the adversarial learning formulation. We evaluate our approach on various high-dimensional complex control tasks. We also test our learned rewards in challenging transfer learning problems where training and testing environments are made to be different from each other in terms of dynamics or structure. The results show that our proposed method not only learns near-optimal rewards and policies that are matching expert behavior but also performs significantly better than state-of-the-art inverse reinforcement learning algorithms.",0
"The objective of our study is to acquire knowledge of the reward and policy from the expert examples amidst unknown dynamics. Our suggested technique employs generative adversarial networks and incorporates the empowerment-regularized maximum-entropy inverse reinforcement learning to grasp rewards and policies that are almost optimal. Our empowerment-based regularization ensures that the policy does not overfit to the expert demonstrations, resulting in more generalized behaviors that result in near-optimal rewards. Our approach learns empowerment, reward, and policy simultaneously through variational information maximization within the adversarial learning formulation. We have tested our method on various complex control tasks with high dimensions. Furthermore, we have evaluated our learned rewards on transfer learning problems that pose a challenge due to the differences in the dynamics or structure of the training and testing environments. Our results demonstrate that our proposed approach not only acquires near-optimal rewards and policies that match the expert behavior but also outperforms existing inverse reinforcement learning algorithms.",1
"Unsupervised learning of compact and relevant state representations has been proved very useful at solving complex reinforcement learning tasks. In this paper, we propose a recurrent capsule network that learns such representations by trying to predict the future observations in an agent's trajectory.",0
"The acquisition of concise and pertinent state representations through unsupervised learning has demonstrated its efficacy in tackling intricate reinforcement learning assignments. In this article, we present a recurrent capsule network that obtains these representations by endeavoring to anticipate the future observations in an agent's path.",1
"For the initial shoulder preoperative diagnosis, it is essential to obtain a three-dimensional (3D) bone mask from medical images, e.g., magnetic resonance (MR). However, obtaining high-resolution and dense medical scans is both costly and time-consuming. In addition, the imaging parameters for each 3D scan may vary from time to time and thus increase the variance between images. Therefore, it is practical to consider the bone extraction on low-resolution data which may influence imaging contrast and make the segmentation work difficult. In this paper, we present a joint segmentation for the humerus and scapula bones on a small dataset with low-contrast and high-shape-variability 3D MR images. The proposed network has a deep end-to-end architecture to obtain the initial 3D bone masks. Because the existing scarce and inaccurate human-labeled ground truth, we design a self-reinforced learning strategy to increase performance. By comparing with the non-reinforced segmentation and a classical multi-atlas method with joint label fusion, the proposed approach obtains better results.",0
"Obtaining a 3D bone mask from medical images, such as MR scans, is crucial for the initial shoulder preoperative diagnosis. However, this process is time-consuming and expensive due to the need for high-resolution and dense scans. Moreover, the imaging parameters may vary from scan to scan, leading to increased variance between images. Therefore, it may be necessary to extract bones from low-resolution data, which can affect imaging contrast and make segmentation challenging. In this study, we propose a joint segmentation method for humerus and scapula bones using a small dataset of low-contrast and high-shape-variability 3D MR images. Our deep end-to-end network is designed to obtain initial 3D bone masks, and we use a self-reinforced learning strategy to improve performance due to scarce and inaccurate ground truth data. Our proposed approach outperforms non-reinforced segmentation and classical multi-atlas methods with joint label fusion.",1
"Our research is focused on understanding and applying biological memory transfers to new AI systems that can fundamentally improve their performance, throughout their fielded lifetime experience. We leverage current understanding of biological memory transfer to arrive at AI algorithms for memory consolidation and replay. In this paper, we propose the use of generative memory that can be recalled in batch samples to train a multi-task agent in a pseudo-rehearsal manner. We show results motivating the need for task-agnostic separation of latent space for the generative memory to address issues of catastrophic forgetting in lifelong learning.",0
"The main objective of our research is to gain a comprehensive understanding of biological memory transfers and utilize them to enhance the performance of AI systems throughout their operational lifespan. We utilize our existing knowledge of biological memory transfer to develop AI algorithms focused on memory consolidation and replay. Our proposed solution involves the use of generative memory, which can be efficiently retrieved in batches to train a multi-task agent in a pseudo-rehearsal manner. Our findings demonstrate the importance of separating the latent space for generative memory to mitigate issues related to catastrophic forgetting during lifelong learning.",1
"We present a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key insight is that DRL algorithms can be decomposed as the combination of some statistical estimator and a method for imputing a return distribution consistent with that set of statistics. With this new understanding, we are able to provide improved analyses of existing DRL algorithms as well as construct a new algorithm (EDRL) based upon estimation of the expectiles of the return distribution. We compare EDRL with existing methods on a variety of MDPs to illustrate concrete aspects of our analysis, and develop a deep RL variant of the algorithm, ER-DQN, which we evaluate on the Atari-57 suite of games.",0
"A comprehensive framework is presented in this paper to design and analyze distributional reinforcement learning (DRL) algorithms. The framework involves recursively estimating statistical parameters of the return distribution. The authors suggest that DRL algorithms can be broken down into two parts: a statistical estimator and a method for imputing a return distribution that is consistent with the set of statistics. This new understanding has led to improved analyses of current DRL algorithms and the creation of a new algorithm (EDRL) based on the estimation of expectiles of the return distribution. To demonstrate the effectiveness of the framework, EDRL is compared to existing methods on various MDPs. Additionally, the authors introduce a deep RL variant of the EDRL algorithm, named ER-DQN, which is evaluated on the Atari-57 suite of games.",1
"Since their introduction a year ago, distributional approaches to reinforcement learning (distributional RL) have produced strong results relative to the standard approach which models expected values (expected RL). However, aside from convergence guarantees, there have been few theoretical results investigating the reasons behind the improvements distributional RL provides. In this paper we begin the investigation into this fundamental question by analyzing the differences in the tabular, linear approximation, and non-linear approximation settings. We prove that in many realizations of the tabular and linear approximation settings, distributional RL behaves exactly the same as expected RL. In cases where the two methods behave differently, distributional RL can in fact hurt performance when it does not induce identical behaviour. We then continue with an empirical analysis comparing distributional and expected RL methods in control settings with non-linear approximators to tease apart where the improvements from distributional RL methods are coming from.",0
"Distributional approaches to reinforcement learning (distributional RL) have shown promising results in comparison to the standard approach of modeling expected values (expected RL) since their introduction a year ago. However, despite convergence guarantees, there have been limited theoretical findings that examine the reasons behind the success of distributional RL. This paper addresses this fundamental question by analyzing differences in tabular, linear approximation, and non-linear approximation settings. Our study reveals that distributional RL and expected RL behave identically in many tabular and linear approximation scenarios. However, in cases where distributional RL does not induce identical behavior, it can negatively impact performance. We also conducted an empirical analysis of distributional and expected RL in control settings with non-linear approximators to identify where the improvements from distributional RL arise.",1
"Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance.",0
"Though reinforcement learning shows promise for solving control problems, its practical use is hindered by the difficulty of engineering reward functions. Establishing objectives and tasks for autonomous machines is challenging, with reward functions and goal states traditionally used to communicate objectives. However, humans can communicate objectives by merely describing or demonstrating them. This raises the question of how to create learning algorithms that allow us to instruct machines on what we want them to achieve. This study delves into the issue of grounding language commands as reward functions using inverse reinforcement learning. It argues that language-conditioned rewards are more transferable than language-conditioned policies to new environments. The authors propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. The model learns rewards that can transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands. In contrast, directly learning a language-conditioned policy results in poor performance.",1
"Deep Reinforcement Learning has shown great success in a variety of control tasks. However, it is unclear how close we are to the vision of putting Deep RL into practice to solve real world problems. In particular, common practice in the field is to train policies on largely deterministic simulators and to evaluate algorithms through training performance alone, without a train/test distinction to ensure models generalise and are not overfitted. Moreover, it is not standard practice to check for generalisation under domain shift, although robustness to such system change between training and testing would be necessary for real-world Deep RL control, for example, in robotics. In this paper we study these issues by first characterising the sources of uncertainty that provide generalisation challenges in Deep RL. We then provide a new benchmark and thorough empirical evaluation of generalisation challenges for state of the art Deep RL methods. In particular, we show that, if generalisation is the goal, then common practice of evaluating algorithms based on their training performance leads to the wrong conclusions about algorithm choice. Finally, we evaluate several techniques for improving generalisation and draw conclusions about the most robust techniques to date.",0
"The success of Deep Reinforcement Learning in various control tasks is evident, but its practical application to solve real-world problems remains uncertain. The current practice in the field involves training policies on deterministic simulators and evaluating algorithms solely based on training performance, without a train/test distinction to ensure models' generalization and prevent overfitting. Additionally, there is no standard practice to test for generalization under domain shift, which is crucial in real-world Deep RL control, such as robotics. This research aims to address these issues by identifying the sources of uncertainty that pose generalization challenges in Deep RL and providing a new benchmark for empirical evaluation of state-of-the-art Deep RL methods. The study shows that evaluating algorithms based on training performance leads to incorrect conclusions about algorithm choice if generalization is the goal. Finally, the research evaluates various techniques for improving generalization and identifies the most robust techniques.",1
"Robust MDPs (RMDPs) can be used to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution are determined by the ambiguity set---the set of plausible transition probabilities---which is usually constructed as a multi-dimensional confidence region. Existing methods construct ambiguity sets as confidence regions using concentration inequalities which leads to overly conservative solutions. This paper proposes a new paradigm that can achieve better solutions with the same robustness guarantees without using confidence regions as ambiguity sets. To incorporate prior knowledge, our algorithms optimize the size and position of ambiguity sets using Bayesian inference. Our theoretical analysis shows the safety of the proposed method, and the empirical results demonstrate its practical promise.",0
"Reinforcement learning can use Robust MDPs (RMDPs) to generate policies that have guaranteed worst-case outcomes. The reliability and strength of an RMDP solution depend on the ambiguity set, which represents the possible transition probabilities and is typically a multi-dimensional confidence region. However, current methods construct ambiguity sets through concentration inequalities, which can result in excessively cautious solutions. This study proposes a new approach that can achieve the same level of robustness without relying on confidence regions as ambiguity sets. Our algorithms optimize ambiguity sets by leveraging prior knowledge through Bayesian inference. The proposed method is theoretically sound and shows practical potential through empirical results.",1
"A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. In sparse-reward environments, the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended appears crucial to enable sample efficient learning. However, reinforcement learning agents have only recently been endowed with such capacity for hindsight. In this paper, we demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms. Our experiments on a diverse selection of sparse-reward environments show that hindsight leads to a remarkable increase in sample efficiency.",0
"To pursue different goals in various episodes, a reinforcement learning agent requires a goal-conditional policy. Such policies have the potential to generalize desirable behavior to unknown goals and allow higher-level planning based on subgoals. In sparse-reward settings, the ability to exploit information on the extent to which an arbitrary goal has been achieved while another goal was intended is crucial for efficient learning. However, incorporating such hindsight capacity in reinforcement learning agents is recent. This paper demonstrates how hindsight can be introduced to policy gradient methods, which can be generalized to a wide range of successful algorithms. The experiments conducted on various sparse-reward environments illustrate that hindsight can significantly increase sample efficiency.",1
"Building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning. However, web navigation tasks are difficult for current deep reinforcement learning (RL) models due to the large discrete action space and the varying number of actions between the states. In this work, we introduce DOM-Q-NET, a novel architecture for RL-based web navigation to address both of these problems. It parametrizes Q functions with separate networks for different action categories: clicking a DOM element and typing a string input. Our model utilizes a graph neural network to represent the tree-structured HTML of a standard web page. We demonstrate the capabilities of our model on the MiniWoB environment where we can match or outperform existing work without the use of expert demonstrations. Furthermore, we show 2x improvements in sample efficiency when training in the multi-task setting, allowing our model to transfer learned behaviours across tasks.",0
"Significant advancements in knowledge understanding and representation learning can be achieved by developing agents that can interact with the web. However, existing deep reinforcement learning (RL) models struggle with web navigation tasks due to the vast discrete action space and varying number of actions between states. To overcome these challenges, we present DOM-Q-NET, a new RL-based web navigation architecture. Our approach leverages separate networks to parameterize Q functions for different action categories, namely, clicking a DOM element and typing a string input. Additionally, we utilize a graph neural network to represent the tree-structured HTML of a standard web page. Our model achieves impressive results on the MiniWoB environment, matching or exceeding existing work without the need for expert demonstrations. Moreover, we demonstrate that our model's sample efficiency is improved twofold when training in the multi-task setting, enabling it to transfer learned behaviours across tasks.",1
"Thompson sampling (TS) is a class of algorithms for sequential decision-making, which requires maintaining a posterior distribution over a model. However, calculating exact posterior distributions is intractable for all but the simplest models. Consequently, efficient computation of an approximate posterior distribution is a crucial problem for scalable TS with complex models, such as neural networks. In this paper, we use distribution optimization techniques to approximate the posterior distribution, solved via Wasserstein gradient flows. Based on the framework, a principled particle-optimization algorithm is developed for TS to approximate the posterior efficiently. Our approach is scalable and does not make explicit distribution assumptions on posterior approximations. Extensive experiments on both synthetic data and real large-scale data demonstrate the superior performance of the proposed methods.",0
"Sequential decision-making using Thompson sampling (TS) involves maintaining a posterior distribution over a model, but exact calculations of this distribution are only feasible for simple models. To address this issue for TS with complex models, such as neural networks, approximating the posterior distribution efficiently is crucial. In this study, we utilize distribution optimization techniques and solve through Wasserstein gradient flows to approximate the posterior distribution. Our particle-optimization algorithm for TS is developed based on this framework, which is scalable and does not require explicit assumptions on posterior approximations. Extensive experiments, including synthetic data and large-scale real data, demonstrate the superior performance of our proposed methods.",1
"Deep reinforcement learning (DRL) has gained a lot of attention in recent years, and has been proven to be able to play Atari games and Go at or above human levels. However, those games are assumed to have a small fixed number of actions and could be trained with a simple CNN network. In this paper, we study a special class of Asian popular card games called Dou Di Zhu, in which two adversarial groups of agents must consider numerous card combinations at each time step, leading to huge number of actions. We propose a novel method to handle combinatorial actions, which we call combinational Q-learning (CQL). We employ a two-stage network to reduce action space and also leverage order-invariant max-pooling operations to extract relationships between primitive actions. Results show that our method prevails over state-of-the art methods like naive Q-learning and A3C. We develop an easy-to-use card game environments and train all agents adversarially from sractch, with only knowledge of game rules and verify that our agents are comparative to humans. Our code to reproduce all reported results will be available online.",0
"Recently, Deep reinforcement learning (DRL) has received considerable attention for its ability to perform at or above human levels in games such as Atari and Go. However, these games have a limited number of actions that can be trained using a simple CNN network. In this study, we investigated a particular type of popular Asian card game known as Dou Di Zhu, which requires two opposing groups of agents to consider numerous card combinations at each time step, leading to a vast number of actions. To address this issue, we proposed a new approach called combinational Q-learning (CQL) to handle combinational actions. We utilized a two-stage network to reduce the action space and order-invariant max-pooling operations to extract the relationships between primitive actions. Our results demonstrate that our method outperforms other state-of-the-art techniques, such as naive Q-learning and A3C. We developed a user-friendly card game environment and trained all agents adversarially from scratch with only knowledge of game rules, and our agents were shown to be comparable to humans. Our code to reproduce all reported results will be available online.",1
"We present a method for fast training of vision based control policies on real robots. The key idea behind our method is to perform multi-task Reinforcement Learning with auxiliary tasks that differ not only in the reward to be optimized but also in the state-space in which they operate. In particular, we allow auxiliary task policies to utilize task features that are available only at training-time. This allows for fast learning of auxiliary policies, which subsequently generate good data for training the main, vision-based control policies. This method can be seen as an extension of the Scheduled Auxiliary Control (SAC-X) framework. We demonstrate the efficacy of our method by using both a simulated and real-world Ball-in-a-Cup game controlled by a robot arm. In simulation, our approach leads to significant learning speed-ups when compared to standard SAC-X. On the real robot we show that the task can be learned from-scratch, i.e., with no transfer from simulation and no imitation learning. Videos of our learned policies running on the real robot can be found at https://sites.google.com/view/rss-2019-sawyer-bic/.",0
"Our approach introduces a technique for quickly training vision-based control policies on physical robots. We achieve this by employing multi-task Reinforcement Learning with auxiliary tasks that vary in both the reward and state-space domains. Additionally, our method allows for the use of task features that are only available during the training phase, which leads to rapid learning of auxiliary policies. These policies then generate high-quality data for training the primary vision-based control policies. Our method is an expansion of the Scheduled Auxiliary Control (SAC-X) framework. We demonstrate the effectiveness of our approach by applying it to a Ball-in-a-Cup game controlled by a robot arm in both a simulated and real-world environment. Our results show significant learning speed enhancements in comparison to standard SAC-X in the simulation. On the physical robot, we prove that the task can be learned from scratch without any transfer from simulation or imitation learning. Videos of our trained policies running on the real robot are available at https://sites.google.com/view/rss-2019-sawyer-bic/.",1
"The goal of task transfer in reinforcement learning is migrating the action policy of an agent to the target task from the source task. Given their successes on robotic action planning, current methods mostly rely on two requirements: exactly-relevant expert demonstrations or the explicitly-coded cost function on target task, both of which, however, are inconvenient to obtain in practice. In this paper, we relax these two strong conditions by developing a novel task transfer framework where the expert preference is applied as a guidance. In particular, we alternate the following two steps: Firstly, letting experts apply pre-defined preference rules to select related expert demonstrates for the target task. Secondly, based on the selection result, we learn the target cost function and trajectory distribution simultaneously via enhanced Adversarial MaxEnt IRL and generate more trajectories by the learned target distribution for the next preference selection. The theoretical analysis on the distribution learning and convergence of the proposed algorithm are provided. Extensive simulations on several benchmarks have been conducted for further verifying the effectiveness of the proposed method.",0
"The aim of task transfer in reinforcement learning involves moving the action policy of an agent from the source task to the target task. Current methods, which have demonstrated success in robotic action planning, typically require either expert demonstrations or an explicitly-coded cost function for the target task. Unfortunately, these requirements are challenging to acquire in practical settings. To address this issue, we present a new task transfer framework that employs expert preference as guidance. Our approach involves two steps: 1) experts use predetermined preference rules to select relevant demonstrations for the target task, and 2) we learn the target cost function and trajectory distribution simultaneously using an enhanced Adversarial MaxEnt IRL method based on the selected demonstrations. We then use the learned target distribution to generate additional trajectories for the next preference selection. We provide theoretical analysis of the algorithm's convergence and distribution learning, and conduct extensive simulations on several benchmarks to demonstrate the effectiveness of our method.",1
"In this paper, we propose a new learning technique named message-dropout to improve the performance for multi-agent deep reinforcement learning under two application scenarios: 1) classical multi-agent reinforcement learning with direct message communication among agents and 2) centralized training with decentralized execution. In the first application scenario of multi-agent systems in which direct message communication among agents is allowed, the message-dropout technique drops out the received messages from other agents in a block-wise manner with a certain probability in the training phase and compensates for this effect by multiplying the weights of the dropped-out block units with a correction probability. The applied message-dropout technique effectively handles the increased input dimension in multi-agent reinforcement learning with communication and makes learning robust against communication errors in the execution phase. In the second application scenario of centralized training with decentralized execution, we particularly consider the application of the proposed message-dropout to Multi-Agent Deep Deterministic Policy Gradient (MADDPG), which uses a centralized critic to train a decentralized actor for each agent. We evaluate the proposed message-dropout technique for several games, and numerical results show that the proposed message-dropout technique with proper dropout rate improves the reinforcement learning performance significantly in terms of the training speed and the steady-state performance in the execution phase.",0
"This paper introduces a novel learning technique called ""message-dropout"" that enhances the performance of multi-agent deep reinforcement learning in two different scenarios. Firstly, in the classical multi-agent reinforcement learning scenario where agents communicate directly with each other, the message-dropout technique randomly drops out received messages in a block-wise manner during training and compensates for this by adjusting weights of the dropped-out block units with a correction probability. This technique handles the increased input dimension in multi-agent reinforcement learning with communication and makes learning more robust against communication errors during execution. Secondly, in the scenario of centralized training with decentralized execution, the proposed message-dropout technique is applied to Multi-Agent Deep Deterministic Policy Gradient (MADDPG), which uses a centralized critic to train a decentralized actor for each agent. The proposed technique is evaluated for several games, and the results show that the performance significantly improves in terms of training speed and steady-state performance in the execution phase with proper dropout rate.",1
"Ordinary stochastic neural networks mostly rely on the expected values of their weights to make predictions, whereas the induced noise is mostly used to capture the uncertainty, prevent overfitting and slightly boost the performance through test-time averaging. In this paper, we introduce variance layers, a different kind of stochastic layers. Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance. We show that such layers can learn surprisingly well, can serve as an efficient exploration tool in reinforcement learning tasks and provide a decent defense against adversarial attacks. We also show that a number of conventional Bayesian neural networks naturally converge to such zero-mean posteriors. We observe that in these cases such zero-mean parameterization leads to a much better training objective than conventional parameterizations where the mean is being learned.",0
"Ordinary stochastic neural networks utilize the expected values of their weights to make predictions and use induced noise to capture uncertainty, prevent overfitting, and slightly enhance performance through test-time averaging. This paper introduces variance layers, a distinct kind of stochastic layer where each weight follows a zero-mean distribution and is only parameterized by its variance. It is demonstrated that these layers can learn effectively, function as an efficient exploration tool in reinforcement learning, and serve as a decent defense against adversarial attacks. Additionally, conventional Bayesian neural networks naturally converge to these zero-mean posteriors, which leads to a much better training objective than traditional parameterizations where the mean is being learned.",1
"In many real-world learning scenarios, features are only acquirable at a cost constrained under a budget. In this paper, we propose a novel approach for cost-sensitive feature acquisition at the prediction-time. The suggested method acquires features incrementally based on a context-aware feature-value function. We formulate the problem in the reinforcement learning paradigm, and introduce a reward function based on the utility of each feature. Specifically, MC dropout sampling is used to measure expected variations of the model uncertainty which is used as a feature-value function. Furthermore, we suggest sharing representations between the class predictor and value function estimator networks. The suggested approach is completely online and is readily applicable to stream learning setups. The solution is evaluated on three different datasets including the well-known MNIST dataset as a benchmark as well as two cost-sensitive datasets: Yahoo Learning to Rank and a dataset in the medical domain for diabetes classification. According to the results, the proposed method is able to efficiently acquire features and make accurate predictions.",0
"In numerous instances of real-life learning, obtaining features incurs a cost that must be constrained within a budget. This research introduces a new technique for acquiring cost-sensitive features during prediction time. This approach involves acquiring features incrementally using a context-aware feature-value function. The problem is framed within the reinforcement learning paradigm, with a reward function based on the usefulness of each feature. The method employs MC dropout sampling to calculate the expected variations of model uncertainty, which serves as the feature-value function. Additionally, the study proposes sharing representations between the class predictor and value function estimator networks. This online approach is easily applied to stream learning setups. The technique is evaluated on three datasets, including the MNIST dataset for benchmarking, as well as two cost-sensitive datasets: Yahoo Learning to Rank and a medical dataset for diabetes classification. The results demonstrate that the proposed method is effective in acquiring features and producing accurate predictions.",1
"Finite-horizon lookahead policies are abundantly used in Reinforcement Learning and demonstrate impressive empirical success. Usually, the lookahead policies are implemented with specific planning methods such as Monte Carlo Tree Search (e.g. in AlphaZero). Referring to the planning problem as tree search, a reasonable practice in these implementations is to back up the value only at the leaves while the information obtained at the root is not leveraged other than for updating the policy. Here, we question the potency of this approach. Namely, the latter procedure is non-contractive in general, and its convergence is not guaranteed. Our proposed enhancement is straightforward and simple: use the return from the optimal tree path to back up the values at the descendants of the root. This leads to a $\gamma^h$-contracting procedure, where $\gamma$ is the discount factor and $h$ is the tree depth. To establish our results, we first introduce a notion called \emph{multiple-step greedy consistency}. We then provide convergence rates for two algorithmic instantiations of the above enhancement in the presence of noise injected to both the tree search stage and value estimation stage.",0
"Reinforcement Learning commonly employs finite-horizon lookahead policies, which have proven to be highly effective in practice. These policies are typically implemented using planning methods such as Monte Carlo Tree Search, as seen in AlphaZero. However, the conventional approach of backing up the value solely at the leaves and disregarding the information obtained at the root except for policy updates is not always effective. This method lacks the guarantee of convergence and is non-contractive. To overcome this, we suggest a simple alternative: using the return from the optimal tree path to back up the values at the descendants of the root, resulting in a $\gamma^h$-contracting approach. We introduce the concept of multiple-step greedy consistency and establish convergence rates for two algorithmic implementations of this approach in the presence of noise during both the tree search stage and value estimation stage.",1
"Deep learning has achieved remarkable successes in solving challenging reinforcement learning (RL) problems when dense reward function is provided. However, in sparse reward environment it still often suffers from the need to carefully shape reward function to guide policy optimization. This limits the applicability of RL in the real world since both reinforcement learning and domain-specific knowledge are required. It is therefore of great practical importance to develop algorithms which can learn from a binary signal indicating successful task completion or other unshaped, sparse reward signals. We propose a novel method called competitive experience replay, which efficiently supplements a sparse reward by placing learning in the context of an exploration competition between a pair of agents. Our method complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum. We evaluate our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm. Each task provides only binary rewards indicating whether or not the goal is achieved. Our method asymmetrically augments these sparse rewards for a pair of agents each learning the same task, creating a competitive game designed to drive exploration. Extensive experiments demonstrate that this method leads to faster converge and improved task performance.",0
"Although deep learning has achieved impressive results in solving challenging reinforcement learning (RL) problems that have a dense reward function, it still faces difficulties in sparse reward environments where the reward function must be carefully shaped to guide policy optimization. This limitation reduces the applicability of RL in the real world, where both reinforcement learning and domain-specific knowledge are required. Therefore, it is crucial to develop algorithms that can learn from a binary signal that indicates a successful task completion or other unshaped, sparse reward signals. Our proposed method, competitive experience replay, efficiently supplements sparse rewards by placing learning in the context of an exploration competition between a pair of agents. This approach complements the recently proposed hindsight experience replay (HER) by inducing an automatic exploratory curriculum. We evaluated our approach on the tasks of reaching various goal locations in an ant maze and manipulating objects with a robotic arm, both of which provide only binary rewards indicating goal achievement. Our method asymmetrically augments these sparse rewards for a pair of agents learning the same task, creating a competitive game designed to drive exploration. Extensive experiments demonstrate that this method leads to faster convergence and improved task performance.",1
"When using reinforcement learning (RL) algorithms it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on an agent's performance, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is currently interest among researchers in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures. One relatively unexplored method of adapting approximation architectures involves using feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. In this article we will: (a) informally discuss the potential advantages offered by such methods; (b) introduce a new algorithm based on such methods which adapts a state aggregation approximation architecture on-line and is designed for use in conjunction with SARSA; (c) provide theoretical results, in a policy evaluation setting, regarding this particular algorithm's complexity, convergence properties and potential to reduce VF error; and finally (d) test experimentally the extent to which this algorithm can improve performance given a number of different test problems. Taken together our results suggest that our algorithm (and potentially such methods more generally) can provide a versatile and computationally lightweight means of significantly boosting RL performance given suitable conditions which are commonly encountered in practice.",0
"When dealing with large state spaces in reinforcement learning (RL) algorithms, it is typical to use an approximation architecture for the value function (VF). However, the choice of architecture can greatly impact an agent's performance, and selecting an appropriate one can be a difficult task. As a result, researchers are interested in the possibility of RL algorithms adaptively generating approximation architectures. One promising method involves using feedback on an agent's state visit frequency to guide the level of approximation in different areas of the state space. In this article, we will explore the benefits of this approach, introduce a new algorithm that adapts a state aggregation approximation architecture online and is intended for use with SARSA, provide theoretical results on its complexity, convergence properties, and ability to reduce VF error in policy evaluation, and finally, experimentally test its effectiveness on a variety of problems. Our findings suggest that this algorithm (and similar methods) can significantly enhance RL performance in common practical scenarios, while remaining versatile and computationally efficient.",1
"Many complex domains, such as robotics control and real-time strategy (RTS) games, require an agent to learn a continuous control. In the former, an agent learns a policy over $\mathbb{R}^d$ and in the latter, over a discrete set of actions each of which is parametrized by a continuous parameter. Such problems are naturally solved using policy based reinforcement learning (RL) methods, but unfortunately these often suffer from high variance leading to instability and slow convergence. Unnecessary variance is introduced whenever policies over bounded action spaces are modeled using distributions with unbounded support by applying a transformation $T$ to the sampled action before execution in the environment. Recently, the variance reduced clipped action policy gradient (CAPG) was introduced for actions in bounded intervals, but to date no variance reduced methods exist when the action is a direction, something often seen in RTS games. To this end we introduce the angular policy gradient (APG), a stochastic policy gradient method for directional control. With the marginal policy gradients family of estimators we present a unified analysis of the variance reduction properties of APG and CAPG; our results provide a stronger guarantee than existing analyses for CAPG. Experimental results on a popular RTS game and a navigation task show that the APG estimator offers a substantial improvement over the standard policy gradient.",0
"Learning a continuous control is necessary in complex domains like robotics control and real-time strategy (RTS) games. The former involves an agent learning a policy over $\mathbb{R}^d$, while the latter involves learning a policy over a discrete set of actions with continuous parameters. Policy-based reinforcement learning (RL) methods are typically used to solve such problems, but they often suffer from high variance, leading to instability and slow convergence. This is because policies over bounded action spaces are modeled using distributions with unbounded support, which introduces unnecessary variance when a transformation $T$ is applied to the sampled action before execution in the environment. While the variance reduced clipped action policy gradient (CAPG) was introduced for actions in bounded intervals, no variance reduced methods exist for directional control, which is often seen in RTS games. To address this, we propose the angular policy gradient (APG), a stochastic policy gradient method for directional control. Our analysis using the marginal policy gradients family of estimators shows that APG offers a stronger guarantee for variance reduction than existing analyses for CAPG. Experimental results on a popular RTS game and a navigation task demonstrate that APG estimator significantly outperforms the standard policy gradient.",1
"We present the first model-free Reinforcement Learning (RL) algorithm to synthesise policies for an unknown Markov Decision Process (MDP), such that a linear time property is satisfied. The given temporal property is converted into a Limit Deterministic Buchi Automaton (LDBA) and a robust reward function is defined over the state-action pairs of the MDP according to the resulting LDBA. With this reward function, the policy synthesis procedure is ""constrained"" by the given specification. These constraints guide the MDP exploration so as to minimize the solution time by only considering the portion of the MDP that is relevant to satisfaction of the LTL property. This improves performance and scalability of the proposed method by avoiding an exhaustive update over the whole state space while the efficiency of standard methods such as dynamic programming is hindered by excessive memory requirements, caused by the need to store a full-model in memory. Additionally, we show that the RL procedure sets up a local value iteration method to efficiently calculate the maximum probability of satisfying the given property, at any given state of the MDP. We prove that our algorithm is guaranteed to find a policy whose traces probabilistically satisfy the LTL property if such a policy exists, and additionally we show that our method produces reasonable control policies even when the LTL property cannot be satisfied. The performance of the algorithm is evaluated via a set of numerical examples. We observe an improvement of one order of magnitude in the number of iterations required for the synthesis compared to existing approaches.",0
"Our study presents a novel Reinforcement Learning (RL) algorithm that can synthesize policies for an unknown Markov Decision Process (MDP) without the need for a model. The algorithm ensures that a linear time property is satisfied by converting the given temporal property into a Limit Deterministic Buchi Automaton (LDBA) and defining a robust reward function over the state-action pairs of the MDP based on the resulting LDBA. This reward function ""constrains"" the policy synthesis process by guiding MDP exploration to focus only on the relevant portion of the MDP that satisfies the LTL property. This approach enhances performance and scalability by avoiding exhaustive updates of the entire state space, unlike standard methods such as dynamic programming that require excessive memory requirements. Moreover, we demonstrate that the RL procedure enables a local value iteration method to efficiently calculate the maximum probability of satisfying the given property at any state of the MDP. Our algorithm is guaranteed to find a policy that satisfies the LTL property, if such a policy exists, and produces reasonable control policies even when the LTL property cannot be satisfied. We evaluate the algorithm's performance using numerical examples and observe a significant improvement in the number of iterations required for synthesis compared to existing approaches.",1
"Hierarchical reinforcement learning deals with the problem of breaking down large tasks into meaningful sub-tasks. Autonomous discovery of these sub-tasks has remained a challenging problem. We propose a novel method of learning sub-tasks by combining paradigms of routing in computer networks and graph based skill discovery within the options framework to define meaningful sub-goals. We apply the recent advancements of learning embeddings using Riemannian optimisation in the hyperbolic space to embed the state set into the hyperbolic space and create a model of the environment. In doing so we enforce a global topology on the states and are able to exploit this topology to learn meaningful sub-tasks. We demonstrate empirically, both in discrete and continuous domains, how these embeddings can improve the learning of meaningful sub-tasks.",0
"The main concern of hierarchical reinforcement learning is how to divide complex tasks into smaller, more manageable sub-tasks. It has been a difficult challenge to autonomously identify these sub-tasks. To address this issue, we propose a new approach that combines the concepts of routing in computer networks and graph-based skill discovery within the options framework to establish significant sub-goals. We utilize the latest developments in Riemannian optimization to embed the state set into the hyperbolic space and create an environment model. This allows us to implement a global topology on the states and utilize it to learn meaningful sub-tasks. We provide empirical evidence, in both discrete and continuous domains, of how these embeddings improve the learning of significant sub-tasks.",1
"In real-world scenarios, the observation data for reinforcement learning with continuous control is commonly noisy and part of it may be dynamically missing over time, which violates the assumption of many current methods developed for this. We addressed the issue within the framework of partially observable Markov Decision Process (POMDP) using a model-based method, in which the transition model is estimated from the incomplete and noisy observations using a newly proposed surrogate loss function with local approximation, while the policy and value function is learned with the help of belief imputation. For the latter purpose, a generative model is constructed and is seamlessly incorporated into the belief updating procedure of POMDP, which enables robust execution even under a significant incompleteness and noise. The effectiveness of the proposed method is verified on a collection of benchmark tasks, showing that our approach outperforms several compared methods under various challenging scenarios.",0
"Many current methods developed for reinforcement learning with continuous control assume that observation data is complete and noise-free, but in real-world scenarios, this is often not the case. Dynamic missing data can be a common occurrence, which can be problematic. To address this issue, we used a partially observable Markov decision process (POMDP) framework and a model-based method. We estimated the transition model from incomplete and noisy observations using a newly proposed surrogate loss function with local approximation. We learned the policy and value function using belief imputation and constructed a generative model to incorporate belief updating. This approach allows for robust execution even with significant incompleteness and noise. Our method was tested on a collection of benchmark tasks and was found to outperform several other methods under various challenging scenarios.",1
"We propose the use of Bayesian networks, which provide both a mean value and an uncertainty estimate as output, to enhance the safety of learned control policies under circumstances in which a test-time input differs significantly from the training set. Our algorithm combines reinforcement learning and end-to-end imitation learning to simultaneously learn a control policy as well as a threshold over the predictive uncertainty of the learned model, with no hand-tuning required. Corrective action, such as a return of control to the model predictive controller or human expert, is taken when the uncertainty threshold is exceeded. We validate our method on fully-observable and vision-based partially-observable systems using cart-pole and autonomous driving simulations using deep convolutional Bayesian neural networks. We demonstrate that our method is robust to uncertainty resulting from varying system dynamics as well as from partial state observability.",0
"To enhance the safety of learned control policies when a test-time input differs significantly from the training set, we suggest utilizing Bayesian networks. These networks provide both a mean value and an uncertainty estimate as output. Our algorithm uses a combination of reinforcement learning and end-to-end imitation learning to learn a control policy and a threshold over the predictive uncertainty of the learned model without any hand-tuning requirements. When the uncertainty threshold is exceeded, corrective action is taken, such as returning control to the model predictive controller or human expert. We validate our approach on fully-observable and vision-based partially-observable systems, including cart-pole and autonomous driving simulations, using deep convolutional Bayesian neural networks. Our method is robust to uncertainty caused by varying system dynamics and partial state observability.",1
"As the most successful variant and improvement for Trust Region Policy Optimization (TRPO), proximal policy optimization (PPO) has been widely applied across various domains with several advantages: efficient data utilization, easy implementation, and good parallelism. In this paper, a first-order gradient reinforcement learning algorithm called Policy Optimization with Penalized Point Probability Distance (POP3D), which is a lower bound to the square of total variance divergence is proposed as another powerful variant. Firstly, we talk about the shortcomings of several commonly used algorithms, by which our method is partly motivated. Secondly, we address to overcome these shortcomings by applying POP3D. Thirdly, we dive into its mechanism from the perspective of solution manifold. Finally, we make quantitative comparisons among several state-of-the-art algorithms based on common benchmarks. Simulation results show that POP3D is highly competitive compared with PPO. Besides, our code is released in https://github.com/paperwithcode/pop3d.",0
"Proximal policy optimization (PPO) has become the most successful and improved version of Trust Region Policy Optimization (TRPO) due to its efficient data utilization, ease of implementation, and good parallelism across various domains. This paper presents another powerful variant called Policy Optimization with Penalized Point Probability Distance (POP3D), a first-order gradient reinforcement learning algorithm that offers a lower bound to the square of total variance divergence. We first discuss the limitations of commonly used algorithms that partly motivated our approach. Then, we demonstrate how POP3D overcomes these limitations. We also provide insight into POP3D's mechanism from the perspective of the solution manifold. Finally, we showcase quantitative comparisons of several state-of-the-art algorithms based on common benchmarks. Our simulation results show that POP3D is highly competitive compared to PPO. Additionally, the code for POP3D is available on https://github.com/paperwithcode/pop3d.",1
"Employing one or more additional classifiers to break the self-learning loop in tracing-by-detection has gained considerable attention. Most of such trackers merely utilize the redundancy to address the accumulating label error in the tracking loop, and suffer from high computational complexity as well as tracking challenges that may interrupt all classifiers (e.g. temporal occlusions). We propose the active co-tracking framework, in which the main classifier of the tracker labels samples of the video sequence, and only consults auxiliary classifier when it is uncertain. Based on the source of the uncertainty and the differences of two classifiers (e.g. accuracy, speed, update frequency, etc.), different policies should be taken to exchange the information between two classifiers. Here, we introduce a reinforcement learning approach to find the appropriate policy by considering the state of the tracker in a specific sequence. The proposed method yields promising results in comparison to the best tracking-by-detection approaches.",0
"The use of additional classifiers to prevent the self-learning loop in tracing-by-detection has become popular. However, most of these trackers have high computational complexity and tracking difficulties, such as temporal occlusions. To address this, we present the active co-tracking framework, wherein the main classifier labels samples of the video sequence and only seeks the assistance of the auxiliary classifier when there is uncertainty. Depending on the cause of the uncertainty and the discrepancies between the classifiers (e.g. accuracy, speed, update frequency), different policies should be implemented to share information between them. To determine the appropriate policy for a specific sequence, we propose a reinforcement learning approach that considers the tracker's state. Our method shows promising results compared to the top tracking-by-detection approaches.",1
"We propose a new policy iteration theory as an important extension of soft policy iteration and Soft Actor-Critic (SAC), one of the most efficient model free algorithms for deep reinforcement learning. Supported by the new theory, arbitrary entropy measures that generalize Shannon entropy, such as Tsallis entropy and Renyi entropy, can be utilized to properly randomize action selection while fulfilling the goal of maximizing expected long-term rewards. Our theory gives birth to two new algorithms, i.e., Tsallis entropy Actor-Critic (TAC) and Renyi entropy Actor-Critic (RAC). Theoretical analysis shows that these algorithms can be more effective than SAC. Moreover, they pave the way for us to develop a new Ensemble Actor-Critic (EAC) algorithm in this paper that features the use of a bootstrap mechanism for deep environment exploration as well as a new value-function based mechanism for high-level action selection. Empirically we show that TAC, RAC and EAC can achieve state-of-the-art performance on a range of benchmark control tasks, outperforming SAC and several cutting-edge learning algorithms in terms of both sample efficiency and effectiveness.",0
"We introduce a novel policy iteration theory that expands upon soft policy iteration and Soft Actor-Critic (SAC), which is a highly efficient model-free algorithm for deep reinforcement learning. With the support of this new theory, we can utilize arbitrary entropy measures that go beyond Shannon entropy, such as Tsallis entropy and Renyi entropy, to randomize action selection in a way that maximizes expected long-term rewards. Our theory has led to the development of two new algorithms, namely Tsallis entropy Actor-Critic (TAC) and Renyi entropy Actor-Critic (RAC). Theoretical analysis indicates that these algorithms can be more effective than SAC. Furthermore, they have paved the way for the creation of a new Ensemble Actor-Critic (EAC) algorithm that incorporates a bootstrap mechanism for deep environment exploration and a value-function based mechanism for high-level action selection. Through empirical testing, we demonstrate that TAC, RAC, and EAC can achieve state-of-the-art performance in various benchmark control tasks, surpassing SAC and several other advanced learning algorithms in terms of both sample efficiency and effectiveness.",1
"Deep reinforcement learning has recently gained a focus on problems where policy or value functions are independent of goals. Evidence exists that the sampling of goals has a strong effect on the learning performance, but there is a lack of general mechanisms that focus on optimizing the goal sampling process. In this work, we present a simple and general goal masking method that also allows us to estimate a goal's difficulty level and thus realize a curriculum learning approach for deep RL. Our results indicate that focusing on goals with a medium difficulty level is appropriate for deep deterministic policy gradient (DDPG) methods, while an ""aim for the stars and reach the moon-strategy"", where hard goals are sampled much more often than simple goals, leads to the best learning performance in cases where DDPG is combined with for hindsight experience replay (HER). We demonstrate that the approach significantly outperforms standard goal sampling for different robotic object manipulation problems.",0
"Recently, there has been a growing interest in applying deep reinforcement learning to problems where policy or value functions are not reliant on specific goals. However, research suggests that the selection of goals can greatly impact the learning performance, yet there are no established methods for optimizing this selection process. Our research introduces a simple and versatile goal masking technique that enables the estimation of a goal's difficulty level, allowing for a curriculum learning approach to deep RL. Our findings reveal that focusing on moderately challenging goals is optimal for deep deterministic policy gradient (DDPG) methods, while a ""shoot for the stars and land on the moon"" strategy, where difficult goals are more frequently sampled than simple goals, yields the best results when combined with hindsight experience replay (HER). Through various robotic object manipulation tasks, we demonstrate that our approach surpasses standard goal sampling.",1
"Reinforcement learning is a promising approach to learning robot controllers. It has recently been shown that algorithms based on finite-difference estimates of the policy gradient are competitive with algorithms based on the policy gradient theorem. We propose a theoretical framework for understanding this phenomenon. Our key insight is that many dynamical systems (especially those of interest in robot control tasks) are \emph{nearly deterministic}---i.e., they can be modeled as a deterministic system with a small stochastic perturbation. We show that for such systems, finite-difference estimates of the policy gradient can have substantially lower variance than estimates based on the policy gradient theorem. We interpret these results in the context of counterfactual estimation. Finally, we empirically evaluate our insights in an experiment on the inverted pendulum.",0
"Learning robot controllers through reinforcement learning shows great potential. Recent studies have revealed that algorithms using finite-difference estimates of the policy gradient can compete with those using the policy gradient theorem. To explain this phenomenon, we propose a theoretical framework. Our research suggests that many dynamical systems, particularly those used in robot control tasks, are mostly deterministic but with a small stochastic perturbation. In such systems, finite-difference estimates of the policy gradient can provide more precise results than estimates based on the policy gradient theorem. We explain these findings in the context of counterfactual estimation and test them in an experiment on the inverted pendulum.",1
"In this paper, we propose a novel conditional-generative-adversarial-nets-based image captioning framework as an extension of traditional reinforcement-learning (RL)-based encoder-decoder architecture. To deal with the inconsistent evaluation problem among different objective language metrics, we are motivated to design some ""discriminator"" networks to automatically and progressively determine whether generated caption is human described or machine generated. Two kinds of discriminator architectures (CNN and RNN-based structures) are introduced since each has its own advantages. The proposed algorithm is generic so that it can enhance any existing RL-based image captioning framework and we show that the conventional RL training method is just a special case of our approach. Empirically, we show consistent improvements over all language evaluation metrics for different state-of-the-art image captioning models. In addition, the well-trained discriminators can also be viewed as objective image captioning evaluators",0
"In this article, we suggest a new framework for image captioning that is based on conditional-generative-adversarial-nets. This is an extension of the traditional RL-based encoder-decoder architecture. Our aim is to address the issue of inconsistent evaluation results across different objective language metrics. To achieve this, we have developed ""discriminator"" networks that can automatically determine whether a caption has been generated by a human or a machine. We have introduced two types of discriminator architectures - CNN and RNN-based - each with its own benefits. Our algorithm is versatile and can be used to improve any existing RL-based image captioning framework. We have demonstrated that the conventional RL training method is just a specific example of our approach. Through empirical testing, we have shown that our approach consistently outperforms other state-of-the-art image captioning models across multiple language evaluation metrics. Furthermore, the well-trained discriminators can also act as objective evaluators for image captioning.",1
"Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failed experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, and even small amount of advice is sufficient for the agent to achieve good performance.",0
"Reinforcement learning (RL) is faced with a difficult problem known as ""sparse reward"". To address this issue, Hindsight Experience Replay (HER) is used to convert a failed experience into a successful one by relabeling the goals. However, HER's effectiveness is limited due to its lack of a compact and universal goal representation. To solve this problem, we introduce Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient RL technique that extends HER by using natural language as the goal representation. Our analysis shows that ACTRCE is capable of solving difficult 3D navigation tasks efficiently, whereas HER with non-language goal representation failed to learn. Additionally, we demonstrate that the agent can generalize to unseen instructions and even instructions with unseen lexicons when language goal representations are used. We emphasize the importance of hindsight advice in solving challenging tasks and show that even a small amount of advice can lead to good performance.",1
"We study active object tracking, where a tracker takes visual observations (i.e., frame sequences) as input and produces the corresponding camera control signals as output (e.g., move forward, turn left, etc.). Conventional methods tackle tracking and camera control tasks separately, and the resulting system is difficult to tune jointly. These methods also require significant human efforts for image labeling and expensive trial-and-error system tuning in the real world. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning. A ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for successful training. The tracker trained in simulators (ViZDoom and Unreal Engine) demonstrates good generalization behaviors in the case of unseen object moving paths, unseen object appearances, unseen backgrounds, and distracting objects. The system is robust and can restore tracking after occasional lost of the target being tracked. We also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios. We demonstrate successful examples of such transfer, via experiments over the VOT dataset and the deployment of a real-world robot using the proposed active tracker trained in simulation.",0
"Our focus is on active object tracking, where a tracker receives visual observations (i.e., frame sequences) and produces camera control signals (e.g., move forward, turn left, etc.) as output. Traditional methods address tracking and camera control separately, which makes it challenging to tune the system effectively. These approaches also require significant human effort for image labeling and expensive trial-and-error system tuning in real-world scenarios. In this paper, we propose an end-to-end solution using deep reinforcement learning. We use a ConvNet-LSTM function approximator for direct frame-to-action prediction and introduce an environment augmentation technique and customized reward function for successful training. The tracker trained in simulators (ViZDoom and Unreal Engine) demonstrates robust generalization behaviors, even in the face of unseen object moving paths, appearances, backgrounds, and distracting objects. The system can recover tracking after occasional losses of the target, and we find that the tracking ability acquired in the simulators can potentially transfer to real-world scenarios. We demonstrate this transfer through experiments on the VOT dataset and the deployment of a real-world robot using the proposed active tracker trained in simulation.",1
"To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.",0
"For intelligent agents to become more accessible and useful, they must learn complex behaviors from non-expert human users while efficiently using their sparse feedback. Recent studies have shown that human feedback can be a critique of the agent's current behavior rather than an alternate reward signal. The COnvergent Actor-Critic by Humans (COACH) algorithm has been developed to make direct policy updates based on human feedback. Our work builds on COACH by using a deep neural network to represent the agent's policy. We have made several modifications to the original COACH algorithm to enable successful learning of behaviors from high-dimensional observations with reduced sample complexity. Our Deep COACH algorithm has been tested in the 3D world of Minecraft, where an agent learns to perform tasks using only real-time human feedback in 10-15 minutes.",1
"In reinforcement learning, a decision needs to be made at some point as to whether it is worthwhile to carry on with the learning process or to terminate it. In many such situations, stochastic elements are often present which govern the occurrence of rewards, with the sequential occurrences of positive rewards randomly interleaved with negative rewards. For most practical learners, the learning is considered useful if the number of positive rewards always exceeds the negative ones. A situation that often calls for learning termination is when the number of negative rewards exceeds the number of positive rewards. However, while this seems reasonable, the error of premature termination, whereby termination is enacted along with the conclusion of learning failure despite the positive rewards eventually far outnumber the negative ones, can be significant. In this paper, using combinatorial analysis we study the error probability in wrongly terminating a reinforcement learning activity which undermines the effectiveness of an optimal policy, and we show that the resultant error can be quite high. Whilst we demonstrate mathematically that such errors can never be eliminated, we propose some practical mechanisms that can effectively reduce such errors. Simulation experiments have been carried out, the results of which are in close agreement with our theoretical findings.",0
"In reinforcement learning, deciding when to continue or end the learning process is crucial. Often, stochastic elements are present that determine the occurrence of rewards, with positive and negative rewards randomly alternating. For practical learners, the learning is considered valuable if positive rewards exceed negative ones. If negative rewards outnumber positive ones, it may be necessary to terminate the learning process. However, premature termination can occur when learning is erroneously deemed unsuccessful despite eventually having more positive than negative rewards. This paper uses combinatorial analysis to investigate the probability of such errors and shows that they can be significant. While it is impossible to eliminate such errors, practical mechanisms can reduce them. Simulation experiments confirm our theoretical findings.",1
"In reinforcement learning episodes, the rewards and punishments are often non-deterministic, and there are invariably stochastic elements governing the underlying situation. Such stochastic elements are often numerous and cannot be known in advance, and they have a tendency to obscure the underlying rewards and punishments patterns. Indeed, if stochastic elements were absent, the same outcome would occur every time and the learning problems involved could be greatly simplified. In addition, in most practical situations, the cost of an observation to receive either a reward or punishment can be significant, and one would wish to arrive at the correct learning conclusion by incurring minimum cost. In this paper, we present a stochastic approach to reinforcement learning which explicitly models the variability present in the learning environment and the cost of observation. Criteria and rules for learning success are quantitatively analyzed, and probabilities of exceeding the observation cost bounds are also obtained.",0
"The rewards and punishments in reinforcement learning episodes are often unpredictable due to stochastic elements that govern the situation. These elements are numerous and cannot be anticipated, making it challenging to identify patterns in the rewards and punishments. If the stochastic elements were absent, the learning problems would be simpler, and the same outcome would occur every time. However, in real-world scenarios, the cost of observation to receive a reward or punishment can be substantial, and it is crucial to minimize it when arriving at the right learning conclusion. This paper proposes a stochastic approach to reinforcement learning that explicitly models the variability and cost of observation. The study analyzes the criteria and rules for learning success quantitatively and calculates the probabilities of exceeding the observation cost limits.",1
"We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,~policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",0
"Our focus is on continuous action reinforcement learning problems where the agent's interaction with the environment must be limited to safe policies. To address these issues, we introduce constrained Markov decision processes (CMDPs) and safe policy optimization algorithms that utilize a Lyapunov approach. Our algorithms work with standard policy gradient (PG) methods, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy. Our algorithms guarantee near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the feasible solutions set induced by state-dependent linearized Lyapunov constraints. Our algorithms are more data-efficient than existing constrained PG algorithms because they can utilize both on-policy and off-policy data. Furthermore, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms on several simulated (MuJoCo) tasks and a real-world indoor robot navigation problem, comparing them to state-of-the-art baselines. Our results demonstrate the effectiveness of our algorithms in balancing performance and constraint satisfaction. Videos of our experiments can be found at the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",1
"Machine learning can provide efficient solutions to the complex problems encountered in autonomous driving, but ensuring their safety remains a challenge. A number of authors have attempted to address this issue, but there are few publicly-available tools to adequately explore the trade-offs between functionality, scalability, and safety.   We thus present WiseMove, a software framework to investigate safe deep reinforcement learning in the context of motion planning for autonomous driving. WiseMove adopts a modular learning architecture that suits our current research questions and can be adapted to new technologies and new questions. We present the details of WiseMove, demonstrate its use on a common traffic scenario, and describe how we use it in our ongoing safe learning research.",0
"Although machine learning can offer efficient solutions for the complex issues that arise in autonomous driving, ensuring safety still poses a significant challenge. Many authors have tried to tackle this problem, but there is a scarcity of publicly available tools that can appropriately balance functionality, scalability, and safety. Thus, our team has developed WiseMove, a software framework that enables the examination of safe deep reinforcement learning in motion planning for autonomous driving. WiseMove features a modular learning architecture that is well-suited for our current research inquiries and can be adapted to new technologies and queries. In this article, we present the particulars of WiseMove, demonstrate its application to a typical traffic scenario, and explain how we employ it in our ongoing safe learning research.",1
"Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy gradient (td3), another off-policy deep RL algorithm which improves over ddpg. We evaluate the resulting method, cem-rl, on a set of benchmarks classically used in deep RL. We show that cem-rl benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.",0
"There are two popular approaches to policy search: deep neuroevolution and deep reinforcement learning (deep RL). Deep neuroevolution is widely applicable and stable, but it is not very sample efficient. On the other hand, deep RL is more sample efficient, but the most efficient variants are unstable and sensitive to hyper-parameter settings. Traditionally, these approaches have been compared as competing tools, but a new approach is emerging that combines them to get the best of both worlds. Two existing combinations use an ad hoc evolutionary algorithm or a goal exploration process with the Deep Deterministic Policy Gradient (DDPG) algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy gradient (TD3) algorithm, which improves over DDPG. We evaluate the resulting method, CEM-RL, on a set of benchmarks used in deep RL and show that it offers a satisfactory trade-off between performance and sample efficiency while providing several advantages over its competitors.",1
"Model-free reinforcement learning has recently been shown to successfully learn navigation policies from raw sensor data. In this work, we address the problem of learning driving policies for an autonomous agent in a high-fidelity simulator. Building upon recent research that applies deep reinforcement learning to navigation problems, we present a modular deep reinforcement learning approach to predict the steering angle of the car from raw images. The first module extracts a low-dimensional latent semantic representation of the image. The control module trained with reinforcement learning takes the latent vector as input to predict the correct steering angle. The experimental results have showed that our method is capable of learning to maneuver the car without any human control signals.",0
"Recently, it has been demonstrated that navigation policies can be successfully learned from raw sensor data using model-free reinforcement learning. Our study aims to tackle the challenge of teaching an autonomous agent how to drive in a high-fidelity simulator. We expand on previous work that applies deep reinforcement learning to navigation problems by presenting a modular approach that uses deep reinforcement learning to predict the steering angle of the car from raw images. The first module extracts a low-dimensional latent semantic representation of the image, while the control module, trained with reinforcement learning, uses the latent vector to accurately predict the steering angle. Our experimental results demonstrate that our method can effectively teach the car how to maneuver without any human control signals.",1
"Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com/gkahn13/GtS",0
"The use of deep reinforcement learning shows potential for controlling real-world robots through vision. However, the success of this technique relies heavily on the amount and diversity of data available for training. Obtaining such data can be challenging for certain types of robots, such as fragile and small-scale quadrotors. While simulated rendering and physics can provide larger datasets, the quality of such data is inferior due to poor modeling of complex physics and air currents, along with other discrepancies between simulation and reality. This study explores a hybrid deep reinforcement learning approach that combines both real-world and simulated data. Real-world data is used to learn about the dynamics of the system, while simulated data is used to develop a perception system that enables the robot to avoid collisions using a single camera. The effectiveness of this approach is demonstrated through a collision avoidance task for a nano aerial vehicle, where the quadrotor is able to avoid collisions in new environments with different lighting and geometry using only an hour of real-world data. The code, instructions for building the aerial vehicles, and experiment videos can be accessed at github.com/gkahn13/GtS.",1
"Deep Reinforcement Learning has been shown to be very successful in complex games, e.g. Atari or Go. These games have clearly defined rules, and hence allow simulation. In many practical applications, however, interactions with the environment are costly and a good simulator of the environment is not available. Further, as environments differ by application, the optimal inductive bias (architecture, hyperparameters, etc.) of a reinforcement agent depends on the application. In this work, we propose a multi-arm bandit framework that selects from a set of different reinforcement learning agents to choose the one with the best inductive bias. To alleviate the problem of sparse rewards, the reinforcement learning agents are augmented with surrogate rewards. This helps the bandit framework to select the best agents early, since these rewards are smoother and less sparse than the environment reward. The bandit has the double objective of maximizing the reward while the agents are learning and selecting the best agent after a finite number of learning steps. Our experimental results on standard environments show that the proposed framework is able to consistently select the optimal agent after a finite number of steps, while collecting more cumulative reward compared to selecting a sub-optimal architecture or uniformly alternating between different agents.",0
"While Deep Reinforcement Learning has proven successful in games with clear rules, like Atari and Go, it is not always feasible to simulate interactions with the environment in practical applications. Additionally, the optimal inductive bias for a reinforcement agent varies by application. To address this, we propose a multi-arm bandit framework that selects from a set of reinforcement learning agents with different biases. The agents are augmented with surrogate rewards to mitigate the problem of sparse rewards and facilitate early identification of the best agent. The bandit aims to maximize reward while selecting the best agent after a finite number of learning steps. Our experimental results demonstrate that this framework consistently selects the optimal agent while achieving greater cumulative reward than selecting a suboptimal architecture or alternating between agents uniformly.",1
"Despite many algorithmic advances, our theoretical understanding of practical distributional reinforcement learning methods remains limited. One exception is Rowland et al. (2018)'s analysis of the C51 algorithm in terms of the Cram\'er distance, but their results only apply to the tabular setting and ignore C51's use of a softmax to produce normalized distributions. In this paper we adapt the Cram\'er distance to deal with arbitrary vectors. From it we derive a new distributional algorithm which is fully Cram\'er-based and can be combined to linear function approximation, with formal guarantees in the context of policy evaluation. In allowing the model's prediction to be any real vector, we lose the probabilistic interpretation behind the method, but otherwise maintain the appealing properties of distributional approaches. To the best of our knowledge, ours is the first proof of convergence of a distributional algorithm combined with function approximation. Perhaps surprisingly, our results provide evidence that Cram\'er-based distributional methods may perform worse than directly approximating the value function.",0
"Our understanding of practical distributional reinforcement learning methods is limited despite algorithmic advancements. Rowland et al. (2018) analyzed the C51 algorithm using the Cram\'er distance, but their findings are limited to the tabular setting and disregard C51's utilization of a softmax for normalized distributions. This paper introduces a Cram\'er-based distributional algorithm that can be combined with linear function approximation for policy evaluation. The model's prediction can be any real vector, which results in the loss of the method's probabilistic interpretation. However, the desirable properties of distributional approaches are still maintained. Our proof of convergence for a distributional algorithm combined with function approximation is the first of its kind. Surprisingly, our results suggest that Cram\'er-based distributional methods may perform worse than directly approximating the value function.",1
"Reinforcement learning (RL) agents have traditionally been tasked with maximizing the value function of a Markov decision process (MDP), either in continuous settings, with fixed discount factor $\gamma < 1$, or in episodic settings, with $\gamma = 1$. While this has proven effective for specific tasks with well-defined objectives (e.g., games), it has never been established that fixed discounting is suitable for general purpose use (e.g., as a model of human preferences). This paper characterizes rationality in sequential decision making using a set of seven axioms and arrives at a form of discounting that generalizes traditional fixed discounting. In particular, our framework admits a state-action dependent ""discount"" factor that is not constrained to be less than 1, so long as there is eventual long run discounting. Although this broadens the range of possible preference structures in continuous settings, we show that there exists a unique ""optimizing MDP"" with fixed $\gamma < 1$ whose optimal value function matches the true utility of the optimal policy, and we quantify the difference between value and utility for suboptimal policies. Our work can be seen as providing a normative justification for (a slight generalization of) Martha White's RL task formalism (2017) and other recent departures from the traditional RL, and is relevant to task specification in RL, inverse RL and preference-based RL.",0
"In traditional reinforcement learning (RL), the objective has been to maximize the value function of a Markov decision process (MDP), either in continuous settings with a fixed discount factor of $\gamma < 1$, or in episodic settings where $\gamma = 1$. While this approach has been effective for specific tasks with defined goals, such as games, it has not been established that fixed discounting is suitable for general use, such as modeling human preferences. This paper presents a framework for rational decision making based on seven axioms, which leads to a generalized form of discounting that is not limited to a discount factor of less than 1. This approach allows for a broader range of preference structures in continuous settings. However, we show that there is a unique ""optimizing MDP"" with fixed $\gamma < 1$ that matches the true utility of the optimal policy, and we quantify the difference between value and utility for suboptimal policies. Our work provides a normative justification for recent departures from traditional RL, such as Martha White's RL task formalism (2017), and is relevant to task specification in RL, inverse RL, and preference-based RL.",1
"Multi-step methods such as Retrace($\lambda$) and $n$-step $Q$-learning have become a crucial component of modern deep reinforcement learning agents. These methods are often evaluated as a part of bigger architectures and their evaluations rarely include enough samples to draw statistically significant conclusions about their performance. This type of methodology makes it difficult to understand how particular algorithmic details of multi-step methods influence learning. In this paper we combine the $n$-step action-value algorithms Retrace, $Q$-learning, Tree Backup, Sarsa, and $Q(\sigma)$ with an architecture analogous to DQN. We test the performance of all these algorithms in the mountain car environment; this choice of environment allows for faster training times and larger sample sizes. We present statistical analyses on the effects of the off-policy correction, the backup length parameter $n$, and the update frequency of the target network on the performance of these algorithms. Our results show that (1) using off-policy correction can have an adverse effect on the performance of Sarsa and $Q(\sigma)$; (2) increasing the backup length $n$ consistently improved performance across all the different algorithms; and (3) the performance of Sarsa and $Q$-learning was more robust to the effect of the target network update frequency than the performance of Tree Backup, $Q(\sigma)$, and Retrace in this particular task.",0
"Modern deep reinforcement learning agents rely heavily on multi-step methods like Retrace($\lambda$) and $n$-step $Q$-learning. However, these methods are often evaluated as part of larger architectures, with limited sample sizes that make it difficult to draw significant conclusions about their performance. This approach hinders our understanding of how specific algorithmic details impact learning. To address this, we use an architecture similar to DQN and combine Retrace, $Q$-learning, Tree Backup, Sarsa, and $Q(\sigma)$ to test their performance in the mountain car environment, which allows for faster training times and larger sample sizes. We conduct statistical analyses on the effects of off-policy correction, backup length parameter $n$, and target network update frequency on algorithm performance. Our results indicate that using off-policy correction can negatively impact Sarsa and $Q(\sigma)$, increasing backup length consistently improved performance across all algorithms, and Sarsa and $Q$-learning were more robust to target network updates than Tree Backup, $Q(\sigma)$, and Retrace in this task.",1
"Multi-armed bandit(MAB) problem is a reinforcement learning framework where an agent tries to maximise her profit by proper selection of actions through absolute feedback for each action. The dueling bandits problem is a variation of MAB problem in which an agent chooses a pair of actions and receives relative feedback for the chosen action pair. The dueling bandits problem is well suited for modelling a setting in which it is not possible to provide quantitative feedback for each action, but qualitative feedback for each action is preferred as in the case of human feedback. The dueling bandits have been successfully applied in applications such as online rank elicitation, information retrieval, search engine improvement and clinical online recommendation. We propose a new method called Sup-KLUCB for K-armed dueling bandit problem specifically Copeland bandit problem by converting it into a standard MAB problem. Instead of using MAB algorithm independently for each action in a pair as in Sparring and in Self-Sparring algorithms, we combine a pair of action and use it as one action. Previous UCB algorithms such as Relative Upper Confidence Bound(RUCB) can be applied only in case of Condorcet dueling bandits, whereas this algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as a special case. Our empirical results outperform state of the art Double Thompson Sampling(DTS) in case of Copeland dueling bandits.",0
"The Multi-armed bandit (MAB) problem involves an agent using feedback to select actions that will maximize their profit. The dueling bandits problem is a variation of the MAB problem where an agent selects a pair of actions and receives relative feedback for that pair. This problem is useful for modeling situations where it is not possible to provide quantitative feedback, but qualitative feedback is preferred, such as in cases where humans provide feedback. Dueling bandits have been applied successfully in various applications, such as online rank elicitation, information retrieval, search engine improvement, and clinical online recommendation. Our proposed method, Sup-KLUCB, is designed specifically for the K-armed dueling bandit problem, specifically the Copeland bandit problem. It converts this problem into a standard MAB problem and combines a pair of actions into one action, unlike Sparring and Self-Sparring algorithms, which use MAB algorithms independently for each action in a pair. This algorithm is suitable for general Copeland dueling bandits, including Condorcet dueling bandits as a special case, unlike prior UCB algorithms like Relative Upper Confidence Bound (RUCB), which only apply to Condorcet dueling bandits. Our empirical results show that Sup-KLUCB outperforms the state-of-the-art Double Thompson Sampling (DTS) in Copeland dueling bandits.",1
"Training intelligent agents through reinforcement learning is a notoriously unstable procedure. Massive parallelization on GPUs and distributed systems has been exploited to generate a large amount of training experiences and consequently reduce instabilities, but the success of training remains strongly influenced by the choice of the hyperparameters. To overcome this issue, we introduce HyperTrick, a new metaoptimization algorithm, and show its effective application to tune hyperparameters in the case of deep reinforcement learning, while learning to play different Atari games on a distributed system. Our analysis provides evidence of the interaction between the identification of the optimal hyperparameters and the learned policy, that is typical of the case of metaoptimization for deep reinforcement learning. When compared with state-of-the-art metaoptimization algorithms, HyperTrick is characterized by a simpler implementation and it allows learning similar policies, while making a more effective use of the computational resources in a distributed system.",0
"The process of training intelligent agents using reinforcement learning is known for being unstable. To combat this, researchers have utilized massive parallelization on GPUs and distributed systems to generate a vast amount of training experiences and reduce instabilities. However, the success of training still heavily relies on the selection of hyperparameters. To address this issue, we developed HyperTrick, a new metaoptimization algorithm that effectively tunes hyperparameters for deep reinforcement learning. We applied HyperTrick to learn how to play various Atari games on a distributed system and found that it interacts with the identification of optimal hyperparameters and the learned policy. Our analysis shows that HyperTrick is simpler to implement than state-of-the-art metaoptimization algorithms, allowing for the learning of similar policies while making more effective use of computational resources.",1
"Machine learning approaches hold great potential for the automated detection of lung nodules in chest radiographs, but training the algorithms requires vary large amounts of manually annotated images, which are difficult to obtain. Weak labels indicating whether a radiograph is likely to contain pulmonary nodules are typically easier to obtain at scale by parsing historical free-text radiological reports associated to the radiographs. Using a repositotory of over 700,000 chest radiographs, in this study we demonstrate that promising nodule detection performance can be achieved using weak labels through convolutional neural networks for radiograph classification. We propose two network architectures for the classification of images likely to contain pulmonary nodules using both weak labels and manually-delineated bounding boxes, when these are available. Annotated nodules are used at training time to deliver a visual attention mechanism informing the model about its localisation performance. The first architecture extracts saliency maps from high-level convolutional layers and compares the estimated position of a nodule against the ground truth, when this is available. A corresponding localisation error is then back-propagated along with the softmax classification error. The second approach consists of a recurrent attention model that learns to observe a short sequence of smaller image portions through reinforcement learning. When a nodule annotation is available at training time, the reward function is modified accordingly so that exploring portions of the radiographs away from a nodule incurs a larger penalty. Our empirical results demonstrate the potential advantages of these architectures in comparison to competing methodologies.",0
"The detection of lung nodules in chest radiographs can be automated using machine learning. However, training the algorithms requires a large number of manually annotated images, which can be difficult to obtain. Weak labels that indicate whether a radiograph is likely to contain pulmonary nodules are easier to obtain by parsing historical radiological reports associated with the radiographs. This study uses over 700,000 chest radiographs to demonstrate that weak labels can be used with convolutional neural networks for radiograph classification. Two network architectures are proposed for classifying images likely to contain pulmonary nodules using both weak labels and manually-delineated bounding boxes. Annotated nodules are used at training time to provide a visual attention mechanism that informs the model about its localization performance. The first architecture extracts saliency maps and compares the estimated position of a nodule against the ground truth. The second approach is a recurrent attention model that learns to observe a short sequence of smaller image portions through reinforcement learning. Results show the potential advantages of these architectures in comparison to other methodologies.",1
"In the NeurIPS 2018 Artificial Intelligence for Prosthetics challenge, participants were tasked with building a controller for a musculoskeletal model with a goal of matching a given time-varying velocity vector. Top participants were invited to describe their algorithms. In this work, we describe the challenge and present thirteen solutions that used deep reinforcement learning approaches. Many solutions use similar relaxations and heuristics, such as reward shaping, frame skipping, discretization of the action space, symmetry, and policy blending. However, each team implemented different modifications of the known algorithms by, for example, dividing the task into subtasks, learning low-level control, or by incorporating expert knowledge and using imitation learning.",0
"The NeurIPS 2018 Artificial Intelligence for Prosthetics challenge required participants to construct a controller for a musculoskeletal model that matched a specific time-varying velocity vector. The top performers were offered the opportunity to detail their algorithms. This paper discusses the challenge and showcases thirteen solutions that utilized deep reinforcement learning methodologies. While many of the solutions implemented similar relaxations and heuristics (such as reward shaping, frame skipping, discretization of the action space, symmetry, and policy blending), each team made unique modifications to the known algorithms. These adaptations included dividing the task into subtasks, learning low-level control, incorporating expert knowledge, and utilizing imitation learning.",1
"In this paper, we present a new class of Markov decision processes (MDPs), called Tsallis MDPs, with Tsallis entropy maximization, which generalizes existing maximum entropy reinforcement learning (RL). A Tsallis MDP provides a unified framework for the original RL problem and RL with various types of entropy, including the well-known standard Shannon-Gibbs (SG) entropy, using an additional real-valued parameter, called an entropic index. By controlling the entropic index, we can generate various types of entropy, including the SG entropy, and a different entropy results in a different class of the optimal policy in Tsallis MDPs. We also provide a full mathematical analysis of Tsallis MDPs, including the optimality condition, performance error bounds, and convergence. Our theoretical result enables us to use any positive entropic index in RL. To handle complex and large-scale problems, we propose a model-free actor-critic RL method using Tsallis entropy maximization. We evaluate the regularization effect of the Tsallis entropy with various values of entropic indices and show that the entropic index controls the exploration tendency of the proposed method. For a different type of RL problems, we find that a different value of the entropic index is desirable. The proposed method is evaluated using the MuJoCo simulator and achieves the state-of-the-art performance.",0
"This paper introduces Tsallis MDPs, a new type of Markov decision processes that utilize Tsallis entropy maximization to generalize existing maximum entropy reinforcement learning. By introducing an entropic index, a Tsallis MDP can incorporate various types of entropy, including the standard Shannon-Gibbs entropy. The entropic index can be adjusted to generate different types of entropy, resulting in different optimal policies for the Tsallis MDP. The paper provides a mathematical analysis of Tsallis MDPs, including optimality conditions, performance error bounds, and convergence. To address complex and large-scale problems, the paper proposes a model-free actor-critic RL method that utilizes Tsallis entropy maximization. The method's exploration tendency can be controlled by adjusting the entropic index, which is shown to have a significant impact on the method's performance. The paper also evaluates the proposed method using the MuJoCo simulator, achieving state-of-the-art performance.",1
"This paper provides an analysis of the tradeoff between asymptotic bias (suboptimality with unlimited data) and overfitting (additional suboptimality due to limited data) in the context of reinforcement learning with partial observability. Our theoretical analysis formally characterizes that while potentially increasing the asymptotic bias, a smaller state representation decreases the risk of overfitting. This analysis relies on expressing the quality of a state representation by bounding L1 error terms of the associated belief states. Theoretical results are empirically illustrated when the state representation is a truncated history of observations, both on synthetic POMDPs and on a large-scale POMDP in the context of smartgrids, with real-world data. Finally, similarly to known results in the fully observable setting, we also briefly discuss and empirically illustrate how using function approximators and adapting the discount factor may enhance the tradeoff between asymptotic bias and overfitting in the partially observable context.",0
"The article examines the balance between asymptotic bias (imperfection with unlimited data) and overfitting (additional imperfection due to limited data) in reinforcement learning with partial observability. The study establishes that although a smaller state representation may increase asymptotic bias, it reduces the risk of overfitting. The quality of the state representation is evaluated by bounding L1 error terms of the associated belief states. Theoretical findings are demonstrated in synthetic POMDPs and a real-world smart grid POMDP with truncated observation histories. Additionally, the article briefly discusses and demonstrates how using function approximators and adjusting the discount factor may improve the tradeoff between asymptotic bias and overfitting in the partially observable setting, similar to fully observable environments.",1
"The transfer of knowledge from one policy to another is an important tool in Deep Reinforcement Learning. This process, referred to as distillation, has been used to great success, for example, by enhancing the optimisation of agents, leading to stronger performance faster, on harder domains [26, 32, 5, 8]. Despite the widespread use and conceptual simplicity of distillation, many different formulations are used in practice, and the subtle variations between them can often drastically change the performance and the resulting objective that is being optimised. In this work, we rigorously explore the entire landscape of policy distillation, comparing the motivations and strengths of each variant through theoretical and empirical analysis. Our results point to three distillation techniques, that are preferred depending on specifics of the task. Specifically a newly proposed expected entropy regularised distillation allows for quicker learning in a wide range of situations, while still guaranteeing convergence.",0
"Deep Reinforcement Learning relies heavily on the transfer of knowledge between policies, which is achieved through a process called distillation. This technique, which has been successfully used to improve agent optimization and performance on difficult domains, is widely used but its many formulations can have a significant impact on performance and objective optimization. This study thoroughly examines the landscape of policy distillation, comparing the strengths and motivations of each variant through both theoretical and empirical analysis. The findings suggest that there are three preferred distillation techniques, each suited to specific tasks. Among these, the newly proposed expected entropy regularised distillation offers faster learning across a broad range of scenarios while still ensuring convergence.",1
"Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms however have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This paper addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multi-agent deep RL (MADRL) is presented, including non-stationarity, partial observability, continuous state and action spaces, multi-agent training schemes, multi-agent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed, with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to future development of more robust and highly useful multi-agent learning methods for solving real-world problems.",0
"For decades, reinforcement learning (RL) algorithms have been utilized to resolve sequential decision-making problems. However, these algorithms have struggled when dealing with high-dimensional environments. The emergence of deep learning has allowed RL approaches to generate optimal policies for sophisticated agents that can effectively operate in challenging environments. This article focuses on an essential aspect of deep RL, which pertains to scenarios where multiple agents must communicate and coordinate to tackle complex tasks. The article provides a survey of diverse approaches to multi-agent deep RL (MADRL) issues, such as non-stationarity, partial observability, continuous state and action spaces, multi-agent training schemes, and multi-agent transfer learning. The benefits and drawbacks of these methods will be scrutinized and discussed, and their relevant applications explored. This review aims to shed light on various MADRL approaches and potentially inspire the development of more resilient and practical multi-agent learning techniques to address real-world problems.",1
"Despite the notable successes in video games such as Atari 2600, current AI is yet to defeat human champions in the domain of real-time strategy (RTS) games. One of the reasons is that an RTS game is a multi-agent game, in which single-agent reinforcement learning methods cannot simply be applied because the environment is not a stationary Markov Decision Process. In this paper, we present a first step toward finding a game-theoretic solution to RTS games by applying Neural Fictitious Self-Play (NFSP), a game-theoretic approach for finding Nash equilibria, to Mini-RTS, a small but nontrivial RTS game provided on the ELF platform. More specifically, we show that NFSP can be effectively combined with policy gradient reinforcement learning and be applied to Mini-RTS. Experimental results also show that the scalability of NFSP can be substantially improved by pretraining the models with simple self-play using policy gradients, which by itself gives a strong strategy despite its lack of theoretical guarantee of convergence.",0
"Human champions remain unbeaten by current AI in real-time strategy (RTS) games, despite the successes achieved in video games like Atari 2600. This is due to the fact that RTS games are multi-agent games, which makes it difficult to apply single-agent reinforcement learning methods as the environment is not a stationary Markov Decision Process. This paper presents the first step towards developing a game-theoretic solution to RTS games by using Neural Fictitious Self-Play (NFSP), a game-theoretic approach that finds Nash equilibria, on Mini-RTS, a challenging RTS game available on the ELF platform. The study demonstrates that NFSP can be effectively combined with policy gradient reinforcement learning and applied to Mini-RTS. Additionally, the experiment shows that the scalability of NFSP can be significantly enhanced by pretraining the models with simple self-play using policy gradients, which yields a powerful strategy despite the lack of theoretical guarantee of convergence.",1
"Backpropagation and the chain rule of derivatives have been prominent; however, the total derivative rule has not enjoyed the same amount of attention. In this work we show how the total derivative rule leads to an intuitive visual framework for creating gradient estimators on graphical models. In particular, previous ""policy gradient theorems"" are easily derived. We derive new gradient estimators based on density estimation, as well as a likelihood ratio gradient, which ""jumps"" to an intermediate node, not directly to the objective function. We evaluate our methods on model-based policy gradient algorithms, achieve good performance, and present evidence towards demystifying the success of the popular PILCO algorithm.",0
"While backpropagation and the chain rule of derivatives have received significant attention, the total derivative rule has been overlooked. Our research demonstrates how this rule can provide a clear and intuitive visual framework for developing gradient estimators on graphical models. Our work includes the derivation of previous ""policy gradient theorems"" and the development of new gradient estimators based on density estimation and a likelihood ratio gradient that jumps to an intermediate node rather than directly to the objective function. We evaluate our methods on model-based policy gradient algorithms and achieve strong performance, shedding light on the success of the widely used PILCO algorithm.",1
"Uplift modeling aims to directly model the incremental impact of a treatment on an individual response. In this work, we address the problem from a new angle and reformulate it as a Markov Decision Process (MDP). We conducted extensive experiments on both a synthetic dataset and real-world scenarios, and showed that our method can achieve significant improvement over previous methods.",0
"The objective of uplift modeling is to model the additional effect of a treatment on an individual's response. To tackle this issue, we propose a novel approach by transforming it into a Markov Decision Process (MDP). Our approach was tested on a synthetic dataset and real-life situations, and the results demonstrate its superiority over prior methods.",1
"Recent efforts on training visual navigation agents conditioned on language using deep reinforcement learning have been successful in learning policies for different multimodal tasks, such as semantic goal navigation and embodied question answering. In this paper, we propose a multitask model capable of jointly learning these multimodal tasks, and transferring knowledge of words and their grounding in visual objects across the tasks. The proposed model uses a novel Dual-Attention unit to disentangle the knowledge of words in the textual representations and visual concepts in the visual representations, and align them with each other. This disentangled task-invariant alignment of representations facilitates grounding and knowledge transfer across both tasks. We show that the proposed model outperforms a range of baselines on both tasks in simulated 3D environments. We also show that this disentanglement of representations makes our model modular, interpretable, and allows for transfer to instructions containing new words by leveraging object detectors.",0
"Efforts to train visual navigation agents with language using deep reinforcement learning have yielded successful results in learning policies for various multimodal tasks, such as semantic goal navigation and embodied question answering. In this study, we propose a multitask model that can jointly learn these tasks and transfer knowledge of words and their association with visual objects across them. The model employs a unique Dual-Attention unit to separate the knowledge of words in textual representations from visual concepts in visual representations and align them with one another. This disentangled and task-invariant alignment of representations facilitates grounding and knowledge transfer across both tasks. Our study demonstrates that the proposed model outperforms a range of baselines on both tasks in simulated 3D environments. Additionally, our disentanglement of representations makes our model modular, interpretable, and allows for transfer to instructions containing new words by utilizing object detectors.",1
"We introduce Dynamic Planning Networks (DPN), a novel architecture for deep reinforcement learning, that combines model-based and model-free aspects for online planning. Our architecture learns to dynamically construct plans using a learned state-transition model by selecting and traversing between simulated states and actions to maximize information before acting. In contrast to model-free methods, model-based planning lets the agent efficiently test action hypotheses without performing costly trial-and-error in the environment. DPN learns to efficiently form plans by expanding a single action-conditional state transition at a time instead of exhaustively evaluating each action, reducing the required number of state-transitions during planning by up to 96%. We observe various emergent planning patterns used to solve environments, including classical search methods such as breadth-first and depth-first search. DPN shows improved data efficiency, performance, and generalization to new and unseen domains in comparison to several baselines.",0
"We present a new deep reinforcement learning architecture, called Dynamic Planning Networks (DPN), which merges model-based and model-free features to enable online planning. Our architecture achieves this by learning to generate plans dynamically using a state-transition model and selecting and moving between simulated states and actions to optimize information before taking action. Unlike model-free techniques, model-based planning allows the agent to test action hypotheses efficiently without requiring extensive trial-and-error in the environment. DPN efficiently creates plans by progressively expanding a single action-conditional state transition instead of evaluating each action exhaustively, resulting in up to 96% reduction in the number of state-transitions required for planning. We observe that DPN employs different planning strategies to address various environments, including classical search methods such as breadth-first and depth-first search. Compared to several baselines, DPN exhibits enhanced data efficiency, performance, and generalization to new and unexplored domains.",1
"Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimization. Our experiments in model-based reinforcement learning imply that the problem is not just a numerical issue, but it may be caused by a fundamental chaos-like nature of long chains of nonlinear computations. Not only do the magnitudes of the gradients become large, the direction of the gradients becomes essentially random. We show that reparameterization gradients suffer from the problem, while likelihood ratio gradients are robust. Using our insights, we develop a model-based policy search framework, Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is easily extensible, and allows for almost arbitrary models and policies, while simultaneously matching the performance of previous data-efficient learning algorithms. Finally, we invent the total propagation algorithm, which efficiently computes a union over all pathwise derivative depths during a single backwards pass, automatically giving greater weight to estimators with lower variance, sometimes improving over reparameterization gradients by $10^6$ times.",0
"The exploding gradient problem has long been known to be a major issue in deep learning and model-based reinforcement learning, leading to numerical problems and instability in optimization. However, our experiments in model-based reinforcement learning suggest that this problem may be more than just a numerical issue and could be caused by the chaotic nature of long chains of nonlinear computations. In addition to the magnitudes of the gradients becoming large, their direction becomes essentially random. While reparameterization gradients are affected by this problem, likelihood ratio gradients are more reliable. Using these insights, we have developed a model-based policy search framework called Probabilistic Inference for Particle-Based Policy Search (PIPPS), which is highly flexible and can accommodate almost any model and policy while achieving the same performance as previous data-efficient learning algorithms. Finally, we have created a new algorithm called the total propagation algorithm, which can efficiently compute a union over all pathwise derivative depths during a single backwards pass, giving more weight to estimators with lower variance and sometimes outperforming reparameterization gradients by a factor of $10^6$.",1
"The effectiveness of model-based versus model-free methods is a long-standing question in reinforcement learning (RL). Motivated by recent empirical success of RL on continuous control tasks, we study the sample complexity of popular model-based and model-free algorithms on the Linear Quadratic Regulator (LQR). We show that for policy evaluation, a simple model-based plugin method requires asymptotically less samples than the classical least-squares temporal difference (LSTD) estimator to reach the same quality of solution; the sample complexity gap between the two methods can be at least a factor of state dimension. For policy evaluation, we study a simple family of problem instances and show that nominal (certainty equivalence principle) control also requires several factors of state and input dimension fewer samples than the policy gradient method to reach the same level of control performance on these instances. Furthermore, the gap persists even when employing commonly used baselines. To the best of our knowledge, this is the first theoretical result which demonstrates a separation in the sample complexity between model-based and model-free methods on a continuous control task.",0
"The question of whether model-based or model-free methods are more effective in reinforcement learning (RL) has been a topic of discussion for a long time. Our study is motivated by recent RL successes in continuous control tasks, and focuses on the sample complexity of popular model-based and model-free algorithms in the Linear Quadratic Regulator (LQR). We discovered that, for policy evaluation, a simple model-based plugin method requires significantly fewer samples than the classical least-squares temporal difference (LSTD) estimator to achieve the same quality of solution. The gap between the two methods can be at least a factor of state dimension. We also investigated a simple family of problem instances for policy evaluation and found that nominal (certainty equivalence principle) control needs several factors of state and input dimension fewer samples than the policy gradient method to achieve the same level of control performance on these instances. Additionally, the gap persists even when commonly used baselines are employed. This is the first theoretical result to demonstrate a difference in sample complexity between model-based and model-free methods in a continuous control task.",1
"StarCraft II poses a grand challenge for reinforcement learning. The main difficulties of it include huge state and action space and a long-time horizon. In this paper, we investigate a hierarchical reinforcement learning approach for StarCraft II. The hierarchy involves two levels of abstraction. One is the macro-action automatically extracted from expert's trajectories, which reduces the action space in an order of magnitude yet remains effective. The other is a two-layer hierarchical architecture which is modular and easy to scale, enabling a curriculum transferring from simpler tasks to more complex tasks. The reinforcement training algorithm for this architecture is also investigated. On a 64x64 map and using restrictive units, we achieve a winning rate of more than 99\% against the difficulty level-1 built-in AI. Through the curriculum transfer learning algorithm and a mixture of combat model, we can achieve over 93\% winning rate of Protoss against the most difficult non-cheating built-in AI (level-7) of Terran, training within two days using a single machine with only 48 CPU cores and 8 K40 GPUs. It also shows strong generalization performance, when tested against never seen opponents including cheating levels built-in AI and all levels of Zerg and Protoss built-in AI. We hope this study could shed some light on the future research of large-scale reinforcement learning.",0
"The task of mastering StarCraft II presents significant challenges for reinforcement learning due to its vast state and action space, as well as its long-term horizon. This research explores a hierarchical approach to reinforcement learning for StarCraft II, utilizing two levels of abstraction. The first level involves macro-actions extracted from expert trajectories, which effectively reduces the action space. The second level employs a modular, scalable, two-layer hierarchical architecture that facilitates curriculum transfer learning from simpler to more complex tasks. The study also investigates the reinforcement training algorithm for this architecture. Results show that on a 64x64 map with restrictive units, the approach achieved a winning rate of over 99% against difficulty level-1 built-in AI. By utilizing curriculum transfer learning and a mixture of combat models, the approach achieved a winning rate of over 93% against the most challenging non-cheating built-in AI (level-7) of Terran, training on a single machine with 48 CPU cores and 8 K40 GPUs within two days. The approach also demonstrated strong generalization performance when tested against never-seen-before opponents, including cheating levels built-in AI, and all levels of Zerg and Protoss built-in AI. The study aims to contribute to the future research of large-scale reinforcement learning.",1
"We introduce Implicit Policy, a general class of expressive policies that can flexibly represent complex action distributions in reinforcement learning, with efficient algorithms to compute entropy regularized policy gradients. We empirically show that, despite its simplicity in implementation, entropy regularization combined with a rich policy class can attain desirable properties displayed under maximum entropy reinforcement learning framework, such as robustness and multi-modality.",0
"Implicit Policy is a versatile type of policy that has the ability to represent intricate action distributions in reinforcement learning. It comes with effective algorithms for calculating policy gradients with entropy regularization. Our experiments demonstrate that, even though it is straightforward to implement, combining entropy regularization with a diverse policy class can achieve desirable characteristics commonly observed in maximum entropy reinforcement learning, such as resilience and multi-modality.",1
In this paper we consider the problem of how a reinforcement learning agent that is tasked with solving a sequence of reinforcement learning problems (a sequence of Markov decision processes) can use knowledge acquired early in its lifetime to improve its ability to solve new problems. We argue that previous experience with similar problems can provide an agent with information about how it should explore when facing a new but related problem. We show that the search for an optimal exploration strategy can be formulated as a reinforcement learning problem itself and demonstrate that such strategy can leverage patterns found in the structure of related problems. We conclude with experiments that show the benefits of optimizing an exploration strategy using our proposed approach.,0
"This paper delves into the issue of how a reinforcement learning agent can enhance its capacity to solve new problems by utilizing knowledge gained from previous experiences with a sequence of Markov decision processes. The authors assert that familiarity with analogous problems can furnish the agent with insights on how to explore when confronted with a new but related problem. They propose a reinforcement learning problem as a means of seeking an optimal exploration strategy and present evidence that such a strategy can take advantage of patterns discovered in the structure of analogous problems. Finally, the authors provide experimental results that illustrate the advantages of optimizing an exploration strategy based on their suggested method.",1
"In this work we explore a new approach for robots to teach themselves about the world simply by observing it. In particular we investigate the effectiveness of learning task-agnostic representations for continuous control tasks. We extend Time-Contrastive Networks (TCN) that learn from visual observations by embedding multiple frames jointly in the embedding space as opposed to a single frame. We show that by doing so, we are now able to encode both position and velocity attributes significantly more accurately. We test the usefulness of this self-supervised approach in a reinforcement learning setting. We show that the representations learned by agents observing themselves take random actions, or other agents perform tasks successfully, can enable the learning of continuous control policies using algorithms like Proximal Policy Optimization (PPO) using only the learned embeddings as input. We also demonstrate significant improvements on the real-world Pouring dataset with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. Video results are available at https://sites.google.com/view/actionablerepresentations .",0
"The focus of our research is exploring a novel method for robots to learn from their environment solely through observation. Specifically, we investigate the effectiveness of developing task-agnostic representations for continuous control tasks. Our research builds upon Time-Contrastive Networks (TCN), which utilize visual observations to learn. However, we extend TCN by embedding multiple frames together in the embedding space, rather than a single frame, resulting in more accurate encoding of both position and velocity attributes. We conduct experiments using a reinforcement learning setting and demonstrate that the self-supervised approach, where agents observe themselves or other agents performing tasks successfully, can facilitate the learning of continuous control policies using only the learned embeddings as input. Our results also show significant improvements on the real-world Pouring dataset, with relative error reductions of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. We have provided video results at https://sites.google.com/view/actionablerepresentations.",1
"This paper presents a novel Subject-dependent Deep Aging Path (SDAP), which inherits the merits of both Generative Probabilistic Modeling and Inverse Reinforcement Learning to model the facial structures and the longitudinal face aging process of a given subject. The proposed SDAP is optimized using tractable log-likelihood objective functions with Convolutional Neural Networks (CNNs) based deep feature extraction. Instead of applying a fixed aging development path for all input faces and subjects, SDAP is able to provide the most appropriate aging development path for individual subject that optimizes the reward aging formulation. Unlike previous methods that can take only one image as the input, SDAP further allows multiple images as inputs, i.e. all information of a subject at either the same or different ages, to produce the optimal aging path for the given subject. Finally, SDAP allows efficiently synthesizing in-the-wild aging faces. The proposed model is experimented in both tasks of face aging synthesis and cross-age face verification. The experimental results consistently show SDAP achieves the state-of-the-art performance on numerous face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). Furthermore, we also evaluate the performance of SDAP on large-scale Megaface challenge to demonstrate the advantages of the proposed solution.",0
"In this paper, a new method called Subject-dependent Deep Aging Path (SDAP) is introduced. The technique combines the benefits of Generative Probabilistic Modeling and Inverse Reinforcement Learning to model the facial structures and aging process of an individual. SDAP is optimized using Convolutional Neural Networks (CNNs) for deep feature extraction and tractable log-likelihood objective functions. Unlike previous approaches that use a fixed aging development path for all subjects, SDAP can provide a personalized aging development path for each individual, optimizing the reward aging formulation. Additionally, SDAP can take multiple images as inputs, allowing the use of all information of a subject at different ages to produce the optimal aging path. The proposed model can efficiently synthesize in-the-wild aging faces and has been evaluated in face aging synthesis and cross-age face verification tasks. The experimental results show that SDAP outperforms existing methods on various face aging databases, including FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). The proposed technique has also been evaluated on the large-scale Megaface challenge, demonstrating its advantages over other solutions.",1
"Interactive Machine Learning is concerned with creating systems that operate in environments alongside humans to achieve a task. A typical use is to extend or amplify the capabilities of a human in cognitive or physical ways, requiring the machine to adapt to the users' intentions and preferences. Often, this takes the form of a human operator providing some type of feedback to the user, which can be explicit feedback, implicit feedback, or a combination of both. Explicit feedback, such as through a mouse click, carries a high cognitive load. The focus of this study is to extend the current state of the art in interactive machine learning by demonstrating that agents can learn a human user's behavior and adapt to preferences with a reduced amount of explicit human feedback in a mixed feedback setting. The learning agent perceives a value of its own behavior from hand gestures given via a spatial interface. This feedback mechanism is termed Spatial Interface Valuing. This method is evaluated experimentally in a simulated environment for a grasping task using a robotic arm with variable grip settings. Preliminary results indicate that learning agents using spatial interface valuing can learn a value function mapping spatial gestures to expected future rewards much more quickly as compared to those same agents just receiving explicit feedback, demonstrating that an agent perceiving feedback from a human user via a spatial interface can serve as an effective complement to existing approaches.",0
"Interactive Machine Learning involves developing systems that work alongside humans to achieve goals in various environments. These systems can enhance human abilities in cognitive or physical ways, and they need to adapt to users' preferences and intentions. Human operators usually provide feedback to the user, which can be explicit, implicit, or a combination of both. Explicit feedback, such as a mouse click, can be mentally taxing. This study aims to advance the field of interactive machine learning by demonstrating that agents can learn a user's behavior and preferences with less explicit feedback. The learning agent perceives feedback through a Spatial Interface Valuing mechanism, which involves hand gestures. This method was tested in a simulated environment for a grasping task using a robotic arm with variable grip settings. The results showed that agents using Spatial Interface Valuing can learn much faster than those receiving only explicit feedback. This finding suggests that a spatial interface can complement existing approaches to interactive machine learning.",1
"Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions.",0
"Automakers are currently developing or producing automated driving systems (ADS) that provide freeway-pilot functions. These ADS are typically limited to restricted-access freeways and require manual completion of the ramp merging process before transitioning to automated modes. The challenge of extending automation to ramp merging lies in the need for the automated vehicle to optimize long-term objectives while safely executing near-term actions and interacting with other unpredictable vehicles. To address this complex control problem, Deep Reinforcement Learning (DRL) techniques are proposed to find an optimal driving policy that maximizes long-term rewards in an interactive environment. The proposed architecture uses a Long Short-Term Memory (LSTM) model to capture the historical impact of the environment on the reward and convey it to a Deep Q-Network (DQN) to generate Q-values for action selection. This DRL architecture can be applied to other autonomous driving scenarios, such as navigating complex intersections or changing lanes in varying traffic conditions.",1
"Due to the capability of deep learning to perform well in high dimensional problems, deep reinforcement learning agents perform well in challenging tasks such as Atari 2600 games. However, clearly explaining why a certain action is taken by the agent can be as important as the decision itself. Deep reinforcement learning models, as other deep learning models, tend to be opaque in their decision-making process. In this work, we propose to make deep reinforcement learning more transparent by visualizing the evidence on which the agent bases its decision. In this work, we emphasize the importance of producing a justification for an observed action, which could be applied to a black-box decision agent.",0
"Deep reinforcement learning agents excel in difficult tasks like Atari 2600 games due to the effectiveness of deep learning in high-dimensional problems. Nonetheless, explaining why an agent chooses a particular action is just as crucial as the decision itself. The decision-making process of deep reinforcement learning models, like other deep learning models, has a tendency to be opaque. We suggest a solution to enhance the transparency of deep reinforcement learning by presenting the evidence that forms the basis for an agent's decision. This paper highlights the significance of providing a rationale for an observed action, which could be employed for a black-box decision agent.",1
"We describe TF-Replicator, a framework for distributed machine learning designed for DeepMind researchers and implemented as an abstraction over TensorFlow. TF-Replicator simplifies writing data-parallel and model-parallel research code. The same models can be effortlessly deployed to different cluster architectures (i.e. one or many machines containing CPUs, GPUs or TPU accelerators) using synchronous or asynchronous training regimes. To demonstrate the generality and scalability of TF-Replicator, we implement and benchmark three very different models: (1) A ResNet-50 for ImageNet classification, (2) a SN-GAN for class-conditional ImageNet image generation, and (3) a D4PG reinforcement learning agent for continuous control. Our results show strong scalability performance without demanding any distributed systems expertise of the user. The TF-Replicator programming model will be open-sourced as part of TensorFlow 2.0 (see https://github.com/tensorflow/community/pull/25).",0
"TF-Replicator is a framework created for DeepMind researchers that enables distributed machine learning. It functions as an abstraction layer over TensorFlow and greatly simplifies the writing of data-parallel and model-parallel research code. The same models can be deployed to various cluster architectures with ease, including machines containing CPUs, GPUs, or TPU accelerators, using either synchronous or asynchronous training methods. To demonstrate its versatility and scalability, we tested TF-Replicator on three different models: a ResNet-50 for ImageNet classification, an SN-GAN for class-conditional ImageNet image generation, and a D4PG reinforcement learning agent for continuous control. Our results show that TF-Replicator performs well without requiring any expertise in distributed systems from the user. The programming model for TF-Replicator will be made available as part of TensorFlow 2.0, and can be found at https://github.com/tensorflow/community/pull/25.",1
"In dynamic environments, learned controllers are supposed to take motion into account when selecting the action to be taken. However, in existing reinforcement learning works motion is rarely treated explicitly; it is rather assumed that the controller learns the necessary motion representation from temporal stacks of frames implicitly. In this paper, we show that for continuous control tasks learning an explicit representation of motion improves the quality of the learned controller in dynamic scenarios. We demonstrate this on common benchmark tasks (Walker, Swimmer, Hopper), on target reaching and ball catching tasks with simulated robotic arms, and on a dynamic single ball juggling task. Moreover, we find that when equipped with an appropriate network architecture, the agent can, on some tasks, learn motion features also with pure reinforcement learning, without additional supervision. Further we find that using an image difference between the current and the previous frame as an additional input leads to better results than a temporal stack of frames.",0
"When confronted with ever-changing circumstances, controllers that have been learned are expected to factor in motion when deciding on the course of action to take. Nonetheless, in current reinforcement learning research, motion is seldom handled directly; instead, it is assumed that the controller will acquire the necessary motion representation implicitly from a series of frames over time. This paper aims to prove that explicitly learning a motion representation for continuous control tasks can enhance the quality of the learned controller in dynamic scenarios. We demonstrate this by conducting experiments on various benchmark tasks, such as Walker, Swimmer, and Hopper, as well as target reaching and ball catching tasks with simulated robotic arms, and a dynamic single ball juggling task. In addition, we discovered that, on some tasks, the agent can learn motion features through pure reinforcement learning if it is equipped with an appropriate network architecture, without requiring additional supervision. We also discovered that using an image difference between the current and previous frames as an extra input yields better outcomes than a temporal stack of frames.",1
"Many reinforcement learning applications involve the use of data that is sensitive, such as medical records of patients or financial information. However, most current reinforcement learning methods can leak information contained within the (possibly sensitive) data on which they are trained. To address this problem, we present the first differentially private approach for off-policy evaluation. We provide a theoretical analysis of the privacy-preserving properties of our algorithm and analyze its utility (speed of convergence). After describing some results of this theoretical analysis, we show empirically that our method outperforms previous methods (which are restricted to the on-policy setting).",0
"Numerous reinforcement learning applications involve utilizing sensitive data like medical records or financial information. However, most of the current reinforcement learning techniques can reveal the information present in the data on which they are trained, including sensitive data. To overcome this obstacle, we introduce the initial differentially private approach for off-policy evaluation. We offer a theoretical examination of our algorithm's privacy-retaining capabilities and its efficiency (convergence rate). After presenting some findings from our theoretical examination, we demonstrate through experiments that our approach outperforms prior methods (which are limited to the on-policy environment).",1
"Black-box optimizers that explore in parameter space have often been shown to outperform more sophisticated action space exploration methods developed specifically for the reinforcement learning problem. We examine these black-box methods closely to identify situations in which they are worse than action space exploration methods and those in which they are superior. Through simple theoretical analyses, we prove that complexity of exploration in parameter space depends on the dimensionality of parameter space, while complexity of exploration in action space depends on both the dimensionality of action space and horizon length. This is also demonstrated empirically by comparing simple exploration methods on several model problems, including Contextual Bandit, Linear Regression and Reinforcement Learning in continuous control.",0
"The effectiveness of black-box optimizers that explore parameter space has been found to surpass that of more intricate action space exploration techniques created specifically for the reinforcement learning problem. Our study delves into these black-box methods to determine when they are inferior or superior to action space exploration methods. By conducting straightforward theoretical analyses, we establish that the complexity of exploration in parameter space is dependent on the dimensionality of parameter space, while the complexity of exploration in action space is determined by both the dimensionality of action space and horizon length. This is further confirmed through empirical testing of basic exploration techniques on various model problems, such as Contextual Bandit, Linear Regression, and Reinforcement Learning in continuous control.",1
"The ability to transfer skills across tasks has the potential to scale up reinforcement learning (RL) agents to environments currently out of reach. Recently, a framework based on two ideas, successor features (SFs) and generalised policy improvement (GPI), has been introduced as a principled way of transferring skills. In this paper we extend the SFs & GPI framework in two ways. One of the basic assumptions underlying the original formulation of SFs & GPI is that rewards for all tasks of interest can be computed as linear combinations of a fixed set of features. We relax this constraint and show that the theoretical guarantees supporting the framework can be extended to any set of tasks that only differ in the reward function. Our second contribution is to show that one can use the reward functions themselves as features for future tasks, without any loss of expressiveness, thus removing the need to specify a set of features beforehand. This makes it possible to combine SFs & GPI with deep learning in a more stable way. We empirically verify this claim on a complex 3D environment where observations are images from a first-person perspective. We show that the transfer promoted by SFs & GPI leads to very good policies on unseen tasks almost instantaneously. We also describe how to learn policies specialised to the new tasks in a way that allows them to be added to the agent's set of skills, and thus be reused in the future.",0
"The potential for scaling up reinforcement learning (RL) agents to currently unattainable environments lies in their ability to transfer skills across tasks. A framework based on successor features (SFs) and generalised policy improvement (GPI) has recently been introduced as a principled approach to skill transfer. This paper presents two extensions to the SFs & GPI framework. Firstly, we relax the assumption that rewards for all tasks of interest can be computed as linear combinations of a fixed set of features, demonstrating that the theoretical guarantees supporting the framework can be extended to any set of tasks that differ only in their reward functions. Secondly, we show that reward functions can themselves be used as features for future tasks, eliminating the need to specify a set of features beforehand and enabling a more stable combination with deep learning. Empirical validation is provided on a complex 3D environment, where SFs & GPI transfer leads to strong policies on unseen tasks almost instantly. Additionally, we describe a method for learning specialized policies for new tasks that can be added to the agent's skill set for future reuse.",1
"As both light transport simulation and reinforcement learning are ruled by the same Fredholm integral equation of the second kind, reinforcement learning techniques may be used for photorealistic image synthesis: Efficiency may be dramatically improved by guiding light transport paths by an approximate solution of the integral equation that is learned during rendering. In the light of the recent advances in reinforcement learning for playing games, we investigate the representation of an approximate solution of an integral equation by artificial neural networks and derive a loss function for that purpose. The resulting Monte Carlo and quasi-Monte Carlo methods train neural networks with standard information instead of linear information and naturally are able to generate an arbitrary number of training samples. The methods are demonstrated for applications in light transport simulation.",0
"Reinforcement learning techniques can be utilized for photorealistic image synthesis by guiding light transport paths through an approximate solution of the Fredholm integral equation of the second kind, which is common to both fields. This approach can significantly enhance efficiency during rendering. Given the recent progress in reinforcement learning for game playing, the research aims to explore the use of artificial neural networks to represent an approximate solution of the integral equation and develop a corresponding loss function. Monte Carlo and quasi-Monte Carlo methods are employed to train the neural networks, which generate an unlimited number of training samples using standard information. These methods are applicable to light transport simulation and are demonstrated through various examples.",1
"In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.",0
"Deep reinforcement learning (RL) has made significant strides in solving complex problems across various domains in recent years. However, reproducing existing work and accurately assessing improvements offered by new methods are crucial for sustaining this progress. Unfortunately, reproducing state-of-the-art deep RL results is often challenging due to non-determinism in standard benchmark environments and intrinsic variance in the methods. This makes reported results difficult to interpret without significance metrics and tighter standardization of experimental reporting. This paper explores the challenges of reproducibility, proper experimental techniques, and reporting procedures. It highlights the variability in reported metrics and results when compared to common baselines and proposes guidelines to improve reproducibility in future deep RL results. The goal is to stimulate discussion on how to minimize wasted effort caused by non-reproducible and easily misinterpreted results, and ensure continued progress in the field.",1
"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.",0
"Traditionally, behavioral skills or policies for autonomous agents are acquired through reinforcement learning using reward functions, or through imitation learning using demonstrations. However, both methods have their limitations: manual engineering is required for reward functions, and human expertise is needed for generating demonstrations. Consequently, using natural language instructions to train machines has become an attractive alternative. Nonetheless, a single instruction may not be enough to convey the full task or may not be sufficient for the autonomous agent to understand how to carry out the task. Therefore, we propose an interactive approach to task specification, where an autonomous agent receives iterative language corrections to acquire the desired skill efficiently. Our language-guided policy learning algorithm integrates instructions with a sequence of corrections to quickly acquire new skills. Our experiments demonstrate that this method outperforms traditional non-interactive instruction following for simulated navigation and manipulation tasks.",1
"Despite remarkable successes, Deep Reinforcement Learning (DRL) is not robust to hyperparameterization, implementation details, or small environment changes (Henderson et al. 2017, Zhang et al. 2018). Overcoming such sensitivity is key to making DRL applicable to real world problems. In this paper, we identify sensitivity to time discretization in near continuous-time environments as a critical factor; this covers, e.g., changing the number of frames per second, or the action frequency of the controller. Empirically, we find that Q-learning-based approaches such as Deep Q- learning (Mnih et al., 2015) and Deep Deterministic Policy Gradient (Lillicrap et al., 2015) collapse with small time steps. Formally, we prove that Q-learning does not exist in continuous time. We detail a principled way to build an off-policy RL algorithm that yields similar performances over a wide range of time discretizations, and confirm this robustness empirically.",0
"Deep Reinforcement Learning (DRL) has achieved impressive results, but its sensitivity to hyperparameterization, implementation details, and minor environmental changes has hindered its applicability to real-world problems (Henderson et al., 2017; Zhang et al., 2018). To address this issue, we focus on the sensitivity to time discretization in near continuous-time environments, which involves altering the number of frames per second or the controller's action frequency. Our empirical findings reveal that Q-learning-based approaches, such as Deep Q-learning (Mnih et al., 2015) and Deep Deterministic Policy Gradient (Lillicrap et al., 2015), become unstable with small time steps. Moreover, we provide a formal proof that Q-learning is not viable in continuous time. To overcome this challenge, we propose an off-policy RL algorithm that maintains consistent performance across various time discretizations, and we verify its robustness through experimentation.",1
"Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.",0
"Successful application of model-free deep reinforcement learning (RL) algorithms has been observed in challenging sequential decision making and control tasks. However, high sample complexity and hyperparameter brittleness pose significant challenges that restrict the application of these methods in real-world domains. Recently, we have introduced Soft Actor-Critic (SAC), an off-policy actor-critic algorithm based on the maximum entropy RL framework. SAC aims to maximize both the expected return and entropy of the actor, allowing it to act as randomly as possible while succeeding in the task. We have made modifications to SAC to improve training acceleration and hyperparameter stability, including a constrained formulation that automatically tunes the temperature hyperparameter. We have evaluated SAC on various benchmark tasks and real-world challenges such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC has achieved state-of-the-art performance, surpassing prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Moreover, our approach is remarkably stable, achieving similar performance across different random seeds, unlike other off-policy algorithms. These results indicate that SAC is a promising algorithm for learning in real-world robotics tasks.",1
"Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are ""actionable."" These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, without explicit reconstruction of the observation. We show how these representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning.",0
"Across various areas of machine learning, representation learning remains a significant challenge. In reinforcement learning, the development of effective and functional representations has the potential to accelerate learning progress significantly and solve more complex problems. While past research on representation learning has primarily focused on generative approaches that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner, our paper aims to learn functionally salient representations instead. These representations do not necessarily capture all factors of variation in the observation space but rather focus on those elements that are critical for decision making. They are aware of the environment's dynamics and capture only the necessary factors of variation for decision making, without explicit observation reconstruction. We demonstrate how such representations can be useful in improving exploration for sparse reward problems, enabling long-horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. Our method is evaluated on various simulated environments and compared to prior methods for representation learning, exploration, and hierarchical reinforcement learning.",1
"In many environments only a tiny subset of all states yield high reward. In these cases, few of the interactions with the environment provide a relevant learning signal. Hence, we may want to preferentially train on those high-reward states and the probable trajectories leading to them. To this end, we advocate for the use of a backtracking model that predicts the preceding states that terminate at a given high-reward state. We can train a model which, starting from a high value state (or one that is estimated to have high value), predicts and sample for which the (state, action)-tuples may have led to that high value state. These traces of (state, action) pairs, which we refer to as Recall Traces, sampled from this backtracking model starting from a high value state, are informative as they terminate in good states, and hence we can use these traces to improve a policy. We provide a variational interpretation for this idea and a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories which lead to large rewards. Our method improves the sample efficiency of both on- and off-policy RL algorithms across several environments and tasks.",0
"In certain situations, only a small number of states result in high rewards, making it difficult to learn from interactions with the environment. Therefore, it may be beneficial to focus on training on those high-reward states and the likely paths that lead to them. To accomplish this, we suggest using a backtracking model that predicts the preceding states that lead to a given high-reward state. By training a model to predict and sample (state, action) pairs that may have led to a high-value state, we can generate Recall Traces that terminate in good states, which can be used to improve a policy. We offer a variational interpretation of this concept and present a practical algorithm in which the backtracking model samples from an approximate posterior distribution over trajectories that lead to large rewards. Our approach enhances the sample efficiency of both on- and off-policy RL algorithms in various environments and tasks.",1
"Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.",0
"Complex predictive models can be learned by humans and animals, enabling them to reason about real-world phenomena with accuracy and reliability. However, deep neural network models lack the ability to adapt quickly in response to unexpected changes. This paper aims to develop a method for continual online learning using deep neural network models to address this limitation. The proposed approach involves an online learning procedure that utilizes stochastic gradient descent to update model parameters and an expectation maximization algorithm with a Chinese restaurant process prior to maintain a mixture of models for non-stationary task distributions. Meta-learning is used to meta-train a model to enable effective direct online adaptation with SGD, which is otherwise not possible for large function approximators. The meta-learning for online learning (MOLe) approach is applied to model-based reinforcement learning, where adapting the predictive model is crucial for control. The results demonstrate that MOLe outperforms alternative methods and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.",1
"Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20\% on average, going as high as 36\% for some images, while maintaining the same 76.4\% top-1 accuracy on ImageNet.",0
"Although very deep convolutional neural networks have the potential for excellent recognition results, their computational cost often limits their practical use. However, we have developed an approach called BlockDrop that addresses this issue by dynamically choosing which layers of a deep network to execute during inference. By exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework can select which residual blocks to evaluate for a given image. We achieve this by training a policy network using associative reinforcement learning, which rewards the use of a minimal number of blocks while maintaining recognition accuracy. Our experiments on CIFAR and ImageNet show that these learned policies not only improve inference speed but also encode meaningful visual information. With a ResNet-101 model, our method achieves an average speedup of 20%, reaching up to 36% for some images, while maintaining a top-1 accuracy of 76.4% on ImageNet.",1
"There are two halves to RL systems: experience collection time and policy learning time. For a large number of samples in rollouts, experience collection time is the major bottleneck. Thus, it is necessary to speed up the rollout generation time with multi-process architecture support. Our work, dubbed WALL-E, utilizes multiple rollout samplers running in parallel to rapidly generate experience. Due to our parallel samplers, we experience not only faster convergence times, but also higher average reward thresholds. For example, on the MuJoCo HalfCheetah-v2 task, with $N = 10$ parallel sampler processes, we are able to achieve much higher average return than those from using only a single process architecture.",0
"RL systems consist of two parts, namely experience collection time and policy learning time. The experience collection time is the primary bottleneck when dealing with a vast number of samples in rollouts. Therefore, to accelerate the rollout generation time, it is essential to implement multi-process architecture support. Our project, WALL-E, employs multiple parallel rollout samplers to generate experience rapidly. With our parallel samplers, we achieve faster convergence times and higher average reward thresholds. For instance, on the MuJoCo HalfCheetah-v2 task, using $N = 10$ parallel sampler processes, we obtain significantly higher average return than with a single process architecture.",1
"In this paper we revisit the method of off-policy corrections for reinforcement learning (COP-TD) pioneered by Hallak et al. (2017). Under this method, online updates to the value function are reweighted to avoid divergence issues typical of off-policy learning. While Hallak et al.'s solution is appealing, it cannot easily be transferred to nonlinear function approximation. First, it requires a projection step onto the probability simplex; second, even though the operator describing the expected behavior of the off-policy learning algorithm is convergent, it is not known to be a contraction mapping, and hence, may be more unstable in practice. We address these two issues by introducing a discount factor into COP-TD. We analyze the behavior of discounted COP-TD and find it better behaved from a theoretical perspective. We also propose an alternative soft normalization penalty that can be minimized online and obviates the need for an explicit projection step. We complement our analysis with an empirical evaluation of the two techniques in an off-policy setting on the game Pong from the Atari domain where we find discounted COP-TD to be better behaved in practice than the soft normalization penalty. Finally, we perform a more extensive evaluation of discounted COP-TD in 5 games of the Atari domain, where we find performance gains for our approach.",0
"This paper reexamines the COP-TD off-policy correction method for reinforcement learning, which was first introduced by Hallak et al. (2017). The method involves reweighting online updates to the value function to counteract the common divergence issues that arise in off-policy learning. While Hallak et al.'s approach is effective, it cannot be easily applied to nonlinear function approximation due to the need for a projection step and possible instability of the expected behavior operator. To overcome these issues, we propose a discounted version of COP-TD, which we demonstrate to be theoretically better behaved. Additionally, we introduce a soft normalization penalty that eliminates the need for an explicit projection step. We evaluate the two techniques in an off-policy setting on the Atari game Pong, finding that discounted COP-TD performs better than the soft normalization penalty. We further evaluate our approach in five other Atari games, observing performance gains.",1
"Reward shaping is one of the most effective methods to tackle the crucial yet challenging problem of credit assignment in Reinforcement Learning (RL). However, designing shaping functions usually requires much expert knowledge and hand-engineering, and the difficulties are further exacerbated given multiple similar tasks to solve. In this paper, we consider reward shaping on a distribution of tasks, and propose a general meta-learning framework to automatically learn the efficient reward shaping on newly sampled tasks, assuming only shared state space but not necessarily action space. We first derive the theoretically optimal reward shaping in terms of credit assignment in model-free RL. We then propose a value-based meta-learning algorithm to extract an effective prior over the optimal reward shaping. The prior can be applied directly to new tasks, or provably adapted to the task-posterior while solving the task within few gradient updates. We demonstrate the effectiveness of our shaping through significantly improved learning efficiency and interpretable visualizations across various settings, including notably a successful transfer from DQN to DDPG.",0
"Reward shaping is a useful method for addressing the challenge of allocating credit in Reinforcement Learning (RL). However, creating shaping functions typically requires considerable expertise and manual engineering, which becomes even more difficult when dealing with multiple similar tasks. This paper investigates reward shaping on a range of tasks and proposes a meta-learning framework that automatically learns efficient reward shaping for newly sampled tasks, presuming that the state space is shared but not necessarily the action space. The authors derive the theoretically optimal reward shaping for model-free RL and introduce a value-based meta-learning algorithm that extracts an effective prior over the optimal reward shaping. This prior can be applied directly to new tasks or adapted to the task-posterior while solving the task with only a few gradient updates. The authors demonstrate the effectiveness of this shaping through improved learning efficiency and interpretable visualizations across various settings, including a successful transfer from DQN to DDPG.",1
"Complex environments and tasks pose a difficult problem for holistic end-to-end learning approaches. Decomposition of an environment into interacting controllable and non-controllable objects allows supervised learning for non-controllable objects and universal value function approximator learning for controllable objects. Such decomposition should lead to a shorter learning time and better generalisation capability. Here, we consider arcade-game environments as sets of interacting objects (controllable, non-controllable) and propose a set of functional modules that are specialized on mastering different types of interactions in a broad range of environments. The modules utilize regression, supervised learning, and reinforcement learning algorithms. Results of this case study in different Atari games suggest that human-level performance can be achieved by a learning agent within a human amount of game experience (10-15 minutes game time) when a proper decomposition of an environment or a task is provided. However, automatization of such decomposition remains a challenging problem. This case study shows how a model of a causal structure underlying an environment or a task can benefit learning time and generalization capability of the agent, and argues in favor of exploiting modular structure in contrast to using pure end-to-end learning approaches.",0
"When dealing with complex environments and tasks, holistic end-to-end learning approaches can be challenging. To address this issue, breaking down the environment into controllable and non-controllable objects can allow for supervised learning for non-controllable objects and universal value function approximator learning for controllable objects. This approach can result in a shorter learning time and better generalisation capability. In this study, arcade-game environments were examined, and a set of functional modules was proposed that specializes in mastering various interactions in different environments. These modules use regression, supervised learning, and reinforcement learning algorithms. The results of this study indicate that a learning agent can achieve human-level performance within a short amount of game experience (10-15 minutes game time) when a suitable environment or task decomposition is provided. However, automating this process remains a challenging task. This study highlights the benefits of using a model of the causal structure of an environment or task to improve learning time and generalization capability, and suggests the use of modular structures instead of pure end-to-end learning approaches.",1
"Dynamic portfolio optimization is the process of sequentially allocating wealth to a collection of assets in some consecutive trading periods, based on investors' return-risk profile. Automating this process with machine learning remains a challenging problem. Here, we design a deep reinforcement learning (RL) architecture with an autonomous trading agent such that, investment decisions and actions are made periodically, based on a global objective, with autonomy. In particular, without relying on a purely model-free RL agent, we train our trading agent using a novel RL architecture consisting of an infused prediction module (IPM), a generative adversarial data augmentation module (DAM) and a behavior cloning module (BCM). Our model-based approach works with both on-policy or off-policy RL algorithms. We further design the back-testing and execution engine which interact with the RL agent in real time. Using historical {\em real} financial market data, we simulate trading with practical constraints, and demonstrate that our proposed model is robust, profitable and risk-sensitive, as compared to baseline trading strategies and model-free RL agents from prior work.",0
"The concept of dynamic portfolio optimization involves distributing wealth among various assets over multiple trading periods, according to an investor's return-risk preferences. However, automating this process through machine learning is a difficult task. In this study, we introduce a deep reinforcement learning (RL) framework that utilizes an autonomous trading agent to make investment decisions based on a global objective and with autonomy. To train our trading agent, we developed a unique RL architecture that incorporates an infused prediction module (IPM), a generative adversarial data augmentation module (DAM), and a behavior cloning module (BCM), without relying solely on a model-free RL agent. Our model-based approach is compatible with both on-policy and off-policy RL algorithms. We also designed a real-time back-testing and execution engine to interact with the RL agent. By testing our approach using actual financial market data, we demonstrated that our model is robust, profitable, and risk-sensitive compared to previous model-free RL agents and baseline trading strategies.",1
"Discrete-action algorithms have been central to numerous recent successes of deep reinforcement learning. However, applying these algorithms to high-dimensional action tasks requires tackling the combinatorial increase of the number of possible actions with the number of action dimensions. This problem is further exacerbated for continuous-action tasks that require fine control of actions via discretization. In this paper, we propose a novel neural architecture featuring a shared decision module followed by several network branches, one for each action dimension. This approach achieves a linear increase of the number of network outputs with the number of degrees of freedom by allowing a level of independence for each individual action dimension. To illustrate the approach, we present a novel agent, called Branching Dueling Q-Network (BDQ), as a branching variant of the Dueling Double Deep Q-Network (Dueling DDQN). We evaluate the performance of our agent on a set of challenging continuous control tasks. The empirical results show that the proposed agent scales gracefully to environments with increasing action dimensionality and indicate the significance of the shared decision module in coordination of the distributed action branches. Furthermore, we show that the proposed agent performs competitively against a state-of-the-art continuous control algorithm, Deep Deterministic Policy Gradient (DDPG).",0
"Recent achievements in deep reinforcement learning have relied heavily on discrete-action algorithms. However, when applied to high-dimensional action tasks, the challenge of dealing with the combinatorial increase of possible actions with the number of action dimensions becomes evident. This issue is further aggravated in continuous-action tasks that require fine control of actions through discretization. To address this, we introduce a novel neural architecture that incorporates a shared decision module and multiple network branches, one for each action dimension. This approach enables a level of independence for each individual action dimension, resulting in a linear increase of network outputs with degrees of freedom. We present a new agent, called Branching Dueling Q-Network (BDQ), which is a branching variant of the Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the efficacy of our approach. Our agent performs well on challenging continuous control tasks, exhibiting graceful scaling in environments with increasing action dimensionality. Additionally, we demonstrate the importance of the shared decision module in coordinating the distributed action branches. Finally, our results show that our proposed agent performs competitively with a state-of-the-art continuous control algorithm, Deep Deterministic Policy Gradient (DDPG).",1
"Deep Reinforcement Learning (DeepRL) agents surpass human-level performances in a multitude of tasks. However, the direct mapping from states to actions makes it hard to interpret the rationale behind the decision making of agents. In contrast to previous a-posteriori methods of visualizing DeepRL policies, we propose an end-to-end trainable framework based on Rainbow, a representative Deep Q-Network (DQN) agent. Our method automatically learns important regions in the input domain, which enables characterizations of the decision making and interpretations for non-intuitive behaviors. Hence we name it Region Sensitive Rainbow (RS-Rainbow). RS-Rainbow utilizes a simple yet effective mechanism to incorporate visualization ability into the learning model, not only improving model interpretability, but leading to improved performance. Extensive experiments on the challenging platform of Atari 2600 demonstrate the superiority of RS-Rainbow. In particular, our agent achieves state of the art at just 25% of the training frames. Demonstrations and code are available at https://github.com/yz93/Learn-to-Interpret-Atari-Agents.",0
"A multitude of tasks have been surpassed by Deep Reinforcement Learning (DeepRL) agents, even surpassing human-level performance. However, the direct mapping from states to actions makes it difficult to understand the decision making process of agents. Our proposed Region Sensitive Rainbow (RS-Rainbow) framework is an end-to-end trainable mechanism based on Rainbow, a representative Deep Q-Network (DQN) agent. Our method automatically learns important regions in the input domain, enabling characterizations of the decision making process and interpretations for non-intuitive behaviors. This not only improves model interpretability but also leads to improved performance. RS-Rainbow incorporates a simple yet effective mechanism to incorporate visualization ability into the learning model. Extensive experiments on the challenging platform of Atari 2600 show the superiority of RS-Rainbow. Our agent achieves state of the art at just 25% of the training frames. Demonstrations and code are available at https://github.com/yz93/Learn-to-Interpret-Atari-Agents.",1
"Deep neural networks are data hungry models and thus face difficulties when attempting to train on small text datasets. Transfer learning is a potential solution but their effectiveness in the text domain is not as explored as in areas such as image analysis. In this paper, we study the problem of transfer learning for text summarization and discuss why existing state-of-the-art models fail to generalize well on other (unseen) datasets. We propose a reinforcement learning framework based on a self-critic policy gradient approach which achieves good generalization and state-of-the-art results on a variety of datasets. Through an extensive set of experiments, we also show the ability of our proposed framework to fine-tune the text summarization model using only a few training samples. To the best of our knowledge, this is the first work that studies transfer learning in text summarization and provides a generic solution that works well on unseen data.",0
"The challenge of training deep neural networks on small text datasets is well-known due to their high data requirements. While transfer learning has been used successfully in areas such as image analysis, its effectiveness in the text domain remains relatively unexplored. This paper explores the issue of transfer learning for text summarization and examines why current state-of-the-art models fail to generalize when applied to new datasets. To address this problem, we propose a reinforcement learning framework that employs a self-critic policy gradient approach, which achieves state-of-the-art results on various datasets. Our experiments demonstrate the framework's ability to fine-tune text summarization models using minimal training samples. This is the first study to investigate transfer learning in text summarization and provide a universal solution that performs well on unseen data.",1
"It has recently been shown that if feedback effects of decisions are ignored, then imposing fairness constraints such as demographic parity or equality of opportunity can actually exacerbate unfairness. We propose to address this challenge by modeling feedback effects as the dynamics of a Markov decision processes (MDPs). First, we define analogs of fairness properties that have been proposed for supervised learning. Second, we propose algorithms for learning fair decision-making policies for MDPs. We also explore extensions to reinforcement learning, where parts of the dynamical system are unknown and must be learned without violating fairness. Finally, we demonstrate the need to account for dynamical effects using simulations on a loan applicant MDP.",0
"Ignoring feedback effects in decision-making can lead to increased unfairness when fairness constraints like demographic parity or equality of opportunity are imposed. To overcome this challenge, we suggest incorporating feedback effects through Markov decision processes (MDPs). Our approach involves defining fairness properties similar to those used in supervised learning and developing algorithms for learning fair decision-making policies for MDPs. We also investigate extensions to reinforcement learning, where some parts of the system are unknown and must be learned without violating fairness. In addition, we use simulations on a loan applicant MDP to demonstrate the importance of considering dynamical effects.",1
"Although exploration in reinforcement learning is well understood from a theoretical point of view, provably correct methods remain impractical. In this paper we study the interplay between exploration and approximation, what we call approximate exploration. Our main goal is to further our theoretical understanding of pseudo-count based exploration bonuses (Bellemare et al., 2016), a practical exploration scheme based on density modelling. As a warm-up, we quantify the performance of an exploration algorithm, MBIE-EB (Strehl and Littman, 2008), when explicitly combined with state aggregation. This allows us to confirm that, as might be expected, approximation allows the agent to trade off between learning speed and quality of the learned policy. Next, we show how a given density model can be related to an abstraction and that the corresponding pseudo-count bonus can act as a substitute in MBIE-EB combined with this abstraction, but may lead to either under- or over-exploration. Then, we show that a given density model also defines an implicit abstraction, and find a surprising mismatch between pseudo-counts derived either implicitly or explicitly. Finally we derive a new pseudo-count bonus alleviating this issue.",0
"Theoretical knowledge on reinforcement learning exploration is well-established, but practical implementation of correct methods remains challenging. In this article, we delve into the relationship between exploration and approximation, which we refer to as approximate exploration. Our primary objective is to enhance our comprehension of the exploration bonus technique known as pseudo-count based exploration bonuses (Bellemare et al., 2016), which is based on density modelling. Initially, we evaluate the effectiveness of an exploration algorithm, MBIE-EB (Strehl and Littman, 2008), when combined with state aggregation. This allows us to confirm that approximation enables the agent to balance between the speed of learning and the quality of the policy learned. Next, we demonstrate how a specific density model can be connected to an abstraction, and how the corresponding pseudo-count bonus can serve as a substitute in MBIE-EB when combined with this abstraction, but may cause either under- or over-exploration. We then uncover a surprising mismatch between pseudo-counts derived either implicitly or explicitly, as a given density model also establishes an implicit abstraction. Finally, we present a new pseudo-count bonus that resolves this issue.",1
"Exploration bonus derived from the novelty of the states in an environment has become a popular approach to motivate exploration for deep reinforcement learning agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. Due to the capacity limitation of the models and difficulty of performing next-frame prediction, however, these methods typically fail to balance between exploration and exploitation in high-dimensional observation tasks, resulting in the agents forgetting the visited paths and exploring those states repeatedly. Such inefficient exploration behavior causes significant performance drops, especially in large environments with sparse reward signals. In this paper, we propose to introduce the concept of optical flow estimation from the field of computer vision to deal with the above issue. We propose to employ optical flow estimation errors to examine the novelty of new observations, such that agents are able to memorize and understand the visited states in a more comprehensive fashion. We compare our method against the previous approaches in a number of experimental experiments. Our results indicate that the proposed method appears to deliver superior and long-lasting performance than the previous methods. We further provide a set of comprehensive ablative analysis of the proposed method, and investigate the impact of optical flow estimation on the learning curves of the DRL agents.",0
"In recent years, the use of exploration bonuses to motivate deep reinforcement learning agents has gained popularity. Curiosity-driven exploration methods estimate the novelty of new observations by predicting errors in their system dynamics models. However, these methods tend to struggle with balancing exploration and exploitation in high-dimensional observation tasks due to model capacity limitations and next-frame prediction difficulties. This causes agents to forget visited paths and repeatedly explore the same states, leading to poor performance in large environments with sparse reward signals. To address this issue, we propose using optical flow estimation from computer vision to examine the novelty of new observations. This allows agents to better understand and remember visited states. We conduct several experiments comparing our approach to previous methods and find that our approach delivers superior and long-lasting performance. We also perform comprehensive ablative analysis and investigate the impact of optical flow estimation on the agents' learning curves.",1
"While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.",0
"Although deep reinforcement learning has successfully tackled many challenging control tasks, its practical use has been limited by the inability to guarantee the safety of learned policies. To address this issue, we propose a verifiable reinforcement learning approach that utilizes decision tree policies. These policies can handle complex scenarios due to their nonparametric nature, while also being easily verifiable with current techniques since they are highly structured. However, training decision tree policies is difficult. Therefore, we introduce VIPER, an algorithm that combines model compression and imitation learning ideas to learn decision tree policies guided by a DNN policy (the oracle) and its Q-function. Our results show that VIPER outperforms two baselines. We apply VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves equal performance to the original DNN policy.",1
"Reinforcement learning (RL) algorithms allow agents to learn skills and strategies to perform complex tasks without detailed instructions or expensive labelled training examples. That is, RL agents can learn, as we learn. Given the importance of learning in our intelligence, RL has been thought to be one of key components to general artificial intelligence, and recent breakthroughs in deep reinforcement learning suggest that neural networks (NN) are natural platforms for RL agents. However, despite the efficiency and versatility of NN-based RL agents, their decision-making remains incomprehensible, reducing their utilities. To deploy RL into a wider range of applications, it is imperative to develop explainable NN-based RL agents. Here, we propose a method to derive a secondary comprehensible agent from a NN-based RL agent, whose decision-makings are based on simple rules. Our empirical evaluation of this secondary agent's performance supports the possibility of building a comprehensible and transparent agent using a NN-based RL agent.",0
"Reinforcement learning (RL) algorithms enable agents to acquire the skills and strategies necessary to perform complex tasks without detailed instructions or expensive labeled training examples. This means that RL agents are capable of learning in the same way that we do. Given that learning is a crucial aspect of intelligence, RL has been identified as a key component of general artificial intelligence, and recent advancements in deep reinforcement learning suggest that neural networks (NN) are a natural fit for RL agents. Despite the efficiency and versatility of NN-based RL agents, however, their decision-making processes remain incomprehensible, limiting their usefulness. To extend the applications of RL, it is essential to create NN-based RL agents that are explicable. In this study, we propose a technique for generating a secondary comprehensible agent from an NN-based RL agent whose decision-making is based on simple rules. Our experimental analysis of this secondary agent's performance indicates that it is feasible to construct a comprehensible and transparent agent utilizing an NN-based RL agent.",1
"Discovering and exploiting the causal structure in the environment is a crucial challenge for intelligent agents. Here we explore whether causal reasoning can emerge via meta-reinforcement learning. We train a recurrent network with model-free reinforcement learning to solve a range of problems that each contain causal structure. We find that the trained agent can perform causal reasoning in novel situations in order to obtain rewards. The agent can select informative interventions, draw causal inferences from observational data, and make counterfactual predictions. Although established formal causal reasoning algorithms also exist, in this paper we show that such reasoning can arise from model-free reinforcement learning, and suggest that causal reasoning in complex settings may benefit from the more end-to-end learning-based approaches presented here. This work also offers new strategies for structured exploration in reinforcement learning, by providing agents with the ability to perform -- and interpret -- experiments.",0
"The ability to detect and make use of the underlying causal structure in the environment is a critical obstacle for intelligent agents. This study aims to investigate whether meta-reinforcement learning can lead to the emergence of causal reasoning. We utilize a recurrent network trained with model-free reinforcement learning to tackle a series of problems that feature causal structures. Our findings indicate that the trained agent can apply causal reasoning in unfamiliar situations to acquire rewards. The agent can choose valuable interventions, deduce causal implications from observational data, and generate counterfactual predictions. Although established formal causal reasoning algorithms exist, we demonstrate that this kind of reasoning can arise from model-free reinforcement learning. Furthermore, we suggest that the more end-to-end, learning-based approaches presented here may benefit causal reasoning in complicated scenarios. This research also provides new techniques for structured exploration in reinforcement learning by equipping agents with the capacity to perform and interpret experiments.",1
"Policy evaluation is a key process in reinforcement learning. It assesses a given policy using estimation of the corresponding value function. When using a parameterized function to approximate the value, it is common to optimize the set of parameters by minimizing the sum of squared Bellman Temporal Differences errors. However, this approach ignores certain distributional properties of both the errors and value parameters. Taking these distributions into account in the optimization process can provide useful information on the amount of confidence in value estimation. In this work we propose to optimize the value by minimizing a regularized objective function which forms a trust region over its parameters. We present a novel optimization method, the Kalman Optimization for Value Approximation (KOVA), based on the Extended Kalman Filter. KOVA minimizes the regularized objective function by adopting a Bayesian perspective over both the value parameters and noisy observed returns. This distributional property provides information on parameter uncertainty in addition to value estimates. We provide theoretical results of our approach and analyze the performance of our proposed optimizer on domains with large state and action spaces.",0
"Reinforcement learning involves policy evaluation, which evaluates a policy by estimating its corresponding value function. To approximate the value using a parameterized function, minimizing the sum of squared Bellman Temporal Differences errors is often employed. However, this method overlooks important distributional properties of both the errors and value parameters. Incorporating these properties in the optimization process can yield valuable information about the confidence in value estimation. In this study, we suggest minimizing a regularized objective function that creates a trust region over its parameters to optimize the value. We introduce a new optimization technique, KOVA, which uses the Extended Kalman Filter and a Bayesian approach to minimize the regularized objective function. This approach considers both the value parameters and observed returns' uncertainties, providing information on parameter uncertainty beyond value estimates. We present theoretical results and performance analysis of our proposed optimizer in domains with large state and action spaces.",1
"A reinforcement learning agent tries to maximize its cumulative payoff by interacting in an unknown environment. It is important for the agent to explore suboptimal actions as well as to pick actions with highest known rewards. Yet, in sensitive domains, collecting more data with exploration is not always possible, but it is important to find a policy with a certain performance guaranty. In this paper, we present a brief survey of methods available in the literature for balancing exploration-exploitation trade off and computing robust solutions from fixed samples in reinforcement learning.",0
"The objective of a reinforcement learning agent is to achieve the maximum cumulative payoff while interacting with an unfamiliar environment. The agent should focus on both selecting actions with the highest known rewards and exploring suboptimal actions. However, in domains that require sensitivity, exploration may not always be feasible, and it is essential to create a policy that guarantees a certain level of performance. This paper provides a concise overview of existing approaches in the literature that balance trade-offs between exploration and exploitation and generate strong solutions using limited data in reinforcement learning.",1
"The task of video grounding, which temporally localizes a natural language description in a video, plays an important role in understanding videos. Existing studies have adopted strategies of sliding window over the entire video or exhaustively ranking all possible clip-sentence pairs in a pre-segmented video, which inevitably suffer from exhaustively enumerated candidates. To alleviate this problem, we formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy. Specifically, we propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training. Our proposed framework achieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video.",0
"Understanding videos requires the task of video grounding, which involves localizing a natural language description within a video. Current approaches slide a window over the entire video or exhaustively rank all possible clip-sentence pairs, resulting in a large number of candidates. To address this issue, we present a novel approach that formulates this task as a sequential decision-making problem by training an agent to regulate temporal grounding boundaries based on its policy. Our reinforcement learning-based framework, combined with multi-task learning, improves performance by incorporating supervised boundary information during training. Remarkably, our proposed framework achieves state-of-the-art results on ActivityNet'18 DenseCaption and Charades-STA datasets, requiring only 10 clips or less per video.",1
"This paper addresses the question of how a previously available control policy $\pi_s$ can be used as a supervisor to more quickly and safely train a new learned control policy $\pi_L$ for a robot. A weighted average of the supervisor and learned policies is used during trials, with a heavier weight initially on the supervisor, in order to allow safe and useful physical trials while the learned policy is still ineffective. During the process, the weight is adjusted to favor the learned policy. As weights are adjusted, the learned network must compensate so as to give safe and reasonable outputs under the different weights. A pioneer network is introduced that pre-learns a policy that performs similarly to the current learned policy under the planned next step for new weights; this pioneer network then replaces the currently learned network in the next set of trials. Experiments in OpenAI Gym demonstrate the effectiveness of the proposed method.",0
"The focus of this article is on utilizing an existing control policy, denoted as $\pi_s$, as a supervisor to expedite and ensure the safety of training a new learned control policy, $\pi_L$, for a robot. A combination of the supervisor and learned policies, where the supervisor initially carries more weight, is implemented during trials to enable secure and worthwhile physical trials while the learned policy is still insufficient. Throughout the process, the weight distribution is altered to favor the learned policy, and as a result, the learned network must compensate to generate secure and reasonable outputs under the varying weights. To address this issue, a pioneer network is introduced that pre-learns a policy that functions similarly to the current learned policy for the anticipated next step in weights. The pioneer network then replaces the current learned network for the subsequent set of trials. The effectiveness of this approach is demonstrated through experiments conducted in OpenAI Gym.",1
