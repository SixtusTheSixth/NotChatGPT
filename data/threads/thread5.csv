"We tackle the blackbox issue of deep neural networks in the settings of reinforcement learning (RL) where neural agents learn towards maximizing reward gains in an uncontrollable way. Such learning approach is risky when the interacting environment includes an expanse of state space because it is then almost impossible to foresee all unwanted outcomes and penalize them with negative rewards beforehand. Unlike reverse analysis of learned neural features from previous works, our proposed method \nj{tackles the blackbox issue by encouraging} an RL policy network to learn interpretable latent features through an implementation of a disentangled representation learning method. Toward this end, our method allows an RL agent to understand self-efficacy by distinguishing its influences from uncontrollable environmental factors, which closely resembles the way humans understand their scenes. Our experimental results show that the learned latent factors not only are interpretable, but also enable modeling the distribution of entire visited state space with a specific action condition. We have experimented that this characteristic of the proposed structure can lead to ex post facto governance for desired behaviors of RL agents.",0
"In the context of reinforcement learning, we address the problem of blackbox deep neural networks, where agents aim to maximize rewards in an unpredictable environment. This approach is risky in complex environments, where it is difficult to anticipate and penalize unwanted outcomes. Rather than reverse engineering previous work, we propose a disentangled representation learning method that encourages interpretable latent features for an RL policy network. This allows the agent to distinguish between controllable and uncontrollable factors, similar to human perception. Our experiments show that the learned factors are interpretable and can model the entire state space with specific actions, leading to better control over agent behavior.",1
"In this paper, we develop a reinforcement learning (RL) based system to learn an effective policy for carpooling that maximizes transportation efficiency so that fewer cars are required to fulfill the given amount of trip demand. For this purpose, first, we develop a deep neural network model, called ST-NN (Spatio-Temporal Neural Network), to predict taxi trip time from the raw GPS trip data. Secondly, we develop a carpooling simulation environment for RL training, with the output of ST-NN and using the NYC taxi trip dataset. In order to maximize transportation efficiency and minimize traffic congestion, we choose the effective distance covered by the driver on a carpool trip as the reward. Therefore, the more effective distance a driver achieves over a trip (i.e. to satisfy more trip demand) the higher the efficiency and the less will be the traffic congestion. We compared the performance of RL learned policy to a fixed policy (which always accepts carpool) as a baseline and obtained promising results that are interpretable and demonstrate the advantage of our RL approach. We also compare the performance of ST-NN to that of state-of-the-art travel time estimation methods and observe that ST-NN significantly improves the prediction performance and is more robust to outliers.",0
"The aim of this study was to develop a carpooling policy that would maximize transportation efficiency and reduce the number of cars needed to meet trip demand. To achieve this, a reinforcement learning (RL) based system was created, with a focus on minimizing traffic congestion and maximizing the distance covered by drivers during carpool trips. The study used the NYC taxi trip dataset and developed a deep neural network model, ST-NN, to predict taxi trip time from raw GPS data. The RL training was done in a carpooling simulation environment using the output of ST-NN. Results showed that the RL learned policy outperformed a fixed policy in terms of efficiency and traffic congestion. Additionally, ST-NN was found to significantly improve travel time prediction performance compared to state-of-the-art methods and was more robust to outliers.",1
"Direct policy search is one of the most important algorithm of reinforcement learning. However, learning from scratch needs a large amount of experience data and can be easily prone to poor local optima. In addition to that, a partially trained policy tends to perform dangerous action to agent and environment. In order to overcome these challenges, this paper proposed a policy initialization algorithm called Policy Learning based on Completely Behavior Cloning (PLCBC). PLCBC first transforms the Model Predictive Control (MPC) controller into a piecewise affine (PWA) function using multi-parametric programming, and uses a neural network to express this function. By this way, PLCBC can completely clone the MPC controller without any performance loss, and is totally training-free. The experiments show that this initialization strategy can help agent learn at the high reward state region, and converge faster and better.",0
"Reinforcement learning relies heavily on the direct policy search algorithm, which is crucial. However, starting from scratch can pose challenges as it requires a vast amount of experience data and may result in suboptimal results. Additionally, an incompletely trained policy may cause harm to the agent and the environment. To address these issues, the authors propose a policy initialization algorithm called Policy Learning based on Completely Behavior Cloning (PLCBC). This algorithm transforms the Model Predictive Control (MPC) controller into a piecewise affine (PWA) function using multi-parametric programming and expresses it using a neural network. The PLCBC can clone the MPC controller without any loss of performance and requires no training. The experiments demonstrate that this initialization approach can help the agent learn in high reward state regions and achieve faster and better convergence.",1
"Correlation filter has been proven to be an effective tool for a number of approaches in visual tracking, particularly for seeking a good balance between tracking accuracy and speed. However, correlation filter based models are susceptible to wrong updates stemming from inaccurate tracking results. To date, little effort has been devoted towards handling the correlation filter update problem. In this paper, we propose a novel approach to address the correlation filter update problem. In our approach, we update and maintain multiple correlation filter models in parallel, and we use deep reinforcement learning for the selection of an optimal correlation filter model among them. To facilitate the decision process in an efficient manner, we propose a decision-net to deal target appearance modeling, which is trained through hundreds of challenging videos using proximal policy optimization and a lightweight learning network. An exhaustive evaluation of the proposed approach on the OTB100 and OTB2013 benchmarks show that the approach is effective enough to achieve the average success rate of 62.3% and the average precision score of 81.2%, both exceeding the performance of traditional correlation filter based trackers.",0
"The correlation filter has been a useful tool in visual tracking, balancing accuracy and speed. However, it is prone to errors caused by inaccurate tracking results. Little has been done to address this update problem. This paper proposes a new method by maintaining multiple correlation filter models and using deep reinforcement learning to select the best one. A decision-net is also proposed to efficiently model target appearance. Through evaluation on challenging videos, the approach achieved an average success rate of 62.3% and an average precision score of 81.2%, surpassing traditional correlation filter based trackers.",1
"Reinforcement Learning (RL) agents require the specification of a reward signal for learning behaviours. However, introduction of corrupt or stochastic rewards can yield high variance in learning. Such corruption may be a direct result of goal misspecification, randomness in the reward signal, or correlation of the reward with external factors that are not known to the agent. Corruption or stochasticity of the reward signal can be especially problematic in robotics, where goal specification can be particularly difficult for complex tasks. While many variance reduction techniques have been studied to improve the robustness of the RL process, handling such stochastic or corrupted reward structures remains difficult. As an alternative for handling this scenario in model-free RL methods, we suggest using an estimator for both rewards and value functions. We demonstrate that this improves performance under corrupted stochastic rewards in both the tabular and non-linear function approximation settings for a variety of noise types and environments. The use of reward estimation is a robust and easy-to-implement improvement for handling corrupted reward signals in model-free RL.",0
"For Reinforcement Learning (RL) agents to learn behaviours, a reward signal needs to be specified. However, if the rewards are corrupt or stochastic, the learning process can have high variance. Such corruption can arise from goal misspecification, reward signal randomness or correlation with external factors unknown to the agent. This issue is particularly challenging in robotics where complex tasks make goal specification difficult. While there are several variance reduction techniques to enhance the RL process's robustness, handling stochastic or corrupt reward structures remains daunting. To address this scenario in model-free RL methods, we propose using an estimator for rewards and value functions. Our findings show that this approach enhances performance under corrupted stochastic rewards across different noise types and environments in both tabular and non-linear function approximation settings. Reward estimation is an easy-to-implement and robust approach to handling corrupt reward signals in model-free RL.",1
"In this paper, we propose the Quantile Option Architecture (QUOTA) for exploration based on recent advances in distributional reinforcement learning (RL). In QUOTA, decision making is based on quantiles of a value distribution, not only the mean. QUOTA provides a new dimension for exploration via making use of both optimism and pessimism of a value distribution. We demonstrate the performance advantage of QUOTA in both challenging video games and physical robot simulators.",0
"The Quantile Option Architecture (QUOTA) is introduced in this paper as a method for exploration that takes advantage of progress in distributional reinforcement learning (RL). With QUOTA, decision making is grounded in the quantiles of a value distribution, rather than solely on the mean. QUOTA allows for exploration to occur through the utilization of both the optimism and pessimism of a value distribution. Our study showcases the performance benefits of QUOTA in difficult video games and physical robot simulators.",1
"In this paper, we propose an actor ensemble algorithm, named ACE, for continuous control with a deterministic policy in reinforcement learning. In ACE, we use actor ensemble (i.e., multiple actors) to search the global maxima of the critic. Besides the ensemble perspective, we also formulate ACE in the option framework by extending the option-critic architecture with deterministic intra-option policies, revealing a relationship between ensemble and options. Furthermore, we perform a look-ahead tree search with those actors and a learned value prediction model, resulting in a refined value estimation. We demonstrate a significant performance boost of ACE over DDPG and its variants in challenging physical robot simulators.",0
"The paper introduces ACE, an algorithm for continuous control that utilizes an actor ensemble approach with a deterministic policy in reinforcement learning. ACE aims to search for the global maxima of the critic by using multiple actors. Additionally, ACE is formulated in the option framework by extending the option-critic architecture with deterministic intra-option policies, which reveals a connection between ensemble and options. Furthermore, a look-ahead tree search is conducted with the actors and a learned value prediction model, which improves the value estimation. The results demonstrate a substantial performance improvement of ACE over DDPG and its variants in physically demanding robot simulators.",1
"This work investigates continual learning of two segmentation tasks in brain MRI with neural networks. To explore in this context the capabilities of current methods for countering catastrophic forgetting of the first task when a new one is learned, we investigate elastic weight consolidation, a recently proposed method based on Fisher information, originally evaluated on reinforcement learning of Atari games. We use it to sequentially learn segmentation of normal brain structures and then segmentation of white matter lesions. Our findings show this recent method reduces catastrophic forgetting, while large room for improvement exists in these challenging settings for continual learning.",0
"The focus of this study is to examine the use of neural networks for continual learning of two segmentation tasks in brain MRI. Specifically, we investigate the efficacy of elastic weight consolidation, a method based on Fisher information and previously tested in reinforcement learning of Atari games, in countering catastrophic forgetting of the first task when a new one is learned. Our experiment involves sequentially learning segmentation of normal brain structures followed by segmentation of white matter lesions. Our results indicate that this method can reduce catastrophic forgetting; however, there is still significant room for improvement in the challenging context of continual learning.",1
"In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent's policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent's actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives.",0
"Humans interact with complex environments through various asynchronous stochastic discrete events in continuous time. The question is whether we can create online interventions that aid humans in achieving specific objectives in this type of setting. Our paper tackles this issue through the lens of deep reinforcement learning of marked temporal point processes, where an agent's actions and the feedback from the environment are asynchronous stochastic discrete events characterized by marked temporal point processes. We establish the agent's policy by utilizing the intensity and mark distribution of the corresponding process and then develop a flexible policy gradient technique that incorporates the agent's actions and feedback into real-valued vectors via deep recurrent neural networks. Our method does not assume any functional form of the intensity and mark distribution of the feedback, and it can handle intricate reward functions. We apply our approach to personalized teaching and viral marketing, using data from Duolingo and Twitter, and demonstrate that it can identify interventions that help learners and marketers achieve their objectives more efficiently than other methods.",1
"Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its statistical efficiency are available. In this work, we provide a simple and explicit finite time analysis of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors standard techniques for analyzing stochastic gradient descent algorithms, and therefore inherits the simplicity and elegance of that literature. Final sections of the paper show how all of our main results extend to the study of TD learning with eligibility traces, known as TD($\lambda$), and to Q-learning applied in high-dimensional optimal stopping problems.",0
"Temporal difference learning (TD) is an iterative algorithm that estimates the value function of a given policy in a Markov decision process. Despite being widely used in reinforcement learning, TD's theoretical analysis is difficult, and there are few guarantees on its statistical efficiency. This study presents a simple and explicit finite time analysis of TD with linear function approximation. Our analysis follows standard techniques for analyzing stochastic gradient descent algorithms, with a few key insights. Additionally, our main results extend to TD learning with eligibility traces (TD($\lambda$)) and Q-learning in high-dimensional optimal stopping problems.",1
"With the growing prevalence of smart grid technology, short-term load forecasting (STLF) becomes particularly important in power system operations. There is a large collection of methods developed for STLF, but selecting a suitable method under varying conditions is still challenging. This paper develops a novel reinforcement learning based dynamic model selection (DMS) method for STLF. A forecasting model pool is first built, including ten state-of-the-art machine learning based forecasting models. Then a Q-learning agent learns the optimal policy of selecting the best forecasting model for the next time step, based on the model performance. The optimal DMS policy is applied to select the best model at each time step with a moving window. Numerical simulations on two-year load and weather data show that the Q-learning algorithm converges fast, resulting in effective and efficient DMS. The developed STLF model with Q-learning based DMS improves the forecasting accuracy by approximately 50%, compared to the state-of-the-art machine learning based STLF models.",0
"The importance of short-term load forecasting (STLF) in power system operations has increased with the growing prevalence of smart grid technology. Although many methods have been developed for STLF, it remains a challenge to select a suitable method under varying conditions. This study proposes a novel reinforcement learning based dynamic model selection (DMS) method for STLF. A forecasting model pool is created, comprising ten state-of-the-art machine learning based forecasting models. The Q-learning agent learns the optimal policy for selecting the best forecasting model for the next time step, based on model performance. The best model is selected at each time step with a moving window, using the optimal DMS policy. Numerical simulations on two-year load and weather data demonstrate that the Q-learning algorithm converges quickly, resulting in effective and efficient DMS. The developed STLF model with Q-learning based DMS improves forecasting accuracy by approximately 50% compared to state-of-the-art machine learning based STLF models.",1
"Recently Neural Architecture Search (NAS) has aroused great interest in both academia and industry, however it remains challenging because of its huge and non-continuous search space. Instead of applying evolutionary algorithm or reinforcement learning as previous works, this paper proposes a Direct Sparse Optimization NAS (DSO-NAS) method. In DSO-NAS, we provide a novel model pruning view to NAS problem. In specific, we start from a completely connected block, and then introduce scaling factors to scale the information flow between operations. Next, we impose sparse regularizations to prune useless connections in the architecture. Lastly, we derive an efficient and theoretically sound optimization method to solve it. Our method enjoys both advantages of differentiability and efficiency, therefore can be directly applied to large datasets like ImageNet. Particularly, On CIFAR-10 dataset, DSO-NAS achieves an average test error 2.84\%, while on the ImageNet dataset DSO-NAS achieves 25.4\% test error under 600M FLOPs with 8 GPUs in 18 hours.",0
"NAS has become a topic of interest for both academia and industry, but its non-continuous and vast search space makes it a challenging task. This paper suggests a novel approach called Direct Sparse Optimization NAS (DSO-NAS) that differs from previous methods that used evolutionary algorithm or reinforcement learning. DSO-NAS adopts a model pruning perspective where a completely connected block is pruned using scaling factors to regulate information flow between operations and sparse regularizations to eliminate useless connections. An efficient and sound optimization method is derived to solve the problem, making it differentiable and efficient for use in large datasets like ImageNet. DSO-NAS achieves an average test error of 2.84% on CIFAR-10 dataset and 25.4% test error on the ImageNet dataset with 600M FLOPs using 8 GPUs in 18 hours.",1
"Sparse reward problems are one of the biggest challenges in Reinforcement Learning. Goal-directed tasks are one such sparse reward problems where a reward signal is received only when the goal is reached. One promising way to train an agent to perform goal-directed tasks is to use Hindsight Learning approaches. In these approaches, even when an agent fails to reach the desired goal, the agent learns to reach the goal it achieved instead. Doing this over multiple trajectories while generalizing the policy learned from the achieved goals, the agent learns a goal conditioned policy to reach any goal. One such approach is Hindsight Experience replay which uses an off-policy Reinforcement Learning algorithm to learn a goal conditioned policy. In this approach, a replay of the past transitions happens in a uniformly random fashion. Another approach is to use a Hindsight version of the policy gradients to directly learn a policy. In this work, we discuss different ways to replay past transitions to improve learning in hindsight experience replay focusing on prioritized variants in particular. Also, we implement the Hindsight Policy gradient methods to robotic tasks.",0
"Reinforcement Learning faces a major obstacle in sparse reward problems, particularly in goal-directed tasks where rewards are only given upon reaching the goal. Hindsight Learning approaches offer a promising solution by training the agent to achieve the goals it has reached, even if it failed to reach the desired goal. This is done through multiple trajectories and generalizing the policy learned from the achieved goals. One such approach is the Hindsight Experience replay, which uses an off-policy Reinforcement Learning algorithm to learn a goal-conditioned policy. It randomly replays past transitions, while another approach uses a Hindsight version of the policy gradients to directly learn a policy. This study delves into various ways to replay past transitions, prioritizing variants to improve learning in hindsight experience replay. Additionally, we apply the Hindsight Policy gradient methods to robotic tasks.",1
"Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).",0
"Although model-based reinforcement learning (RL) algorithms are highly efficient with minimal samples, they often struggle to match the asymptotic performance of the best model-free algorithms, especially when using high-capacity parametric function approximators like deep networks. To address this gap, this study investigates the use of uncertainty-aware dynamics models to improve results. The proposed algorithm, Probabilistic Ensembles with Trajectory Sampling (PETS), integrates deep network dynamics models with sampling-based uncertainty propagation. Comparisons to current model-based and model-free deep RL algorithms demonstrate that PETS achieves similar asymptotic performance as model-free algorithms for challenging benchmark tasks, while using significantly fewer samples (e.g., PETS required 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization, respectively, for the half-cheetah task).",1
"Dantzig Selector (DS) is widely used in compressed sensing and sparse learning for feature selection and sparse signal recovery. Since the DS formulation is essentially a linear programming optimization, many existing linear programming solvers can be simply applied for scaling up. The DS formulation can be explained as a basis pursuit denoising problem, wherein the data matrix (or measurement matrix) is employed as the denoising matrix to eliminate the observation noise. However, we notice that the data matrix may not be the optimal denoising matrix, as shown by a simple counter-example. This motivates us to pursue a better denoising matrix for defining a general DS formulation. We first define the optimal denoising matrix through a minimax optimization, which turns out to be an NPhard problem. To make the problem computationally tractable, we propose a novel algorithm, termed as Optimal Denoising Dantzig Selector (ODDS), to approximately estimate the optimal denoising matrix. Empirical experiments validate the proposed method. Finally, a novel sparse reinforcement learning algorithm is formulated by extending the proposed ODDS algorithm to temporal difference learning, and empirical experimental results demonstrate to outperform the conventional vanilla DS-TD algorithm.",0
"The Dantzig Selector (DS) is a commonly used technique for feature selection and sparse signal recovery in compressed sensing and sparse learning. The DS approach is based on linear programming optimization and can be easily scaled up using existing solvers. Essentially, the DS formulation is a basis pursuit denoising problem where the denoising matrix, typically the data or measurement matrix, is used to remove observation noise. However, the data matrix may not always be the best denoising matrix, as demonstrated by a counter-example. To address this limitation, we aim to find a better denoising matrix by minimizing the maximization problem, which is unfortunately an NPhard problem. To overcome this, we introduce a new algorithm called Optimal Denoising Dantzig Selector (ODDS) that can approximately estimate the optimal denoising matrix. Empirical experiments support the effectiveness of our proposed approach. Additionally, we extend the ODDS algorithm to temporal difference learning to formulate a novel sparse reinforcement learning algorithm that outperforms traditional DS-TD techniques.",1
"The problem-solving in automated theorem proving (ATP) can be interpreted as a search problem where the prover constructs a proof tree step by step. In this paper, we propose a deep reinforcement learning algorithm for proof search in intuitionistic propositional logic. The most significant challenge in the application of deep learning to the ATP is the absence of large, public theorem database. We, however, overcame this issue by applying a novel data augmentation procedure at each iteration of the reinforcement learning. We also improve the efficiency of the algorithm by representing the syntactic structure of formulas by a novel compact graph representation. Using the large volume of augmented data, we train highly accurate graph neural networks that approximate the value function for the set of the syntactic structures of formulas. Our method is also cost-efficient in terms of computational time. We will show that our prover outperforms Coq's $\texttt{tauto}$ tactic, a prover based on human-engineered heuristics. Within the specified time limit, our prover solved 84% of the theorems in a benchmark library, while $\texttt{tauto}$ was able to solve only 52%.",0
"Automated theorem proving (ATP) involves constructing a proof tree through a search process. We propose a deep reinforcement learning algorithm for proof search in intuitionistic propositional logic, which faces challenges due to the absence of a large, public theorem database. To overcome this issue, we apply a novel data augmentation procedure during each iteration of the reinforcement learning process and represent formula syntax using a compact graph structure to improve algorithm efficiency. Our method is cost-efficient and outperforms Coq's $\texttt{tauto}$ tactic, solving 84% of benchmark library theorems within the specified time limit, while $\texttt{tauto}$ solves only 52%. We achieve this high accuracy through training graph neural networks on a large volume of augmented data that approximate the value function for the set of syntactic structures of formulas.",1
"Deep generative models have been successfully used to learn representations for high-dimensional discrete spaces by representing discrete objects as sequences and employing powerful sequence-based deep models. Unfortunately, these sequence-based models often produce invalid sequences: sequences which do not represent any underlying discrete structure; invalid sequences hinder the utility of such models. As a step towards solving this problem, we propose to learn a deep recurrent validator model, which can estimate whether a partial sequence can function as the beginning of a full, valid sequence. This validator provides insight as to how individual sequence elements influence the validity of the overall sequence, and can be used to constrain sequence based models to generate valid sequences -- and thus faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle which can evaluate validity of complete sequences provides a sparse reward signal. We demonstrate its effectiveness as a generative model of Python 3 source code for mathematical expressions, and in improving the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures.",0
"To learn representations for high-dimensional discrete spaces, deep generative models have been successfully employed. They represent discrete objects as sequences and use powerful sequence-based deep models. However, these models often generate invalid sequences that do not represent any underlying discrete structure. This hinders the utility of the models. To address this issue, we propose the use of a deep recurrent validator model that can estimate whether a partial sequence can function as the beginning of a full and valid sequence. This validator provides insights into how individual sequence elements influence the validity of the overall sequence. It can also be used to constrain sequence-based models to generate valid sequences, thus enabling them to faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle that can evaluate the validity of complete sequences provides a sparse reward signal. We demonstrate the effectiveness of our approach as a generative model of Python 3 source code for mathematical expressions. Additionally, we show how it improves the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures.",1
"When environmental interaction is expensive, model-based reinforcement learning offers a solution by planning ahead and avoiding costly mistakes. Model-based agents typically learn a single-step transition model. In this paper, we propose a multi-step model that predicts the outcome of an action sequence with variable length. We show that this model is easy to learn, and that the model can make policy-conditional predictions. We report preliminary results that show a clear advantage for the multi-step model compared to its one-step counterpart.",0
"Model-based reinforcement learning can be useful in cases where environmental interaction is costly as it allows for planning ahead and minimizing errors. Traditionally, model-based agents only learn single-step transition models. However, our paper proposes a multi-step model that can predict the outcome of action sequences with varying lengths. This model is simple to learn and can make predictions based on policies. Our preliminary results demonstrate that the multi-step model outperforms the single-step model.",1
"We introduce TensorFlow Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in TensorFlow. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the TensorFlow execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce BatchPPO, an efficient implementation of the proximal policy optimization algorithm. By open sourcing TensorFlow Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.",0
"TensorFlow Agents is a new infrastructure paradigm that enables the development of parallel reinforcement learning algorithms in TensorFlow. By simulating multiple environments in parallel and grouping them to perform neural network computation on a batch, the TensorFlow execution engine can parallelize computation without requiring manual synchronization. Environments are stepped in separate Python processes to allow for parallel progression without interference from the global interpreter lock. As part of this initiative, BatchPPO, a highly efficient implementation of the proximal policy optimization algorithm, has been introduced. Our goal in open sourcing TensorFlow Agents is to provide a flexible foundation for future research projects that will accelerate advancements in the field.",1
"Policy optimization is an effective reinforcement learning approach to solve continuous control tasks. Recent achievements have shown that alternating online and offline optimization is a successful choice for efficient trajectory reuse. However, deciding when to stop optimizing and collect new trajectories is non-trivial, as it requires to account for the variance of the objective function estimate. In this paper, we propose a novel, model-free, policy search algorithm, POIS, applicable in both action-based and parameter-based settings. We first derive a high-confidence bound for importance sampling estimation; then we define a surrogate objective function, which is optimized offline whenever a new batch of trajectories is collected. Finally, the algorithm is tested on a selection of continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods.",0
"To address continuous control tasks, policy optimization is a reinforcement learning approach that has proven effective. Recent advancements demonstrate that alternating online and offline optimization is a successful approach for efficient trajectory reuse. However, determining the optimal time to stop optimizing and gather new trajectories is challenging due to the need to consider the variance of the objective function estimate. This paper proposes a novel policy search algorithm, POIS, that is model-free and applicable in both action-based and parameter-based settings. The algorithm derives a high-confidence bound for importance sampling estimation and defines a surrogate objective function that is offline-optimized when a new batch of trajectories is collected. The algorithm is tested on various continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods.",1
"Recently, with convolutional neural networks gaining significant achievements in many challenging machine learning fields, hand-crafted neural networks no longer satisfy our requirements as designing a network will cost a lot, and automatically generating architectures has attracted increasingly more attention and focus. Some research on auto-generated networks has achieved promising results. However, they mainly aim at picking a series of single layers such as convolution or pooling layers one by one. There are many elegant and creative designs in the carefully hand-crafted neural networks, such as Inception-block in GoogLeNet, residual block in residual network and dense block in dense convolutional network. Based on reinforcement learning and taking advantages of the superiority of these networks, we propose a novel automatic process to design a multi-block neural network, whose architecture contains multiple types of blocks mentioned above, with the purpose to do structure learning of deep neural networks and explore the possibility whether different blocks can be composed together to form a well-behaved neural network. The optimal network is created by the Q-learning agent who is trained to sequentially pick different types of blocks. To verify the validity of our proposed method, we use the auto-generated multi-block neural network to conduct experiments on image benchmark datasets MNIST, SVHN and CIFAR-10 image classification task with restricted computational resources. The results demonstrate that our method is very effective, achieving comparable or better performance than hand-crafted networks and advanced auto-generated neural networks.",0
"Convolutional neural networks have made significant progress in many challenging machine learning fields, rendering hand-crafted neural networks inadequate due to high design costs. Consequently, there is a growing interest in automatically generating architectures by selecting individual layers such as convolution or pooling layers. However, this approach has limitations as it fails to incorporate the elegant and creative designs found in carefully hand-crafted neural networks. Therefore, we propose a novel approach that uses reinforcement learning to design a multi-block neural network that contains various types of blocks, including Inception-block, residual block, and dense block, to explore the possibility of creating a well-behaved neural network. Our Q-learning agent sequentially selects different types of blocks to create an optimal network, which we test on image benchmark datasets with limited computational resources. Our results demonstrate that our approach is highly effective, achieving comparable or better performance than both hand-crafted and advanced auto-generated neural networks.",1
"This work provides a thorough study on how reward scaling can affect performance of deep reinforcement learning agents. In particular, we would like to answer the question that how does reward scaling affect non-saturating ReLU networks in RL? This question matters because ReLU is one of the most effective activation functions for deep learning models. We also propose an Adaptive Network Scaling framework to find a suitable scale of the rewards during learning for better performance. We conducted empirical studies to justify the solution.",0
"The focus of this study is to investigate the impact of reward scaling on the performance of deep reinforcement learning agents. Specifically, the inquiry centers around how non-saturating ReLU networks in RL are influenced by reward scaling. This query is significant as ReLU is a highly efficient activation function for deep learning models. Additionally, we introduce the Adaptive Network Scaling framework as a means of identifying an optimal reward scale for improved performance during the learning process. Our proposed solution is supported by empirical research.",1
"Attention modules connecting encoder and decoders have been widely applied in the field of object recognition, image captioning, visual question answering and neural machine translation, and significantly improves the performance. In this paper, we propose a bottom-up gated hierarchical attention (GHA) mechanism for image captioning. Our proposed model employs a CNN as the decoder which is able to learn different concepts at different layers, and apparently, different concepts correspond to different areas of an image. Therefore, we develop the GHA in which low-level concepts are merged into high-level concepts and simultaneously low-level attended features pass to the top to make predictions. Our GHA significantly improves the performance of the model that only applies one level attention, for example, the CIDEr score increases from 0.923 to 0.999, which is comparable to the state-of-the-art models that employ attributes boosting and reinforcement learning (RL). We also conduct extensive experiments to analyze the CNN decoder and our proposed GHA, and we find that deeper decoders cannot obtain better performance, and when the convolutional decoder becomes deeper the model is likely to collapse during training.",0
"The use of attention modules that connect encoders and decoders has become prevalent in the fields of object recognition, image captioning, visual question answering, and neural machine translation. This has led to significant improvements in performance. In this study, we introduce a new bottom-up gated hierarchical attention (GHA) mechanism for image captioning. Our approach employs a CNN as the decoder, which can learn various concepts at different layers with each concept corresponding to specific areas of an image. The GHA merges low-level concepts into high-level ones while passing low-level attended features to the top for predictions. Our GHA model significantly outperforms one-level attention models, as evidenced by the increase in CIDEr score from 0.923 to 0.999. This performance is competitive with state-of-the-art models that use attributes boosting and reinforcement learning. We also conducted extensive experiments to analyze the CNN decoder and our proposed GHA and discovered that deeper decoders do not necessarily lead to better performance. In fact, when the convolutional decoder is too deep, the model is prone to collapse during training.",1
"We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",0
"To enhance deep reinforcement learning techniques, we propose an easy-to-implement exploration bonus with minimal computational overhead. This bonus is determined by the difference between the error of a fixed randomly initialized neural network and that of a neural network predicting observations' features. Additionally, we introduce a method that allows for the flexible combination of intrinsic and extrinsic rewards. By using the random network distillation (RND) bonus in conjunction with this increased flexibility, we achieve significant progress in several challenging exploration Atari games, including Montezuma's Revenge. Our approach sets a new benchmark for this game, which is notoriously difficult for deep reinforcement learning methods, surpassing average human performance without requiring demonstrations or access to the game's underlying state. Moreover, our method occasionally completes the first level of the game.",1
"This paper investigates the vision-based autonomous driving with deep learning and reinforcement learning methods. Different from the end-to-end learning method, our method breaks the vision-based lateral control system down into a perception module and a control module. The perception module which is based on a multi-task learning neural network first takes a driver-view image as its input and predicts the track features. The control module which is based on reinforcement learning then makes a control decision based on these features. In order to improve the data efficiency, we propose visual TORCS (VTORCS), a deep reinforcement learning environment which is based on the open racing car simulator (TORCS). By means of the provided functions, one can train an agent with the input of an image or various physical sensor measurement, or evaluate the perception algorithm on this simulator. The trained reinforcement learning controller outperforms the linear quadratic regulator (LQR) controller and model predictive control (MPC) controller on different tracks. The experiments demonstrate that the perception module shows promising performance and the controller is capable of controlling the vehicle drive well along the track center with visual input.",0
"This article explores the use of deep learning and reinforcement learning techniques in autonomous driving that relies on vision. Our approach differs from the end-to-end learning method by breaking down the vision-based lateral control system into two modules: a perception and a control module. The perception module uses a multi-task learning neural network to analyze driver-view images and predict track features. The control module then uses reinforcement learning to make control decisions based on these features. To enhance data efficiency, we introduce visual TORCS (VTORCS), a deep reinforcement learning environment based on the open racing car simulator (TORCS). This allows for training an agent using image inputs or various physical sensor measurements, as well as evaluating the perception algorithm on the simulator. Our trained reinforcement learning controller outperforms the linear quadratic regulator (LQR) controller and model predictive control (MPC) controller on different tracks. The results demonstrate the effectiveness of our perception module and the controller's ability to successfully drive the vehicle along the track center using visual input.",1
"Generalization has been one of the major challenges for learning dynamics models in model-based reinforcement learning. However, previous work on action-conditioned dynamics prediction focuses on learning the pixel-level motion and thus does not generalize well to novel environments with different object layouts. In this paper, we present a novel object-oriented framework, called object-oriented dynamics predictor (OODP), which decomposes the environment into objects and predicts the dynamics of objects conditioned on both actions and object-to-object relations. It is an end-to-end neural network and can be trained in an unsupervised manner. To enable the generalization ability of dynamics learning, we design a novel CNN-based relation mechanism that is class-specific (rather than object-specific) and exploits the locality principle. Empirical results show that OODP significantly outperforms previous methods in terms of generalization over novel environments with various object layouts. OODP is able to learn from very few environments and accurately predict dynamics in a large number of unseen environments. In addition, OODP learns semantically and visually interpretable dynamics models.",0
"Learning dynamics models in model-based reinforcement learning has been a significant challenge due to the issue of generalization. Prior research on action-conditioned dynamics prediction has focused on learning pixel-level motion, which does not generalize effectively to new environments with different object arrangements. Our paper introduces a novel object-oriented framework, the object-oriented dynamics predictor (OODP), which breaks down the environment into objects and predicts their dynamics based on both actions and object-to-object relationships. OODP is an end-to-end neural network that can be trained unsupervisedly. To promote the generalization capabilities of dynamics learning, we have developed a unique CNN-based relation mechanism that is class-specific and utilizes the locality principle. Our experimental results indicate that OODP outperforms previous methods in terms of generalization to various object layouts in new environments. OODP can learn from only a few environments and accurately forecast dynamics in numerous unseen environments. Furthermore, OODP learns dynamics models that are semantically and visually interpretable.",1
"Multi-objective optimizations are frequently encountered in engineering practices. The solution techniques and parametric selections however are usually problem-specific. In this study we formulate a reinforcement learning hyper-heuristic scheme, and propose four low-level heuristics which can work coherently with the single point search algorithm MOSA/R (Multi-Objective Simulated Annealing Algorithm based on Re-seed) towards multi-objective optimization problems of general applications. Making use of the domination amount, crowding distance and hypervolume calculations, the proposed hyper-heuristic scheme can meet various optimization requirements adaptively and autonomously. The approach developed not only exhibits improved and more robust performance compared to AMOSA, NSGA-II and MOEA/D when applied to benchmark test cases, but also shows promising results when applied to a generic structural fault identification problem. The outcome of this research can be extended to a variety of design and manufacturing optimization applications.",0
"In engineering practices, multi-objective optimizations are common, but the techniques and parameters used are typically specific to the problem at hand. To address this issue, we have developed a reinforcement learning hyper-heuristic scheme that employs four low-level heuristics that work in tandem with the MOSA/R algorithm to solve multi-objective optimization problems of general applicability. Our hyper-heuristic scheme employs domination amount, crowding distance, and hypervolume calculations to adaptively and autonomously meet various optimization requirements. Our approach outperforms AMOSA, NSGA-II, and MOEA/D when applied to benchmark test cases, and also demonstrates promising results when applied to a generic structural fault identification problem. This research has broad implications for design and manufacturing optimization applications.",1
"Collecting training data from the physical world is usually time-consuming and even dangerous for fragile robots, and thus, recent advances in robot learning advocate the use of simulators as the training platform. Unfortunately, the reality gap between synthetic and real visual data prohibits direct migration of the models trained in virtual worlds to the real world. This paper proposes a modular architecture for tackling the virtual-to-real problem. The proposed architecture separates the learning model into a perception module and a control policy module, and uses semantic image segmentation as the meta representation for relating these two modules. The perception module translates the perceived RGB image to semantic image segmentation. The control policy module is implemented as a deep reinforcement learning agent, which performs actions based on the translated image segmentation. Our architecture is evaluated in an obstacle avoidance task and a target following task. Experimental results show that our architecture significantly outperforms all of the baseline methods in both virtual and real environments, and demonstrates a faster learning curve than them. We also present a detailed analysis for a variety of variant configurations, and validate the transferability of our modular architecture.",0
"Gathering training data from the physical world can be a time-consuming and risky process for delicate robots. Therefore, recent robot learning advancements suggest using simulators as a training platform. However, the gap between synthetic and real visual data makes it difficult to transfer models trained in virtual worlds to the real world. This study proposes a modular architecture to address the virtual-to-real problem. The architecture divides the learning model into a perception module and a control policy module, using semantic image segmentation as the meta representation to relate the two modules. The perception module converts RGB images to semantic image segmentation, while the control policy module functions as a deep reinforcement learning agent, taking action based on the translated image segmentation. In two tasks, obstacle avoidance and target following, our architecture outperforms all baseline methods in virtual and real environments, with a faster learning curve. A thorough analysis of various configurations confirms the transferability of our modular architecture.",1
"Dynamic spectrum access (DSA) is regarded as an effective and efficient technology to share radio spectrum among different networks. As a secondary user (SU), a DSA device will face two critical problems: avoiding causing harmful interference to primary users (PUs), and conducting effective interference coordination with other secondary users. These two problems become even more challenging for a distributed DSA network where there is no centralized controllers for SUs. In this paper, we investigate communication strategies of a distributive DSA network under the presence of spectrum sensing errors. To be specific, we apply the powerful machine learning tool, deep reinforcement learning (DRL), for SUs to learn ""appropriate"" spectrum access strategies in a distributed fashion assuming NO knowledge of the underlying system statistics. Furthermore, a special type of recurrent neural network (RNN), called the reservoir computing (RC), is utilized to realize DRL by taking advantage of the underlying temporal correlation of the DSA network. Using the introduced machine learning-based strategy, SUs could make spectrum access decisions distributedly relying only on their own current and past spectrum sensing outcomes. Through extensive experiments, our results suggest that the RC-based spectrum access strategy can help the SU to significantly reduce the chances of collision with PUs and other SUs. We also show that our scheme outperforms the myopic method which assumes the knowledge of system statistics, and converges faster than the Q-learning method when the number of channels is large.",0
"Dynamic spectrum access (DSA) is considered an efficient technology for sharing radio spectrum among various networks. Secondary users (SUs) using DSA devices face two critical issues: avoiding harmful interference to primary users (PUs) and coordinating with other SUs to manage interference. These challenges become even more complex in a distributed DSA network without centralized controllers for SUs. In this research, we explore communication strategies for a distributed DSA network in the presence of spectrum sensing errors. Our approach employs deep reinforcement learning (DRL), a powerful machine learning tool, for SUs to learn appropriate spectrum access strategies in a distributed manner without knowledge of underlying system statistics. We use a type of recurrent neural network (RNN) called reservoir computing (RC) to facilitate DRL and leverage the temporal correlation of the DSA network. Our machine learning-based strategy allows SUs to make spectrum access decisions based solely on their current and past spectrum sensing outcomes. Our experimental results demonstrate that the RC-based spectrum access strategy significantly reduces the likelihood of collision with PUs and other SUs. Our approach outperforms the myopic method that assumes knowledge of system statistics and converges faster than the Q-learning method for a large number of channels.",1
"Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.",0
"Challenging control tasks have been successfully tackled using Deep Reinforcement Learning (DRL) algorithms, but these methods typically face three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are highly sensitive to hyperparameters. Consequently, these challenges limit the applicability of these approaches in real-world scenarios. Evolutionary Algorithms (EAs), which are optimization techniques inspired by natural evolution, can address these challenges, but they struggle with large parameters and high sample complexity. This paper presents Evolutionary Reinforcement Learning (ERL), which is a hybrid algorithm that uses the population of an EA to train an RL agent with diversified data and injects gradient information into the EA by periodically reinserting the RL agent into the population. ERL combines EA's temporal credit assignment, effective exploration, and stability with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments on a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.",1
"We present a reinforcement learning approach for detecting objects within an image. Our approach performs a step-wise deformation of a bounding box with the goal of tightly framing the object. It uses a hierarchical tree-like representation of predefined region candidates, which the agent can zoom in on. This reduces the number of region candidates that must be evaluated so that the agent can afford to compute new feature maps before each step to enhance detection quality. We compare an approach that is based purely on zoom actions with one that is extended by a second refinement stage to fine-tune the bounding box after each zoom step. We also improve the fitting ability by allowing for different aspect ratios of the bounding box. Finally, we propose different reward functions to lead to a better guidance of the agent while following its search trajectories. Experiments indicate that each of these extensions leads to more correct detections. The best performing approach comprises a zoom stage and a refinement stage, uses aspect-ratio modifying actions and is trained using a combination of three different reward metrics.",0
"Our method employs reinforcement learning to identify objects in images, by iteratively adjusting the bounding box to tightly enclose the object. To achieve this, we utilize a hierarchical representation of region candidates that can be selectively zoomed into by the agent. This reduces the number of regions that need to be evaluated, allowing the agent to compute new feature maps to enhance detection quality. Our study compares the performance of two approaches - one purely based on zoom actions and the other extended with a refinement stage to fine-tune the bounding box. We also introduce the ability to modify bounding box aspect ratios and propose various reward functions to guide the agent's search. Our experiments demonstrate that these extensions lead to more accurate detections. The best approach involves both zoom and refinement stages, aspect-ratio modifications, and is trained using a combination of three reward metrics.",1
"We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.",0
"Our focus is on the issue of multiset prediction, which involves creating a predictor that can map an input to a multiset of multiple items. Unlike other supervised learning problems like classification, ranking, and sequence generation, there is no established order among the items in the multiset, and each item may recur, making this a particularly difficult challenge. We propose a new multiset loss function in this paper, which is based on sequential decision making. We evaluate this function empirically on two types of datasets with varying levels of complexity, one synthetic and the other real, and compare it to reinforcement learning, sequence, and aggregated distribution matching loss functions. The results indicate that our proposed function outperforms the others.",1
"Variable speed limits (VSL) control is a flexible way to improve traffic condition,increase safety and reduce emission. There is an emerging trend of using reinforcement learning technique for VSL control and recent studies have shown promising results. Currently, deep learning is enabling reinforcement learning to develope autonomous control agents for problems that were previously intractable. In this paper, we propose a more effective deep reinforcement learning (DRL) model for differential variable speed limits (DVSL) control, in which the dynamic and different speed limits among lanes can be imposed. The proposed DRL models use a novel actor-critic architecture which can learn a large number of discrete speed limits in a continues action space. Different reward signals, e.g. total travel time, bottleneck speed, emergency braking, and vehicular emission are used to train the DVSL controller, and comparison between these reward signals are conducted. We test proposed DRL baased DVSL controllers on a simulated freeway recurrent bottleneck. Results show that the efficiency, safety and emissions can be improved by the proposed method. We also show some interesting findings through the visulization of the control policies generated from DRL models.",0
"A flexible method to enhance traffic conditions, safety, and reduce emissions is variable speed limits (VSL) control. There is a growing trend of using reinforcement learning techniques for VSL control, which have shown promising results. The use of deep learning is enabling reinforcement learning to develop autonomous control agents for previously unsolvable problems. This paper proposes a more effective deep reinforcement learning (DRL) model for differential variable speed limits (DVSL) control, where dynamic and different speed limits can be imposed among lanes. The proposed DRL model uses a novel actor-critic architecture, which can learn a large number of discrete speed limits in a continuous action space. Various reward signals, such as total travel time, bottleneck speed, emergency braking, and vehicular emissions, are used to train the DVSL controller, and comparisons are made between these reward signals. The proposed DRL-based DVSL controllers are tested on a simulated freeway recurrent bottleneck, where the efficiency, safety, and emissions are improved. Additionally, interesting findings are shown through the visualization of the control policies generated from DRL models.",1
"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial observations by using finite length observation histories or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to an advantage-like function and is robust to partially observed state. We demonstrate that this new algorithm can substantially outperform strong baseline methods on several partially observed reinforcement learning tasks: learning first-person 3D navigation in Doom and Minecraft, and acting in the presence of partially observed objects in Doom and Pong.",0
"Various challenging domains have benefitted from the effectiveness of deep reinforcement learning algorithms that evaluate state and state-action value functions, such as learning control strategies from raw image pixels. However, these algorithms typically assume a fully observable state and must compensate for partial observations through the use of finite length observation histories or recurrent networks. This study introduces a novel deep reinforcement learning algorithm that employs counterfactual regret minimization to iteratively update an approximation of an advantage-like function while remaining robust to partially observed states. Through experimentation, we demonstrate that this new algorithm surpasses strong baseline methods in numerous partially observed reinforcement learning tasks, including first-person 3D navigation in Doom and Minecraft, and acting in the presence of partially observed objects in Doom and Pong.",1
"This paper presents a new meta-modeling framework to employ deep reinforcement learning (DRL) to generate mechanical constitutive models for interfaces. The constitutive models are conceptualized as information flow in directed graphs. The process of writing constitutive models are simplified as a sequence of forming graph edges with the goal of maximizing the model score (a function of accuracy, robustness and forward prediction quality). Thus meta-modeling can be formulated as a Markov decision process with well-defined states, actions, rules, objective functions, and rewards. By using neural networks to estimate policies and state values, the computer agent is able to efficiently self-improve the constitutive model it generated through self-playing, in the same way AlphaGo Zero (the algorithm that outplayed the world champion in the game of Go)improves its gameplay. Our numerical examples show that this automated meta-modeling framework not only produces models which outperform existing cohesive models on benchmark traction-separation data but is also capable of detecting hidden mechanisms among micro-structural features and incorporating them in constitutive models to improve the forward prediction accuracy, which are difficult tasks to do manually.",0
"A novel framework for meta-modeling is introduced in this paper, which utilizes deep reinforcement learning (DRL) to generate mechanical constitutive models for interfaces. The constitutive models are represented as directed graphs, with the process of creating these models simplified as a sequence of forming edges in the graph. The objective of this process is to maximize the model score, which is determined by accuracy, robustness, and forward prediction quality. This framework can be viewed as a Markov decision process, with well-defined states, actions, rules, objective functions, and rewards. By using neural networks to estimate policies and state values, the computer agent can self-improve the generated constitutive model through self-playing, similar to AlphaGo Zero. Numerical examples demonstrate that this automated meta-modeling framework outperforms existing cohesive models on benchmark traction-separation data. Furthermore, it can identify hidden mechanisms among micro-structural features and incorporate them into constitutive models to improve the forward prediction accuracy - tasks that are difficult to do manually.",1
"Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efficiency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.",0
"Achieving superhuman performance in video game environments using deep reinforcement learning is possible, but it requires a designer to manually specify a reward function. In contrast, providing demonstrations of a target behavior is often simpler. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but applying IRL to high-dimensional video games has received little attention. Our CNN-AIRL baseline modifies the state-of-the-art adversarial IRL (AIRL) algorithm by using CNNs for the generator and discriminator to improve training stability. Additionally, we learn a low-dimensional state representation using a novel autoencoder architecture that is optimized for video game environments. This embedding is utilized as input to the reward network, improving the sample efficiency of expert demonstrations. Our approach achieves high-level performance on the Catcher video game, surpassing the CNN-AIRL baseline. However, on the Enduro Atari racing game, we score points but do not match expert performance, indicating the need for further research.",1
"Robust Reinforcement Learning aims to derive optimal behavior that accounts for model uncertainty in dynamical systems. However, previous studies have shown that by considering the worst case scenario, robust policies can be overly conservative. Our soft-robust framework is an attempt to overcome this issue. In this paper, we present a novel Soft-Robust Actor-Critic algorithm (SR-AC). It learns an optimal policy with respect to a distribution over an uncertainty set and stays robust to model uncertainty but avoids the conservativeness of robust strategies. We show the convergence of SR-AC and test the efficiency of our approach on different domains by comparing it against regular learning methods and their robust formulations.",0
"The objective of Robust Reinforcement Learning is to determine the best possible behavior that takes into account the uncertainty of models in dynamic systems. However, prior research has demonstrated that robust policies can be excessively cautious when they contemplate worst-case scenarios. To address this issue, we have developed a soft-robust framework. Our paper introduces a new Soft-Robust Actor-Critic algorithm (SR-AC) that learns an optimal policy based on a distribution of uncertainty and remains robust to model uncertainty while avoiding the overly-conservative nature of robust strategies. We demonstrate the convergence of SR-AC and compare its effectiveness against conventional learning approaches and their robust counterparts in various domains.",1
"Since the inception of Deep Reinforcement Learning (DRL) algorithms, there has been a growing interest in both research and industrial communities in the promising potentials of this paradigm. The list of current and envisioned applications of deep RL ranges from autonomous navigation and robotics to control applications in the critical infrastructure, air traffic control, defense technologies, and cybersecurity. While the landscape of opportunities and the advantages of deep RL algorithms are justifiably vast, the security risks and issues in such algorithms remain largely unexplored. To facilitate and motivate further research on these critical challenges, this paper presents a foundational treatment of the security problem in DRL. We formulate the security requirements of DRL, and provide a high-level threat model through the classification and identification of vulnerabilities, attack vectors, and adversarial capabilities. Furthermore, we present a review of current literature on security of deep RL from both offensive and defensive perspectives. Lastly, we enumerate critical research venues and open problems in mitigation and prevention of intentional attacks against deep RL as a roadmap for further research in this area.",0
"The potential of Deep Reinforcement Learning (DRL) algorithms has captured the attention of both the research and industrial communities. Applications of deep RL are numerous, ranging from autonomous navigation to air traffic control and cybersecurity. Despite the vast opportunities and advantages of deep RL algorithms, security risks and issues in such algorithms have not been explored thoroughly. This paper aims to encourage further research on these critical challenges by providing a foundational treatment of the security problem in DRL. We outline the security requirements of DRL and present a high-level threat model that identifies vulnerabilities, attack vectors, and adversarial capabilities. We also review current literature on the security of deep RL from both offensive and defensive perspectives. Finally, we enumerate critical research areas and open problems in the mitigation and prevention of intentional attacks against deep RL, providing a roadmap for further research in this field.",1
"We explore Deep Reinforcement Learning in a parameterized action space. Specifically, we investigate how to achieve sample-efficient end-to-end training in these tasks. We propose a new compact architecture for the tasks where the parameter policy is conditioned on the output of the discrete action policy. We also propose two new methods based on the state-of-the-art algorithms Trust Region Policy Optimization (TRPO) and Stochastic Value Gradient (SVG) to train such an architecture. We demonstrate that these methods outperform the state of the art method, Parameterized Action DDPG, on test domains.",0
"Our focus is on Deep Reinforcement Learning in an action space that is parameterized. We are particularly interested in discovering ways to attain efficient training from beginning to end in these tasks. To achieve this, we suggest a new, streamlined structure for tasks that rely on a policy parameter which is based on the outcome of the action policy. We also present two novel methods that build upon the latest algorithms, Trust Region Policy Optimization (TRPO) and Stochastic Value Gradient (SVG), to train this kind of architecture. Our results indicate that these approaches are more effective than the current state-of-the-art technique, Parameterized Action DDPG, in test domains.",1
"We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available. We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a $d$-dimensional state space and the discounted factor $\gamma \in (0,1)$, given an arbitrary sample path with ""covering time"" $ L $, we establish that the algorithm is guaranteed to output an $\varepsilon$-accurate estimate of the optimal Q-function using $\tilde{O}\big(L/(\varepsilon^3(1-\gamma)^7)\big)$ samples. For instance, for a well-behaved MDP, the covering time of the sample path under the purely random policy scales as $ \tilde{O}\big(1/\varepsilon^d\big),$ so the sample complexity scales as $\tilde{O}\big(1/\varepsilon^{d+3}\big).$ Indeed, we establish a lower bound that argues that the dependence of $ \tilde{\Omega}\big(1/\varepsilon^{d+2}\big)$ is necessary.",0
"We explore the use of model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel. In this scenario, only a single sample path of the system under an arbitrary policy is available. To solve this problem, we utilize the Nearest Neighbor Q-Learning (NNQL) algorithm and employ a nearest neighbor regression method to learn the optimal Q function. Our main contribution is to provide a thorough analysis of the convergence rate with a tight finite sample bound. For MDPs with a state space of dimension $d$ and a discounted factor $\gamma \in (0,1)$, we guarantee that the algorithm will produce an $\varepsilon$-accurate estimate of the optimal Q-function using $\tilde{O}\big(L/(\varepsilon^3(1-\gamma)^7)\big)$ samples, where $L$ represents the covering time of the sample path. For well-behaved MDPs, the covering time scales as $\tilde{O}\big(1/\varepsilon^d\big),$ which results in a sample complexity of $\tilde{O}\big(1/\varepsilon^{d+3}\big).$ However, we prove that the dependence of $ \tilde{\Omega}\big(1/\varepsilon^{d+2}\big)$ is necessary by establishing a lower bound.",1
"We propose a practical non-episodic PSRL algorithm that unlike recent state-of-the-art PSRL algorithms uses a deterministic, model-independent episode switching schedule. Our algorithm termed deterministic schedule PSRL (DS-PSRL) is efficient in terms of time, sample, and space complexity. We prove a Bayesian regret bound under mild assumptions. Our result is more generally applicable to multiple parameters and continuous state action problems. We compare our algorithm with state-of-the-art PSRL algorithms on standard discrete and continuous problems from the literature. Finally, we show how the assumptions of our algorithm satisfy a sensible parametrization for a large class of problems in sequential recommendations.",0
"Our proposed non-episodic PSRL algorithm, called deterministic schedule PSRL (DS-PSRL), utilizes a deterministic, model-independent episode switching schedule, which sets it apart from recent state-of-the-art PSRL algorithms. Our algorithm is efficient in terms of time, sample, and space complexity, and we provide a Bayesian regret bound under mild assumptions. Our approach is applicable to multiple parameters and continuous state action problems and we compare it with other PSRL algorithms on standard discrete and continuous problems from the literature. Furthermore, we demonstrate how the assumptions of our algorithm satisfy a sensible parametrization for a wide range of problems in sequential recommendations.",1
"Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this work, we show that the \textsc{Tree Backup} and \textsc{Retrace} algorithms are unstable with linear function approximation, both in theory and in practice with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms using a quadratic convex-concave saddle-point formulation. By exploiting the problem structure proper to these algorithms, we are able to provide convergence guarantees and finite-sample bounds. The applicability of our new analysis also goes beyond \textsc{Tree Backup} and \textsc{Retrace} and allows us to provide new convergence rates for the GTD and GTD2 algorithms without having recourse to projections or Polyak averaging.",0
"Scaling up reinforcement learning relies on off-policy learning, which involves learning about a target policy through the experience generated by a different behavior policy. However, combining off-policy learning with function approximation and multi-step bootstrapping has been problematic in terms of achieving both stability and efficiency. This study demonstrates the instability of the \textsc{Tree Backup} and \textsc{Retrace} algorithms with linear function approximation and provides a solution in the form of stable and efficient gradient-based algorithms using a quadratic convex-concave saddle-point formulation. The analysis also extends to the GTD and GTD2 algorithms, offering new convergence rates without requiring projections or Polyak averaging.",1
"Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.   Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",0
"Reinforcement learning is a field that utilizes policy gradient methods to optimize the cumulative reward and can be used with nonlinear function approximators like neural networks. However, there are two main challenges that arise: the large number of samples required and the difficulty of obtaining stable improvement due to nonstationarity. To address the first challenge, value functions are used to reduce variance while introducing bias, with an exponentially-weighted estimator of the advantage function that resembles TD(lambda). The second challenge is tackled through trust region optimization procedures for both the policy and value function, which are represented by neural networks. Our method produced strong empirical results in challenging 3D locomotion tasks, such as learning running gaits for bipedal and quadrupedal simulated robots, and developing a policy for getting a biped to stand up from lying on the ground. Unlike previous work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",1
"Transportation and traffic are currently undergoing a rapid increase in terms of both scale and complexity. At the same time, an increasing share of traffic participants are being transformed into agents driven or supported by artificial intelligence resulting in mixed-intelligence traffic. This work explores the implications of distributed decision-making in mixed-intelligence traffic. The investigations are carried out on the basis of an online-simulated highway scenario, namely the MIT \emph{DeepTraffic} simulation. In the first step traffic agents are trained by means of a deep reinforcement learning approach, being deployed inside an elitist evolutionary algorithm for hyperparameter search. The resulting architectures and training parameters are then utilized in order to either train a single autonomous traffic agent and transfer the learned weights onto a multi-agent scenario or else to conduct multi-agent learning directly. Both learning strategies are evaluated on different ratios of mixed-intelligence traffic. The strategies are assessed according to the average speed of all agents driven by artificial intelligence. Traffic patterns that provoke a reduction in traffic flow are analyzed with respect to the different strategies.",0
"The size and complexity of transportation and traffic are rapidly increasing, and more participants are being transformed into agents supported by artificial intelligence, leading to mixed-intelligence traffic. This study examines the implications of distributed decision-making in mixed-intelligence traffic, using an online-simulated highway scenario called MIT DeepTraffic. The first step is to train traffic agents using deep reinforcement learning within an elitist evolutionary algorithm for hyperparameter search. The resulting architectures and parameters are then used to train a single autonomous traffic agent or conduct multi-agent learning directly. Both strategies are evaluated on various ratios of mixed-intelligence traffic, with the average speed of AI-driven agents as the assessment criterion. Different traffic patterns that cause a reduction in traffic flow are analyzed in terms of the two strategies.",1
"We propose Ephemeral Value Adjusments (EVA): a means of allowing deep reinforcement learning agents to rapidly adapt to experience in their replay buffer. EVA shifts the value predicted by a neural network with an estimate of the value function found by planning over experience tuples from the replay buffer near the current state. EVA combines a number of recent ideas around combining episodic memory-like structures into reinforcement learning agents: slot-based storage, content-based retrieval, and memory-based planning. We show that EVAis performant on a demonstration task and Atari games.",0
"Our proposal is Ephemeral Value Adjustments (EVA), which enables deep reinforcement learning agents to swiftly adjust to their experience in the replay buffer. EVA involves altering the predicted value by a neural network with a value function estimate derived from planning over experience tuples that are close to the current state. By integrating slot-based storage, content-based retrieval, and memory-based planning, EVA combines several recent concepts on incorporating episodic memory-like structures into reinforcement learning agents. We demonstrate the effectiveness of EVA on a demonstration task and Atari games.",1
"Reinforcement learning methods carry a well known bias-variance trade-off in n-step algorithms for optimal control. Unfortunately, this has rarely been addressed in current research. This trade-off principle holds independent of the choice of the algorithm, such as n-step SARSA, n-step Expected SARSA or n-step Tree backup. A small n results in a large bias, while a large n leads to large variance. The literature offers no straightforward recipe for the best choice of this value. While currently all n-step algorithms use a fixed value of n over the state space we extend the framework of n-step updates by allowing each state to have its specific n.   We propose a solution to this problem within the context of human aided reinforcement learning. Our approach is based on the observation that a human can learn more efficiently if she receives input regarding the criticality of a given state and thus the amount of attention she needs to invest into the learning in that state. This observation is related to the idea that each state of the MDP has a certain measure of criticality which indicates how much the choice of the action in that state influences the return. In our algorithm the RL agent utilizes the criticality measure, a function provided by a human trainer, in order to locally choose the best stepnumber n for the update of the Q function.",0
"The use of reinforcement learning methods for optimal control in n-step algorithms is well-known to have a trade-off between bias and variance. Despite this, current research has not adequately addressed this issue. This principle applies regardless of the algorithm used, including n-step SARSA, n-step Expected SARSA, and n-step Tree backup. The choice of n can result in either a large bias with a small n or a large variance with a large n, and there is no clear guidance on the best value to use. While current algorithms use a fixed n value across all states, we propose a solution by allowing each state to have its own specific n value. Our approach is based on the observation that humans can learn more efficiently when they understand the criticality of a given state and how much attention they need to give to learning in that state. This criticality measure is related to how much the action in that state influences the return. Our algorithm uses this criticality measure, provided by a human trainer, to choose the best step number n for updating the Q function on a local level.",1
"Deep reinforcement learning enables algorithms to learn complex behavior, deal with continuous action spaces and find good strategies in environments with high dimensional state spaces. With deep reinforcement learning being an active area of research and many concurrent inventions, we decided to focus on a relatively simple robotic task to evaluate a set of ideas that might help to solve recent reinforcement learning problems. We test a newly created combination of two commonly used reinforcement learning methods, whether it is able to learn more effectively than a baseline. We also compare different ideas to preprocess information before it is fed to the reinforcement learning algorithm. The goal of this strategy is to reduce training time and eventually help the algorithm to converge. The concluding evaluation proves the general applicability of the described concepts by testing them using a simulated environment. These concepts might be reused for future experiments.",0
"By utilizing deep reinforcement learning, algorithms can acquire intricate behaviors, cope with continuous action spaces, and identify effective strategies in environments with high dimensional state spaces. As deep reinforcement learning currently remains a dynamic field of research with numerous concurrent inventions, we opted to concentrate on a rather uncomplicated robotic task to evaluate a range of concepts that may aid in resolving recent reinforcement learning challenges. We examined a recently developed combination of two commonly utilized reinforcement learning techniques to determine if it could learn more efficiently than a baseline. Additionally, we compared various methods for preprocessing data before feeding it into the reinforcement learning algorithm, aiming to lessen training time and ultimately promote algorithm convergence. The final assessment validates the general applicability of the aforementioned concepts, as we tested them in a simulated environment, and suggests that they may be repurposed for future experiments.",1
"How do humans navigate to target objects in novel scenes? Do we use the semantic/functional priors we have built over years to efficiently search and navigate? For example, to search for mugs, we search cabinets near the coffee machine and for fruits we try the fridge. In this work, we focus on incorporating semantic priors in the task of semantic navigation. We propose to use Graph Convolutional Networks for incorporating the prior knowledge into a deep reinforcement learning framework. The agent uses the features from the knowledge graph to predict the actions. For evaluation, we use the AI2-THOR framework. Our experiments show how semantic knowledge improves performance significantly. More importantly, we show improvement in generalization to unseen scenes and/or objects. The supplementary video can be accessed at the following link: https://youtu.be/otKjuO805dE .",0
"Is the way humans navigate to new objects in unfamiliar settings guided by the semantic and functional expectations that we have acquired over time? For instance, when searching for cups, we typically check the cupboards by the coffee maker, whereas we look in the refrigerator for fruits. Our research concentrates on integrating semantic priors in semantic navigation tasks. Our approach proposes to employ Graph Convolutional Networks in a deep reinforcement learning system to incorporate prior knowledge. The agent employs the knowledge graph's features to predict actions. We assess our technique using the AI2-THOR framework and demonstrate that incorporating semantic knowledge enhances performance significantly. Furthermore, we illustrate how our method improves generalization in previously unencountered environments and/or objects. You can watch the supplementary video at the following link: https://youtu.be/otKjuO805dE.",1
"We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",0
"There are two concerns regarding the set of algorithms based on the Adversarial Imitation Learning framework. Firstly, the reward functions used in these algorithms have an inherent bias, which can work well in certain situations but lead to suboptimal behavior in others. Secondly, although these algorithms can learn from a limited number of expert demonstrations, they necessitate an excessive number of interactions with the environment to imitate the expert for practical uses. To tackle these problems, we suggest a novel approach called Discriminator-Actor-Critic that employs off-policy Reinforcement Learning to reduce the complexity of policy-environment interaction sampling by an average of 10. Moreover, our unbiased reward function enables us to apply this algorithm to various problems without any task-specific customization.",1
"We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.",0
"An overview of deep reinforcement learning is presented, in which a detailed big picture is drawn. Contemporary and historical contexts are examined, and six core elements, six important mechanisms, and twelve applications are discussed. The background of artificial intelligence, machine learning, deep learning, and reinforcement learning is explored, along with resources. The core elements of RL, such as policy, reward, and exploration vs. exploitation, are then analyzed, as are important mechanisms like attention and memory, unsupervised learning, and relational RL. Finally, RL applications in areas like robotics, NLP, and finance are discussed, along with challenges and opportunities, before ending with an epilogue.",1
"Two of the leading approaches for model-free reinforcement learning are policy gradient methods and $Q$-learning methods. $Q$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the $Q$-values they estimate are very inaccurate. A partial explanation may be that $Q$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between $Q$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that ""soft"" (entropy-regularized) $Q$-learning is exactly equivalent to a policy gradient method. We also point out a connection between $Q$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of $Q$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a $Q$-learning method that closely matches the learning dynamics of A3C without using a target network or $\epsilon$-greedy exploration schedule.",0
"Policy gradient methods and $Q$-learning methods are two prominent model-free reinforcement learning approaches. While $Q$-learning methods can be efficient and effective, their ability to estimate $Q$-values accurately is not well understood. This may be because they implement policy gradient updates, as we demonstrate in our study of entropy-regularized reinforcement learning. We establish a precise equivalence between $Q$-learning and policy gradient methods in this setting, revealing that ""soft"" $Q$-learning is equivalent to a policy gradient method. We also identify a link between $Q$-learning and natural policy gradient methods. Our experimental investigation of entropy-regularized versions of $Q$-learning and policy gradients shows them to perform as well as or slightly better than standard variants on the Atari benchmark. We further prove the practical applicability of our findings by constructing a $Q$-learning method that replicates A3C's learning dynamics without resorting to a target network or $\epsilon$-greedy exploration schedule.",1
"Reinforcement learning methods require careful design involving a reward function to obtain the desired action policy for a given task. In the absence of hand-crafted reward functions, prior work on the topic has proposed several methods for reward estimation by using expert state trajectories and action pairs. However, there are cases where complete or good action information cannot be obtained from expert demonstrations. We propose a novel reinforcement learning method in which the agent learns an internal model of observation on the basis of expert-demonstrated state trajectories to estimate rewards without completely learning the dynamics of the external environment from state-action pairs. The internal model is obtained in the form of a predictive model for the given expert state distribution. During reinforcement learning, the agent predicts the reward as a function of the difference between the actual state and the state predicted by the internal model. We conducted multiple experiments in environments of varying complexity, including the Super Mario Bros and Flappy Bird games. We show our method successfully trains good policies directly from expert game-play videos.",0
"To achieve the desired action policy, reinforcement learning methods require careful design that includes a reward function. Some prior work in this area has suggested methods for reward estimation that use expert state trajectories and action pairs when hand-crafted reward functions are not available. However, there are situations where expert demonstrations do not provide complete or sufficient action information. In response, we propose a new reinforcement learning approach that allows the agent to learn an internal model of observation based on expert-demonstrated state trajectories. This allows for reward estimation without having to learn the external environment dynamics from state-action pairs. The internal model takes the form of a predictive model for the given expert state distribution. During reinforcement learning, the agent predicts the reward based on the difference between the actual state and the state predicted by the internal model. We conducted several experiments in different environments, including Super Mario Bros and Flappy Bird games, and our method successfully trained good policies directly from expert game-play videos.",1
"Sample efficiency is critical in solving real-world reinforcement learning problems, where agent-environment interactions can be costly. Imitation learning from expert advice has proved to be an effective strategy for reducing the number of interactions required to train a policy. Online imitation learning, which interleaves policy evaluation and policy optimization, is a particularly effective technique with provable performance guarantees. In this work, we seek to further accelerate the convergence rate of online imitation learning, thereby making it more sample efficient. We propose two model-based algorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based on solving variational inequalities and MoBIL-Prox based on stochastic first-order updates. These two methods leverage a model to predict future gradients to speed up policy learning. When the model oracle is learned online, these algorithms can provably accelerate the best known convergence rate up to an order. Our algorithms can be viewed as a generalization of stochastic Mirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style analysis of performance.",0
"When dealing with reinforcement learning problems in the real world, it's important to be efficient in order to avoid costly interactions between agents and their environment. To reduce the number of interactions needed to train a policy, imitation learning from an expert is a useful strategy. Online imitation learning, which combines policy evaluation and optimization, is a particularly effective technique with guaranteed performance. The aim of this study is to further improve the efficiency of online imitation learning by proposing two model-based algorithms inspired by Follow-the-Leader (FTL) with prediction. MoBIL-VI solves variational inequalities, while MoBIL-Prox uses stochastic first-order updates. Both methods use a model to predict future gradients and accelerate policy learning. When the model oracle is learned online, these algorithms can improve the convergence rate up to an order. Our algorithms are similar to stochastic Mirror-Prox and can be easily analyzed for performance using FTL-style analysis.",1
"Predicting movement of objects while the action of learning agent interacts with the dynamics of the scene still remains a key challenge in robotics. We propose a multi-layer Long Short Term Memory (LSTM) autoendocer network that predicts future frames for a robot navigating in a dynamic environment with moving obstacles. The autoencoder network is composed of a state and action conditioned decoder network that reconstructs the future frames of video, conditioned on the action taken by the agent. The input image frames are first transformed into low dimensional feature vectors with a pre-trained encoder network and then reconstructed with the LSTM autoencoder network to generate the future frames. A virtual environment, based on the OpenAi-Gym framework for robotics, is used to gather training data and test the proposed network. The initial experiments show promising results indicating that these predicted frames can be used by an appropriate reinforcement learning framework in future to navigate around dynamic obstacles.",0
"The challenge of forecasting the movement of objects in the presence of a learning agent that interacts with the scene dynamics remains a major obstacle in robotics. Our proposed solution involves a multi-layered Long Short-Term Memory (LSTM) autoencoder network that can predict future frames for a robot navigating through a dynamic environment with obstacles that move. This autoencoder network includes a state and action-conditioned decoder network that can reconstruct video frames of the future based on the actions taken by the agent. Initially, the input image frames are transformed into low-dimensional feature vectors using a pre-trained encoder network, which are then reconstructed using the LSTM autoencoder network to generate the future frames. We gather training data and test our proposed network using a virtual environment based on the OpenAI-Gym framework for robotics. Our initial experiments show that these predicted frames can be used by an appropriate reinforcement learning framework in the future to navigate around dynamic obstacles with promising results.",1
"E-learning systems are capable of providing more adaptive and efficient learning experiences for students than the traditional classroom setting. A key component of such systems is the learning strategy, the algorithm that designs the learning paths for students based on information such as the students' current progresses, their skills, learning materials, and etc. In this paper, we address the problem of finding the optimal learning strategy for an E-learning system. To this end, we first develop a model for students' hierarchical skills in the E-learning system. Based on the hierarchical skill model and the classical cognitive diagnosis model, we further develop a framework to model various proficiency levels of hierarchical skills. The optimal learning strategy on top of the hierarchical structure is found by applying a model-free reinforcement learning method, which does not require information on students' learning transition process. The effectiveness of the proposed framework is demonstrated via numerical experiments.",0
"E-learning systems offer students a more adaptive and efficient learning experience compared to traditional classrooms. One of the crucial components of these systems is the learning strategy, which designs the learning paths for students based on their progress, skills, learning materials, and other relevant information. This paper focuses on finding the best learning strategy for an E-learning system. To achieve this, we first introduce a model for students' hierarchical skills within the system. We then use this model, along with the classical cognitive diagnosis model, to develop a framework that models various levels of hierarchical skills proficiency. The optimal learning strategy is determined by using a model-free reinforcement learning method, which does not require information on students' learning transition process. Numerical experiments demonstrate the effectiveness of our proposed framework.",1
"Exploration is a difficult challenge in reinforcement learning and is of prime importance in sparse reward environments. However, many of the state of the art deep reinforcement learning algorithms, that rely on epsilon-greedy, fail on these environments. In such cases, empowerment can serve as an intrinsic reward signal to enable the agent to maximize the influence it has over the near future. We formulate empowerment as the channel capacity between states and actions and is calculated by estimating the mutual information between the actions and the following states. The mutual information is estimated using Mutual Information Neural Estimator and a forward dynamics model. We demonstrate that an empowerment driven agent is able to improve significantly the score of a baseline DQN agent on the game of Montezuma's Revenge.",0
"The challenge of exploration is a significant obstacle in reinforcement learning, particularly when dealing with sparse reward environments. Despite this, many advanced deep reinforcement learning algorithms that rely on epsilon-greedy techniques struggle in these settings. In such instances, empowerment can function as an intrinsic reward signal, allowing the agent to maximize its influence over the near future. Our approach defines empowerment as the channel capacity between states and actions, which is calculated by estimating the mutual information between actions and subsequent states. We use a Mutual Information Neural Estimator and a forward dynamics model to estimate mutual information. Our results demonstrate that an empowerment-based agent significantly improves upon a baseline DQN agent's scores in the game of Montezuma's Revenge.",1
"The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons: First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goal-achieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent.",0
"The geometry of a weighted graph can be represented succinctly by the smallest eigenvectors of the graph Laplacian. This approach shows promise in state representation learning for reinforcement learning (RL), where the weighted graph can be interpreted as the state transition process induced by a behavior policy in the environment. However, the existing methods for approximating the eigenvectors of the Laplacian are not suitable for general RL settings due to their computational cost and lack of justification beyond finite-state settings. This paper presents a scalable and fully general method for approximating the eigenvectors of the Laplacian in a model-free RL context. The method is evaluated systematically and shows better performance than previous proposals even in tabular, finite-state settings. Furthermore, the Laplacian representation learned using this method has potential benefits in goal-achieving RL tasks, demonstrating significant improvement in the performance of an RL agent.",1
"State representation learning aims at learning compact representations from raw observations in robotics and control applications. Approaches used for this objective are auto-encoders, learning forward models, inverse dynamics or learning using generic priors on the state characteristics. However, the diversity in applications and methods makes the field lack standard evaluation datasets, metrics and tasks. This paper provides a set of environments, data generators, robotic control tasks, metrics and tools to facilitate iterative state representation learning and evaluation in reinforcement learning settings.",0
"In robotics and control applications, the goal of state representation learning is to acquire condensed representations from unprocessed observations. Various techniques such as auto-encoders, inverse dynamics, learning forward models, and utilizing general priors on state attributes are employed for this purpose. Nonetheless, due to the broad range of practices and strategies used in this domain, there is a lack of standardized benchmarks, evaluation criteria, and assignments. This paper presents a collection of settings, data generators, metrics, and utilities to simplify the iterative process of state representation learning and evaluation in reinforcement learning contexts.",1
"We introduce a novel Deep Reinforcement Learning (DRL) algorithm called Deep Quality-Value (DQV) Learning. DQV uses temporal-difference learning to train a Value neural network and uses this network for training a second Quality-value network that learns to estimate state-action values. We first test DQV's update rules with Multilayer Perceptrons as function approximators on two classic RL problems, and then extend DQV with the use of Deep Convolutional Neural Networks, `Experience Replay' and `Target Neural Networks' for tackling four games of the Atari Arcade Learning environment. Our results show that DQV learns significantly faster and better than Deep Q-Learning and Double Deep Q-Learning, suggesting that our algorithm can potentially be a better performing synchronous temporal difference algorithm than what is currently present in DRL.",0
"The Deep Quality-Value (DQV) Learning algorithm is a new approach to Deep Reinforcement Learning (DRL). It utilizes temporal-difference learning to train a Value neural network, which is then used to train a second network that estimates state-action values. To test DQV's efficacy, we applied it to two classic RL problems using Multilayer Perceptrons as function approximators. We then extended DQV using Deep Convolutional Neural Networks, 'Experience Replay', and 'Target Neural Networks' to tackle four games of the Atari Arcade Learning environment. Our findings demonstrate that DQV outperforms Deep Q-Learning and Double Deep Q-Learning in terms of speed and accuracy, indicating that it could potentially become the leading synchronous temporal difference algorithm in DRL.",1
"Most existing deep reinforcement learning (DRL) frameworks consider either discrete action space or continuous action space solely. Motivated by applications in computer games, we consider the scenario with discrete-continuous hybrid action space. To handle hybrid action space, previous works either approximate the hybrid space by discretization, or relax it into a continuous set. In this paper, we propose a parametrized deep Q-network (P- DQN) framework for the hybrid action space without approximation or relaxation. Our algorithm combines the spirits of both DQN (dealing with discrete action space) and DDPG (dealing with continuous action space) by seamlessly integrating them. Empirical results on a simulation example, scoring a goal in simulated RoboCup soccer and the solo mode in game King of Glory (KOG) validate the efficiency and effectiveness of our method.",0
"Current deep reinforcement learning (DRL) frameworks typically focus on either a discrete action space or a continuous action space. However, our study is inspired by computer game applications and explores the potential of a scenario with a hybrid action space that is both discrete and continuous. Past research on hybrid action space has involved approximating it through discretization or relaxation into a continuous set. Our paper proposes a parametrized deep Q-network (P-DQN) framework that deals with the hybrid action space without approximation or relaxation. Our algorithm draws on the strengths of both DQN (for discrete action space) and DDPG (for continuous action space) and seamlessly integrates them. The effectiveness and efficiency of our method are validated through empirical results from a simulation example, scoring a goal in a simulated RoboCup soccer match, and the solo mode in King of Glory (KOG) game.",1
"The enactive approach to cognition is typically proposed as a viable alternative to traditional cognitive science. Enactive cognition displaces the explanatory focus from the internal representations of the agent to the direct sensorimotor interaction with its environment. In this paper, we investigate enactive learning through means of artificial agent simulations. We compare the performances of the enactive agent to an agent operating on classical reinforcement learning in foraging tasks within maze environments. The characteristics of the agents are analysed in terms of the accessibility of the environmental states, goals, and exploration/exploitation tradeoffs. We confirm that the enactive agent can successfully interact with its environment and learn to avoid unfavourable interactions using intrinsically defined goals. The performance of the enactive agent is shown to be limited by the number of affordable actions.",0
"The enactive approach to cognition is often suggested as an alternative to traditional cognitive science, as it shifts the focus away from an agent's internal representations to its direct sensorimotor interaction with the environment. This study examines enactive learning using artificial agent simulations and compares its performance to classical reinforcement learning in foraging tasks within maze environments. The agents' characteristics are evaluated based on the accessibility of environmental states, goals, and exploration/exploitation tradeoffs. The results show that the enactive agent can successfully interact with its environment and avoid unfavorable interactions using intrinsically defined goals. However, the agent's performance is limited by the number of affordable actions.",1
"Consider an assistive system that guides visually impaired users through speech and haptic feedback to their destination. Existing robotic and ubiquitous navigation technologies (e.g., portable, ground, or wearable systems) often operate in a generic, user-agnostic manner. However, to minimize confusion and navigation errors, our real-world analysis reveals a crucial need to adapt the instructional guidance across different end-users with diverse mobility skills. To address this practical issue in scalable system design, we propose a novel model-based reinforcement learning framework for personalizing the system-user interaction experience. When incrementally adapting the system to new users, we propose to use a weighted experts model for addressing data-efficiency limitations in transfer learning with deep models. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for (1) more accurate long-term human behavior prediction (up to 20 seconds into the future) through improved reasoning over personal mobility characteristics, interaction with surrounding obstacles, and the current navigation goal, and (2) quick adaptation at the onset of learning, when data is limited.",0
"An assistive system is being considered to guide visually impaired users to their destination through speech and haptic feedback. However, existing navigation technologies operate in a generic manner, which may lead to confusion and errors. To address this issue, a novel model-based reinforcement learning framework is proposed to personalize the system's interaction with different end-users with diverse mobility skills. The proposed approach uses a weighted experts model to address data-efficiency limitations in transfer learning with deep models when incrementally adapting the system to new users. A real-world dataset of navigation by blind users is used to show that the proposed approach allows for more accurate long-term human behavior prediction and quick adaptation at the onset of learning when data is limited.",1
"In statistical modelling the biggest threat is concept drift which makes the model gradually showing deteriorating performance over time. There are state of the art methodologies to detect the impact of concept drift, however general strategy considered to overcome the issue in performance is to rebuild or re-calibrate the model periodically as the variable patterns for the model changes significantly due to market change or consumer behavior change etc. Quantitative research is the most widely spread application of data science in Marketing or financial domain where applicability of state of the art reinforcement learning for auto-learning is less explored paradigm. Reinforcement learning is heavily dependent on having a simulated environment which is majorly available for gaming or online systems, to learn from the live feedback. However, there are some research happened on the area of online advertisement, pricing etc where due to the nature of the online learning environment scope of reinforcement learning is explored. Our proposed solution is a reinforcement learning based, true self-learning algorithm which can adapt to the data change or concept drift and auto learn and self-calibrate for the new patterns of the data solving the problem of concept drift.   Keywords - Reinforcement learning, Genetic Algorithm, Q-learning, Classification modelling, CMA-ES, NES, Multi objective optimization, Concept drift, Population stability index, Incremental learning, F1-measure, Predictive Modelling, Self-learning, MCTS, AlphaGo, AlphaZero",0
"The primary concern in statistical modelling is concept drift, which causes the model's performance to gradually decline over time. Although there are advanced techniques to detect the impact of concept drift, the general solution to mitigate performance issues is to periodically rebuild or re-calibrate the model when variable patterns change significantly due to market or consumer behavior changes. Quantitative research is the most popular application of data science in financial and marketing domains, where the applicability of reinforcement learning for auto-learning is less explored. Reinforcement learning relies on a simulated environment, primarily available for gaming or online systems, to learn from live feedback. However, some research has been conducted on online advertisement, pricing, and other areas where the scope of reinforcement learning is explored due to the nature of the online learning environment. Our proposed solution is a true self-learning algorithm based on reinforcement learning that can adapt to data changes or concept drift, auto-learn, and self-calibrate for new data patterns, solving the problem of concept drift. The proposed algorithm employs genetic algorithms, Q-learning, classification modelling, CMA-ES, NES, multi-objective optimization, population stability index, incremental learning, F1-measure, predictive modelling, MCTS, AlphaGo, and AlphaZero.",1
"In this paper we obtain several informative error bounds on function approximation for the policy evaluation algorithm proposed by Basu et al. when the aim is to find the risk-sensitive cost represented using exponential utility. The main idea is to use classical Bapat's inequality and to use Perron-Frobenius eigenvectors (exists if we assume irreducible Markov chain) to get the new bounds. The novelty of our approach is that we use the irreduciblity of Markov chain to get the new bounds whereas the earlier work by Basu et al. used spectral variation bound which is true for any matrix. We also give examples where all our bounds achieve the ""actual error"" whereas the earlier bound given by Basu et al. is much weaker in comparison. We show that this happens due to the absence of difference term in the earlier bound which is always present in all our bounds when the state space is large. Additionally, we discuss how all our bounds compare with each other. As a corollary of our main result we provide a bound between largest eigenvalues of two irreducibile matrices in terms of the matrix entries.",0
"This paper presents informative error bounds for function approximation in the policy evaluation algorithm proposed by Basu et al. Specifically, we aim to find the risk-sensitive cost represented using exponential utility. Our approach involves using classical Bapat's inequality and Perron-Frobenius eigenvectors, which are only available if we assume an irreducible Markov chain. This is a novel approach, as previous work by Basu et al. used spectral variation bound, which is true for any matrix. Our bounds achieve the ""actual error"" in certain examples, whereas the earlier bound given by Basu et al. is weaker. This is due to the absence of a difference term in their bound, which is present in all of ours when the state space is large. We also compare all of our bounds with each other, and as a corollary of our main result, we provide a bound between the largest eigenvalues of two irreducible matrices in terms of the matrix entries.",1
"Model-free approaches for reinforcement learning (RL) and continuous control find policies based only on past states and rewards, without fitting a model of the system dynamics. They are appealing as they are general purpose and easy to implement; however, they also come with fewer theoretical guarantees than model-based RL. In this work, we present a new model-free algorithm for controlling linear quadratic (LQ) systems, and show that its regret scales as $O(T^{\xi+2/3})$ for any small $\xi>0$ if time horizon satisfies $T>C^{1/\xi}$ for a constant $C$. The algorithm is based on a reduction of control of Markov decision processes to an expert prediction problem. In practice, it corresponds to a variant of policy iteration with forced exploration, where the policy in each phase is greedy with respect to the average of all previous value functions. This is the first model-free algorithm for adaptive control of LQ systems that provably achieves sublinear regret and has a polynomial computation cost. Empirically, our algorithm dramatically outperforms standard policy iteration, but performs worse than a model-based approach.",0
"Reinforcement learning (RL) and continuous control can be achieved using model-free approaches that rely solely on past states and rewards, without the need for a model of the system dynamics. Although these methods are desirable due to their versatility and ease of implementation, they do not offer as many theoretical guarantees as model-based RL. This study presents a novel model-free algorithm for controlling linear quadratic (LQ) systems, which demonstrates regret scaling as $O(T^{\xi+2/3})$ for any small $\xi>0$ if the time horizon satisfies $T>C^{1/\xi}$ for a constant $C$. The algorithm is based on a reduction of control of Markov decision processes to an expert prediction problem, and in practice, it corresponds to a policy iteration variant with forced exploration. The policy in each phase is greedy with respect to the average of all previous value functions. This is the first model-free algorithm for adaptive control of LQ systems that can achieve sublinear regret and has a polynomial computation cost. Our algorithm outperforms standard policy iteration in empirical testing, but is not as effective as a model-based approach.",1
"Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.",0
"The use of hierarchical reinforcement learning (HRL) is a promising method to solve more complex tasks beyond traditional reinforcement learning (RL) techniques. However, current HRL methods require task-specific design and on-policy training, making them challenging to apply in real-world situations. In this study, we explore how we can develop HRL algorithms that are both general and efficient, making them suitable for real-world problems like robotic control. Our approach involves supervising lower-level controllers with automatically learned and proposed goals from higher-level controllers, and using off-policy experience for both higher and lower-level training. This presents a significant challenge, but we introduce an off-policy correction to address it, allowing us to learn both higher- and lower-level policies using fewer environment interactions than on-policy algorithms. Our resulting HRL agent, HIRO, is generally applicable and highly sample-efficient, as demonstrated through experiments that show it can learn complex behaviors for simulated robots with only a few million samples, equivalent to a few days of real-time interaction. Comparisons with prior HRL methods reveal that our approach outperforms previous state-of-the-art techniques.",1
"Recent analyses of certain gradient descent optimization methods have shown that performance can degrade in some settings - such as with stochasticity or implicit momentum. In deep reinforcement learning (Deep RL), such optimization methods are often used for training neural networks via the temporal difference error or policy gradient. As an agent improves over time, the optimization target changes and thus the loss landscape (and local optima) change. Due to the failure modes of those methods, the ideal choice of optimizer for Deep RL remains unclear. As such, we provide an empirical analysis of the effects that a wide range of gradient descent optimizers and their hyperparameters have on policy gradient methods, a subset of Deep RL algorithms, for benchmark continuous control tasks. We find that adaptive optimizers have a narrow window of effective learning rates, diverging in other cases, and that the effectiveness of momentum varies depending on the properties of the environment. Our analysis suggests that there is significant interplay between the dynamics of the environment and Deep RL algorithm properties which aren't necessarily accounted for by traditional adaptive gradient methods. We provide suggestions for optimal settings of current methods and further lines of research based on our findings.",0
"Analyses have revealed that certain gradient descent optimization techniques may not perform well in certain scenarios, such as those involving stochasticity or implicit momentum. These methods are commonly employed in Deep RL to train neural networks through temporal difference error or policy gradient. However, the optimization target changes as the agent improves over time, leading to a shift in the loss landscape and local optima. Due to the limitations of these methods, the best optimizer for Deep RL is uncertain. To address this, we conducted an empirical analysis of how various gradient descent optimizers and their hyperparameters affect policy gradient methods, a type of Deep RL algorithm, in benchmark continuous control tasks. Our results indicate that adaptive optimizers have a limited range of effective learning rates and diverge in other cases. Additionally, the effectiveness of momentum varies depending on the environment's properties. Our findings suggest that traditional adaptive gradient methods do not fully account for the interplay between the environment's dynamics and Deep RL algorithm properties. We provide recommendations for optimal settings and potential areas for further research.",1
"Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.",0
"Recent advances in deep learning have led to the success of model-free reinforcement learning (RL) methods in many tasks. However, these methods often have high sample complexity, making them less applicable in real-world domains. On the other hand, model-based reinforcement learning can reduce sample complexity but requires careful tuning and has only been successful in restrictive domains. In this study, we investigate the behavior of model-based RL methods when deep neural networks are used to learn both the model and the policy. Our findings reveal that the learned policy exploits regions where the model has insufficient data, leading to instability during training. To address this issue, we propose the use of an ensemble of models to maintain model uncertainty and regularize the learning process. We also demonstrate that using likelihood ratio derivatives is more stable than backpropagation through time. Overall, our approach, Model-Ensemble Trust-Region Policy Optimization (ME-TRPO), significantly reduces sample complexity compared to model-free deep RL methods in challenging continuous control benchmark tasks.",1
"Designing optimal controllers continues to be challenging as systems are becoming complex and are inherently nonlinear. The principal advantage of reinforcement learning (RL) is its ability to learn from the interaction with the environment and provide optimal control strategy. In this paper, RL is explored in the context of control of the benchmark cartpole dynamical system with no prior knowledge of the dynamics. RL algorithms such as temporal-difference, policy gradient actor-critic, and value function approximation are compared in this context with the standard LQR solution. Further, we propose a novel approach to integrate RL and swing-up controllers.",0
"The task of designing controllers that are optimal remains difficult due to the increasing complexity and inherent nonlinearity of systems. The main benefit of reinforcement learning (RL) is its capacity to extract an optimal control strategy by learning from interactions with the environment. This study investigates how RL can be applied to control the cartpole dynamical system, a recognized benchmark, without prior knowledge of its dynamics. The study compares RL algorithms such as temporal-difference, policy gradient actor-critic, and value function approximation to the traditional LQR solution. Additionally, the study proposes a new method for integrating RL and swing-up controllers.",1
"In an RF-powered backscatter cognitive radio network, multiple secondary users communicate with a secondary gateway by backscattering or harvesting energy and actively transmitting their data depending on the primary channel state. To coordinate the transmission of multiple secondary transmitters, the secondary gateway needs to schedule the backscattering time, energy harvesting time, and transmission time among them. However, under the dynamics of the primary channel and the uncertainty of the energy state of the secondary transmitters, it is challenging for the gateway to find a time scheduling mechanism which maximizes the total throughput. In this paper, we propose to use the deep reinforcement learning algorithm to derive an optimal time scheduling policy for the gateway. Specifically, to deal with the problem with large state and action spaces, we adopt a Double Deep-Q Network (DDQN) that enables the gateway to learn the optimal policy. The simulation results clearly show that the proposed deep reinforcement learning algorithm outperforms non-learning schemes in terms of network throughput.",0
"A network of secondary users communicates with a secondary gateway in an RF-powered backscatter cognitive radio system. These users backscatter or harvest energy and transmit their data depending on the state of the primary channel. To manage multiple secondary transmitters, the gateway schedules their backscattering, energy harvesting, and transmission times. However, the primary channel's dynamics and uncertainty in the secondary transmitter's energy state make it difficult to maximize throughput with an optimal time scheduling mechanism. This paper proposes using the deep reinforcement learning algorithm to derive an optimal policy for the gateway, using a Double Deep-Q Network (DDQN) to handle large state and action spaces. Simulation results show that this algorithm yields higher network throughput than non-learning schemes.",1
"Many recent algorithms for reinforcement learning are model-free and founded on the Bellman equation. Here we present a method founded on the costate equation and models of the state dynamics. We use the costate -- the gradient of cost with respect to state -- to improve the policy and also to ""focus"" the model, training it to detect and mimic those features of the environment that are most relevant to its task. We show that this method can handle difficult time-optimal control problems, driving deterministic or stochastic mechanical systems quickly to a target. On these tasks it works well compared to deep deterministic policy gradient, a recent Bellman method. And because it creates a model, the costate method can also learn from mental practice.",0
"Several recent reinforcement learning algorithms rely on the Bellman equation and are model-free. However, our approach is based on the costate equation and involves models of state dynamics. By utilizing the costate - the gradient of cost concerning state - we can enhance the policy and direct the model to focus on the most crucial environmental features for the task at hand. Our method can handle challenging time-optimal control problems, efficiently steering deterministic or stochastic mechanical systems towards a set target. Compared to a recent Bellman approach called deep deterministic policy gradient, our method performs equally well. Additionally, since our approach creates a model, it can also learn through mental practice.",1
"Reinforcement learning has shown great potential in generalizing over raw sensory data using only a single neural network for value optimization. There are several challenges in the current state-of-the-art reinforcement learning algorithms that prevent them from converging towards the global optima. It is likely that the solution to these problems lies in short- and long-term planning, exploration and memory management for reinforcement learning algorithms. Games are often used to benchmark reinforcement learning algorithms as they provide a flexible, reproducible, and easy to control environment. Regardless, few games feature a state-space where results in exploration, memory, and planning are easily perceived. This paper presents The Dreaming Variational Autoencoder (DVAE), a neural network based generative modeling architecture for exploration in environments with sparse feedback. We further present Deep Maze, a novel and flexible maze engine that challenges DVAE in partial and fully-observable state-spaces, long-horizon tasks, and deterministic and stochastic problems. We show initial findings and encourage further work in reinforcement learning driven by generative exploration.",0
"Using a single neural network for value optimization, reinforcement learning has demonstrated significant potential in generalizing over raw sensory data. However, there are various challenges in the current state-of-the-art reinforcement learning algorithms that hinder them from reaching the global optima. Addressing these issues may involve short- and long-term planning, memory management, and exploration for reinforcement learning algorithms. Although games are commonly used to benchmark reinforcement learning algorithms due to their flexible, reproducible, and easy-to-control nature, few games feature a state-space that enables easy perception of exploration, memory, and planning outcomes. This paper introduces The Dreaming Variational Autoencoder (DVAE), a generative modeling architecture for exploration in environments with sparse feedback, and Deep Maze, a flexible maze engine that challenges DVAE in partial and fully-observable state-spaces, long-horizon tasks, and deterministic and stochastic problems. The paper presents initial findings and encourages further research in reinforcement learning driven by generative exploration.",1
"We study a reinforcement learning setting, where the state transition function is a convex combination of a stochastic continuous function and a deterministic function. Such a setting generalizes the widely-studied stochastic state transition setting, namely the setting of deterministic policy gradient (DPG).   We firstly give a simple example to illustrate that the deterministic policy gradient may be infinite under deterministic state transitions, and introduce a theoretical technique to prove the existence of the policy gradient in this generalized setting. Using this technique, we prove that the deterministic policy gradient indeed exists for a certain set of discount factors, and further prove two conditions that guarantee the existence for all discount factors. We then derive a closed form of the policy gradient whenever exists. Furthermore, to overcome the challenge of high sample complexity of DPG in this setting, we propose the Generalized Deterministic Policy Gradient (GDPG) algorithm. The main innovation of the algorithm is a new method of applying model-based techniques to the model-free algorithm, the deep deterministic policy gradient algorithm (DDPG). GDPG optimize the long-term rewards of the model-based augmented MDP subject to a constraint that the long-rewards of the MDP is less than the original one.   We finally conduct extensive experiments comparing GDPG with state-of-the-art methods and the direct model-based extension method of DDPG on several standard continuous control benchmarks. Results demonstrate that GDPG substantially outperforms DDPG, the model-based extension of DDPG and other baselines in terms of both convergence and long-term rewards in most environments.",0
"This paper investigates a reinforcement learning scenario where the state transition function is a combination of a stochastic continuous function and a deterministic function, thus generalizing the commonly studied stochastic state transition setting (i.e., deterministic policy gradient or DPG). Initially, we provide a simple example illustrating the infinite nature of DPG under deterministic state transitions, and introduce a theoretical technique to prove the existence of the policy gradient in this extended setting. By applying this technique, we prove that the deterministic policy gradient exists for a specific range of discount factors and establish two conditions that ensure its existence for all discount factors. We also derive a closed-form policy gradient if it exists. However, to address the high sample complexity of DPG in this setting, we propose a new approach called Generalized Deterministic Policy Gradient (GDPG) algorithm that applies model-based techniques to the model-free deep deterministic policy gradient algorithm (DDPG). GDPG optimizes the long-term rewards of the model-based augmented MDP with a constraint that the long-rewards of the MDP are less than the original one. Finally, we perform extensive experiments comparing GDPG with state-of-the-art methods and the direct model-based extension of DDPG on various continuous control benchmarks. Results show that GDPG outperforms DDPG, its model-based extension, and other baselines in terms of both convergence and long-term rewards in most environments.",1
"Reinforcement learning and planning methods require an objective or reward function that encodes the desired behavior. Yet, in practice, there is a wide range of scenarios where an objective is difficult to provide programmatically, such as tasks with visual observations involving unknown object positions or deformable objects. In these cases, prior methods use engineered problem-specific solutions, e.g., by instrumenting the environment with additional sensors to measure a proxy for the objective. Such solutions require a significant engineering effort on a per-task basis, and make it impractical for robots to continuously learn complex skills outside of laboratory settings. We aim to find a more general and scalable solution for specifying goals for robot learning in unconstrained environments. To that end, we formulate the few-shot objective learning problem, where the goal is to learn a task objective from only a few example images of successful end states for that task. We propose a simple solution to this problem: meta-learn a classifier that can recognize new goals from a few examples. We show how this approach can be used with both model-free reinforcement learning and visual model-based planning and show results in three domains: rope manipulation from images in simulation, visual navigation in a simulated 3D environment, and object arrangement into user-specified configurations on a real robot.",0
"In order for reinforcement learning and planning methods to work, an objective or reward function must be established to represent the desired behavior. However, this can be challenging in scenarios where tasks involve visual observations with unknown object positions or deformable objects. Current methods involve creating problem-specific solutions, such as adding sensors to measure a proxy for the objective, but this requires a lot of effort and is not practical for robots to learn complex skills outside of a controlled environment. Our aim is to find a more scalable and general solution for teaching robots in unconstrained environments. We propose a few-shot objective learning problem, where we teach the robot to learn a task objective from a few successful end state images. We suggest a simple solution to this problem by meta-learning a classifier to recognize new goals from just a few examples. We demonstrate how this can be used with both model-free reinforcement learning and visual model-based planning, and provide results from three domains: rope manipulation in a simulated environment, visual navigation in a 3D simulation, and object arrangement on a real robot.",1
"We propose a probabilistic framework to directly insert prior knowledge in reinforcement learning (RL) algorithms by defining the behaviour policy as a Bayesian posterior distribution. Such a posterior combines task specific information with prior knowledge, thus allowing to achieve transfer learning across tasks. The resulting method is flexible and it can be easily incorporated to any standard off-policy and on-policy algorithms, such as those based on temporal differences and policy gradients. We develop a specific instance of this Bayesian transfer RL framework by expressing prior knowledge as general deterministic rules that can be useful in a large variety of tasks, such as navigation tasks. Also, we elaborate more on recent probabilistic and entropy-regularised RL by developing a novel temporal learning algorithm and show how to combine it with Bayesian transfer RL. Finally, we demonstrate our method for solving mazes and show that significant speed ups can be obtained.",0
"Our proposal involves a probabilistic framework that integrates previous knowledge into reinforcement learning (RL) algorithms. We achieve this by defining the behavior policy as a Bayesian posterior distribution, which merges task-specific information with prior knowledge. This approach allows for transfer learning across tasks, making it flexible enough to be incorporated into any standard off-policy or on-policy algorithms. We have created a specific instance of this Bayesian transfer RL framework, where prior knowledge is expressed as general deterministic rules that can be used in various tasks, including navigation tasks. Additionally, we have expanded on recent probabilistic and entropy-regularized RL by developing a new temporal learning algorithm and demonstrating how it can be combined with Bayesian transfer RL. Finally, we showcase the effectiveness of our method in solving mazes and achieving significant speed improvements.",1
"Deep Reinforcement Learning (DRL) has become a powerful strategy to solve complex decision making problems based on Deep Neural Networks (DNNs). However, it is highly data demanding, so unfeasible in physical systems for most applications. In this work, we approach an alternative Interactive Machine Learning (IML) strategy for training DNN policies based on human corrective feedback, with a method called Deep COACH (D-COACH). This approach not only takes advantage of the knowledge and insights of human teachers as well as the power of DNNs, but also has no need of a reward function (which sometimes implies the need of external perception for computing rewards). We combine Deep Learning with the COrrective Advice Communicated by Humans (COACH) framework, in which non-expert humans shape policies by correcting the agent's actions during execution. The D-COACH framework has the potential to solve complex problems without much data or time required. Experimental results validated the efficiency of the framework in three different problems (two simulated, one with a real robot), with state spaces of low and high dimensions, showing the capacity to successfully learn policies for continuous action spaces like in the Car Racing and Cart-Pole problems faster than with DRL.",0
"Utilizing Deep Neural Networks (DNNs), Deep Reinforcement Learning (DRL) has emerged as an effective approach to resolving intricate decision making dilemmas. However, DRL is data-intensive, rendering it impractical for most physical systems. In this study, we propose an Interactive Machine Learning (IML) strategy termed Deep COACH (D-COACH) that leverages human corrective feedback to train DNN policies. This approach capitalizes on both the DNNs' capabilities and the knowledge and insights of human teachers. D-COACH does not require a reward function, which often necessitates external perception for computing rewards. We merge Deep Learning with the COrrective Advice Communicated by Humans (COACH) framework, enabling non-expert humans to correct the agent's actions during execution and shape policies. The D-COACH framework has the potential to solve intricate problems with minimal data or time requirements. In three different problems, including two simulated and one with a real robot, with low and high-dimensional state spaces, the experimental outcomes demonstrated the framework's efficacy, showing faster policy learning in continuous action spaces such as in the Car Racing and Cart-Pole problems, outperforming DRL.",1
"Learning in sparse reward settings remains a challenge in Reinforcement Learning, which is often addressed by using intrinsic rewards. One promising strategy is inspired by human curiosity, requiring the agent to learn to predict the future. In this paper a curiosity-driven agent is extended to use these predictions directly for training. To achieve this, the agent predicts the value function of the next state at any point in time. Subsequently, the consistency of this prediction with the current value function is measured, which is then used as a regularization term in the loss function of the algorithm. Experiments were made on grid-world environments as well as on a 3D navigation task, both with sparse rewards. In the first case the extended agent is able to learn significantly faster than the baselines.",0
"Reinforcement Learning faces a challenge when learning in sparse reward settings, which can be overcome by incorporating intrinsic rewards. A promising approach inspired by human curiosity involves the agent learning to predict the future. This study extends a curiosity-driven agent to incorporate these predictions directly into its training. The agent predicts the value function of the next state at any given time and measures its consistency with the current value function to use as a regularization term in the loss function. Experiments were conducted on grid-world environments and a 3D navigation task, both with sparse rewards. The extended agent showed significantly faster learning than the baselines in the grid-world environment.",1
"Reinforcement learning refers to a group of methods from artificial intelligence where an agent performs learning through trial and error. It differs from supervised learning, since reinforcement learning requires no explicit labels; instead, the agent interacts continuously with its environment. That is, the agent starts in a specific state and then performs an action, based on which it transitions to a new state and, depending on the outcome, receives a reward. Different strategies (e.g. Q-learning) have been proposed to maximize the overall reward, resulting in a so-called policy, which defines the best possible action in each state. Mathematically, this process can be formalized by a Markov decision process and it has been implemented by packages in R; however, there is currently no package available for reinforcement learning. As a remedy, this paper demonstrates how to perform reinforcement learning in R and, for this purpose, introduces the ReinforcementLearning package. The package provides a remarkably flexible framework and is easily applied to a wide range of different problems. We demonstrate its use by drawing upon common examples from the literature (e.g. finding optimal game strategies).",0
"Reinforcement learning is a type of artificial intelligence that utilizes trial and error to facilitate learning. Unlike supervised learning, the agent does not require explicit labels but rather interacts continuously with the environment. Through this process, the agent starts in a specific state, takes an action, transitions to a new state, and depending on the outcome, receives a reward. To maximize the reward, different strategies such as Q-learning have been proposed, which results in a policy that identifies the best possible action in each state. This process can be formalized by a Markov decision process and implemented in R using packages. However, there is currently no package available for reinforcement learning. To address this, the ReinforcementLearning package was introduced, providing a flexible framework for various problems. This paper demonstrates the use of the package through examples from the literature, such as finding optimal game strategies.",1
"Building deep reinforcement learning agents that can generalize and adapt to unseen environments remains a fundamental challenge for AI. This paper describes progresses on this challenge in the context of man-made environments, which are visually diverse but contain intrinsic semantic regularities. We propose a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target for the sub-policy to execute, and updates the semantic model based on new observations. We perform experiments in visual navigation tasks using House3D, a 3D environment that contains diverse human-designed indoor scenes with real-world objects. LEAPS outperforms strong baselines that do not explicitly plan using the semantic content.",0
"The challenge of creating deep reinforcement learning agents that can adapt and generalize to unfamiliar environments remains a significant obstacle for AI. This research paper outlines advancements made in addressing this challenge in the context of man-made environments, which feature diverse visual elements but also have regular semantic structures. Our proposed approach, called LEArning and Planning with Semantics (LEAPS), combines both model-based and model-free strategies. LEAPS includes a multi-target sub-policy that operates on visual inputs, as well as a Bayesian model that focuses on semantic structures. When placed in an unfamiliar environment, the agent uses the semantic model to make informed decisions, suggests the next sub-target for the sub-policy to execute, and updates the semantic model based on new observations. To test our approach, we conducted experiments in visual navigation tasks using House3D, a 3D environment that features human-designed indoor scenes with real-world objects. Our results show that LEAPS outperforms other strong baselines that do not explicitly factor in semantic content during planning.",1
"This paper proposes an exploration method for deep reinforcement learning based on parameter space noise. Recent studies have experimentally shown that parameter space noise results in better exploration than the commonly used action space noise. Previous methods devised a way to update the diagonal covariance matrix of a noise distribution and did not consider the direction of the noise vector and its correlation. In addition, fast updates of the noise distribution are required to facilitate policy learning. We propose a method that deforms the noise distribution according to the accumulated returns and the noises that have led to the returns. Moreover, this method switches isotropic exploration and directional exploration in parameter space with regard to obtained rewards. We validate our exploration strategy in the OpenAI Gym continuous environments and modified environments with sparse rewards. The proposed method achieves results that are competitive with a previous method at baseline tasks. Moreover, our approach exhibits better performance in sparse reward environments by exploration with the switching strategy.",0
"A technique for deep reinforcement learning exploration utilizing parameter space noise is introduced in this paper. Research has indicated that parameter space noise leads to better exploration than action space noise. However, previous approaches neglected the direction and correlation of the noise vector and only focused on updating the diagonal covariance matrix of a noise distribution. Additionally, rapid updates of the noise distribution are essential for policy learning. To address these issues, we propose a method that alters the noise distribution based on the accumulated returns and the noises that led to the returns. Our method also switches between isotropic and directional exploration in parameter space based on achieved rewards. We demonstrate the effectiveness of our exploration strategy in continuous environments of OpenAI Gym and modified sparse reward environments. The proposed method achieves comparable results to a prior approach in standard tasks and outperforms it in sparse reward environments via the switching strategy.",1
"Modern vision-based reinforcement learning techniques often use convolutional neural networks (CNN) as universal function approximators to choose which action to take for a given visual input. Until recently, CNNs have been treated like black-box functions, but this mindset is especially dangerous when used for control in safety-critical settings. In this paper, we present our extensions of CNN visualization algorithms to the domain of vision-based reinforcement learning. We use a simulated drone environment as an example scenario. These visualization algorithms are an important tool for behavior introspection and provide insight into the qualities and flaws of trained policies when interacting with the physical world. A video may be seen at https://sites.google.com/view/drlvisual .",0
"To select an action based on visual input, contemporary reinforcement learning methods rely on convolutional neural networks (CNNs) as universal function approximators. However, CNNs were previously viewed as black-box functions, a potentially hazardous approach for safety-critical control scenarios. Our study focuses on extending CNN visualization algorithms for vision-based reinforcement learning in a simulated drone environment. These visualization algorithms allow for introspection of behavior and highlight the strengths and weaknesses of trained policies in real-world interactions. A video demonstration is available at https://sites.google.com/view/drlvisual.",1
"Anderson acceleration is an old and simple method for accelerating the computation of a fixed point. However, as far as we know and quite surprisingly, it has never been applied to dynamic programming or reinforcement learning. In this paper, we explain briefly what Anderson acceleration is and how it can be applied to value iteration, this being supported by preliminary experiments showing a significant speed up of convergence, that we critically discuss. We also discuss how this idea could be applied more generally to (deep) reinforcement learning.",0
"Although Anderson acceleration has been a long-standing and straightforward approach to speed up computing a fixed point, it has never been utilized in dynamic programming or reinforcement learning, which is quite unexpected. This paper gives a brief overview of Anderson acceleration and demonstrates how it can be implemented in value iteration. The effectiveness of this approach is supported by initial experiments that show a considerable improvement in convergence speed, which we analyze critically. We also explore the potential application of this method in (deep) reinforcement learning.",1
"Low precision networks in the reinforcement learning (RL) setting are relatively unexplored because of the limitations of binary activations for function approximation. Here, in the discrete action ATARI domain, we demonstrate, for the first time, that low precision policy distillation from a high precision network provides a principled, practical way to train an RL agent. As an application, on 10 different ATARI games, we demonstrate real-time end-to-end game playing on low-power neuromorphic hardware by converting a sequence of game frames into discrete actions.",0
"Due to the constraints of binary activations in function approximation, there has been little investigation into low precision networks for reinforcement learning (RL). However, we have achieved a breakthrough in the discrete action ATARI domain by showing that low precision policy distillation derived from a high precision network is a viable and systematic method for training an RL agent. Our demonstration involves real-time gameplay on low-power neuromorphic hardware, where we convert game frames into discrete actions across 10 different ATARI games.",1
"In the real world, agents often have to operate in situations with incomplete information, limited sensing capabilities, and inherently stochastic environments, making individual observations incomplete and unreliable. Moreover, in many situations it is preferable to delay a decision rather than run the risk of making a bad decision. In such situations it is necessary to aggregate information before taking an action; however, most state of the art reinforcement learning (RL) algorithms are biased towards taking actions \textit{at every time step}, even if the agent is not particularly confident in its chosen action. This lack of caution can lead the agent to make critical mistakes, regardless of prior experience and acclimation to the environment. Motivated by theories of dynamic resolution of uncertainty during decision making in biological brains, we propose a simple accumulator module which accumulates evidence in favor of each possible decision, encodes uncertainty as a dynamic competition between actions, and acts on the environment only when it is sufficiently confident in the chosen action. The agent makes no decision by default, and the burden of proof to make a decision falls on the policy to accrue evidence strongly in favor of a single decision. Our results show that this accumulator module achieves near-optimal performance on a simple guessing game, far outperforming deep recurrent networks using traditional, forced action selection policies.",0
"In situations where agents operate with incomplete information, limited sensing capabilities, and stochastic environments, individual observations can be unreliable. Sometimes, delaying a decision is preferred to making a bad decision. However, most state of the art reinforcement learning algorithms tend to bias towards taking actions at every time step, even if the agent lacks confidence in its chosen action. This recklessness can lead to critical mistakes, despite prior experience and acclimation to the environment. Inspired by theories of dynamic resolution of uncertainty in biological brains, we propose an accumulator module that accumulates evidence for each possible decision, encodes uncertainty as a dynamic competition between actions, and acts on the environment only when sufficiently confident in the chosen action. The agent does not make decisions by default, and the policy must accrue evidence strongly in favor of a single decision. Our results show that this accumulator module performs near-optimally on a simple guessing game, outperforming deep recurrent networks with traditional forced action selection policies.",1
"Epistasis (gene-gene interaction) is crucial to predicting genetic disease. Our work tackles the computational challenges faced by previous works in epistasis detection by modeling it as a one-step Markov Decision Process where the state is genome data, the actions are the interacted genes, and the reward is an interaction measurement for the selected actions. A reinforcement learning agent using policy gradient method then learns to discover a set of highly interacted genes.",0
"The importance of gene-gene interaction, or epistasis, in the prediction of genetic disease is significant. Our research addresses the computational hurdles previously encountered in detecting epistasis by formulating it as a one-step Markov Decision Process. In this process, the genome data represents the state, the interacted genes serve as the actions, and the reward is the measurement of the interaction for the chosen actions. To identify a group of genes with high interaction, our approach involves a reinforcement learning agent that employs the policy gradient method.",1
"Modern information technology services largely depend on cloud infrastructures to provide their services. These cloud infrastructures are built on top of datacenter networks (DCNs) constructed with high-speed links, fast switching gear, and redundancy to offer better flexibility and resiliency. In this environment, network traffic includes long-lived (elephant) and short-lived (mice) flows with partitioned and aggregated traffic patterns. Although SDN-based approaches can efficiently allocate networking resources for such flows, the overhead due to network reconfiguration can be significant. With limited capacity of Ternary Content-Addressable Memory (TCAM) deployed in an OpenFlow enabled switch, it is crucial to determine which forwarding rules should remain in the flow table, and which rules should be processed by the SDN controller in case of a table-miss on the SDN switch. This is needed in order to obtain the flow entries that satisfy the goal of reducing the long-term control plane overhead introduced between the controller and the switches. To achieve this goal, we propose a machine learning technique that utilizes two variations of reinforcement learning (RL) algorithms-the first of which is traditional reinforcement learning algorithm based while the other is deep reinforcement learning based. Emulation results using the RL algorithm show around 60% improvement in reducing the long-term control plane overhead, and around 14% improvement in the table-hit ratio compared to the Multiple Bloom Filters (MBF) method given a fixed size flow table of 4KB.",0
"Cloud infrastructures are integral to modern information technology services and rely on datacenter networks (DCNs) with high-speed links and redundancy for flexibility and resiliency. Traffic on these networks includes both long-lived and short-lived flows, with a mix of partitioned and aggregated traffic patterns. While SDN-based approaches can effectively allocate networking resources for these flows, the overhead of network reconfiguration can be significant. Therefore, it is crucial to determine which forwarding rules should remain in the flow table and which should be processed by the SDN controller. To minimize control plane overhead, we propose a machine learning technique utilizing two variations of reinforcement learning algorithms. The results show a 60% improvement in reducing long-term control plane overhead and a 14% improvement in table-hit ratio compared to the Multiple Bloom Filters method when using a fixed-size flow table of 4KB.",1
"Autonomous cyber-physical agents and systems play an increasingly large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. These constraints and norms can come from any number of sources including regulations, business process guidelines, laws, ethical principles, social norms, and moral values. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations of the task, and reinforcement learning to learn to maximize the environment rewards. More precisely, we assume that an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using a Pac-Man domain and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.",0
"Our lives are increasingly impacted by autonomous cyber-physical agents and systems. To ensure that these agents act in accordance with societal values, it is necessary to develop techniques that allow them to learn and follow implicit constraints, in addition to maximizing their rewards in a given environment. These constraints may arise from various sources such as ethical principles, social norms, regulations, laws, and business process guidelines. We propose a new method that involves using inverse reinforcement learning to learn unspecified constraints from task demonstrations, and reinforcement learning to maximize environmental rewards. The agent observes behavior traces of society members, but has no explicit access to the set of constraints that govern the behavior. The learned constraints are combined with a value function through a contextual bandit-based orchestrator that selects the most suitable choice between the two policies (constraint-based and environment reward-based) when taking actions. This orchestrator allows the agent to combine policies in innovative ways, selecting the best actions from either a reward-maximizing or constrained policy. Additionally, it is clear which policy is being employed at each time step. We demonstrate the effectiveness of our approach by testing it on a Pac-Man domain, showing that the agent can learn to act optimally while adhering to the demonstrated constraints, and mixing these two functions in complex ways.",1
"In this paper, we investigate a new form of automated curriculum learning based on adaptive selection of accuracy requirements, called accuracy-based curriculum learning. Using a reinforcement learning agent based on the Deep Deterministic Policy Gradient algorithm and addressing the Reacher environment, we first show that an agent trained with various accuracy requirements sampled randomly learns more efficiently than when asked to be very accurate at all times. Then we show that adaptive selection of accuracy requirements, based on a local measure of competence progress, automatically generates a curriculum where difficulty progressively increases, resulting in a better learning efficiency than sampling randomly.",0
"The focus of our study is accuracy-based curriculum learning, a novel form of automated curriculum learning that involves selecting accuracy requirements adaptively. Our research employs a reinforcement learning agent that follows the Deep Deterministic Policy Gradient algorithm and deals with the Reacher environment. Our findings reveal that randomly sampled accuracy requirements facilitate more efficient learning than always striving for high accuracy. We also demonstrate that the selection of accuracy requirements can be adjusted automatically, based on a local measure of competence progress, to create a curriculum in which the level of difficulty escalates progressively. This approach produces better learning efficiency than random sampling.",1
"We consider the problem of reinforcement learning under safety requirements, in which an agent is trained to complete a given task, typically formalized as the maximization of a reward signal over time, while concurrently avoiding undesirable actions or states, associated to lower rewards, or penalties. The construction and balancing of different reward components can be difficult in the presence of multiple objectives, yet is crucial for producing a satisfying policy. For example, in reaching a target while avoiding obstacles, low collision penalties can lead to reckless movements while high penalties can discourage exploration. To circumvent this limitation, we examine the effect of past actions in terms of safety to estimate which are acceptable or should be avoided in the future. We then actively reshape the action space of the agent during reinforcement learning, so that reward-driven exploration is constrained within safety limits. We propose an algorithm enabling the learning of such safety constraints in parallel with reinforcement learning and demonstrate its effectiveness in terms of both task completion and training time.",0
"The problem of reinforcement learning with safety requirements is our focus, where an agent is trained to complete a given task while avoiding undesirable actions or states. The objective is typically to maximize a reward signal over time, but the construction and balancing of different reward components can be challenging with multiple objectives. For example, low collision penalties can lead to reckless movements, while high penalties can discourage exploration. We propose an algorithm that examines the safety of past actions and actively reshapes the action space of the agent during reinforcement learning. This approach constrains reward-driven exploration within safety limits, allowing for effective task completion and reduced training time.",1
"In reinforcement learning (RL) , one of the key components is policy evaluation, which aims to estimate the value function (i.e., expected long-term accumulated reward) of a policy. With a good policy evaluation method, the RL algorithms will estimate the value function more accurately and find a better policy. When the state space is large or continuous \emph{Gradient-based Temporal Difference(GTD)} policy evaluation algorithms with linear function approximation are widely used. Considering that the collection of the evaluation data is both time and reward consuming, a clear understanding of the finite sample performance of the policy evaluation algorithms is very important to reinforcement learning. Under the assumption that data are i.i.d. generated, previous work provided the finite sample analysis of the GTD algorithms with constant step size by converting them into convex-concave saddle point problems. However, it is well-known that, the data are generated from Markov processes rather than i.i.d. in RL problems.. In this paper, in the realistic Markov setting, we derive the finite sample bounds for the general convex-concave saddle point problems, and hence for the GTD algorithms. We have the following discussions based on our bounds. (1) With variants of step size, GTD algorithms converge. (2) The convergence rate is determined by the step size, with the mixing time of the Markov process as the coefficient. The faster the Markov processes mix, the faster the convergence. (3) We explain that the experience replay trick is effective by improving the mixing property of the Markov process. To the best of our knowledge, our analysis is the first to provide finite sample bounds for the GTD algorithms in Markov setting.",0
"Policy evaluation is a crucial element in reinforcement learning (RL), as it aims to estimate the expected long-term accumulated reward of a policy. The accuracy of the value function estimation can impact the effectiveness of the RL algorithms and the policy identified. When the state space is vast or continuous, Gradient-based Temporal Difference (GTD) algorithms using linear function approximation are commonly utilized. However, given the time and reward costs associated with data collection, understanding the finite sample performance of policy evaluation algorithms is critical. Prior studies have provided finite sample analysis of GTD algorithms with constant step size on the assumption of i.i.d. generated data, converted to convex-concave saddle point problems. Nonetheless, data in RL problems are typically generated by Markov processes. This paper introduces finite sample bounds for the general convex-concave saddle point problems, and consequently, for GTD algorithms in a realistic Markov setting. The results indicate that GTD algorithms converge with varying step sizes, and the convergence rate is determined by the step size and mixing time of the Markov process. Furthermore, the study demonstrates the effectiveness of the experience replay trick in enhancing the mixing property of the Markov process and its impact on GTD algorithm convergence. This analysis is the first to provide finite sample bounds for GTD algorithms in a Markov setting.",1
"Q-learning is one of the most popular methods in Reinforcement Learning (RL). Transfer Learning aims to utilize the learned knowledge from source tasks to help new tasks to improve the sample complexity of the new tasks. Considering that data collection in RL is both more time and cost consuming and Q-learning converges slowly comparing to supervised learning, different kinds of transfer RL algorithms are designed. However, most of them are heuristic with no theoretical guarantee of the convergence rate. Therefore, it is important for us to clearly understand when and how will transfer learning help RL method and provide the theoretical guarantee for the improvement of the sample complexity. In this paper, we propose to transfer the Q-function learned in the source task to the target of the Q-learning in the new task when certain safe conditions are satisfied. We call this new transfer Q-learning method target transfer Q-Learning. The safe conditions are necessary to avoid the harm to the new tasks and thus ensure the convergence of the algorithm. We study the convergence rate of the target transfer Q-learning. We prove that if the two tasks are similar with respect to the MDPs, the optimal Q-functions in the source and new RL tasks are similar which means the error of the transferred target Q-function in new MDP is small. Also, the convergence rate analysis shows that the target transfer Q-Learning will converge faster than Q-learning if the error of the transferred target Q-function is smaller than the current Q-function in the new task. Based on our theoretical results, we design the safe condition as the Bellman error of the transferred target Q-function is less than the current Q-function. Our experiments are consistent with our theoretical founding and verified the effectiveness of our proposed target transfer Q-learning method.",0
"Reinforcement Learning (RL) commonly employs Q-learning, while Transfer Learning utilizes previously learned knowledge from source tasks to improve new task sample complexity. This is particularly relevant in RL, where data collection is expensive and time-consuming, and Q-learning convergence is slow relative to supervised learning. Although various transfer RL algorithms exist, most are heuristic with no guaranteed convergence rate. Thus, it is crucial to understand when and how Transfer Learning can enhance RL, and provide theoretical guarantees for sample complexity improvement. This paper proposes a new Transfer Q-learning method, called target transfer Q-Learning, which transfers the Q-function learned from a source task to the target of the Q-learning in the new task under safe conditions. The safe conditions prevent harm to the new tasks and ensure algorithm convergence. The convergence rate analysis indicates that target transfer Q-Learning will converge faster than Q-learning if the transferred target Q-function error is smaller than the current Q-function in the new task. The theory-based safe condition is that the Bellman error of the transferred target Q-function is less than the current Q-function. Empirical results confirm the effectiveness of the proposed method.",1
"Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.",0
"Model-free reinforcement learning has proven to be a promising solution for continuous control robotic tasks, thanks to recent simulation successes. Open source implementations of learning algorithms and simulated benchmark tasks have allowed for reproducibility and rapid analysis of these results. However, to apply these successes to real-world applications, it is important to experiment with physical robots and take advantage of simulation-specific advantages. Unfortunately, reinforcement learning research with physical robots faces resistance due to the lack of benchmark tasks and source code. To address this, we introduce several reinforcement learning tasks using commercially available robots with varying levels of difficulty, setup, and repeatability. We test off-the-shelf implementations of four reinforcement learning algorithms on these tasks and analyze their sensitivity to hyper-parameters to determine their applicability to real-world tasks. Our results show that careful task interface and computation setups can make some implementations applicable to physical robots. We also found that state-of-the-art learning algorithms are sensitive to their hyper-parameters and need to be re-tuned for each task for optimal performance. However, the best hyper-parameter configuration from one task may work well for held-out tasks with different robots. We make these benchmark tasks publicly available to improve reproducibility in real-world reinforcement learning.",1
"In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG. We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments. Supplementary videos and discussion can be found at http://frama.link/gep_pg, the code at http://github.com/flowersteam/geppg.",0
"The efficiency of standard deep reinforcement learning algorithms such as DDPG is compromised when dealing with sparse or deceptive reward problems in continuous action domains. In contrast, evolutionary and developmental methods that prioritize exploration, such as Quality-Diversity, Goal Exploration Processes, and Novelty Search, are effective in exploring more robustly, but less efficient in fine-tuning policies through gradient descent. This paper introduces the GEP-PG approach, which combines the benefits of both methods by sequentially integrating a Goal Exploration Process and two DDPG variants. The study examines the learning performance of these components and their combination on a deceptive reward problem with low dimensions and the larger Half-Cheetah benchmark. The results show that DDPG is ineffective in the former, while GEP-PG outperforms the best DDPG variant in both environments. For supplementary videos and discussion, visit http://frama.link/gep_pg, and for the code, visit http://github.com/flowersteam/geppg.",1
"Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning, via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work \cite{efroni2018beyond}, multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work, we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty, arising with soft-policy updates: even in the absence of approximations, and contrary to the 1-step-greedy case, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty, we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator.",0
"The effectiveness of multiple-step lookahead policies in Reinforcement Learning has been demonstrated through the use of Monte Carlo Tree Search or Model Predictive Control. Recently, a study by Efroni proposed and analyzed the use of multiple-step greedy policies in vanilla Policy Iteration algorithms. In this work, we aim to examine the practical applications of multiple-step greedy algorithms. One challenge we address is the surprising difficulty that arises with soft-policy updates, where monotonic policy improvement cannot be guaranteed without a sufficiently large update stepsize. To overcome this challenge, we develop and analyze online and approximate algorithms that incorporate a multi-step greedy operator.",1
"Temporal difference (TD) learning is an important approach in reinforcement learning, as it combines ideas from dynamic programming and Monte Carlo methods in a way that allows for online and incremental model-free learning. A key idea of TD learning is that it is learning predictive knowledge about the environment in the form of value functions, from which it can derive its behavior to address long-term sequential decision making problems. The agent's horizon of interest, that is, how immediate or long-term a TD learning agent predicts into the future, is adjusted through a discount rate parameter. In this paper, we introduce an alternative view on the discount rate, with insight from digital signal processing, to include complex-valued discounting. Our results show that setting the discount rate to appropriately chosen complex numbers allows for online and incremental estimation of the Discrete Fourier Transform (DFT) of a signal of interest with TD learning. We thereby extend the types of knowledge representable by value functions, which we show are particularly useful for identifying periodic effects in the reward sequence.",0
"Reinforcement learning employs temporal difference (TD) learning, a significant technique that combines the concepts of dynamic programming and Monte Carlo methods. This method facilitates model-free learning in an online and incremental manner by learning predictive knowledge through value functions. Such knowledge is used to arrive at long-term sequential decision-making solutions. The discount rate parameter in TD learning determines the agent's horizon of interest, ranging from immediate to long-term prediction. In this study, we present an alternative view on this parameter, incorporating complex-valued discounting, inspired by digital signal processing. Our findings demonstrate that effective estimation of the Discrete Fourier Transform (DFT) can be achieved through TD learning by setting the discount rate to suitable complex numbers. By broadening the representable knowledge types by value functions, we show how this approach can identify periodic effects in reward sequences.",1
"The score function estimator is widely used for estimating gradients of stochastic objectives in stochastic computation graphs (SCG), eg, in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order derivatives is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order derivative involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for estimators of higher-order derivatives. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct estimators of derivatives of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and numerical evaluation of the DiCE derivative estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://www.github.com/alshedivat/lola.",0
"The score function estimator is a commonly used method for estimating stochastic objective gradients in stochastic computation graphs (SCG), such as in reinforcement learning and meta-learning. While deriving first-order gradient estimators by differentiating a surrogate loss (SL) objective is straightforward in terms of computation and concept, it becomes more challenging when calculating higher-order derivatives. Firstly, deriving and implementing such estimators analytically is laborious and does not comply with automatic differentiation. Secondly, constructing new objectives for each order derivative using SL requires increasingly cumbersome graph manipulations. Lastly, SL treats part of the cost as a fixed sample to match the first-order gradient under differentiation, which leads to missing and incorrect terms for estimating higher-order derivatives. To address these issues, we introduce DiCE, which provides a single objective that can be repeatedly differentiated to generate correct estimators of derivatives of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing necessary graph manipulations. We prove the correctness of DiCE and evaluate its derivative estimates numerically. Additionally, we use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available on https://www.github.com/alshedivat/lola.",1
"We explore the problem of learning to decompose spatial tasks into segments, as exemplified by the problem of a painting robot covering a large object. Inspired by the ability of classical decision tree algorithms to construct structured partitions of their input spaces, we formulate the problem of decomposing objects into segments as a parsing approach. We make the insight that the derivation of a parse-tree that decomposes the object into segments closely resembles a decision tree constructed by ID3, which can be done when the ground-truth available. We learn to imitate an expert parsing oracle, such that our neural parser can generalize to parse natural images without ground truth. We introduce a novel deterministic policy gradient update, DRAG (i.e., DeteRministically AGgrevate) in the form of a deterministic actor-critic variant of AggreVaTeD, to train our neural parser. From another perspective, our approach is a variant of the Deterministic Policy Gradient suitable for the imitation learning setting. The deterministic policy representation offered by training our neural parser with DRAG allows it to outperform state of the art imitation and reinforcement learning approaches.",0
"Our focus is on the challenge of breaking down spatial tasks into segments, as illustrated by the task of a painting robot coating a large object. We draw inspiration from classical decision tree algorithms that can construct structured partitions of input spaces. To address the task of segmenting objects, we take a parsing approach. We observe that the process of creating a parse-tree that breaks down an object into its constituent parts is similar to constructing a decision tree using ID3 when ground-truth data is available. We aim to teach our neural parser to imitate an expert parsing oracle, allowing it to generalize to parse natural images without access to ground-truth information. We introduce a new policy gradient update technique, DRAG (DeteRministically AGgrevate), which is a deterministic actor-critic variant of AggreVaTeD, to train our neural parser. Our approach also represents a variation of the Deterministic Policy Gradient designed for imitation learning. Training our neural parser with DRAG results in a deterministic policy representation that outperforms existing imitation and reinforcement learning techniques.",1
"We present an effective technique for training deep learning agents capable of negotiating on a set of clauses in a contract agreement using a simple communication protocol. We use Multi Agent Reinforcement Learning to train both agents simultaneously as they negotiate with each other in the training environment. We also model selfish and prosocial behavior to varying degrees in these agents. Empirical evidence is provided showing consistency in agent behaviors. We further train a meta agent with a mixture of behaviors by learning an ensemble of different models using reinforcement learning. Finally, to ascertain the deployability of the negotiating agents, we conducted experiments pitting the trained agents against human players. Results demonstrate that the agents are able to hold their own against human players, often emerging as winners in the negotiation. Our experiments demonstrate that the meta agent is able to reasonably emulate human behavior.",0
"Our study showcases an effective method for training deep learning agents to negotiate on a contract agreement's set of clauses using a straightforward communication protocol. To achieve this, we utilize Multi Agent Reinforcement Learning to concurrently train both agents while they negotiate in a training environment. These agents exhibit varying degrees of selfish and prosocial behavior, and we provide empirical proof of their consistent actions. Furthermore, we train a meta agent with a blend of behaviors by employing reinforcement learning to learn an ensemble of various models. To test the deployability of the negotiating agents, we conduct experiments that pit them against human players. Remarkably, the agents hold their ground against human players, frequently emerge victorious in negotiations. Our findings demonstrate that the meta agent can effectively mimic human behavior.",1
"We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",0
"Our proposition is to employ boosted regression trees to derive human-understandable resolutions to reinforcement learning predicaments. Boosting merges multiple regression trees to enhance their precision without considerably diminishing their innate comprehensibility. Previous research has concentrated on both reinforcement learning and machine learning that can be interpreted, but making reinforcement learning interpretable has seen little advancement. Our trial outcomes indicate that boosted regression trees produce solutions that are both explainable and equivalent in standard to the top reinforcement learning approaches.",1
"The main focus of this paper is on enhancement of two types of game-theoretic learning algorithms: log-linear learning and reinforcement learning. The standard analysis of log-linear learning needs a highly structured environment, i.e. strong assumptions about the game from an implementation perspective. In this paper, we introduce a variant of log-linear learning that provides asymptotic guarantees while relaxing the structural assumptions to include synchronous updates and limitations in information available to the players. On the other hand, model-free reinforcement learning is able to perform even under weaker assumptions on players' knowledge about the environment and other players' strategies. We propose a reinforcement algorithm that uses a double-aggregation scheme in order to deepen players' insight about the environment and constant learning step-size which achieves a higher convergence rate. Numerical experiments are conducted to verify each algorithm's robustness and performance.",0
"This paper focuses on improving two game-theoretic learning algorithms: log-linear learning and reinforcement learning. The traditional analysis of log-linear learning requires a highly structured environment with strict implementation assumptions. However, we introduce a modified version of log-linear learning that maintains asymptotic guarantees while relaxing these structural assumptions, such as synchronous updates and limited player information. Conversely, reinforcement learning can succeed under weaker assumptions about player knowledge and strategy. Our proposed reinforcement algorithm incorporates a double-aggregation scheme and constant learning step-size to enhance players' understanding and achieve faster convergence. We conduct numerical experiments to test the robustness and performance of each algorithm.",1
"Many reinforcement-learning researchers treat the reward function as a part of the environment, meaning that the agent can only know the reward of a state if it encounters that state in a trial run. However, we argue that this is an unnecessary limitation and instead, the reward function should be provided to the learning algorithm. The advantage is that the algorithm can then use the reward function to check the reward for states that the agent hasn't even encountered yet. In addition, the algorithm can simultaneously learn policies for multiple reward functions. For each state, the algorithm would calculate the reward using each of the reward functions and add the rewards to its experience replay dataset. The Hindsight Experience Replay algorithm developed by Andrychowicz et al. (2017) does just this, and learns to generalize across a distribution of sparse, goal-based rewards. We extend this algorithm to linearly-weighted, multi-objective rewards and learn a single policy that can generalize across all linear combinations of the multi-objective reward. Whereas other multi-objective algorithms teach the Q-function to generalize across the reward weights, our algorithm enables the policy to generalize, and can thus be used with continuous actions.",0
"There is a debate among reinforcement-learning researchers about treating the reward function as a part of the environment. This means that the agent can only learn the reward of a state by encountering it in a trial run. However, we believe that this is an unnecessary limitation and that the reward function should be provided to the learning algorithm. This allows the algorithm to check the reward for states that the agent hasn't encountered yet and learn policies for multiple reward functions simultaneously. Our algorithm calculates the reward for each state using each of the reward functions and adds the rewards to its experience replay dataset. This approach is similar to the Hindsight Experience Replay algorithm developed by Andrychowicz et al. (2017), which generalizes across a distribution of sparse, goal-based rewards. However, we extend this algorithm to linearly-weighted, multi-objective rewards and learn a single policy that can generalize across all linear combinations of the multi-objective reward. Unlike other multi-objective algorithms, our algorithm enables the policy to generalize and can be used with continuous actions.",1
"Learning robot tasks or controllers using deep reinforcement learning has been proven effective in simulations. Learning in simulation has several advantages. For example, one can fully control the simulated environment, including halting motions while performing computations. Another advantage when robots are involved, is that the amount of time a robot is occupied learning a task---rather than being productive---can be reduced by transferring the learned task to the real robot. Transfer learning requires some amount of fine-tuning on the real robot. For tasks which involve complex (non-linear) dynamics, the fine-tuning itself may take a substantial amount of time. In order to reduce the amount of fine-tuning we propose to learn robustified controllers in simulation. Robustified controllers are learned by exploiting the ability to change simulation parameters (both appearance and dynamics) for successive training episodes. An additional benefit for this approach is that it alleviates the precise determination of physics parameters for the simulator, which is a non-trivial task. We demonstrate our proposed approach on a real setup in which a robot aims to solve a maze game, which involves complex dynamics due to static friction and potentially large accelerations. We show that the amount of fine-tuning in transfer learning for a robustified controller is substantially reduced compared to a non-robustified controller.",0
"Using deep reinforcement learning to teach robots tasks or controllers has been found to be effective in simulations. Learning in simulations has various advantages, such as having complete control over the simulated environment, including the ability to pause movements while performing computations. It also reduces the amount of time the robot spends learning a task, allowing for more productivity, by transferring the learned task to the real robot. However, fine-tuning the task on the real robot can take a significant amount of time, particularly for tasks involving complex dynamics. To minimize the need for fine-tuning, we suggest learning robustified controllers in simulation. Robustified controllers are learned by changing simulation parameters for successive training episodes, allowing for more efficient learning. This approach also eases the precise determination of physics parameters for the simulator, which can be difficult. Our approach was tested on a real setup, where a robot was programmed to solve a maze game with complex dynamics. We found that using a robustified controller reduced the amount of fine-tuning needed for transfer learning compared to a non-robustified controller.",1
"Dynamic treatment recommendation systems based on large-scale electronic health records (EHRs) become a key to successfully improve practical clinical outcomes. Prior relevant studies recommend treatments either use supervised learning (e.g. matching the indicator signal which denotes doctor prescriptions), or reinforcement learning (e.g. maximizing evaluation signal which indicates cumulative reward from survival rates). However, none of these studies have considered to combine the benefits of supervised learning and reinforcement learning. In this paper, we propose Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which fuses them into a synergistic learning framework. Specifically, SRL-RNN applies an off-policy actor-critic framework to handle complex relations among multiple medications, diseases and individual characteristics. The ""actor"" in the framework is adjusted by both the indicator signal and evaluation signal to ensure effective prescription and low mortality. RNN is further utilized to solve the Partially-Observed Markov Decision Process (POMDP) problem due to the lack of fully observed states in real world applications. Experiments on the publicly real-world dataset, i.e., MIMIC-3, illustrate that our model can reduce the estimated mortality, while providing promising accuracy in matching doctors' prescriptions.",0
"Large-scale electronic health records (EHRs) have paved the way for dynamic treatment recommendation systems that can significantly enhance practical clinical outcomes. Prior research suggests that supervised learning (such as matching doctor prescriptions) or reinforcement learning (such as maximizing survival rates) are viable options for treatment recommendations. However, none of these studies have explored the potential benefits of combining the two approaches. In this study, we propose a novel approach called Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which integrates supervised and reinforcement learning into a synergistic framework. Our model utilizes an off-policy actor-critic method to handle complex relationships between medications, diseases, and individual characteristics. The ""actor"" in the framework is optimized using both indicator and evaluation signals to ensure effective prescription and low mortality. To address the Partially-Observed Markov Decision Process (POMDP) challenges in real-world settings, we employ RNN. Our experiments using the publicly available MIMIC-3 dataset demonstrate that our model can reduce estimated mortality while accurately matching doctors' prescriptions.",1
"Deep reinforcement learning has become popular over recent years, showing superiority on different visual-input tasks such as playing Atari games and robot navigation. Although objects are important image elements, few work considers enhancing deep reinforcement learning with object characteristics. In this paper, we propose a novel method that can incorporate object recognition processing to deep reinforcement learning models. This approach can be adapted to any existing deep reinforcement learning frameworks. State-of-the-art results are shown in experiments on Atari games. We also propose a new approach called ""object saliency maps"" to visually explain the actions made by deep reinforcement learning agents.",0
"Over the past few years, deep reinforcement learning has gained popularity due to its exceptional performance in various visual-input tasks like robot navigation and playing Atari games. However, despite the significance of objects in image elements, only a few studies have explored the possibility of enhancing deep reinforcement learning with object characteristics. This paper suggests a new approach that can integrate object recognition processing into existing deep reinforcement learning frameworks. The proposed method shows state-of-the-art results in experiments conducted on Atari games. Additionally, this paper introduces a novel technique called ""object saliency maps"" that visually explains the actions taken by deep reinforcement learning agents.",1
"Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of ""object saliency maps"", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.",0
"In the near future, self-governing AI systems will join human society to provide services and collaborate with people. For these systems to gain acceptance and trust, they must be transparent, and users should be able to comprehend their decision-making process. Transparency allows humans to create logical explanations of the system's actions and decisions, which is essential not only for user trust but also for software debugging and certification. Despite the significant advancements of Deep Neural Networks in various domains, they are not transparent. This paper addresses transparency in Deep Reinforcement Learning Networks (DRLN), which have shown exceptional success in controlling actions in image input domains such as Atari games. The paper proposes a new and comprehensive method that (a) combines explicit object recognition processing into deep reinforcement learning models, (b) creates ""object saliency maps"" that visualize internal states of DRLNs, allowing for the development of explanations, and (c) can be included in any existing deep reinforcement learning framework. The paper also presents computational results and human experiments to evaluate the effectiveness of this approach.",1
"Current imitation learning techniques are too restrictive because they require the agent and expert to share the same action space. However, oftentimes agents that act differently from the expert can solve the task just as good. For example, a person lifting a box can be imitated by a ceiling mounted robot or a desktop-based robotic-arm. In both cases, the end goal of lifting the box is achieved, perhaps using different strategies. We denote this setup as \textit{Inspiration Learning} - knowledge transfer between agents that operate in different action spaces. Since state-action expert demonstrations can no longer be used, Inspiration learning requires novel methods to guide the agent towards the end goal. In this work, we rely on ideas of Preferential based Reinforcement Learning (PbRL) to design Advantage Actor-Critic algorithms for solving inspiration learning tasks. Unlike classic actor-critic architectures, the critic we use consists of two parts: a) a state-value estimation as in common actor-critic algorithms and b) a single step reward function derived from an expert/agent classifier. We show that our method is capable of extending the current imitation framework to new horizons. This includes continuous-to-discrete action imitation, as well as primitive-to-macro action imitation.",0
"The current methods of imitation learning are too limiting as they require the agent and expert to have the same action space. However, agents that use different actions from the expert can still achieve the same task. For instance, a robot mounted on the ceiling or a desktop-based robotic arm can lift a box just like a person. This approach is known as Inspiration Learning, which involves knowledge transfer between agents operating in different action spaces. Since state-action expert demonstrations cannot be used, novel methods are required to guide agents to the intended goal. In this research, we employ Preferential based Reinforcement Learning (PbRL) concepts to develop Advantage Actor-Critic algorithms that solve Inspiration learning tasks. Our approach differs from traditional actor-critic frameworks as our critic has two components: a state-value estimation and a single-step reward function from an expert/agent classifier. Our results demonstrate that our method can expand the current imitation framework by enabling continuous-to-discrete action imitation and primitive-to-macro action imitation.",1
"Complex autonomous control systems are subjected to sensor failures, cyber-attacks, sensor noise, communication channel failures, etc. that introduce errors in the measurements. The corrupted information, if used for making decisions, can lead to degraded performance. We develop a framework for using adversarial deep reinforcement learning to design observer strategies that are robust to adversarial errors in information channels. We further show through simulation studies that the learned observation strategies perform remarkably well when the adversary's injected errors are bounded in some sense. We use neural network as function approximator in our studies with the understanding that any other suitable function approximating class can be used within our framework.",0
"Autonomous control systems of a complex nature are vulnerable to various factors such as sensor malfunctions, cyber-attacks, sensor noise, and communication channel failures. These factors can produce inaccuracies in the measurements, which when utilized for decision-making purposes can result in a decline in performance. To tackle this issue, we have developed a framework that leverages adversarial deep reinforcement learning to create observer strategies that can withstand adversarial errors in information channels. Our simulation studies demonstrate that the learned observation strategies perform exceptionally well when the errors introduced by the adversary are limited in some way. We have utilized neural networks as function approximators in our research, but our framework can adapt to other suitable function approximating classes as well.",1
"A probability density function (pdf) encodes the entire stochastic knowledge about data distribution, where data may represent stochastic observations in robotics, transition state pairs in reinforcement learning or any other empirically acquired modality. Inferring data pdf is of prime importance, allowing to analyze various model hypotheses and perform smart decision making. However, most density estimation techniques are limited in their representation expressiveness to specific kernel type or predetermined distribution family, and have other restrictions. For example, kernel density estimation (KDE) methods require meticulous parameter search and are extremely slow at querying new points. In this paper we present a novel non-parametric density estimation approach, DeepPDF, that uses a neural network to approximate a target pdf given samples from thereof. Such a representation provides high inference accuracy for a wide range of target pdfs using a relatively simple network structure, making our method highly statistically robust. This is done via a new stochastic optimization algorithm, \emph{Probabilistic Surface Optimization} (PSO), that turns to advantage the stochastic nature of sample points in order to force network output to be identical to the output of a target pdf. Once trained, query point evaluation can be efficiently done in DeepPDF by a simple network forward pass, with linear complexity in the number of query points. Moreover, the PSO algorithm is capable of inferring the frequency of data samples and may also be used in other statistical tasks such as conditional estimation and distribution transformation. We compare the derived approach with KDE methods showing its superior performance and accuracy.",0
"The probability density function (pdf) contains all the stochastic information about the distribution of data, which can represent various empirical modalities such as stochastic observations in robotics or transition state pairs in reinforcement learning. It is crucial to infer the data pdf to analyze different model hypotheses and make intelligent decisions. However, most density estimation techniques have limitations in terms of their expressiveness in representing specific kernel types or predetermined distribution families, as well as other restrictions. For instance, kernel density estimation (KDE) methods require a meticulous search for parameters and are sluggish when querying new points. This paper proposes a novel non-parametric density estimation approach called DeepPDF that employs a neural network to approximate a target pdf using samples. This method has a high inference accuracy for a broad range of target pdfs with a relatively straightforward network structure, making it statistically robust. The proposed approach uses a new stochastic optimization algorithm called Probabilistic Surface Optimization (PSO) that leverages the stochastic nature of sample points to ensure that the network output is identical to the target pdf output. Once trained, the DeepPDF method can efficiently evaluate query points using a simple network forward pass with linear complexity in the number of query points. Furthermore, the PSO algorithm can infer the frequency of data samples and can be used for other statistical tasks such as conditional estimation and distribution transformation. The proposed approach outperforms KDE methods in terms of performance and accuracy.",1
"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffers from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments. We consider this as a problem of transferring knowledge within a family of similar Markov decision processes.   For this purpose we assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.",0
"Although Reinforcement Learning (RL) methods are powerful problem solvers, they often produce suboptimal policies in slightly different environments. This is particularly true in robotics, where diverse training and deployment conditions can be costly and retraining is not ideal. Simulation training offers a practical solution, but it is not entirely reliable in real-world scenarios due to the reality-gap. Therefore, this highlights the need for adaptive policies that can function efficiently in new environments. We propose solving this issue by transferring knowledge within similar Markov decision processes. Our approach utilizes a low-dimensional latent variable to generate Q-functions, which allows us to identify a master policy that can adapt to different values of the latent variable. Our method involves learning the generative mapping and an approximate posterior of the latent variables, enabling us to identify policies for new tasks by searching solely in the latent space. The low-dimensional space and master policy generated by our method enable fast adaptation to new environments. We validate our approach on both a pendulum swing-up task in simulation and a pushing task for simulation-to-real transfer.",1
"Early detection of cyber-attacks is crucial for a safe and reliable operation of the smart grid. In the literature, outlier detection schemes making sample-by-sample decisions and online detection schemes requiring perfect attack models have been proposed. In this paper, we formulate the online attack/anomaly detection problem as a partially observable Markov decision process (POMDP) problem and propose a universal robust online detection algorithm using the framework of model-free reinforcement learning (RL) for POMDPs. Numerical studies illustrate the effectiveness of the proposed RL-based algorithm in timely and accurate detection of cyber-attacks targeting the smart grid.",0
"Detecting cyber-attacks early is essential to ensure the smart grid operates safely and reliably. Current literature suggests outlier detection schemes that make decisions based on individual samples, and online detection schemes that require perfect attack models. In this study, we present a solution to the online attack/anomaly detection problem using a partially observable Markov decision process (POMDP) framework. We propose a universal robust online detection algorithm using model-free reinforcement learning (RL) for POMDPs. Our numerical studies demonstrate the effectiveness of the RL-based algorithm in detecting cyber-attacks targeting the smart grid accurately and promptly.",1
"Model-based reinforcement learning approaches carry the promise of being data efficient. However, due to challenges in learning dynamics models that sufficiently match the real-world dynamics, they struggle to achieve the same asymptotic performance as model-free methods. We propose Model-Based Meta-Policy-Optimization (MB-MPO), an approach that foregoes the strong reliance on accurate learned dynamics models. Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step. This steers the meta-policy towards internalizing consistent dynamics predictions among the ensemble while shifting the burden of behaving optimally w.r.t. the model discrepancies towards the adaptation step. Our experiments show that MB-MPO is more robust to model imperfections than previous model-based approaches. Finally, we demonstrate that our approach is able to match the asymptotic performance of model-free methods while requiring significantly less experience.",0
"While model-based reinforcement learning approaches have the potential to be efficient with data, they often struggle to achieve the same performance as model-free methods due to difficulties in accurately learning dynamics models that match the real world. To address this, we introduce Model-Based Meta-Policy-Optimization (MB-MPO), which does not rely heavily on accurate learned dynamics models. Instead, MB-MPO uses an ensemble of learned dynamic models to meta-learn a policy that can adapt quickly to any model in the ensemble, requiring only one policy gradient step. This approach helps the meta-policy learn consistent dynamics predictions among the ensemble while shifting the responsibility for optimal behavior in the face of model discrepancies to the adaptation step. Our experiments demonstrate that MB-MPO is more resilient to model imperfections than previous model-based methods and can match the asymptotic performance of model-free methods while using significantly less experience.",1
"Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.",0
"Advanced performance in image captioning has been demonstrated by Reinforcement Learning (RL) approaches through direct optimization of testing metrics. However, this approach introduces learning biases that decrease the readability of generated text, and the large sample space leads to unstable and slow training. To solve these issues, we propose a coherent solution that constrains the action space using an n-gram language prior. Our evaluations on benchmarks show that RL with this add-on module performs favorably in terms of readability and convergence speed. Human evaluations also indicate that our model produces more human-readable and graceful captions. The implementation will be publicly available upon paper acceptance.",1
"Achieving machine intelligence requires a smooth integration of perception and reasoning, yet models developed to date tend to specialize in one or the other; sophisticated manipulation of symbols acquired from rich perceptual spaces has so far proved elusive. Consider a visual arithmetic task, where the goal is to carry out simple arithmetical algorithms on digits presented under natural conditions (e.g. hand-written, placed randomly). We propose a two-tiered architecture for tackling this problem. The lower tier consists of a heterogeneous collection of information processing modules, which can include pre-trained deep neural networks for locating and extracting characters from the image, as well as modules performing symbolic transformations on the representations extracted by perception. The higher tier consists of a controller, trained using reinforcement learning, which coordinates the modules in order to solve the high-level task. For instance, the controller may learn in what contexts to execute the perceptual networks and what symbolic transformations to apply to their outputs. The resulting model is able to solve a variety of tasks in the visual arithmetic domain, and has several advantages over standard, architecturally homogeneous feedforward networks including improved sample efficiency.",0
"To achieve machine intelligence, the integration of perception and reasoning is crucial. However, current models tend to focus on one over the other, making it difficult to manipulate symbols from complex perceptual spaces. For example, performing arithmetic algorithms on handwritten digits presented under natural conditions presents a challenge. To address this issue, we propose a two-tiered architecture. The lower tier comprises various information processing modules, including pre-trained deep neural networks for character recognition and modules for symbolic transformations of extracted representations. The higher tier is a controller trained using reinforcement learning that coordinates the modules to solve the task. The resulting model can perform various visual arithmetic tasks efficiently, surpassing standard feedforward networks in several ways.",1
"Recently it has shown that the policy-gradient methods for reinforcement learning have been utilized to train deep end-to-end systems on natural language processing tasks. What's more, with the complexity of understanding image content and diverse ways of describing image content in natural language, image captioning has been a challenging problem to deal with. To the best of our knowledge, most state-of-the-art methods follow a pattern of sequential model, such as recurrent neural networks (RNN). However, in this paper, we propose a novel architecture for image captioning with deep reinforcement learning to optimize image captioning tasks. We utilize two networks called ""policy network"" and ""value network"" to collaboratively generate the captions of images. The experiments are conducted on Microsoft COCO dataset, and the experimental results have verified the effectiveness of the proposed method.",0
"Recent developments have seen policy-gradient methods for reinforcement learning being applied to train deep end-to-end systems for natural language processing tasks. This has posed a challenge in image captioning due to the intricate nature of comprehending image content and the various ways of describing it in natural language. Most state-of-the-art techniques have followed a sequential model such as recurrent neural networks (RNN). However, this paper introduces a novel approach using deep reinforcement learning to optimize image captioning tasks. The proposed method employs two networks, the ""policy network"" and the ""value network,"" to cooperatively generate captions for images. The effectiveness of the method is tested on the Microsoft COCO dataset, and the experimental results validate its efficacy.",1
"Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].",0
"Over the last few years, deep learning has achieved significant success across various application domains. This emerging field of machine learning has rapidly grown and is now being applied in numerous areas, leading to new opportunities. Different learning approaches have been proposed, including supervised, semi-supervised, and un-supervised learning. Empirical results demonstrate that deep learning outperforms traditional machine learning techniques in various fields such as Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and more. This report provides a concise overview of the development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we cover recent advanced DL techniques based on these approaches, as well as their evaluation in different application domains. We also include recently developed frameworks, SDKs, and benchmark datasets used to implement and evaluate deep learning approaches. While some surveys have been published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234], our paper provides a comprehensive overview of advanced techniques used to train large-scale deep learning models and the recently developed method of generative models [1].",1
"The reinforcement learning community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequential-decision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent's updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.",0
"The reinforcement learning community has developed algorithms that can outperform humans in specific tasks. However, these algorithms are typically trained for one task at a time, meaning that each agent can only solve the task it was trained on. This poses a challenge when trying to master multiple sequential-decision tasks simultaneously. Multi-task learning requires a balance between the needs of each task, as some tasks can be more salient to the learning process due to their rewards. To address this, we propose adapting the contribution of each task to the agent's updates so that all tasks have a similar impact on the learning dynamics. This approach resulted in state-of-the-art performance, with a single agent surpassing human-level performance in a set of 57 Atari games and 30 tasks in the DeepMind Lab.",1
"We demonstrate the first application of deep reinforcement learning to autonomous driving. From randomly initialised parameters, our model is able to learn a policy for lane following in a handful of training episodes using a single monocular image as input. We provide a general and easy to obtain reward: the distance travelled by the vehicle without the safety driver taking control. We use a continuous, model-free deep reinforcement learning algorithm, with all exploration and optimisation performed on-vehicle. This demonstrates a new framework for autonomous driving which moves away from reliance on defined logical rules, mapping, and direct supervision. We discuss the challenges and opportunities to scale this approach to a broader range of autonomous driving tasks.",0
"In this research, we showcase the initial use of deep reinforcement learning for self-driving purposes. Our model starts from random parameters and can quickly learn a lane-following policy with just one monocular image as input. We adopt an accessible and universal reward metric: the vehicle's distance traveled without the need for human intervention. Our approach employs a model-free, continuous deep reinforcement learning algorithm that carries out all exploration and optimization while driving. This marks a new direction for autonomous driving, as it deviates from conventional reliance on fixed logical rules, mapping, and direct supervision. We also address the challenges and possibilities of extending this method to a broader spectrum of autonomous driving tasks.",1
"Real-time advertising allows advertisers to bid for each impression for a visiting user. To optimize specific goals such as maximizing revenue and return on investment (ROI) led by ad placements, advertisers not only need to estimate the relevance between the ads and user's interests, but most importantly require a strategic response with respect to other advertisers bidding in the market. In this paper, we formulate bidding optimization with multi-agent reinforcement learning. To deal with a large number of advertisers, we propose a clustering method and assign each cluster with a strategic bidding agent. A practical Distributed Coordinated Multi-Agent Bidding (DCMAB) has been proposed and implemented to balance the tradeoff between the competition and cooperation among advertisers. The empirical study on our industry-scaled real-world data has demonstrated the effectiveness of our methods. Our results show cluster-based bidding would largely outperform single-agent and bandit approaches, and the coordinated bidding achieves better overall objectives than purely self-interested bidding agents.",0
"Real-time advertising enables advertisers to bid on every impression a user makes. To achieve specific goals like maximizing revenue and return on investment (ROI) from ad placements, advertisers must gauge the relevance of their ads to a user's interests and act strategically in response to competing bidders. This paper proposes optimizing bidding through multi-agent reinforcement learning and cluster-based bidding. The Distributed Coordinated Multi-Agent Bidding (DCMAB) approach balances competition and cooperation among advertisers and outperforms single-agent and bandit approaches. Empirical studies on real-world data demonstrate the effectiveness of our methods, showing that coordinated bidding achieves better overall objectives than purely self-interested bidding agents.",1
"Making the right decision in traffic is a challenging task that is highly dependent on individual preferences as well as the surrounding environment. Therefore it is hard to model solely based on expert knowledge. In this work we use Deep Reinforcement Learning to learn maneuver decisions based on a compact semantic state representation. This ensures a consistent model of the environment across scenarios as well as a behavior adaptation function, enabling on-line changes of desired behaviors without re-training. The input for the neural network is a simulated object list similar to that of Radar or Lidar sensors, superimposed by a relational semantic scene description. The state as well as the reward are extended by a behavior adaptation function and a parameterization respectively. With little expert knowledge and a set of mid-level actions, it can be seen that the agent is capable to adhere to traffic rules and learns to drive safely in a variety of situations.",0
"Traffic decisions can be difficult to make due to personal preferences and environmental factors, making it challenging to model using only expert knowledge. To address this, we utilized Deep Reinforcement Learning to teach decision-making based on a concise semantic state representation, ensuring consistent modeling across various scenarios and adaptation capabilities for behavior changes without retraining. Our neural network input imitates Radar or Lidar sensors with a semantic scene description. We added a behavior adaptation function and parameterization to the state and reward. Our agent demonstrated an ability to follow traffic rules and drive safely in diverse situations with minimal expert knowledge and mid-level actions.",1
"Flatland is a simple, lightweight environment for fast prototyping and testing of reinforcement learning agents. It is of lower complexity compared to similar 3D platforms (e.g. DeepMind Lab or VizDoom), but emulates physical properties of the real world, such as continuity, multi-modal partially-observable states with first-person view and coherent physics. We propose to use it as an intermediary benchmark for problems related to Lifelong Learning. Flatland is highly customizable and offers a wide range of task difficulty to extensively evaluate the properties of artificial agents. We experiment with three reinforcement learning baseline agents and show that they can rapidly solve a navigation task in Flatland. A video of an agent acting in Flatland is available here: https://youtu.be/I5y6Y2ZypdA.",0
"Flatland is a lightweight and uncomplicated environment that allows for rapid prototyping and testing of reinforcement learning agents. While it is not as intricate as other 3D platforms like DeepMind Lab or VizDoom, it still replicates essential physical properties of the real world, such as coherent physics, first-person view, and multi-modal partially-observable states with continuity. We recommend using Flatland as an intermediary benchmark for Lifelong Learning problems since it is customizable and offers many different task difficulties to thoroughly assess artificial agents' capabilities. To demonstrate its effectiveness, we conducted experiments with three baseline reinforcement learning agents and found that they could quickly complete a navigation task in Flatland. Additionally, a video showcasing an agent in Flatland is available for viewing at https://youtu.be/I5y6Y2ZypdA.",1
"Autonomous vehicles (AVs) are on the road. To safely and efficiently interact with other road participants, AVs have to accurately predict the behavior of surrounding vehicles and plan accordingly. Such prediction should be probabilistic, to address the uncertainties in human behavior. Such prediction should also be interactive, since the distribution over all possible trajectories of the predicted vehicle depends not only on historical information, but also on future plans of other vehicles that interact with it. To achieve such interaction-aware predictions, we propose a probabilistic prediction approach based on hierarchical inverse reinforcement learning (IRL). First, we explicitly consider the hierarchical trajectory-generation process of human drivers involving both discrete and continuous driving decisions. Based on this, the distribution over all future trajectories of the predicted vehicle is formulated as a mixture of distributions partitioned by the discrete decisions. Then we apply IRL hierarchically to learn the distributions from real human demonstrations. A case study for the ramp-merging driving scenario is provided. The quantitative results show that the proposed approach can accurately predict both the discrete driving decisions such as yield or pass as well as the continuous trajectories.",0
"AVs require accurate prediction of surrounding vehicles' behavior to ensure safe and efficient road interactions. This prediction must address uncertainties in human behavior, be probabilistic, and interactive, as it depends on future plans of interacting vehicles. To achieve this, we propose a probabilistic prediction approach using hierarchical inverse reinforcement learning (IRL). Our approach considers the trajectory-generation process of human drivers and formulates the distribution over all future trajectories of the predicted vehicle as a mixture of distributions, partitioned by discrete decisions. We apply IRL hierarchically to learn distributions from human demonstrations, and provide a case study for the ramp-merging scenario. Our results show that our approach accurately predicts both discrete decisions and continuous trajectories.",1
"Rogue is a famous dungeon-crawling video-game of the 80ies, the ancestor of its gender. Rogue-like games are known for the necessity to explore partially observable and always different randomly-generated labyrinths, preventing any form of level replay. As such, they serve as a very natural and challenging task for reinforcement learning, requiring the acquisition of complex, non-reactive behaviors involving memory and planning. In this article we show how, exploiting a version of A3C partitioned on different situations, the agent is able to reach the stairs and descend to the next level in 98% of cases.",0
"The 80s video-game called Rogue is a renowned dungeon-crawling game and is considered the precursor of its genre. These kinds of games are referred to as Rogue-like games, and they require players to explore randomly generated labyrinths that are partially observable and always different. This means that there is no possibility of replaying levels. As a result, they offer a natural and challenging task for reinforcement learning, which involves acquiring complex, non-reactive behaviors that require memory and planning. Our article demonstrates how an agent can use a partitioned version of A3C to reach the stairs and descend to the next level in 98% of cases.",1
"Deep reinforcement learning has obtained significant breakthroughs in recent years. Most methods in deep-RL achieve good results via the maximization of the reward signal provided by the environment, typically in the form of discounted cumulative returns. Such reward signals represent the immediate feedback of a particular action performed by an agent. However, tasks with sparse reward signals are still challenging to on-policy methods. In this paper, we introduce an effective characterization of past reward statistics (which can be seen as long-term feedback signals) to supplement this immediate reward feedback. In particular, value functions are learned with multi-critics supervision, enabling complex value functions to be more easily approximated in on-policy learning, even when the reward signals are sparse. We also introduce a novel exploration mechanism called ""hot-wiring"" that can give a boost to seemingly trapped agents. We demonstrate the effectiveness of our advantage actor multi-critic (A2MC) method across the discrete domains in Atari games as well as continuous domains in the MuJoCo environments. A video demo is provided at https://youtu.be/zBmpf3Yz8tc.",0
"In recent years, deep reinforcement learning has achieved significant breakthroughs. The majority of deep-RL methods attain favorable results by maximizing the reward signal, typically in the form of discounted cumulative returns, provided by the environment. This signal represents immediate feedback about an agent's actions. However, on-policy methods still find it challenging to handle tasks with sparse reward signals. In this article, we propose an approach that supplements immediate reward feedback with a characterization of past reward statistics, which can be viewed as long-term feedback signals. We accomplish this by training value functions with multi-critics supervision, making it easier to approximate complex value functions in on-policy learning, even in the presence of sparse reward signals. Furthermore, we introduce a new exploration mechanism called ""hot-wiring"" that can help agents that appear to be stuck. We demonstrate the effectiveness of our advantage actor multi-critic (A2MC) method in both discrete domains, such as Atari games, and continuous domains, such as MuJoCo environments. A video demonstration is available at https://youtu.be/zBmpf3Yz8tc.",1
"Experience replay is an important technique for addressing sample-inefficiency in deep reinforcement learning (RL), but faces difficulty in learning from binary and sparse rewards due to disproportionately few successful experiences in the replay buffer. Hindsight experience replay (HER) was recently proposed to tackle this difficulty by manipulating unsuccessful transitions, but in doing so, HER introduces a significant bias in the replay buffer experiences and therefore achieves a suboptimal improvement in sample-efficiency. In this paper, we present an analysis on the source of bias in HER, and propose a simple and effective method to counter the bias, to most effectively harness the sample-efficiency provided by HER. Our method, motivated by counter-factual reasoning and called ARCHER, extends HER with a trade-off to make rewards calculated for hindsight experiences numerically greater than real rewards. We validate our algorithm on two continuous control environments from DeepMind Control Suite - Reacher and Finger, which simulate manipulation tasks with a robotic arm - in combination with various reward functions, task complexities and goal sampling strategies. Our experiments consistently demonstrate that countering bias using more aggressive hindsight rewards increases sample efficiency, thus establishing the greater benefit of ARCHER in RL applications with limited computing budget.",0
"To combat the issue of sample-inefficiency in deep reinforcement learning (RL), experience replay is a valuable technique, but its efficacy is hindered by the scarcity of successful experiences in the replay buffer when dealing with binary and sparse rewards. Hindsight experience replay (HER) was introduced to address this problem by modifying unsuccessful transitions, but this approach results in a significant bias in the replay buffer experiences, resulting in suboptimal sample-efficiency improvements. This study examines the source of bias in HER and proposes an effective method called ARCHER, which employs counter-factual reasoning and increases the rewards calculated for hindsight experiences to counter the bias. The algorithm was tested on two continuous control environments, Reacher and Finger, with various reward functions, task complexities, and goal sampling strategies, demonstrating that using more aggressive hindsight rewards enhances sample efficiency and establishing ARCHER's superiority in RL applications with limited computing resources.",1
"We apply neural nets with ReLU gates in online reinforcement learning. Our goal is to train these networks in an incremental manner, without the computationally expensive experience replay. By studying how individual neural nodes behave in online training, we recognize that the global nature of ReLU gates can cause undesirable learning interference in each node's learning behavior. We propose reducing such interferences with two efficient input transformation methods that are geometric in nature and match well the geometric property of ReLU gates. The first one is tile coding, a classic binary encoding scheme originally designed for local generalization based on the topological structure of the input space. The second one (EmECS) is a new method we introduce; it is based on geometric properties of convex sets and topological embedding of the input space into the boundary of a convex set. We discuss the behavior of the network when it operates on the transformed inputs. We also compare it experimentally with some neural nets that do not use the same input transformations, and with the classic algorithm of tile coding plus a linear function approximator, and on several online reinforcement learning tasks, we show that the neural net with tile coding or EmECS can achieve not only faster learning but also more accurate approximations. Our results strongly suggest that geometric input transformation of this type can be effective for interference reduction and takes us a step closer to fully incremental reinforcement learning with neural nets.",0
"In online reinforcement learning, we utilize neural nets with ReLU gates and aim to train these networks incrementally, avoiding the costly experience replay. However, we have observed that the global nature of ReLU gates can cause unwanted learning interference among individual neural nodes during online training. To mitigate this interference, we propose two geometry-based input transformation methods: tile coding and EmECS. Tile coding, originally designed for local generalization, employs a binary encoding scheme based on the input space's topological structure. EmECS, a novel approach we introduce, is based on geometric properties of convex sets and topological embedding of the input space into a convex set's boundary. We evaluate the network's performance on transformed inputs and compare it with other neural nets that do not use similar input transformations, as well as the classic tile coding algorithm combined with a linear function approximator. Our experiments on various online reinforcement learning tasks demonstrate that neural nets using tile coding or EmECS achieve faster learning and more precise approximations. Our findings suggest that geometric input transformation can effectively reduce interference and bring us closer to fully incremental reinforcement learning with neural nets.",1
"Research in deep reinforcement learning (RL) has coalesced around improving performance on benchmarks like the Arcade Learning Environment. However, these benchmarks conspicuously miss important characteristics like abrupt context-dependent shifts in strategy and temporal sensitivity that are often present in real-world domains. As a result, RL research has not focused on these challenges, resulting in algorithms which do not understand critical changes in context, and have little notion of real world time. To tackle this issue, this paper introduces the game of Space Fortress as a RL benchmark which incorporates these characteristics. We show that existing state-of-the-art RL algorithms are unable to learn to play the Space Fortress game. We then confirm that this poor performance is due to the RL algorithms' context insensitivity and reward sparsity. We also identify independent axes along which to vary context and temporal sensitivity, allowing Space Fortress to be used as a testbed for understanding both characteristics in combination and also in isolation. We release Space Fortress as an open-source Gym environment.",0
"The focus of deep reinforcement learning (RL) research has been to improve performance on benchmarks such as the Arcade Learning Environment. However, these benchmarks lack important features such as sudden shifts in strategy and temporal sensitivity that are common in real-world situations. Therefore, RL research has not addressed these challenges, resulting in algorithms that cannot recognize significant changes in context or real-world time. In order to address this issue, this study proposes Space Fortress as a RL benchmark that incorporates these features. It is demonstrated that existing state-of-the-art RL algorithms cannot learn to play Space Fortress due to their context insensitivity and sparse rewards. The study also identifies independent axes for varying context and temporal sensitivity, making Space Fortress a useful testbed for investigating these features both in combination and isolation. The open-source Gym environment for Space Fortress is also released.",1
"We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: ""Are there any apples in the fridge?"" The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive objects (code and dataset available at https://github.com/danielgordon10/thor-iqa-cvpr-2018). IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98",0
"The task of Interactive Question Answering (IQA) involves answering questions that require an autonomous agent to interact with a dynamic visual environment. For example, a question like ""Are there any apples in the fridge?"" presented to the agent requires the agent to navigate the scene, understand the visual elements, interact with objects, and plan a series of actions based on the question. However, popular reinforcement learning approaches with a single controller are not effective for IQA due to the large and diverse state space. To address this, we propose the Hierarchical Interactive Memory Network (HIMN) with a factorized set of controllers that allows the system to operate at multiple levels of temporal abstraction. To evaluate HIMN's performance, we introduce IQUAD V1, a new dataset built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 comprises 75,000 questions, each paired with a unique scene configuration. Our experiments show that HIMN outperforms popular single controller-based approaches on IQUAD V1. For more information, please refer to our video: https://youtu.be/pXd3C-1jr98.",1
"Considering its advantages in dealing with high-dimensional visual input and learning control policies in discrete domain, Deep Q Network (DQN) could be an alternative method of traditional auto-focus means in the future. In this paper, based on Deep Reinforcement Learning, we propose an end-to-end approach that can learn auto-focus policies from visual input and finish at a clear spot automatically. We demonstrate that our method - discretizing the action space with coarse to fine steps and applying DQN is not only a solution to auto-focus but also a general approach towards vision-based control problems. Separate phases of training in virtual and real environments are applied to obtain an effective model. Virtual experiments, which are carried out after the virtual training phase, indicates that our method could achieve 100% accuracy on a certain view with different focus range. Further training on real robots could eliminate the deviation between the simulator and real scenario, leading to reliable performances in real applications.",0
"The potential of Deep Q Network (DQN) as an alternative to traditional auto-focus methods in the future is evident due to its ability to handle high-dimensional visual input and learn control policies in a discrete domain. To achieve this, our paper proposes an end-to-end approach based on Deep Reinforcement Learning, which can learn auto-focus policies from visual input and automatically focus on a clear spot. By discretizing the action space with coarse to fine steps and applying DQN, our method not only provides a solution to auto-focus but also offers a general approach to vision-based control problems. To obtain an effective model, we use separate phases of training in virtual and real environments. Our virtual experiments after the virtual training phase show that our method achieves 100% accuracy on a certain view with different focus ranges. Further training on real robots can eliminate the deviation between the simulator and real scenarios, ensuring reliable performance in real applications.",1
"Recently deep reinforcement learning (DRL) has achieved outstanding success on solving many difficult and large-scale RL problems. However the high sample cost required for effective learning often makes DRL unaffordable in resource-limited applications. With the aim of improving sample efficiency and learning performance, we will develop a new DRL algorithm in this paper that seamless integrates entropy-induced and bootstrap-induced techniques for efficient and deep exploration of the learning environment. Specifically, a general form of Tsallis entropy regularizer will be utilized to drive entropy-induced exploration based on efficient approximation of optimal action-selection policies. Different from many existing works that rely on action dithering strategies for exploration, our algorithm is efficient in exploring actions with clear exploration value. Meanwhile, by employing an ensemble of Q-networks under varied Tsallis entropy regularization, the diversity of the ensemble can be further enhanced to enable effective bootstrap-induced exploration. Experiments on Atari game playing tasks clearly demonstrate that our new algorithm can achieve more efficient and effective exploration for DRL, in comparison to recently proposed exploration methods including Bootstrapped Deep Q-Network and UCB Q-Ensemble.",0
"In recent times, deep reinforcement learning (DRL) has proven to be a remarkable solution for tackling numerous challenging and extensive RL problems. However, the cost of obtaining sufficient samples for effective learning often renders DRL impractical for resource-limited applications. To address this challenge and enhance sample efficiency and learning performance, this paper will introduce a new DRL algorithm that seamlessly integrates entropy-induced and bootstrap-induced techniques for deep and efficient exploration of the learning environment. The paper will employ a general form of Tsallis entropy regularizer to motivate entropy-induced exploration based on optimal action-selection policies' efficient approximation. Unlike many existing works that rely on action dithering strategies for exploration, the proposed algorithm efficiently explores actions with clear exploration value. Additionally, the algorithm will utilize an ensemble of Q-networks under varied Tsallis entropy regularization to further enhance the ensemble's diversity and enable effective bootstrap-induced exploration. Results from experiments on Atari game playing tasks show that the new algorithm achieves more efficient and effective exploration for DRL compared to other recently proposed exploration methods, including Bootstrapped Deep Q-Network and UCB Q-Ensemble.",1
"A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper at https://worldmodels.github.io",0
"By utilizing compressed spatio-temporal representations, a generative recurrent neural network is trained in an unsupervised manner to simulate widely used reinforcement learning environments. The extracted features of the world model are then utilized to train simple and condensed policies using evolution, which results in excellent outcomes in a range of environments. Furthermore, we train our agent exclusively within a self-generated environment produced by its internal world model, before transferring this policy back to the real environment. For an interactive version of the paper, please visit https://worldmodels.github.io.",1
"In the past few years, deep reinforcement learning has been proven to solve problems which have complex states like video games or board games. The next step of intelligent agents would be able to generalize between tasks, and using prior experience to pick up new skills more quickly. However, most reinforcement learning algorithms for now are often suffering from catastrophic forgetting even when facing a very similar target task. Our approach enables the agents to generalize knowledge from a single source task, and boost the learning progress with a semisupervised learning method when facing a new task. We evaluate this approach on Atari games, which is a popular reinforcement learning benchmark, and show that it outperforms common baselines based on pre-training and fine-tuning.",0
"Recently, deep reinforcement learning has demonstrated its ability to tackle complex state problems such as video and board games. However, the next step for intelligent agents is to generalize between tasks and effectively apply prior experiences to expedite the acquisition of new skills. Despite this goal, current reinforcement learning algorithms frequently experience catastrophic forgetting, even when presented with similar target tasks. Our approach aims to address this issue by enabling agents to apply knowledge from a single source task and enhance learning through a semisupervised learning method when presented with a new task. We tested this approach on Atari games, a popular reinforcement learning benchmark, and found that it surpassed common baselines that rely on pre-training and fine-tuning.",1
"Most existing video summarisation methods are based on either supervised or unsupervised learning. In this paper, we propose a reinforcement learning-based weakly supervised method that exploits easy-to-obtain, video-level category labels and encourages summaries to contain category-related information and maintain category recognisability. Specifically, We formulate video summarisation as a sequential decision-making process and train a summarisation network with deep Q-learning (DQSN). A companion classification network is also trained to provide rewards for training the DQSN. With the classification network, we develop a global recognisability reward based on the classification result. Critically, a novel dense ranking-based reward is also proposed in order to cope with the temporally delayed and sparse reward problems for long sequence reinforcement learning. Extensive experiments on two benchmark datasets show that the proposed approach achieves state-of-the-art performance.",0
"The majority of current approaches to video summarisation rely on supervised or unsupervised learning. Our research introduces a novel method that employs reinforcement learning and weak supervision, which utilises video-level category labels to facilitate inclusion of category-related information in summaries while maintaining category recognisability. Our approach involves formulating video summarisation as a sequential decision-making process and training a summarisation network using deep Q-learning (DQSN). Additionally, we train a companion classification network to provide rewards for training the DQSN. Our method incorporates a global recognisability reward based on the classification result, as well as a novel dense ranking-based reward to account for temporally delayed and sparse reward problems during long sequence reinforcement learning. Through extensive experiments conducted on two benchmark datasets, we demonstrate that our approach achieves state-of-the-art performance.",1
"We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.",0
"Our focus is on developing deep neural network policies for reinforcement learning that have a hierarchical structure. Unlike other methods that limit the lower layers of a hierarchy to use higher-level modulating signals, we train each layer in our framework to directly solve the task. However, we incorporate a range of diverse strategies into each layer through a maximum entropy reinforcement learning objective and latent random variables, which are sampled during training. This objective results in the latent variables being included in the layer's policy, allowing the higher level layer to control the lower layer's behavior through this latent space. We retain full expressivity by ensuring that the mapping from latent variables to actions is invertible, thereby avoiding constraints on either layer's behavior. Our experiments show that our method outperforms single-layer policies on standard benchmark tasks by adding more layers and can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.",1
"This paper explores the use of deep reinforcement learning agents to transfer knowledge from one environment to another. More specifically, the method takes advantage of asynchronous advantage actor critic (A3C) architecture to generalize a target game using an agent trained on a source game in Atari. Instead of fine-tuning a pre-trained model for the target game, we propose a learning approach to update the model using multiple agents trained in parallel with different representations of the target game. Visual mapping between video sequences of transfer pairs is used to derive new representations of the target game; training on these visual representations of the target game improves model updates in terms of performance, data efficiency and stability. In order to demonstrate the functionality of the architecture, Atari games Pong-v0 and Breakout-v0 are being used from the OpenAI gym environment; as the source and target environment.",0
"The aim of this study is to investigate the use of deep reinforcement learning agents for transferring knowledge between different environments. Specifically, the study utilizes the asynchronous advantage actor critic (A3C) architecture to generalize a target game by training an agent on a source game in Atari. Rather than fine-tuning a pre-trained model for the target game, the authors propose a learning method that involves updating the model using multiple agents trained in parallel with various representations of the target game. To achieve this, visual mapping is utilized between video sequences of transfer pairs to generate new representations of the target game. Training on these visual representations enhances model updates concerning performance, data efficiency, and stability. To demonstrate the effectiveness of the architecture, the authors utilize Atari games Pong-v0 and Breakout-v0 as the source and target environments from the OpenAI gym environment.",1
"Recent success in deep reinforcement learning is having an agent learn how to play Go and beat the world champion without any prior knowledge of the game. In that task, the agent has to make a decision on what action to take based on the positions of the pieces. Person Search is recently explored using natural language based text description of images for video surveillance applications (S.Li et.al). We see (Fu.et al) provides an end to end approach for object-based retrieval using deep reinforcement learning without constraints placed on which objects are being detected. However, we believe for real-world applications such as person search defining specific constraints which identify a person as opposed to starting with a general object detection will have benefits in terms of performance and computational resources required. In our task, Deep reinforcement learning would localize the person in an image by reshaping the sizes of the bounding boxes. Deep Reinforcement learning with appropriate constraints would look only for the relevant person in the image as opposed to an unconstrained approach where each individual objects in the image are ranked. For person search, the agent is trying to form a tight bounding box around the person in the image who matches the description. The bounding box is initialized to the full image and at each time step, the agent makes a decision on how to change the current bounding box so that it has a tighter bound around the person based on the description of the person and the pixel values of the current bounding box. After the agent takes an action, it will be given a reward based on the Intersection over Union (IoU) of the current bounding box and the ground truth box. Once the agent believes that the bounding box is covering the person, it will indicate that the person is found.",0
"The latest achievement in deep reinforcement learning involves an agent mastering the game of Go and beating the world champion without any prior knowledge of the game. Meanwhile, a recent study (S.Li et.al) has been conducted to explore Person Search using natural language-based text description of images for video surveillance applications. Although Fu.et al provides an end-to-end approach for object-based retrieval using deep reinforcement learning, it is believed that defining specific constraints for identifying a person in an image would be more beneficial in real-world applications, such as person search, as it would enhance performance and minimize computational resources required. In this task, deep reinforcement learning techniques are utilized to localize the person in an image by reshaping the sizes of the bounding boxes. By applying appropriate constraints, the agent can focus solely on the relevant person in the image instead of ranking each individual object in an unconstrained approach. The goal of the agent in person search is to create a tight bounding box around the person in the image, whose description matches the given criteria. The bounding box is initially set to the full image, and at each time step, the agent makes a decision on how to modify the current bounding box to create a tighter bound around the person based on the description and pixel values. The agent is rewarded based on the Intersection over Union (IoU) of the current bounding box and the ground truth box, and it will indicate that the person has been found once it believes that the bounding box covers the person.",1
"We present research using the latest reinforcement learning algorithm for end-to-end driving without any mediated perception (object recognition, scene understanding). The newly proposed reward and learning strategies lead together to faster convergence and more robust driving using only RGB image from a forward facing camera. An Asynchronous Actor Critic (A3C) framework is used to learn the car control in a physically and graphically realistic rally game, with the agents evolving simultaneously on tracks with a variety of road structures (turns, hills), graphics (seasons, location) and physics (road adherence). A thorough evaluation is conducted and generalization is proven on unseen tracks and using legal speed limits. Open loop tests on real sequences of images show some domain adaption capability of our method.",0
"Our study utilizes the latest reinforcement learning algorithm to achieve end-to-end driving, without the need for any mediated perception such as object recognition or scene understanding. Our newly proposed reward and learning strategies work in tandem to enhance convergence and increase the robustness of driving, using only RGB images from a forward-facing camera. We employ an Asynchronous Actor Critic (A3C) framework to teach our car control in a physically and graphically realistic rally game. Our agents evolve simultaneously on various tracks featuring a range of road structures, graphics, and physics. We conduct a comprehensive evaluation and prove the generalization of our approach on unseen tracks and within legal speed limits. Our method also demonstrates some domain adaption capability during open loop tests on real image sequences.",1
"Assisted by neural networks, reinforcement learning agents have been able to solve increasingly complex tasks over the last years. The simulation environment in which the agents interact is an essential component in any reinforcement learning problem. The environment simulates the dynamics of the agents' world and hence provides feedback to their actions in terms of state observations and external rewards. To ease the design and simulation of such environments this work introduces $\texttt{APES}$, a highly customizable and open source package in Python to create 2D grid-world environments for reinforcement learning problems. $\texttt{APES}$ equips agents with algorithms to simulate any field of vision, it allows the creation and positioning of items and rewards according to user-defined rules, and supports the interaction of multiple agents.",0
"Neural networks have assisted reinforcement learning agents in solving increasingly complex tasks in recent years. The simulation environment plays a crucial role in any reinforcement learning problem as it simulates the agents' world dynamics and provides feedback to their actions through state observations and external rewards. To simplify the creation and simulation of such environments, this study introduces $\texttt{APES}$, a customizable and open source Python package that creates 2D grid-world environments for reinforcement learning problems. $\texttt{APES}$ comes equipped with algorithms that enable agents to simulate any field of vision, create and position items and rewards according to user-defined rules, and allow multiple agents to interact.",1
"We study an exploration method for model-free RL that generalizes the counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than a single step look-ahead. We propose a model-free RL method that modifies Delayed Q-learning and utilizes the long-term exploration bonus with provable efficiency. We show that our proposed method finds a near-optimal policy in polynomial time (PAC-MDP), and also provide experimental evidence that our proposed algorithm is an efficient exploration method.",0
"Our focus is on exploring a model-free RL technique that extends the counter-based exploration bonus methods by considering the long-term exploratory value of actions instead of a single step look-ahead. We present a modified version of Delayed Q-learning that integrates the long-term exploration bonus, which can be proven to be efficient. Our method is demonstrated to achieve close to optimal results in polynomial time (PAC-MDP), and we provide empirical data to support that it is an effective exploration approach.",1
"We introduce a new virtual environment for simulating a card game known as ""Big 2"". This is a four-player game of imperfect information with a relatively complicated action space (being allowed to play 1,2,3,4 or 5 card combinations from an initial starting hand of 13 cards). As such it poses a challenge for many current reinforcement learning methods. We then use the recently proposed ""Proximal Policy Optimization"" algorithm to train a deep neural network to play the game, purely learning via self-play, and find that it is able to reach a level which outperforms amateur human players after only a relatively short amount of training time and without needing to search a tree of future game states.",0
"A new virtual environment has been developed to simulate the card game ""Big 2"". This game involves four players and has an action space that allows players to play 1, 2, 3, 4 or 5 card combinations from an initial hand of 13 cards, making it complex and challenging for reinforcement learning methods. To address this challenge, a deep neural network was trained using the ""Proximal Policy Optimization"" algorithm through self-play. The network achieved a higher level of performance than amateur human players in a relatively short period of training time, without requiring a search of future game states.",1
"Estimating the value function for a fixed policy is a fundamental problem in reinforcement learning. Policy evaluation algorithms---to estimate value functions---continue to be developed, to improve convergence rates, improve stability and handle variability, particularly for off-policy learning. To understand the properties of these algorithms, the experimenter needs high-confidence estimates of the accuracy of the learned value functions. For environments with small, finite state-spaces, like chains, the true value function can be easily computed, to compute accuracy. For large, or continuous state-spaces, however, this is no longer feasible. In this paper, we address the largely open problem of how to obtain these high-confidence estimates, for general state-spaces. We provide a high-confidence bound on an empirical estimate of the value error to the true value error. We use this bound to design an offline sampling algorithm, which stores the required quantities to repeatedly compute value error estimates for any learned value function. We provide experiments investigating the number of samples required by this offline algorithm in simple benchmark reinforcement learning domains, and highlight that there are still many open questions to be solved for this important problem.",0
"Reinforcement learning involves the fundamental issue of estimating the value function for a fixed policy. While policy evaluation algorithms have been developed to estimate value functions and improve convergence rates, stability, and variability handling for off-policy learning, accurate evaluation of learned value functions is crucial to understanding these algorithms' properties. Small, finite state-space environments like chains can easily compute accuracy, while large or continuous state-spaces cannot. This paper addresses the challenge of obtaining high-confidence estimates of accuracy for general state-spaces, providing a high-confidence bound on empirical estimates of value error to true value error. An offline sampling algorithm is designed, storing the necessary data to repeatedly compute value error estimates for any learned value function. Simple benchmark reinforcement learning domains' experiments assess the number of samples required by this offline algorithm, highlighting the need to address unanswered questions regarding this critical issue.",1
"Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English- German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.",0
"According to recent research, reinforcement learning (RL) is a successful technique for enhancing the performance of neural machine translation (NMT) systems. However, RL training can be challenging due to instability, especially in real-world systems that utilize deep models and large datasets. This study aims to improve NMT models using reinforcement learning by conducting a systematic evaluation of various factors, such as baseline reward and reward shaping, on several large-scale translation tasks. Additionally, we introduce a new approach that uses RL to further enhance the performance of NMT systems trained with monolingual data. By combining our findings, we achieve competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, including a state-of-the-art performance on the WMT17 Chinese-English translation task.",1
"We examine the problem of learning and planning on high-dimensional domains with long horizons and sparse rewards. Recent approaches have shown great successes in many Atari 2600 domains. However, domains with long horizons and sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup, and Singh 1999) have shown to be useful in tackling long-horizon problems. We combine recent techniques of deep reinforcement learning with existing model-based approaches using an expert-provided state abstraction. We construct toy domains that elucidate the problem of long horizons, sparse rewards and high-dimensional inputs, and show that our algorithm significantly outperforms previous methods on these domains. Our abstraction-based approach outperforms Deep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and exhibits backtracking behavior that is absent from previous methods.",0
"The focus of our investigation is the challenge of acquiring knowledge and creating strategies for extensive, sparsely rewarded domains with high dimensions and long horizons. While recent methodologies have proven effective in various Atari 2600 domains, such as Montezuma's Revenge and Venture, which require long-range planning and have sparse rewards, remain problematic for current techniques. However, previous investigations have found that methods utilizing abstraction, such as those proposed by Dietterich in 2000 and Sutton, Precup, and Singh in 1999, can be advantageous when tackling long-range challenges. By combining the latest deep reinforcement learning methods with established model-based approaches and incorporating an expert-provided state abstraction, we have developed a new algorithm. We have also created sample domains that illustrate the obstacles of long horizons, sparse rewards, and high-dimensional inputs. The results demonstrate that our abstraction-based approach surpasses previous methods on these domains, including Montezuma's Revenge and Venture, and exhibits backtracking behavior absent in previous methods.",1
"Indoor navigation aims at performing navigation within buildings. In scenes like home and factory, most intelligent mobile devices require an functionality of routing to guide itself precisely through indoor scenes to complete various tasks in order to serve human. In most scenarios, we expected an intelligent device capable of navigating itself in unseen environment. Although several solutions have been proposed to deal with this issue, they usually require pre-installed beacons or a map pre-built with SLAM, which means that they are not capable of working in novel environments. To address this, we proposed NavigationNet, a computer vision dataset and benchmark to allow the utilization of deep reinforcement learning on scene-understanding-based indoor navigation. We also proposed and formalized several typical indoor routing problems that are suitable for deep reinforcement learning.",0
"The goal of indoor navigation is to guide mobile devices through buildings such as homes and factories to perform various tasks for humans. However, in order to navigate through new and unfamiliar environments, these devices require advanced routing functionality. While some solutions have been proposed, such as pre-installed beacons or pre-built maps using SLAM technology, these methods can only be used in specific environments. To address this issue, we have introduced NavigationNet, which is a computer vision dataset and benchmark for deep reinforcement learning in indoor navigation. We have also defined several typical indoor routing problems that are well-suited for this approach.",1
"We study offline data poisoning attacks in contextual bandits, a class of reinforcement learning problems with important applications in online recommendation and adaptive medical treatment, among others. We provide a general attack framework based on convex optimization and show that by slightly manipulating rewards in the data, an attacker can force the bandit algorithm to pull a target arm for a target contextual vector. The target arm and target contextual vector are both chosen by the attacker. That is, the attacker can hijack the behavior of a contextual bandit. We also investigate the feasibility and the side effects of such attacks, and identify future directions for defense. Experiments on both synthetic and real-world data demonstrate the efficiency of the attack algorithm.",0
"Our focus is on offline data poisoning attacks in the area of contextual bandits, which are a type of reinforcement learning problems with significant applications in various fields, including online recommendations and personalized medical treatments. We present a comprehensive attack framework that employs convex optimization and proves that a slight modification of rewards in the data can result in the bandit algorithm selecting a specific arm for a particular contextual vector as dictated by the attacker. The attacker has the freedom to pick both the target arm and the target contextual vector, effectively taking over the decision-making process of the contextual bandit. Our study also explores the practicality and side effects of such attacks and suggests possible defense measures. Experiments carried out on synthetic and real-world data reveal the effectiveness of our attack algorithm.",1
"The large volume of video content and high viewing frequency demand automatic video summarization algorithms, of which a key property is the capability of modeling diversity. If videos are lengthy like hours-long egocentric videos, it is necessary to track the temporal structures of the videos and enforce local diversity. The local diversity refers to that the shots selected from a short time duration are diverse but visually similar shots are allowed to co-exist in the summary if they appear far apart in the video. In this paper, we propose a novel probabilistic model, built upon SeqDPP, to dynamically control the time span of a video segment upon which the local diversity is imposed. In particular, we enable SeqDPP to learn to automatically infer how local the local diversity is supposed to be from the input video. The resulting model is extremely involved to train by the hallmark maximum likelihood estimation (MLE), which further suffers from the exposure bias and non-differentiable evaluation metrics. To tackle these problems, we instead devise a reinforcement learning algorithm for training the proposed model. Extensive experiments verify the advantages of our model and the new learning algorithm over MLE-based methods.",0
"Due to the vast amount of video content available and the high frequency of viewing, there is a need for algorithms that can automatically summarize videos. A crucial feature of such algorithms is the ability to model diversity, particularly for lengthy egocentric videos where it is necessary to track the temporal structures and enforce local diversity. Local diversity pertains to selecting visually diverse shots within a short time duration, but visually similar shots can co-exist if they are far apart in the video. In this paper, we introduce a novel probabilistic model based on SeqDPP, which dynamically controls the time span of a video segment to impose local diversity. The model learns to infer the degree of local diversity required from the input video. Since the model is complex and difficult to train using maximum likelihood estimation (MLE), which is also susceptible to exposure bias and non-differentiable evaluation metrics, we propose a reinforcement learning algorithm for training instead. Our experiments demonstrate the superiority of our model and learning algorithm over MLE-based methods.",1
"Reinforcement learning approaches have long appealed to the data management community due to their ability to learn to control dynamic behavior from raw system performance. Recent successes in combining deep neural networks with reinforcement learning have sparked significant new interest in this domain. However, practical solutions remain elusive due to large training data requirements, algorithmic instability, and lack of standard tools. In this work, we introduce LIFT, an end-to-end software stack for applying deep reinforcement learning to data management tasks. While prior work has frequently explored applications in simulations, LIFT centers on utilizing human expertise to learn from demonstrations, thus lowering online training times. We further introduce TensorForce, a TensorFlow library for applied deep reinforcement learning exposing a unified declarative interface to common RL algorithms, thus providing a backend to LIFT. We demonstrate the utility of LIFT in two case studies in database compound indexing and resource management in stream processing. Results show LIFT controllers initialized from demonstrations can outperform human baselines and heuristics across latency metrics and space usage by up to 70%.",0
"The data management community has been interested in reinforcement learning techniques for a while due to their ability to learn how to control dynamic behavior from raw system performance. Recent advances in combining deep neural networks with reinforcement learning have led to renewed interest in this area. However, there are still challenges to finding practical solutions, such as the need for a large amount of training data, algorithmic instability, and a lack of standard tools. In this study, we present LIFT, a complete software stack that uses deep reinforcement learning for data management tasks. While previous research has focused on simulations, LIFT emphasizes the use of human expertise to learn from demonstrations, which reduces online training times. We also introduce TensorForce, a TensorFlow library that provides a unified declarative interface to common RL algorithms, which serves as a backend for LIFT. We demonstrate the effectiveness of LIFT in two case studies involving database compound indexing and resource management in stream processing, where the controllers initialized from demonstrations outperformed human baselines and heuristics in terms of latency metrics and space usage by up to 70%.",1
"Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the ""exposure bias"" during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (e.g., visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (e.g., ""man riding horse"") and comparisons (e.g., ""smaller cat""). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model --- CAVP and its subsequent language policy network --- can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at https://github.com/daqingliu/CAVP",0
"The problem of sequence prediction for natural language output can encompass many vision-language tasks. Deep reinforcement learning (RL) has been used in recent advancements in image captioning to address ""exposure bias"" during training, where the ground-truth subsequence is exposed in every step prediction, leading to bias during testing when only the predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy and not the visual policy, resulting in a failure to capture visual context crucial for compositional reasoning, such as visual relationships and comparisons. To address this gap, we introduce the Context-Aware Visual Policy network (CAVP), which leverages previous visual attention as context at every time step to make optimal decisions for current word generation. CAVP can attend to complex visual compositions over time, as opposed to traditional visual attention that only fixes a single image region at every step. The entire image captioning model, including CAVP and its subsequent language policy network, can be efficiently optimized end-to-end using an actor-critic policy gradient method with respect to any caption evaluation metric. Our method outperforms state-of-the-art performances on MS-COCO offline split and online server using various metrics and sensible visualizations of qualitative visual context. The code is available at https://github.com/daqingliu/CAVP.",1
"In this paper we discuss policy iteration methods for approximate solution of a finite-state discounted Markov decision problem, with a focus on feature-based aggregation methods and their connection with deep reinforcement learning schemes. We introduce features of the states of the original problem, and we formulate a smaller ""aggregate"" Markov decision problem, whose states relate to the features. We discuss properties and possible implementations of this type of aggregation, including a new approach to approximate policy iteration. In this approach the policy improvement operation combines feature-based aggregation with feature construction using deep neural networks or other calculations. We argue that the cost function of a policy may be approximated much more accurately by the nonlinear function of the features provided by aggregation, than by the linear function of the features provided by neural network-based reinforcement learning, thereby potentially leading to more effective policy improvement.",0
The focus of this paper is policy iteration methods for solving a finite-state discounted Markov decision problem. The paper explores feature-based aggregation methods and their relation to deep reinforcement learning schemes. The authors introduce features of the states in the original problem and use them to formulate a smaller Markov decision problem. This new approach to approximate policy iteration involves combining feature-based aggregation with feature construction through deep neural networks or other calculations. The paper argues that the nonlinear function of the features provided by aggregation can lead to more effective policy improvement compared to the linear function of the features provided by neural network-based reinforcement learning.,1
This paper describes some of the possibilities of artificial neural networks that open up after solving the problem of catastrophic forgetting. A simple model and reinforcement learning applications of existing methods are also proposed.,0
"In this article, the potential of artificial neural networks is explored in light of overcoming catastrophic forgetting. Additionally, a basic model and reinforcement learning techniques using current methods are suggested.",1
"Deep neuroevolution, that is evolutionary policy search methods based on deep neural networks, have recently emerged as a competitor to deep reinforcement learning algorithms due to their better parallelization capabilities. However, these methods still suffer from a far worse sample efficiency. In this paper we investigate whether a mechanism known as ""importance mixing"" can significantly improve their sample efficiency. We provide a didactic presentation of importance mixing and we explain how it can be extended to reuse more samples. Then, from an empirical comparison based on a simple benchmark, we show that, though it actually provides better sample efficiency, it is still far from the sample efficiency of deep reinforcement learning, though it is more stable.",0
"Recently, deep neuroevolution, which is based on deep neural networks, has emerged as a competitor to deep reinforcement learning algorithms due to its superior parallelization capabilities. However, its sample efficiency still remains unsatisfactory. This paper aims to explore whether ""importance mixing"" can enhance its sample efficiency. The authors provide a clear explanation of importance mixing and its extension to reuse more samples. Furthermore, they conduct an empirical comparison using a basic benchmark and demonstrate that, although importance mixing does improve sample efficiency, it still lags behind deep reinforcement learning in this regard. Nonetheless, it is more stable.",1
"One of the major challenges of model-free visual tracking problem has been the difficulty originating from the unpredictable and drastic changes in the appearance of objects we target to track. Existing methods tackle this problem by updating the appearance model on-line in order to adapt to the changes in the appearance. Despite the success of these methods however, inaccurate and erroneous updates of the appearance model result in a tracker drift. In this paper, we introduce a novel real-time visual tracking algorithm based on a template selection strategy constructed by deep reinforcement learning methods. The tracking algorithm utilizes this strategy to choose the appropriate template for tracking a given frame. The template selection strategy is self-learned by utilizing a simple policy gradient method on numerous training episodes randomly generated from a tracking benchmark dataset. Our proposed reinforcement learning framework is generally applicable to other confidence map based tracking algorithms. The experiment shows that our tracking algorithm runs in real-time speed of 43 fps and the proposed policy network effectively decides the appropriate template for successful visual tracking.",0
"The model-free visual tracking problem presents a significant challenge due to the unpredictable and dramatic changes in the appearance of tracked objects. To address this issue, existing methods update the appearance model in real-time to adapt to these changes. However, inaccurate updates can cause the tracker to drift. This paper presents a new visual tracking algorithm that employs a template selection strategy developed through deep reinforcement learning methods. This strategy allows the algorithm to choose the most appropriate template for each frame, and it is learned through a policy gradient method using a tracking benchmark dataset. Our reinforcement learning framework is applicable to other confidence map-based tracking algorithms. Our experiments showed that our algorithm operates in real-time at 43 fps and successfully selects the optimal template for effective visual tracking.",1
"Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",0
"Although reinforcement learning is a useful and comprehensive approach to decision making and control, its implementation is often impeded by the requirement for extensive feature and reward engineering. Although deep reinforcement learning techniques can eliminate the necessity for explicit policy or value feature engineering, they still necessitate a manually specified reward function. Inverse reinforcement learning offers the potential for automated reward acquisition, but it has proven to be highly challenging to employ in large, high-dimensional problems with unknown dynamics. This study proposes adversarial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm using an adversarial reward learning formulation. AIRL can retrieve reward functions that are resistant to changes in dynamics, allowing us to learn policies even in instances of significant variation in the training environment. Our experiments show that AIRL surpasses previous methods in these transfer settings.",1
"Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github.io/large-scale-curiosity/",0
"Reinforcement learning algorithms require environment rewards that are designed outside of the agent. However, manually annotating each environment with dense rewards is not practical for scaling up, leading to the development of intrinsic reward functions for agents. Curiosity is an intrinsic reward function that uses prediction error as a reward signal. This paper presents (a) the first large-scale study of curiosity-driven learning without any extrinsic rewards in 54 standard benchmark environments, including Atari games, showing surprisingly good performance and alignment with hand-designed extrinsic rewards in many game environments; (b) an investigation into the impact of using different feature spaces to compute prediction error, indicating that random features are sufficient for many popular RL game benchmarks, while learned features have better generalization to novel game levels in Super Mario Bros.; and (c) the limitations of prediction-based rewards in stochastic setups. Code and game-play videos are available at https://pathak22.github.io/large-scale-curiosity/.",1
"We present an approach for reconfiguration of dynamic visual sensor networks with deep reinforcement learning (RL). Our RL agent uses a modified asynchronous advantage actor-critic framework and the recently proposed Relational Network module at the foundation of its network architecture. To address the issue of sample inefficiency in current approaches to model-free reinforcement learning, we train our system in an abstract simulation environment that represents inputs from a dynamic scene. Our system is validated using inputs from a real-world scenario and preexisting object detection and tracking algorithms.",0
"Our study proposes a method for the modification of dynamic visual sensor networks using deep reinforcement learning (RL). Our RL agent employs a modified asynchronous advantage actor-critic framework and incorporates the Relational Network module into its network architecture. To combat the problem of sample inefficiency in current model-free reinforcement learning approaches, we train our system in an abstract simulation environment that simulates inputs from a dynamic scene. We validate our system using inputs from a real-world scenario and preexisting object detection and tracking algorithms.",1
"Many currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.",0
"Numerous Reinforcement Learning agents presently operate in an environment where they interact with humans, including colleagues, users, or clients. To ensure that these agents are effective, it is crucial that they adapt to individuals' preferences, learn quickly with their assistance, and behave safely around them. However, the majority of current approaches for learning from human feedback are hazardous since rewarding or punishing an agent after the fact does not immediately prevent it from engaging in undesirable behavior. In this article, we introduce Directed Policy Gradient (DPG), which extends Policy Gradient and makes it impervious to external directives that could otherwise undermine its on-policy nature. DPG allows a teacher or backup policy to override the agent before it behaves inappropriately, while also enabling the agent to benefit from human advice or directives to learn more quickly. Our experiments demonstrate that DPG enables the agent to learn significantly faster than reward-based methods while requiring substantially less advice.",1
