"Proximal policy optimization (PPO) has yielded state-of-the-art results in policy search, a subfield of reinforcement learning, with one of its key points being the use of a surrogate objective function to restrict the step size at each policy update. Although such restriction is helpful, the algorithm still suffers from performance instability and optimization inefficiency from the sudden flattening of the curve. To address this issue we present a PPO variant, named Proximal Policy Optimization Smooth Algorithm (PPOS), and its critical improvement is the use of a functional clipping method instead of a flat clipping method. We compare our method with PPO and PPORB, which adopts a rollback clipping method, and prove that our method can conduct more accurate updates at each time step than other PPO methods. Moreover, we show that it outperforms the latest PPO variants on both performance and stability in challenging continuous control tasks.",0
"The technique known as Proximal Policy Optimization (PPO) has shown impressive results in the subfield of reinforcement learning called policy search. One of its key features is the use of a surrogate objective function to limit the size of each policy update. Despite this beneficial restriction, the algorithm still experiences issues with unstable performance and inefficient optimization due to sudden flattening of the curve. To tackle this problem, we introduce a new version of PPO, called Proximal Policy Optimization Smooth Algorithm (PPOS). Our method improves on the original by employing a functional clipping method instead of a flat clipping method. We compare PPOS to both PPO and PPORB, which uses a rollback clipping method, and demonstrate that our approach can perform more precise updates at each time step than other PPO methods. Additionally, we demonstrate that PPOS outperforms the latest PPO variants in challenging continuous control tasks in terms of both stability and performance.",1
"Many dynamic processes, including common scenarios in robotic control and reinforcement learning (RL), involve a set of interacting subprocesses. Though the subprocesses are not independent, their interactions are often sparse, and the dynamics at any given time step can often be decomposed into locally independent causal mechanisms. Such local causal structures can be leveraged to improve the sample efficiency of sequence prediction and off-policy reinforcement learning. We formalize this by introducing local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space. We propose an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for Counterfactual Data Augmentation (CoDA). CoDA uses local structures and an experience replay to generate counterfactual experiences that are causally valid in the global model. We find that CoDA significantly improves the performance of RL agents in locally factored tasks, including the batch-constrained and goal-conditioned settings.",0
"Interacting subprocesses are involved in various dynamic processes, such as those used in robotic control and reinforcement learning. Although these subprocesses are not independent, their interactions are often sparse, and the dynamics at a given time step can be broken down into locally independent causal mechanisms. This local causal structure can be utilized to enhance the sample efficiency of sequence prediction and off-policy reinforcement learning. To achieve this, we introduce local causal models (LCMs), which are derived from a global causal model by conditioning on a subset of the state space. We propose an approach for inferring these structures using an object-oriented state representation and a new algorithm called Counterfactual Data Augmentation (CoDA). CoDA generates causally valid counterfactual experiences based on local structures and experience replay. Our experiments show that CoDA improves the performance of RL agents in locally factored tasks, such as batch-constrained and goal-conditioned settings.",1
"In this paper, we investigate a radio access network (RAN) slicing problem for Internet of vehicles (IoV) services with different quality of service (QoS) requirements, in which multiple logically-isolated slices are constructed on a common roadside network infrastructure. A dynamic RAN slicing framework is presented to dynamically allocate radio spectrum and computing resource, and distribute computation workloads for the slices. To obtain an optimal RAN slicing policy for accommodating the spatial-temporal dynamics of vehicle traffic density, we first formulate a constrained RAN slicing problem with the objective to minimize long-term system cost. This problem cannot be directly solved by traditional reinforcement learning (RL) algorithms due to complicated coupled constraints among decisions. Therefore, we decouple the problem into a resource allocation subproblem and a workload distribution subproblem, and propose a two-layer constrained RL algorithm, named Resource Allocation and Workload diStribution (RAWS) to solve them. Specifically, an outer layer first makes the resource allocation decision via an RL algorithm, and then an inner layer makes the workload distribution decision via an optimization subroutine. Extensive trace-driven simulations show that the RAWS effectively reduces the system cost while satisfying QoS requirements with a high probability, as compared with benchmarks.",0
"The aim of this study is to address the issue of radio access network (RAN) slicing for Internet of vehicles (IoV) services that require different quality of service (QoS) standards. To achieve this, multiple isolated slices are created on a single roadside network infrastructure. A dynamic RAN slicing framework is proposed to support the allocation of radio spectrum and computing resources, as well as the distribution of computational workloads for the slices. To develop an optimal RAN slicing policy that accounts for the dynamic changes in vehicle traffic density, a constrained RAN slicing problem is first formulated. However, the complex coupled constraints among decisions make it difficult to solve the problem using traditional reinforcement learning (RL) algorithms. Thus, the problem is divided into a resource allocation subproblem and a workload distribution subproblem, and a two-layer constrained RL algorithm called Resource Allocation and Workload diStribution (RAWS) is proposed to address them. The RAWS algorithm successfully reduces system costs while meeting QoS requirements with a high probability, as demonstrated by the extensive trace-driven simulations.",1
"In this paper we introduce DeepCrawl, a fully-playable Roguelike prototype for iOS and Android in which all agents are controlled by policy networks trained using Deep Reinforcement Learning (DRL). Our aim is to understand whether recent advances in DRL can be used to develop convincing behavioral models for non-player characters in videogames. We begin with an analysis of requirements that such an AI system should satisfy in order to be practically applicable in video game development, and identify the elements of the DRL model used in the DeepCrawl prototype. The successes and limitations of DeepCrawl are documented through a series of playability tests performed on the final game. We believe that the techniques we propose offer insight into innovative new avenues for the development of behaviors for non-player characters in video games, as they offer the potential to overcome critical issues with",0
"non-player character AI.   This paper presents DeepCrawl, a prototype Roguelike game available on iOS and Android. Our objective is to investigate whether Deep Reinforcement Learning (DRL) can be utilized to create realistic behavioral models for non-player characters in video games. To begin, we outline the requirements that an AI system must meet to be functional in video game development and describe the DRL elements used in the DeepCrawl prototype. Through a series of gameplay tests, we assess the strengths and limitations of DeepCrawl. Our proposed techniques represent an innovative approach to developing non-player character behavior in video games and offer a potential solution to critical issues with AI.",1
"Digital Twins have been described as beneficial in many areas, such as virtual commissioning, fault prediction or reconfiguration planning. Equipping Digital Twins with artificial intelligence functionalities can greatly expand those beneficial applications or open up altogether new areas of application, among them cross-phase industrial transfer learning. In the context of machine learning, transfer learning represents a set of approaches that enhance learning new tasks based upon previously acquired knowledge. Here, knowledge is transferred from one lifecycle phase to another in order to reduce the amount of data or time needed to train a machine learning algorithm. Looking at common challenges in developing and deploying industrial machinery with deep learning functionalities, embracing this concept would offer several advantages: Using an intelligent Digital Twin, learning algorithms can be designed, configured and tested in the design phase before the physical system exists and real data can be collected. Once real data becomes available, the algorithms must merely be fine-tuned, significantly speeding up commissioning and reducing the probability of costly modifications. Furthermore, using the Digital Twin's simulation capabilities virtually injecting rare faults in order to train an algorithm's response or using reinforcement learning, e.g. to teach a robot, become practically feasible. This article presents several cross-phase industrial transfer learning use cases utilizing intelligent Digital Twins. A real cyber physical production system consisting of an automated welding machine and an automated guided vehicle equipped with a robot arm is used to illustrate the respective benefits.",0
"Digital Twins have been described as advantageous in various fields, including virtual commissioning, fault prediction, and reconfiguration planning. The addition of artificial intelligence functionalities to Digital Twins can significantly expand the range of applications or even create new ones, such as cross-phase industrial transfer learning. In the realm of machine learning, transfer learning involves enhancing the learning of new tasks based on previously acquired knowledge. This approach can reduce the amount of data or time required to train a machine learning algorithm by transferring knowledge from one lifecycle phase to another. Implementing this concept can offer numerous benefits when developing and deploying industrial machinery with deep learning capabilities. An intelligent Digital Twin can be used to design, configure, and test learning algorithms in the design phase before the physical system exists and real data can be collected. Fine-tuning the algorithms once real data is available can significantly accelerate commissioning and minimize costly modifications. In addition, the Digital Twin's simulation capabilities can enable rare faults to be virtually injected to train an algorithm's response or use reinforcement learning to teach a robot. This article showcases various cross-phase industrial transfer learning use cases that utilize intelligent Digital Twins. The respective benefits are demonstrated using a cyber physical production system comprising an automated welding machine and an automated guided vehicle equipped with a robot arm.",1
"Deep model-based Reinforcement Learning (RL) has the potential to substantially improve the sample-efficiency of deep RL. While various challenges have long held it back, a number of papers have recently come out reporting success with deep model-based methods. This is a great development, but the lack of a consistent metric to evaluate such methods makes it difficult to compare various approaches. For example, the common single-task sample-efficiency metric conflates improvements due to model-based learning with various other aspects, such as representation learning, making it difficult to assess true progress on model-based RL. To address this, we introduce an experimental setup to evaluate model-based behavior of RL methods, inspired by work from neuroscience on detecting model-based behavior in humans and animals. Our metric based on this setup, the Local Change Adaptation (LoCA) regret, measures how quickly an RL method adapts to a local change in the environment. Our metric can identify model-based behavior, even if the method uses a poor representation and provides insight in how close a method's behavior is from optimal model-based behavior. We use our setup to evaluate the model-based behavior of MuZero on a variation of the classic Mountain Car task.",0
"The potential of deep model-based Reinforcement Learning (RL) to substantially improve the sample-efficiency of deep RL has been hindered by various challenges. However, recent papers have reported success with deep model-based methods. The lack of a consistent metric for evaluating such methods makes it difficult to compare various approaches. The common single-task sample-efficiency metric does not accurately assess true progress on model-based RL as it conflates improvements due to model-based learning with other aspects such as representation learning. To address this issue, we introduce a new experimental setup inspired by neuroscience to evaluate model-based behavior of RL methods. Our metric, called Local Change Adaptation (LoCA) regret, measures how quickly an RL method adapts to a local change in the environment and can identify model-based behavior even with a poor representation. We apply our setup to evaluate MuZero's model-based behavior on a variation of the classic Mountain Car task.",1
"There has been an increased interest in discovering heuristics for combinatorial problems on graphs through machine learning. While existing techniques have primarily focused on obtaining high-quality solutions, scalability to billion-sized graphs has not been adequately addressed. In addition, the impact of budget-constraint, which is necessary for many practical scenarios, remains to be studied. In this paper, we propose a framework called GCOMB to bridge these gaps. GCOMB trains a Graph Convolutional Network (GCN) using a novel probabilistic greedy mechanism to predict the quality of a node. To further facilitate the combinatorial nature of the problem, GCOMB utilizes a Q-learning framework, which is made efficient through importance sampling. We perform extensive experiments on real graphs to benchmark the efficiency and efficacy of GCOMB. Our results establish that GCOMB is 100 times faster and marginally better in quality than state-of-the-art algorithms for learning combinatorial algorithms. Additionally, a case-study on the practical combinatorial problem of Influence Maximization (IM) shows GCOMB is 150 times faster than the specialized IM algorithm IMM with similar quality.",0
"Machine learning has sparked a greater interest in discovering heuristics for combinatorial problems on graphs. However, while current techniques have focused on obtaining high-quality solutions, the issue of scalability to billion-sized graphs and budget constraints has been insufficiently addressed. This paper proposes a framework called GCOMB to address these gaps. GCOMB utilizes a Graph Convolutional Network (GCN) and a novel probabilistic greedy mechanism to predict the quality of a node. To accommodate the combinatorial nature of the problem, GCOMB also incorporates a Q-learning framework, which is made efficient through importance sampling. Extensive experiments on real graphs demonstrate GCOMB's efficiency and efficacy, with results indicating that it is 100 times faster and marginally better in quality than state-of-the-art algorithms for learning combinatorial algorithms. Furthermore, a case-study on the practical combinatorial problem of Influence Maximization (IM) shows that GCOMB is 150 times faster than the specialized IM algorithm IMM, while maintaining a similar level of quality.",1
"Inverse Reinforcement Learning (IRL) aims to facilitate a learner's ability to imitate expert behavior by acquiring reward functions that explain the expert's decisions. Regularized IRL applies strongly convex regularizers to the learner's policy in order to avoid the expert's behavior being rationalized by arbitrary constant rewards, also known as degenerate solutions. We propose tractable solutions, and practical methods to obtain them, for regularized IRL. Current methods are restricted to the maximum-entropy IRL framework, limiting them to Shannon-entropy regularizers, as well as proposing the solutions that are intractable in practice. We present theoretical backing for our proposed IRL method's applicability for both discrete and continuous controls, empirically validating our performance on a variety of tasks.",0
"The goal of Inverse Reinforcement Learning (IRL) is to help learners mimic expert behavior by acquiring reward functions that explain why the expert made certain decisions. To prevent degenerate solutions, also known as arbitrary constant rewards, regularized IRL applies strongly convex regularizers to the learner's policy. We offer practical and feasible solutions for regularized IRL and show that current methods are limited to Shannon-entropy regularizers within the maximum-entropy IRL framework. We also demonstrate that our proposed IRL method is applicable for both discrete and continuous controls and provide empirical evidence of its effectiveness on various tasks.",1
"The goal of policy-based reinforcement learning (RL) is to search the maximal point of its objective. However, due to the inherent non-concavity of its objective, convergence to a first-order stationary point (FOSP) can not guarantee the policy gradient methods finding a maximal point. A FOSP can be a minimal or even a saddle point, which is undesirable for RL. Fortunately, if all the saddle points are \emph{strict}, all the second-order stationary points (SOSP) are exactly equivalent to local maxima. Instead of FOSP, we consider SOSP as the convergence criteria to character the sample complexity of policy gradient. Our result shows that policy gradient converges to an $(\epsilon,\sqrt{\epsilon\chi})$-SOSP with probability at least $1-\widetilde{\mathcal{O}}(\delta)$ after the total cost of $\mathcal{O}\left(\dfrac{\epsilon^{-\frac{9}{2}}}{(1-\gamma)\sqrt\chi}\log\dfrac{1}{\delta}\right)$, where $\gamma\in(0,1)$. Our result improves the state-of-the-art result significantly where it requires $\mathcal{O}\left(\dfrac{\epsilon^{-9}\chi^{\frac{3}{2}}}{\delta}\log\dfrac{1}{\epsilon\chi}\right)$. Our analysis is based on the key idea that decomposes the parameter space $\mathbb{R}^p$ into three non-intersected regions: non-stationary point, saddle point, and local optimal region, then making a local improvement of the objective of RL in each region. This technique can be potentially generalized to extensive policy gradient methods.",0
"The main objective of policy-based reinforcement learning (RL) is to find the highest point on the objective function. However, since the objective function is inherently non-concave, convergence to a first-order stationary point (FOSP) does not guarantee that policy gradient methods will find the highest point. A FOSP can be a minimum or even a saddle point, which is not desirable for RL. Fortunately, if all saddle points are \emph{strict}, then all second-order stationary points (SOSP) are equivalent to local maxima. Therefore, we consider SOSP as the convergence criteria to determine the sample complexity of policy gradient. Our study shows that policy gradient converges to an $(\epsilon,\sqrt{\epsilon\chi})$-SOSP with a probability of at least $1-\widetilde{\mathcal{O}}(\delta)$ after incurring a total cost of $\mathcal{O}\left(\dfrac{\epsilon^{-\frac{9}{2}}}{(1-\gamma)\sqrt\chi}\log\dfrac{1}{\delta}\right)$, where $\gamma\in(0,1)$. This result is a significant improvement over the current state-of-the-art, which requires $\mathcal{O}\left(\dfrac{\epsilon^{-9}\chi^{\frac{3}{2}}}{\delta}\log\dfrac{1}{\epsilon\chi}\right)$. Our analysis is based on the key concept of dividing the parameter space $\mathbb{R}^p$ into three separate regions: non-stationary points, saddle points, and local optimal regions. We then make a local improvement of the RL objective in each region. This approach has the potential to be generalized to extensive policy gradient methods.",1
"Distributed machine learning (DML) techniques, such as federated learning, partitioned learning, and distributed reinforcement learning, have been increasingly applied to wireless communications. This is due to improved capabilities of terminal devices, explosively growing data volume, congestion in the radio interfaces, and increasing concern of data privacy. The unique features of wireless systems, such as large scale, geographically dispersed deployment, user mobility, and massive amount of data, give rise to new challenges in the design of DML techniques. There is a clear gap in the existing literature in that the DML techniques are yet to be systematically reviewed for their applicability to wireless systems. This survey bridges the gap by providing a contemporary and comprehensive survey of DML techniques with a focus on wireless networks. Specifically, we review the latest applications of DML in power control, spectrum management, user association, and edge cloud computing. The optimality, scalability, convergence rate, computation cost, and communication overhead of DML are analyzed. We also discuss the potential adversarial attacks faced by DML applications, and describe state-of-the-art countermeasures to preserve privacy and security. Last but not least, we point out a number of key issues yet to be addressed, and collate potentially interesting and challenging topics for future research.",0
"The use of distributed machine learning (DML) techniques in wireless communications has become increasingly prevalent due to advancements in terminal device capabilities, a significant increase in data volume, radio interface congestion, and a growing concern for data privacy. However, the unique features of wireless systems, such as their large scale, wide geographic distribution, user mobility, and vast amounts of data, present new challenges for DML technique design. Despite this, there is currently a lack of literature systematically reviewing the applicability of DML techniques in wireless systems. To address this gap, this survey offers a contemporary and comprehensive examination of DML techniques, with a focus on their use in wireless networks. Specifically, the latest DML applications in power control, spectrum management, user association, and edge cloud computing are reviewed, along with an analysis of their optimality, scalability, convergence rate, computation cost, and communication overhead. Additionally, potential adversarial attacks on DML applications and state-of-the-art countermeasures for privacy and security preservation are discussed. Finally, key issues yet to be addressed and potential topics for future research are identified.",1
"We prove under commonly used assumptions the convergence of actor-critic reinforcement learning algorithms, which simultaneously learn a policy function, the actor, and a value function, the critic. Both functions can be deep neural networks of arbitrary complexity. Our framework allows showing convergence of the well known Proximal Policy Optimization (PPO) and of the recently introduced RUDDER. For the convergence proof we employ recently introduced techniques from the two time-scale stochastic approximation theory. Our results are valid for actor-critic methods that use episodic samples and that have a policy that becomes more greedy during learning. Previous convergence proofs assume linear function approximation, cannot treat episodic examples, or do not consider that policies become greedy. The latter is relevant since optimal policies are typically deterministic.",0
"Our research establishes the convergence of actor-critic reinforcement learning algorithms, with the actor and critic functions being represented by deep neural networks of any level of complexity, given commonly used assumptions. Our framework accommodates both the widely recognized Proximal Policy Optimization (PPO) and the newly introduced RUDDER. We employ advanced techniques from the two-time scale stochastic approximation theory to demonstrate convergence. Our findings are applicable to episodic sample-based actor-critic methodologies with a policy that becomes increasingly greedy during the learning process. Previous convergence analyses were based on linear function approximation, could not handle episodic examples, or did not consider the policies' growing greediness, which is crucial because optimal policies are typically deterministic.",1
"Deep Reinforcement Learning (DRL) connects the classic Reinforcement Learning algorithms with Deep Neural Networks. A problem in DRL is that CNNs are black-boxes and it is hard to understand the decision-making process of agents. In order to be able to use RL agents in highly dangerous environments for humans and machines, the developer needs a debugging tool to assure that the agent does what is expected. Currently, rewards are primarily used to interpret how well an agent is learning. However, this can lead to deceptive conclusions if the agent receives more rewards by memorizing a policy and not learning to respond to the environment. In this work, it is shown that this problem can be recognized with the help of gradient visualization techniques. This work brings some of the best-known visualization methods from the field of image classification to the area of Deep Reinforcement Learning. Furthermore, two new visualization techniques have been developed, one of which provides particularly good results. It is being proven to what extent the algorithms can be used in the area of Reinforcement learning. Also, the question arises on how well the DRL algorithms can be visualized across different environments with varying visualization techniques.",0
"The integration of Deep Neural Networks with classic Reinforcement Learning algorithms is known as Deep Reinforcement Learning (DRL). However, using Convolutional Neural Networks (CNNs) in DRL can be problematic as they are opaque, making it difficult to comprehend how agents make decisions. To ensure that agents behave as expected in high-risk settings for both humans and machines, developers require debugging tools. Currently, rewards are the primary means of assessing an agent's learning progress, but this approach can be misleading if the agent merely memorizes a policy instead of learning to respond to its environment. This study proposes gradient visualization methods to identify this problem, bringing well-established image classification visualization techniques into the realm of DRL. Additionally, two new visualization methods have been developed, one of which has shown particularly promising results. The extent to which these algorithms can be applied to Reinforcement Learning, as well as the efficacy of various visualization techniques across different environments, are being investigated.",1
"In this paper, we consider jointly optimizing cell load balance and network throughput via a reinforcement learning (RL) approach, where inter-cell handover (i.e., user association assignment) and massive MIMO antenna tilting are configured as the RL policy to learn. Our rationale behind using RL is to circumvent the challenges of analytically modeling user mobility and network dynamics. To accomplish this joint optimization, we integrate vector rewards into the RL value network and conduct RL action via a separate policy network. We name this method as Pareto deterministic policy gradients (PDPG). It is an actor-critic, model-free and deterministic policy algorithm which can handle the coupling objectives with the following two merits: 1) It solves the optimization via leveraging the degree of freedom of vector reward as opposed to choosing handcrafted scalar-reward; 2) Cross-validation over multiple policies can be significantly reduced. Accordingly, the RL enabled network behaves in a self-organized way: It learns out the underlying user mobility through measurement history to proactively operate handover and antenna tilt without environment assumptions. Our numerical evaluation demonstrates that the introduced RL method outperforms scalar-reward based approaches. Meanwhile, to be self-contained, an ideal static optimization based brute-force search solver is included as a benchmark. The comparison shows that the RL approach performs as well as this ideal strategy, though the former one is constrained with limited environment observations and lower action frequency, whereas the latter ones have full access to the user mobility. The convergence of our introduced approach is also tested under different user mobility environment based on our measurement data from a real scenario.",0
"The focus of this paper is to optimize cell load balance and network throughput simultaneously using a reinforcement learning (RL) approach. Our RL policy consists of inter-cell handover and massive MIMO antenna tilting. The rationale behind using RL is to overcome the challenges of analytically modeling user mobility and network dynamics. To achieve this joint optimization, we incorporate vector rewards into the RL value network and conduct RL action through a separate policy network, which we call Pareto deterministic policy gradients (PDPG). This approach is a model-free and deterministic policy algorithm that addresses the coupling objectives and leverages the degree of freedom of vector reward as opposed to choosing handcrafted scalar-reward. Our numerical evaluation shows that the introduced RL method outperforms scalar-reward based approaches. Additionally, we include an ideal static optimization based brute-force search solver as a benchmark. The comparison shows that the RL approach performs as well as the ideal strategy, despite having limited environment observations and lower action frequency. The RL enabled network behaves in a self-organized way, learning underlying user mobility through measurement history to operate handover and antenna tilt proactively without environment assumptions. We also test the convergence of our introduced approach under different user mobility environments using real scenario measurement data.",1
"Deep reinforcement learning has shown remarkable success in the past few years. Highly complex sequential decision making problems have been solved in tasks such as game playing and robotics. Unfortunately, the sample complexity of most deep reinforcement learning methods is high, precluding their use in some important applications. Model-based reinforcement learning creates an explicit model of the environment dynamics to reduce the need for environment samples. Current deep learning methods use high-capacity networks to solve high-dimensional problems. Unfortunately, high-capacity models typically require many samples, negating the potential benefit of lower sample complexity in model-based methods. A challenge for deep model-based methods is therefore to achieve high predictive power while maintaining low sample complexity. In recent years, many model-based methods have been introduced to address this challenge. In this paper, we survey the contemporary model-based landscape. First we discuss definitions and relations to other fields. We propose a taxonomy based on three approaches: using explicit planning on given transitions, using explicit planning on learned transitions, and end-to-end learning of both planning and transitions. We use these approaches to organize a comprehensive overview of important recent developments such as latent models. We describe methods and benchmarks, and we suggest directions for future work for each of the approaches. Among promising research directions are curriculum learning, uncertainty modeling, and use of latent models for transfer learning.",0
"Over the past few years, deep reinforcement learning has achieved impressive results in solving highly complex sequential decision-making problems in game playing and robotics. However, most deep reinforcement learning methods require a high sample complexity, which limits their application in certain domains. To address this issue, model-based reinforcement learning has been introduced to reduce the need for environment samples by creating an explicit model of the environment dynamics. However, high-capacity models used in deep learning typically require many samples, which defeats the purpose of lower sample complexity in model-based methods. Therefore, the challenge for deep model-based methods is to achieve high predictive power while maintaining low sample complexity. This paper surveys the current landscape of model-based methods and proposes a taxonomy based on three approaches: explicit planning on given transitions, explicit planning on learned transitions, and end-to-end learning of both planning and transitions. The paper provides a comprehensive overview of recent developments, such as latent models, and suggests future research directions, including curriculum learning, uncertainty modeling, and the use of latent models for transfer learning.",1
"Current deep reinforcement learning (DRL) algorithms utilize randomness in simulation environments to assume complete coverage in the state space. However, particularly in high dimensions, relying on randomness may lead to gaps in coverage of the trained DRL neural network model, which in turn may lead to drastic and often fatal real-world situations. To the best of the author's knowledge, the assessment of coverage for DRL is lacking in current research literature. Therefore, in this paper, a novel measure, Approximate Pseudo-Coverage (APC), is proposed for assessing the coverage in DRL applications. We propose to calculate APC by projecting the high dimensional state space on to a lower dimensional manifold and quantifying the occupied space. Furthermore, we utilize an exploration-exploitation strategy for coverage maximization using Rapidly-Exploring Random Tree (RRT). The efficacy of the assessment and the acceleration of coverage is demonstrated on standard tasks such as Cartpole, highway-env.",0
"Deep reinforcement learning (DRL) algorithms currently rely on randomness in simulation environments to cover the state space. However, this approach may leave gaps in coverage, especially in high dimensions, which can lead to dangerous real-world scenarios. The lack of research on coverage assessment in DRL is a concern. Therefore, this paper proposes a novel measure called Approximate Pseudo-Coverage (APC) to evaluate coverage in DRL applications. The APC is calculated by projecting the high-dimensional state space onto a lower-dimensional manifold and measuring the occupied space. Additionally, an exploration-exploitation strategy is utilized for coverage maximization using Rapidly-Exploring Random Tree (RRT). The effectiveness of this approach is demonstrated on standard tasks such as Cartpole and highway-env.",1
"Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efficient learning methods. This is often attributed to their ability to distinguish between epistemic and aleatoric uncertainty. However, while most algorithms distinguish these two uncertainties for learning the model, they ignore it when optimizing the policy, which leads to greedy and insufficient exploration. At the same time, there are no practical solvers for optimistic exploration algorithms. In this paper, we propose a practical optimistic exploration algorithm (H-UCRL). H-UCRL reparameterizes the set of plausible models and hallucinates control directly on the epistemic uncertainty. By augmenting the input space with the hallucinated inputs, H-UCRL can be solved using standard greedy planners. Furthermore, we analyze H-UCRL and construct a general regret bound for well-calibrated models, which is provably sublinear in the case of Gaussian Process models. Based on this theoretical foundation, we show how optimistic exploration can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models. Our experiments demonstrate that optimistic exploration significantly speeds-up learning when there are penalties on actions, a setting that is notoriously difficult for existing model-based reinforcement learning algorithms.",0
"Probabilistic dynamical models in model-based reinforcement learning are known for their efficiency in data learning due to their capability to differentiate between epistemic and aleatoric uncertainty. However, while most algorithms distinguish these uncertainties when learning the model, they neglect it when optimizing the policy, resulting in insufficient exploration. Additionally, there are no practical solvers for optimistic exploration algorithms. To address this, we propose a practical optimistic exploration algorithm called H-UCRL, which reparameterizes plausible models and hallucinates control directly on epistemic uncertainty. By adding the hallucinated inputs to the input space, H-UCRL can be solved using standard greedy planners. We also provide a general regret bound for well-calibrated models, which is provably sublinear for Gaussian Process models. Our experiments show that H-UCRL can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models, and it significantly speeds up learning in penalty-based settings, which is challenging for existing model-based reinforcement learning algorithms.",1
"Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the development of challenging environments that test the limits of current methods. While existing RL environments are either sufficiently complex or based on fast simulation, they are rarely both. Here, we present the NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack. We argue that NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience. We compare NLE and its task suite to existing alternatives, and discuss why it is an ideal medium for testing the robustness and systematic generalization of RL agents. We demonstrate empirical success for early stages of the game using a distributed Deep RL baseline and Random Network Distillation exploration, alongside qualitative analysis of various agents trained in the environment. NLE is open source at https://github.com/facebookresearch/nle.",0
"The development of challenging environments that push the limits of current methods is essential for progress in Reinforcement Learning (RL) algorithms. However, existing RL environments are either too complex or rely on fast simulation, but not both. To address this issue, we introduce the NetHack Learning Environment (NLE), which is a scalable, stochastic, procedurally generated, challenging, and rich environment for RL research. The environment is based on the popular single-player terminal-based roguelike game, NetHack. We assert that NetHack is sufficiently complex to foster long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, and it reduces the computational resources required to gather a large amount of experience. In this article, we compare NLE and its task suite to existing alternatives and argue that it is the ideal medium for testing the robustness and systematic generalization of RL agents. Finally, we demonstrate how a distributed Deep RL baseline and Random Network Distillation exploration can achieve empirical success for early stages of the game, and we provide qualitative analysis of various agents trained in the environment. NLE is open source and available at https://github.com/facebookresearch/nle.",1
"The problem of Offline Policy Evaluation (OPE) in Reinforcement Learning (RL) is a critical step towards applying RL in real-life applications. Existing work on OPE mostly focus on evaluating a fixed target policy $\pi$, which does not provide useful bounds for offline policy learning as $\pi$ will then be data-dependent. We address this problem by simultaneously evaluating all policies in a policy class $\Pi$ -- uniform convergence in OPE -- and obtain nearly optimal error bounds for a number of global / local policy classes. Our results imply that the model-based planning achieves an optimal episode complexity of $\widetilde{O}(H^3/d_m\epsilon^2)$ in identifying an $\epsilon$-optimal policy under the time-inhomogeneous episodic MDP model ($H$ is the planning horizon, $d_m$ is a quantity that reflects the exploration of the logging policy $\mu$). To the best of our knowledge, this is the first time the optimal rate is shown to be possible for the offline RL setting and the paper is the first that systematically investigates the uniform convergence in OPE.",0
"The Offline Policy Evaluation (OPE) problem in Reinforcement Learning (RL) is crucial for implementing RL in practical applications. Current OPE research mainly focuses on evaluating a fixed target policy $\pi$, which is not useful for offline policy learning since $\pi$ depends on the data. To overcome this issue, we simultaneously evaluate all policies in a policy class $\Pi"" and achieve nearly optimal error bounds for various global/local policy classes through uniform convergence in OPE. Our findings indicate that model-based planning can identify an $\epsilon$-optimal policy under the time-inhomogeneous episodic MDP model with an optimal episode complexity of $\widetilde{O}(H^3/d_m\epsilon^2)$ ($H$ represents the planning horizon, $d_m$ reflects the exploration of the logging policy $\mu$). This is the first time that the optimal rate has been demonstrated to be possible in the offline RL setting, and our paper is the first to systematically investigate uniform convergence in OPE.",1
"Inverse reinforcement learning (IRL) is a common technique for inferring human preferences from data. Standard IRL techniques tend to assume that the human demonstrator is stationary, that is that their policy $\pi$ doesn't change over time. In practice, humans interacting with a novel environment or performing well on a novel task will change their demonstrations as they learn more about the environment or task. We investigate the consequences of relaxing this assumption of stationarity, in particular by modelling the human as learning. Surprisingly, we find in some small examples that this can lead to better inference than if the human was stationary. That is, by observing a demonstrator who is themselves learning, a machine can infer more than by observing a demonstrator who is noisily rational. In addition, we find evidence that misspecification can lead to poor inference, suggesting that modelling human learning is important, especially when the human is facing an unfamiliar environment.",0
"Inverse reinforcement learning (IRL) is a widely used method to deduce human preferences from data. However, traditional IRL techniques assume that the human demonstrator's policy remains unchanged over time, which is not the case in reality. Humans tend to alter their demonstrations as they gain more knowledge about a new environment or task. To address this issue, we explore the implications of relaxing this assumption by modeling the human as a learner. Surprisingly, our findings reveal that this approach can result in better inference compared to assuming a stationary demonstrator. In fact, observing a learning demonstrator can provide a machine with more information than a noisy rational one. Moreover, we also discovered that poor inference can result from misrepresenting the human's learning process, especially when dealing with an unfamiliar environment.",1
"Motivated by the prevailing paradigm of using unsupervised learning for efficient exploration in reinforcement learning (RL) problems [tang2017exploration,bellemare2016unifying], we investigate when this paradigm is provably efficient. We study episodic Markov decision processes with rich observations generated from a small number of latent states. We present a general algorithmic framework that is built upon two components: an unsupervised learning algorithm and a no-regret tabular RL algorithm. Theoretically, we prove that as long as the unsupervised learning algorithm enjoys a polynomial sample complexity guarantee, we can find a near-optimal policy with sample complexity polynomial in the number of latent states, which is significantly smaller than the number of observations. Empirically, we instantiate our framework on a class of hard exploration problems to demonstrate the practicality of our theory.",0
"Our investigation is inspired by the current trend of utilizing unsupervised learning to enhance exploration in reinforcement learning (RL) problems [tang2017exploration,bellemare2016unifying]. We aim to determine the conditions under which this approach is truly effective. Our focus is on episodic Markov decision processes that generate complex observations from a limited number of hidden states. We introduce a universal algorithmic structure that comprises two main components: an unsupervised learning algorithm and a no-regret tabular RL algorithm. We prove that if the unsupervised learning algorithm has a polynomial sample complexity guarantee, we can attain a nearly optimal policy with a sample complexity that is polynomial in the number of latent states. This is significantly lower than the number of observations. To demonstrate the practicality of our theory, we apply our framework to a set of difficult exploration problems.",1
"Model-free reinforcement learning (RL) is a powerful tool to learn a broad range of robot skills and policies. However, a lack of policy interpretability can inhibit their successful deployment in downstream applications, particularly when differences in environmental conditions may result in unpredictable behaviour or generalisation failures. As a result, there has been a growing emphasis in machine learning around the inclusion of stronger inductive biases in models to improve generalisation. This paper proposes an alternative strategy, inverse value estimation for interpretable policy certificates (IV-Posterior), which seeks to identify the inductive biases or idealised conditions of operation already held by pre-trained policies, and then use this information to guide their deployment. IV-Posterior uses MaskedAutoregressive Flows to fit distributions over the set of conditions or environmental parameters in which a policy is likely to be effective. This distribution can then be used as a policy certificate in downstream applications. We illustrate the use of IV-Posterior across a two environments, and show that substantial performance gains can be obtained when policy selection incorporates knowledge of the inductive biases that these policies hold.",0
"Although model-free reinforcement learning (RL) is a potent tool for acquiring a diverse range of robot skills and policies, it can be challenging to deploy in downstream applications due to a lack of policy interpretability. This is particularly problematic when inconsistencies in environmental conditions result in unpredictable behavior or generalization failures. Consequently, there has been a growing emphasis in machine learning on incorporating stronger inductive biases in models to enhance generalization. This paper proposes an alternative approach called inverse value estimation for interpretable policy certificates (IV-Posterior), which aims to identify the inductive biases or idealized operating conditions of pre-trained policies to guide their deployment. IV-Posterior leverages Masked Autoregressive Flows to fit distributions over the conditions or environmental parameters in which a policy is likely to be effective. This distribution can then serve as a policy certificate in downstream applications. We demonstrate the effectiveness of IV-Posterior in two environments and show that incorporating knowledge of the inductive biases held by these policies leads to significant performance improvements.",1
"A very successful model for simulating emergency evacuation is the social-force model. At the heart of the model is the self-driven force that is applied to an agent and is directed towards the exit. However, it is not clear if the application of this force results in optimal evacuation, especially in complex environments with obstacles. Here, we develop a deep reinforcement learning algorithm in association with the social force model to train agents to find the fastest evacuation path. During training, we penalize every step of an agent in the room and give zero reward at the exit. We adopt the Dyna-Q learning approach. We first show that in the case of a room without obstacles the resulting self-driven force points directly towards the exit as in the social force model and that the median exit time intervals calculated using the two methods are not significantly different. Then, we investigate evacuation of a room with one obstacle and one exit. We show that our method produces similar results with the social force model when the obstacle is convex. However, in the case of concave obstacles, which sometimes can act as traps for agents governed purely by the social force model and prohibit complete room evacuation, our approach is clearly advantageous since it derives a policy that results in object avoidance and complete room evacuation without additional assumptions. We also study evacuation of a room with multiple exits. We show that agents are able to evacuate efficiently from the nearest exit through a shared network trained for a single agent. Finally, we test the robustness of the Dyna-Q learning approach in a complex environment with multiple exits and obstacles. Overall, we show that our model can efficiently simulate emergency evacuation in complex environments with multiple room exits and obstacles where it is difficult to obtain an intuitive rule for fast evacuation.",0
"The social-force model is a successful method to simulate emergency evacuation, utilizing self-driven force directed towards the exit. However, it is uncertain if this method is optimal in complex environments with obstructions. To address this issue, we introduce a deep reinforcement learning algorithm in conjunction with the social force model to train agents in finding the quickest evacuation path. During training, we impose penalties for every step taken by the agent in the room, with zero reward at the exit. Our approach employs the Dyna-Q learning technique. We first demonstrate that in a room without obstructions, the self-driven force points towards the exit, similar to the social force model, and the exit time intervals calculated using both methods are comparable. We then examine the evacuation of a room with one obstacle and one exit and observe that our method produces similar results to the social force model for convex obstacles. However, for concave obstacles, our approach is more effective in avoiding objects and evacuating the entire room without additional assumptions. We further investigate multiple exits in a room and show that agents can evacuate efficiently through a shared network trained for a single agent. Lastly, we test the robustness of our model in a complex environment with multiple exits and obstacles, demonstrating its effectiveness in simulating emergency evacuation in challenging scenarios where intuitive rules for fast evacuation are hard to obtain.",1
"Continuously learning to solve unseen tasks with limited experience has been extensively pursued in meta-learning and continual learning, but with restricted assumptions such as accessible task distributions, independently and identically distributed tasks, and clear task delineations. However, real-world physical tasks frequently violate these assumptions, resulting in performance degradation. This paper proposes a continual online model-based reinforcement learning approach that does not require pre-training to solve task-agnostic problems with unknown task boundaries. We maintain a mixture of experts to handle nonstationarity, and represent each different type of dynamics with a Gaussian Process to efficiently leverage collected data and expressively model uncertainty. We propose a transition prior to account for the temporal dependencies in streaming data and update the mixture online via sequential variational inference. Our approach reliably handles the task distribution shift by generating new models for never-before-seen dynamics and reusing old models for previously seen dynamics. In experiments, our approach outperforms alternative methods in non-stationary tasks, including classic control with changing dynamics and decision making in different driving scenarios.",0
"The pursuit of continuously learning to solve new tasks with limited experience has been a focus in meta-learning and continual learning. However, these approaches have been limited by assumptions such as accessible task distributions, independently and identically distributed tasks, and clear task delineations which do not hold true for real-world physical tasks. This often results in performance degradation. To address this, a continual online model-based reinforcement learning approach has been proposed which does not require pre-training and can solve task-agnostic problems with unknown task boundaries. Nonstationarity is handled by maintaining a mixture of experts and representing each type of dynamics with a Gaussian Process. A transition prior is proposed to account for temporal dependencies in streaming data and the mixture is updated online via sequential variational inference. This approach can generate new models for never-before-seen dynamics and reuse old models for previously seen dynamics, thus reliably handling task distribution shifts. Experiments have shown that this approach outperforms alternative methods in non-stationary tasks, such as classic control with changing dynamics and decision making in different driving scenarios.",1
"Competitive Self-Play (CSP) based Multi-Agent Reinforcement Learning (MARL) has shown phenomenal breakthroughs recently. Strong AIs are achieved for several benchmarks, including Dota 2, Glory of Kings, Quake III, StarCraft II, to name a few. Despite the success, the MARL training is extremely data thirsty, requiring typically billions of (if not trillions of) frames be seen from the environment during training in order for learning a high performance agent. This poses non-trivial difficulties for researchers or engineers and prevents the application of MARL to a broader range of real-world problems. To address this issue, in this manuscript we describe a framework, referred to as TLeague, that aims at large-scale training and implements several main-stream CSP-MARL algorithms. The training can be deployed in either a single machine or a cluster of hybrid machines (CPUs and GPUs), where the standard Kubernetes is supported in a cloud native manner. TLeague achieves a high throughput and a reasonable scale-up when performing distributed training. Thanks to the modular design, it is also easy to extend for solving other multi-agent problems or implementing and verifying MARL algorithms. We present experiments over StarCraft II, ViZDoom and Pommerman to show the efficiency and effectiveness of TLeague. The code is open-sourced and available at https://github.com/tencent-ailab/tleague_projpage",0
"In recent times, Competitive Self-Play (CSP) based Multi-Agent Reinforcement Learning (MARL) has made remarkable progress, achieving strong Artificial Intelligences (AIs) for various benchmarks such as Dota 2, Glory of Kings, Quake III and StarCraft II. However, the training process for MARL is highly data-intensive, requiring billions (or trillions) of frames to be seen from the environment during training to develop a high-performance agent. This poses significant challenges for researchers and engineers and limits the application of MARL to real-world problems. To tackle this issue, the TLeague framework has been introduced in this paper. This framework implements various CSP-MARL algorithms and aims to facilitate large-scale training, which can be deployed on a single machine or a cluster of hybrid machines (CPUs and GPUs) using the standard Kubernetes in a cloud-native manner. TLeague ensures high throughput and scalability during distributed training and can be easily extended to solve other multi-agent problems or implement and verify MARL algorithms. The efficacy and efficiency of TLeague are demonstrated through experiments over StarCraft II, ViZDoom and Pommerman. The code for TLeague is open-sourced and available at https://github.com/tencent-ailab/tleague_projpage.",1
"Autonomous agents need large repertoires of skills to act reasonably on new tasks that they have not seen before. However, acquiring these skills using only a stream of high-dimensional, unstructured, and unlabeled observations is a tricky challenge for any autonomous agent. Previous methods have used variational autoencoders to encode a scene into a low-dimensional vector that can be used as a goal for an agent to discover new skills. Nevertheless, in compositional/multi-object environments it is difficult to disentangle all the factors of variation into such a fixed-length representation of the whole scene. We propose to use object-centric representations as a modular and structured observation space, which is learned with a compositional generative world model. We show that the structure in the representations in combination with goal-conditioned attention policies helps the autonomous agent to discover and learn useful skills. These skills can be further combined to address compositional tasks like the manipulation of several different objects.",0
"It is essential for autonomous agents to possess a vast range of skills to perform well on unfamiliar tasks. However, mastering these skills using only a continuous flow of unstructured, unlabelled, and high-dimensional observations is a challenging feat for autonomous agents. Prior methods have employed variational autoencoders to transform a scene into a low-dimensional vector, which can serve as a target for agents to develop new skills. Yet, in multi-object environments with complex compositions, it is problematic to break down all the variables of variation into a fixed-length representation of the entire scenery. To solve this problem, we introduce object-centric representations as a structured and modular observation space, which is learned through a compositional generative world model. Our findings reveal that the structure in the representations, combined with goal-driven attention policies, enables autonomous agents to acquire and master useful skills. These capabilities can be further integrated to handle composite duties, such as manipulating various objects.",1
"Offline Reinforcement Learning (RL) aims to turn large datasets into powerful decision-making engines without any online interactions with the environment. This great promise has motivated a large amount of research that hopes to replicate the success RL has experienced in simulation settings. This work ambitions to reflect upon these efforts from a practitioner viewpoint. We start by discussing the dataset properties that we hypothesise can characterise the type of offline methods that will be the most successful. We then verify these claims through a set of experiments and designed datasets generated from environments with both discrete and continuous action spaces. We experimentally validate that diversity and high-return examples in the data are crucial to the success of offline RL and show that behavioural cloning remains a strong contender compared to its contemporaries. Overall, this work stands as a tutorial to help people build their intuition on today's offline RL methods and their applicability.",0
"The objective of Offline Reinforcement Learning (RL) is to create strong decision-making systems by utilizing vast datasets without any interaction with the environment. This has led to considerable research efforts to replicate the success of RL in simulated environments. This paper assesses these efforts from a practitioner's perspective. Firstly, we identify the dataset features that we believe are most important for successful offline methods. Next, we conduct experiments on various datasets generated from environments with discrete and continuous action spaces to validate our claims. Our findings demonstrate that diversity and high-return examples are crucial for the success of offline RL, and that behavioural cloning remains a strong contender. This paper serves as a tutorial to help individuals understand modern offline RL methods and their practical applications.",1
"Off-policy evaluation is a key component of reinforcement learning which evaluates a target policy with offline data collected from behavior policies. It is a crucial step towards safe reinforcement learning and has been used in advertisement, recommender systems and many other applications. In these applications, sometimes the offline data is collected from multiple behavior policies. Previous works regard data from different behavior policies equally. Nevertheless, some behavior policies are better at producing good estimators while others are not. This paper starts with discussing how to correctly mix estimators produced by different behavior policies. We propose three ways to reduce the variance of the mixture estimator when all sub-estimators are unbiased or asymptotically unbiased. Furthermore, experiments on simulated recommender systems show that our methods are effective in reducing the Mean-Square Error of estimation.",0
"Reinforcement learning relies heavily on off-policy evaluation, which involves assessing a target policy using offline data gathered from behavior policies. This is a crucial aspect of ensuring safe reinforcement learning and has practical applications in advertising, recommender systems, and other areas. In some cases, the data may come from multiple behavior policies, but previous methods have treated all data sources equally, despite some policies being better at producing accurate estimations. This paper proposes three new methods for correctly combining estimators from different behavior policies to reduce the variance of the mixture estimator, particularly when all sub-estimators are unbiased or asymptotically unbiased. Testing on simulated recommender systems indicates that these methods effectively reduce the Mean-Square Error of estimation.",1
"We introduce DREAM, a deep reinforcement learning algorithm that finds optimal strategies in imperfect-information games with multiple agents. Formally, DREAM converges to a Nash Equilibrium in two-player zero-sum games and to an extensive-form coarse correlated equilibrium in all other games. Our primary innovation is an effective algorithm that, in contrast to other regret-based deep learning algorithms, does not require access to a perfect simulator of the game to achieve good performance. We show that DREAM empirically achieves state-of-the-art performance among model-free algorithms in popular benchmark games, and is even competitive with algorithms that do use a perfect simulator.",0
"DREAM is a novel deep reinforcement learning algorithm designed to identify optimal strategies in games with multiple agents and imperfect information. It can converge to a Nash Equilibrium in two-player zero-sum games and to an extensive-form coarse correlated equilibrium in all other games. The key innovation of DREAM is its efficient algorithm, which does not rely on a perfect game simulator to achieve superior performance, unlike other regret-based deep learning algorithms. Our experiments demonstrate that DREAM outperforms other model-free algorithms in various benchmark games and can even compete with algorithms that use a perfect simulator.",1
"The empirical success of Multi-agent reinforcement learning is encouraging, while few theoretical guarantees have been revealed. In this work, we prove that the plug-in solver approach, probably the most natural reinforcement learning algorithm, achieves minimax sample complexity for turn-based stochastic game (TBSG). Specifically, we plan in an empirical TBSG by utilizing a `simulator' that allows sampling from arbitrary state-action pair. We show that the empirical Nash equilibrium strategy is an approximate Nash equilibrium strategy in the true TBSG and give both problem-dependent and problem-independent bound. We develop absorbing TBSG and reward perturbation techniques to tackle the complex statistical dependence. The key idea is artificially introducing a suboptimality gap in TBSG and then the Nash equilibrium strategy lies in a finite set.",0
"Despite the empirical success of Multi-agent reinforcement learning, there are limited theoretical guarantees available. This study aims to prove that the plug-in solver approach, which is a commonly used reinforcement learning algorithm, achieves minimax sample complexity for turn-based stochastic games (TBSG). To do so, we utilize a simulator that allows for sampling from arbitrary state-action pairs in an empirical TBSG. Our research shows that the empirical Nash equilibrium strategy is an approximate Nash equilibrium strategy in the true TBSG, and we provide both problem-dependent and problem-independent bounds. We also introduce absorbing TBSG and reward perturbation techniques to address the complex statistical dependence. The fundamental concept is to artificially introduce a suboptimality gap in TBSG, resulting in the Nash equilibrium strategy lying in a finite set.",1
"Meta-learning has proven to be successful for few-shot learning across the regression, classification, and reinforcement learning paradigms. Recent approaches have adopted Bayesian interpretations to improve gradient-based meta-learners by quantifying the uncertainty of the post-adaptation estimates. Most of these works almost completely ignore the latent relationship between the covariate distribution $(p(x))$ of a task and the corresponding conditional distribution $p(y|x)$. In this paper, we identify the need to explicitly model the meta-distribution over the task covariates in a hierarchical Bayesian framework. We begin by introducing a graphical model that leverages the samples from the marginal $p(x)$ to better infer the posterior over the optimal parameters of the conditional distribution $(p(y|x))$ for each task. Based on this model we propose a computationally feasible meta-learning algorithm by introducing meaningful relaxations in our final objective. We demonstrate the gains of our algorithm over initialization based meta-learning baselines on popular classification benchmarks. Finally, to understand the potential benefit of modeling task covariates we further evaluate our method on a synthetic regression dataset.",0
"Few-shot learning has benefited from meta-learning, which has been successful across various paradigms like regression, classification, and reinforcement learning. Recent approaches have incorporated Bayesian interpretations to enhance gradient-based meta-learners by measuring the uncertainty of post-adaptation estimates. However, these approaches have disregarded the latent relationship between the covariate distribution (p(x)) of a task and the corresponding conditional distribution p(y|x). This paper addresses this issue by proposing a hierarchical Bayesian framework that explicitly models the meta-distribution over the task covariates. We introduce a graphical model that uses samples from the marginal p(x) to improve the posterior estimation of optimal parameters of the conditional distribution (p(y|x)) for each task. We propose a computationally feasible meta-learning algorithm that relaxes the final objective. We demonstrate the effectiveness of our algorithm on popular classification benchmarks and a synthetic regression dataset.",1
"Self-supervised learning and data augmentation have significantly reduced the performance gap between state and image-based reinforcement learning agents in continuous control tasks. However, it is still unclear whether current techniques can face a variety of visual conditions required by real-world environments. We propose a challenging benchmark that tests agents' visual generalization by adding graphical variety to existing continuous control domains. Our empirical analysis shows that current methods struggle to generalize across a diverse set of visual changes, and we examine the specific factors of variation that make these tasks difficult. We find that data augmentation techniques outperform self-supervised learning approaches and that more significant image transformations provide better visual generalization \footnote{The benchmark and our augmented actor-critic implementation are open-sourced @ https://github.com/QData/dmc_remastered)",0
"The use of self-supervised learning and data augmentation has resulted in a significant reduction in the disparity of performance between state and image-based reinforcement learning agents in continuous control tasks. However, it remains unclear whether these techniques are equipped to handle the numerous visual conditions that real-world environments demand. To address this, we propose a challenging benchmark that evaluates agents' visual generalization by incorporating graphical diversity into existing continuous control domains. Our empirical analysis demonstrates that current methods struggle to generalize across a wide range of visual changes, and we investigate the specific factors of variation that complicate these tasks. We discover that data augmentation techniques are more effective than self-supervised learning approaches, and that more extensive image transformations lead to better visual generalization. (Our benchmark and augmented actor-critic implementation are available on Github @ https://github.com/QData/dmc_remastered)",1
"While improvements in deep learning architectures have played a crucial role in improving the state of supervised and unsupervised learning in computer vision and natural language processing, neural network architecture choices for reinforcement learning remain relatively under-explored. We take inspiration from successful architectural choices in computer vision and generative modelling, and investigate the use of deeper networks and dense connections for reinforcement learning on a variety of simulated robotic learning benchmark environments. Our findings reveal that current methods benefit significantly from dense connections and deeper networks, across a suite of manipulation and locomotion tasks, for both proprioceptive and image-based observations. We hope that our results can serve as a strong baseline and further motivate future research into neural network architectures for reinforcement learning. The project website with code is at this link https://sites.google.com/view/d2rl/home.",0
"Although deep learning architectures have greatly improved the state of supervised and unsupervised learning in computer vision and natural language processing, there has been little exploration of neural network architecture choices for reinforcement learning. Our study draws inspiration from successful architectural choices in computer vision and generative modelling, and investigates the effectiveness of deeper networks and dense connections for reinforcement learning across a variety of simulated robotic learning benchmark environments. Our research indicates that dense connections and deeper networks are highly beneficial for current methods, including both proprioceptive and image-based observations, in a range of manipulation and locomotion tasks. We hope that our findings can serve as a solid foundation for further research into neural network architectures for reinforcement learning and provide access to our project website and code at this link: https://sites.google.com/view/d2rl/home.",1
"Behavior cloning (BC) is often practical for robot learning because it allows a policy to be trained offline without rewards, by supervised learning on expert demonstrations. However, BC does not effectively leverage what we will refer to as unlabeled experience: data of mixed and unknown quality without reward annotations. This unlabeled data can be generated by a variety of sources such as human teleoperation, scripted policies and other agents on the same robot. Towards data-driven offline robot learning that can use this unlabeled experience, we introduce Offline Reinforced Imitation Learning (ORIL). ORIL first learns a reward function by contrasting observations from demonstrator and unlabeled trajectories, then annotates all data with the learned reward, and finally trains an agent via offline reinforcement learning. Across a diverse set of continuous control and simulated robotic manipulation tasks, we show that ORIL consistently outperforms comparable BC agents by effectively leveraging unlabeled experience.",0
"Robot learning can benefit from Behavior cloning (BC) as it enables the training of a policy without rewards through supervised learning on expert demonstrations. However, BC fails to make the most of unlabeled experience, which refers to data of mixed and unknown quality without reward annotations. This type of data can be obtained through various sources such as human teleoperation, scripted policies, and other agents on the same robot. To address this issue, we propose Offline Reinforced Imitation Learning (ORIL) for data-driven offline robot learning. ORIL first learns a reward function by comparing observations from the demonstrator and unlabeled trajectories, then annotates all data with the learned reward, and finally trains an agent through offline reinforcement learning. Our experiments across a diverse set of continuous control and simulated robotic manipulation tasks demonstrate that ORIL consistently outperforms comparable BC agents by effectively leveraging unlabeled experience.",1
"Deploying Reinforcement Learning (RL) agents to solve real-world applications often requires satisfying complex system constraints. Often the constraint thresholds are incorrectly set due to the complex nature of a system or the inability to verify the thresholds offline (e.g, no simulator or reasonable offline evaluation procedure exists). This results in solutions where a task cannot be solved without violating the constraints. However, in many real-world cases, constraint violations are undesirable yet they are not catastrophic, motivating the need for soft-constrained RL approaches. We present a soft-constrained RL approach that utilizes meta-gradients to find a good trade-off between expected return and minimizing constraint violations. We demonstrate the effectiveness of this approach by showing that it consistently outperforms the baselines across four different MuJoCo domains.",0
"To apply Reinforcement Learning (RL) agents to real-world scenarios, intricate system limitations must be taken into account. These constraints are often inaccurately established due to the intricacy of a system or the absence of a simulator or offline evaluation method to verify the thresholds. As a result, solutions are generated where a task cannot be accomplished without breaching the constraints. However, in many real-life situations, violations of constraints are not ideal but are not catastrophic, warranting the requirement for soft-constrained RL techniques. In this study, we present a soft-constrained RL approach that employs meta-gradients to obtain an optimal balance between anticipated returns and reducing constraint violations. We validate the effectiveness of this approach by demonstrating that it consistently outperforms the baseline techniques in four distinct MuJoCo domains.",1
"Unmanned Aerial Vehicles (UAVs) have attracted considerable research interest recently. Especially when it comes to the realm of Internet of Things, the UAVs with Internet connectivity are one of the main demands. Furthermore, the energy constraint i.e. battery limit is a bottle-neck of the UAVs that can limit their applications. We try to address and solve the energy problem. Therefore, a path planning method for a cellular-connected UAV is proposed that will enable the UAV to plan its path in an area much larger than its battery range by getting recharged in certain positions equipped with power stations (PSs). In addition to the energy constraint, there are also no-fly zones; for example, due to Air to Air (A2A) and Air to Ground (A2G) interference or for lack of necessary connectivity that impose extra constraints in the trajectory optimization of the UAV. No-fly zones determine the infeasible areas that should be avoided. We have used a reinforcement learning (RL) hierarchically to extend typical short-range path planners to consider battery recharge and solve the problem of UAVs in long missions. The problem is simulated for the UAV that flies over a large area, and Q-learning algorithm could enable the UAV to find the optimal path and recharge policy.",0
"Recently, there has been significant research interest in Unmanned Aerial Vehicles (UAVs), particularly with regards to their potential use in the Internet of Things. However, one major limitation of UAVs is their battery life, which restricts their applications. To address this issue, we propose a path planning method for cellular-connected UAVs that incorporates recharge points equipped with power stations (PSs). Additionally, there are no-fly zones that must be avoided due to Air to Air (A2A) and Air to Ground (A2G) interference or lack of necessary connectivity. To optimize the trajectory of the UAV, we use a reinforcement learning (RL) algorithm that considers battery recharge and extends short-range path planners for long missions. Through simulation, the Q-learning algorithm enables the UAV to identify the optimal path and recharge policy.",1
"Reinforcement learning has recently experienced increased prominence in the machine learning community. There are many approaches to solving reinforcement learning problems with new techniques developed constantly. When solving problems using reinforcement learning, there are various difficult challenges to overcome. To ensure progress in the field, benchmarks are important for testing new algorithms and comparing with other approaches. The reproducibility of results for fair comparison is therefore vital in ensuring that improvements are accurately judged. This paper provides an overview of different contributions to reinforcement learning benchmarking and discusses how they can assist researchers to address the challenges facing reinforcement learning. The contributions discussed are the most used and recent in the literature. The paper discusses the contributions in terms of implementation, tasks and provided algorithm implementations with benchmarks. The survey aims to bring attention to the wide range of reinforcement learning benchmarking tasks available and to encourage research to take place in a standardised manner. Additionally, this survey acts as an overview for researchers not familiar with the different tasks that can be used to develop and test new reinforcement learning algorithms.",0
"Reinforcement learning has gained significant prominence in the machine learning community recently, with a multitude of constantly evolving techniques being developed to solve various problems using this approach. However, there are several challenging obstacles to overcome while implementing reinforcement learning, and benchmarks play a crucial role in ensuring progress in the field. To accurately judge the effectiveness of new algorithms and compare them with other approaches, it is vital to ensure the reproducibility of results. This paper offers an overview of the most recent and commonly used contributions to reinforcement learning benchmarking, discussing their implementation, tasks, and algorithm implementations with benchmarks. The survey aims to highlight the vast range of reinforcement learning benchmarking tasks available and encourage standardized research practices. Additionally, it serves as a useful reference for researchers unfamiliar with the various tasks that can be used to develop and test new reinforcement learning algorithms.",1
"Recent advances in deep Reinforcement Learning (RL) have created unprecedented opportunities for intelligent automation, where a machine can autonomously learn an optimal policy for performing a given task. However, current deep RL algorithms predominantly specialize in a narrow range of tasks, are sample inefficient, and lack sufficient stability, which in turn hinder their industrial adoption. This article tackles this limitation by developing and testing a Hyper-Actor Soft Actor-Critic (HASAC) RL framework based on the notions of task modularization and transfer learning. The goal of the proposed HASAC is to enhance the adaptability of an agent to new tasks by transferring the learned policies of former tasks to the new task via a ""hyper-actor"". The HASAC framework is tested on a new virtual robotic manipulation benchmark, Meta-World. Numerical experiments show superior performance by HASAC over state-of-the-art deep RL algorithms in terms of reward value, success rate, and task completion time.",0
"There have been recent breakthroughs in deep Reinforcement Learning (RL) that have opened up new possibilities for intelligent automation, where a machine can learn on its own the best way to perform a given task. Unfortunately, current deep RL algorithms are limited in their ability to handle a wide range of tasks, they require a lot of data to work effectively, and they lack stability, making them unsuitable for industrial use. To address these issues, this article introduces the Hyper-Actor Soft Actor-Critic (HASAC) RL framework, which is based on the principles of task modularization and transfer learning. The goal of HASAC is to make agents more adaptable to new tasks by using a ""hyper-actor"" to transfer knowledge from previously learned policies to new tasks. To test the effectiveness of HASAC, it was applied to a new virtual robotic manipulation benchmark called Meta-World. Numerical experiments revealed that HASAC outperforms other deep RL algorithms in terms of reward value, success rate, and task completion time.",1
"Network seeding for efficient information diffusion over time-varying graphs~(TVGs) is a challenging task with many real-world applications. There are several ways to model this spatio-temporal influence maximization problem, but the ultimate goal is to determine the best moment for a node to start the diffusion process. In this context, we propose Spatio-Temporal Influence Maximization~(STIM), a model trained with Reinforcement Learning and Graph Embedding over a set of artificial TVGs that is capable of learning the temporal behavior and connectivity pattern of each node, allowing it to predict the best moment to start a diffusion through the TVG. We also develop a special set of artificial TVGs used for training that simulate a stochastic diffusion process in TVGs, showing that the STIM network can learn an efficient policy even over a non-deterministic environment. STIM is also evaluated with a real-world TVG, where it also manages to efficiently propagate information through the nodes. Finally, we also show that the STIM model has a time complexity of $O(|E|)$. STIM, therefore, presents a novel approach for efficient information diffusion in TVGs, being highly versatile, where one can change the goal of the model by simply changing the adopted reward function.",0
"Efficiently diffusing information over time-varying graphs poses several challenges and has many practical applications. While there are various ways to model this spatio-temporal influence maximization problem, the main objective is to determine the optimal moment for a node to initiate the diffusion process. To address this issue, we introduce Spatio-Temporal Influence Maximization (STIM), a model that leverages Reinforcement Learning and Graph Embedding techniques to train on a set of artificial TVGs. STIM can learn the temporal behavior and connectivity pattern of each node, enabling it to predict the best moment to start a diffusion across the TVG. We create a specialized set of artificial TVGs for training, simulating a stochastic diffusion process in TVGs, demonstrating that STIM can effectively learn an efficient policy even in a non-deterministic environment. We further evaluate STIM with real-world TVGs, where it propagates information through the nodes efficiently. Moreover, we demonstrate that STIM has a time complexity of O(|E|). Thus, STIM offers a novel approach for efficient information diffusion in TVGs, with high versatility, where the model's goal can be altered by merely changing the adopted reward function.",1
"This chapter presents an overview of Interactive Machine Learning (IML) techniques applied to the analysis and design of musical gestures. We go through the main challenges and needs related to capturing, analysing, and applying IML techniques to human bodily gestures with the purpose of performing with sound synthesis systems. We discuss how different algorithms may be used to accomplish different tasks, including interacting with complex synthesis techniques and exploring interaction possibilities by means of Reinforcement Learning (RL) in an interaction paradigm we developed called Assisted Interactive Machine Learning (AIML). We conclude the chapter with a description of how some of these techniques were employed by the authors for the development of four musical pieces, thus outlining the implications that IML have for musical practice.",0
"In this chapter, the use of Interactive Machine Learning (IML) techniques for the analysis and design of musical gestures is presented. The challenges and requirements associated with capturing, analyzing, and employing IML techniques for human bodily gestures to perform with sound synthesis systems are discussed. Various algorithms are explored, which can accomplish different tasks, such as interacting with complex synthesis techniques and exploring interaction possibilities through Reinforcement Learning (RL) in an interaction paradigm called Assisted Interactive Machine Learning (AIML). The chapter concludes with an explanation of how the authors utilized some of these techniques to develop four musical pieces, highlighting the implications that IML has for musical practice.",1
"We present GradientDICE for estimating the density ratio between the state distribution of the target policy and the sampling distribution in off-policy reinforcement learning. GradientDICE fixes several problems of GenDICE (Zhang et al., 2020), the state-of-the-art for estimating such density ratios. Namely, the optimization problem in GenDICE is not a convex-concave saddle-point problem once nonlinearity in optimization variable parameterization is introduced to ensure positivity, so any primal-dual algorithm is not guaranteed to converge or find the desired solution. However, such nonlinearity is essential to ensure the consistency of GenDICE even with a tabular representation. This is a fundamental contradiction, resulting from GenDICE's original formulation of the optimization problem. In GradientDICE, we optimize a different objective from GenDICE by using the Perron-Frobenius theorem and eliminating GenDICE's use of divergence. Consequently, nonlinearity in parameterization is not necessary for GradientDICE, which is provably convergent under linear function approximation.",0
"GradientDICE is introduced as a method for estimating the density ratio between the state distributions of the target policy and the sampling distribution in off-policy reinforcement learning. It aims to address issues with the state-of-the-art approach, GenDICE, which experiences problems with optimization when introducing nonlinearity in parameterization to ensure positivity. This issue arises due to a fundamental contradiction in GenDICE's original formulation of the optimization problem and can cause difficulties with convergence and finding the desired solution. To overcome this, GradientDICE uses the Perron-Frobenius theorem and eliminates the use of divergence, allowing it to optimize a different objective from GenDICE. As a result, nonlinearity in parameterization is not necessary for GradientDICE, and it is shown to be provably convergent under linear function approximation.",1
"Nature provides a way to understand physics with reinforcement learning since nature favors the economical way for an object to propagate. In the case of classical mechanics, nature favors the object to move along the path according to the integral of the Lagrangian, called the action $\mathcal{S}$. We consider setting the reward/penalty as a function of $\mathcal{S}$, so the agent could learn the physical trajectory of particles in various kinds of environments with reinforcement learning. In this work, we verified the idea by using a Q-Learning based algorithm on learning how light propagates in materials with different refraction indices, and show that the agent could recover the minimal-time path equivalent to the solution obtained by Snell's law or Fermat's Principle. We also discuss the similarity of our reinforcement learning approach to the path integral formalism.",0
"Reinforcement learning can be applied to understand physics through the principles of nature, which prioritize the most efficient propagation of an object. Classical mechanics, for example, follows the integral of the Lagrangian, known as the action $\mathcal{S}$, and we can use this to set rewards or penalties for an agent to learn the trajectory of particles in diverse environments. Our study shows that a Q-Learning algorithm successfully learned how light travels through materials with varying refraction indices and found the optimal path in minimum time, consistent with the solutions found through Snell's law or Fermat's Principle. We also draw parallels between our reinforcement learning approach and the path integral formalism.",1
"Prioritized experience replay (PER) samples important transitions, rather than uniformly, to improve the performance of a deep reinforcement learning agent. We claim that such prioritization has to be balanced with sample diversity for making the DQN stabilized and preventing forgetting. Our proposed improvement over PER, called Predictive PER (PPER), takes three countermeasures (TDInit, TDClip, TDPred) to (i) eliminate priority outliers and explosions and (ii) improve the sample diversity and distributions, weighted by priorities, both leading to stabilizing the DQN. The most notable among the three is the introduction of the second DNN called TDPred to generalize the in-distribution priorities. Ablation study and full experiments with Atari games show that each countermeasure by its own way and PPER contribute to successfully enhancing stability and thus performance over PER.",0
"To enhance the performance of a deep reinforcement learning agent, important transitions are sampled instead of uniformly through Prioritized Experience Replay (PER). However, it is crucial to maintain sample diversity to prevent forgetting and stabilize the DQN. Our proposed solution, Predictive PER (PPER), implements TDInit, TDClip, and TDPred to eliminate priority outliers and explosions, improve sample diversity and distributions, and stabilize the DQN. TDPred introduces a second DNN to generalize in-distribution priorities, which is the most significant of the three countermeasures. Ablation study and experiments with Atari games demonstrate the effective contribution of each countermeasure and PPER in improving stability and performance over PER.",1
"System identification is a fundamental problem in reinforcement learning, control theory and signal processing, and the non-asymptotic analysis of the corresponding sample complexity is challenging and elusive, even for linear time-varying (LTV) systems. To tackle this challenge, we develop an episodic block model for the LTV system where the model parameters remain constant within each block but change from block to block. Based on the observation that the model parameters across different blocks are related, we treat each episodic block as a learning task and then run meta-learning over many blocks for system identification, using two steps, namely offline meta-learning and online adaptation. We carry out a comprehensive non-asymptotic analysis of the performance of meta-learning based system identification. To deal with the technical challenges rooted in the sample correlation and small sample sizes in each block, we devise a new two-scale martingale small-ball approach for offline meta-learning, for arbitrary model correlation structure across blocks. We then quantify the finite time error of online adaptation by leveraging recent advances in linear stochastic approximation with correlated samples.",0
"The task of system identification poses a significant challenge in reinforcement learning, control theory, and signal processing. Even linear time-varying systems require non-asymptotic analysis, which is a difficult and complex problem. In order to address this issue, we introduce an episodic block model for LTV systems, where the model parameters remain constant within each block but change from block to block. By recognizing the relationship between the model parameters across different blocks, we treat each episodic block as a learning task, using offline meta-learning and online adaptation to identify the system over many blocks. We conduct a thorough non-asymptotic analysis of the performance of this meta-learning approach. To overcome the challenges posed by sample correlation and small sample sizes in each block, we develop a new two-scale martingale small-ball approach for offline meta-learning, which can handle arbitrary model correlation structure across blocks. We then use recent advancements in linear stochastic approximation with correlated samples to quantify the finite time error of online adaptation.",1
"One approach to deal with the statistical inefficiency of neural networks is to rely on auxiliary losses that help to build useful representations. However, it is not always trivial to know if an auxiliary task will be helpful for the main task and when it could start hurting. We propose to use the cosine similarity between gradients of tasks as an adaptive weight to detect when an auxiliary loss is helpful to the main loss. We show that our approach is guaranteed to converge to critical points of the main task and demonstrate the practical usefulness of the proposed algorithm in a few domains: multi-task supervised learning on subsets of ImageNet, reinforcement learning on gridworld, and reinforcement learning on Atari games.",0
"To address the statistical inefficiency of neural networks, one strategy is to incorporate auxiliary losses that aid in constructing meaningful representations. However, it can be challenging to determine if an auxiliary task will be advantageous or detrimental to the primary task. Our proposed solution is to utilize cosine similarity between task gradients as a flexible weight to identify when an auxiliary loss benefits the primary loss. We demonstrate that our method is certain to converge to critical points of the primary task and provide evidence of its utility in various domains, including multi-task supervised learning on select ImageNet subsets, reinforcement learning on gridworld, and reinforcement learning on Atari games.",1
"Automating molecular design using deep reinforcement learning (RL) has the potential to greatly accelerate the search for novel materials. Despite recent progress on leveraging graph representations to design molecules, such methods are fundamentally limited by the lack of three-dimensional (3D) information. In light of this, we propose a novel actor-critic architecture for 3D molecular design that can generate molecular structures unattainable with previous approaches. This is achieved by exploiting the symmetries of the design process through a rotationally covariant state-action representation based on a spherical harmonics series expansion. We demonstrate the benefits of our approach on several 3D molecular design tasks, where we find that building in such symmetries significantly improves generalization and the quality of generated molecules.",0
"The use of deep reinforcement learning (RL) to automate molecular design has the potential to hasten the search for new materials. However, current methods that rely on graph representations to design molecules are limited by the absence of three-dimensional (3D) information. To address this issue, we propose an innovative actor-critic architecture for 3D molecular design that can create previously unattainable molecular structures. This is accomplished by utilizing the symmetries of the design process via a rotationally covariant state-action representation based on a spherical harmonics series expansion. Through several 3D molecular design tasks, we demonstrate that incorporating such symmetries significantly enhances generalization and the quality of generated molecules.",1
"Designing a multi-layer optical system with designated optical characteristics is an inverse design problem in which the resulting design is determined by several discrete and continuous parameters. In particular, we consider three design parameters to describe a multi-layer stack: Each layer's dielectric material and thickness as well as the total number of layers. Such a combination of both, discrete and continuous parameters is a challenging optimization problem that often requires a computationally expensive search for an optimal system design. Hence, most methods merely determine the optimal thicknesses of the system's layers. To incorporate layer material and the total number of layers as well, we propose a method that considers the stacking of consecutive layers as parameterized actions in a Markov decision process. We propose an exponentially transformed reward signal that eases policy optimization and adapt a recent variant of Q-learning for inverse design optimization. We demonstrate that our method outperforms human experts and a naive reinforcement learning algorithm concerning the achieved optical characteristics. Moreover, the learned Q-values contain information about the optical properties of multi-layer optical systems, thereby allowing physical interpretation or what-if analysis.",0
"The challenge of designing a multi-layer optical system with specific optical qualities involves an inverse design problem that depends on both discrete and continuous parameters. The design parameters for a multi-layer stack consist of the dielectric material and thickness of each layer, as well as the total number of layers. Finding an optimal system design is a complex optimization problem that often requires a computationally intensive search. Therefore, most methods focus only on determining the optimal thicknesses of the layers. To address this limitation, we propose a method that uses a Markov decision process with stacking of consecutive layers as parameterized actions. We use an exponentially transformed reward signal to optimize the policy and adapt a recent variant of Q-learning for inverse design optimization. Our approach outperforms human experts and a naive reinforcement learning algorithm in terms of achieving the desired optical characteristics. Additionally, the Q-values provide information about the optical properties of multi-layer optical systems, enabling physical interpretation and what-if analysis.",1
"We present Graph-$Q$-SAT, a branching heuristic for a Boolean SAT solver trained with value-based reinforcement learning (RL) using Graph Neural Networks for function approximation. Solvers using Graph-$Q$-SAT are complete SAT solvers that either provide a satisfying assignment or proof of unsatisfiability, which is required for many SAT applications. The branching heuristics commonly used in SAT solvers make poor decisions during their warm-up period, whereas Graph-$Q$-SAT is trained to examine the structure of the particular problem instance to make better decisions early in the search. Training Graph-$Q$-SAT is data efficient and does not require elaborate dataset preparation or feature engineering. We train Graph-$Q$-SAT using RL interfacing with MiniSat solver and show that Graph-$Q$-SAT can reduce the number of iterations required to solve SAT problems by 2-3X. Furthermore, it generalizes to unsatisfiable SAT instances, as well as to problems with 5X more variables than it was trained on. We show that for larger problems, reductions in the number of iterations lead to wall clock time reductions, the ultimate goal when designing heuristics. We also show positive zero-shot transfer behavior when testing Graph-$Q$-SAT on a task family different from that used for training. While more work is needed to apply Graph-$Q$-SAT to reduce wall clock time in modern SAT solving settings, it is a compelling proof-of-concept showing that RL equipped with Graph Neural Networks can learn a generalizable branching heuristic for SAT search.",0
"Introducing Graph-$Q$-SAT, a branching heuristic that utilizes Graph Neural Networks for function approximation and is trained through value-based reinforcement learning (RL). This method enables complete SAT solvers that provide either a satisfying assignment or proof of unsatisfiability, which is necessary for many SAT applications. Unlike traditional branching heuristics used in SAT solvers, Graph-$Q$-SAT is trained to analyze the structure of the problem instance to make better decisions early on in the search process. This training approach is data efficient and does not require complex dataset preparation or feature engineering. By interfacing with MiniSat solver, we demonstrate that Graph-$Q$-SAT can reduce the number of iterations required to solve SAT problems by 2-3X. Moreover, it can generalize to larger and unsatisfiable SAT instances. We show that this reduction in the number of iterations leads to a decrease in wall clock time for larger problems. Additionally, we test Graph-$Q$-SAT on a different task family from the one used for training and observe positive zero-shot transfer behavior. Although more work is necessary to apply Graph-$Q$-SAT to modern SAT solving settings, it is a promising proof-of-concept that RL equipped with Graph Neural Networks can learn a generalizable branching heuristic for SAT search.",1
"Multi-scene reinforcement learning involves training the RL agent across multiple scenes / levels from the same task, and has become essential for many generalization applications. However, the inclusion of multiple scenes leads to an increase in sample variance for policy gradient computations, often resulting in suboptimal performance with the direct application of traditional methods (e.g. PPO, A3C). One strategy for variance reduction is to consider each scene as a distinct Markov decision process (MDP) and learn a joint value function dependent on both state (s) and MDP (M). However, this is non-trivial as the agent is usually unaware of the underlying level at train / test times in multi-scene RL. Recently, Singh et al. [1] tried to address this by proposing a dynamic value estimation approach that models the true joint value function distribution as a Gaussian mixture model (GMM). In this paper, we argue that the error between the true scene-specific value function and the predicted dynamic estimate can be further reduced by progressively enforcing sparse cluster assignments once the agent has explored most of the state space. The resulting agents not only show significant improvements in the final reward score across a range of OpenAI ProcGen environments, but also exhibit increased navigation efficiency while completing a game level.",0
"The training of RL agents across multiple scenes has become essential for generalization applications, but it often leads to suboptimal performance due to an increase in sample variance for policy gradient computations. To address this, one strategy is to treat each scene as a distinct Markov decision process and learn a joint value function dependent on both state and MDP. However, this is challenging as the agent is usually unaware of the underlying level during training and testing. Singh et al. proposed a dynamic value estimation approach that models the true joint value function distribution as a Gaussian mixture model. We argue that sparse cluster assignments can further reduce the error between the true scene-specific value function and the predicted dynamic estimate. This approach results in improved final reward scores and increased navigation efficiency in OpenAI ProcGen environments.",1
"Reinforcement Learning (RL) has made remarkable achievements, but it still suffers from inadequate exploration strategies, sparse reward signals, and deceptive reward functions. These problems motivate the need for a more efficient and directed exploration. For solving this, a Population-guided Novelty Search (PNS) parallel learning method is proposed. In PNS, the population is divided into multiple sub-populations, each of which has one chief agent and several exploring agents. The role of the chief agent is to evaluate the policies learned by exploring agents and to share the optimal policy with all sub-populations. The role of exploring agents is to learn their policies in collaboration with the guidance of the optimal policy and, simultaneously, upload their policies to the chief agent. To balance exploration and exploitation, the Novelty Search (NS) is employed in chief agents to encourage policies with high novelty while maximizing per-episode performance. The introduction of sub-populations and NS mechanisms promote directed exploration and enables better policy search. In the numerical experiment section, the proposed scheme is applied to the twin delayed deep deterministic (TD3) policy gradient algorithm, and the effectiveness of PNS to promote exploration and improve performance in both continuous control domains and discrete control domains is demonstrated. Notably, the proposed method achieves rewards that far exceed the SOTA methods in Delayed MoJoCo environments.",0
"Despite its impressive accomplishments, Reinforcement Learning (RL) still faces challenges such as insufficient exploration strategies, meager reward signals, and misleading reward functions. To address these issues and improve exploration efficiency, a Population-guided Novelty Search (PNS) parallel learning approach has been proposed. PNS divides a population into several sub-populations, each with a chief agent and multiple exploring agents. The chief agent evaluates the policies learned by the exploring agents and shares the optimal policy with all sub-populations, while the exploring agents learn their policies in collaboration with the optimal policy and share their policies with the chief agent. To balance exploration and exploitation, the Novelty Search (NS) is employed in chief agents, promoting high novelty policies while maximizing per-episode performance. This approach promotes directed exploration and enables better policy search. In numerical experiments, PNS was applied to the TD3 policy gradient algorithm, demonstrating its efficacy in enhancing exploration and performance in both continuous and discrete control domains. The proposed method achieves rewards that surpass the state-of-the-art methods in Delayed MoJoCo environments.",1
"Active reinforcement learning (ARL) is a variant on reinforcement learning where the agent does not observe the reward unless it chooses to pay a query cost c > 0. The central question of ARL is how to quantify the long-term value of reward information. Even in multi-armed bandits, computing the value of this information is intractable and we have to rely on heuristics. We propose and evaluate several heuristic approaches for ARL in multi-armed bandits and (tabular) Markov decision processes, and discuss and illustrate some challenging aspects of the ARL problem.",0
"Active reinforcement learning (ARL) is a type of reinforcement learning where the agent can only access the reward by paying a query cost c > 0. The main concern in ARL is how to assess the long-term worth of reward information. Even in multi-armed bandits, determining the value of this information is difficult, so heuristics are necessary. Our study puts forth various heuristic techniques for ARL in multi-armed bandits and (tabular) Markov decision processes, and delves into some of the difficulties of the ARL problem.",1
"Cooperative game is a critical research area in the multi-agent reinforcement learning (MARL). Global reward game is a subclass of cooperative games, where all agents aim to maximize the global reward. Credit assignment is an important problem studied in the global reward game. Most of previous works stood by the view of non-cooperative-game theoretical framework with the shared reward approach, i.e., each agent being assigned a shared global reward directly. This, however, may give each agent an inaccurate reward on its contribution to the group, which could cause inefficient learning. To deal with this problem, we i) introduce a cooperative-game theoretical framework called extended convex game (ECG) that is a superset of global reward game, and ii) propose a local reward approach called Shapley Q-value. Shapley Q-value is able to distribute the global reward, reflecting each agent's own contribution in contrast to the shared reward approach. Moreover, we derive an MARL algorithm called Shapley Q-value deep deterministic policy gradient (SQDDPG), using Shapley Q-value as the critic for each agent. We evaluate SQDDPG on Cooperative Navigation, Prey-and-Predator and Traffic Junction, compared with the state-of-the-art algorithms, e.g., MADDPG, COMA, Independent DDPG and Independent A2C. In the experiments, SQDDPG shows a significant improvement on the convergence rate. Finally, we plot Shapley Q-value and validate the property of fair credit assignment.",0
"Multi-agent reinforcement learning (MARL) research focuses on cooperative games, including the subclass of global reward games where all agents strive to maximize the global reward. This type of game presents a challenge in credit assignment, which has traditionally been approached from a non-cooperative game theoretical framework using the shared reward approach. However, this method can result in inaccurate rewards for each agent's contribution to the group, leading to inefficient learning. To address this issue, we propose a cooperative-game theoretical framework called extended convex game (ECG) that includes the global reward game and a local reward approach called Shapley Q-value. Shapley Q-value distributes the global reward based on each agent's individual contribution, in contrast to the shared reward approach. We also introduce the Shapley Q-value deep deterministic policy gradient (SQDDPG) algorithm, which uses Shapley Q-value as the critic for each agent. We evaluate SQDDPG on Cooperative Navigation, Prey-and-Predator and Traffic Junction, and compare it to state-of-the-art algorithms such as MADDPG, COMA, Independent DDPG and Independent A2C. Our experiments show that SQDDPG significantly improves the convergence rate, and we validate the fair credit assignment property through the plotting of Shapley Q-value.",1
"Learned representations of dynamical systems reduce dimensionality, potentially supporting downstream reinforcement learning (RL). However, no established methods predict a representation's suitability for control and evaluation is largely done via downstream RL performance, slowing representation design. Towards a principled evaluation of representations for control, we consider the relationship between the true state and the corresponding representations, proposing that ideally each representation corresponds to a unique true state. This motivates two metrics: temporal smoothness and high mutual information between true state/representation. These metrics are related to established representation objectives, and studied on Lagrangian systems where true state, information requirements, and statistical properties of the state can be formalized for a broad class of systems. These metrics are shown to predict reinforcement learning performance in a simulated peg-in-hole task when comparing variants of autoencoder-based representations.",0
"Dimensionality can be reduced by learned representations of dynamical systems, which may assist in downstream reinforcement learning. However, there are no established methods to predict a representation's suitability for control, and evaluation is typically reliant on downstream RL performance, which can slow the process of representation design. To create a more principled evaluation of representations for control, we suggest examining the relationship between the true state and the corresponding representations, with the goal being for each representation to correspond to a unique true state. This results in two metrics: temporal smoothness and high mutual information between true state/representation. These metrics are related to established representation objectives and are tested on Lagrangian systems. The true state, information requirements, and statistical properties of the state can be formalized for a wide range of systems. These metrics are used to predict reinforcement learning performance in a simulated peg-in-hole task, where various autoencoder-based representations are compared.",1
"SegBlocks reduces the computational cost of existing neural networks, by dynamically adjusting the processing resolution of image regions based on their complexity. Our method splits an image into blocks and downsamples blocks of low complexity, reducing the number of operations and memory consumption. A lightweight policy network, selecting the complex regions, is trained using reinforcement learning. In addition, we introduce several modules implemented in CUDA to process images in blocks. Most important, our novel BlockPad module prevents the feature discontinuities at block borders of which existing methods suffer, while keeping memory consumption under control. Our experiments on Cityscapes and Mapillary Vistas semantic segmentation show that dynamically processing images offers a better accuracy versus complexity trade-off compared to static baselines of similar complexity. For instance, our method reduces the number of floating-point operations of SwiftNet-RN18 by 60% and increases the inference speed by 50%, with only 0.3% decrease in mIoU accuracy on Cityscapes.",0
"The computational cost of existing neural networks can be reduced by SegBlocks, which adjusts the processing resolution of image regions based on their complexity. To do this, the method splits an image into blocks and downsamples blocks with low complexity, which reduces the number of operations and memory consumption. To select the complex regions, a lightweight policy network is trained with reinforcement learning. Additionally, several modules implemented in CUDA are introduced to process images in blocks. The BlockPad module is a novel approach that prevents feature discontinuities at block borders, while keeping memory consumption under control. In experiments conducted on Cityscapes and Mapillary Vistas semantic segmentation, dynamically processing images offered a better accuracy versus complexity trade-off compared to static baselines of similar complexity. For example, the number of floating-point operations of SwiftNet-RN18 was reduced by 60% and the inference speed was increased by 50% with only a 0.3% decrease in mIoU accuracy on Cityscapes.",1
"Uncertainty quantification is crucial for building reliable and trustable machine learning systems. We propose to estimate uncertainty in recurrent neural networks (RNNs) via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. Alongside uncertainty quantification, our proposed method offers several advantages in different settings. The proposed method can (1) learn deterministic and probabilistic automata from data, (2) learn well-calibrated models on real-world classification tasks, (3) improve the performance of out-of-distribution detection, and (4) control the exploration-exploitation trade-off in reinforcement learning.",0
"To construct dependable and credible machine learning systems, it is imperative to have an understanding of uncertainty quantification. We suggest an approach to assess uncertainty in RNNs by employing stochastic discrete state transitions over recurrent timesteps. The model's uncertainty can be gauged by conducting multiple predictions and sampling from the recurrent state transition distribution, which may produce varying outcomes if the model is unsure. Our proposed technique has numerous benefits in diverse scenarios, such as (1) learning deterministic and probabilistic automata from data, (2) developing well-calibrated models for real-world classification tasks, (3) enhancing the performance of out-of-distribution detection, and (4) regulating the exploration-exploitation trade-off in reinforcement learning.",1
"In this paper, we propose a framework that enables a human teacher to shape a robot behaviour by interactively providing it with unlabeled instructions. We ground the meaning of instruction signals in the task-learning process, and use them simultaneously for guiding the latter. We implement our framework as a modular architecture, named TICS (Task-Instruction-Contingency-Shaping) that combines different information sources: a predefined reward function, human evaluative feedback and unlabeled instructions. This approach provides a novel perspective for robotic task learning that lies between Reinforcement Learning and Supervised Learning paradigms. We evaluate our framework both in simulation and with a real robot. The experimental results demonstrate the effectiveness of our framework in accelerating the task-learning process and in reducing the number of required teaching signals.",0
"The paper presents a framework that allows a human teacher to influence the behavior of a robot by providing it with unlabeled instructions. The instructions are used to guide the task-learning process and are grounded in its meaning. The framework, called TICS, combines various sources of information such as predefined reward function, human evaluative feedback, and unlabeled instructions. This approach fills the gap between Reinforcement Learning and Supervised Learning paradigms. The framework is tested in simulation and with a real robot, and the results show that it accelerates the task-learning process and reduces the number of required teaching signals.",1
"Reinforcement Learning (RL) is an area of machine learning concerned with enabling an agent to navigate an environment with uncertainty in order to maximize some notion of cumulative long-term reward. In this paper, we implement and analyze two different RL techniques, Sarsa and Deep QLearning, on OpenAI Gym's LunarLander-v2 environment. We then introduce additional uncertainty to the original problem to test the robustness of the mentioned techniques. With our best models, we are able to achieve average rewards of 170+ with the Sarsa agent and 200+ with the Deep Q-Learning agent on the original problem. We also show that these techniques are able to overcome the additional uncertainities and achieve positive average rewards of 100+ with both agents. We then perform a comparative analysis of the two techniques to conclude which agent peforms better.",0
"The objective of Reinforcement Learning (RL) in machine learning is to empower an agent to navigate through an uncertain environment and maximize cumulative long-term reward. This study conducts an analysis of two RL techniques, Sarsa and Deep Q-Learning, on OpenAI Gym's LunarLander-v2 environment. The study also introduces additional uncertainty to the original problem to evaluate the robustness of these techniques. The best models were able to achieve average rewards of more than 170 with the Sarsa agent and 200 with the Deep Q-Learning agent on the original problem. The study demonstrates that both techniques can overcome additional uncertainty and achieve positive average rewards of over 100 with both agents. A comparative analysis of the two techniques is presented to determine which agent performs better.",1
"Reinforcement learning (RL) has achieved tremendous success as a general framework for learning how to make decisions. However, this success relies on the interactive hand-tuning of a reward function by RL experts. On the other hand, inverse reinforcement learning (IRL) seeks to learn a reward function from readily-obtained human demonstrations. Yet, IRL suffers from two major limitations: 1) reward ambiguity - there are an infinite number of possible reward functions that could explain an expert's demonstration and 2) heterogeneity - human experts adopt varying strategies and preferences, which makes learning from multiple demonstrators difficult due to the common assumption that demonstrators seeks to maximize the same reward. In this work, we propose a method to jointly infer a task goal and humans' strategic preferences via network distillation. This approach enables us to distill a robust task reward (addressing reward ambiguity) and to model each strategy's objective (handling heterogeneity). We demonstrate our algorithm can better recover task reward and strategy rewards and imitate the strategies in two simulated tasks and a real-world table tennis task.",0
"The framework of reinforcement learning (RL) has been highly successful in decision-making, but it depends on experts fine-tuning the reward function interactively. Inverse reinforcement learning (IRL) aims to learn the reward function from human demonstrations, but it faces two major challenges: 1) there are infinite possible reward functions to explain an expert's demonstration, causing reward ambiguity, and 2) human experts have different strategies and preferences, making it hard to learn from multiple demonstrators who seek to maximize the same reward. This study proposes a network distillation method to jointly infer the task goal and strategic preferences of humans, thus addressing reward ambiguity and handling heterogeneity. Results demonstrate that the algorithm can recover task and strategy rewards and imitate strategies in simulated and real-world table tennis tasks.",1
"Knowledge distillation between machine learning models has opened many new avenues for parameter count reduction, performance improvements, or amortizing training time when changing architectures between the teacher and student network. In the case of reinforcement learning, this technique has also been applied to distill teacher policies to students. Until now, policy distillation required access to a simulator or real world trajectories.   In this paper we introduce a simulator-free approach to knowledge distillation in the context of reinforcement learning. A key challenge is having the student learn the multiplicity of cases that correspond to a given action. While prior work has shown that data-free knowledge distillation is possible with supervised learning models by generating synthetic examples, these approaches to are vulnerable to only producing a single prototype example for each class. We propose an extension to explicitly handle multiple observations per output class that seeks to find as many exemplars as possible for a given output class by reinitializing our data generator and making use of an adversarial loss.   To the best of our knowledge, this is the first demonstration of simulator-free knowledge distillation between a teacher and a student policy. This new approach improves over the state of the art on data-free learning of student networks on benchmark datasets (MNIST, Fashion-MNIST, CIFAR-10), and we also demonstrate that it specifically tackles issues with multiple input modes. We also identify open problems when distilling agents trained in high dimensional environments such as Pong, Breakout, or Seaquest.",0
"The use of knowledge distillation in machine learning models has opened up various possibilities, such as reducing parameter count, enhancing performance, and shortening training time when switching between teacher and student networks. Previously, policy distillation in reinforcement learning required access to either a simulator or real world trajectories. Nevertheless, this paper introduces a simulator-free technique for knowledge distillation in reinforcement learning. The challenge is for the student to learn multiple cases for a given action. While supervised learning models have shown data-free knowledge distillation is feasible by generating synthetic examples, these approaches are prone to producing only a single prototype example for each class. To address this, the authors propose an extension that explicitly handles multiple observations per output class by reinitializing the data generator and employing an adversarial loss. This is the first demonstration of simulator-free knowledge distillation between a teacher and a student policy, which outperforms the state of the art on data-free learning of student networks on benchmark datasets (MNIST, Fashion-MNIST, CIFAR-10) and addresses issues with multiple input modes. However, there are still open problems when distilling agents trained in high dimensional environments, such as Pong, Breakout, or Seaquest.",1
"Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, however, do not account for component interactions. Hence, neither of them allows for principled sampling of novel scenes. Here we present GENESIS, the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes by capturing relationships between scene components. GENESIS parameterises a spatial GMM over images which is decoded from a set of object-centric latent variables that are either inferred sequentially in an amortised fashion or sampled from an autoregressive prior. We train GENESIS on several publicly available datasets and evaluate its performance on scene generation, decomposition, and semi-supervised learning.",0
"Robotics and reinforcement learning have seen the emergence of generative latent-variable models as promising tools. However, despite the fact that these domains typically involve distinct objects, most current generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, take an unsupervised approach to decomposing scenes into objects. However, their underlying generative processes do not consider component interactions, making them unsuitable for principled sampling of novel scenes. To address this issue, we introduce GENESIS, the first object-centric generative model of 3D visual scenes that can both decompose and generate scenes by capturing the relationships between scene components. GENESIS parameterises a spatial GMM over images, which is decoded from a set of object-centric latent variables that are either inferred sequentially in an amortised fashion or sampled from an autoregressive prior. We train GENESIS on several publicly available datasets and evaluate its performance on scene generation, decomposition, and semi-supervised learning.",1
"Planning is a powerful approach to reinforcement learning with several desirable properties. However, it requires a model of the world, which is not readily available in many real-life problems. In this paper, we propose to learn a world model that enables Evolutionary Planning in Latent Space (EPLS). We use a Variational Auto Encoder (VAE) to learn a compressed latent representation of individual observations and extend a Mixture Density Recurrent Neural Network (MDRNN) to learn a stochastic, multi-modal forward model of the world that can be used for planning. We use the Random Mutation Hill Climbing (RMHC) to find a sequence of actions that maximize expected reward in this learned model of the world. We demonstrate how to build a model of the world by bootstrapping it with rollouts from a random policy and iteratively refining it with rollouts from an increasingly accurate planning policy using the learned world model. After a few iterations of this refinement, our planning agents are better than standard model-free reinforcement learning approaches demonstrating the viability of our approach.",0
"The process of planning is effective in reinforcement learning and has many benefits. However, it relies on having a comprehensive understanding of the environment, which is often not accessible in real-life scenarios. This study introduces a new concept called Evolutionary Planning in Latent Space (EPLS), which involves developing a world model to facilitate planning. To achieve this, the authors propose using a Variational Auto Encoder (VAE) to create a compressed latent representation of observations and expanding a Mixture Density Recurrent Neural Network (MDRNN) to build a stochastic, multi-modal forward model of the world. Through the Random Mutation Hill Climbing (RMHC) technique, a sequence of actions that produce the highest expected reward can be discovered using this learned model. The researchers illustrate how to construct a world model by bootstrapping it with random policy rollouts and refining it iteratively with more accurate planning policy rollouts. After a few rounds of refinement, the planning agents produced using this method outperformed traditional model-free reinforcement learning approaches, demonstrating the effectiveness of this approach.",1
"Reinforcement Learning (RL) has recently been applied to sequential estimation and prediction problems identifying and developing hypothetical treatment strategies for septic patients, with a particular focus on offline learning with observational data. In practice, successful RL relies on informative latent states derived from sequential observations to develop optimal treatment strategies. To date, how best to construct such states in a healthcare setting is an open question. In this paper, we perform an empirical study of several information encoding architectures using data from septic patients in the MIMIC-III dataset to form representations of a patient state. We evaluate the impact of representation dimension, correlations with established acuity scores, and the treatment policies derived from them. We find that sequentially formed state representations facilitate effective policy learning in batch settings, validating a more thoughtful approach to representation learning that remains faithful to the sequential and partial nature of healthcare data.",0
"Recently, Reinforcement Learning (RL) has been utilized to address sequential estimation and prediction problems in identifying and developing hypothetical treatment strategies for septic patients. There has been a particular emphasis on offline learning with observational data. The success of RL in practice depends on the knowledge of informative latent states derived from sequential observations to create optimal treatment strategies. However, how to construct such states in a healthcare setting is still an open question. This paper presents an empirical study of several information encoding architectures using data from septic patients in the MIMIC-III dataset to form representations of a patient state. The study evaluates the impact of representation dimension, correlations with established acuity scores, and the treatment policies derived from them. The findings reveal that sequentially formed state representations promote effective policy learning in batch settings, validating a more thoughtful approach to representation learning that remains faithful to the sequential and partial nature of healthcare data.",1
"Deep reinforcement learning (DRL) is a very active research area. However, several technical and scientific issues require to be addressed, amongst which we can mention data inefficiency, exploration-exploitation trade-off, and multi-task learning. Therefore, distributed modifications of DRL were introduced; agents that could be run on many machines simultaneously. In this article, we provide a survey of the role of the distributed approaches in DRL. We overview the state of the field, by studying the key research works that have a significant impact on how we can use distributed methods in DRL. We choose to overview these papers, from the perspective of distributed learning, and not the aspect of innovations in reinforcement learning algorithms. Also, we evaluate these methods on different tasks and compare their performance with each other and with single actor and learner agents.",0
"The field of deep reinforcement learning (DRL) is highly active, but there are a number of technical and scientific issues that must be addressed. These include inefficiencies in data, the trade-off between exploration and exploitation, and multi-task learning. To address these issues, distributed modifications of DRL have been introduced, enabling agents to run on multiple machines simultaneously. This article provides a survey of the role of distributed approaches in DRL, examining the key research that has had a significant impact on the use of these methods. Rather than focusing on innovations in reinforcement learning algorithms, we evaluate these methods from a distributed learning perspective, comparing their performance on different tasks with single actor and learner agents.",1
"Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task. The code is available at https://github.com/tianheyu927/mopo.",0
"The problem of offline reinforcement learning (RL) involves learning policies from previously collected data, which eliminates the need for costly or dangerous active exploration. However, this problem is challenging due to the difference between the offline training data and the states visited by the learned policy. Although there have been recent advances in this area, the most successful methods are model-free and do not allow for generalization to unseen states. In this paper, we propose modifying existing model-based RL methods by penalizing rewards artificially to avoid the distributional shift issue. Our proposed algorithm, Model-based Offline Policy Optimization (MOPO), maximizes a lower bound of the policy's return under the true MDP, and we analyze the trade-off between gain and risk. MOPO outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on offline RL benchmarks and two challenging continuous control tasks. The code is available at https://github.com/tianheyu927/mopo.",1
"This paper aims to develop a unified paradigm that models one's learning behavior and the system's equilibrating processes in a routing game among atomic selfish agents. Such a paradigm can assist policymakers in devising optimal operational and planning countermeasures under both normal and abnormal circumstances. To this end, a multi-agent reinforcement learning (MARL) paradigm is proposed in which each agent learns and updates her own en-route path choice policy while interacting with others on transportation networks. This paradigm is shown to generalize the classical notion of dynamic user equilibrium (DUE) to model-free and data-driven scenarios. We also illustrate that the equilibrium outcomes computed from our developed MARL paradigm coincide with DUE and dynamic system optimal (DSO), respectively, when rewards are set differently. In addition, with the goal to optimize some systematic objective (e.g., overall traffic condition) of city planners, we formulate a bilevel optimization problem with the upper level as city planners and the lower level as a multi-agent system where each rational and selfish traveler aims to minimize her travel cost. We demonstrate the effect of two administrative measures, namely tolling and signal control, on the behavior of travelers and show that the systematic objective of city planners can be optimized by a proper control. The results show that on the Braess network, the optimal toll charge on the central link is greater or equal to 25, with which the average travel time of selfish agents is minimized and the emergence of Braess paradox could be avoided. In a large-sized real-world road network with 69 nodes and 166 links, the optimal offset for signal control on Broadway is derived as 4 seconds, with which the average travel time of all controllable agents is minimized.",0
"The objective of this study is to create a cohesive framework that models the learning behavior of individuals and the equilibrating processes of a system in a routing game involving selfish agents. This framework can aid policymakers in devising optimal measures for normal and abnormal situations. The proposed multi-agent reinforcement learning (MARL) paradigm allows each agent to learn and update their path choice policy while interacting with others on transportation networks. This model extends the classical Dynamic User Equilibrium (DUE) to model-free and data-driven scenarios. The equilibrium outcomes obtained from the MARL paradigm coincide with DUE and Dynamic System Optimal (DSO) when rewards are set differently. Additionally, a bilevel optimization problem is formulated to optimize the systematic objective of city planners. The upper level represents the city planners, and the lower level represents a multi-agent system where each selfish traveler is attempting to minimize their travel cost. Furthermore, the study demonstrates the effect of tolling and signal control on the behavior of travelers and shows that the systematic objective of city planners can be optimized by proper control. Based on the results, the optimal toll charge on the central link of the Braess network is greater than or equal to 25, and on a large-sized real-world road network, the optimal offset for signal control on Broadway is 4 seconds.",1
"We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes reward in infinite-horizon problem settings. The attacker can manipulate the rewards and the transition dynamics in the learning environment at training-time, and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an optimal stealthy attack for different measures of attack cost. We provide lower/upper bounds on the attack cost, and instantiate our attacks in two settings: (i) an offline setting where the agent is doing planning in the poisoned environment, and (ii) an online setting where the agent is learning a policy with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.",0
"Our focus is on a security concern for reinforcement learning, in which an adversary poisons the learning environment to manipulate the agent into executing a specific policy of the attacker's choice. Our analysis is geared towards RL agents whose goal is to find a policy that maximizes reward in infinite-horizon problems. The attacker can manipulate rewards and transition dynamics during training, with the aim of remaining undetected. To address this, we present an optimization framework that identifies an optimal stealthy attack for various measures of attack cost. We establish upper and lower bounds on attack cost and test our attacks in two scenarios: offline planning in a poisoned environment, and online policy learning with manipulated feedback. Our experiments demonstrate that even under mild conditions, the attacker can successfully teach the victim any target policy, highlighting a significant security threat to RL agents in practical applications.",1
"We are interested in learning models of non-stationary environments, which can be framed as a multi-task learning problem. Model-free reinforcement learning algorithms can achieve good asymptotic performance in multi-task learning at a cost of extensive sampling, due to their approach, which requires learning from scratch. While model-based approaches are among the most data efficient learning algorithms, they still struggle with complex tasks and model uncertainties. Meta-reinforcement learning addresses the efficiency and generalization challenges on multi task learning by quickly leveraging the meta-prior policy for a new task. In this paper, we propose a meta-reinforcement learning approach to learn the dynamic model of a non-stationary environment to be used for meta-policy optimization later. Due to the sample efficiency of model-based learning methods, we are able to simultaneously train both the meta-model of the non-stationary environment and the meta-policy until dynamic model convergence. Then, the meta-learned dynamic model of the environment will generate simulated data for meta-policy optimization. Our experiment demonstrates that our proposed method can meta-learn the policy in a non-stationary environment with the data efficiency of model-based learning approaches while achieving the high asymptotic performance of model-free meta-reinforcement learning.",0
"Our focus is on acquiring knowledge of non-stationary environments, which involves multi-task learning. Although model-free reinforcement learning algorithms can excel in multi-task learning, they require extensive sampling due to their approach of learning from scratch. On the other hand, model-based approaches are more data-efficient, but they struggle with complex tasks and model uncertainties. To address these challenges, we propose a meta-reinforcement learning approach in which we quickly leverage the meta-prior policy for a new task. Our method involves learning the dynamic model of a non-stationary environment and using it for meta-policy optimization later. Since model-based learning methods are sample-efficient, we can train both the meta-model of the non-stationary environment and the meta-policy simultaneously until the dynamic model convergence. Then, we use the meta-learned dynamic model of the environment to generate simulated data for meta-policy optimization. Our experiments show that our proposed method can efficiently meta-learn the policy in a non-stationary environment while achieving high asymptotic performance, similar to model-free meta-reinforcement learning.",1
"We consider the regret minimization problem in reinforcement learning (RL) in the episodic setting. In many real-world RL environments, the state and action spaces are continuous or very large. Existing approaches establish regret guarantees by either a low-dimensional representation of the stochastic transition model or an approximation of the $Q$-functions. However, the understanding of function approximation schemes for state-value functions largely remains missing. In this paper, we propose an online model-based RL algorithm, namely the CME-RL, that learns representations of transition distributions as embeddings in a reproducing kernel Hilbert space while carefully balancing the exploitation-exploration tradeoff. We demonstrate the efficiency of our algorithm by proving a frequentist (worst-case) regret bound that is of order $\tilde{O}\big(H\gamma_N\sqrt{N}\big)$, where $H$ is the episode length, $N$ is the total number of time steps and $\gamma_N$ is an information theoretic quantity relating the effective dimension of the state-action feature space. Our method bypasses the need for estimating transition probabilities and applies to any domain on which kernels can be defined. It also brings new insights into the general theory of kernel methods for approximate inference and RL regret minimization.",0
"In the context of reinforcement learning (RL), we examine the problem of minimizing regret in the episodic scenario. In many real-world RL situations, the state and action spaces are either continuous or vast. Existing solutions ensure regret guarantees by either representing the stochastic transition model in a low-dimensional format or approximating the $Q$-functions. However, the absence of comprehension regarding function approximation techniques for state-value functions remains a significant challenge. This paper proposes an online model-based RL algorithm, the CME-RL, which learns representations of transition distributions as embeddings in a reproducing kernel Hilbert space while maintaining a balance between exploitation and exploration. We demonstrate the effectiveness of our algorithm by establishing a frequentist regret bound of order $\tilde{O}\big(H\gamma_N\sqrt{N}\big)$, which has implications for episode length ($H$), total time steps ($N$), and the effective dimension of the state-action feature space ($\gamma_N$). Our method eliminates the need for estimating transition probabilities and can be applied to domains where kernels can be defined. Additionally, it provides new insights into the general theory of kernel methods for approximate inference and RL regret minimization.",1
"Balancing exploration and exploitation is crucial in reinforcement learning (RL). In this paper, we study the model-based posterior sampling algorithm in continuous state-action spaces theoretically and empirically. First, we improve the regret bound: with the assumption that reward and transition functions can be modeled as Gaussian Processes with linear kernels, we develop a Bayesian regret bound of $\tilde{O}(H^{3/2}d\sqrt{T})$, where $H$ is the episode length, $d$ is the dimension of the state-action space, and $T$ indicates the total time steps. Our bound can be extended to nonlinear cases as well: using linear kernels on the feature representation $\phi$, the Bayesian regret bound becomes $\tilde{O}(H^{3/2}d_{\phi}\sqrt{T})$, where $d_\phi$ is the dimension of the representation space. Moreover, we present MPC-PSRL, a model-based posterior sampling algorithm with model predictive control for action selection. To capture the uncertainty in models and realize posterior sampling, we use Bayesian linear regression on the penultimate layer (the feature representation layer $\phi$) of neural networks. Empirical results show that our algorithm achieves the best sample efficiency in benchmark control tasks compared to prior model-based algorithms, and matches the asymptotic performance of model-free algorithms.",0
"In reinforcement learning, it is critical to find a balance between exploration and exploitation. This study focuses on the model-based posterior sampling algorithm in continuous state-action spaces, exploring both theoretical and empirical aspects. The regret bound has been improved, and a Bayesian regret bound of $\tilde{O}(H^{3/2}d\sqrt{T})$ has been developed, assuming that reward and transition functions can be modeled as Gaussian Processes with linear kernels. This bound can be extended to nonlinear cases using linear kernels on the feature representation $\phi$, resulting in a Bayesian regret bound of $\tilde{O}(H^{3/2}d_{\phi}\sqrt{T})$. The study also presents MPC-PSRL, a model-based posterior sampling algorithm that utilizes model predictive control for action selection. To capture uncertainty in the models, Bayesian linear regression is employed on the penultimate layer (the feature representation layer $\phi$) of neural networks. Empirical results demonstrate that our algorithm achieves the best sample efficiency in benchmark control tasks compared to prior model-based algorithms and matches the asymptotic performance of model-free algorithms.",1
"In reinforcement learning, policy gradient algorithms optimize the policy directly and rely on sampling efficiently an environment. Nevertheless, while most sampling procedures are based on direct policy sampling, self-performance measures could be used to improve such sampling prior to each policy update. Following this line of thought, we introduce SAUNA, a method where non-informative transitions are rejected from the gradient update. The level of information is estimated according to the fraction of variance explained by the value function: a measure of the discrepancy between V and the empirical returns. In this work, we use this metric to select samples that are useful to learn from, and we demonstrate that this selection can significantly improve the performance of policy gradient methods. In this paper: (a) We define SAUNA's metric and introduce its method to filter transitions. (b) We conduct experiments on a set of benchmark continuous control problems. SAUNA significantly improves performance. (c) We investigate how SAUNA reliably selects samples with the most positive impact on learning and study its improvement on both performance and sample efficiency.",0
"Reinforcement learning involves optimizing the policy through policy gradient algorithms, which rely on efficiently sampling the environment. However, direct policy sampling may not always be effective, and self-performance measures could be utilized to enhance the sampling process before each policy update. To address this issue, we propose SAUNA, a method that rejects non-informative transitions from the gradient update. Our approach estimates the level of information through the variance explained by the value function, which measures the discrepancy between V and the empirical returns. By selecting useful samples to learn from, we show that SAUNA can significantly enhance the performance of policy gradient methods. In this paper, we define SAUNA's metric and introduce its transition filtering method, conduct experiments on benchmark continuous control problems, and investigate its reliability in selecting samples with the most positive impact on learning, leading to improvements in both performance and sample efficiency.",1
"Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin.",0
"Flexible decision making and control can be achieved through reinforcement learning, but it requires extensive data collection for each new task. In contrast, pre-training on large datasets has proven to be effective in reducing data requirements for new tasks in other machine learning fields like natural language processing and computer vision. This paper explores the possibility of enabling pre-training for reinforcement learning agents, proposing a method for pre-training behavioral priors that can capture complex input-output relationships from successful trials of previously seen tasks. This learned prior can be used to rapidly learn new tasks without hindering the RL agent's ability to try out novel behaviors. The effectiveness of this approach is demonstrated in challenging robotic manipulation domains with image observations and sparse reward functions, where it outperforms prior works significantly.",1
"Reinforcement learning (RL), particularly in sparse reward settings, often requires prohibitively large numbers of interactions with the environment, thereby limiting its applicability to complex problems. To address this, several prior approaches have used natural language to guide the agent's exploration. However, these approaches typically operate on structured representations of the environment, and/or assume some structure in the natural language commands. In this work, we propose a model that directly maps pixels to rewards, given a free-form natural language description of the task, which can then be used for policy learning. Our experiments on the Meta-World robot manipulation domain show that language-based rewards significantly improves the sample efficiency of policy learning, both in sparse and dense reward settings.",0
"Reinforcement learning (RL) is often limited in its effectiveness for complex problems, especially in sparse reward settings, because it requires a large number of interactions with the environment. Previous attempts to address this issue have used natural language to guide the agent's exploration, but these typically rely on structured representations of the environment or assume some structure in the language commands. Our approach directly maps pixels to rewards based on a free-form natural language description of the task, which can be used for policy learning. Our experiments on the Meta-World robot manipulation domain demonstrate that language-based rewards significantly increase the efficiency of policy learning, both in sparse and dense reward settings.",1
"Deep reinforcement learning has achieved impressive successes yet often requires a very large amount of interaction data. This result is perhaps unsurprising, as using complicated function approximation often requires more data to fit, and early theoretical results on linear Markov decision processes provide regret bounds that scale with the dimension of the linear approximation. Ideally, we would like to automatically identify the minimal dimension of the approximation that is sufficient to encode an optimal policy. Towards this end, we consider the problem of model selection in RL with function approximation, given a set of candidate RL algorithms with known regret guarantees. The learner's goal is to adapt to the complexity of the optimal algorithm without knowing it \textit{a priori}. We present a meta-algorithm that successively rejects increasingly complex models using a simple statistical test. Given at least one candidate that satisfies realizability, we prove the meta-algorithm adapts to the optimal complexity with $\tilde{O}(L^{5/6} T^{2/3})$ regret compared to the optimal candidate's $\tilde{O}(\sqrt T)$ regret, where $T$ is the number of episodes and $L$ is the number of algorithms. The dimension and horizon dependencies remain optimal with respect to the best candidate, and our meta-algorithmic approach is flexible to incorporate multiple candidate algorithms and models. Finally, we show that the meta-algorithm automatically admits significantly improved instance-dependent regret bounds that depend on the gaps between the maximal values attainable by the candidates.",0
"Although deep reinforcement learning has had impressive achievements, it often necessitates a large amount of interaction data. This outcome is not surprising since using complicated function approximation generally requires more data for fitting. Early theoretical findings on linear Markov decision processes also demonstrate that regret bounds scale with the dimension of the linear approximation. Ideally, the minimal dimension of the approximation necessary for encoding an optimal policy should be identified automatically. To achieve this, we explore model selection in RL with function approximation, given a set of candidate RL algorithms with known regret guarantees. The learner's objective is to adapt to the complexity of the optimal algorithm without prior knowledge. We present a meta-algorithm that rejects increasingly complex models using a simple statistical test. When there is at least one candidate that satisfies realizability, we prove that the meta-algorithm adapts to the optimal complexity with $\tilde{O}(L^{5/6} T^{2/3})$ regret compared to the optimal candidate's $\tilde{O}(\sqrt T)$ regret, where $T$ is the number of episodes and $L$ is the number of algorithms. The dimension and horizon dependencies remain optimal with respect to the best candidate, and our meta-algorithmic approach is flexible to incorporate multiple candidate algorithms and models. Finally, we demonstrate that the meta-algorithm automatically generates significantly improved instance-dependent regret bounds that depend on the gaps between the maximal values attainable by the candidates.",1
"The goal of inverse reinforcement learning (IRL) is to infer a reward function that explains the behavior of an agent performing a task. The assumption that most approaches make is that the demonstrated behavior is near-optimal. In many real-world scenarios, however, examples of truly optimal behavior are scarce, and it is desirable to effectively leverage sets of demonstrations of suboptimal or heterogeneous performance, which are easier to obtain. We propose an algorithm that learns a reward function from such demonstrations together with a weak supervision signal in the form of a distribution over rewards collected during the demonstrations (or, more generally, a distribution over cumulative discounted future rewards). We view such distributions, which we also refer to as optimality profiles, as summaries of the degree of optimality of the demonstrations that may, for example, reflect the opinion of a human expert. Given an optimality profile and a small amount of additional supervision, our algorithm fits a reward function, modeled as a neural network, by essentially minimizing the Wasserstein distance between the corresponding induced distribution and the optimality profile. We show that our method is capable of learning reward functions such that policies trained to optimize them outperform the demonstrations used for fitting the reward functions.",0
"Inverse reinforcement learning (IRL) aims to deduce a reward function that can account for an agent's behavior during a task. Most IRL approaches assume that the demonstrated behavior is nearly optimal. However, optimal behavior examples are often scarce in real-world situations, prompting the need to utilize sets of suboptimal or varied performance demonstrations that are easier to acquire. Our proposed algorithm learns a reward function from such demonstrations alongside weak supervision in the form of a distribution over rewards collected during the demonstrations. These distributions, also referred to as optimality profiles, summarize the degree of optimality of the demonstrations and may reflect the opinion of a human expert. Our algorithm fits a neural network modeled reward function using an optimality profile and a small amount of additional supervision by minimizing the Wasserstein distance between the resulting distribution and the optimality profile. Our method successfully learned reward functions that resulted in policies that outperformed the demonstrations used for fitting the reward functions.",1
"One of the challenging problems in sequence generation tasks is the optimized generation of sequences with specific desired goals. Current sequential generative models mainly generate sequences to closely mimic the training data, without direct optimization of desired goals or properties specific to the task. We introduce OptiGAN, a generative model that incorporates both Generative Adversarial Networks (GAN) and Reinforcement Learning (RL) to optimize desired goal scores using policy gradients. We apply our model to text and real-valued sequence generation, where our model is able to achieve higher desired scores out-performing GAN and RL baselines, while not sacrificing output sample diversity.",0
"Generating sequences with specific desired goals is a challenging problem in sequence generation tasks. Existing sequential generative models focus on generating sequences that closely resemble the training data, rather than directly optimizing for desired goals or task-specific properties. To address this issue, we propose OptiGAN, a generative model that combines Generative Adversarial Networks (GAN) and Reinforcement Learning (RL) to optimize desired goal scores using policy gradients. Our model is applied to both text and real-valued sequence generation, and is able to outperform GAN and RL baselines in achieving higher desired scores, while also maintaining output sample diversity.",1
"Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners of machine learning and beyond to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions to traditional IRL methods for handling: inaccurate and incomplete perception, an incomplete model, multiple reward functions, and nonlinear reward functions. This survey concludes the discussion with some broad advances in the research area and currently open research questions.",0
"The task of discovering the reward function of an agent based on its policy or observed actions is known as Inverse Reinforcement Learning (IRL). Like RL, IRL is both a problem and a category of methods. This article presents a comprehensive review of the current literature on IRL to serve as a resource for machine learning researchers and practitioners. The review defines the IRL problem and highlights its core challenges, such as the difficulty in accurately making inferences and achieving generalization, sensitivity to prior knowledge, and increasing complexity with problem size. The article also discusses the ways in which current methods attempt to overcome these challenges. Additionally, it explores how traditional IRL methods are extended to handle inaccurate and incomplete perception, incomplete models, multiple reward functions, and nonlinear reward functions. The review concludes with an overview of current advances in the field and identifies open research questions.",1
"Credit assignment in reinforcement learning is the problem of measuring an action influence on future rewards. In particular, this requires separating skill from luck, ie. disentangling the effect of an action on rewards from that of external factors and subsequent actions. To achieve this, we adapt the notion of counterfactuals from causality theory to a model-free RL setup. The key idea is to condition value functions on future events, by learning to extract relevant information from a trajectory. We then propose to use these as future-conditional baselines and critics in policy gradient algorithms and we develop a valid, practical variant with provably lower variance, while achieving unbiasedness by constraining the hindsight information not to contain information about the agent actions. We demonstrate the efficacy and validity of our algorithm on a number of illustrative problems.",0
"Reinforcement learning involves determining the impact of an action on future rewards, a process known as credit assignment. To accurately assess this, it is necessary to distinguish between skill and luck, meaning the effect of an action on rewards must be separated from the influence of external factors and subsequent actions. To address this, we utilize counterfactuals from causality theory in a model-free RL approach. This involves conditioning value functions on future events and extracting relevant information from a trajectory. These future-conditional baselines and critics can then be used in policy gradient algorithms, with our method achieving lower variance and unbiasedness by restricting hindsight information to exclude agent actions. Through several illustrative problems, we demonstrate the effectiveness and validity of our algorithm.",1
"Bayesian meta-learning enables robust and fast adaptation to new tasks with uncertainty assessment. The key idea behind Bayesian meta-learning is empirical Bayes inference of hierarchical model. In this work, we extend this framework to include a variety of existing methods, before proposing our variant based on gradient-EM algorithm. Our method improves computational efficiency by avoiding back-propagation computation in the meta-update step, which is exhausting for deep neural networks. Furthermore, it provides flexibility to the inner-update optimization procedure by decoupling it from meta-update. Experiments on sinusoidal regression, few-shot image classification, and policy-based reinforcement learning show that our method not only achieves better accuracy with less computation cost, but is also more robust to uncertainty.",0
"By utilizing Bayesian meta-learning, one can quickly and confidently adapt to new tasks. The central concept behind this approach is the empirical Bayes inference of a hierarchical model. In this study, we expand upon this framework by incorporating various existing methods and presenting our version that utilizes the gradient-EM algorithm. Our approach enhances computational efficiency by circumventing back-propagation calculations during the meta-update stage, which can be laborious for deep neural networks. Additionally, it allows for flexibility in the inner-update optimization procedure by separating it from the meta-update. Through experiments involving sinusoidal regression, few-shot image classification, and policy-based reinforcement learning, we demonstrate that our method not only produces better accuracy while utilizing fewer computational resources, but is also more resilient to uncertainty.",1
"We generalize the existing principle of the maximum Shannon entropy in reinforcement learning (RL) to weighted entropy by characterizing the state-action pairs with some qualitative weights, which can be connected with prior knowledge, experience replay, and evolution process of the policy. We propose an algorithm motivated for self-balancing exploration with the introduced weight function, which leads to state-of-the-art performance on Mujoco tasks despite its simplicity in implementation.",0
"Our approach involves extending the maximum Shannon entropy principle used in reinforcement learning (RL) to incorporate weighted entropy. This is achieved by assigning qualitative weights to state-action pairs, which can be informed by prior knowledge, experience replay, and policy evolution. To facilitate self-balancing exploration, we introduce a weight function and develop an algorithm that is both simple to implement and achieves state-of-the-art performance on Mujoco tasks.",1
"5G beyond is an end-edge-cloud orchestrated network that can exploit heterogeneous capabilities of the end devices, edge servers, and the cloud and thus has the potential to enable computation-intensive and delay-sensitive applications via computation offloading. However, in multi user wireless networks, diverse application requirements and the possibility of various radio access modes for communication among devices make it challenging to design an optimal computation offloading scheme. In addition, having access to complete network information that includes variables such as wireless channel state, and available bandwidth and computation resources, is a major issue. Deep Reinforcement Learning (DRL) is an emerging technique to address such an issue with limited and less accurate network information. In this paper, we utilize DRL to design an optimal computation offloading and resource allocation strategy for minimizing system energy consumption. We first present a multi-user end-edge-cloud orchestrated network where all devices and base stations have computation capabilities. Then, we formulate the joint computation offloading and resource allocation problem as a Markov Decision Process (MDP) and propose a new DRL algorithm to minimize system energy consumption. Numerical results based on a real-world dataset demonstrate that the proposed DRL-based algorithm significantly outperforms the benchmark policies in terms of system energy consumption. Extensive simulations show that learning rate, discount factor, and number of devices have considerable influence on the performance of the proposed algorithm.",0
"The end-edge-cloud orchestrated network of 5G has the potential to allow for computation-intensive and delay-sensitive applications through computation offloading. However, designing an optimal offloading scheme is difficult due to the diverse application requirements and various radio access modes in multi user wireless networks. Limited and inaccurate network information also poses a major challenge. To address this issue, we propose using Deep Reinforcement Learning (DRL) to minimize system energy consumption. Our approach involves formulating the problem as a Markov Decision Process (MDP) and utilizing a new DRL algorithm. Our algorithm outperforms benchmark policies in terms of system energy consumption, as demonstrated by real-world dataset-based numerical results. We also found that the learning rate, discount factor, and number of devices greatly influence the algorithm's performance. Our paper presents a multi-user end-edge-cloud orchestrated network where all devices and base stations have computation capabilities.",1
"The rapid development of Industrial Internet of Things (IIoT) requires industrial production towards digitalization to improve network efficiency. Digital Twin is a promising technology to empower the digital transformation of IIoT by creating virtual models of physical objects. However, the provision of network efficiency in IIoT is very challenging due to resource-constrained devices, stochastic tasks, and resources heterogeneity. Distributed resources in IIoT networks can be efficiently exploited through computation offloading to reduce energy consumption while enhancing data processing efficiency. In this paper, we first propose a new paradigm Digital Twin Networks (DTN) to build network topology and the stochastic task arrival model in IIoT systems. Then, we formulate the stochastic computation offloading and resource allocation problem to minimize the long-term energy efficiency. As the formulated problem is a stochastic programming problem, we leverage Lyapunov optimization technique to transform the original problem into a deterministic per-time slot problem. Finally, we present Asynchronous Actor-Critic (AAC) algorithm to find the optimal stochastic computation offloading policy. Illustrative results demonstrate that our proposed scheme is able to significantly outperforms the benchmarks.",0
"The Industrial Internet of Things (IIoT) is rapidly developing and requires the digitalization of industrial production to enhance network efficiency. Digital Twin technology creates virtual models of physical objects and is a promising tool for the digital transformation of IIoT. However, network efficiency in IIoT is challenging due to the limited resources of devices, stochastic tasks, and resource heterogeneity. Computation offloading can efficiently utilize distributed resources in IIoT networks to reduce energy consumption and enhance data processing efficiency. This paper proposes a Digital Twin Networks (DTN) paradigm to establish network topology and a stochastic task arrival model in IIoT systems. We formulate a stochastic computation offloading and resource allocation problem to minimize long-term energy efficiency and use the Lyapunov optimization technique to transform it into a deterministic per-time slot problem. Finally, we present the Asynchronous Actor-Critic (AAC) algorithm to find the optimal stochastic computation offloading policy. Our proposed scheme significantly outperforms benchmarks, as shown by illustrative results.",1
"Reinforcement learning (RL) is a powerful framework for learning to take actions to solve tasks. However, in many settings, an agent must winnow down the inconceivably large space of all possible tasks to the single task that it is currently being asked to solve. Can we instead constrain the space of tasks to those that are semantically meaningful? In this work, we introduce a framework for using weak supervision to automatically disentangle this semantically meaningful subspace of tasks from the enormous space of nonsensical ""chaff"" tasks. We show that this learned subspace enables efficient exploration and provides a representation that captures distance between states. On a variety of challenging, vision-based continuous control problems, our approach leads to substantial performance gains, particularly as the complexity of the environment grows.",0
"The framework of reinforcement learning (RL) is a formidable tool for tackling tasks by learning to take actions. However, in many scenarios, the agent must narrow down the vast expanse of all possible tasks to the specific task at hand. The question is whether we can limit the task space to those that hold semantic significance. This study presents a method for utilizing weak supervision to automatically extract the semantically meaningful subspace of tasks from the vast pool of nonsensical tasks. This technique enables efficient exploration and creates a representation that captures the distance between states. Our approach leads to significant performance gains on various challenging, vision-based continuous control problems, especially as the environment's complexity increases.",1
"The deployment of reinforcement learning (RL) in the real world comes with challenges in calibrating user trust and expectations. As a step toward developing RL systems that are able to communicate their competencies, we present a method of generating human-interpretable abstract behavior models that identify the experiential conditions leading to different task execution strategies and outcomes. Our approach consists of extracting experiential features from state representations, abstracting strategy descriptors from trajectories, and training an interpretable decision tree that identifies the conditions most predictive of different RL behaviors. We demonstrate our method on trajectory data generated from interactions with the environment and on imagined trajectory data that comes from a trained probabilistic world model in a model-based RL setting.",0
"Deploying reinforcement learning (RL) in the real world presents challenges in managing user expectations and trust. To address this, we propose a method for creating abstract behavior models that outline the experiential conditions that result in different task execution strategies and outcomes. Our approach entails extracting experiential features from state representations, abstracting strategy descriptors from trajectories, and training an interpretable decision tree that identifies the most predictive conditions for different RL behaviors. We demonstrate our method using trajectory data from interactions with the environment, as well as imagined trajectory data generated from a trained probabilistic world model in a model-based RL setting.",1
"In this work, we propose a multi-agent actor-critic reinforcement learning (RL) algorithm to accelerate the multi-level Monte Carlo Markov Chain (MCMC) sampling algorithms. The policies (actors) of the agents are used to generate the proposal in the MCMC steps; and the critic, which is centralized, is in charge of estimating the long term reward. We verify our proposed algorithm by solving an inverse problem with multiple scales. There are several difficulties in the implementation of this problem by using traditional MCMC sampling. Firstly, the computation of the posterior distribution involves evaluating the forward solver, which is very time consuming for a problem with heterogeneous. We hence propose to use the multi-level algorithm. More precisely, we use the generalized multiscale finite element method (GMsFEM) as the forward solver in evaluating a posterior distribution in the multi-level rejection procedure. Secondly, it is hard to find a function which can generate samplings which are meaningful. To solve this issue, we learn an RL policy as the proposal generator. Our experiments show that the proposed method significantly improves the sampling process",0
"Our study introduces a novel approach to accelerate multi-level Monte Carlo Markov Chain (MCMC) sampling algorithms using a multi-agent actor-critic reinforcement learning (RL) algorithm. The actors generate proposals in the MCMC steps, while the centralized critic estimates the long term reward. We demonstrate the effectiveness of our algorithm in solving an inverse problem with multiple scales. Traditional MCMC sampling methods often face challenges in such problems due to time-consuming forward solver computations and difficulty in generating meaningful samplings. To overcome these issues, we utilize the generalized multiscale finite element method (GMsFEM) as the forward solver and learn an RL policy as the proposal generator. Our experimental results confirm the significant improvement in the sampling process achieved by our proposed method.",1
"In video games, non-player characters (NPCs) are used to enhance the players' experience in a variety of ways, e.g., as enemies, allies, or innocent bystanders. A crucial component of NPCs is navigation, which allows them to move from one point to another on the map. The most popular approach for NPC navigation in the video game industry is to use a navigation mesh (NavMesh), which is a graph representation of the map, with nodes and edges indicating traversable areas. Unfortunately, complex navigation abilities that extend the character's capacity for movement, e.g., grappling hooks, jetpacks, teleportation, or double-jumps, increases the complexity of the NavMesh, making it intractable in many practical scenarios. Game designers are thus constrained to only add abilities that can be handled by a NavMesh if they want to have NPC navigation. As an alternative, we propose to use Deep Reinforcement Learning (Deep RL) to learn how to navigate 3D maps using any navigation ability. We test our approach on complex 3D environments in the Unity game engine that are notably an order of magnitude larger than maps typically used in the Deep RL literature. One of these maps is directly modeled after a Ubisoft AAA game. We find that our approach performs surprisingly well, achieving at least $90\%$ success rate on all tested scenarios. A video of our results is available at https://youtu.be/WFIf9Wwlq8M.",0
"Non-player characters (NPCs) play a crucial role in video games by providing a range of experiences such as serving as enemies, allies, or bystanders. NPC navigation is essential for these characters to move around the map. The most commonly used method for NPC navigation is through a navigation mesh (NavMesh), which is a graph representation of the map indicating traversable areas. However, the complexity of the NavMesh increases with the addition of new abilities, such as grappling hooks, jetpacks, teleportation, or double-jumps. This has limited game designers to only add abilities that can be handled by NavMesh. To overcome this limitation, we propose the use of Deep Reinforcement Learning (Deep RL), which enables NPCs to navigate 3D maps with any ability. Our approach was tested on large, complex 3D environments in the Unity game engine, including one modeled after a Ubisoft AAA game. We achieved a minimum success rate of 90%, and a video demonstrating our results is available at https://youtu.be/WFIf9Wwlq8M.",1
"This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.",0
"The article details the development of REALab, a reinforcement learning (RL) platform that enables embedded agency research. The platform is specifically designed to replicate the structure of tampering issues that can occur in real-world RL deployments. Traditional MDP RL formulations and simulated environments assume that feedback (rewards) is secure, which may not be the case when agents are embedded and can corrupt feedback processes, such as human supervisors or reward functions. To address this issue, the article introduces a Corrupt Feedback MDP formulation and the REALab environment platform, which eliminates the secure feedback assumption. The authors hope that REALab's design will provide valuable insights into tampering issues and serve as a unit test for detecting tampering incentives in RL agent designs.",1
"Real-time object detection in videos using lightweight hardware is a crucial component of many robotic tasks. Detectors using different modalities and with varying computational complexities offer different trade-offs. One option is to have a very lightweight model that can predict from all modalities at once for each frame. However, in some situations (e.g., in static scenes) it might be better to have a more complex but more accurate model and to extrapolate from previous predictions for the frames coming in at processing time. We formulate this task as a sequential decision making problem and use reinforcement learning (RL) to generate a policy that decides from the RGB input which detector out of a portfolio of different object detectors to take for the next prediction. The objective of the RL agent is to maximize the accuracy of the predictions per image. We evaluate the approach on the Waymo Open Dataset and show that it exceeds the performance of each single detector.",0
"A crucial aspect of many robotic tasks involves real-time object detection in videos using lightweight hardware. Various detectors with different modalities and computational complexities present different trade-offs. One option is to employ a very lightweight model capable of predicting from all modalities concurrently for each frame. However, in certain situations, such as static scenes, it may be preferable to use a more complex but more accurate model and extrapolate from prior predictions for the incoming frames during processing. To achieve this, we frame the task as a sequential decision-making problem and utilize reinforcement learning (RL) to develop a policy that selects from a range of different object detectors based on RGB input for the next prediction. The RL agent aims to maximize the accuracy of the predictions for each image. Our evaluation on the Waymo Open Dataset demonstrates that our approach outperforms individual detectors.",1
"Amodal recognition is the ability of the system to detect occluded objects. Most state-of-the-art Visual Recognition systems lack the ability to perform amodal recognition. Few studies have achieved amodal recognition through passive prediction or embodied recognition approaches. However, these approaches suffer from challenges in real-world applications, such as dynamic objects. We propose SeekNet, an improved optimization method for amodal recognition through embodied visual recognition. Additionally, we implement SeekNet for social robots, where there are multiple interactions with crowded humans. Hence, we focus on occluded human detection & tracking and showcase the superiority of our algorithm over other baselines. We also experiment with SeekNet to improve the confidence of COVID-19 symptoms pre-screening algorithms using our efficient embodied recognition system.",0
"The ability to detect occluded objects is known as amodal recognition, which is lacking in most advanced Visual Recognition systems. Some studies have achieved amodal recognition through passive prediction or embodied recognition approaches, but these methods face challenges in real-world scenarios involving dynamic objects. Our proposal, SeekNet, involves an improved optimization method for amodal recognition using embodied visual recognition. We implement this method for social robots, which require the detection and tracking of occluded humans in crowded environments. We demonstrate the superiority of SeekNet over other baselines. Moreover, we experiment with SeekNet to enhance the accuracy of COVID-19 symptoms pre-screening algorithms using our efficient embodied recognition system.",1
"This paper introduces a method for constructing an upper bound for exploration policy using either the weighted variance of return sequences or the weighted temporal difference (TD) error. We demonstrate that the variance of the return sequence for a specific state-action pair is an important information source that can be leveraged to guide exploration in reinforcement learning. The intuition is that fluctuation in the return sequence indicates greater uncertainty in the near future returns. This divergence occurs because of the cyclic nature of value-based reinforcement learning; the evolving value function begets policy improvements which in turn modify the value function. Although both variance and TD errors capture different aspects of this uncertainty, our analysis shows that both can be valuable to guide exploration. We propose a two-stream network architecture to estimate weighted variance/TD errors within DQN agents for our exploration method and show that it outperforms the baseline on a wide range of Atari games.",0
"In this paper, a technique is presented to establish an upper limit for exploration policy by utilizing either the weighted variance of return sequences or the weighted temporal difference (TD) error. The authors demonstrate that the return sequence variance for a specific state-action pair is a crucial source of information that can be utilized to direct exploration in reinforcement learning. The authors assert that fluctuations in the return sequence indicate greater uncertainty in future returns. This divergence arises due to the cyclic nature of value-based reinforcement learning, where the evolving value function leads to policy improvements that modify the value function. Despite the fact that variance and TD errors capture distinct aspects of this uncertainty, the authors' analysis reveals that both can be advantageous in guiding exploration. The authors suggest a two-stream network architecture for estimating weighted variance/TD errors within DQN agents for their exploration approach and demonstrate its superiority over the baseline on a wide range of Atari games.",1
"The problem of inverse reinforcement learning (IRL) is relevant to a variety of tasks including value alignment and robot learning from demonstration. Despite significant algorithmic contributions in recent years, IRL remains an ill-posed problem at its core; multiple reward functions coincide with the observed behavior and the actual reward function is not identifiable without prior knowledge or supplementary information. This paper presents an IRL framework called Bayesian optimization-IRL (BO-IRL) which identifies multiple solutions that are consistent with the expert demonstrations by efficiently exploring the reward function space. BO-IRL achieves this by utilizing Bayesian Optimization along with our newly proposed kernel that (a) projects the parameters of policy invariant reward functions to a single point in a latent space and (b) ensures nearby points in the latent space correspond to reward functions yielding similar likelihoods. This projection allows the use of standard stationary kernels in the latent space to capture the correlations present across the reward function space. Empirical results on synthetic and real-world environments (model-free and model-based) show that BO-IRL discovers multiple reward functions while minimizing the number of expensive exact policy optimizations.",0
"Inverse reinforcement learning (IRL) is a significant issue in various tasks, such as robot learning from demonstration and value alignment. Despite recent algorithmic advancements, IRL remains a challenging problem as multiple reward functions can coincide with observed behavior, and the actual reward function is not identifiable without prior knowledge or additional information. This article introduces a new IRL framework called Bayesian optimization-IRL (BO-IRL), which explores the reward function space efficiently and identifies multiple solutions consistent with expert demonstrations. BO-IRL achieves this by using Bayesian Optimization with a newly proposed kernel that projects policy invariant reward function parameters to a single point in a latent space while ensuring nearby points correspond to reward functions with similar likelihoods. This projection enables the use of standard stationary kernels in the latent space to capture correlations present across the reward function space. Empirical results on synthetic and real-world environments (model-free and model-based) demonstrate that BO-IRL discovers multiple reward functions while minimizing the number of expensive exact policy optimizations.",1
"Emerging technologies such as digital twins and 6th Generation mobile networks (6G) have accelerated the realization of edge intelligence in Industrial Internet of Things (IIoT). The integration of digital twin and 6G bridges the physical system with digital space and enables robust instant wireless connectivity. With increasing concerns on data privacy, federated learning has been regarded as a promising solution for deploying distributed data processing and learning in wireless networks. However, unreliable communication channels, limited resources, and lack of trust among users, hinder the effective application of federated learning in IIoT. In this paper, we introduce the Digital Twin Wireless Networks (DTWN) by incorporating digital twins into wireless networks, to migrate real-time data processing and computation to the edge plane. Then, we propose a blockchain empowered federated learning framework running in the DTWN for collaborative computing, which improves the reliability and security of the system, and enhances data privacy. Moreover, to balance the learning accuracy and time cost of the proposed scheme, we formulate an optimization problem for edge association by jointly considering digital twin association, training data batch size, and bandwidth allocation. We exploit multi-agent reinforcement learning to find an optimal solution to the problem. Numerical results on real-world dataset show that the proposed scheme yields improved efficiency and reduced cost compared to benchmark learning method.",0
"The application of edge intelligence in Industrial Internet of Things (IIoT) has been accelerated by emerging technologies like digital twins and 6th Generation mobile networks (6G). These technologies enable the integration of physical systems with digital space and provide robust instant wireless connectivity. To address data privacy concerns, federated learning has been proposed as a promising solution for distributed data processing and learning. However, challenges such as unreliable communication channels, limited resources, and a lack of trust among users hinder the effective application of federated learning in IIoT. This paper proposes the integration of digital twins into wireless networks, creating the Digital Twin Wireless Networks (DTWN), to facilitate real-time data processing and computation at the edge. A blockchain empowered federated learning framework is then introduced in the DTWN for collaborative computing, improving the reliability and security of the system, and enhancing data privacy. To balance learning accuracy and time cost, an optimization problem for edge association is formulated, taking into account digital twin association, training data batch size, and bandwidth allocation. Multi-agent reinforcement learning is used to find an optimal solution to the problem. Numerical results on real-world datasets demonstrate that the proposed scheme is more efficient and cost-effective compared to benchmark learning methods.",1
"Microgrids (MGs) are small, local power grids that can operate independently from the larger utility grid. Combined with the Internet of Things (IoT), a smart MG can leverage the sensory data and machine learning techniques for intelligent energy management. This paper focuses on deep reinforcement learning (DRL)-based energy dispatch for IoT-driven smart isolated MGs with diesel generators (DGs), photovoltaic (PV) panels, and a battery. A finite-horizon Partial Observable Markov Decision Process (POMDP) model is formulated and solved by learning from historical data to capture the uncertainty in future electricity consumption and renewable power generation. In order to deal with the instability problem of DRL algorithms and unique characteristics of finite-horizon models, two novel DRL algorithms, namely, finite-horizon deep deterministic policy gradient (FH-DDPG) and finite-horizon recurrent deterministic policy gradient (FH-RDPG), are proposed to derive energy dispatch policies with and without fully observable state information. A case study using real isolated MG data is performed, where the performance of the proposed algorithms are compared with the other baseline DRL and non-DRL algorithms. Moreover, the impact of uncertainties on MG performance is decoupled into two levels and evaluated respectively.",0
"This paper examines the application of Microgrids (MGs) and the Internet of Things (IoT) in creating smart, self-sufficient power grids. By utilizing sensory data and machine learning techniques, MGs can effectively manage energy consumption and generation. The focus of this study is on the use of deep reinforcement learning (DRL) for energy dispatch in smart, isolated MGs with diesel generators, photovoltaic panels, and a battery. To account for uncertainty in future electricity consumption and renewable power generation, a finite-horizon Partial Observable Markov Decision Process (POMDP) model is formulated and solved using historical data. To address instability issues with DRL algorithms and finite-horizon models, two new DRL algorithms, finite-horizon deep deterministic policy gradient (FH-DDPG) and finite-horizon recurrent deterministic policy gradient (FH-RDPG), are introduced. These algorithms are compared with other baseline DRL and non-DRL algorithms using real isolated MG data. Additionally, the impact of uncertainties on MG performance is evaluated at two levels.",1
"Reinforcement learning typically assumes that the state update from the previous actions happens instantaneously, and thus can be used for making future decisions. However, this may not always be true. When the state update is not available, the decision taken is partly in the blind since it cannot rely on the current state information. This paper proposes an approach, where the delay in the knowledge of the state can be used, and the decisions are made based on the available information which may not include the current state information. One approach could be to include the actions after the last-known state as a part of the state information, however, that leads to an increased state-space making the problem complex and slower in convergence. The proposed algorithm gives an alternate approach where the state space is not enlarged, as compared to the case when there is no delay in the state update. Evaluations on the basic RL environments further illustrate the improved performance of the proposed algorithm.",0
"The conventional assumption in reinforcement learning is that the state update occurs instantly and can be utilized for future decision-making. However, this may not always be the case, resulting in partially blind decisions when state information is unavailable. A new approach is presented in this paper, which utilizes delayed state knowledge for decision-making based on the available information. One possible solution is to include actions after the last-known state in the state information; however, this increases the state-space complexity and slows convergence. The proposed algorithm offers an alternative approach that does not expand the state space, resulting in improved performance compared to the case without state update delays. Evaluations on basic RL environments demonstrate the algorithm's effectiveness.",1
"Deep reinforcement learning methods have achieved state-of-the-art results in a variety of challenging, high-dimensional domains ranging from video games to locomotion. The key to success has been the use of deep neural networks used to approximate the policy and value function. Yet, substantial tuning of weights is required for good results. We instead use randomized function approximation. Such networks are not only cheaper than training fully connected networks but also improve the numerical performance. We present \texttt{RANDPOL}, a generalized policy iteration algorithm for MDPs with continuous state and action spaces. Both the policy and value functions are represented with randomized networks. We also give finite time guarantees on the performance of the algorithm. Then we show the numerical performance on challenging environments and compare them with deep neural network based algorithms.",0
"The use of deep reinforcement learning methods has led to groundbreaking achievements in complex domains, like video games and locomotion. These methods rely on deep neural networks to approximate the policy and value function, but they require significant weight tuning to produce good results. In contrast, we propose using randomized function approximation, which is not only less expensive than training fully connected networks but also enhances numerical performance. Our approach, called \texttt{RANDPOL}, is a generalized policy iteration algorithm designed for continuous state and action spaces in MDPs. We represent both the policy and value functions using randomized networks and provide finite time guarantees for algorithm performance. Finally, we demonstrate the effectiveness of our approach on challenging environments and compare our results to those obtained using deep neural network-based techniques.",1
"In this work, we present a learning based approach to analog circuit design, where the goal is to optimize circuit performance subject to certain design constraints. One of the aspects that makes this problem challenging to optimize, is that measuring the performance of candidate configurations with simulation can be computationally expensive, particularly in the post-layout design. Additionally, the large number of design constraints and the interaction between the relevant quantities makes the problem complex. Therefore, to better facilitate supporting the human designers, it is desirable to gain knowledge about the whole space of feasible solutions. In order to tackle these challenges, we take inspiration from model-based reinforcement learning and propose a method with two key properties. First, it learns a reward model, i.e., surrogate model of the performance approximated by neural networks, to reduce the required number of simulation. Second, it uses a stochastic policy generator to explore the diverse solution space satisfying constraints. Together we combine these in a Dyna-style optimization framework, which we call DynaOpt, and empirically evaluate the performance on a circuit benchmark of a two-stage operational amplifier. The results show that, compared to the model-free method applied with 20,000 circuit simulations to train the policy, DynaOpt achieves even much better performance by learning from scratch with only 500 simulations.",0
"Our work proposes a learning-based approach to optimize analog circuit design while adhering to certain constraints. However, this task is complex due to the high computational cost of simulating candidate configurations, particularly in post-layout design, as well as the numerous design constraints and interdependent quantities. To aid human designers, we require knowledge about the entire space of feasible solutions. Therefore, we draw inspiration from model-based reinforcement learning and present a method that has two key features. Firstly, it uses a reward model, approximated by neural networks, to reduce the number of required simulations. Secondly, it employs a stochastic policy generator to explore diverse solutions that satisfy constraints. We combine these properties in a Dyna-style optimization framework, DynaOpt, and evaluate its success using a two-stage operational amplifier circuit benchmark. Our results indicate that DynaOpt outperforms the model-free method, which requires 20,000 circuit simulations to train the policy, by learning from scratch with just 500 simulations.",1
"Robust perception relies on both bottom-up and top-down signals. Bottom-up signals consist of what's directly observed through sensation. Top-down signals consist of beliefs and expectations based on past experience and short-term memory, such as how the phrase `peanut butter and~...' will be completed. The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow. We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention. Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data. We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \emph{bidirectional} information flow can improve results over strong baselines.",0
"To achieve robust perception, a combination of both bottom-up and top-down signals is necessary. The former refers to direct sensory observations, while the latter is based on past experiences and short-term memory. The ideal way of combining these signals remains unknown, but it must be adaptable according to the context and task at hand. To make use of top-down information effectively, mechanisms are required to regulate information flow and avoid confusion. Our research investigates the use of deep recurrent neural nets, which utilize attention and modularity to direct information flow and improve performance in perceptual and language tasks. This approach is particularly useful in noisy or distracting environments. Our experiments demonstrate that bidirectional information flow can enhance results over strong baselines in areas such as language modeling, sequential image classification, video prediction, and reinforcement learning.",1
"We present HiDe, a novel hierarchical reinforcement learning architecture that successfully solves long horizon control tasks and generalizes to unseen test scenarios. Functional decomposition between planning and low-level control is achieved by explicitly separating the state-action spaces across the hierarchy, which allows the integration of task-relevant knowledge per layer. We propose an RL-based planner to efficiently leverage the information in the planning layer of the hierarchy, while the control layer learns a goal-conditioned control policy. The hierarchy is trained jointly but allows for the composition of different policies such as transferring layers across multiple agents. We experimentally show that our method generalizes across unseen test environments and can scale to tasks well beyond 3x horizon length compared to both learning and non-learning based approaches. We evaluate on complex continuous control tasks with sparse rewards, including navigation and robot manipulation.",0
"Introducing HiDe, a pioneering reinforcement learning structure that can competently tackle long-term control problems and adapt to new test scenarios. The hierarchical design efficiently divides the planning and low-level control functions by segregating the state-action spaces, enabling the incorporation of task-specific knowledge in each layer. Our approach employs an RL-based planner to effectively use the information in the planning layer, while the control layer learns a goal-oriented control policy. The architecture is trained together, but it permits diverse policies to be combined, such as sharing layers among multiple agents. Through experimentation, we demonstrate that our method can generalize to unseen environments and is capable of handling tasks beyond three times the horizon length, compared to other learning and non-learning-based methods. Our evaluation includes complex continuous control tasks that have sparse rewards, such as navigation and robot manipulation.",1
"One of the most challenging aspects of real-world reinforcement learning (RL) is the multitude of unpredictable and ever-changing distractions that could divert an agent from what was tasked to do in its training environment. While an agent could learn from reward signals to ignore them, the complexity of the real-world can make rewards hard to acquire, or, at best, extremely sparse. A recent class of self-supervised methods have shown promise that reward-free adaptation under challenging distractions is possible. However, previous work focused on a short one-episode adaptation setting. In this paper, we consider a long-term adaptation setup that is more akin to the specifics of the real-world and propose a geometric perspective on self-supervised adaptation. We empirically describe the processes that take place in the embedding space during this adaptation process, reveal some of its undesirable effects on performance and show how they can be eliminated. Moreover, we theoretically study how actor-based and actor-free agents can further generalise to the target environment by manipulating the geometry of the manifolds described by the actor and critic functions.",0
"Real-world reinforcement learning (RL) is challenging due to the unpredictable and ever-changing distractions that can divert an agent from its training tasks. Although an agent can learn to ignore these distractions through reward signals, obtaining rewards can be difficult in complex real-world environments. Recent self-supervised methods have shown promise for reward-free adaptation under challenging distractions, but previous work focused on short one-episode adaptation settings. In this paper, we propose a geometric perspective on self-supervised adaptation for a long-term adaptation setup that is more similar to real-world scenarios. We describe the processes that occur in the embedding space during this adaptation process and identify some undesirable effects on performance, which we show can be eliminated. Additionally, we explore how actor-based and actor-free agents can further generalise to the target environment by manipulating the geometry of the manifolds described by the actor and critic functions.",1
"A major challenge in the pharmaceutical industry is to design novel molecules with specific desired properties, especially when the property evaluation is costly. Here, we propose MNCE-RL, a graph convolutional policy network for molecular optimization with molecular neighborhood-controlled embedding grammars through reinforcement learning. We extend the original neighborhood-controlled embedding grammars to make them applicable to molecular graph generation and design an efficient algorithm to infer grammatical production rules from given molecules. The use of grammars guarantees the validity of the generated molecular structures. By transforming molecular graphs to parse trees with the inferred grammars, the molecular structure generation task is modeled as a Markov decision process where a policy gradient strategy is utilized. In a series of experiments, we demonstrate that our approach achieves state-of-the-art performance in a diverse range of molecular optimization tasks and exhibits significant superiority in optimizing molecular properties with a limited number of property evaluations.",0
"The pharmaceutical industry faces a significant hurdle in creating new molecules that possess specific characteristics, particularly when the process of evaluating those properties is expensive. Our proposed solution to this problem is MNCE-RL, a graph convolutional policy network that utilizes molecular neighborhood-controlled embedding grammars through reinforcement learning. To make these grammars applicable to molecular graph generation, we have extended their use and developed an efficient algorithm to infer grammatical production rules from given molecules. The use of grammars ensures the validity of the generated molecular structures. By converting molecular graphs to parse trees with the inferred grammars, we have modeled the molecular structure generation task as a Markov decision process, utilizing a policy gradient strategy. Through a series of experiments, we have demonstrated the effectiveness of our approach in a diverse range of molecular optimization tasks and its superiority in optimizing molecular properties with a limited number of property evaluations.",1
"Convex optimizers have known many applications as differentiable layers within deep neural architectures. One application of these convex layers is to project points into a convex set. However, both forward and backward passes of these convex layers are significantly more expensive to compute than those of a typical neural network. We investigate in this paper whether an inexact, but cheaper projection, can drive a descent algorithm to an optimum. Specifically, we propose an interpolation-based projection that is computationally cheap and easy to compute given a convex, domain defining, function. We then propose an optimization algorithm that follows the gradient of the composition of the objective and the projection and prove its convergence for linear objectives and arbitrary convex and Lipschitz domain defining inequality constraints. In addition to the theoretical contributions, we demonstrate empirically the practical interest of the interpolation projection when used in conjunction with neural networks in a reinforcement learning and a supervised learning setting.",0
"Convex optimizers are commonly used as differentiable layers in deep neural networks. One of their applications is projecting points into a convex set, but this process is computationally expensive for both forward and backward passes compared to a typical neural network. In this paper, we explore the possibility of using a cheaper, inexact projection to reach an optimum through a descent algorithm. Our proposal is an interpolation-based projection that is easy to compute given a convex function that defines the domain. We also introduce an optimization algorithm that follows the gradient of the objective and the projection composition, and prove its convergence for linear objectives and arbitrary convex and Lipschitz domain defining inequality constraints. Additionally, we demonstrate the practical usefulness of the interpolation projection in reinforcement and supervised learning settings when combined with neural networks. Along with our theoretical contributions, these empirical results highlight the potential benefits of the proposed approach.",1
"This paper extends the Distributionally Robust Optimization (DRO) approach for offline contextual bandits. Specifically, we leverage this framework to introduce a convex reformulation of the Counterfactual Risk Minimization principle. Besides relying on convex programs, our approach is compatible with stochastic optimization, and can therefore be readily adapted tothe large data regime. Our approach relies on the construction of asymptotic confidence intervals for offline contextual bandits through the DRO framework. By leveraging known asymptotic results of robust estimators, we also show how to automatically calibrate such confidence intervals, which in turn removes the burden of hyper-parameter selection for policy optimization. We present preliminary empirical results supporting the effectiveness of our approach.",0
The Distributionally Robust Optimization (DRO) approach is expanded in this paper to incorporate offline contextual bandits. A convex reformulation of the Counterfactual Risk Minimization principle is introduced by utilizing this framework. Our approach is compatible with stochastic optimization and can be easily adapted to large data sets since it relies on convex programs. The DRO framework is used to construct asymptotic confidence intervals for offline contextual bandits. We also demonstrate how to automatically calibrate these confidence intervals by utilizing known asymptotic results of robust estimators. This eliminates the need for hyper-parameter selection for policy optimization. Preliminary empirical results are presented to support the effectiveness of our approach.,1
"Current image-based reinforcement learning (RL) algorithms typically operate on the whole image without performing object-level reasoning. This leads to inefficient goal sampling and ineffective reward functions. In this paper, we improve upon previous visual self-supervised RL by incorporating object-level reasoning and occlusion reasoning. Specifically, we use unknown object segmentation to ignore distractors in the scene for better reward computation and goal generation; we further enable occlusion reasoning by employing a novel auxiliary loss and training scheme. We demonstrate that our proposed algorithm, ROLL (Reinforcement learning with Object Level Learning), learns dramatically faster and achieves better final performance compared with previous methods in several simulated visual control tasks. Project video and code are available at https://sites.google.com/andrew.cmu.edu/roll.",0
"Typically, image-based reinforcement learning (RL) algorithms work on the entire image and do not engage in object-level reasoning. Consequently, this leads to inefficient goal sampling and ineffective reward functions. In this study, we have advanced the prior visual self-supervised RL by integrating object-level reasoning and occlusion reasoning. Specifically, we have utilized unknown object segmentation to disregard extraneous elements in the surroundings while computing rewards and generating goals. Additionally, we have introduced a new auxiliary loss and training scheme to enable occlusion reasoning. We have proved that our proposed algorithm, ROLL (Reinforcement learning with Object Level Learning), learns much faster and delivers better final outcomes than previous techniques in various simulated visual control tasks. Our project video and code can be found at https://sites.google.com/andrew.cmu.edu/roll.",1
"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods from AlphaGo to Muzero have enjoyed huge success in discrete domains, such as chess and Go. Unfortunately, in real-world applications like robot control and inverted pendulum, whose action space is normally continuous, those tree-based planning techniques will be struggling. To address those limitations, in this paper, we present a novel model-based reinforcement learning frameworks called Critic PI2, which combines the benefits from trajectory optimization, deep actor-critic learning, and model-based reinforcement learning. Our method is evaluated for inverted pendulum models with applicability to many continuous control systems. Extensive experiments demonstrate that Critic PI2 achieved a new state of the art in a range of challenging continuous domains. Furthermore, we show that planning with a critic significantly increases the sample efficiency and real-time performance. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.",0
"For a long time, creating agents with planning abilities has been a major obstacle in the development of artificial intelligence. Although tree-based planning techniques like those used in AlphaGo and Muzero have been successful in discrete domains such as chess and Go, they struggle in real-world applications like robot control and inverted pendulum, where the action space is continuous. To overcome these limitations, we introduce a new model-based reinforcement learning framework called Critic PI2, which combines trajectory optimization, deep actor-critic learning, and model-based reinforcement learning. We evaluate our method on inverted pendulum models, which can be applied to various continuous control systems. Our extensive experiments demonstrate that Critic PI2 outperforms previous techniques in challenging continuous domains and significantly increases sample efficiency and real-time performance by planning with a critic. Our work represents a new direction in learning the components of a model-based planning system and how to utilize them.",1
"In searching for a generalizable representation of temporally extended tasks, we spot two necessary constituents: the utility needs to be non-Markovian to transfer temporal relations invariant to a probability shift, the utility also needs to be lifted to abstract out specific grounding objects. In this work, we study learning such utility from human demonstrations. While inverse reinforcement learning (IRL) has been accepted as a general framework of utility learning, its fundamental formulation is one concrete Markov Decision Process. Thus the learned reward function does not specify the task independently of the environment. Going beyond that, we define a domain of generalization that spans a set of planning problems following a schema. We hence propose a new quest, Generalized Inverse Planning, for utility learning in this domain. We further outline a computational framework, Maximum Entropy Inverse Planning (MEIP), that learns non-Markovian utility and associated concepts in a generative manner. The learned utility and concepts form a task representation that generalizes regardless of probability shift or structural change. Seeing that the proposed generalization problem has not been widely studied yet, we carefully define an evaluation protocol, with which we illustrate the effectiveness of MEIP on two proof-of-concept domains and one challenging task: learning to fold from demonstrations.",0
"In order to find a way to represent tasks that can be applied in a broad sense, we have identified two important factors: the utility must be non-Markovian in order to maintain temporal relationships despite changes in probability, and it must also be abstracted to remove specific grounding objects. Our research involves studying how to learn this utility from human demonstrations. While inverse reinforcement learning (IRL) is a commonly used framework for utility learning, it is limited to a concrete Markov Decision Process and cannot specify tasks independently of the environment. To address this limitation, we propose a new approach called Generalized Inverse Planning, which involves defining a domain of generalization that encompasses a set of planning problems following a schema. We also introduce a computational framework called Maximum Entropy Inverse Planning (MEIP) that can learn non-Markovian utility and associated concepts in a generative way. The learned utility and concepts can be used as a task representation that generalizes regardless of probability shift or structural change. Since this generalization problem has not been extensively studied yet, we have defined an evaluation protocol to demonstrate the effectiveness of MEIP on two proof-of-concept domains and a challenging task: learning to fold from demonstrations.",1
"Advances in visual navigation methods have led to intelligent embodied navigation agents capable of learning meaningful representations from raw RGB images and perform a wide variety of tasks involving structural and semantic reasoning. However, most learning-based navigation policies are trained and tested in simulation environments. In order for these policies to be practically useful, they need to be transferred to the real-world. In this paper, we propose an unsupervised domain adaptation method for visual navigation. Our method translates the images in the target domain to the source domain such that the translation is consistent with the representations learned by the navigation policy. The proposed method outperforms several baselines across two different navigation tasks in simulation. We further show that our method can be used to transfer the navigation policies learned in simulation to the real world.",0
"Recent improvements in visual navigation techniques have resulted in the development of smart navigation agents that can learn valuable representations from raw RGB images and accomplish a broad range of tasks involving structural and semantic reasoning. However, most navigation policies that are learned through training are tested in simulation environments. To make these policies practically useful, they must be transferred to the real world. This paper introduces an unsupervised domain adaptation method for visual navigation that translates target domain images to the source domain. This translation is consistent with the navigation policy's learned representations. The proposed technique surpasses several baselines for two different navigation tasks in simulation. Additionally, we demonstrate that our method can be used to transfer navigation policies learned through simulation to the real world.",1
"Reinforcement learning is a powerful framework for robots to acquire skills from experience, but often requires a substantial amount of online data collection. As a result, it is difficult to collect sufficiently diverse experiences that are needed for robots to generalize broadly. Videos of humans, on the other hand, are a readily available source of broad and interesting experiences. In this paper, we consider the question: can we perform reinforcement learning directly on experience collected by humans? This problem is particularly difficult, as such videos are not annotated with actions and exhibit substantial visual domain shift relative to the robot's embodiment. To address these challenges, we propose a framework for reinforcement learning with videos (RLV). RLV learns a policy and value function using experience collected by humans in combination with data collected by robots. In our experiments, we find that RLV is able to leverage such videos to learn challenging vision-based skills with less than half as many samples as RL methods that learn from scratch.",0
"The acquisition of skills by robots through experience is facilitated by reinforcement learning, but the process often necessitates extensive data collection, making it challenging to gather adequate and diverse experiences that enable robots to generalize effectively. Conversely, human videos offer a readily accessible source of diverse and intriguing experiences. The central question addressed in this paper is whether we can use human experience for reinforcement learning. However, this is a daunting task since videos are not annotated with actions and feature significant visual differences from the robot's embodiment. To address these obstacles, we propose a reinforcement learning with videos (RLV) framework that combines human-collected experience with robot-collected data to learn policies and value functions. Our experiments demonstrate that RLV can learn complex vision-based abilities using less than half the samples required by RL methods that start from scratch.",1
"In recent years, the interest in leveraging quantum effects for enhancing machine learning tasks has significantly increased. Many algorithms speeding up supervised and unsupervised learning were established. The first framework in which ways to exploit quantum resources specifically for the broader context of reinforcement learning were found is projective simulation. Projective simulation presents an agent-based reinforcement learning approach designed in a manner which may support quantum walk-based speed-ups. Although classical variants of projective simulation have been benchmarked against common reinforcement learning algorithms, very few formal theoretical analyses have been provided for its performance in standard learning scenarios. In this paper, we provide a detailed formal discussion of the properties of this model. Specifically, we prove that one version of the projective simulation model, understood as a reinforcement learning approach, converges to optimal behavior in a large class of Markov decision processes. This proof shows that a physically-inspired approach to reinforcement learning can guarantee to converge.",0
"The interest in utilizing quantum effects to enhance machine learning tasks has grown significantly in recent years. Numerous algorithms have been established to accelerate supervised and unsupervised learning. The first framework to explore ways to utilize quantum resources specifically for reinforcement learning on a broader scale is projective simulation. This approach to agent-based reinforcement learning is designed to support quantum walk-based speed-ups. While classical versions of projective simulation have been compared to common reinforcement learning algorithms, there have been few formal theoretical analyses of its performance in standard learning scenarios. This paper provides a detailed formal discussion of the model's properties, demonstrating that one version of the projective simulation model, as a reinforcement learning approach, achieves optimal behavior in a large class of Markov decision processes. This proof shows that a physically-inspired approach to reinforcement learning can guarantee convergence.",1
"Learning-based approaches, such as reinforcement and imitation learning are gaining popularity in decision-making for autonomous driving. However, learned policies often fail to generalize and cannot handle novel situations well. Asking and answering questions in the form of ""Would a policy perform well if the other agents had behaved differently?"" can shed light on whether a policy has seen similar situations during training and generalizes well. In this work, a counterfactual policy evaluation is introduced that makes use of counterfactual worlds - worlds in which the behaviors of others are non-actual. If a policy can handle all counterfactual worlds well, it either has seen similar situations during training or it generalizes well and is deemed to be fit enough to be executed in the actual world. Additionally, by performing the counterfactual policy evaluation, causal relations and the influence of changing vehicle's behaviors on the surrounding vehicles becomes evident. To validate the proposed method, we learn a policy using reinforcement learning for a lane merging scenario. In the application-phase, the policy is only executed after the counterfactual policy evaluation has been performed and if the policy is found to be safe enough. We show that the proposed approach significantly decreases the collision-rate whilst maintaining a high success-rate.",0
"Autonomous driving decision-making is increasingly relying on learning-based techniques, such as reinforcement and imitation learning. However, policies learned through these approaches often do not work well with new situations or fail to generalize. To determine if a policy can handle different scenarios, questions like ""Would it work if the other agents behaved differently?"" can be asked. This work introduces counterfactual policy evaluation, which assesses policies in counterfactual worlds where other agents behave differently. If a policy performs well in all counterfactual worlds, it is deemed fit for execution in the actual world. Furthermore, this evaluation method reveals causal relationships and the impact of a vehicle's behavior on surrounding vehicles. To validate this method, a reinforcement learning-based policy for a lane merging scenario is learned and evaluated using counterfactual policy evaluation. The policy is executed only if it is deemed safe, resulting in a significant decrease in collision rates while maintaining a high success rate.",1
"Wirelessly streaming high quality 360 degree videos is still a challenging problem. When there are many users watching different 360 degree videos and competing for the computing and communication resources, the streaming algorithm at hand should maximize the average quality of experience (QoE) while guaranteeing a minimum rate for each user. In this paper, we propose a \emph{cross layer} optimization approach that maximizes the available rate to each user and efficiently uses it to maximize users' QoE. Particularly, we consider a tile based 360 degree video streaming, and we optimize a QoE metric that balances the tradeoff between maximizing each user's QoE and ensuring fairness among users. We show that the problem can be decoupled into two interrelated subproblems: (i) a physical layer subproblem whose objective is to find the download rate for each user, and (ii) an application layer subproblem whose objective is to use that rate to find a quality decision per tile such that the user's QoE is maximized. We prove that the physical layer subproblem can be solved optimally with low complexity and an actor-critic deep reinforcement learning (DRL) is proposed to leverage the parallel training of multiple independent agents and solve the application layer subproblem. Extensive experiments reveal the robustness of our scheme and demonstrate its significant performance improvement compared to several baseline algorithms.",0
"Streaming high quality 360 degree videos wirelessly remains a challenge, especially when multiple users are viewing different videos and competing for computing and communication resources. To ensure a satisfactory Quality of Experience (QoE) for all users, we propose a cross-layer optimization approach that maximizes the rate available to each user while balancing QoE and fairness. Our approach optimizes a QoE metric based on tile-based 360 degree video streaming, with two subproblems: a physical layer subproblem that finds the optimal download rate for each user, and an application layer subproblem that decides the quality per tile to maximize the user's QoE. We prove that the physical layer subproblem can be solved optimally with low complexity, while the application layer subproblem can be solved using an actor-critic deep reinforcement learning (DRL) approach. Our experiments demonstrate the effectiveness and robustness of our approach, which outperforms several baseline algorithms.",1
"Sparse-reward domains are challenging for reinforcement learning algorithms since significant exploration is needed before encountering reward for the first time. Hierarchical reinforcement learning can facilitate exploration by reducing the number of decisions necessary before obtaining a reward. In this paper, we present a novel hierarchical reinforcement learning framework based on the compression of an invariant state space that is common to a range of tasks. The algorithm introduces subtasks which consist of moving between the state partitions induced by the compression. Results indicate that the algorithm can successfully solve complex sparse-reward domains, and transfer knowledge to solve new, previously unseen tasks more quickly.",0
"Reinforcement learning algorithms face difficulties in sparse-reward domains as extensive exploration is required to find the reward. To tackle this issue, hierarchical reinforcement learning can be used to minimize the number of decisions needed to obtain a reward. Our study introduces a new hierarchical reinforcement learning framework that compresses an invariant state space shared by various tasks. This algorithm creates subtasks that involve transitioning between state partitions resulting from the compression. Our findings show that this approach can efficiently solve challenging sparse-reward domains and speed up learning for new, unencountered tasks.",1
"Previous studies on image classification have mainly focused on the performance of the networks, not on real-time operation or model compression. We propose a Gaussian Deep Recurrent visual Attention Model (GDRAM)- a reinforcement learning based lightweight deep neural network for large scale image classification that outperforms the conventional CNN (Convolutional Neural Network) which uses the entire image as input. Highly inspired by the biological visual recognition process, our model mimics the stochastic location of the retina with Gaussian distribution. We evaluate the model on Large cluttered MNIST, Large CIFAR-10 and Large CIFAR-100 datasets which are resized to 128 in both width and height.",0
"The focus of previous research on image classification has been centered on network performance rather than real-time usage or model compression. Our proposal is a lightweight deep neural network called the Gaussian Deep Recurrent visual Attention Model (GDRAM) that employs reinforcement learning and surpasses the conventional CNN (Convolutional Neural Network) which utilizes the entire image as input. Our model imitates the stochastic location of the retina through a Gaussian distribution, drawing inspiration from biological visual recognition processes. To assess the model's effectiveness, we tested it on Large cluttered MNIST, Large CIFAR-10, and Large CIFAR-100 datasets resized to 128 in both width and height.",1
"Traffic simulators are important tools in autonomous driving development. While continuous progress has been made to provide developers more options for modeling various traffic participants, tuning these models to increase their behavioral diversity while maintaining quality is often very challenging. This paper introduces an easily-tunable policy generation algorithm for autonomous driving agents. The proposed algorithm balances diversity and driving skills by leveraging the representation and exploration abilities of deep reinforcement learning via a distinct policy set selector. Moreover, we present an algorithm utilizing intrinsic rewards to widen behavioral differences in the training. To provide quantitative assessments, we develop two trajectory-based evaluation metrics which measure the differences among policies and behavioral coverage. We experimentally show the effectiveness of our methods on several challenging intersection scenes.",0
"The development of autonomous driving heavily relies on traffic simulators, which are crucial tools. Even though there have been advancements in creating a variety of traffic models, it remains challenging to adjust them to increase the behavioral diversity while maintaining quality. This research paper introduces an algorithm that allows easy tuning of policies for autonomous driving agents. The proposed algorithm balances diversity and driving skills by utilizing deep reinforcement learning, which enhances representation and exploration abilities through a unique policy set selector. Additionally, an intrinsic reward algorithm is used to expand behavioral differences during training. To quantify the effectiveness of the methods, two trajectory-based evaluation metrics were developed to measure differences among policies and behavioral coverage. The experiments conducted on various intersection scenes showed the effectiveness of the proposed methods.",1
"Proximal policy optimization (PPO) algorithm is a deep reinforcement learning algorithm with outstanding performance, especially in continuous control tasks. But the performance of this method is still affected by its exploration ability. For classical reinforcement learning, there are some schemes that make exploration more full and balanced with data exploitation, but they can't be applied in complex environments due to the complexity of algorithm. Based on continuous control tasks with dense reward, this paper analyzes the assumption of the original Gaussian action exploration mechanism in PPO algorithm, and clarifies the influence of exploration ability on performance. Afterward, aiming at the problem of exploration, an exploration enhancement mechanism based on uncertainty estimation is designed in this paper. Then, we apply exploration enhancement theory to PPO algorithm and propose the proximal policy optimization algorithm with intrinsic exploration module (IEM-PPO) which can be used in complex environments. In the experimental parts, we evaluate our method on multiple tasks of MuJoCo physical simulator, and compare IEM-PPO algorithm with curiosity driven exploration algorithm (ICM-PPO) and original algorithm (PPO). The experimental results demonstrate that IEM-PPO algorithm needs longer training time, but performs better in terms of sample efficiency and cumulative reward, and has stability and robustness.",0
"The PPO algorithm, a deep reinforcement learning algorithm, has exceptional performance in continuous control tasks, but its exploration ability still affects its performance. Although classical reinforcement learning has exploration schemes to balance data exploitation, they cannot be applied in complex environments due to the algorithm's complexity. This paper examines the Gaussian action exploration mechanism in PPO algorithm and its impact on exploration ability and performance in continuous control tasks with dense reward. To address the exploration problem, the paper introduces an exploration enhancement mechanism based on uncertainty estimation. The proposed intrinsic exploration module (IEM-PPO) algorithm integrates the exploration enhancement theory into PPO and can be used in more complex environments. The paper evaluates the IEM-PPO algorithm on multiple tasks of MuJoCo physical simulator and compares it with the curiosity-driven exploration algorithm (ICM-PPO) and the original PPO algorithm. The experimental results show that the IEM-PPO algorithm requires longer training time but performs better in terms of sample efficiency and cumulative reward and exhibits stability and robustness.",1
"Deep reinforcement learning (DRL) algorithms have successfully been demonstrated on a range of challenging decision making and control tasks. One dominant component of recent deep reinforcement learning algorithms is the target network which mitigates the divergence when learning the Q function. However, target networks can slow down the learning process due to delayed function updates. Our main contribution in this work is a self-regularized TD-learning method to address divergence without requiring a target network. Additionally, we propose a self-guided policy improvement method by combining policy-gradient with zero-order optimization to search for actions associated with higher Q-values in a broad neighborhood. This makes learning more robust to local noise in the Q function approximation and guides the updates of our actor network. Taken together, these components define GRAC, a novel self-guided and self-regularized actor critic algorithm. We evaluate GRAC on the suite of OpenAI gym tasks, achieving or outperforming state of the art in every environment tested.",0
"Challenging decision making and control tasks have been successfully tackled by deep reinforcement learning (DRL) algorithms. Among recent DRL algorithms, the target network has been a key feature that helps prevent divergence when learning the Q function. However, updating the target network can slow down the learning process. This research proposes a self-regularized TD-learning method to avoid divergence without relying on a target network. Also, a self-guided policy improvement method is suggested by incorporating policy-gradient with zero-order optimization to explore actions associated with higher Q-values. This approach increases the learning robustness to local noise in the Q function approximation and guides updates to the actor network. These two approaches define the GRAC algorithm, a novel self-guided and self-regularized actor critic algorithm. The algorithm is tested on the OpenAI gym tasks, and it achieves or outperforms the state of the art in all environments tested.",1
"Recent research on structured exploration placed emphasis on identifying novel states in the state space and incentivizing the agent to revisit them through intrinsic reward bonuses. In this study, we question whether the performance boost demonstrated through these methods is indeed due to the discovery of structure in exploratory schedule of the agent or is the benefit largely attributed to the perturbations in the policy and reward space manifested in pursuit of structured exploration. In this study we investigate the effect of perturbations in policy and reward spaces on the exploratory behavior of the agent. We proceed to show that simple acts of perturbing the policy just before the softmax layer and introduction of sporadic reward bonuses into the domain can greatly enhance exploration in several domains of the arcade learning environment. In light of these findings, we recommend benchmarking any enhancements to structured exploration research against the backdrop of noisy exploration.",0
"The recent study of structured exploration has focused on finding new states in the state space and motivating the agent to revisit them using intrinsic reward bonuses. Our study aims to determine whether the performance improvement seen in these methods is the result of discovering structure in the agent's exploratory schedule or if it is mainly due to policy and reward space perturbations that occur during structured exploration. We investigate the impact of policy and reward space perturbations on the agent's exploratory behavior and demonstrate that small changes made to the policy before the softmax layer and the introduction of occasional reward bonuses significantly enhance exploration in various arcade learning environment domains. Based on these results, we recommend that any improvements made to structured exploration research should be benchmarked against the noise exploration method.",1
"Multi-agent reinforcement learning (MARL) has shown recent success in increasingly complex fixed-team zero-sum environments. However, the real world is not zero-sum nor does it have fixed teams; humans face numerous social dilemmas and must learn when to cooperate and when to compete. To successfully deploy agents into the human world, it may be important that they be able to understand and help in our conflicts. Unfortunately, selfish MARL agents typically fail when faced with social dilemmas. In this work, we show evidence of emergent direct reciprocity, indirect reciprocity and reputation, and team formation when training agents with randomized uncertain social preferences (RUSP), a novel environment augmentation that expands the distribution of environments agents play in. RUSP is generic and scalable; it can be applied to any multi-agent environment without changing the original underlying game dynamics or objectives. In particular, we show that with RUSP these behaviors can emerge and lead to higher social welfare equilibria in both classic abstract social dilemmas like Iterated Prisoner's Dilemma as well in more complex intertemporal environments.",0
"MARL has been successful in fixed-team zero-sum environments, but the real world is more complex and humans face social dilemmas where they must determine when to cooperate and compete. To successfully integrate agents into society, they must be able to understand and assist with conflicts. However, selfish MARL agents struggle in social dilemmas. This study introduces randomized uncertain social preferences (RUSP), which expands the range of environments agents play in without altering game dynamics or objectives. RUSP allows for the emergence of direct and indirect reciprocity, reputation, and team formation, resulting in higher social welfare equilibria in both abstract social dilemmas and more complex intertemporal environments.",1
"Reinforcement learning has the potential to automate the acquisition of behavior in complex settings, but in order for it to be successfully deployed, a number of practical challenges must be addressed. First, in real world settings, when an agent attempts a task and fails, the environment must somehow ""reset"" so that the agent can attempt the task again. While easy in simulation, this could require considerable human effort in the real world, especially if the number of trials is very large. Second, real world learning often involves complex, temporally extended behavior that is often difficult to acquire with random exploration. While these two problems may at first appear unrelated, in this work, we show how a single method can allow an agent to acquire skills with minimal supervision while removing the need for resets. We do this by exploiting the insight that the need to ""reset"" an agent to a broad set of initial states for a learning task provides a natural setting to learn a diverse set of ""reset-skills"". We propose a general-sum game formulation that balances the objectives of resetting and learning skills, and demonstrate that this approach improves performance on reset-free tasks, and additionally show that the skills we obtain can be used to significantly accelerate downstream learning.",0
"Automating the acquisition of behavior in complex settings through reinforcement learning has immense potential. However, several practical challenges need to be addressed for its successful deployment. Real-world settings require the environment to reset when an agent fails to perform a task. This can be a daunting task, particularly when the number of trials is high. Additionally, learning complex, temporally extended behavior through random exploration is challenging in the real world. In this work, we present a method that addresses both these problems. By leveraging the need to reset an agent to learn a diverse set of reset-skills, we propose a general-sum game formulation that balances the objectives of resetting and learning skills. This approach improves performance on reset-free tasks and accelerates downstream learning by obtaining reusable skills.",1
"Modern deep learning is primarily an experimental science, in which empirical advances occasionally come at the expense of probabilistic rigor. Here we focus on one such example; namely the use of the categorical cross-entropy loss to model data that is not strictly categorical, but rather takes values on the simplex. This practice is standard in neural network architectures with label smoothing and actor-mimic reinforcement learning, amongst others. Drawing on the recently discovered continuous-categorical distribution, we propose probabilistically-inspired alternatives to these models, providing an approach that is more principled and theoretically appealing. Through careful experimentation, including an ablation study, we identify the potential for outperformance in these models, thereby highlighting the importance of a proper probabilistic treatment, as well as illustrating some of the failure modes thereof.",0
"The field of modern deep learning relies heavily on experimentation and empirical advances, often sacrificing probabilistic rigor. This is exemplified by the common use of categorical cross-entropy loss to model data that is not strictly categorical, but rather takes values on the simplex. Although this practice is standard in neural network architectures with label smoothing and actor-mimic reinforcement learning, among others, it is not theoretically sound. To address this issue, we propose alternatives to these models based on the recently discovered continuous-categorical distribution, which offers a more principled and theoretically appealing approach. Through careful experimentation, including an ablation study, we demonstrate the potential for superior performance in these models, highlighting the importance of a proper probabilistic treatment and exposing some of the failure modes associated with the current approach.",1
"Summarizing video content is an important task in many applications. This task can be defined as the computation of the ordered list of actions present in a video. Such a list could be extracted using action detection algorithms. However, it is not necessary to determine the temporal boundaries of actions to know their existence. Moreover, localizing precise boundaries usually requires dense video analysis to be effective. In this work, we propose to directly compute this ordered list by sparsely browsing the video and selecting one frame per action instance, task known as action spotting in literature. To do this, we propose ActionSpotter, a spotting algorithm that takes advantage of Deep Reinforcement Learning to efficiently spot actions while adapting its video browsing speed, without additional supervision. Experiments performed on datasets THUMOS14 and ActivityNet show that our framework outperforms state of the art detection methods. In particular, the spotting mean Average Precision on THUMOS14 is significantly improved from 59.7% to 65.6% while skipping 23% of video.",0
"In numerous applications, summarizing video content is a crucial task that involves determining the ordered list of actions present in a video. This list can be obtained by utilizing action detection algorithms, although identifying the temporal boundaries of actions is not always necessary to establish their existence. Moreover, accurately localizing boundaries generally requires extensive video analysis to be effective. In this research, we propose a method called ActionSpotter, which involves sparsely scanning the video and selecting one frame per action instance, a process known as action spotting. To accomplish this, we employ Deep Reinforcement Learning to efficiently spot actions while adjusting the speed of video browsing without requiring additional supervision. Our experiments on THUMOS14 and ActivityNet datasets demonstrate that our approach outperforms existing detection techniques, significantly improving the spotting mean Average Precision on THUMOS14 from 59.7% to 65.6% while skipping 23% of video.",1
"Reinforcement learning was carried out in a simulated environment to learn continuous velocity control over multiple motor axes. This was then applied to a real-world optical tweezers experiment with the objective of moving a laser-trapped microsphere to a target location whilst avoiding collisions with other free-moving microspheres. The concept of training a neural network in a virtual environment has significant potential in the application of machine learning for experimental optimization and control, as the neural network can discover optimal methods for problem solving without the risk of damage to equipment, and at a speed not limited by movement in the physical environment. As the neural network treats both virtual and physical environments equivalently, we show that the network can also be applied to an augmented environment, where a virtual environment is combined with the physical environment. This technique may have the potential to unlock capabilities associated with mixed and augmented reality, such as enforcing safety limits for machine motion or as a method of inputting observations from additional sensors.",0
"In order to achieve continuous velocity control across multiple motor axes, reinforcement learning was implemented in a simulated environment. This approach was then tested in a real-world optical tweezers experiment, where the goal was to manipulate a laser-trapped microsphere to a specific location while avoiding collisions with other freely moving microspheres. The use of virtual environments for training neural networks has great potential for optimizing and controlling experiments through machine learning. Since virtual environments pose no risk to equipment and allow for faster experimentation, the neural network can discover optimal solutions without limitations. Additionally, the neural network can be applied to augmented environments, which combine virtual and physical environments. This technique may have the potential to enhance mixed and augmented reality capabilities, such as limiting machine motion for safety purposes and incorporating observations from additional sensors.",1
"In recent years, challenging control problems became solvable with deep reinforcement learning (RL). To be able to use RL for large-scale real-world applications, a certain degree of reliability in their performance is necessary. Reported results of state-of-the-art algorithms are often difficult to reproduce. One reason for this is that certain implementation details influence the performance significantly. Commonly, these details are not highlighted as important techniques to achieve state-of-the-art performance. Additionally, techniques from supervised learning are often used by default but influence the algorithms in a reinforcement learning setting in different and not well-understood ways. In this paper, we investigate the influence of certain initialization, input normalization, and adaptive learning techniques on the performance of state-of-the-art RL algorithms. We make suggestions which of those techniques to use by default and highlight areas that could benefit from a solution specifically tailored to RL.",0
"Deep reinforcement learning (RL) has become a solution for challenging control problems in recent years. However, reliable performance is crucial for large-scale real-world applications. State-of-the-art algorithms have reported results that are often challenging to reproduce due to certain implementation details significantly influencing their performance. These details are often not highlighted as important techniques to achieve state-of-the-art performance. Additionally, techniques from supervised learning have been used by default but have different and not well-understood effects on RL algorithms. Therefore, in this paper, we examine the impact of certain initialization, input normalization, and adaptive learning techniques on the performance of state-of-the-art RL algorithms. We recommend the default use of certain techniques and highlight areas that require specific solutions tailored to RL.",1
"Two timescale stochastic approximation (SA) has been widely used in value-based reinforcement learning algorithms. In the policy evaluation setting, it can model the linear and nonlinear temporal difference learning with gradient correction (TDC) algorithms as linear SA and nonlinear SA, respectively. In the policy optimization setting, two timescale nonlinear SA can also model the greedy gradient-Q (Greedy-GQ) algorithm. In previous studies, the non-asymptotic analysis of linear TDC and Greedy-GQ has been studied in the Markovian setting, with diminishing or accuracy-dependent stepsize. For the nonlinear TDC algorithm, only the asymptotic convergence has been established. In this paper, we study the non-asymptotic convergence rate of two timescale linear and nonlinear TDC and Greedy-GQ under Markovian sampling and with accuracy-independent constant stepsize. For linear TDC, we provide a novel non-asymptotic analysis and show that it attains an $\epsilon$-accurate solution with the optimal sample complexity of $\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$ under a constant stepsize. For nonlinear TDC and Greedy-GQ, we show that both algorithms attain $\epsilon$-accurate stationary solution with sample complexity $\mathcal{O}(\epsilon^{-2})$. It is the first non-asymptotic convergence result established for nonlinear TDC under Markovian sampling and our result for Greedy-GQ outperforms the previous result orderwisely by a factor of $\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$.",0
"The use of two timescale stochastic approximation (SA) has become popular in value-based reinforcement learning algorithms. In the context of policy evaluation, it can represent linear and nonlinear temporal difference learning with gradient correction (TDC) algorithms as linear SA and nonlinear SA, respectively. Furthermore, in the policy optimization setting, two timescale nonlinear SA can also represent the greedy gradient-Q (Greedy-GQ) algorithm. Previous studies have analyzed the non-asymptotic behavior of linear TDC and Greedy-GQ in the Markovian setting, with diminishing or accuracy-dependent stepsize. However, for the nonlinear TDC algorithm, only asymptotic convergence has been established. This paper analyzes the non-asymptotic convergence rate of two timescale linear and nonlinear TDC and Greedy-GQ under Markovian sampling and with accuracy-independent constant stepsize. Specifically, for linear TDC, a novel non-asymptotic analysis is provided, showing that it achieves an $\epsilon$-accurate solution with an optimal sample complexity of $\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$ using a constant stepsize. For nonlinear TDC and Greedy-GQ, it is shown that both algorithms achieve an $\epsilon$-accurate stationary solution with a sample complexity of $\mathcal{O}(\epsilon^{-2})$. Notably, this is the first non-asymptotic convergence result established for nonlinear TDC under Markovian sampling, and the result for Greedy-GQ outperforms the previous result by a factor of $\mathcal{O}(\epsilon^{-1}\log(1/\epsilon))$.",1
"Dynamic dispatching aims to smartly allocate the right resources to the right place at the right time. Dynamic dispatching is one of the core problems for operations optimization in the mining industry. Theoretically, deep reinforcement learning (RL) should be a natural fit to solve this problem. However, the industry relies on heuristics or even human intuitions, which are often short-sighted and sub-optimal solutions. In this paper, we review the main challenges in using deep RL to address the dynamic dispatching problem in the mining industry.",0
"The objective of dynamic dispatching is to efficiently allocate resources to their intended destinations at the appropriate time. This is a fundamental challenge for optimizing operations within the mining sector. Although deep reinforcement learning (RL) is ideally suited for addressing this issue, the industry often relies on heuristic approaches or human intuition, which may result in suboptimal and short-sighted solutions. This article examines the primary difficulties associated with utilizing deep RL to tackle the dynamic dispatching problem in the mining industry.",1
"Adversary emulation is an offensive exercise that provides a comprehensive assessment of a system's resilience against cyber attacks. However, adversary emulation is typically a manual process, making it costly and hard to deploy in cyber-physical systems (CPS) with complex dynamics, vulnerabilities, and operational uncertainties. In this paper, we develop an automated, domain-aware approach to adversary emulation for CPS. We formulate a Markov Decision Process (MDP) model to determine an optimal attack sequence over a hybrid attack graph with cyber (discrete) and physical (continuous) components and related physical dynamics. We apply model-based and model-free reinforcement learning (RL) methods to solve the discrete-continuous MDP in a tractable fashion. As a baseline, we also develop a greedy attack algorithm and compare it with the RL procedures. We summarize our findings through a numerical study on sensor deception attacks in buildings to compare the performance and solution quality of the proposed algorithms.",0
"Adversary emulation evaluates a system's ability to withstand cyber attacks through offensive exercises. However, the manual nature of the process makes it expensive and difficult to execute in complex cyber-physical systems (CPS) with uncertainties and vulnerabilities. This paper presents an automated approach to adversary emulation for CPS by formulating a Markov Decision Process (MDP) model to identify the optimal attack sequence across a hybrid attack graph, which includes both discrete cyber and continuous physical components. We utilize both model-based and model-free reinforcement learning (RL) methods to solve the MDP. Additionally, we develop a greedy attack algorithm as a baseline for comparison with the RL procedures. A numerical study on sensor deception attacks in buildings is conducted to compare the performance and solution quality of the proposed algorithms.",1
"In multi-agent reinforcement learning, discovering successful collective behaviors is challenging as it requires exploring a joint action space that grows exponentially with the number of agents. While the tractability of independent agent-wise exploration is appealing, this approach fails on tasks that require elaborate group strategies. We argue that coordinating the agents' policies can guide their exploration and we investigate techniques to promote such an inductive bias. We propose two policy regularization methods: TeamReg, which is based on inter-agent action predictability and CoachReg that relies on synchronized behavior selection. We evaluate each approach on four challenging continuous control tasks with sparse rewards that require varying levels of coordination as well as on the discrete action Google Research Football environment. Our experiments show improved performance across many cooperative multi-agent problems. Finally, we analyze the effects of our proposed methods on the policies that our agents learn and show that our methods successfully enforce the qualities that we propose as proxies for coordinated behaviors.",0
"The exploration of a joint action space in multi-agent reinforcement learning presents a difficult challenge when attempting to discover successful collective behaviors. Although independent exploration of agents is appealing, it proves to be ineffective in tasks that demand intricate group strategies. Our argument is that coordinating policies among agents can guide exploration and we investigate techniques to encourage such a bias. We introduce two policy regularization methods, TeamReg and CoachReg, which respectively rely on inter-agent action predictability and synchronized behavior selection. Our evaluation of these approaches on various challenging continuous control tasks with sparse rewards and the Google Research Football environment demonstrate improved performance in cooperative multi-agent problems. Additionally, we analyze the effects of our proposed methods on agent policies and show that they successfully enforce qualities that we propose as proxies for coordinated behaviors.",1
"A fundamental challenge in reinforcement learning is to learn policies that generalize beyond the operating domains experienced during training. In this paper, we approach this challenge through the following invariance principle: an agent must find a representation such that there exists an action-predictor built on top of this representation that is simultaneously optimal across all training domains. Intuitively, the resulting invariant policy enhances generalization by finding causes of successful actions. We propose a novel learning algorithm, Invariant Policy Optimization (IPO), that implements this principle and learns an invariant policy during training. We compare our approach with standard policy gradient methods and demonstrate significant improvements in generalization performance on unseen domains for linear quadratic regulator and grid-world problems, and an example where a robot must learn to open doors with varying physical properties.",0
"Reinforcement learning faces a fundamental obstacle of acquiring policies that can be applied beyond the training domains. In this article, we tackle this challenge with an invariance principle that requires the agent to identify a representation that permits the construction of an action-predictor, which is optimal for all the training domains. The resulting invariant policy improves generalization by identifying the causes of effective actions. We present a new learning algorithm, Invariant Policy Optimization (IPO), that implements this principle and learns an invariant policy during training. We compare our approach with standard policy gradient methods and demonstrate significant improvements in the ability to generalize to unseen domains for linear quadratic regulator and grid-world problems, as well as an example where a robot must learn to open doors with varying physical properties.",1
"Deep reinforcement learning (DRL) has achieved significant success in various robot tasks: manipulation, navigation, etc. However, complex visual observations in natural environments remains a major challenge. This paper presents Contrastive Variational Reinforcement Learning (CVRL), a model-based method that tackles complex visual observations in DRL. CVRL learns a contrastive variational model by maximizing the mutual information between latent states and observations discriminatively, through contrastive learning. It avoids modeling the complex observation space unnecessarily, as the commonly used generative observation model often does, and is significantly more robust. CVRL achieves comparable performance with state-of-the-art model-based DRL methods on standard Mujoco tasks. It significantly outperforms them on Natural Mujoco tasks and a robot box-pushing task with complex observations, e.g., dynamic shadows. The CVRL code is available publicly at https://github.com/Yusufma03/CVRL.",0
"Various robot tasks such as manipulation and navigation have seen significant success in Deep Reinforcement Learning (DRL). However, the challenge of handling complex visual observations in natural environments remains. To address this issue, Contrastive Variational Reinforcement Learning (CVRL) is introduced as a model-based method. CVRL learns a contrastive variational model by maximizing mutual information between latent states and observations discriminatively through contrastive learning. Unlike generative observation models, CVRL avoids unnecessary modeling of the complex observation space, making it more robust. In standard Mujoco tasks, CVRL performs comparably to state-of-the-art model-based DRL methods. However, it outperforms them in Natural Mujoco tasks and a robot box-pushing task with complex observations, such as dynamic shadows. The CVRL code is publicly available at https://github.com/Yusufma03/CVRL.",1
"We study multi-agent reinforcement learning (MARL) in a time-varying network of agents. The objective is to find localized policies that maximize the (discounted) global reward. In general, scalability is a challenge in this setting because the size of the global state/action space can be exponential in the number of agents. Scalable algorithms are only known in cases where dependencies are static, fixed and local, e.g., between neighbors in a fixed, time-invariant underlying graph. In this work, we propose a Scalable Actor Critic framework that applies in settings where the dependencies can be non-local and time-varying, and provide a finite-time error bound that shows how the convergence rate depends on the speed of information spread in the network. Additionally, as a byproduct of our analysis, we obtain novel finite-time convergence results for a general stochastic approximation scheme and for temporal difference learning with state aggregation, which apply beyond the setting of RL in networked systems.",0
"Our focus is on studying multi-agent reinforcement learning (MARL) in a time-varying network of agents with the goal of finding localized policies that can maximize the discounted global reward. However, scalability is a significant challenge in this context due to the exponential size of the global state/action space when compared to the number of agents. Although scalable algorithms exist for static, fixed and local dependencies like those between neighbors in a fixed, time-invariant underlying graph, non-local and time-varying dependencies pose a challenge. In this paper, we present a Scalable Actor Critic framework that addresses these challenges and provides a finite-time error bound that shows how the convergence rate depends on the speed of information spread in the network. Moreover, our analysis yields novel finite-time convergence results for a general stochastic approximation scheme and temporal difference learning with state aggregation that extend beyond the context of RL in networked systems.",1
"This paper provides a statistical analysis of high-dimensional batch Reinforcement Learning (RL) using sparse linear function approximation. When there is a large number of candidate features, our result sheds light on the fact that sparsity-aware methods can make batch RL more sample efficient. We first consider the off-policy policy evaluation problem. To evaluate a new target policy, we analyze a Lasso fitted Q-evaluation method and establish a finite-sample error bound that has no polynomial dependence on the ambient dimension. To reduce the Lasso bias, we further propose a post model-selection estimator that applies fitted Q-evaluation to the features selected via group Lasso. Under an additional signal strength assumption, we derive a sharper instance-dependent error bound that depends on a divergence function measuring the distribution mismatch between the data distribution and occupancy measure of the target policy. Further, we study the Lasso fitted Q-iteration for batch policy optimization and establish a finite-sample error bound depending on the ratio between the number of relevant features and restricted minimal eigenvalue of the data's covariance. In the end, we complement the results with minimax lower bounds for batch-data policy evaluation/optimization that nearly match our upper bounds. The results suggest that having well-conditioned data is crucial for sparse batch policy learning.",0
"This article presents a statistical analysis of high-dimensional batch Reinforcement Learning (RL) that uses sparse linear function approximation. It demonstrates that sparsity-aware methods can improve the sample efficiency of batch RL when dealing with a large number of candidate features. The study focuses on the off-policy policy evaluation problem and proposes a Lasso fitted Q-evaluation method that establishes a finite-sample error bound without polynomial dependence on the ambient dimension. To reduce the Lasso bias, the authors suggest a post model-selection estimator that applies fitted Q-evaluation to the features selected via group Lasso. Additionally, the article derives a sharper instance-dependent error bound based on a divergence function measuring the distribution mismatch between the data distribution and occupancy measure of the target policy. The study also examines the Lasso fitted Q-iteration for batch policy optimization and establishes a finite-sample error bound that depends on the ratio between the number of relevant features and restricted minimal eigenvalue of the data's covariance. Finally, the article provides minimax lower bounds for batch-data policy evaluation/optimization to complement the results. The findings underscore the importance of having well-conditioned data for sparse batch policy learning.",1
"The balance of exploration and exploitation plays a crucial role in accelerating reinforcement learning (RL). To deploy an RL agent in human society, its explainability is also essential. However, basic RL approaches have difficulties in deciding when to choose exploitation as well as in extracting useful points for a brief explanation of its operation. One reason for the difficulties is that these approaches treat all states the same way. Here, we show that identifying critical states and treating them specially is commonly beneficial to both problems. These critical states are the states at which the action selection changes the potential of success and failure substantially. We propose to identify the critical states using the variance in the Q-function for the actions and to perform exploitation with high probability on the identified states. These simple methods accelerate RL in a grid world with cliffs and two baseline tasks of deep RL. Our results also demonstrate that the identified critical states are intuitively interpretable regarding the crucial nature of the action selection. Furthermore, our analysis of the relationship between the timing of the identification of especially critical states and the rapid progress of learning suggests there are a few especially critical states that have important information for accelerating RL rapidly.",0
"The optimal balance between exploration and exploitation is vital in expediting reinforcement learning (RL), while also ensuring that the RL agent can be explained when deployed in human society. However, traditional RL approaches face challenges in determining when to choose exploitation and extracting relevant information for a concise explanation of its operation. This is partly due to the fact that all states are treated equally. To address these issues, we propose identifying critical states that significantly affect the potential for success or failure during action selection. By using the variance in the Q-function for actions, we can identify critical states and prioritize exploitation on them. Our methods have proven successful in accelerating RL in a grid world with cliffs and two deep RL baseline tasks. Furthermore, the identified critical states are easily interpretable and provide valuable insights into the action selection process. Our analysis also suggests that the identification of especially critical states at the right time can significantly enhance RL progress.",1
"One of the main challenges in imitation learning is determining what action an agent should take when outside the state distribution of the demonstrations. Inverse reinforcement learning (IRL) can enable generalization to new states by learning a parameterized reward function, but these approaches still face uncertainty over the true reward function and corresponding optimal policy. Existing safe imitation learning approaches based on IRL deal with this uncertainty using a maxmin framework that optimizes a policy under the assumption of an adversarial reward function, whereas risk-neutral IRL approaches either optimize a policy for the mean or MAP reward function. While completely ignoring risk can lead to overly aggressive and unsafe policies, optimizing in a fully adversarial sense is also problematic as it can lead to overly conservative policies that perform poorly in practice. To provide a bridge between these two extremes, we propose Bayesian Robust Optimization for Imitation Learning (BROIL). BROIL leverages Bayesian reward function inference and a user specific risk tolerance to efficiently optimize a robust policy that balances expected return and conditional value at risk. Our empirical results show that BROIL provides a natural way to interpolate between return-maximizing and risk-minimizing behaviors and outperforms existing risk-sensitive and risk-neutral inverse reinforcement learning algorithms. Code is available at https://github.com/dsbrown1331/broil.",0
"Imitation learning presents a major obstacle in determining the appropriate course of action for agents when outside the scope of the demonstrations. Inverse reinforcement learning (IRL) can aid in generalizing to new states by acquiring a parameterized reward function, but the uncertainty surrounding the true reward function and optimal policy remains. IRL-driven safe imitation learning approaches rely on a maxmin structure, optimizing a policy under the assumption of an adversarial reward function, while risk-neutral IRL approaches optimize a policy for the mean or MAP reward function. While neglecting risk can result in overly aggressive and unsafe policies, a fully adversarial approach can produce overly conservative policies that are ineffective in practice. To bridge the gap between these extremes, we introduce Bayesian Robust Optimization for Imitation Learning (BROIL). BROIL combines Bayesian reward function inference with user-specific risk tolerance to efficiently optimize a robust policy that balances expected return and conditional value at risk. Our empirical analysis shows that BROIL offers a natural way to interpolate between return-maximizing and risk-minimizing behaviors and outperforms current risk-neutral and risk-sensitive inverse reinforcement learning algorithms. Code can be found at https://github.com/dsbrown1331/broil.",1
"We consider the problem of finitely parameterized multi-armed bandits where the model of the underlying stochastic environment can be characterized based on a common unknown parameter. The true parameter is unknown to the learning agent. However, the set of possible parameters, which is finite, is known a priori. We propose an algorithm that is simple and easy to implement, which we call Finitely Parameterized Upper Confidence Bound (FP-UCB) algorithm, which uses the information about the underlying parameter set for faster learning. In particular, we show that the FP-UCB algorithm achieves a bounded regret under some structural condition on the underlying parameter set. We also show that, if the underlying parameter set does not satisfy the necessary structural condition, the FP-UCB algorithm achieves a logarithmic regret, but with a smaller preceding constant compared to the standard UCB algorithm. We also validate the superior performance of the FP-UCB algorithm through extensive numerical simulations.",0
"The problem we examine is the use of finitely parameterized multi-armed bandits, in which the stochastic environment's model relies on a single unknown parameter. Although the learning agent is unaware of the exact value of the parameter, they know the finite set of possible parameters beforehand. Our proposed solution, the Finitely Parameterized Upper Confidence Bound (FP-UCB) algorithm, is straightforward to implement and utilizes the information regarding the parameter set to hasten learning. We demonstrate that the FP-UCB algorithm produces a limited regret when a specific structural condition is met by the underlying parameter set. However, if the parameter set does not fulfill the necessary structural condition, the FP-UCB algorithm still delivers a logarithmic regret but with a smaller constant than the standard UCB algorithm. We validate the FP-UCB algorithm's superior performance through extensive numerical simulations.",1
"The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.",0
"In recent years, the concept of meta-learning, also known as learning-to-learn, has gained significant attention. Unlike traditional artificial intelligence approaches that use a rigid learning algorithm to solve tasks from scratch, meta-learning seeks to enhance the learning algorithm itself by leveraging the knowledge gained from multiple learning experiences. This approach presents an opportunity to overcome the challenges of deep learning, such as data and computation limitations, and improve generalization. This survey outlines the current state of the field of meta-learning, including its definitions and how it relates to similar fields like transfer learning and hyperparameter optimization. Additionally, a new taxonomy is proposed to provide a more extensive categorization of meta-learning methods. The survey also covers promising applications of meta-learning, including few-shot learning and reinforcement learning, as well as the challenges and future research directions in the field.",1
"This article proposes a Universal Activation Function (UAF) that achieves near optimal performance in quantification, classification, and reinforcement learning (RL) problems. For any given problem, the optimization algorithms are able to evolve the UAF to a suitable activation function by tuning the UAF's parameters. For the CIFAR-10 classification and VGG-8, the UAF converges to the Mish like activation function, which has near optimal performance $F_{1} = 0.9017\pm0.0040$ when compared to other activation functions. For the quantification of simulated 9-gas mixtures in 30 dB signal-to-noise ratio (SNR) environments, the UAF converges to the identity function, which has near optimal root mean square error of $0.4888 \pm 0.0032$ $\mu M$. In the BipedalWalker-v2 RL dataset, the UAF achieves the 250 reward in $961 \pm 193$ epochs, which proves that the UAF converges in the lowest number of epochs. Furthermore, the UAF converges to a new activation function in the BipedalWalker-v2 RL dataset.",0
"A Universal Activation Function (UAF) is proposed in this article, which can achieve almost optimal performance in quantification, classification, and reinforcement learning (RL) problems. The UAF can be tuned to a suitable activation function by adjusting its parameters for any given problem using optimization algorithms. For instance, in the case of CIFAR-10 classification and VGG-8, the UAF converges to the Mish-like activation function, which exhibits near-optimal performance with an F1 score of 0.9017±0.0040 when compared to other activation functions. On the other hand, for the quantification of simulated 9-gas mixtures in 30 dB signal-to-noise ratio (SNR) environments, the UAF converges to the identity function, which has a root mean square error of 0.4888±0.0032 μM, close to the optimal value. In addition, the UAF achieves a reward of 250 in 961±193 epochs in the BipedalWalker-v2 RL dataset, indicating that it converges in the lowest number of epochs. Furthermore, in the BipedalWalker-v2 RL dataset, the UAF converges to a new activation function.",1
"Deep Reinforcement Learning (DRL) has recently witnessed significant advances that have led to multiple successes in solving sequential decision-making problems in various domains, particularly in wireless communications. The future sixth-generation (6G) networks are expected to provide scalable, low-latency, ultra-reliable services empowered by the application of data-driven Artificial Intelligence (AI). The key enabling technologies of future 6G networks, such as intelligent meta-surfaces, aerial networks, and AI at the edge, involve more than one agent which motivates the importance of multi-agent learning techniques. Furthermore, cooperation is central to establishing self-organizing, self-sustaining, and decentralized networks. In this context, this tutorial focuses on the role of DRL with an emphasis on deep Multi-Agent Reinforcement Learning (MARL) for AI-enabled 6G networks. The first part of this paper will present a clear overview of the mathematical frameworks for single-agent RL and MARL. The main idea of this work is to motivate the application of RL beyond the model-free perspective which was extensively adopted in recent years. Thus, we provide a selective description of RL algorithms such as Model-Based RL (MBRL) and cooperative MARL and we highlight their potential applications in 6G wireless networks. Finally, we overview the state-of-the-art of MARL in fields such as Mobile Edge Computing (MEC), Unmanned Aerial Vehicles (UAV) networks, and cell-free massive MIMO, and identify promising future research directions. We expect this tutorial to stimulate more research endeavors to build scalable and decentralized systems based on MARL.",0
"Recent progress in Deep Reinforcement Learning (DRL) has resulted in successful solutions to sequential decision-making problems, especially in wireless communications. The upcoming sixth-generation (6G) networks are anticipated to deliver AI-driven, low-latency, and reliable services. The future 6G technologies require multi-agent learning approaches to cater to the needs of intelligent meta-surfaces, aerial networks, and AI at the edge. Cooperation is fundamental in establishing self-sustaining and decentralized networks. This tutorial highlights the significance of DRL, specifically Multi-Agent Reinforcement Learning (MARL), in AI-enabled 6G networks. The tutorial presents an overview of mathematical frameworks for single-agent RL and MARL, with a focus on applying RL beyond the model-free perspective. The paper discusses RL algorithms such as Model-Based RL (MBRL) and cooperative MARL and their potential applications in 6G wireless networks. The tutorial also surveys the current state-of-the-art of MARL in MEC, UAV networks, and cell-free massive MIMO, and identifies promising future research directions. Ultimately, this tutorial aims to inspire further research and development of scalable and decentralized systems based on MARL.",1
"Learning models of the environment from data is often viewed as an essential component to building intelligent reinforcement learning (RL) agents. The common practice is to separate the learning of the model from its use, by constructing a model of the environment's dynamics that correctly predicts the observed state transitions. In this paper we argue that the limited representational resources of model-based RL agents are better used to build models that are directly useful for value-based planning. As our main contribution, we introduce the principle of value equivalence: two models are value equivalent with respect to a set of functions and policies if they yield the same Bellman updates. We propose a formulation of the model learning problem based on the value equivalence principle and analyze how the set of feasible solutions is impacted by the choice of policies and functions. Specifically, we show that, as we augment the set of policies and functions considered, the class of value equivalent models shrinks, until eventually collapsing to a single point corresponding to a model that perfectly describes the environment. In many problems, directly modelling state-to-state transitions may be both difficult and unnecessary. By leveraging the value-equivalence principle one may find simpler models without compromising performance, saving computation and memory. We illustrate the benefits of value-equivalent model learning with experiments comparing it against more traditional counterparts like maximum likelihood estimation. More generally, we argue that the principle of value equivalence underlies a number of recent empirical successes in RL, such as Value Iteration Networks, the Predictron, Value Prediction Networks, TreeQN, and MuZero, and provides a first theoretical underpinning of those results.",0
"The development of intelligent reinforcement learning agents often involves teaching them about the environment through data. Typically, a model of the environment's dynamics is created to correctly predict observed state transitions, but we propose that limited resources can be better used to create models that are directly useful for value-based planning. Our main contribution is introducing the principle of value equivalence, which states that two models are equivalent if they yield the same Bellman updates for a set of functions and policies. By formulating the model learning problem based on this principle, we can analyze how the set of feasible solutions is impacted by policy and function choices. As we increase the set of policies and functions considered, the class of value equivalent models becomes smaller until it reaches a single point that describes the environment accurately. By using the value-equivalence principle, we can find simpler models that do not compromise performance and save computation and memory. We demonstrate the benefits of this approach through experiments comparing it to maximum likelihood estimation. We also argue that the principle of value equivalence underlies recent empirical successes in RL, providing a theoretical foundation for results like Value Iteration Networks, the Predictron, Value Prediction Networks, TreeQN, and MuZero.",1
"Agent-based Models (ABMs) are valuable tools for policy analysis. ABMs help analysts explore the emergent consequences of policy interventions in multi-agent decision-making settings. But the validity of inferences drawn from ABM explorations depends on the quality of the ABM agents' behavioral models. Standard specifications of agent behavioral models rely either on heuristic decision-making rules or on regressions trained on past data. Both prior specification modes have limitations. This paper examines the value of reinforcement learning (RL) models as adaptive, high-performing, and behaviorally-valid models of agent decision-making in ABMs. We test the hypothesis that RL agents are effective as utility-maximizing agents in policy ABMs. We also address the problem of adapting RL algorithms to handle multi-agency in games by adapting and extending methods from recent literature. We evaluate the performance of such RL-based ABM agents via experiments on two policy-relevant ABMs: a minority game ABM, and an ABM of Influenza Transmission. We run some analytic experiments on our AI-equipped ABMs e.g. explorations of the effects of behavioral heterogeneity in a population and the emergence of synchronization in a population. The experiments show that RL behavioral models are effective at producing reward-seeking or reward-maximizing behaviors in ABM agents. Furthermore, RL behavioral models can learn to outperform the default adaptive behavioral models in the two ABMs examined.",0
"Policy analysts find Agent-based Models (ABMs) to be valuable tools for exploring the consequences of policy interventions in multi-agent decision-making settings. However, the validity of inferences drawn from ABM explorations is dependent on the quality of the ABM agents' behavioral models. The standard specifications of agent behavioral models rely on either heuristic decision-making rules or past data-based regressions, which are limited in their effectiveness. This paper evaluates the use of reinforcement learning (RL) models as adaptive, high-performing, and behaviorally-valid models of agent decision-making in ABMs. The hypothesis that RL agents are effective as utility-maximizing agents in policy ABMs is tested while addressing the challenge of adapting RL algorithms to handle multi-agency in games. The performance of RL-based ABM agents is evaluated through experiments on two policy-relevant ABMs: a minority game ABM and an ABM of Influenza Transmission. Analytic experiments are conducted on AI-equipped ABMs to explore the effects of behavioral heterogeneity in a population and the emergence of synchronization in a population. The results demonstrate that RL behavioral models are effective in producing reward-seeking or reward-maximizing behaviors in ABM agents. Additionally, the RL models can learn to outperform the default adaptive behavioral models in the two ABMs examined.",1
"We introduce a sampling perspective to tackle the challenging task of training robust Reinforcement Learning (RL) agents. Leveraging the powerful Stochastic Gradient Langevin Dynamics, we present a novel, scalable two-player RL algorithm, which is a sampling variant of the two-player policy gradient method. Our algorithm consistently outperforms existing baselines, in terms of generalization across different training and testing conditions, on several MuJoCo environments. Our experiments also show that, even for objective functions that entirely ignore potential environmental shifts, our sampling approach remains highly robust in comparison to standard RL algorithms.",0
"To address the difficult task of training resilient Reinforcement Learning (RL) agents, we propose a sampling approach. Our technique employs the Stochastic Gradient Langevin Dynamics and introduces a novel, scalable two-player RL algorithm. This algorithm is a sampling version of the two-player policy gradient method. Our method surpasses current benchmarks in terms of its ability to generalize across various training and testing scenarios on several MuJoCo environments. Additionally, our experiments demonstrate that our sampling approach is highly robust compared to conventional RL algorithms, even when objective functions do not account for potential environmental changes.",1
"Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.",0
"Reinforcement Learning (RL) faces a challenging task of learning from visual observations, and while convolutional neural networks combined with algorithmic advancements have been successful, current methods lack in two areas: (a) efficient data learning and (b) adaptation to new environments. To address this, we introduce Reinforcement Learning with Augmented Data (RAD), which is a simple module that can enhance most RL algorithms. Our study of general data augmentations for RL on both pixel-based and state-based inputs includes two new augmentations: random translate and random amplitude scale. Our research demonstrates that simple RL algorithms can outperform state-of-the-art methods across various benchmarks by using augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale. RAD establishes new benchmarks for data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control and OpenAI Gym benchmark for state-based control. We also demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. The RAD module and training code are available at https://www.github.com/MishaLaskin/rad.",1
"Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneficial shaping rewards, and meanwhile ignore unbeneficial shaping rewards or even transform them into beneficial ones.",0
"Reward shaping is a technique that can be used to incorporate domain knowledge into reinforcement learning (RL) effectively. However, existing methods, such as potential-based reward shaping, tend to rely too heavily on a given shaping reward function, which can lead to suboptimal performance in RL algorithms. The reason for this is that the transformation of human knowledge into numeric reward values is often imperfect due to human cognitive bias. To address this issue, we propose an adaptive approach to utilizing shaping reward functions. Our approach formulates the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We derive the gradient of the expected true reward with respect to the shaping weight function parameters and propose three learning algorithms based on different assumptions. Our experiments in sparse-reward cartpole and MuJoCo environments demonstrate that our algorithms can effectively leverage beneficial shaping rewards while ignoring or even transforming unbeneficial shaping rewards into beneficial ones.",1
"Reinforcement Learning has yielded promising results for Neural Architecture Search (NAS). In this paper, we demonstrate how its performance can be improved by using a simplified Transformer block to model the policy network. The simplified Transformer uses a 2-stream attention-based mechanism to model hyper-parameter dependencies while avoiding layer normalization and position encoding. We posit that this parsimonious design balances model complexity against expressiveness, making it suitable for discovering optimal architectures in high-dimensional search spaces with limited exploration budgets. We demonstrate how the algorithm's performance can be further improved by a) using an actor-critic style algorithm instead of plain vanilla policy gradient and b) ensembling Transformer blocks with shared parameters, each block conditioned on a different auto-regressive factorization order. Our algorithm works well as both a NAS and generic hyper-parameter optimization (HPO) algorithm: it outperformed most algorithms on NAS-Bench-101, a public data-set for benchmarking NAS algorithms. In particular, it outperformed RL based methods that use alternate architectures to model the policy network, underlining the value of using attention-based networks in this setting. As a generic HPO algorithm, it outperformed Random Search in discovering more accurate multi-layer perceptron model architectures across 2 regression tasks. We have adhered to guidelines listed in Lindauer and Hutter while designing experiments and reporting results.",0
"The success of Neural Architecture Search (NAS) has been enhanced by Reinforcement Learning. This study presents a method to further improve its performance by implementing a simplified Transformer block to model the policy network. This design uses a 2-stream attention-based mechanism to model hyper-parameter dependencies, avoiding layer normalization and position encoding. The authors argue that this parsimonious design balances model complexity and expressiveness, making it suitable for discovering optimal architectures in high-dimensional search spaces with limited exploration budgets. The algorithm's performance is enhanced by using an actor-critic style algorithm and ensembling Transformer blocks with shared parameters, each block conditioned on a different auto-regressive factorization order. The algorithm performs well as both a NAS and generic hyper-parameter optimization (HPO) algorithm and outperformed most algorithms on NAS-Bench-101, a public dataset for benchmarking NAS algorithms. The use of attention-based networks in this setting was emphasized as it outperformed RL based methods that use alternate architectures to model the policy network. As a generic HPO algorithm, it outperformed Random Search in discovering more accurate multi-layer perceptron model architectures across 2 regression tasks. The authors adhere to guidelines listed in Lindauer and Hutter while designing experiments and reporting results.",1
"Quality-Diversity (QD) is a concept from Neuroevolution with some intriguing applications to Reinforcement Learning. It facilitates learning a population of agents where each member is optimized to simultaneously accumulate high task-returns and exhibit behavioral diversity compared to other members. In this paper, we build on a recent kernel-based method for training a QD policy ensemble with Stein variational gradient descent. With kernels based on $f$-divergence between the stationary distributions of policies, we convert the problem to that of efficient estimation of the ratio of these stationary distributions. We then study various distribution ratio estimators used previously for off-policy evaluation and imitation and re-purpose them to compute the gradients for policies in an ensemble such that the resultant population is diverse and of high-quality.",0
"The idea of Quality-Diversity (QD) has potential uses in Reinforcement Learning, as it allows for the training of a group of agents that are optimized to achieve high task-returns while also exhibiting diverse behavior compared to their peers. This paper expands on a recent kernel-based approach to training a QD policy ensemble using Stein variational gradient descent. By using kernels based on the $f$-divergence between policy stationary distributions, the problem is transformed into a more efficient estimation of the ratio of these distributions. This paper repurposes previously used distribution ratio estimators for off-policy evaluation and imitation to compute gradients for policies in an ensemble, resulting in a diverse and high-quality population.",1
"Predictive power allocation is conceived for energy-efficient video streaming over mobile networks using deep reinforcement learning. The goal is to minimize the accumulated energy consumption of each base station over a complete video streaming session under the constraint that avoids video playback interruptions. To handle the continuous state and action spaces, we resort to deep deterministic policy gradient (DDPG) algorithm for solving the formulated problem. In contrast to previous predictive power allocation policies that first predict future information with historical data and then optimize the power allocation based on the predicted information, the proposed policy operates in an on-line and end-to-end manner. By judiciously designing the action and state that only depend on slowly-varying average channel gains, we reduce the signaling overhead between the edge server and the base stations, and make it easier to learn a good policy. To further avoid playback interruption throughout the learning process and improve the convergence speed, we exploit the partially known model of the system dynamics by integrating the concepts of safety layer, post-decision state, and virtual experiences into the basic DDPG algorithm. Our simulation results show that the proposed policies converge to the optimal policy that is derived based on perfect large-scale channel prediction and outperform the first-predict-then-optimize policy in the presence of prediction errors. By harnessing the partially known model, the convergence speed can be dramatically improved.",0
"This paper proposes a method called predictive power allocation, which uses deep reinforcement learning to optimize energy-efficient video streaming over mobile networks. The objective is to minimize energy consumption of base stations while ensuring no interruptions occur during video playback. To address the issue of continuous state and action spaces, the deep deterministic policy gradient (DDPG) algorithm is utilized. Unlike prior predictive power allocation policies that rely on historical data to predict future information and then optimize power allocation, the proposed policy operates in a real-time manner. By designing the action and state to depend only on slowly-varying average channel gains, signaling overhead between the edge server and base stations is reduced, making it easier to learn a good policy. To improve convergence speed and prevent playback disruptions during learning, the authors integrate the concepts of safety layer, post-decision state, and virtual experiences into the DDPG algorithm. The simulation results demonstrate that the proposed policies outperform the first-predict-then-optimize policy in the presence of prediction errors, and by harnessing the partially known model, convergence speed can be significantly improved.",1
"Temporal abstraction allows reinforcement learning agents to represent knowledge and develop strategies over different temporal scales. The option-critic framework has been demonstrated to learn temporally extended actions, represented as options, end-to-end in a model-free setting. However, feasibility of option-critic remains limited due to two major challenges, multiple options adopting very similar behavior, or a shrinking set of task relevant options. These occurrences not only void the need for temporal abstraction, they also affect performance. In this paper, we tackle these problems by learning a diverse set of options. We introduce an information-theoretic intrinsic reward, which augments the task reward, as well as a novel termination objective, in order to encourage behavioral diversity in the option set. We show empirically that our proposed method is capable of learning options end-to-end on several discrete and continuous control tasks, outperforms option-critic by a wide margin. Furthermore, we show that our approach sustainably generates robust, reusable, reliable and interpretable options, in contrast to option-critic.",0
"Temporal abstraction allows reinforcement learning agents to develop strategies and represent knowledge across various temporal scales. The option-critic framework has shown success in learning temporally extended actions in a model-free setting; however, it faces limitations due to two primary challenges: multiple options exhibiting similar behavior and a decreasing number of relevant options. These issues not only undermine the need for temporal abstraction but also hinder performance. This article proposes a solution to these problems by teaching the agent a heterogeneous set of options. It accomplishes this by introducing a novel intrinsic reward based on information theory, which supplements the task reward, as well as a unique termination objective that promotes behavioral variety in the option set. Empirical evidence demonstrates that this method is capable of end-to-end option learning on both discrete and continuous control tasks, outperforming option-critic by a significant margin. Moreover, the approach produces robust, reusable, reliable, and interpretable options that persist over time, distinguishing it from option-critic.",1
"Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts `RL as inference' and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to approximation. We demonstrate that the popular `RL as inference' approximation can perform poorly in even very basic problems. However, we show that with a small modification the framework does yield algorithms that can provably perform well, and we show that the resulting algorithm is equivalent to the recently proposed K-learning, which we further connect with Thompson sampling.",0
"Reinforcement learning (RL) is a combination of a control problem and statistical estimation. The agent does not know the system dynamics but can learn them through experience. Recent research suggests that RL can be generalized as probabilistic inference, but our paper uncovers a flaw in this approach. RL requires the agent to consider the impact of its actions on future rewards and observations, resulting in an exploration-exploitation tradeoff. In complex settings, this inference is computationally intractable, so practical RL algorithms require approximation. However, the popular 'RL as inference' approximation can perform poorly, even in simple problems. With a slight modification, the framework can yield algorithms that perform well and are equivalent to K-learning and Thompson sampling.",1
"Bootstrapping is a core mechanism in Reinforcement Learning (RL). Most algorithms, based on temporal differences, replace the true value of a transiting state by their current estimate of this value. Yet, another estimate could be leveraged to bootstrap RL: the current policy. Our core contribution stands in a very simple idea: adding the scaled log-policy to the immediate reward. We show that slightly modifying Deep Q-Network (DQN) in that way provides an agent that is competitive with distributional methods on Atari games, without making use of distributional RL, n-step returns or prioritized replay. To demonstrate the versatility of this idea, we also use it together with an Implicit Quantile Network (IQN). The resulting agent outperforms Rainbow on Atari, installing a new State of the Art with very little modifications to the original algorithm. To add to this empirical study, we provide strong theoretical insights on what happens under the hood -- implicit Kullback-Leibler regularization and increase of the action-gap.",0
"Reinforcement Learning (RL) heavily relies on Bootstrapping as a fundamental mechanism. The majority of algorithms, which are based on temporal differences, substitute the actual value of a transitioning state with their current approximation of the value. However, there is another estimation that can be utilized to bootstrap RL, which is the current policy. Our primary contribution is a simple concept of including the scaled log-policy to the immediate reward. By slightly modifying Deep Q-Network (DQN) in this manner, we present an agent that competes with distributional methods on Atari games without resorting to distributional RL, n-step returns, or prioritized replay. We also demonstrate the flexibility of this idea by using it alongside an Implicit Quantile Network (IQN), resulting in an agent that surpasses Rainbow on Atari, setting a new State of the Art with minimal adjustments to the original algorithm. In addition, we provide compelling theoretical insights into the implicit Kullback-Leibler regularization and the augmentation of the action-gap in this empirical study.",1
"Distributed learning frameworks often rely on exchanging model parameters across workers, instead of revealing their raw data. A prime example is federated learning that exchanges the gradients or weights of each neural network model. Under limited communication resources, however, such a method becomes extremely costly particularly for modern deep neural networks having a huge number of model parameters. In this regard, federated distillation (FD) is a compelling distributed learning solution that only exchanges the model outputs whose dimensions are commonly much smaller than the model sizes (e.g., 10 labels in the MNIST dataset). The goal of this chapter is to provide a deep understanding of FD while demonstrating its communication efficiency and applicability to a variety of tasks. To this end, towards demystifying the operational principle of FD, the first part of this chapter provides a novel asymptotic analysis for two foundational algorithms of FD, namely knowledge distillation (KD) and co-distillation (CD), by exploiting the theory of neural tangent kernel (NTK). Next, the second part elaborates on a baseline implementation of FD for a classification task, and illustrates its performance in terms of accuracy and communication efficiency compared to FL. Lastly, to demonstrate the applicability of FD to various distributed learning tasks and environments, the third part presents two selected applications, namely FD over asymmetric uplink-and-downlink wireless channels and FD for reinforcement learning.",0
"Distributed learning frameworks often use the exchange of model parameters instead of raw data. Federated learning is an example of this, where the neural network model weights or gradients are exchanged. However, this method is costly for deep neural networks with many parameters when communication resources are limited. Federated distillation (FD) is a more efficient solution that only exchanges outputs with smaller dimensions, such as the labels in the MNIST dataset. This chapter aims to explain FD and its communication efficiency, with examples for various tasks. The first part uses neural tangent kernel theory to analyze foundational algorithms of FD, including knowledge distillation and co-distillation. The second part compares FD to federated learning in terms of accuracy and communication efficiency for a classification task. Lastly, the third part presents two applications of FD: using asymmetric wireless channels and reinforcement learning.",1
"Recently, Wang et al. (2020) showed a highly intriguing hardness result for batch reinforcement learning (RL) with linearly realizable value function and good feature coverage in the finite-horizon case. In this note we show that once adapted to the discounted setting, the construction can be simplified to a 2-state MDP with 1-dimensional features, such that learning is impossible even with an infinite amount of data.",0
"Wang and colleagues (2020) presented an engaging finding on the complexity of batch reinforcement learning (RL) with a value function that is linearly realizable and good feature coverage within a finite-horizon scenario. Our study demonstrates that when applied to the discounted setting, the process can be reduced to a 2-state MDP with 1-dimensional features, rendering learning impracticable even with an unlimited quantity of data.",1
"We study the efficient off-policy evaluation of natural stochastic policies, which are defined in terms of deviations from the behavior policy. This is a departure from the literature on off-policy evaluation where most work consider the evaluation of explicitly specified policies. Crucially, offline reinforcement learning with natural stochastic policies can help alleviate issues of weak overlap, lead to policies that build upon current practice, and improve policies' implementability in practice. Compared with the classic case of a pre-specified evaluation policy, when evaluating natural stochastic policies, the efficiency bound, which measures the best-achievable estimation error, is inflated since the evaluation policy itself is unknown. In this paper, we derive the efficiency bounds of two major types of natural stochastic policies: tilting policies and modified treatment policies. We then propose efficient nonparametric estimators that attain the efficiency bounds under very lax conditions. These also enjoy a (partial) double robustness property.",0
"Our focus is on the effective evaluation of natural stochastic policies, which differ from explicitly defined policies in that they are based on deviations from the behavior policy. This approach departs from previous off-policy evaluation literature, which mostly focuses on evaluating explicitly stated policies. The use of offline reinforcement learning with natural stochastic policies can help address issues related to weak overlap, promote policies that build upon current practices, and enhance the practical implementation of policies. However, evaluating natural stochastic policies can be challenging as the evaluation policy is unknown, leading to an inflation of the efficiency bound that measures the best-achievable estimation error. Therefore, we derive the efficiency bounds of two major types of natural stochastic policies: tilting policies and modified treatment policies. We also propose efficient nonparametric estimators that achieve the efficiency bounds under lenient conditions and partially double robustness.",1
"This paper evaluates adaptive Q-learning (AQL) and single-partition adaptive Q-learning (SPAQL), two algorithms for efficient model-free episodic reinforcement learning (RL), in two classical control problems (Pendulum and Cartpole). AQL adaptively partitions the state-action space of a Markov decision process (MDP), while learning the control policy, i. e., the mapping from states to actions. The main difference between AQL and SPAQL is that the latter learns time-invariant policies, where the mapping from states to actions does not depend explicitly on the time step. This paper also proposes the SPAQL with terminal state (SPAQL-TS), an improved version of SPAQL tailored for the design of regulators for control problems. The time-invariant policies are shown to result in a better performance than the time-variant ones in both problems studied. These algorithms are particularly fitted to RL problems where the action space is finite, as is the case with the Cartpole problem. SPAQL-TS solves the OpenAI Gym Cartpole problem, while also displaying a higher sample efficiency than trust region policy optimization (TRPO), a standard RL algorithm for solving control tasks. Moreover, the policies learned by SPAQL are interpretable, while TRPO policies are typically encoded as neural networks, and therefore hard to interpret. Yielding interpretable policies while being sample-efficient are the major advantages of SPAQL.",0
"In this study, two algorithms for model-free episodic reinforcement learning (RL) are evaluated in classical control problems (Pendulum and Cartpole): adaptive Q-learning (AQL) and single-partition adaptive Q-learning (SPAQL). AQL partitions the state-action space of a Markov decision process (MDP) while learning the control policy. SPAQL, on the other hand, learns time-invariant policies where the mapping from states to actions does not depend on the time step. Additionally, the paper proposes SPAQL with terminal state (SPAQL-TS), an improved version of SPAQL that is tailored for control problem regulation design. The results show that time-invariant policies perform better than time-variant ones in both problems studied. These algorithms are well-suited for RL problems where the action space is finite, such as the Cartpole problem. SPAQL-TS solves the OpenAI Gym Cartpole problem and is more sample-efficient than trust region policy optimization (TRPO), a standard RL algorithm for solving control tasks. Moreover, SPAQL policies are interpretable, while TRPO policies are typically encoded as neural networks, making them difficult to interpret. The main advantages of SPAQL are its interpretable policies and sample efficiency.",1
"A fundamental trait of intelligence is the ability to achieve goals in the face of novel circumstances, such as making decisions from new action choices. However, standard reinforcement learning assumes a fixed set of actions and requires expensive retraining when given a new action set. To make learning agents more adaptable, we introduce the problem of zero-shot generalization to new actions. We propose a two-stage framework where the agent first infers action representations from action information acquired separately from the task. A policy flexible to varying action sets is then trained with generalization objectives. We benchmark generalization on sequential tasks, such as selecting from an unseen tool-set to solve physical reasoning puzzles and stacking towers with novel 3D shapes. Videos and code are available at https://sites.google.com/view/action-generalization",0
"The ability to achieve goals when faced with unfamiliar situations, such as making decisions from new actions, is a key characteristic of intelligence. However, traditional reinforcement learning assumes a fixed set of actions, requiring costly retraining when presented with new actions. To enhance the adaptability of learning agents, we introduce the concept of zero-shot generalization to novel actions. Our proposed two-stage framework involves the agent first deducing action representations from action information obtained independently from the task. A policy that can handle varying action sets is then trained using generalization objectives. We evaluate generalization performance on sequential tasks, such as selecting from an unknown tool-set to solve physical reasoning puzzles and assembling towers with new 3D shapes. For further information, including videos and code, please visit https://sites.google.com/view/action-generalization.",1
"Policy gradient methods are extensively used in reinforcement learning as a way to optimize expected return. In this paper, we explore the evolution of the policy parameters, for a special class of exactly solvable POMDPs, as a continuous-state Markov chain, whose transition probabilities are determined by the gradient of the distribution of the policy's value. Our approach relies heavily on random walk theory, specifically on affine Weyl groups. We construct a class of novel partially observable environments with controllable exploration difficulty, in which the value distribution, and hence the policy parameter evolution, can be derived analytically. Using these environments, we analyze the probabilistic convergence of policy gradient to different local maxima of the value function. To our knowledge, this is the first approach developed to analytically compute the landscape of policy gradient in POMDPs for a class of such environments, leading to interesting insights into the difficulty of this problem.",0
"In reinforcement learning, policy gradient methods are commonly utilized to optimize expected return. The aim of this paper is to examine the continuous-state Markov chain evolution of policy parameters for a specific type of POMDPs that can be solved exactly. The transition probabilities of this Markov chain are determined by the policy's value distribution gradient. Our approach is based on random walk theory, specifically affine Weyl groups. We introduce a new group of partially observable environments that allows for controlled exploration difficulty and enables the derivation of the value distribution and policy parameter evolution through analytical means. By utilizing these environments, we investigate the probabilistic convergence of policy gradient to different local maxima of the value function. As far as we know, this is the first analytical approach to determining the landscape of policy gradient in POMDPs for this particular group of environments, which provides valuable insights into the challenges of this problem.",1
"Joining multiple decision-makers together is a powerful way to obtain more sophisticated decision-making systems, but requires to address the questions of division of labor and specialization. We investigate in how far information constraints in hierarchies of experts not only provide a principled method for regularization but also to enforce specialization. In particular, we devise an information-theoretically motivated on-line learning rule that allows partitioning of the problem space into multiple sub-problems that can be solved by the individual experts. We demonstrate two different ways to apply our method: (i) partitioning problems based on individual data samples and (ii) based on sets of data samples representing tasks. Approach (i) equips the system with the ability to solve complex decision-making problems by finding an optimal combination of local expert decision-makers. Approach (ii) leads to decision-makers specialized in solving families of tasks, which equips the system with the ability to solve meta-learning problems. We show the broad applicability of our approach on a range of problems including classification, regression, density estimation, and reinforcement learning problems, both in the standard machine learning setup and in a meta-learning setting.",0
"Combining decision-makers is a potent method to enhance decision-making systems, but it requires addressing the issues of division of labor and specialization. Our study delves into the extent to which information restrictions in hierarchies of experts not only offer a systematic approach for regularization but also for enforcing specialization. Specifically, we develop an information-theoretically motivated online learning rule that permits partitioning the problem space into multiple sub-problems that can be tackled by individual experts. We illustrate two ways of employing our method: (i) segmenting problems based on individual data samples and (ii) based on sets of data samples representing tasks. The former equips the system with the ability to solve complex decision-making problems by finding the best combination of local expert decision-makers, while the latter leads to decision-makers specialized in solving families of tasks, enabling the system to solve meta-learning problems. Our approach is broadly applicable to a range of problems, including classification, regression, density estimation, and reinforcement learning problems, both in the standard machine learning setup and in a meta-learning setting.",1
"Route planning is important in transportation. Existing works focus on finding the shortest path solution or using metrics such as safety and energy consumption to determine the planning. It is noted that most of these studies rely on prior knowledge of road network, which may be not available in certain situations. In this paper, we design a route planning algorithm based on deep reinforcement learning (DRL) for pedestrians. We use travel time consumption as the metric, and plan the route by predicting pedestrian flow in the road network. We put an agent, which is an intelligent robot, on a virtual map. Different from previous studies, our approach assumes that the agent does not need any prior information about road network, but simply relies on the interaction with the environment. We propose a dynamically adjustable route planning (DARP) algorithm, where the agent learns strategies through a dueling deep Q network to avoid congested roads. Simulation results show that the DARP algorithm saves 52% of the time under congestion condition when compared with traditional shortest path planning algorithms.",0
"Transportation requires careful route planning, with previous studies focusing on either finding the shortest path or using safety and energy consumption metrics. However, these studies often rely on prior knowledge of road networks that may not be available in certain situations. To address this issue, we developed a route planning algorithm for pedestrians based on deep reinforcement learning (DRL). Our algorithm uses travel time consumption as the metric and predicts pedestrian flow in the road network. Unlike previous studies, our approach assumes that the agent, an intelligent robot, has no prior information about the road network and instead relies solely on interaction with the environment. We propose a dynamically adjustable route planning (DARP) algorithm, where the agent learns strategies through a dueling deep Q network to avoid congested roads. Our simulation results showed that the DARP algorithm saves 52% of the time under congestion conditions compared to traditional shortest path planning algorithms.",1
"Efficient exploration is one of the most important issues in deep reinforcement learning. To address this issue, recent methods consider the value function parameters as random variables, and resort variational inference to approximate the posterior of the parameters. In this paper, we propose an amortized variational inference framework to approximate the posterior distribution of the action value function in Deep Q Network. We establish the equivalence between the loss of the new model and the amortized variational inference loss. We realize the balance of exploration and exploitation by assuming the posterior as Cauchy and Gaussian, respectively in a two-stage training process. We show that the amortized framework can results in significant less learning parameters than existing state-of-the-art method. Experimental results on classical control tasks in OpenAI Gym and chain Markov Decision Process tasks show that the proposed method performs significantly better than state-of-art methods and requires much less training time.",0
"Deep reinforcement learning relies heavily on efficient exploration, which is a critical concern. To tackle this problem, current approaches view value function parameters as random variables and employ variational inference to approximate the parameters' posterior. This paper introduces an amortized variational inference framework to estimate the posterior distribution of the action value function in Deep Q Network. By assuming the posterior as Cauchy and Gaussian, respectively, in a two-stage training process, we strike a balance between exploration and exploitation. We demonstrate that the loss of the new model is equivalent to the amortized variational inference loss. Compared to existing state-of-the-art methods, our approach yields significantly fewer learning parameters. Empirical results on OpenAI Gym's classical control tasks and chain Markov Decision Process tasks reveal that our proposed method outperforms state-of-the-art methods and requires much less training time.",1
"Proximal Policy Optimization (PPO) is a highly popular model-free reinforcement learning (RL) approach. However, we observe that in a continuous action space, PPO can prematurely shrink the exploration variance, which leads to slow progress and may make the algorithm prone to getting stuck in local optima. Drawing inspiration from CMA-ES, a black-box evolutionary optimization method designed for robustness in similar situations, we propose PPO-CMA, a proximal policy optimization approach that adaptively expands the exploration variance to speed up progress. With only minor changes to PPO, our algorithm considerably improves performance in Roboschool continuous control benchmarks. Our results also show that PPO-CMA, as opposed to PPO, is significantly less sensitive to the choice of hyperparameters, allowing one to use it in complex movement optimization tasks without requiring tedious tuning.",0
"The model-free reinforcement learning approach called Proximal Policy Optimization (PPO) is widely used, but we have noticed that it can cause issues in continuous action spaces by shrinking the exploration variance too early. This can result in slow progress and make the algorithm more likely to get stuck in local optima. To address this, we have developed PPO-CMA, which is inspired by the black-box evolutionary optimization method CMA-ES and can adaptively expand the exploration variance to speed up progress. Our modifications to PPO are minor, but they significantly improve performance in Roboschool continuous control benchmarks. We have also found that PPO-CMA is much less sensitive to hyperparameter choices than PPO, making it more suitable for complex movement optimization tasks without requiring extensive tuning.",1
"Recent progress in reinforcement learning has led to remarkable performance in a range of applications, but its deployment in high-stakes settings remains quite rare. One reason is a limited understanding of the behavior of reinforcement algorithms, both in terms of their regret and their ability to learn the underlying system dynamics---existing work is focused almost exclusively on characterizing rates, with little attention paid to the constants multiplying those rates that can be critically important in practice. To start to address this challenge, we study perhaps the simplest non-bandit reinforcement learning problem: linear quadratic adaptive control (LQAC). By carefully combining recent finite-sample performance bounds for the LQAC problem with a particular (less-recent) martingale central limit theorem, we are able to derive asymptotically-exact expressions for the regret, estimation error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm. In simulations on both stable and unstable systems, we find that our asymptotic theory also describes the algorithm's finite-sample behavior remarkably well.",0
"Although recent advancements in reinforcement learning have yielded impressive results across various applications, its implementation in high-stakes environments is still infrequent. This is partially due to a lack of comprehension regarding the actions of reinforcement algorithms, including their regret and ability to grasp underlying system dynamics. Currently, research is primarily focused on determining rates, with little emphasis placed on the constants that can play a crucial role in practical settings. To tackle this dilemma, we investigated the linear quadratic adaptive control (LQAC) problem, which is the simplest non-bandit reinforcement learning issue. By combining finite-sample performance bounds for the LQAC problem with a specific martingale central limit theorem, we were able to establish precise expressions for the regret, estimation error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm. In simulations conducted on both stable and unstable systems, we found that our asymptotic theory accurately described the algorithm's finite-sample behavior.",1
"Reinforcement learning is a powerful learning paradigm in which agents can learn to maximize sparse and delayed reward signals. Although RL has had many impressive successes in complex domains, learning can take hours, days, or even years of training data. A major challenge of contemporary RL research is to discover how to learn with less data. Previous work has shown that domain information can be successfully used to shape the reward; by adding additional reward information, the agent can learn with much less data. Furthermore, if the reward is constructed from a potential function, the optimal policy is guaranteed to be unaltered. While such potential-based reward shaping (PBRS) holds promise, it is limited by the need for a well-defined potential function. Ideally, we would like to be able to take arbitrary advice from a human or other agent and improve performance without affecting the optimal policy. The recently introduced dynamic potential based advice (DPBA) method tackles this challenge by admitting arbitrary advice from a human or other agent and improves performance without affecting the optimal policy. The main contribution of this paper is to expose, theoretically and empirically, a flaw in DPBA. Alternatively, to achieve the ideal goals, we present a simple method called policy invariant explicit shaping (PIES) and show theoretically and empirically that PIES succeeds where DPBA fails.",0
"Reinforcement learning is an effective way for agents to learn how to maximize delayed rewards, but it can be time-consuming. The current challenge in RL research is to find ways to learn with less data. Previous studies have shown that adding domain information to the reward can help agents learn quicker. However, this approach is limited as it requires a well-defined potential function. To address this limitation, the dynamic potential based advice (DPBA) method was introduced. However, this method has a flaw that this paper exposes. To overcome this flaw and achieve the ideal goals, a new method called policy invariant explicit shaping (PIES) is presented. The paper shows that PIES succeeds where DPBA fails, both theoretically and empirically.",1
"Agents trained via deep reinforcement learning (RL) routinely fail to generalize to unseen environments, even when these share the same underlying dynamics as the training levels. Understanding the generalization properties of RL is one of the challenges of modern machine learning. Towards this goal, we analyze policy learning in the context of Partially Observable Markov Decision Processes (POMDPs) and formalize the dynamics of training levels as instances. We prove that, independently of the exploration strategy, reusing instances introduces significant changes on the effective Markov dynamics the agent observes during training. Maximizing expected rewards impacts the learned belief state of the agent by inducing undesired instance specific speedrunning policies instead of generalizeable ones, which are suboptimal on the training set. We provide generalization bounds to the value gap in train and test environments based on the number of training instances, and use insights based on these to improve performance on unseen levels. We propose training a shared belief representation over an ensemble of specialized policies, from which we compute a consensus policy that is used for data collection, disallowing instance specific exploitation. We experimentally validate our theory, observations, and the proposed computational solution over the CoinRun benchmark.",0
"Deep reinforcement learning (RL) agents often struggle to adapt to new environments, even when they have the same underlying dynamics as the training levels. This is a challenge for modern machine learning, as understanding how RL can generalize is crucial. To explore this, we examine policy learning in Partially Observable Markov Decision Processes (POMDPs) and consider training levels as instances. We discover that reusing instances introduces significant changes to the effective Markov dynamics seen during training, and that maximizing rewards can lead to specific speedrunning policies that are not generalizable. We provide generalization bounds for the value gap between training and test environments based on the number of training instances and propose training a shared belief representation over an ensemble of specialized policies to improve performance on unseen levels. We validate our theory and proposed solution on the CoinRun benchmark.",1
"Reinforcement learning provides a framework for learning to control which actions to take towards completing a task through trial-and-error. In many applications observing interactions is costly, necessitating sample-efficient learning. In model-based reinforcement learning efficiency is improved by learning to simulate the world dynamics. The challenge is that model inaccuracies rapidly accumulate over planned trajectories. We introduce deep Gaussian processes where the depth of the compositions introduces model complexity while incorporating prior knowledge on the dynamics brings smoothness and structure. Our approach is able to sample a Bayesian posterior over trajectories. We demonstrate highly improved early sample-efficiency over competing methods. This is shown across a number of continuous control tasks, including the half-cheetah whose contact dynamics have previously posed an insurmountable problem for earlier sample-efficient Gaussian process based models.",0
"By using reinforcement learning, one can learn how to control actions to achieve a goal through trial-and-error. However, observing interactions can be expensive, which requires a more efficient method of learning. Model-based reinforcement learning attempts to improve efficiency by learning to simulate how the world behaves. However, model inaccuracies accumulate quickly over planned trajectories. Our solution is to use deep Gaussian processes that incorporate prior knowledge and bring structure and smoothness to the model. This approach allows us to sample a Bayesian posterior over trajectories and improves early sample-efficiency compared to other methods. This is demonstrated in various continuous control tasks, including the half-cheetah, which has been problematic for previous sample-efficient Gaussian process based models.",1
"Solving real-life sequential decision making problems under partial observability involves an exploration-exploitation problem. To be successful, an agent needs to efficiently gather valuable information about the state of the world for making rewarding decisions. However, in real-life, acquiring valuable information is often highly costly, e.g., in the medical domain, information acquisition might correspond to performing a medical test on a patient. This poses a significant challenge for the agent to perform optimally for the task while reducing the cost for information acquisition. In this paper, we propose a model-based reinforcement learning framework that learns an active feature acquisition policy to solve the exploration-exploitation problem during its execution. Key to the success is a novel sequential variational auto-encoder that learns high-quality representations from partially observed states, which are then used by the policy to maximize the task reward in a cost efficient manner. We demonstrate the efficacy of our proposed framework in a control domain as well as using a medical simulator. In both tasks, our proposed method outperforms conventional baselines and results in policies with greater cost efficiency.",0
"The successful resolution of real-life decision making problems that involve partial observability necessitates the resolution of an exploration-exploitation problem. For an agent to make rewarding decisions, they must efficiently gather valuable information about the state of the world. However, obtaining valuable information in real-life situations can be highly expensive, such as in the medical field where information acquisition may involve conducting medical tests on patients. This presents a significant challenge for the agent to perform optimally while minimizing the cost of information acquisition. In this study, we propose a model-based reinforcement learning framework that learns an active feature acquisition policy for solving the exploration-exploitation problem during execution. Our approach incorporates a novel sequential variational auto-encoder that learns high-quality representations from partially observed states, which the policy uses to maximize task rewards in a cost-effective manner. We demonstrate the effectiveness of our framework in a control domain and a medical simulator. In both scenarios, our approach outperforms conventional baselines and yields policies with greater cost efficiency.",1
"Numerous deep reinforcement learning agents have been proposed, and each of them has its strengths and flaws. In this work, we present a Cooperative Heterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a policy by integrating the advantages of heterogeneous agents. Specifically, we propose a cooperative learning framework that classifies heterogeneous agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from the other agents. Local agents are either on-policy agents or population-based evolutionary algorithms (EAs) agents that can explore the local area effectively. We employ global agents, which are sample-efficient, to guide the learning of local agents so that local agents can benefit from sample-efficient agents and simultaneously maintain their advantages, e.g., stability. Global agents also benefit from effective local searches. Experimental studies on a range of continuous control tasks from the Mujoco benchmark show that CHDRL achieves better performance compared with state-of-the-art baselines.",0
"In the field of deep reinforcement learning, various agents have been suggested, each with its own pros and cons. The purpose of this study is to introduce a Cooperative Heterogeneous Deep Reinforcement Learning (CHDRL) framework that combines the strengths of different agents to learn policies. The framework includes a cooperative learning method that separates agents into two categories: global agents and local agents. Global agents, which are off-policy, can learn from other agents' experiences, while local agents, such as on-policy agents or evolutionary algorithms, can explore their immediate surroundings efficiently. Global agents assist in guiding local agents' learning, allowing them to benefit from sample-efficient agents while maintaining their own advantages. In this way, both global and local agents benefit from the other's strengths. The CHDRL framework has been tested on various continuous control tasks from the Mujoco benchmark, and it has outperformed existing baselines.",1
"There has recently been a surge in research in batch Deep Reinforcement Learning (DRL), which aims for learning a high-performing policy from a given dataset without additional interactions with the environment. We propose a new algorithm, Best-Action Imitation Learning (BAIL), which strives for both simplicity and performance. BAIL learns a V function, uses the V function to select actions it believes to be high-performing, and then uses those actions to train a policy network using imitation learning. For the MuJoCo benchmark, we provide a comprehensive experimental study of BAIL, comparing its performance to four other batch Q-learning and imitation-learning schemes for a large variety of batch datasets. Our experiments show that BAIL's performance is much higher than the other schemes, and is also computationally much faster than the batch Q-learning schemes.",0
"In recent times, there has been a surge in research on batch Deep Reinforcement Learning (DRL), which focuses on learning a high-performing policy from a given dataset without any additional interactions with the environment. Our proposed algorithm, Best-Action Imitation Learning (BAIL), aims to achieve both simplicity and performance. BAIL learns a V function, utilizes it to select high-performing actions, and trains a policy network using imitation learning. We conducted a comprehensive experimental study of BAIL for the MuJoCo benchmark, comparing its performance to four other batch Q-learning and imitation-learning methods across a large variety of batch datasets. Results indicate that BAIL outperforms the other schemes and is also computationally faster than the batch Q-learning schemes.",1
"Applying machine learning techniques to graph drawing has become an emergent area of research in visualization. In this paper, we interpret graph drawing as a multi-agent reinforcement learning (MARL) problem. We first demonstrate that a large number of classic graph drawing algorithms, including force-directed layouts and stress majorization, can be interpreted within the framework of MARL. Using this interpretation, a node in the graph is assigned to an agent with a reward function. Via multi-agent reward maximization, we obtain an aesthetically pleasing graph layout that is comparable to the outputs of classic algorithms. The main strength of a MARL framework for graph drawing is that it not only unifies a number of classic drawing algorithms in a general formulation but also supports the creation of novel graph drawing algorithms by introducing a diverse set of reward functions.",0
"The use of machine learning techniques in graph drawing has emerged as a popular area of research in visualization. In this article, we view graph drawing as a problem of multi-agent reinforcement learning (MARL). Our study shows that many traditional graph drawing algorithms, such as force-directed layouts and stress majorization, can be interpreted within the MARL framework. We assign an agent with a reward function to each node in the graph and use multi-agent reward maximization to obtain a visually appealing graph layout that is comparable to the outputs of classic algorithms. The MARL framework for graph drawing offers a significant advantage in that it not only consolidates various traditional drawing algorithms but also allows for the creation of new algorithms through the use of diverse reward functions.",1
"Off-Policy Actor-Critic (Off-PAC) methods have proven successful in a variety of continuous control tasks. Normally, the critic's action-value function is updated using temporal-difference, and the critic in turn provides a loss for the actor that trains it to take actions with higher expected return. In this paper, we introduce a novel and flexible meta-critic that observes the learning process and meta-learns an additional loss for the actor that accelerates and improves actor-critic learning. Compared to the vanilla critic, the meta-critic network is explicitly trained to accelerate the learning process; and compared to existing meta-learning algorithms, meta-critic is rapidly learned online for a single task, rather than slowly over a family of tasks. Crucially, our meta-critic framework is designed for off-policy based learners, which currently provide state-of-the-art reinforcement learning sample efficiency. We demonstrate that online meta-critic learning leads to improvements in avariety of continuous control environments when combined with contemporary Off-PAC methods DDPG, TD3 and the state-of-the-art SAC.",0
"Various continuous control tasks have seen success with Off-Policy Actor-Critic (Off-PAC) methods. Typically, the critic's action-value function receives updates via temporal-difference, providing a loss for the actor to learn actions with higher expected return. In this report, a novel meta-critic is introduced that observes the learning process and meta-learns an additional loss for the actor, resulting in improved and accelerated actor-critic learning. The meta-critic network is explicitly trained to hasten the learning process and can be learned online for a single task, unlike existing meta-learning algorithms that slowly learn over a family of tasks. Our meta-critic framework is tailored for off-policy based learners, which currently exhibit state-of-the-art reinforcement learning sample efficiency. Combining contemporary Off-PAC methods, such as DDPG, TD3, and SAC with online meta-critic learning leads to enhancements in various continuous control environments.",1
"This work presents a novel algorithm that integrates a data-efficient function approximator with reinforcement learning in continuous state spaces. An online and incremental algorithm capable of learning from a single pass through data, called Incremental Gaussian Mixture Network (IGMN), was employed as a sample-efficient function approximator for the joint state and Q-values space, all in a single model, resulting in a concise and data-efficient algorithm, i.e., a reinforcement learning algorithm that learns from very few interactions with the environment. Results are analyzed to explain the properties of the obtained algorithm, and it is observed that the use of the IGMN function approximator brings some important advantages to reinforcement learning in relation to conventional neural networks trained by gradient descent methods.",0
"The study introduces a new approach that combines reinforcement learning in continuous state spaces with a data-efficient function approximator. The Incremental Gaussian Mixture Network (IGMN) algorithm is utilized as a sample-efficient function approximator for the joint state and Q-values space, resulting in a concise and data-efficient model that can learn from a single pass through data. The obtained algorithm is capable of learning from very few interactions with the environment, and its properties are analyzed through the results. The study also highlights the advantages of IGMN function approximator over conventional neural networks trained by gradient descent methods in reinforcement learning.",1
"Potential-based reward shaping provides an approach for designing good reward functions, with the purpose of speeding up learning. However, automatically finding potential functions for complex environments is a difficult problem (in fact, of the same difficulty as learning a value function from scratch). We propose a new framework for learning potential functions by leveraging ideas from graph representation learning. Our approach relies on Graph Convolutional Networks which we use as a key ingredient in combination with the probabilistic inference view of reinforcement learning. More precisely, we leverage Graph Convolutional Networks to perform message passing from rewarding states. The propagated messages can then be used as potential functions for reward shaping to accelerate learning. We verify empirically that our approach can achieve considerable improvements in both small and high-dimensional control problems.",0
"The use of potential-based reward shaping is a way to create effective reward functions that can speed up the learning process. However, the difficulty of finding potential functions for complex environments is equivalent to learning a value function from scratch, making it a challenging task. To address this challenge, we introduce a new framework that incorporates graph representation learning. Our approach utilizes Graph Convolutional Networks and the probabilistic inference view of reinforcement learning to perform message passing from rewarding states. These messages can be used as potential functions for reward shaping to enhance learning. Empirical evidence confirms that our approach can lead to significant improvements in small and large-scale control problems.",1
"In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.",0
"The purpose of this tutorial article is to equip readers with the necessary conceptual tools for researching offline reinforcement learning algorithms. These algorithms use previously collected data without requiring any additional online data collection. The potential benefits of these algorithms are significant as they can turn large datasets into powerful decision-making engines. Successful offline reinforcement learning methods can extract policies with the highest possible utility from the available data, which can automate decision-making in various domains, including healthcare, education, and robotics. However, the current algorithms have limitations that make this challenging. This article aims to provide readers with an understanding of these challenges, especially in the context of modern deep reinforcement learning methods. It also discusses potential solutions that researchers have explored to mitigate these challenges, along with recent applications and a discussion of open problems in the field.",1
"We present a Reverse Reinforcement Learning (Reverse RL) approach for representing retrospective knowledge. General Value Functions (GVFs) have enjoyed great success in representing predictive knowledge, i.e., answering questions about possible future outcomes such as ""how much fuel will be consumed in expectation if we drive from A to B?"". GVFs, however, cannot answer questions like ""how much fuel do we expect a car to have given it is at B at time $t$?"". To answer this question, we need to know when that car had a full tank and how that car came to B. Since such questions emphasize the influence of possible past events on the present, we refer to their answers as retrospective knowledge. In this paper, we show how to represent retrospective knowledge with Reverse GVFs, which are trained via Reverse RL. We demonstrate empirically the utility of Reverse GVFs in both representation learning and anomaly detection.",0
"Our paper introduces a new approach, called Reverse Reinforcement Learning (Reverse RL), that represents retrospective knowledge. While General Value Functions (GVFs) have been successful in representing predictive knowledge, they cannot answer questions that require knowledge of past events. For example, to determine how much fuel a car has at a certain time, we need to know when it last had a full tank and how it arrived at its current location. Such questions require an understanding of the impact of past events on the present, which we refer to as retrospective knowledge. We propose using Reverse GVFs, trained through Reverse RL, to represent this type of knowledge. Through empirical analysis, we demonstrate the usefulness of Reverse GVFs in both representation learning and anomaly detection.",1
"Instrumental variable (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by utilizing an instrumental variable, which affects the outcome only through the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to the outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally efficient manner. DFIV outperforms recent state-of-the-art methods on challenging IV benchmarks, including settings involving high dimensional image data. DFIV also exhibits competitive performance in off-policy policy evaluation for reinforcement learning, which can be understood as an IV regression task.",0
"The use of instrumental variable (IV) regression is a common approach to determine causal relationships between confounded treatment and outcome variables in observational data. An instrumental variable only affects the outcome through the treatment. The classical IV regression method consists of two stages: stage 1 involves linear regression from the instrument to the treatment, and stage 2 involves linear regression from the treatment to the outcome, conditioned on the instrument. To address cases where the relationships between instruments, treatments, and outcomes are nonlinear, we propose a new method called deep feature instrumental variable regression (DFIV). This method uses deep neural nets to define informative nonlinear features on the instruments and treatments. We use an alternating training regime to ensure good end-to-end performance when composing stages 1 and 2, resulting in highly flexible feature maps in a computationally efficient manner. DFIV outperforms recent state-of-the-art methods on difficult IV benchmarks, including settings involving high dimensional image data. Additionally, DFIV exhibits competitive performance in off-policy policy evaluation for reinforcement learning, which can be understood as an IV regression task.",1
"Industrial Internet of Things (IoT) enables distributed intelligent services varying with the dynamic and realtime industrial devices to achieve Industry 4.0 benefits. In this paper, we consider a new architecture of digital twin empowered Industrial IoT where digital twins capture the characteristics of industrial devices to assist federated learning. Noticing that digital twins may bring estimation deviations from the actual value of device state, a trusted based aggregation is proposed in federated learning to alleviate the effects of such deviation. We adaptively adjust the aggregation frequency of federated learning based on Lyapunov dynamic deficit queue and deep reinforcement learning, to improve the learning performance under the resource constraints. To further adapt to the heterogeneity of Industrial IoT, a clustering-based asynchronous federated learning framework is proposed. Numerical results show that the proposed framework is superior to the benchmark in terms of learning accuracy, convergence, and energy saving.",0
"The potential benefits of Industry 4.0 can be achieved through the use of Industrial Internet of Things (IoT), which enables the provision of distributed intelligent services that vary in accordance with dynamic and real-time industrial devices. This paper proposes a new architecture for digital twin-empowered Industrial IoT, where digital twins capture the features of industrial devices to assist federated learning. To address the issue of estimation deviations caused by digital twins, a trusted-based aggregation is suggested in federated learning to mitigate the impact of such deviations. To improve the learning performance under resource limitations, the aggregation frequency of federated learning is adaptively adjusted based on Lyapunov dynamic deficit queue and deep reinforcement learning. Furthermore, a clustering-based asynchronous federated learning framework is proposed to better adapt to the heterogeneity of Industrial IoT. The numerical results demonstrate that the proposed framework outperforms the benchmark in terms of learning accuracy, convergence, and energy efficiency.",1
"In this paper, a simple and efficient Hybrid Classifier is presented which is based on deep learning and reinforcement learning. Here, Q-Learning has been used with two states and 'two or three' actions. Other techniques found in the literature use feature map extracted from Convolutional Neural Networks and use these in the Q-states along with past history. This leads to technical difficulties in these approaches because the number of states is high due to large dimensions of the feature map. Because the proposed technique uses only two Q-states it is straightforward and consequently has much lesser number of optimization parameters, and thus also has a simple reward function. Also, the proposed technique uses novel actions for processing images as compared to other techniques found in literature. The performance of the proposed technique is compared with other recent algorithms like ResNet50, InceptionV3, etc. on popular databases including ImageNet, Cats and Dogs Dataset, and Caltech-101 Dataset. The proposed approach outperforms others techniques on all the datasets used.",0
"This paper presents a Hybrid Classifier that combines deep learning and reinforcement learning, which is simple and efficient. Unlike other techniques that use feature maps extracted from Convolutional Neural Networks, the proposed technique only uses two Q-states and novel actions for processing images. This results in a much simpler reward function and fewer optimization parameters. The performance of the proposed approach is compared to ResNet50, InceptionV3, and other recent algorithms on popular databases such as ImageNet, Cats and Dogs Dataset, and Caltech-101 Dataset, and it outperforms them all.",1
"Anomaly detection is concerned with identifying data patterns that deviate remarkably from the expected behaviour. This is an important research problem, due to its broad set of application domains, from data analysis to e-health, cybersecurity, predictive maintenance, fault prevention, and industrial automation. Herein, we review state-of-the-art methods that may be employed to detect anomalies in the specific area of sensor systems, which poses hard challenges in terms of information fusion, data volumes, data speed, and network/energy efficiency, to mention but the most pressing ones. In this context, anomaly detection is a particularly hard problem, given the need to find computing-energy accuracy trade-offs in a constrained environment. We taxonomize methods ranging from conventional techniques (statistical methods, time-series analysis, signal processing, etc.) to data-driven techniques (supervised learning, reinforcement learning, deep learning, etc.). We also look at the impact that different architectural environments (Cloud, Fog, Edge) can have on the sensors ecosystem. The review points to the most promising intelligent-sensing methods, and pinpoints a set of interesting open issues and challenges.",0
"The identification of data patterns that deviate significantly from expected behavior is the focus of anomaly detection, which is a critical research area with a wide range of applications including data analysis, e-health, cybersecurity, predictive maintenance, fault prevention, and industrial automation. Our review examines the latest techniques for detecting anomalies in sensor systems, which present significant challenges such as information fusion, large data volumes, high data speeds, and network and energy efficiency. This is a particularly challenging problem because of the need to balance computing-energy accuracy trade-offs in a constrained environment. Our taxonomy of methods covers both conventional techniques (such as statistical methods, time-series analysis, and signal processing) and data-driven techniques (such as supervised learning, reinforcement learning, and deep learning), as well as the impact of different architectural environments (Cloud, Fog, Edge) on the sensors ecosystem. The review highlights the most promising intelligent-sensing methods and identifies several interesting open issues and challenges.",1
"Offline Reinforcement Learning (RL) is a promising approach for learning optimal policies in environments where direct exploration is expensive or unfeasible. However, the adoption of such policies in practice is often challenging, as they are hard to interpret within the application context, and lack measures of uncertainty for the learned policy value and its decisions. To overcome these issues, we propose an Expert-Supervised RL (ESRL) framework which uses uncertainty quantification for offline policy learning. In particular, we have three contributions: 1) the method can learn safe and optimal policies through hypothesis testing, 2) ESRL allows for different levels of risk averse implementations tailored to the application context, and finally, 3) we propose a way to interpret ESRL's policy at every state through posterior distributions, and use this framework to compute off-policy value function posteriors. We provide theoretical guarantees for our estimators and regret bounds consistent with Posterior Sampling for RL (PSRL). Sample efficiency of ESRL is independent of the chosen risk aversion threshold and quality of the behavior policy.",0
"Learning optimal policies through direct exploration can be expensive or impractical in certain environments, making Offline Reinforcement Learning (RL) a promising alternative. However, implementing these policies in practice can be challenging due to their lack of interpretability and measures of uncertainty. To address these issues, we propose an Expert-Supervised RL (ESRL) framework that utilizes uncertainty quantification for offline policy learning. Our approach offers three key contributions: 1) safe and optimal policies can be learned through hypothesis testing, 2) ESRL can be customized to different levels of risk aversion based on the application context, and 3) the policy can be interpreted at each state through posterior distributions, which can be used to compute off-policy value function posteriors. Our estimators have theoretical guarantees and regret bounds consistent with Posterior Sampling for RL (PSRL), and the sample efficiency of ESRL is independent of the chosen risk aversion threshold and quality of the behavior policy.",1
"Despite success on a wide range of problems related to vision, generative adversarial networks (GANs) often suffer from inferior performance due to unstable training, especially for text generation. To solve this issue, we propose a new variational GAN training framework which enjoys superior training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that improves discriminator training by downplaying bad-quality fake samples. Moreover, our variational GAN framework can provably overcome the training issue in many GANs that an optimal discriminator cannot provide any informative gradient to training generator. By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain significantly improved performance over a range of tasks, including text generation, text style transfer, and image generation.",0
"GANs often struggle with unstable training, particularly when it comes to text generation, despite achieving success in various vision-related problems. To address this issue, we present a new training framework for variational GANs that boasts superior training stability. Our inspiration comes from the relationship between GANs and reinforcement learning from a variational viewpoint, resulting in two key features: probability ratio clipping to regulate generator training and prevent excessively large updates, and a sample re-weighting mechanism that enhances discriminator training by giving less weight to poor-quality fake samples. Additionally, our variational GAN framework can solve a common training problem in many GANs where a perfect discriminator cannot provide informative feedback to the generator. We have incorporated our training approach into a variety of state-of-the-art GAN architectures and achieved significant improvements in tasks such as text generation, text style transfer, and image generation.",1
"A fundamental question in neuroscience is how the brain creates an internal model of the world to guide actions using sequences of ambiguous sensory information. This is naturally formulated as a reinforcement learning problem under partial observations, where an agent must estimate relevant latent variables in the world from its evidence, anticipate possible future states, and choose actions that optimize total expected reward. This problem can be solved by control theory, which allows us to find the optimal actions for a given system dynamics and objective function. However, animals often appear to behave suboptimally. Why? We hypothesize that animals have their own flawed internal model of the world, and choose actions with the highest expected subjective reward according to that flawed model. We describe this behavior as rational but not optimal. The problem of Inverse Rational Control (IRC) aims to identify which internal model would best explain an agent's actions. Our contribution here generalizes past work on Inverse Rational Control which solved this problem for discrete control in partially observable Markov decision processes. Here we accommodate continuous nonlinear dynamics and continuous actions, and impute sensory observations corrupted by unknown noise that is private to the animal. We first build an optimal Bayesian agent that learns an optimal policy generalized over the entire model space of dynamics and subjective rewards using deep reinforcement learning. Crucially, this allows us to compute a likelihood over models for experimentally observable action trajectories acquired from a suboptimal agent. We then find the model parameters that maximize the likelihood using gradient ascent.",0
"Neuroscience aims to understand how the brain creates an internal model of the world to guide actions, even with ambiguous sensory information. This is a reinforcement learning problem that requires the agent to estimate latent variables, anticipate future states, and choose actions that optimize reward. Control theory can solve this problem, but animals often behave suboptimally, suggesting they have a flawed internal model. Inverse Rational Control (IRC) aims to identify the internal model that best explains an agent's actions. The new contribution is a generalization of past work on IRC, accommodating continuous nonlinear dynamics and continuous actions, and imputing sensory observations corrupted by unknown noise. An optimal Bayesian agent is built using deep reinforcement learning to learn an optimal policy for the entire model space. This allows for the computation of a likelihood over models for experimentally observable action trajectories from suboptimal agents. Finally, model parameters are found using gradient ascent.",1
"Many open problems in machine learning are intrinsically related to causality, however, the use of causal analysis in machine learning is still in its early stage. Within a general reinforcement learning setting, we consider the problem of building a general reinforcement learning agent which uses experience to construct a causal graph of the environment, and use this graph to inform its policy. Our approach has three characteristics: First, we learn a simple, coarse-grained causal graph, in which the variables reflect states at many time instances, and the interventions happen at the level of policies, rather than individual actions. Secondly, we use mediation analysis to obtain an optimization target. By minimizing this target, we define the causal variables. Thirdly, our approach relies on estimating conditional expectations rather the familiar expected return from reinforcement learning, and we therefore apply a generalization of Bellman's equations. We show the method can learn a plausible causal graph in a grid-world environment, and the agent obtains an improvement in performance when using the causally informed policy. To our knowledge, this is the first attempt to apply causal analysis in a reinforcement learning setting without strict restrictions on the number of states. We have observed that mediation analysis provides a promising avenue for transforming the problem of causal acquisition into one of cost-function minimization, but importantly one which involves estimating conditional expectations. This is a new challenge, and we think that causal reinforcement learning will involve development methods suited for online estimation of such conditional expectations. Finally, a benefit of our approach is the use of very simple causal models, which are arguably a more natural model of human causal understanding.",0
"Causality is a fundamental issue in many unresolved problems in machine learning. Despite this, the use of causal analysis in machine learning is still in its early stages. In this study, we aim to create a general reinforcement learning agent that constructs a causal graph of the environment using its experience and uses this graph to inform its policy. Our method has three key features. First, we develop a simple, coarse-grained causal graph that reflects states at various time intervals, with interventions taking place at the policy level rather than individual actions. Second, we use mediation analysis to obtain our optimization target, which defines the causal variables by minimizing this target. Third, we estimate conditional expectations rather than the expected return from reinforcement learning, which requires a generalization of Bellman's equations. Our findings show that our approach can learn a plausible causal graph in a grid-world environment, and the agent's performance improves when using the causally informed policy. Our approach is the first to apply causal analysis in a reinforcement learning setting without restrictions on the number of states. We believe that mediation analysis is a promising method for transforming the problem of causal acquisition into cost-function minimization. However, we acknowledge that estimating conditional expectations is a new challenge in causal reinforcement learning, and we anticipate the development of methods suited for online estimation of such expectations. Lastly, our approach benefits from using simple causal models, which align with human causal understanding.",1
"Reinforcement learning is a promising approach for learning control policies for robot tasks. However, specifying complex tasks (e.g., with multiple objectives and safety constraints) can be challenging, since the user must design a reward function that encodes the entire task. Furthermore, the user often needs to manually shape the reward to ensure convergence of the learning algorithm. We propose a language for specifying complex control tasks, along with an algorithm that compiles specifications in our language into a reward function and automatically performs reward shaping. We implement our approach in a tool called SPECTRL, and show that it outperforms several state-of-the-art baselines.",0
"Learning control policies for robot tasks through reinforcement learning shows great potential. However, it can be difficult to define complex tasks that involve multiple objectives and safety constraints. This is because the user must create a reward function that represents the entire task, and often must manually adjust it for the learning algorithm to converge. To address this issue, we introduce a language for specifying complex control tasks and an algorithm that automatically generates a reward function and shapes it. Our tool, SPECTRL, implements this approach and outperforms several current methods.",1
"Recent studies have revealed that neural network-based policies can be easily fooled by adversarial examples. However, while most prior works analyze the effects of perturbing every pixel of every frame assuming white-box policy access, in this paper we take a more restrictive view towards adversary generation - with the goal of unveiling the limits of a model's vulnerability. In particular, we explore minimalistic attacks by defining three key settings: (1) black-box policy access: where the attacker only has access to the input (state) and output (action probability) of an RL policy; (2) fractional-state adversary: where only several pixels are perturbed, with the extreme case being a single-pixel adversary; and (3) tactically-chanced attack: where only significant frames are tactically chosen to be attacked. We formulate the adversarial attack by accommodating the three key settings and explore their potency on six Atari games by examining four fully trained state-of-the-art policies. In Breakout, for example, we surprisingly find that: (i) all policies showcase significant performance degradation by merely modifying 0.01% of the input state, and (ii) the policy trained by DQN is totally deceived by perturbation to only 1% frames.",0
"Recent research has shown that neural network-based policies are susceptible to being tricked by adversarial examples. However, this paper takes a more limited approach to generating adversaries in order to reveal the extent of a model's vulnerability. Instead of analyzing the effects of perturbing every pixel in every frame with white-box policy access, we explore minimalistic attacks using three key settings: black-box policy access, fractional-state adversary, and tactically-chanced attack. By accommodating these settings, we formulate the adversarial attack and investigate its potency on six Atari games with four fully trained state-of-the-art policies. Surprisingly, in Breakout, we discover that modifying only 0.01% of the input state results in significant performance degradation for all policies, and DQN's policy is completely deceived by perturbing only 1% of the frames.",1
"Sufficient exploration is paramount for the success of a reinforcement learning agent. Yet, exploration is rarely assessed in an algorithm-independent way. We compare the behavior of three data-based, offline exploration metrics described in the literature on intuitive simple distributions and highlight problems to be aware of when using them. We propose a fourth metric,uniform relative entropy, and implement it using either a k-nearest-neighbor or a nearest-neighbor-ratio estimator, highlighting that the implementation choices have a profound impact on these measures.",0
"The success of a reinforcement learning agent heavily relies on thorough exploration. However, evaluating exploration in a manner that is not dependent on the algorithm is uncommon. We analyze three offline exploration metrics that are based on data and discussed in literature, and identify potential issues that should be taken into consideration when utilizing them. In addition, we introduce a fourth metric, which is the uniform relative entropy, and demonstrate how it can be implemented using either a k-nearest-neighbor or a nearest-neighbor-ratio estimator. We emphasize that the implementation method chosen can significantly affect the results obtained from these metrics.",1
"Despite empirical success, the theory of reinforcement learning (RL) with value function approximation remains fundamentally incomplete. Prior work has identified a variety of pathological behaviours that arise in RL algorithms that combine approximate on-policy evaluation and greedification. One prominent example is policy oscillation, wherein an algorithm may cycle indefinitely between policies, rather than converging to a fixed point. What is not well understood however is the quality of the policies in the region of oscillation. In this paper we present simple examples illustrating that in addition to policy oscillation and multiple fixed points -- the same basic issue can lead to convergence to the worst possible policy for a given approximation. Such behaviours can arise when algorithms optimize evaluation accuracy weighted by the distribution of states that occur under the current policy, but greedify based on the value of states which are rare or nonexistent under this distribution. This means the values used for greedification are unreliable and can steer the policy in undesirable directions. Our observation that this can lead to the worst possible policy shows that in a general sense such algorithms are unreliable. The existence of such examples helps to narrow the kind of theoretical guarantees that are possible and the kind of algorithmic ideas that are likely to be helpful. We demonstrate analytically and experimentally that such pathological behaviours can impact a wide range of RL and dynamic programming algorithms; such behaviours can arise both with and without bootstrapping, and with linear function approximation as well as with more complex parameterized functions like neural networks.",0
"Although reinforcement learning (RL) with value function approximation has been successful in practice, there are still fundamental incompleteness issues with this theory. Previous research has highlighted several problematic behaviours in RL algorithms that combine approximate on-policy evaluation and greedification, such as policy oscillation, where an algorithm cycles between policies instead of converging to a fixed point. The impact of this oscillation on policy quality is not well understood. Our paper presents simple examples that demonstrate that policy oscillation and multiple fixed points can also lead to convergence to the worst possible policy for a given approximation. This occurs when algorithms optimize evaluation accuracy based on the distribution of states that occur under the current policy but greedify based on the value of states that are rare or nonexistent under this distribution. The resulting values used for greedification are unreliable and can lead to undesirable policy directions. Our findings suggest that such algorithms are generally unreliable, which provides insight into the theoretical guarantees and algorithmic ideas that may be helpful. We demonstrate that such pathological behaviours can impact a wide range of RL and dynamic programming algorithms, including those with and without bootstrapping, and those that use linear function approximation or more complex parameterized functions like neural networks.",1
"Stochastic approximation, a data-driven approach for finding the fixed point of an unknown operator, provides a unified framework for treating many problems in stochastic optimization and reinforcement learning. Motivated by a growing interest in multi-agent and multi-task learning, we consider in this paper a decentralized variant of stochastic approximation. A network of agents, each with their own unknown operator and data observations, cooperatively find the fixed point of the aggregate operator. The agents work by running a local stochastic approximation algorithm using noisy samples from their operators while averaging their iterates with their neighbors' on a decentralized communication graph. Our main contribution provides a finite-time analysis of this decentralized stochastic approximation algorithm and characterizes the impacts of the underlying communication topology between agents. Our model for the data observed at each agent is that it is sampled from a Markov processes; this lack of independence makes the iterates biased and (potentially) unbounded. Under mild assumptions on the Markov processes, we show that the convergence rate of the proposed methods is essentially the same as if the samples were independent, differing only by a log factor that represents the mixing time of the Markov process. We also present applications of the proposed method on a number of interesting learning problems in multi-agent systems, including a decentralized variant of Q-learning for solving multi-task reinforcement learning.",0
"In this paper, we explore a decentralized adaptation of stochastic approximation, which is a data-driven technique for discovering the fixed point of an unknown operator that has proven effective in solving problems in stochastic optimization and reinforcement learning. Our objective is to apply this approach to multi-task and multi-agent learning scenarios. Our method involves agents who each possess their own unknown operator and data, working collaboratively to locate the fixed point of the aggregate operator. The agents utilize a local stochastic approximation algorithm to compute their iterates, using noisy samples from their operator and averaging their results with their neighbors on a decentralized communication graph. We present a finite-time analysis of this decentralized stochastic approximation algorithm and examine the impact of the communication topology between agents. We assume that the data observed by each agent is sampled from a Markov process, which results in biased and potentially unbounded iterates. However, we demonstrate that under mild assumptions on the Markov processes, the convergence rate of our method is similar to that of independent samples, with only a log factor representing the mixing time of the process. Finally, we demonstrate the usefulness of our approach by applying it to several multi-agent learning problems, including a decentralized variant of Q-learning for solving multi-task reinforcement learning.",1
"Model-based reinforcement learning methods learn a dynamics model with real data sampled from the environment and leverage it to generate simulated data to derive an agent. However, due to the potential distribution mismatch between simulated data and real data, this could lead to degraded performance. Despite much effort being devoted to reducing this distribution mismatch, existing methods fail to solve it explicitly. In this paper, we investigate how to bridge the gap between real and simulated data due to inaccurate model estimation for better policy optimization. To begin with, we first derive a lower bound of the expected return, which naturally inspires a bound maximization algorithm by aligning the simulated and real data distributions. To this end, we propose a novel model-based reinforcement learning framework AMPO, which introduces unsupervised model adaptation to minimize the integral probability metric (IPM) between feature distributions from real and simulated data. Instantiating our framework with Wasserstein-1 distance gives a practical model-based approach. Empirically, our approach achieves state-of-the-art performance in terms of sample efficiency on a range of continuous control benchmark tasks.",0
"Reinforcement learning methods that rely on models learn the dynamics of an environment by using real data to generate simulated data that helps to create an agent. However, this approach can result in poor performance due to differences between the simulated and real data. Despite attempts to minimize this issue, current methods do not fully address it. This paper explores ways to overcome the gap between real and simulated data caused by inaccurate model estimation, in order to improve policy optimization. We introduce a new model-based reinforcement learning framework called AMPO, which uses unsupervised model adaptation to minimize the difference between feature distributions from real and simulated data. Our approach surpasses previous methods in terms of sample efficiency on several continuous control benchmark tasks.",1
"We study a decentralized cooperative multi-agent multi-armed bandit problem with $K$ arms and $N$ agents connected over a network. In our model, each arm's reward distribution is same for all agents, and rewards are drawn independently across agents and over time steps. In each round, agents choose an arm to play and subsequently send a message to their neighbors. The goal is to minimize cumulative regret averaged over the entire network. We propose a decentralized Bayesian multi-armed bandit framework that extends single-agent Bayesian bandit algorithms to the decentralized setting. Specifically, we study an information assimilation algorithm that can be combined with existing Bayesian algorithms, and using this, we propose a decentralized Thompson Sampling algorithm and decentralized Bayes-UCB algorithm. We analyze the decentralized Thompson Sampling algorithm under Bernoulli rewards and establish a problem-dependent upper bound on the cumulative regret. We show that regret incurred scales logarithmically over the time horizon with constants that match those of an optimal centralized agent with access to all observations across the network. Our analysis also characterizes the cumulative regret in terms of the network structure. Through extensive numerical studies, we show that our extensions of Thompson Sampling and Bayes-UCB incur lesser cumulative regret than the state-of-art algorithms inspired by the Upper Confidence Bound algorithm. We implement our proposed decentralized Thompson Sampling under gossip protocol, and over time-varying networks, where each communication link has a fixed probability of failure.",0
"Our focus is on a decentralized cooperative multi-agent multi-armed bandit problem that involves $K$ arms and a network of $N$ agents. In our model, all agents experience the same reward distribution for each arm, with rewards independently drawn across time steps and agents. In each round, agents select an arm to play, and then communicate their decision to their neighbors. The objective is to minimize cumulative regret across the entire network. To achieve this, we introduce a decentralized Bayesian multi-armed bandit framework that extends single-agent Bayesian bandit algorithms to the decentralized setting. We propose a decentralized Thompson Sampling algorithm and decentralized Bayes-UCB algorithm, which are analyzed under Bernoulli rewards to establish an upper bound on cumulative regret that scales logarithmically over the time horizon. Our approach also characterizes cumulative regret in relation to the network structure. Numerical studies demonstrate that our extensions of Thompson Sampling and Bayes-UCB outperform state-of-the-art algorithms inspired by the Upper Confidence Bound algorithm. Finally, we implement our proposed decentralized Thompson Sampling under a gossip protocol and study its performance over time-varying networks with a fixed probability of communication link failure.",1
"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.",0
"Recently, Contrastive Learning has gained attention for its achievements in self-supervised representation learning within the computer vision field. However, the roots of Contrastive Learning can be traced back to the 1990s, and its progress has extended to many other areas, such as Metric Learning and natural language processing. In this article, we conduct a thorough review of relevant literature and suggest a general framework for Contrastive Representation Learning that streamlines and consolidates various contrastive learning approaches. Additionally, we present a categorization for each aspect of contrastive learning in order to summarize and differentiate it from other types of machine learning. We also examine the inductive biases in any contrastive learning system and evaluate our framework from various sub-fields of Machine Learning. We provide examples of how contrastive learning has been implemented in computer vision, natural language processing, audio processing, and Reinforcement Learning. Finally, we discuss the challenges and possibilities for future research in this area.",1
"Safety is an essential component for deploying reinforcement learning (RL) algorithms in real-world scenarios, and is critical during the learning process itself. A natural first approach toward safe RL is to manually specify constraints on the policy's behavior. However, just as learning has enabled progress in large-scale development of AI systems, learning safety specifications may also be necessary to ensure safety in messy open-world environments where manual safety specifications cannot scale. Akin to how humans learn incrementally starting in child-safe environments, we propose to learn how to be safe in one set of tasks and environments, and then use that learned intuition to constrain future behaviors when learning new, modified tasks. We empirically study this form of safety-constrained transfer learning in three challenging domains: simulated navigation, quadruped locomotion, and dexterous in-hand manipulation. In comparison to standard deep RL techniques and prior approaches to safe RL, we find that our method enables the learning of new tasks and in new environments with both substantially fewer safety incidents, such as falling or dropping an object, and faster, more stable learning. This suggests a path forward not only for safer RL systems, but also for more effective RL systems.",0
"To deploy reinforcement learning (RL) algorithms in real-world settings, safety is a vital component both during the learning process and in practice. Initially, manually specifying constraints on the policy's behavior was the primary method for safe RL. Nonetheless, such constraints may not be scalable in messy open-world environments, necessitating the learning of safety specifications. Our approach involves learning how to be safe in one set of tasks and environments and using that acquired intuition to constrain future behaviors when learning new, modified tasks. We evaluate this safety-constrained transfer learning in three challenging domains: simulated navigation, quadruped locomotion, and dexterous in-hand manipulation. Our method outperforms standard deep RL techniques and previous safe RL approaches, enabling faster, more stable learning while experiencing fewer safety incidents. This not only suggests a way forward for safer RL systems but also for more effective RL systems.",1
"Reinforcement learning has been applied to a wide variety of robotics problems, but most of such applications involve collecting data from scratch for each new task. Since the amount of robot data we can collect for any single task is limited by time and cost considerations, the learned behavior is typically narrow: the policy can only execute the task in a handful of scenarios that it was trained on. What if there was a way to incorporate a large amount of prior data, either from previously solved tasks or from unsupervised or undirected environment interaction, to extend and generalize learned behaviors? While most prior work on extending robotic skills using pre-collected data focuses on building explicit hierarchies or skill decompositions, we show in this paper that we can reuse prior data to extend new skills simply through dynamic programming. We show that even when the prior data does not actually succeed at solving the new task, it can still be utilized for learning a better policy, by providing the agent with a broader understanding of the mechanics of its environment. We demonstrate the effectiveness of our approach by chaining together several behaviors seen in prior datasets for solving a new task, with our hardest experimental setting involving composing four robotic skills in a row: picking, placing, drawer opening, and grasping, where a +1/0 sparse reward is provided only on task completion. We train our policies in an end-to-end fashion, mapping high-dimensional image observations to low-level robot control commands, and present results in both simulated and real world domains. Additional materials and source code can be found on our project website: https://sites.google.com/view/cog-rl",0
"The use of reinforcement learning in robotics has been widespread, but most applications require the collection of new data for each new task. This approach has limitations due to time and cost constraints, resulting in a narrow policy that can only perform the task in a few scenarios. However, there may be a way to incorporate prior data from previously solved tasks or unsupervised/undirected environment interaction to expand and generalize learned behavior. Previous work has focused on building explicit hierarchies or skill decompositions, but this paper proposes using dynamic programming to reuse prior data for learning new skills. The approach is effective even when the prior data does not solve the new task, as it provides a broader understanding of the environment mechanics. The authors demonstrate the effectiveness of this approach by combining multiple behaviors from prior datasets to solve a challenging task involving four robotic skills. The policies are trained end-to-end, mapping high-dimensional image observations to low-level robot control commands, and the results are presented in both simulated and real-world domains. Additional information and source code are available on the project website.",1
"We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions, approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by previous instances of the value network, more gradient updates decrease the expressivity of the current value network. We characterize this loss of expressivity in terms of a drop in the rank of the learned value network features, and show that this corresponds to a drop in performance. We demonstrate this phenomenon on widely studies domains, including Atari and Gym benchmarks, in both offline and online RL settings. We formally analyze this phenomenon and show that it results from a pathological interaction between bootstrapping and gradient-based optimization. We further show that mitigating implicit under-parameterization by controlling rank collapse improves performance.",0
"In value-based deep RL methods that use bootstrapping, we have discovered an implicit under-parameterization phenomenon. This happens when deep neural networks approximate value functions and are trained using gradient descent, with iterated regression onto target values generated by previous instances of the value network. We have observed that as more gradient updates are applied, the current value network's expressivity decreases. We have characterized this loss of expressivity by a drop in the rank of the learned value network features, which leads to a subsequent drop in performance. Our research demonstrates this phenomenon in various domains, including Atari and Gym benchmarks, in both offline and online RL settings. We have analyzed this phenomenon formally and discovered that it results from a pathological interaction between bootstrapping and gradient-based optimization. To improve performance, we have shown that controlling rank collapse can mitigate implicit under-parameterization.",1
"Reinforcement learning (RL) algorithms should learn as much as possible about the environment but not the properties of the physics engines that generate the environment. There are multiple algorithms that solve the task in a physics engine based environment but there is no work done so far to understand if the RL algorithms can generalize across physics engines. In this work, we compare the generalization performance of various deep reinforcement learning algorithms on a variety of control tasks. Our results show that MuJoCo is the best engine to transfer the learning to other engines. On the other hand, none of the algorithms generalize when trained on PyBullet. We also found out that various algorithms have a promising generalizability if the effect of random seeds can be minimized on their performance.",0
"The aim of reinforcement learning (RL) algorithms is to gain knowledge about the environment, without acquiring knowledge about the properties of the physics engines that create the environment. While several algorithms have been developed to solve tasks in physics engine-based environments, research has yet to investigate whether RL algorithms can generalize across different physics engines. In this study, we evaluate the ability of various deep RL algorithms to generalize across a range of control tasks. Our findings indicate that MuJoCo is the most effective engine for transferring learning to other engines. However, none of the algorithms demonstrated generalization when trained on PyBullet. Additionally, we discovered that some algorithms show promising generalization capabilities, provided the impact of random seeds on their performance is minimized.",1
"Local feature frameworks are difficult to learn in an end-to-end fashion, due to the discreteness inherent to the selection and matching of sparse keypoints. We introduce DISK (DIScrete Keypoints), a novel method that overcomes these obstacles by leveraging principles from Reinforcement Learning (RL), optimizing end-to-end for a high number of correct feature matches. Our simple yet expressive probabilistic model lets us keep the training and inference regimes close, while maintaining good enough convergence properties to reliably train from scratch. Our features can be extracted very densely while remaining discriminative, challenging commonly held assumptions about what constitutes a good keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on three public benchmarks.",0
"Learning local feature frameworks in an end-to-end manner is challenging due to the discrete nature of selecting and matching sparse keypoints. To address this issue, we propose a new approach called DISK (DIScrete Keypoints) that utilizes Reinforcement Learning principles to optimize for a high number of accurate feature matches. Our probabilistic model is both simple and expressive, allowing us to keep the training and inference processes closely aligned. Additionally, our model can generate highly-dense and discriminative features, which challenges conventional assumptions about what makes a good keypoint. Our approach delivers superior results on three public benchmarks.",1
"Reinforcement learning (RL) algorithms have demonstrated promising results on complex tasks, yet often require impractical numbers of samples since they learn from scratch. Meta-RL aims to address this challenge by leveraging experience from previous tasks so as to more quickly solve new tasks. However, in practice, these algorithms generally also require large amounts of on-policy experience during the meta-training process, making them impractical for use in many problems. To this end, we propose to learn a reinforcement learning procedure in a federated way, where individual off-policy learners can solve the individual meta-training tasks, and then consolidate these solutions into a single meta-learner. Since the central meta-learner learns by imitating the solutions to the individual tasks, it can accommodate either the standard meta-RL problem setting or a hybrid setting where some or all tasks are provided with example demonstrations. The former results in an approach that can leverage policies learned for previous tasks without significant amounts of on-policy data during meta-training, whereas the latter is particularly useful in cases where demonstrations are easy for a person to provide. Across a number of continuous control meta-RL problems, we demonstrate significant improvements in meta-RL sample efficiency in comparison to prior work as well as the ability to scale to domains with visual observations.",0
"Although reinforcement learning (RL) algorithms have shown promise in tackling complex tasks, they often require an impractical amount of samples as they learn from scratch. Meta-RL aims to overcome this issue by utilizing previous task experience to solve new tasks more quickly. However, these algorithms often need substantial on-policy experience during the meta-training process, making them unsuitable for many problems. Therefore, we suggest learning a reinforcement learning procedure in a federated way, where individual off-policy learners can solve the meta-training tasks, and their solutions are consolidated into a single meta-learner. The central meta-learner learns by imitating the individual task solutions and can be adapted to a standard or hybrid meta-RL problem setting. The standard setting leverages policies learned from previous tasks without significant on-policy data during meta-training, while the hybrid setting is useful when example demonstrations are easy to provide. We demonstrate significant improvements in meta-RL sample efficiency and scalability to domains with visual observations across various continuous control meta-RL problems compared to previous research.",1
"We present VisualHints, a novel environment for multimodal reinforcement learning (RL) involving text-based interactions along with visual hints (obtained from the environment). Real-life problems often demand that agents interact with the environment using both natural language information and visual perception towards solving a goal. However, most traditional RL environments either solve pure vision-based tasks like Atari games or video-based robotic manipulation; or entirely use natural language as a mode of interaction, like Text-based games and dialog systems. In this work, we aim to bridge this gap and unify these two approaches in a single environment for multimodal RL. We introduce an extension of the TextWorld cooking environment with the addition of visual clues interspersed throughout the environment. The goal is to force an RL agent to use both text and visual features to predict natural language action commands for solving the final task of cooking a meal. We enable variations and difficulties in our environment to emulate various interactive real-world scenarios. We present a baseline multimodal agent for solving such problems using CNN-based feature extraction from visual hints and LSTMs for textual feature extraction. We believe that our proposed visual-lingual environment will facilitate novel problem settings for the RL community.",0
"VisualHints is a new environment for multimodal reinforcement learning (RL) that incorporates both text-based interactions and visual hints from the environment. Many real-life problems require agents to use both natural language information and visual perception to achieve a goal, yet traditional RL environments focus solely on either vision-based tasks or text-based interactions. Our aim is to bridge this gap by creating a single environment for multimodal RL that combines both approaches. We extend the TextWorld cooking environment by adding visual clues throughout the environment, forcing the RL agent to use both text and visual features to predict natural language action commands for cooking a meal. Our environment includes various difficulty levels to emulate real-world scenarios. We also present a baseline multimodal agent that uses CNN-based feature extraction for visual hints and LSTMs for textual feature extraction. We anticipate that our visual-lingual environment will provide new problem settings for the RL community.",1
"Goal-directed Reinforcement Learning (RL) traditionally considers an agent interacting with an environment, prescribing a real-valued reward to an agent proportional to the completion of some goal. Goal-directed RL has seen large gains in sample efficiency, due to the ease of reusing or generating new experience by proposing goals. One approach,self-play, allows an agent to ""play"" against itself by alternatively setting and accomplishing goals, creating a learned curriculum through which an agent can learn to accomplish progressively more difficult goals. However, self-play has been limited to goal curriculum learning or learning progressively harder goals within a single environment. Recent work on robotic agents has shown that varying the environment during training, for example with domain randomization, leads to more robust transfer. As a result, we extend the self-play framework to jointly learn a goal and environment curriculum, leading to an approach that learns the most fruitful domain randomization strategy with self-play. Our method, Self-Supervised Active Domain Randomization(SS-ADR), generates a coupled goal-task curriculum, where agents learn through progressively more difficult tasks and environment variations. By encouraging the agent to try tasks that are just outside of its current capabilities, SS-ADR builds a domain randomization curriculum that enables state-of-the-art results on varioussim2real transfer tasks. Our results show that a curriculum of co-evolving the environment difficulty together with the difficulty of goals set in each environment provides practical benefits in the goal-directed tasks tested.",0
"The traditional approach of Goal-directed Reinforcement Learning (RL) involves an agent receiving a real-valued reward proportional to the completion of a goal while interacting with an environment. This method has shown impressive gains in sample efficiency, as goals can be reused or generated to create new experiences. Self-play is a technique that allows an agent to set and accomplish goals alternatively, creating a curriculum that enables the agent to achieve progressively harder objectives. However, this approach has been limited to learning progressively harder goals within a single environment. Recent research on robotic agents has shown that varying the environment during training leads to more robust transfer, and thus we extend the self-play framework to jointly learn a goal and environment curriculum. Our approach, Self-Supervised Active Domain Randomization (SS-ADR), generates a coupled goal-task curriculum, where agents learn through increasingly challenging tasks and environment variations. By pushing the agent to attempt tasks just beyond their current abilities, SS-ADR builds a domain randomization curriculum that produces state-of-the-art results on various sim2real transfer tasks. Our findings demonstrate that co-evolving the environment difficulty with the difficulty of goals set in each environment provides practical benefits in goal-directed tasks.",1
"Heterogeneous Information Networks (HINs), involving a diversity of node types and relation types, are pervasive in many real-world applications. Recently, increasing attention has been paid to heterogeneous graph representation learning (HGRL) which aims to embed rich structural and semantics information in HIN into low-dimensional node representations. To date, most HGRL models rely on manual customisation of meta paths to capture the semantics underlying the given HIN. However, the dependency on the handcrafted meta-paths requires rich domain knowledge which is extremely difficult to obtain for complex and semantic rich HINs. Moreover, strictly defined meta-paths will limit the HGRL's access to more comprehensive information in HINs. To fully unleash the power of HGRL, we present a Reinforcement Learning enhanced Heterogeneous Graph Neural Network (RL-HGNN), to design different meta-paths for the nodes in a HIN. Specifically, RL-HGNN models the meta-path design process as a Markov Decision Process and uses a policy network to adaptively design a meta-path for each node to learn its effective representations. The policy network is trained with deep reinforcement learning by exploiting the performance of the model on a downstream task. We further propose an extension, RL-HGNN++, to ameliorate the meta-path design procedure and accelerate the training process. Experimental results demonstrate the effectiveness of RL-HGNN, and reveals that it can identify meaningful meta-paths that would have been ignored by human knowledge.",0
"In numerous real-world applications, Heterogeneous Information Networks (HINs) are prevalent and comprise a variety of node and relation types. Heterogeneous graph representation learning (HGRL) has gained increasing attention as it aims to embed the rich structural and semantic information in HINs into low-dimensional node representations. However, current HGRL models mainly depend on manually customizing meta-paths to capture the HIN's underlying semantics, which is challenging for complex and semantic-rich HINs. Besides, strictly defined meta-paths limit HGRL's access to more comprehensive information in HINs. To address this, we propose a Reinforcement Learning enhanced Heterogeneous Graph Neural Network (RL-HGNN) that designs different meta-paths for each node in a HIN. RL-HGNN models the meta-path design process as a Markov Decision Process and uses a policy network to adaptively design a meta-path for each node. The policy network is trained using deep reinforcement learning by exploiting the model's performance on a downstream task. Additionally, we propose RL-HGNN++, an extension to improve the meta-path design procedure and accelerate training. Our experimental results demonstrate the effectiveness of RL-HGNN, which can identify meaningful meta-paths that may have been overlooked by human knowledge.",1
"Detailed routing is one of the most critical steps in analog circuit design. Complete routing has become increasingly more challenging in advanced node analog circuits, making advances in efficient automatic routers ever more necessary. In this work, we propose a machine learning driven method for solving the track-assignment detailed routing problem for advanced node analog circuits. Our approach adopts an attention-based reinforcement learning (RL) policy model. Our main insight and advancement over this RL model is the use of supervision as a way to leverage solutions generated by a conventional genetic algorithm (GA). For this, our approach minimizes the Kullback-Leibler divergence loss between the output from the RL policy model and a solution distribution obtained from the genetic solver. The key advantage of this approach is that the router can learn a policy in an offline setting with supervision, while improving the run-time performance nearly 100x over the genetic solver. Moreover, the quality of the solutions our approach produces matches well with those generated by GA. We show that especially for complex problems, our supervised RL method provides good quality solution similar to conventional attention-based RL without comprising run time performance. The ability to learn from example designs and train the router to get similar solutions with orders of magnitude run-time improvement can impact the design flow dramatically, potentially enabling increased design exploration and routability-driven placement.",0
"The detailed routing stage is crucial in analog circuit design, particularly in advanced node circuits where it has become more challenging. Therefore, efficient automatic routers are needed. In this study, we propose a machine learning approach to solve the track-assignment detailed routing problem in advanced node analog circuits. Our method uses an attention-based reinforcement learning (RL) policy model, with the addition of supervision to leverage solutions from a genetic algorithm (GA). We minimize the Kullback-Leibler divergence loss between the output from the RL policy model and the solution distribution obtained from the genetic solver. This approach allows the router to learn a policy offline with supervision, resulting in a 100x improvement in run-time performance compared to the genetic solver. Additionally, the quality of our solutions matches those generated by GA. Our supervised RL method provides similar quality solutions to attention-based RL without compromising run-time performance, particularly for complex problems. This capability to learn from example designs and train the router for similar solutions with significantly improved run-time can have a significant impact on the design flow, potentially enabling increased design exploration and routability-driven placement.",1
"We address the problem of credit assignment in reinforcement learning and explore fundamental questions regarding the way in which an agent can best use additional computation to propagate new information, by planning with internal models of the world to improve its predictions. Particularly, we work to understand the gains and peculiarities of planning employed as forethought via forward models or as hindsight operating with backward models. We establish the relative merits, limitations and complementary properties of both planning mechanisms in carefully constructed scenarios. Further, we investigate the best use of models in planning, primarily focusing on the selection of states in which predictions should be (re)-evaluated. Lastly, we discuss the issue of model estimation and highlight a spectrum of methods that stretch from explicit environment-dynamics predictors to more abstract planner-aware models.",0
"Our focus is on solving the credit assignment problem in reinforcement learning by delving into the optimal utilization of additional computation for propagating new information. We employ internal models of the world to enhance agent predictions through forward and backward planning mechanisms. Our aim is to identify the advantages, limitations, and complementary features of both planning techniques in controlled settings. Additionally, we investigate the selection of states for (re)-evaluation of predictions during planning and examine model estimation methods ranging from explicit environment-dynamics predictors to abstract planner-aware models.",1
"We present Revel, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that Revel enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration.",0
"Revel is a framework for reinforcement learning (RL) that employs a partially neural approach to ensure safe exploration in continuous state and action spaces. The biggest obstacle in ensuring safety is the computational infeasibility of repeatedly verifying neural networks within a learning loop. To overcome this, we utilize two policy classes: a general neurosymbolic class with approximate gradients and a more limited symbolic policy class that enables efficient verification. Our learning algorithm is a mirror descent over policies that lifts a safe symbolic policy into the neurosymbolic space, performs safe gradient updates, and returns the updated policy to the safe symbolic subset. This is done without explicit verification of neural networks. Empirical results demonstrate that Revel enforces safe exploration in scenarios where Constrained Policy Optimization fails and can discover policies that outperform those learned using previous approaches to verified exploration.",1
"Many processes, such as discrete event systems in engineering or population dynamics in biology, evolve in discrete space and continuous time. We consider the problem of optimal decision making in such discrete state and action space systems under partial observability. This places our work at the intersection of optimal filtering and optimal control. At the current state of research, a mathematical description for simultaneous decision making and filtering in continuous time with finite state and action spaces is still missing. In this paper, we give a mathematical description of a continuous-time partial observable Markov decision process (POMDP). By leveraging optimal filtering theory we derive a Hamilton-Jacobi-Bellman (HJB) type equation that characterizes the optimal solution. Using techniques from deep learning we approximately solve the resulting partial integro-differential equation. We present (i) an approach solving the decision problem offline by learning an approximation of the value function and (ii) an online algorithm which provides a solution in belief space using deep reinforcement learning. We show the applicability on a set of toy examples which pave the way for future methods providing solutions for high dimensional problems.",0
"The evolution of many processes, such as population dynamics in biology or discrete event systems in engineering, occurs in discrete space and continuous time. Our focus is on achieving optimal decision making in such systems that have partial observability in discrete state and action space, which requires the intersection of optimal control and optimal filtering. Unfortunately, there is currently no mathematical description available for simultaneous decision making and filtering in continuous time with finite state and action spaces. In this paper, we provide a mathematical description of a partial observable Markov decision process (POMDP) that operates in continuous time. By utilizing optimal filtering theory, we establish a Hamilton-Jacobi-Bellman (HJB) equation that defines the optimal solution. Using deep learning techniques, we solve the resulting partial integro-differential equation approximately. Specifically, we present two approaches: (i) an offline method that learns an approximation of the value function to solve the decision problem, and (ii) an online algorithm that uses deep reinforcement learning to provide a solution in belief space. We demonstrate the effectiveness of our approach on a set of toy examples that can pave the way for future solutions to high dimensional problems.",1
"Learning heuristics for combinatorial optimization problems through graph neural networks have recently shown promising results on some classic NP-hard problems. These are single-level optimization problems with only one player. Multilevel combinatorial optimization problems are their generalization, encompassing situations with multiple players taking decisions sequentially. By framing them in a multi-agent reinforcement learning setting, we devise a value-based method to learn to solve multilevel budgeted combinatorial problems involving two players in a zero-sum game over a graph. Our framework is based on a simple curriculum: if an agent knows how to estimate the value of instances with budgets up to $B$, then solving instances with budget $B+1$ can be done in polynomial time regardless of the direction of the optimization by checking the value of every possible afterstate. Thus, in a bottom-up approach, we generate datasets of heuristically solved instances with increasingly larger budgets to train our agent. We report results close to optimality on graphs up to $100$ nodes and a $185 \times$ speedup on average compared to the quickest exact solver known for the Multilevel Critical Node problem, a max-min-max trilevel problem that has been shown to be at least $\Sigma_2^p$-hard.",0
"Graph neural networks have recently shown promise in learning heuristics for combinatorial optimization problems, particularly for single-level optimization problems with only one player. However, multilevel combinatorial optimization problems, which involve multiple players making sequential decisions, are more complex and require a different approach. To tackle this, we propose a value-based method in a multi-agent reinforcement learning setting to solve multilevel budgeted combinatorial problems involving two players in a zero-sum game over a graph. Our framework has a simple curriculum, where an agent can solve instances with budget $B+1$ if it can estimate the value of instances with budgets up to $B$. We generate datasets of heuristically solved instances with increasingly larger budgets in a bottom-up approach to train our agent. Our results show close to optimal performance on graphs up to 100 nodes and a significant speedup compared to the fastest exact solver for the Multilevel Critical Node problem, a max-min-max trilevel problem known to be at least $\Sigma_2^p$-hard.",1
"Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.",0
"Learning directly from image observations using deep reinforcement learning (RL) algorithms is possible with the help of high-capacity deep networks. However, the use of high-dimensional observation spaces presents challenges in practice, as the policy must solve both representation learning and task learning. To address these two problems separately, we propose to explicitly learn latent representations that can accelerate reinforcement learning from images. Our approach, called the stochastic latent actor-critic (SLAC) algorithm, offers a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. By learning a compact latent representation and performing RL in the model's learned latent space, SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method. Our experimental evaluation shows that SLAC outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency on a range of difficult image-based control tasks. Our website provides access to the code and videos of our results.",1
"Autonomous deployment of unmanned aerial vehicles (UAVs) supporting next-generation communication networks requires efficient trajectory planning methods. We propose a new end-to-end reinforcement learning (RL) approach to UAV-enabled data collection from Internet of Things (IoT) devices in an urban environment. An autonomous drone is tasked with gathering data from distributed sensor nodes subject to limited flying time and obstacle avoidance. While previous approaches, learning and non-learning based, must perform expensive recomputations or relearn a behavior when important scenario parameters such as the number of sensors, sensor positions, or maximum flying time, change, we train a double deep Q-network (DDQN) with combined experience replay to learn a UAV control policy that generalizes over changing scenario parameters. By exploiting a multi-layer map of the environment fed through convolutional network layers to the agent, we show that our proposed network architecture enables the agent to make movement decisions for a variety of scenario parameters that balance the data collection goal with flight time efficiency and safety constraints. Considerable advantages in learning efficiency from using a map centered on the UAV's position over a non-centered map are also illustrated.",0
"Efficient trajectory planning methods are crucial for the autonomous deployment of unmanned aerial vehicles (UAVs) that support next-generation communication networks. In this study, we introduce a novel end-to-end reinforcement learning (RL) approach for UAV-enabled data collection from Internet of Things (IoT) devices in urban areas. The objective is for an autonomous drone to collect data from distributed sensor nodes while adhering to limited flying time and obstacle avoidance. Unlike previous approaches, which require expensive recomputations or relearning of behaviors when scenario parameters, such as the number of sensors, sensor positions, or maximum flying time, change, we use a double deep Q-network (DDQN) with combined experience replay to train a UAV control policy that generalizes over changing scenario parameters. Our proposed network architecture exploits a multi-layer map of the environment fed through convolutional network layers to the agent. This enables the agent to make movement decisions for various scenario parameters that balance the data collection goal with flight time efficiency and safety constraints. Additionally, we demonstrate the considerable advantages in learning efficiency from using a map centered on the UAV's position over a non-centered map.",1
"Model-based reinforcement learning (RL) has shown great potential in various control tasks in terms of both sample-efficiency and final performance. However, learning a generalizable dynamics model robust to changes in dynamics remains a challenge since the target transition dynamics follow a multi-modal distribution. In this paper, we present a new model-based RL algorithm, coined trajectory-wise multiple choice learning, that learns a multi-headed dynamics model for dynamics generalization. The main idea is updating the most accurate prediction head to specialize each head in certain environments with similar dynamics, i.e., clustering environments. Moreover, we incorporate context learning, which encodes dynamics-specific information from past experiences into the context latent vector, enabling the model to perform online adaptation to unseen environments. Finally, to utilize the specialized prediction heads more effectively, we propose an adaptive planning method, which selects the most accurate prediction head over a recent experience. Our method exhibits superior zero-shot generalization performance across a variety of control tasks, compared to state-of-the-art RL methods. Source code and videos are available at https://sites.google.com/view/trajectory-mcl.",0
"Various control tasks have demonstrated the potential of model-based reinforcement learning (RL) in terms of sample-efficiency and final performance. However, creating a generalizable dynamics model that can withstand changes in dynamics is still a challenge due to the multi-modal distribution of the target transition dynamics. This paper introduces a new model-based RL algorithm, named trajectory-wise multiple choice learning, which learns a multi-headed dynamics model for dynamics generalization. The algorithm updates the most accurate prediction head to specialize each head in similar environments with the same dynamics, i.e., clustering environments. Additionally, context learning is incorporated to encode dynamics-specific information from past experiences into the context latent vector, which allows the model to adapt to unseen environments in real-time. Finally, an adaptive planning method is proposed to effectively utilize the specialized prediction heads by selecting the most accurate prediction head over recent experiences. Compared to state-of-the-art RL methods, our method exhibits excellent zero-shot generalization performance across a range of control tasks. The source code and videos are available at https://sites.google.com/view/trajectory-mcl.",1
"We study the Safe Reinforcement Learning (SRL) problem using the Constrained Markov Decision Process (CMDP) formulation in which an agent aims to maximize the expected total reward subject to a safety constraint on the expected total value of a utility function. We focus on an episodic setting with the function approximation where the Markov transition kernels have a linear structure but do not impose any additional assumptions on the sampling model. Designing SRL algorithms with provable computational and statistical efficiency is particularly challenging under this setting because of the need to incorporate both the safety constraint and the function approximation into the fundamental exploitation/exploration tradeoff. To this end, we present an \underline{O}ptimistic \underline{P}rimal-\underline{D}ual Proximal Policy \underline{OP}timization (OPDOP) algorithm where the value function is estimated by combining the least-squares policy evaluation and an additional bonus term for safe exploration. We prove that the proposed algorithm achieves an $\tilde{O}(d H^{2.5}\sqrt{T})$ regret and an $\tilde{O}(d H^{2.5}\sqrt{T})$ constraint violation, where $d$ is the dimension of the feature mapping, $H$ is the horizon of each episode, and $T$ is the total number of steps. These bounds hold when the reward/utility functions are fixed but the feedback after each episode is bandit. Our bounds depend on the capacity of the state-action space only through the dimension of the feature mapping and thus our results hold even when the number of states goes to infinity. To the best of our knowledge, we provide the first provably efficient online policy optimization algorithm for CMDP with safe exploration in the function approximation setting.",0
"The Constrained Markov Decision Process (CMDP) approach is used to study the Safe Reinforcement Learning (SRL) problem, where the objective is to maximize the expected total reward, while adhering to a safety constraint on the expected total value of a utility function. In this study, we focus on an episodic scenario with function approximation, where Markov transition kernels have a linear structure and no additional assumptions are imposed on the sampling model. Developing efficient SRL algorithms that incorporate both safety constraints and function approximation into the exploitation/exploration tradeoff is challenging. In this regard, we propose an Optimistic Primal-Dual Proximal Policy Optimization (OPDOP) algorithm that estimates the value function using a combination of the least-squares policy evaluation and an additional bonus term for safe exploration. We demonstrate that our algorithm achieves an $\tilde{O}(d H^{2.5}\sqrt{T})$ regret and an $\tilde{O}(d H^{2.5}\sqrt{T})$ constraint violation, where $d$, $H$, and $T$ are the dimension of the feature mapping, the horizon of each episode, and the total number of steps, respectively. These bounds hold even when the number of states goes to infinity. Our study is the first to provide a provably efficient online policy optimization algorithm for CMDP with safe exploration in the function approximation setting.",1
"In many reinforcement learning (RL) problems, it takes some time until a taken action by the agent reaches its maximum effect on the environment and consequently the agent receives the reward corresponding to that action by a delay called action-effect delay. Such delays reduce the performance of the learning algorithm and increase the computational costs, as the reinforcement learning agent values the immediate rewards more than the future reward that is more related to the taken action. This paper addresses this issue by introducing an applicable enhanced Q-learning method in which at the beginning of the learning phase, the agent takes a single action and builds a function that reflects the environments response to that action, called the reflexive $\gamma$ - function. During the training phase, the agent utilizes the created reflexive $\gamma$- function to update the Q-values. We have applied the developed method to a structural control problem in which the goal of the agent is to reduce the vibrations of a building subjected to earthquake excitations with a specified delay. Seismic control problems are considered as a complex task in structural engineering because of the stochastic and unpredictable nature of earthquakes and the complex behavior of the structure. Three scenarios are presented to study the effects of zero, medium, and long action-effect delays and the performance of the Enhanced method is compared to the standard Q-learning method. Both RL methods use neural network to learn to estimate the state-action value function that is used to control the structure. The results show that the enhanced method significantly outperforms the performance of the original method in all cases, and also improves the stability of the algorithm in dealing with action-effect delays.",0
"Action-effect delays are a common issue in reinforcement learning (RL) problems, as it takes time for the agent's actions to have an effect on the environment and for the agent to receive the corresponding reward. This delay can decrease the performance of the learning algorithm and increase computational costs, as immediate rewards are valued over future rewards. To address this issue, an enhanced Q-learning method is proposed in this paper, where the agent takes a single action to build a reflexive $\gamma$-function during the learning phase. This function is used to update Q-values during the training phase, and the proposed method is applied to a structural control problem with specified delays. Seismic control problems are known for their complexity due to the stochastic and unpredictable nature of earthquakes and the structure's behavior. Three scenarios with varying action-effect delays are presented, and the enhanced method is compared to the standard Q-learning method using neural networks to estimate the state-action value function. The results show that the enhanced method outperforms the original method in all cases, improving stability in dealing with action-effect delays.",1
"In reinforcement learning, an agent attempts to learn high-performing behaviors through interacting with the environment, such behaviors are often quantified in the form of a reward function. However some aspects of behavior-such as ones which are deemed unsafe and to be avoided-are best captured through constraints. We propose a novel approach called First Order Constrained Optimization in Policy Space (FOCOPS) which maximizes an agent's overall reward while ensuring the agent satisfies a set of cost constraints. Using data generated from the current policy, FOCOPS first finds the optimal update policy by solving a constrained optimization problem in the nonparameterized policy space. FOCOPS then projects the update policy back into the parametric policy space. Our approach has an approximate upper bound for worst-case constraint violation throughout training and is first-order in nature therefore simple to implement. We provide empirical evidence that our simple approach achieves better performance on a set of constrained robotics locomotive tasks.",0
"The aim of reinforcement learning is for an agent to acquire high-performing behaviors by interacting with its surroundings, with these behaviors often being measured via a reward function. However, certain behaviors, such as those deemed unsafe, are better captured by constraints. Our proposed method, First Order Constrained Optimization in Policy Space (FOCOPS), optimizes an agent's reward while ensuring it meets a set of cost constraints. FOCOPS first determines the optimal update policy by solving a constrained optimization problem in the nonparameterized policy space using data from the current policy. The update policy is then projected back into the parametric policy space. Due to its first-order nature and approximate upper bound for worst-case constraint violation throughout training, our approach is simple to implement. Empirical evidence shows that our method achieves better performance for constrained robotics locomotion tasks.",1
"We propose a fully distributed actor-critic algorithm approximated by deep neural networks, named \textit{Diff-DAC}, with application to single-task and to average multitask reinforcement learning (MRL). Each agent has access to data from its local task only, but it aims to learn a policy that performs well on average for the whole set of tasks. During the learning process, agents communicate their value-policy parameters to their neighbors, diffusing the information across the network, so that they converge to a common policy, with no need for a central node. The method is scalable, since the computational and communication costs per agent grow with its number of neighbors. We derive Diff-DAC's from duality theory and provide novel insights into the standard actor-critic framework, showing that it is actually an instance of the dual ascent method that approximates the solution of a linear program. Experiments suggest that Diff-DAC can outperform the single previous distributed MRL approach (i.e., Dist-MTLPS) and even the centralized architecture.",0
"Our proposal, called Diff-DAC, is a decentralized algorithm that utilizes deep neural networks in actor-critic approximation. It can be applied to both single-task and average multitask reinforcement learning (MRL). Despite each agent having access to data from its local task only, the aim is to learn a policy that performs well on average across all tasks. During the learning process, agents communicate their value-policy parameters to their neighbors, allowing for the diffusion of information across the network. This leads to a convergence towards a common policy without the need for a central node. The approach is scalable as the computational and communication costs per agent increase with the number of neighbors. We derived Diff-DAC from duality theory and provide new insights into the standard actor-critic framework. It is shown that this framework is an instance of the dual ascent method that approximates the solution of a linear program. Our experiments indicate that Diff-DAC outperforms Dist-MTLPS, the previous distributed MRL approach, and even the centralized architecture.",1
"We present two elegant solutions for modeling continuous-time dynamics, in a novel model-based reinforcement learning (RL) framework for semi-Markov decision processes (SMDPs), using neural ordinary differential equations (ODEs). Our models accurately characterize continuous-time dynamics and enable us to develop high-performing policies using a small amount of data. We also develop a model-based approach for optimizing time schedules to reduce interaction rates with the environment while maintaining the near-optimal performance, which is not possible for model-free methods. We experimentally demonstrate the efficacy of our methods across various continuous-time domains.",0
"In this paper, we introduce two sophisticated approaches to represent continuous-time dynamics within a newly proposed model-based reinforcement learning (RL) scheme for semi-Markov decision processes (SMDPs), leveraging neural ordinary differential equations (ODEs). Our models effectively capture continuous-time dynamics and facilitate the construction of successful policies with minimal data. Additionally, we devise a model-based strategy for optimizing time schedules that reduces interaction rates with the environment, while preserving near-optimal performance, a feat not achievable by model-free techniques. Through experimentation in diverse continuous-time domains, we validate the effectiveness of our methods.",1
"Partial observability is a common challenge in many reinforcement learning applications, which requires an agent to maintain memory, infer latent states, and integrate this past information into exploration. This challenge leads to a number of computational and statistical hardness results for learning general Partially Observable Markov Decision Processes (POMDPs). This work shows that these hardness barriers do not preclude efficient reinforcement learning for rich and interesting subclasses of POMDPs. In particular, we present a sample-efficient algorithm, OOM-UCB, for episodic finite undercomplete POMDPs, where the number of observations is larger than the number of latent states and where exploration is essential for learning, thus distinguishing our results from prior works. OOM-UCB achieves an optimal sample complexity of $\tilde{\mathcal{O}}(1/\varepsilon^2)$ for finding an $\varepsilon$-optimal policy, along with being polynomial in all other relevant quantities. As an interesting special case, we also provide a computationally and statistically efficient algorithm for POMDPs with deterministic state transitions.",0
"Reinforcement learning faces the challenge of partial observability in many applications, necessitating an agent to retain memory, deduce hidden states, and incorporate prior information into exploration. This poses computational and statistical difficulties for learning general Partially Observable Markov Decision Processes (POMDPs), as evidenced by previous research. However, this study demonstrates that effective reinforcement learning is still possible for specific subclasses of POMDPs that are complex and intriguing. Specifically, we introduce OOM-UCB, a sample-efficient algorithm for episodic finite undercomplete POMDPs, where exploration is vital for learning and the number of observations exceeds the number of latent states. Our approach distinguishes itself from prior work in this area. OOM-UCB attains an optimal sample complexity of $\tilde{\mathcal{O}}(1/\varepsilon^2)$ for identifying an $\varepsilon$-optimal policy, and is polynomial in all other pertinent variables. Additionally, we offer a practical and statistically efficient algorithm for POMDPs with deterministic state transitions as a noteworthy special case.",1
"Local policy search is performed by most Deep Reinforcement Learning (D-RL) methods, which increases the risk of getting trapped in a local minimum. Furthermore, the availability of a simulation model is not fully exploited in D-RL even in simulation-based training, which potentially decreases efficiency. To better exploit simulation models in policy search, we propose to integrate a kinodynamic planner in the exploration strategy and to learn a control policy in an offline fashion from the generated environment interactions. We call the resulting model-based reinforcement learning method PPS (Planning for Policy Search). We compare PPS with state-of-the-art D-RL methods in typical RL settings including underactuated systems. The comparison shows that PPS, guided by the kinodynamic planner, collects data from a wider region of the state space. This generates training data that helps PPS discover better policies.",0
"Most Deep Reinforcement Learning (D-RL) methods rely on local policy search, which can lead to being trapped in a local minimum. Additionally, the potential efficiency of simulation-based training is not fully utilized in D-RL. To address this, we propose integrating a kinodynamic planner into the exploration strategy and learning a control policy offline from the resulting environment interactions. Our resulting model-based reinforcement learning method, PPS (Planning for Policy Search), is compared with state-of-the-art D-RL methods in typical RL settings, including underactuated systems. PPS, guided by the kinodynamic planner, collects data from a wider range of the state space, resulting in better policies discovered through the training data.",1
"In reinforcement learning, Return, which is the weighted accumulated future rewards, and Value, which is the expected return, serve as the objective that guides the learning of the policy. In classic RL, return is defined as the exponentially discounted sum of future rewards. One key insight is that there could be many feasible ways to define the form of the return function (and thus the value), from which the same optimal policy can be derived, yet these different forms might render dramatically different speeds of learning this policy. In this paper, we research how to modify the form of the return function to enhance the learning towards the optimal policy. We propose to use a general mathematical form for return function, and employ meta-learning to learn the optimal return function in an end-to-end manner. We test our methods on a specially designed maze environment and several Atari games, and our experimental results clearly indicate the advantages of automatically learning optimal return functions in reinforcement learning.",0
"Reinforcement learning employs Return and Value as guiding objectives for policy learning. Return, defined in classic RL as the exponentially discounted sum of future rewards, and Value, the expected return, are both crucial components in policy learning. However, there are various ways to define the form of the return function, resulting in different speeds of learning towards the optimal policy. This study aims to enhance the learning process by modifying the return function through a general mathematical form and meta-learning to learn the optimal return function in an end-to-end manner. Tests conducted on a maze environment and several Atari games have demonstrated the advantages of automatically learning optimal return functions in reinforcement learning.",1
"Long-term temporal credit assignment is an important challenge in deep reinforcement learning (RL). It refers to the ability of the agent to attribute actions to consequences that may occur after a long time interval. Existing policy-gradient and Q-learning algorithms typically rely on dense environmental rewards that provide rich short-term supervision and help with credit assignment. However, they struggle to solve tasks with delays between an action and the corresponding rewarding feedback. To make credit assignment easier, recent works have proposed algorithms to learn dense ""guidance"" rewards that could be used in place of the sparse or delayed environmental rewards. This paper is in the same vein -- starting with a surrogate RL objective that involves smoothing in the trajectory-space, we arrive at a new algorithm for learning guidance rewards. We show that the guidance rewards have an intuitive interpretation, and can be obtained without training any additional neural networks. Due to the ease of integration, we use the guidance rewards in a few popular algorithms (Q-learning, Actor-Critic, Distributional-RL) and present results in single-agent and multi-agent tasks that elucidate the benefit of our approach when the environmental rewards are sparse or delayed.",0
"Deep reinforcement learning (RL) faces the challenge of long-term temporal credit assignment, which involves linking an agent's actions to their eventual consequences, even if they occur long after the action is taken. While current policy-gradient and Q-learning algorithms rely on dense rewards that offer short-term supervision, they struggle when there are delays between actions and feedback. To address this issue, some recent works have proposed learning dense ""guidance"" rewards that can be used instead of sparse or delayed environmental rewards. This paper builds on these ideas by introducing a new algorithm that learns guidance rewards using a surrogate RL objective that involves trajectory-space smoothing. Unlike other methods, our algorithm does not require training any additional neural networks, and the guidance rewards have an intuitive interpretation. We demonstrate the effectiveness of our approach by integrating it into several popular algorithms, including Q-learning, Actor-Critic, and Distributional-RL, and showing improved performance in single-agent and multi-agent tasks where environmental rewards are sparse or delayed.",1
"When expert supervision is available, practitioners often use imitation learning with varying degrees of success. We show that when an expert has access to privileged information that is unavailable to the student, this information is marginalized in the student policy during imitation learning resulting in an ""imitation gap"" and, potentially, poor results. Prior work bridges this gap via a progression from imitation learning to reinforcement learning. While often successful, gradual progression fails for tasks that require frequent switches between exploration and memorization skills. To better address these tasks and alleviate the imitation gap we propose 'Adaptive Insubordination' (ADVISOR), which dynamically weights imitation and reward-based reinforcement learning losses during training, enabling switching between imitation and exploration. On a suite of challenging didactic and MiniGrid tasks, we show that ADVISOR outperforms pure imitation, pure reinforcement learning, as well as their sequential and parallel combinations.",0
"Imitation learning is commonly used by practitioners when expert supervision is available, but its effectiveness varies. However, our study reveals that when an expert has access to privileged information that the student does not, this information is not fully utilized in the imitation learning process, resulting in an ""imitation gap"" and potentially subpar outcomes. Previous attempts to bridge this gap by transitioning from imitation learning to reinforcement learning have been successful, but not for tasks that require frequent switching between exploration and memorization. To address this limitation and the imitation gap, we introduce 'Adaptive Insubordination' (ADVISOR), which adjusts the weights of imitation and reward-based reinforcement learning losses during training to allow for seamless switching between exploration and imitation. Our experiments on difficult didactic and MiniGrid tasks demonstrate that ADVISOR outperforms pure imitation, pure reinforcement learning, as well as their sequential and parallel combinations.",1
"Preference-based Reinforcement Learning (PbRL) replaces reward values in traditional reinforcement learning by preferences to better elicit human opinion on the target objective, especially when numerical reward values are hard to design or interpret. Despite promising results in applications, the theoretical understanding of PbRL is still in its infancy. In this paper, we present the first finite-time analysis for general PbRL problems. We first show that a unique optimal policy may not exist if preferences over trajectories are deterministic for PbRL. If preferences are stochastic, and the preference probability relates to the hidden reward values, we present algorithms for PbRL, both with and without a simulator, that are able to identify the best policy up to accuracy $\varepsilon$ with high probability. Our method explores the state space by navigating to under-explored states, and solves PbRL using a combination of dueling bandits and policy search. Experiments show the efficacy of our method when it is applied to real-world problems.",0
"Traditional reinforcement learning uses numerical reward values, but Preference-based Reinforcement Learning (PbRL) uses preferences to better understand human opinion on the target objective when designing or interpreting numerical reward values is difficult. Although PbRL has shown promise in applications, its theoretical understanding is still in its early stages. This paper presents the first finite-time analysis for general PbRL problems. The study reveals that a unique optimal policy may not exist if preferences over trajectories are deterministic for PbRL. However, if preferences are stochastic and the preference probability relates to the hidden reward values, we present algorithms for PbRL, both with and without a simulator, that can identify the best policy up to accuracy $\varepsilon$ with high probability. Our method explores the state space by navigating to under-explored states and solves PbRL through a combination of dueling bandits and policy search. Experiments demonstrate the effectiveness of our method when applied to real-world problems.",1
"We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple datasets collected from different tasks, we train a multi-task policy to perform well in unseen tasks sampled from the same distribution. The task identities of the unseen tasks are not provided. To perform well, the policy must infer the task identity from collected transitions by modelling its dependency on states, actions and rewards. Because the different datasets may have state-action distributions with large divergence, the task inference module can learn to ignore the rewards and spuriously correlate $\textit{only}$ state-action pairs to the task identity, leading to poor test time performance. To robustify task inference, we propose a novel application of the triplet loss. To mine hard negative examples, we relabel the transitions from the training tasks by approximating their reward functions. When we allow further training on the unseen tasks, using the trained policy as an initialization leads to significantly faster convergence compared to randomly initialized policies (up to $80\%$ improvement and across 5 different Mujoco task distributions). We name our method $\textbf{MBML}$ ($\textbf{M}\text{ulti-task}$ $\textbf{B}\text{atch}$ RL with $\textbf{M}\text{etric}$ $\textbf{L}\text{earning}$).",0
"Our focus is on solving the Multi-task Batch Reinforcement Learning problem, in which we aim to train a multi-task policy to perform well on unseen tasks from various datasets while not having access to their identities. To achieve this, the policy must infer the task identity by modeling the dependencies between states, actions, and rewards. However, since the state-action distributions of the different datasets can be significantly different, the policy may learn to associate the task identity with only state-action pairs, leading to poor test performance. To enhance task inference, we propose a novel approach using the triplet loss and relabeling the transitions from the training tasks. By approximating the reward functions, we can mine hard negative examples. Additionally, using the trained policy as an initialization for further training on the unseen tasks leads to significantly faster convergence compared to randomly initialized policies, with up to 80% improvement across five different Mujoco task distributions. Our method is called MBML, which stands for Multi-task Batch RL with Metric Learning.",1
"We study the reward-free reinforcement learning framework, which is particularly suitable for batch reinforcement learning and scenarios where one needs policies for multiple reward functions. This framework has two phases. In the exploration phase, the agent collects trajectories by interacting with the environment without using any reward signal. In the planning phase, the agent needs to return a near-optimal policy for arbitrary reward functions. We give a new efficient algorithm, \textbf{S}taged \textbf{S}ampling + \textbf{T}runcated \textbf{P}lanning (\algoname), which interacts with the environment at most $O\left( \frac{S^2A}{\epsilon^2}\text{poly}\log\left(\frac{SAH}{\epsilon}\right) \right)$ episodes in the exploration phase, and guarantees to output a near-optimal policy for arbitrary reward functions in the planning phase. Here, $S$ is the size of state space, $A$ is the size of action space, $H$ is the planning horizon, and $\epsilon$ is the target accuracy relative to the total reward. Notably, our sample complexity scales only \emph{logarithmically} with $H$, in contrast to all existing results which scale \emph{polynomially} with $H$. Furthermore, this bound matches the minimax lower bound $\Omega\left(\frac{S^2A}{\epsilon^2}\right)$ up to logarithmic factors.   Our results rely on three new techniques : 1) A new sufficient condition for the dataset to plan for an $\epsilon$-suboptimal policy; 2) A new way to plan efficiently under the proposed condition using soft-truncated planning; 3) Constructing extended MDP to maximize the truncated accumulative rewards efficiently.",0
"The focus of our study is on the reward-free reinforcement learning framework, which is well-suited for batch reinforcement learning and situations where multiple reward functions are required. The framework consists of two phases: exploration and planning. During the exploration phase, the agent gathers trajectories by interacting with the environment without relying on any reward signal. In the planning phase, the agent must produce a policy that is near-optimal for any given reward function. We present a highly efficient algorithm, named \algoname, that utilizes Staged Sampling and Truncated Planning to achieve this objective. In the exploration phase, our algorithm interacts with the environment for a maximum of $O\left( \frac{S^2A}{\epsilon^2}\text{poly}\log\left(\frac{SAH}{\epsilon}\right) \right)$ episodes. In the planning phase, it guarantees a near-optimal policy for arbitrary reward functions. Our sample complexity scales logarithmically with the planning horizon $H$, which is in contrast to all previous results that scale polynomially with $H$. Moreover, our bound matches the minimax lower bound $\Omega\left(\frac{S^2A}{\epsilon^2}\right)$ up to logarithmic factors. Our results are based on three new techniques: a new sufficient condition for planning an $\epsilon$-suboptimal policy, an innovative approach to planning efficiently under the proposed condition using soft-truncated planning, and the construction of an extended MDP to maximize the truncated accumulative rewards efficiently.",1
"We introduce the technique of adaptive discretization to design an efficient model-based episodic reinforcement learning algorithm in large (potentially continuous) state-action spaces. Our algorithm is based on optimistic one-step value iteration extended to maintain an adaptive discretization of the space. From a theoretical perspective we provide worst-case regret bounds for our algorithm which are competitive compared to the state-of-the-art model-based algorithms. Moreover, our bounds are obtained via a modular proof technique which can potentially extend to incorporate additional structure on the problem.   From an implementation standpoint, our algorithm has much lower storage and computational requirements due to maintaining a more efficient partition of the state and action spaces. We illustrate this via experiments on several canonical control problems, which shows that our algorithm empirically performs significantly better than fixed discretization in terms of both faster convergence and lower memory usage. Interestingly, we observe empirically that while fixed-discretization model-based algorithms vastly outperform their model-free counterparts, the two achieve comparable performance with adaptive discretization.",0
"We present a new strategy called adaptive discretization that can be used to develop a model-based episodic reinforcement learning algorithm that is efficient in large state-action spaces, including continuous ones. Our algorithm is based on optimistic one-step value iteration, but with the added feature of adapting the discretization of the space. We provide theoretical worst-case regret bounds for our algorithm, which are competitive with other state-of-the-art model-based algorithms. Our proof technique is modular, meaning that it can be extended to incorporate additional problem structure.   From an implementation perspective, our algorithm requires less storage and fewer computations because it maintains a more efficient partition of the state and action spaces. We demonstrate this through experiments on various control problems, which show that our algorithm has faster convergence and lower memory usage than using a fixed discretization. Interestingly, we find that while fixed-discretization model-based algorithms are much better than their model-free counterparts, our adaptive discretization algorithm performs comparably.",1
"Graph neural networks (GNNs) have been attracting increasing popularity due to their simplicity and effectiveness in a variety of fields. However, a large number of labeled data is generally required to train these networks, which could be very expensive to obtain in some domains. In this paper, we study active learning for GNNs, i.e., how to efficiently label the nodes on a graph to reduce the annotation cost of training GNNs. We formulate the problem as a sequential decision process on graphs and train a GNN-based policy network with reinforcement learning to learn the optimal query strategy. By jointly training on several source graphs with full labels, we learn a transferable active learning policy which can directly generalize to unlabeled target graphs. Experimental results on multiple datasets from different domains prove the effectiveness of the learned policy in promoting active learning performance in both settings of transferring between graphs in the same domain and across different domains.",0
"The popularity of Graph neural networks (GNNs) has increased due to their effectiveness and simplicity in various fields. However, obtaining a significant number of labeled data to train the networks is usually expensive in certain domains. Therefore, this paper examines active learning for GNNs, which aims to decrease the annotation cost of training GNNs by efficiently labeling graph nodes. We approach the issue as a sequential decision process on graphs and use reinforcement learning to train a GNN-based policy network to learn the best query strategy. By jointly training on numerous source graphs with full labels, we develop a transferable active learning policy that can generalize to unlabeled target graphs. The efficacy of the learned policy is demonstrated in various settings, including transferring between graphs in the same domain and across different domains, through experimental results on multiple datasets from different domains.",1
"Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.",0
"The Priority Dispatching Rule (PDR) is a commonly used method for solving Job-shop scheduling problems (JSSP) in real-life situations. However, creating effective PDRs can be a time-consuming and specialized process that often yields limited results. This research proposes using an end-to-end deep reinforcement learning agent to automatically learn PDRs. By using a Graph Neural Network based approach to embed the states encountered during problem-solving, the resulting policy network is capable of generalizing on large-scale instances. The experiments conducted show that the agent can learn high-quality PDRs from scratch using basic raw features and performs strongly against existing PDRs. Additionally, the learned policies demonstrate good performance on large-scale instances that were not included in the training process.",1
"Despite recent advances, goal-directed generation of structured discrete data remains challenging. For problems such as program synthesis (generating source code) and materials design (generating molecules), finding examples which satisfy desired constraints or exhibit desired properties is difficult. In practice, expensive heuristic search or reinforcement learning algorithms are often employed. In this paper we investigate the use of conditional generative models which directly attack this inverse problem, by modeling the distribution of discrete structures given properties of interest. Unfortunately, maximum likelihood training of such models often fails with the samples from the generative model inadequately respecting the input properties. To address this, we introduce a novel approach to directly optimize a reinforcement learning objective, maximizing an expected reward. We avoid high-variance score-function estimators that would otherwise be required by sampling from an approximation to the normalized rewards, allowing simple Monte Carlo estimation of model gradients. We test our methodology on two tasks: generating molecules with user-defined properties and identifying short python expressions which evaluate to a given target value. In both cases, we find improvements over maximum likelihood estimation and other baselines.",0
"Despite recent progress, generating structured discrete data with specific properties remains a challenge, particularly in fields like program synthesis and materials design. Finding examples that meet desired constraints or exhibit desired properties can be difficult, often requiring expensive heuristic search or reinforcement learning algorithms. In this paper, we explore the use of conditional generative models to tackle this inverse problem by modeling the distribution of discrete structures based on desired properties. However, training such models using maximum likelihood often results in inadequate samples that fail to respect input properties. To overcome this issue, we propose a novel approach that directly optimizes a reinforcement learning objective to maximize expected reward. By avoiding high-variance score-function estimators and sampling from an approximation to normalized rewards, we can estimate model gradients using simple Monte Carlo methods. We apply our methodology to two tasks: generating molecules with user-defined properties and identifying short python expressions that evaluate to a given target value. In both cases, our approach outperformed maximum likelihood estimation and other baseline methods.",1
"Curriculum reinforcement learning (CRL) improves the learning speed and stability of an agent by exposing it to a tailored series of tasks throughout learning. Despite empirical successes, an open question in CRL is how to automatically generate a curriculum for a given reinforcement learning (RL) agent, avoiding manual design. In this paper, we propose an answer by interpreting the curriculum generation as an inference problem, where distributions over tasks are progressively learned to approach the target task. This approach leads to an automatic curriculum generation, whose pace is controlled by the agent, with solid theoretical motivation and easily integrated with deep RL algorithms. In the conducted experiments, the curricula generated with the proposed algorithm significantly improve learning performance across several environments and deep RL algorithms, matching or outperforming state-of-the-art existing CRL algorithms.",0
"The learning speed and stability of an agent can be enhanced by Curriculum reinforcement learning (CRL), which entails exposing the agent to a personalized sequence of tasks during learning. Although CRL has been successful, generating a curriculum for a specific reinforcement learning (RL) agent automatically remains an open issue to avoid manual design. This article proposes a solution by treating the curriculum generation as an inference problem, where distributions over tasks progressively learn to approach the target task. This approach results in an automatic curriculum generation that the agent controls, with strong theoretical backing and easily integrated with deep RL algorithms. The curricula generated using this approach have significantly improved learning performance in various environments and deep RL algorithms, matching or surpassing state-of-the-art CRL algorithms in the conducted experiments.",1
"Direct optimization is an appealing framework that replaces integration with optimization of a random objective for approximating gradients in models with discrete random variables. A$^\star$ sampling is a framework for optimizing such random objectives over large spaces. We show how to combine these techniques to yield a reinforcement learning algorithm that approximates a policy gradient by finding trajectories that optimize a random objective. We call the resulting algorithms ""direct policy gradient"" (DirPG) algorithms. A main benefit of DirPG algorithms is that they allow the insertion of domain knowledge in the form of upper bounds on return-to-go at training time, like is used in heuristic search, while still directly computing a policy gradient. We further analyze their properties, showing there are cases where DirPG has an exponentially larger probability of sampling informative gradients compared to REINFORCE. We also show that there is a built-in variance reduction technique and that a parameter that was previously viewed as a numerical approximation can be interpreted as controlling risk sensitivity. Empirically, we evaluate the effect of key degrees of freedom and show that the algorithm performs well in illustrative domains compared to baselines.",0
"The direct optimization framework is an attractive alternative to integration for approximating gradients in models with discrete random variables. A$^\star$ sampling is a framework that optimizes random objectives over large spaces. By combining these techniques, we propose a reinforcement learning algorithm called ""direct policy gradient"" (DirPG) algorithms that approximates a policy gradient by optimizing a random objective for finding trajectories. The main advantage of DirPG algorithms is the ability to incorporate domain knowledge in the form of upper bounds on return-to-go at training time, similar to heuristic search, while still directly computing a policy gradient. Additionally, we analyze their properties and find that DirPG has a higher probability of sampling informative gradients compared to REINFORCE. We also discover a built-in variance reduction technique and a parameter that can control risk sensitivity. We evaluate the algorithm's performance in various domains and compare it to baselines, demonstrating its effectiveness.",1
"Sample efficiency has been one of the major challenges for deep reinforcement learning. Recently, model-based reinforcement learning has been proposed to address this challenge by performing planning on imaginary trajectories with a learned world model. However, world model learning may suffer from overfitting to training trajectories, and thus model-based value estimation and policy search will be pone to be sucked in an inferior local policy. In this paper, we propose a novel model-based reinforcement learning algorithm, called BrIdging Reality and Dream (BIRD). It maximizes the mutual information between imaginary and real trajectories so that the policy improvement learned from imaginary trajectories can be easily generalized to real trajectories. We demonstrate that our approach improves sample efficiency of model-based planning, and achieves state-of-the-art performance on challenging visual control benchmarks.",0
"Deep reinforcement learning has faced a significant obstacle in achieving sample efficiency. A solution to this problem has been proposed through model-based reinforcement learning, which involves planning on hypothetical trajectories using a learned world model. However, training the world model can lead to overfitting on specific training trajectories, which can result in suboptimal local policies. To address this, our paper introduces a new model-based reinforcement learning algorithm, called BrIdging Reality and Dream (BIRD). BIRD focuses on maximizing the mutual information between real and imaginary trajectories to promote generalization of policy improvement from imaginary trajectories to real ones. Our results show that BIRD improves sample efficiency of model-based planning and achieves superior performance on challenging visual control benchmarks.",1
"Value-function-based methods have long played an important role in reinforcement learning. However, finding the best next action given a value function of arbitrary complexity is nontrivial when the action space is too large for enumeration. We develop a framework for value-function-based deep reinforcement learning with a combinatorial action space, in which the action selection problem is explicitly formulated as a mixed-integer optimization problem. As a motivating example, we present an application of this framework to the capacitated vehicle routing problem (CVRP), a combinatorial optimization problem in which a set of locations must be covered by a single vehicle with limited capacity. On each instance, we model an action as the construction of a single route, and consider a deterministic policy which is improved through a simple policy iteration algorithm. Our approach is competitive with other reinforcement learning methods and achieves an average gap of 1.7% with state-of-the-art OR methods on standard library instances of medium size.",0
"For a while now, value-function-based methods have been significant in reinforcement learning. However, when the action space is too vast for enumeration, determining the best next action using an intricate value function is a challenging task. To this end, we have developed a framework for deep reinforcement learning based on value functions, which deals with combinatorial action spaces. Our framework explicitly formulates the action selection issue as a mixed-integer optimization problem. We present an example of how this framework applies to the capacitated vehicle routing problem (CVRP), which is a combinatorial optimization problem that entails covering a set of locations using a single vehicle of limited capacity. In our approach, we model an action as the creation of a single route and utilize a deterministic policy that we enhance through a simple policy iteration algorithm. Our method is competitive with other reinforcement learning methods and has an average gap of 1.7% with state-of-the-art OR methods on standard library instances of medium size.",1
"Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One approach for leveraging prior knowledge is to transfer skills learned on prior tasks to the new task. However, as the amount of prior experience increases, the number of transferable skills grows too, making it challenging to explore the full set of available skills during downstream learning. Yet, intuitively, not all skills should be explored with equal probability; for example information about the current state can hint which skills are promising to explore. In this work, we propose to implement this intuition by learning a prior over skills. We propose a deep latent variable model that jointly learns an embedding space of skills and the skill prior from offline agent experience. We then extend common maximum-entropy RL approaches to use skill priors to guide downstream learning. We validate our approach, SPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks and show that learned skill priors are essential for effective skill transfer from rich datasets. Videos and code are available at https://clvrai.com/spirl.",0
"Most modern reinforcement learning (RL) approaches start learning a new task from scratch, even though intelligent agents rely heavily on prior experience. To leverage prior knowledge, one approach is to transfer skills learned from prior tasks to the new task. However, exploring the full set of available skills during downstream learning becomes challenging as the amount of prior experience increases and the number of transferable skills grows. To address this, we propose learning a prior over skills that considers the current state to guide skill exploration. Our approach, SPiRL (Skill-Prior RL), is a deep latent variable model that learns a skill embedding space and skill prior from offline agent experience. We extend common maximum-entropy RL approaches to use skill priors and validate our approach on complex navigation and robotic manipulation tasks, demonstrating the importance of learned skill priors for effective skill transfer from rich datasets. Videos and code are available at https://clvrai.com/spirl.",1
"Offline reinforcement learning seeks to utilize offline (observational) data to guide the learning of (causal) sequential decision making strategies. The hope is that offline reinforcement learning coupled with function approximation methods (to deal with the curse of dimensionality) can provide a means to help alleviate the excessive sample complexity burden in modern sequential decision making problems. However, the extent to which this broader approach can be effective is not well understood, where the literature largely consists of sufficient conditions.   This work focuses on the basic question of what are necessary representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. Perhaps surprisingly, our main result shows that even if: i) we have realizability in that the true value function of \emph{every} policy is linear in a given set of features and 2) our off-policy data has good coverage over all features (under a strong spectral condition), then any algorithm still (information-theoretically) requires a number of offline samples that is exponential in the problem horizon in order to non-trivially estimate the value of \emph{any} given policy. Our results highlight that sample-efficient offline policy evaluation is simply not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift (where the offline data distribution is close to the distribution of the policy to be evaluated) or significantly stronger representational conditions (beyond realizability).",0
"The goal of offline reinforcement learning is to use observational data to guide the development of decision-making strategies. This approach, when coupled with function approximation techniques, aims to reduce the high sample complexity associated with modern sequential decision-making problems. However, the effectiveness of this method is not well understood, and the literature mainly consists of sufficient conditions. This study focuses on identifying the necessary representational and distributional conditions for sample-efficient offline reinforcement learning. Despite having a linear value function and good coverage of off-policy data, our findings reveal that any algorithm still requires an exponential number of offline samples to estimate the value of any policy significantly. Our study highlights that effective offline policy assessment is only possible under stronger conditions, such as low distribution shift or more robust representational requirements beyond realizability.",1
"Imitation learning trains a policy by mimicking expert demonstrations. Various imitation methods were proposed and empirically evaluated, meanwhile, their theoretical understanding needs further studies. In this paper, we firstly analyze the value gap between the expert policy and imitated policies by two imitation methods, behavioral cloning and generative adversarial imitation. The results support that generative adversarial imitation can reduce the compounding errors compared to behavioral cloning, and thus has a better sample complexity. Noticed that by considering the environment transition model as a dual agent, imitation learning can also be used to learn the environment model. Therefore, based on the bounds of imitating policies, we further analyze the performance of imitating environments. The results show that environment models can be more effectively imitated by generative adversarial imitation than behavioral cloning, suggesting a novel application of adversarial imitation for model-based reinforcement learning. We hope these results could inspire future advances in imitation learning and model-based reinforcement learning.",0
"Imitation learning involves training a policy to imitate expert demonstrations. While various methods have been proposed and evaluated, further studies are needed to understand their theoretical basis. This paper analyzes the difference in value between expert policies and imitated policies using two imitation methods: behavioral cloning and generative adversarial imitation. The results show that generative adversarial imitation has better sample complexity and reduces compounding errors compared to behavioral cloning. In addition, by treating the environment transition model as a dual agent, imitation learning can also be used to learn the environment model. Therefore, the paper further analyzes the performance of imitating environments, finding that generative adversarial imitation is more effective than behavioral cloning. This suggests a new application of adversarial imitation in model-based reinforcement learning and may inspire future advances in imitation learning and model-based reinforcement learning.",1
"Prioritized Experience Replay (PER) is a deep reinforcement learning technique in which agents learn from transitions sampled with non-uniform probability proportionate to their temporal-difference error. We show that any loss function evaluated with non-uniformly sampled data can be transformed into another uniformly sampled loss function with the same expected gradient. Surprisingly, we find in some environments PER can be replaced entirely by this new loss function without impact to empirical performance. Furthermore, this relationship suggests a new branch of improvements to PER by correcting its uniformly sampled loss function equivalent. We demonstrate the effectiveness of our proposed modifications to PER and the equivalent loss function in several MuJoCo and Atari environments.",0
"The technique of Prioritized Experience Replay (PER) in deep reinforcement learning involves agents learning from transitions sampled based on their temporal-difference error, with non-uniform probability. Our research has shown that any loss function evaluated using non-uniformly sampled data can be transformed into an equivalent uniformly sampled loss function with the same expected gradient. Interestingly, we have found that in certain environments, the new loss function can replace PER without affecting its empirical performance. This relationship has opened up new avenues for improving PER by correcting its uniformly sampled loss function equivalent. Our proposed modifications to PER and the equivalent loss function have been tested in various MuJoCo and Atari environments, demonstrating their effectiveness.",1
"QMIX is a popular $Q$-learning algorithm for cooperative MARL in the centralised training and decentralised execution paradigm. In order to enable easy decentralisation, QMIX restricts the joint action $Q$-values it can represent to be a monotonic mixing of each agent's utilities. However, this restriction prevents it from representing value functions in which an agent's ordering over its actions can depend on other agents' actions. To analyse this representational limitation, we first formalise the objective QMIX optimises, which allows us to view QMIX as an operator that first computes the $Q$-learning targets and then projects them into the space representable by QMIX. This projection returns a representable $Q$-value that minimises the unweighted squared error across all joint actions. We show in particular that this projection can fail to recover the optimal policy even with access to $Q^*$, which primarily stems from the equal weighting placed on each joint action. We rectify this by introducing a weighting into the projection, in order to place more importance on the better joint actions. We propose two weighting schemes and prove that they recover the correct maximal action for any joint action $Q$-values, and therefore for $Q^*$ as well. Based on our analysis and results in the tabular setting, we introduce two scalable versions of our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW) QMIX and demonstrate improved performance on both predator-prey and challenging multi-agent StarCraft benchmark tasks.",0
"The QMIX algorithm is widely used for cooperative MARL in the centralised training and decentralised execution approach. It limits the joint action $Q$-values it can represent to a monotonic mixing of each agent's utilities to facilitate decentralisation. However, this restriction prevents it from representing value functions in which an agent's ordering over its actions can depend on other agents' actions. To investigate this issue, we formalised QMIX's optimisation objective and viewed it as an operator that computes $Q$-learning targets and then projects them into QMIX's representable space. This projection returns a $Q$-value that minimises the unweighted squared error across all joint actions but can fail to recover the optimal policy, primarily due to the equal weighting of each joint action. To address this, we introduced a weighting into the projection to give more importance to better joint actions. We proposed two weighting schemes and proved their ability to recover the correct maximal action for any joint action $Q$-values, including $Q^*$. We then developed two scalable versions of our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW) QMIX, and demonstrated their improved performance on predator-prey and challenging multi-agent StarCraft benchmark tasks.",1
"We present a multi-agent actor-critic method that aims to implicitly address the credit assignment problem under fully cooperative settings. Our key motivation is that credit assignment among agents may not require an explicit formulation as long as (1) the policy gradients derived from a centralized critic carry sufficient information for the decentralized agents to maximize their joint action value through optimal cooperation and (2) a sustained level of exploration is enforced throughout training. Under the centralized training with decentralized execution (CTDE) paradigm, we achieve the former by formulating the centralized critic as a hypernetwork such that a latent state representation is integrated into the policy gradients through its multiplicative association with the stochastic policies; to achieve the latter, we derive a simple technique called adaptive entropy regularization where magnitudes of the entropy gradients are dynamically rescaled based on the current policy stochasticity to encourage consistent levels of exploration. Our algorithm, referred to as LICA, is evaluated on several benchmarks including the multi-agent particle environments and a set of challenging StarCraft II micromanagement tasks, and we show that LICA significantly outperforms previous methods.",0
"Our approach is a multi-agent actor-critic method designed to tackle the credit assignment problem in fully cooperative settings. We believe that explicit formulation for credit assignment among agents may not be necessary if two conditions are met. Firstly, policy gradients obtained from a centralized critic must provide enough information for decentralized agents to maximize their joint action value through optimal cooperation. Secondly, there must be a sustained level of exploration enforced during training. To achieve the former, we use a centralized critic in the form of a hypernetwork that integrates a latent state representation into policy gradients through its multiplicative association with stochastic policies. To achieve the latter, we introduce adaptive entropy regularization, which rescales entropy gradients based on current policy stochasticity to encourage consistent levels of exploration. Our algorithm, LICA, is evaluated on various benchmarks, including multi-agent particle environments and challenging StarCraft II micromanagement tasks, and we demonstrate superior performance compared to previous methods.",1
"The future of mobility-as-a-Service (Maas)should embrace an integrated system of ride-hailing, street-hailing and ride-sharing with optimised intelligent vehicle routing in response to a real-time, stochastic demand pattern. We aim to optimise routing policies for a large fleet of vehicles for street-hailing services, given a stochastic demand pattern in small to medium-sized road networks. A model-based dispatch algorithm, a high performance model-free reinforcement learning based algorithm and a novel hybrid algorithm combining the benefits of both the top-down approach and the model-free reinforcement learning have been proposed to route the \emph{vacant} vehicles. We design our reinforcement learning based routing algorithm using proximal policy optimisation and combined intrinsic and extrinsic rewards to strike a balance between exploration and exploitation. Using a large-scale agent-based microscopic simulation platform to evaluate our proposed algorithms, our model-free reinforcement learning and hybrid algorithm show excellent performance on both artificial road network and community-based Singapore road network with empirical demands, and our hybrid algorithm can significantly accelerate the model-free learner in the process of learning.",0
"An integrated system of ride-hailing, street-hailing, and ride-sharing with optimized intelligent vehicle routing in response to real-time demand patterns should be adopted for the future of Mobility-as-a-Service (MaaS). Our goal is to optimize routing policies for a large fleet of vehicles for street-hailing services, in small to medium-sized road networks, amidst a stochastic demand pattern. To route the vacant vehicles, we proposed a model-based dispatch algorithm, a high-performance model-free reinforcement learning algorithm, and a hybrid algorithm combining the benefits of both the top-down approach and the model-free reinforcement learning. Our reinforcement learning-based routing algorithm uses proximal policy optimization and combined intrinsic and extrinsic rewards to balance exploration and exploitation. We evaluated our proposed algorithms using a large-scale agent-based microscopic simulation platform. Our model-free reinforcement learning and hybrid algorithm show excellent performance on both an artificial road network and a community-based Singapore road network with empirical demands. Additionally, our hybrid algorithm significantly accelerates the model-free learner during the learning process.",1
"We study high-confidence behavior-agnostic off-policy evaluation in reinforcement learning, where the goal is to estimate a confidence interval on a target policy's value, given only access to a static experience dataset collected by unknown behavior policies. Starting from a function space embedding of the linear program formulation of the $Q$-function, we obtain an optimization problem with generalized estimating equation constraints. By applying the generalized empirical likelihood method to the resulting Lagrangian, we propose CoinDICE, a novel and efficient algorithm for computing confidence intervals. Theoretically, we prove the obtained confidence intervals are valid, in both asymptotic and finite-sample regimes. Empirically, we show in a variety of benchmarks that the confidence interval estimates are tighter and more accurate than existing methods.",0
"Our focus is on high-confidence off-policy evaluation in reinforcement learning, where the aim is to assess the value of a target policy using only a static dataset collected by unknown behavior policies. To achieve this, we begin by embedding the linear program formulation of the $Q$-function into a function space. This leads us to an optimization problem that incorporates generalized estimating equation constraints. We propose a new algorithm called CoinDICE, which applies the generalized empirical likelihood method to the Lagrangian resulting from this problem to efficiently compute confidence intervals. We prove that the confidence intervals produced by CoinDICE are valid both in asymptotic and finite-sample regimes. In our experiments, we demonstrate that CoinDICE yields tighter and more accurate confidence interval estimates than existing methods across a range of benchmarks.",1
"We present a novel framework for design space search on analog circuit sizing using deep reinforcement learning (DRL). Nowadays, analog circuit design is a manual routine that requires heavy design efforts due to the absence of automation tools, motivating the urge to develop one. Prior approaches cast this process as an optimization problem. They use global search strategies based on DRL with complex network architectures. Nonetheless, the models are hard to converge and neglected various working conditions of PVT (process, voltage, temperature).In this work, we reduce the problem to a constraint satisfaction problem, where a local strategy is adopted. Thus, a simple feed-forward network with few layers can be used to implement a model-based reinforcement learning agent. To evaluate the value of the our framework in production, we cooperate with R&Ds in an IC design company. On circuits with TSMC advanced 5 and 6nm process, our agents can deliver PPA (performance, power, area) beyond human level. Furthermore, the product will be taped out in the near future.",0
"A new approach for analog circuit sizing design space search using deep reinforcement learning (DRL) is introduced. The current manual routine for analog circuit design requires significant design efforts, highlighting the need for automation tools. Previous methods have treated this process as an optimization problem and utilized global search strategies with complex network architectures based on DRL. However, these models have had difficulty converging and have neglected certain working conditions. In this study, the problem is simplified to a constraint satisfaction problem, and a local strategy is utilized. As a result, a model-based reinforcement learning agent can be implemented using a simple feed-forward network with few layers. The effectiveness of the framework is evaluated in collaboration with R&Ds in an IC design company. The agents developed using this framework can deliver performance, power, and area (PPA) beyond human levels on circuits with TSMC advanced 5 and 6nm processes. The product is expected to be taped out soon.",1
"Deep Reinforcement Learning (DRL) has become an appealing solution to algorithmic trading such as high frequency trading of stocks and cyptocurrencies. However, DRL have been shown to be susceptible to adversarial attacks. It follows that algorithmic trading DRL agents may also be compromised by such adversarial techniques, leading to policy manipulation. In this paper, we develop a threat model for deep trading policies, and propose two attack techniques for manipulating the performance of such policies at test-time. Furthermore, we demonstrate the effectiveness of the proposed attacks against benchmark and real-world DQN trading agents.",0
"Algorithmic trading in high frequency stocks and cryptocurrencies has found Deep Reinforcement Learning (DRL) to be an attractive solution. Nonetheless, DRL has proven to be vulnerable to adversarial attacks, which may compromise DRL agents for algorithmic trading and lead to policy manipulation. This paper outlines a threat model for deep trading policies and presents two attack techniques to manipulate the performance of such policies during testing. Additionally, we showcase the efficacy of the proposed attacks on both benchmark and real-world DQN trading agents.",1
"Complex networks are often either too large for full exploration, partially accessible, or partially observed. Downstream learning tasks on these incomplete networks can produce low quality results. In addition, reducing the incompleteness of the network can be costly and nontrivial. As a result, network discovery algorithms optimized for specific downstream learning tasks given resource collection constraints are of great interest. In this paper, we formulate the task-specific network discovery problem in an incomplete network setting as a sequential decision making problem. Our downstream task is selective harvesting, the optimal collection of vertices with a particular attribute. We propose a framework, called Network Actor Critic (NAC), which learns a policy and notion of future reward in an offline setting via a deep reinforcement learning algorithm. The NAC paradigm utilizes a task-specific network embedding to reduce the state space complexity. A detailed comparative analysis of popular network embeddings is presented with respect to their role in supporting offline planning. Furthermore, a quantitative study is presented on several synthetic and real benchmarks using NAC and several baselines. We show that offline models of reward and network discovery policies lead to significantly improved performance when compared to competitive online discovery algorithms. Finally, we outline learning regimes where planning is critical in addressing sparse and changing reward signals.",0
"Exploring complex networks can be difficult due to their large size or incompleteness, which can lead to poor results in downstream learning tasks. Improving network completeness can be costly, so there is a need for network discovery algorithms optimized for specific learning tasks given resource constraints. This paper presents a framework called Network Actor Critic (NAC) that formulates the problem of selective harvesting in an incomplete network as a sequential decision making problem. NAC uses a deep reinforcement learning algorithm to learn a policy and notion of future reward in an offline setting, utilizing a task-specific network embedding to reduce state space complexity. The paper presents a comparative analysis of popular network embeddings and a quantitative study on synthetic and real benchmarks. The results show that offline models lead to improved performance compared to online discovery algorithms, and planning is critical for addressing sparse and changing reward signals.",1
"Challenging problems of deep reinforcement learning systems with regard to the application on real systems are their adaptivity to changing environments and their efficiency w.r.t. computational resources and data. In the application of learning lane-change behavior for autonomous driving, agents have to deal with a varying number of surrounding vehicles. Furthermore, the number of required transitions imposes a bottleneck, since test drivers cannot perform an arbitrary amount of lane changes in the real world. In the off-policy setting, additional information on solving the task can be gained by observing actions from others. While in the classical RL setup this knowledge remains unused, we use other drivers as surrogates to learn the agent's value function more efficiently. We propose Surrogate Q-learning that deals with the aforementioned problems and reduces the required driving time drastically. We further propose an efficient implementation based on a permutation-equivariant deep neural network architecture of the Q-function to estimate action-values for a variable number of vehicles in sensor range. We show that the architecture leads to a novel replay sampling technique we call Scene-centric Experience Replay and evaluate the performance of Surrogate Q-learning and Scene-centric Experience Replay in the open traffic simulator SUMO. Additionally, we show that our methods enhance real-world applicability of RL systems by learning policies on the real highD dataset.",0
"The adaptivity of deep reinforcement learning systems to changing environments and their efficiency in terms of computational resources and data are challenging problems when it comes to their application on real systems. When learning lane-change behavior for autonomous driving, agents face the challenge of dealing with a varying number of surrounding vehicles and the bottleneck of required transitions due to limited real-world testing. In contrast to the classical RL setup, Surrogate Q-learning utilizes other drivers' actions to gain additional information on solving the task efficiently. We propose an efficient implementation based on a permutation-equivariant deep neural network architecture to estimate action-values for a variable number of vehicles in sensor range. Our architecture leads to a novel replay sampling technique called Scene-centric Experience Replay. We evaluate the performance of Surrogate Q-learning and Scene-centric Experience Replay in the open traffic simulator SUMO and demonstrate their enhancement of real-world applicability of RL systems by learning policies on the real highD dataset.",1
"Machine learning models have widely been used in fraud detection systems. Most of the research and development efforts have been concentrated on improving the performance of the fraud scoring models. Yet, the downstream fraud alert systems still have limited to no model adoption and rely on manual steps. Alert systems are pervasively used across all payment channels in retail banking and play an important role in the overall fraud detection process. Current fraud detection systems end up with large numbers of dropped alerts due to their inability to account for the alert processing capacity. Ideally, alert threshold selection enables the system to maximize the fraud detection while balancing the upstream fraud scores and the available bandwidth of the alert processing teams. However, in practice, fixed thresholds that are used for their simplicity do not have this ability. In this paper, we propose an enhanced threshold selection policy for fraud alert systems. The proposed approach formulates the threshold selection as a sequential decision making problem and uses Deep Q-Network based reinforcement learning. Experimental results show that this adaptive approach outperforms the current static solutions by reducing the fraud losses as well as improving the operational efficiency of the alert system.",0
"Fraud detection systems have widely utilized machine learning models, with a significant focus on enhancing the performance of fraud scoring models. However, downstream fraud alert systems continue to rely on manual procedures with limited model adoption. Alert systems are crucial to the fraud detection process in retail banking and are extensively used across all payment channels. Unfortunately, existing fraud detection systems often drop numerous alerts due to their incapacity to handle alert processing. Selecting an alert threshold that maximizes fraud detection while balancing upstream fraud scores and the alert processing team's available bandwidth can be challenging. In practice, fixed thresholds are often used for their simplicity, but they do not possess the necessary adaptability. In this research, we suggest an improved threshold selection policy for fraud alert systems that employs Deep Q-Network based reinforcement learning to formulate the threshold selection as a sequential decision-making problem. Experimental findings reveal that our adaptive approach outperforms current static solutions by decreasing fraud losses and enhancing the alert system's operational efficiency.",1
"Reinforcement learning is a promising approach to synthesizing policies for challenging robotics tasks. A key problem is how to ensure safety of the learned policy---e.g., that a walking robot does not fall over or that an autonomous car does not run into an obstacle. We focus on the setting where the dynamics are known, and the goal is to ensure that a policy trained in simulation satisfies a given safety constraint. We propose an approach, called model predictive shielding (MPS), that switches on-the-fly between a learned policy and a backup policy to ensure safety. We prove that our approach guarantees safety, and empirically evaluate it on the cart-pole.",0
"The use of reinforcement learning presents a promising solution for creating policies that can tackle complex robotics tasks. However, one major challenge that arises is the need to ensure the safety of the learned policy. For instance, preventing a walking robot from falling over or stopping an autonomous car from hitting an obstacle. To address this issue, we concentrate on situations where the dynamics are known, and the aim is to verify that a policy trained in a simulation satisfies a specific safety constraint. Our proposed solution, referred to as model predictive shielding (MPS), involves seamlessly alternating between a learned policy and a backup policy to ensure safety. We have demonstrated that our approach guarantees safety and have evaluated its effectiveness using the cart-pole.",1
"Deep reinforcement learning (RL) agents trained in a limited set of environments tend to suffer overfitting and fail to generalize to unseen testing environments. To improve their generalizability, data augmentation approaches (e.g. cutout and random convolution) are previously explored to increase the data diversity. However, we find these approaches only locally perturb the observations regardless of the training environments, showing limited effectiveness on enhancing the data diversity and the generalization performance. In this work, we introduce a simple approach, named mixreg, which trains agents on a mixture of observations from different training environments and imposes linearity constraints on the observation interpolations and the supervision (e.g. associated reward) interpolations. Mixreg increases the data diversity more effectively and helps learn smoother policies. We verify its effectiveness on improving generalization by conducting extensive experiments on the large-scale Procgen benchmark. Results show mixreg outperforms the well-established baselines on unseen testing environments by a large margin. Mixreg is simple, effective and general. It can be applied to both policy-based and value-based RL algorithms. Code is available at https://github.com/kaixin96/mixreg .",0
"When RL agents are trained in a limited number of environments, they often suffer from overfitting and cannot effectively generalize to new testing environments. Previous attempts to increase data diversity through augmentation methods like cutout and random convolution have shown limited success as they only slightly alter observations and do not improve generalization. In this study, we propose a new approach called mixreg that trains agents on a combination of observations from various training environments while imposing linearity constraints on observation and reward interpolations. This approach increases data diversity and helps agents learn smoother policies. Our experiments on the Procgen benchmark demonstrate that mixreg significantly outperforms established baselines on unseen testing environments. Mixreg is simple, effective, and versatile, as it can be used with both policy-based and value-based RL algorithms. The code for mixreg is available at https://github.com/kaixin96/mixreg.",1
"A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classification task.",0
"Self-supervised representation learning often involves contrasting similar and dissimilar pairs of samples. In the absence of labels, negative points are typically randomly sampled, even though they may share the same label in reality. However, our experiments show that selecting negative examples from truly different labels leads to improved performance in a synthetic setting where labels are available. Building on this finding, we introduce a debiased contrastive objective that addresses the issue of same-label datapoints, even without knowledge of the true labels. Our proposed objective consistently outperforms the current state-of-the-art in vision, language, and reinforcement learning benchmarks. We also provide generalization bounds for the downstream classification task.",1
"Model-based reinforcement learning (RL) has emerged as a promising tool for developing controllers for real world systems (e.g., robotics, autonomous driving, etc.). However, real systems often have constraints imposed on their state space which must be satisfied to ensure the safety of the system and its environment. Developing a verification tool for RL algorithms is challenging because the non-linear structure of neural networks impedes analytical verification of such models or controllers. To this end, we present a novel safety verification framework for model-based RL controllers using reachable set analysis. The proposed frame-work can efficiently handle models and controllers which are represented using neural networks. Additionally, if a controller fails to satisfy the safety constraints in general, the proposed framework can also be used to identify the subset of initial states from which the controller can be safely executed.",0
"Model-based reinforcement learning (RL) presents a promising solution for creating controllers for real world systems, such as robotics and autonomous driving. However, these systems often have constraints on their state space that must be met to ensure safety. Developing a verification tool for RL algorithms is difficult due to the non-linear structure of neural networks, which impedes analytical verification. Our new safety verification framework addresses this issue by using reachable set analysis to efficiently handle neural network-based models and controllers. Additionally, if a controller fails to meet safety constraints, the framework can identify the subset of initial states from which the controller can be safely executed.",1
"Policy networks are a central feature of deep reinforcement learning (RL) algorithms for continuous control, enabling the estimation and sampling of high-value actions. From the variational inference perspective on RL, policy networks, when employed with entropy or KL regularization, are a form of amortized optimization, optimizing network parameters rather than the policy distributions directly. However, this direct amortized mapping can empirically yield suboptimal policy estimates. Given this perspective, we consider the more flexible class of iterative amortized optimizers. We demonstrate that the resulting technique, iterative amortized policy optimization, yields performance improvements over conventional direct amortization methods on benchmark continuous control tasks.",0
"Deep reinforcement learning (RL) algorithms for continuous control rely heavily on policy networks. These networks allow for the estimation and sampling of high-value actions. From the variational inference perspective, policy networks optimize network parameters instead of policy distributions directly, which can be achieved through entropy or KL regularization. However, this approach may lead to suboptimal policy estimates. To address this issue, we propose using iterative amortized optimizers, which are more flexible than direct amortization methods. Our technique, known as iterative amortized policy optimization, outperforms conventional methods on benchmark continuous control tasks.",1
"The airworthiness and safety of a non-pedigreed autopilot must be verified, but the cost to formally do so can be prohibitive. We can bypass formal verification of non-pedigreed components by incorporating Runtime Safety Assurance (RTSA) as mechanism to ensure safety. RTSA consists of a meta-controller that observes the inputs and outputs of a non-pedigreed component and verifies formally specified behavior as the system operates. When the system is triggered, a verified recovery controller is deployed. Recovery controllers are designed to be safe but very likely disruptive to the operational objective of the system, and thus RTSA systems must balance safety and efficiency. The objective of this paper is to design a meta-controller capable of identifying unsafe situations with high accuracy. High dimensional and non-linear dynamics in which modern controllers are deployed along with the black-box nature of the nominal controllers make this a difficult problem. Current approaches rely heavily on domain expertise and human engineering. We frame the design of RTSA with the Markov decision process (MDP) framework and use reinforcement learning (RL) to solve it. Our learned meta-controller consistently exhibits superior performance in our experiments compared to our baseline, human engineered approach.",0
"Verifying the airworthiness and safety of a non-pedigreed autopilot can be costly, but Runtime Safety Assurance (RTSA) can bypass formal verification. RTSA uses a meta-controller to observe inputs and outputs and ensure safety by verifying formal behavior while the system is operating. This triggers a verified recovery controller that balances safety and efficiency. The paper aims to design a meta-controller that can accurately identify unsafe situations in high-dimensional and non-linear dynamics, which is challenging due to the black-box nature of nominal controllers. Current approaches rely on human expertise, but the paper uses the Markov decision process (MDP) framework and reinforcement learning (RL) to design RTSA. The experiments consistently show that the learned meta-controller outperforms the human-engineered approach.",1
"Value estimation is a critical component of the reinforcement learning (RL) paradigm. The question of how to effectively learn value predictors from data is one of the major problems studied by the RL community, and different approaches exploit structure in the problem domain in different ways. Model learning can make use of the rich transition structure present in sequences of observations, but this approach is usually not sensitive to the reward function. In contrast, model-free methods directly leverage the quantity of interest from the future, but receive a potentially weak scalar signal (an estimate of the return). We develop an approach for representation learning in RL that sits in between these two extremes: we propose to learn what to model in a way that can directly help value prediction. To this end, we determine which features of the future trajectory provide useful information to predict the associated return. This provides tractable prediction targets that are directly relevant for a task, and can thus accelerate learning the value function. The idea can be understood as reasoning, in hindsight, about which aspects of the future observations could help past value prediction. We show how this can help dramatically even in simple policy evaluation settings. We then test our approach at scale in challenging domains, including on 57 Atari 2600 games.",0
"The estimation of value is a crucial aspect of the reinforcement learning paradigm. The RL community has been exploring various ways to effectively learn value predictors from data, taking advantage of different problem domain structures. While model learning can utilize the transition structure in observation sequences, it often overlooks the reward function. On the other hand, model-free methods directly use future values, but with a potentially weak scalar signal. Our proposed approach for representation learning in RL lies between these two extremes. We aim to learn what to model to facilitate value prediction by identifying the features of future trajectories that offer useful information to predict the associated return. This results in relevant and feasible prediction targets that can hasten the value function's learning. The approach involves retrospectively reasoning about the future observations that could aid past value prediction, and it has shown significant success in simple policy evaluation settings. We have tested this approach in challenging domains, including 57 Atari 2600 games.",1
"In this paper, a framework for lane merge coordination is presented utilising a centralised system, for connected vehicles. The delivery of trajectory recommendations to the connected vehicles on the road is based on a Traffic Orchestrator and a Data Fusion as the main components. Deep Reinforcement Learning and data analysis is used to predict trajectory recommendations for connected vehicles, taking into account unconnected vehicles for those suggestions. The results highlight the adaptability of the Traffic Orchestrator, when employing Dueling Deep Q-Network in an unseen real world merging scenario. A performance comparison of different reinforcement learning models and evaluation against Key Performance Indicator (KPI) are also presented.",0
"This paper introduces a centralized system for coordinating lane merges among connected vehicles. The system relies on a Traffic Orchestrator and a Data Fusion to provide trajectory recommendations to connected vehicles on the road. To generate these recommendations, the system employs Deep Reinforcement Learning and data analysis, considering unconnected vehicles in its predictions. The study demonstrates the adaptability of the Traffic Orchestrator in an unseen merging scenario, utilizing Dueling Deep Q-Network. Additionally, the paper includes a performance comparison of various reinforcement learning models and evaluation against Key Performance Indicators (KPIs).",1
"The year 2020 has seen the COVID-19 virus lead to one of the worst global pandemics in history. As a result, governments around the world are faced with the challenge of protecting public health, while keeping the economy running to the greatest extent possible. Epidemiological models provide insight into the spread of these types of diseases and predict the effects of possible intervention policies. However, to date,the even the most data-driven intervention policies rely on heuristics. In this paper, we study how reinforcement learning (RL) can be used to optimize mitigation policies that minimize the economic impact without overwhelming the hospital capacity. Our main contributions are (1) a novel agent-based pandemic simulator which, unlike traditional models, is able to model fine-grained interactions among people at specific locations in a community; and (2) an RL-based methodology for optimizing fine-grained mitigation policies within this simulator. Our results validate both the overall simulator behavior and the learned policies under realistic conditions.",0
"The COVID-19 virus has caused one of the worst pandemics in history, impacting global public health and economies. Governments are tasked with the challenge of balancing these two priorities. While epidemiological models can provide insight into disease spread and intervention policies, current data-driven policies still rely on heuristics. In this study, we explore the use of reinforcement learning (RL) to optimize mitigation policies that minimize economic impact and hospital capacity overload. Our contributions include a novel agent-based pandemic simulator that models fine-grained interactions and an RL-based methodology for optimizing mitigation policies within the simulator. Our results confirm the effectiveness of both the simulator and learned policies under realistic conditions.",1
"When autonomous agents interact in the same environment, they must often cooperate to achieve their goals. One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it. However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement. Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols. More general methods usually require human input or domain-specific data, and so do not scale. To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning. Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven. We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory. Additionally, we investigate how the physical location of agents influences negotiation outcomes.",0
"When autonomous agents interact with each other, it is often necessary for them to collaborate in order to achieve their objectives. To do so effectively, they can form a team, establish a binding agreement on a joint plan, and execute it. However, if the agents are motivated by self-interest, it is essential to allocate team formation benefits in a way that encourages agreement. Although a variety of multi-agent negotiation methods have been proposed, they typically only work for specific negotiation protocols. More general methods typically require human input or domain-specific information, making them impractical for large-scale applications. To address this issue, we introduce a framework for training agents to negotiate and form teams using deep reinforcement learning. Importantly, our approach is experience-driven and does not rely on any specific negotiation protocol. We evaluate our framework on both non-spatial and spatially-extended team-formation negotiation environments, and our results show that our agents outperform hand-crafted bots and achieve negotiation outcomes consistent with cooperative game theory's fair solutions. Additionally, we investigate how the physical location of agents affects negotiation outcomes.",1
"The purpose of this paper is to use reinforcement learning to model learning agents which can recognize formal languages. Agents are modeled as simple multi-head automaton, a new model of finite automaton that uses multiple heads, and six different languages are formulated as reinforcement learning problems. Two different algorithms are used for optimization. First algorithm is Q-learning which trains gated recurrent units to learn optimal policies. The second one is genetic algorithm which searches for the optimal solution by using evolution inspired operations. The results show that genetic algorithm performs better than Q-learning algorithm in general but Q-learning algorithm finds solutions faster for regular languages.",0
"This paper aims to utilize reinforcement learning in developing learning agents capable of identifying formal languages. The agents are represented as basic multi-head automata, a novel type of finite automaton that employs multiple heads. Six distinct languages are formulated as reinforcement learning tasks, and two different optimization algorithms are employed. The initial algorithm is Q-learning, which employs gated recurrent units to learn optimal policies. The second algorithm is a genetic algorithm that employs evolution-inspired operations to search for the best solution. The findings indicate that the genetic algorithm generally performs better than the Q-learning algorithm, although the latter can rapidly identify solutions for regular languages.",1
"After generalizing the Archimedean property of real numbers in such a way as to make it adaptable to non-numeric structures, we demonstrate that the real numbers cannot be used to accurately measure non-Archimedean structures. We argue that, since an agent with Artificial General Intelligence (AGI) should have no problem engaging in tasks that inherently involve non-Archimedean rewards, and since traditional reinforcement learning rewards are real numbers, therefore traditional reinforcement learning probably will not lead to AGI. We indicate two possible ways traditional reinforcement learning could be altered to remove this roadblock.",0
"We have adapted and generalized the Archimedean property of real numbers to apply to non-numeric structures. Through this, we have found that real numbers are not a precise method of measuring non-Archimedean structures. Our argument is that an agent with Artificial General Intelligence (AGI) should be able to perform tasks that involve non-Archimedean rewards but traditional reinforcement learning rewards are real numbers, which may hinder the development of AGI. To overcome this hurdle, we suggest two potential modifications to traditional reinforcement learning.",1
"The challenge of developing powerful and general Reinforcement Learning (RL) agents has received increasing attention in recent years. Much of this effort has focused on the single-agent setting, in which an agent maximizes a predefined extrinsic reward function. However, a long-term question inevitably arises: how will such independent agents cooperate when they are continually learning and acting in a shared multi-agent environment? Observing that humans often provide incentives to influence others' behavior, we propose to equip each RL agent in a multi-agent environment with the ability to give rewards directly to other agents, using a learned incentive function. Each agent learns its own incentive function by explicitly accounting for its impact on the learning of recipients and, through them, the impact on its own extrinsic objective. We demonstrate in experiments that such agents significantly outperform standard RL and opponent-shaping agents in challenging general-sum Markov games, often by finding a near-optimal division of labor. Our work points toward more opportunities and challenges along the path to ensure the common good in a multi-agent future.",0
"In recent years, there has been a growing focus on the development of powerful and versatile Reinforcement Learning (RL) agents. While much of this emphasis has been on single-agent scenarios where agents maximize a specific extrinsic reward function, questions have arisen about how independent agents will collaborate in a shared multi-agent environment as they continually learn and act. To address this, we propose equipping each RL agent in a multi-agent setting with the ability to give rewards to other agents via a learned incentive function, as humans do to influence behavior. Agents learn their own incentive functions by considering their impact on the learning of recipients and, consequently, their own extrinsic goals. In experiments, we show that such agents outperform standard RL and opponent-shaping agents in challenging general-sum Markov games, often achieving near-optimal division of labor. Our work highlights the opportunities and challenges involved in ensuring the common good in a multi-agent future.",1
"Many optimizers have been proposed for training deep neural networks, and they often have multiple hyperparameters, which make it tricky to benchmark their performance. In this work, we propose a new benchmarking protocol to evaluate both end-to-end efficiency (training a model from scratch without knowing the best hyperparameter) and data-addition training efficiency (the previously selected hyperparameters are used for periodically re-training the model with newly collected data). For end-to-end efficiency, unlike previous work that assumes random hyperparameter tuning, which over-emphasizes the tuning time, we propose to evaluate with a bandit hyperparameter tuning strategy. A human study is conducted to show that our evaluation protocol matches human tuning behavior better than the random search. For data-addition training, we propose a new protocol for assessing the hyperparameter sensitivity to data shift. We then apply the proposed benchmarking framework to 7 optimizers and various tasks, including computer vision, natural language processing, reinforcement learning, and graph mining. Our results show that there is no clear winner across all the tasks.",0
"Numerous optimizers have been suggested for the training of deep neural networks, and they frequently comprise multiple hyperparameters, which complicates the process of evaluating their performance. This study introduces a novel benchmarking protocol to assess the end-to-end and data-addition training efficiency of these optimizers. To evaluate end-to-end efficiency, we suggest using a bandit hyperparameter tuning strategy instead of the previous random hyperparameter tuning method, which overemphasizes tuning time. A human study demonstrates that our evaluation protocol aligns more closely with human tuning behavior than random search. For data-addition training, we propose a new protocol to evaluate hyperparameter sensitivity to data shift. We then apply this benchmarking framework to 7 optimizers and various tasks, including computer vision, natural language processing, reinforcement learning, and graph mining, and our findings indicate that no optimizer performs best across all tasks.",1
"To improve the sample efficiency of policy-gradient based reinforcement learning algorithms, we propose implicit distributional actor-critic (IDAC) that consists of a distributional critic, built on two deep generator networks (DGNs), and a semi-implicit actor (SIA), powered by a flexible policy distribution. We adopt a distributional perspective on the discounted cumulative return and model it with a state-action-dependent implicit distribution, which is approximated by the DGNs that take state-action pairs and random noises as their input. Moreover, we use the SIA to provide a semi-implicit policy distribution, which mixes the policy parameters with a reparameterizable distribution that is not constrained by an analytic density function. In this way, the policy's marginal distribution is implicit, providing the potential to model complex properties such as covariance structure and skewness, but its parameter and entropy can still be estimated. We incorporate these features with an off-policy algorithm framework to solve problems with continuous action space and compare IDAC with state-of-the-art algorithms on representative OpenAI Gym environments. We observe that IDAC outperforms these baselines in most tasks. Python code is provided.",0
"Our proposal for enhancing the efficiency of policy-gradient based reinforcement learning algorithms is the implicit distributional actor-critic (IDAC). IDAC is composed of a distributional critic that employs two deep generator networks (DGNs) and a semi-implicit actor (SIA) that utilizes a flexible policy distribution. We adopt a distributional approach to the discounted cumulative return and represent it using a state-action-dependent implicit distribution, which is approximated by the DGNs that take state-action pairs and random noises as inputs. Additionally, we use the SIA to create a semi-implicit policy distribution that combines the policy parameters with a reparameterizable distribution, which is not limited by an analytic density function. This strategy makes the policy's marginal distribution implicit, enabling us to model complex properties like covariance structure and skewness, while still allowing us to estimate its parameter and entropy. We incorporate these features into an off-policy algorithm structure to address problems with continuous action space and compare IDAC with state-of-the-art algorithms on representative OpenAI Gym environments. We observe that IDAC outperforms the baselines in most tasks and provide Python code for implementation.",1
"Learning and planning with latent space dynamics has been shown to be useful for sample efficiency in model-based reinforcement learning (MBRL) for discrete and continuous control tasks. In particular, recent work, for discrete action spaces, demonstrated the effectiveness of latent-space planning via Monte-Carlo Tree Search (MCTS) for bootstrapping MBRL during learning and at test time. However, the potential gains from latent-space tree search have not yet been demonstrated for environments with continuous action spaces. In this work, we propose and explore an MBRL approach for continuous action spaces based on tree-based planning over learned latent dynamics. We show that it is possible to demonstrate the types of bootstrapping benefits as previously shown for discrete spaces. In particular, the approach achieves improved sample efficiency and performance on a majority of challenging continuous-control benchmarks compared to the state-of-the-art.",0
"Using latent space dynamics for learning and planning has proven to be beneficial in model-based reinforcement learning (MBRL) for both discrete and continuous control tasks, as it increases sample efficiency. For discrete action spaces, recent research has shown the effectiveness of latent-space planning through Monte-Carlo Tree Search (MCTS) in MBRL during learning and testing. However, the advantages of latent-space tree search have yet to be demonstrated in environments with continuous action spaces. In this study, we present and investigate an MBRL approach for continuous action spaces that is based on tree-based planning over learned latent dynamics. Our findings demonstrate that it is possible to achieve the same bootstrapping benefits as with discrete spaces, resulting in improved sample efficiency and performance on most challenging continuous-control benchmarks compared to the state-of-the-art.",1
"In this paper we explore previously unidentified connections between relational event model (REM) from the field of network science and inverse reinforcement learning (IRL) from the field of machine learning with respect to their ability to characterize sequences of directed social interaction events in group settings. REM is a conventional approach to tackle such a problem whereas the application of IRL is a largely unbeaten path. We begin by examining the mathematical components of both REM and IRL and find straightforward analogies between the two methods as well as unique characteristics of the IRL approach. We demonstrate the special utility of IRL in characterizing group social interactions with an empirical experiment, in which we use IRL to infer individual behavioral preferences based on a sequence of directed communication events from a group of virtual-reality game players interacting and cooperating to accomplish a shared goal. Our comparison and experiment introduce fresh perspectives for social behavior analytics and help inspire new research opportunities at the nexus of social network analysis and machine learning.",0
"The aim of this paper is to investigate the potential connections between the relational event model (REM) in network science and inverse reinforcement learning (IRL) in machine learning. Specifically, we explore their ability to describe directed social interaction events within group settings, which has traditionally been tackled through REM. However, the application of IRL in this area is relatively unexplored. We compare the mathematical components of both methods and identify similarities and unique features of IRL. To demonstrate the usefulness of IRL, we conduct an empirical experiment in which we use it to infer individual behavioral preferences based on directed communication events between virtual-reality game players working towards a shared goal. Our findings provide new insights into social behavior analytics and highlight potential research opportunities at the intersection of social network analysis and machine learning.",1
"We propose a new framework for Imitation Learning (IL) via density estimation of the expert's occupancy measure followed by Maximum Occupancy Entropy Reinforcement Learning (RL) using the density as a reward. Our approach maximizes a non-adversarial model-free RL objective that provably lower bounds reverse Kullback-Leibler divergence between occupancy measures of the expert and imitator. We present a practical IL algorithm, Neural Density Imitation (NDI), which obtains state-of-the-art demonstration efficiency on benchmark control tasks.",0
"Our suggestion is a fresh structure for Imitation Learning (IL) that involves estimating the expert's occupancy measure using density, and then employing Maximum Occupancy Entropy Reinforcement Learning (RL) with the density serving as a reward. Our method focuses on achieving a non-adversarial model-free RL objective that guarantees a lower bound on the reverse Kullback-Leibler divergence between the expert and imitator's occupancy measures. We have also created a practical IL algorithm called Neural Density Imitation (NDI), which has shown remarkable demonstration efficiency on standard control tasks.",1
"For a network of stochastic units trained on a reinforcement learning task, one biologically plausible way of learning is to treat each unit as a reinforcement learning unit and train each unit by REINFORCE using the same global reward signal. In this case, only a global reward signal has to be broadcast to all units, and the learning rule given is local. Although this learning rule follows the gradient of return in expectation, it suffers from high variance and cannot be used to train a deep network in practice. In this paper, we propose an algorithm called Weight Maximization, which can significantly improve the speed of applying REINFORCE to all units. Essentially, we replace the global reward to each hidden unit with the change in the norm of output weights, such that each hidden unit in the network is trying to maximize the norm of output weights instead of the global reward. We found that the new algorithm can solve simple reinforcement learning tasks significantly faster than the baseline model. We also prove that the resulting learning rule is approximately following gradient ascent on the reward in expectation when applied to a multi-layer network of Bernoulli logistic unit. It illustrates an example of intelligent behavior arising from a population of self-interested hedonistic neurons, which corresponds to Klopf's hedonistic neuron hypothesis.",0
"To train a network of stochastic units on a reinforcement learning task, one option is to treat each unit as a reinforcement learning unit and use the same global reward signal for training. This approach only requires broadcasting a global reward signal to all units, and the learning rule is local. However, this approach has high variance and is not practical for training deep networks. To address this issue, we propose an algorithm called Weight Maximization, which replaces the global reward with the change in the norm of output weights for each hidden unit. This allows each unit to maximize the norm of output weights instead of the global reward. Our experiments show that this new algorithm can solve simple reinforcement learning tasks faster than the baseline model. We also prove that the resulting learning rule approximately follows gradient ascent on the reward in expectation when applied to a multi-layer network of Bernoulli logistic unit. This demonstrates an example of intelligent behavior arising from a population of self-interested hedonistic neurons, which aligns with Klopf's hedonistic neuron hypothesis.",1
"This paper proposes a lexicographic Deep Reinforcement Learning (DeepRL)-based approach to chance-constrained Markov Decision Processes, in which the controller seeks to ensure that the probability of satisfying the constraint is above a given threshold. Standard DeepRL approaches require i) the constraints to be included as additional weighted terms in the cost function, in a multi-objective fashion, and ii) the tuning of the introduced weights during the training phase of the Deep Neural Network (DNN) according to the probability thresholds. The proposed approach, instead, requires to separately train one constraint-free DNN and one DNN associated to each constraint and then, at each time-step, to select which DNN to use depending on the system observed state. The presented solution does not require any hyper-parameter tuning besides the standard DNN ones, even if the probability thresholds changes. A lexicographic version of the well-known DeepRL algorithm DQN is also proposed and validated via simulations.",0
"The article suggests a new method for chance-constrained Markov Decision Processes through lexicographic Deep Reinforcement Learning (DeepRL). The goal is to maintain a probability of satisfying the constraint above a certain threshold. Traditional DeepRL approaches involve adding weighted constraints to the cost function and tuning the weights during the Deep Neural Network (DNN) training phase. However, the proposed approach involves training one constraint-free DNN and one DNN for each constraint separately. Afterward, the appropriate DNN selection is made depending on the observed state of the system. This method does not require hyper-parameter tuning, except for standard DNN tuning. Also, a lexicographic version of the DQN DeepRL algorithm is proposed and verified through simulations.",1
"A widely-used actor-critic reinforcement learning algorithm for continuous control, Deep Deterministic Policy Gradients (DDPG), suffers from the overestimation problem, which can negatively affect the performance. Although the state-of-the-art Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm mitigates the overestimation issue, it can lead to a large underestimation bias. In this paper, we propose to use the Boltzmann softmax operator for value function estimation in continuous control. We first theoretically analyze the softmax operator in continuous action space. Then, we uncover an important property of the softmax operator in actor-critic algorithms, i.e., it helps to smooth the optimization landscape, which sheds new light on the benefits of the operator. We also design two new algorithms, Softmax Deep Deterministic Policy Gradients (SD2) and Softmax Deep Double Deterministic Policy Gradients (SD3), by building the softmax operator upon single and double estimators, which can effectively improve the overestimation and underestimation bias. We conduct extensive experiments on challenging continuous control tasks, and results show that SD3 outperforms state-of-the-art methods.",0
"The Deep Deterministic Policy Gradients (DDPG) algorithm, commonly used for continuous control in reinforcement learning, is plagued by the overestimation problem, which can have negative impacts on performance. While the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm is the current state-of-the-art solution to mitigate overestimation, it can result in significant underestimation bias. This study proposes the use of the Boltzmann softmax operator for value function estimation in continuous control. Theoretical analysis of the operator in continuous action space reveals that it smooths the optimization landscape, which is a valuable benefit in actor-critic algorithms. Two new algorithms, Softmax Deep Deterministic Policy Gradients (SD2) and Softmax Deep Double Deterministic Policy Gradients (SD3), have been developed, using the softmax operator on single and double estimators, respectively, to effectively address both overestimation and underestimation bias. Extensive experiments on challenging continuous control tasks demonstrate that SD3 surpasses existing methods.",1
"Embodied agents operating in human spaces must be able to master how their environment works: what objects can the agent use, and how can it use them? We introduce a reinforcement learning approach for exploration for interaction, whereby an embodied agent autonomously discovers the affordance landscape of a new unmapped 3D environment (such as an unfamiliar kitchen). Given an egocentric RGB-D camera and a high-level action space, the agent is rewarded for maximizing successful interactions while simultaneously training an image-based affordance segmentation model. The former yields a policy for acting efficiently in new environments to prepare for downstream interaction tasks, while the latter yields a convolutional neural network that maps image regions to the likelihood they permit each action, densifying the rewards for exploration. We demonstrate our idea with AI2-iTHOR. The results show agents can learn how to use new home environments intelligently and that it prepares them to rapidly address various downstream tasks like ""find a knife and put it in the drawer."" Project page: http://vision.cs.utexas.edu/projects/interaction-exploration/",0
"For embodied agents to operate effectively in human spaces, they must possess knowledge of their surroundings, including what objects are available and how they can be utilized. To address this issue, we propose a reinforcement learning method for exploration and interaction in which an autonomous embodied agent can discover the affordance landscape of an unmapped 3D environment, such as an unfamiliar kitchen. This approach involves using an egocentric RGB-D camera and a high-level action space to train an image-based affordance segmentation model while maximizing successful interactions for rewards. This method enables agents to efficiently act in new environments and prepares them for downstream interaction tasks. Our study with AI2-iTHOR demonstrates that agents can learn to intelligently use new home environments and quickly complete tasks such as ""find a knife and put it in the drawer."" Further details can be found on our project page: http://vision.cs.utexas.edu/projects/interaction-exploration/",1
"A novel framework is proposed for cellular offloading with the aid of multiple unmanned aerial vehicles (UAVs), while the non-orthogonal multiple access (NOMA) technique is employed at each UAV to further improve the spectrum efficiency of the wireless network. The optimization problem of joint three-dimensional (3D) trajectory design and power allocation is formulated for maximizing the throughput. Since ground mobile users are considered as roaming continuously, the UAVs need to be re-deployed timely based on the movement of users. In an effort to solve this pertinent dynamic problem, a K-means based clustering algorithm is first adopted for periodically partitioning users. Afterward, a mutual deep Q-network (MDQN) algorithm is proposed to jointly determine the optimal 3D trajectory and power allocation of UAVs. In contrast to the conventional DQN algorithm, the MDQN algorithm enables the experience of multi-agent to be input into a shared neural network to shorten the training time with the assistance of state abstraction. Numerical results demonstrate that: 1) the proposed MDQN algorithm is capable of converging under minor constraints and has a faster convergence rate than the conventional DQN algorithm in the multi-agent case; 2) The achievable sum rate of the NOMA enhanced UAV network is 23% superior to the case of orthogonal multiple access (OMA); 3) By designing the optimal 3D trajectory of UAVs with the aid of the MDON algorithm, the sum rate of the network enjoys 142% and 56% gains than that of invoking the circular trajectory and the 2D trajectory, respectively.",0
"The article proposes a new approach to cellular offloading using multiple unmanned aerial vehicles (UAVs) and non-orthogonal multiple access (NOMA) to improve the wireless network's spectrum efficiency. The article presents an optimization problem for maximizing throughput by jointly designing a three-dimensional (3D) trajectory and power allocation. Since ground mobile users continually roam, the UAVs must be redeployed based on user movement. To solve this problem, the article proposes a K-means clustering algorithm to partition users and a mutual deep Q-network (MDQN) algorithm to determine the optimal 3D trajectory and power allocation of UAVs. The MDQN algorithm allows multi-agent experience to be input into a shared neural network, which shortens the training time with state abstraction. Numerical results show that the proposed MDQN algorithm converges faster than the conventional DQN algorithm, achieving a 23% superior sum rate compared to orthogonal multiple access (OMA). Additionally, designing the optimal 3D trajectory with the MDON algorithm provides a 142% and 56% gain in sum rate compared to invoking the circular trajectory and the 2D trajectory, respectively.",1
"We study an approach to offline reinforcement learning (RL) based on optimally solving finitely-represented MDPs derived from a static dataset of experience. This approach can be applied on top of any learned representation and has the potential to easily support multiple solution objectives as well as zero-shot adjustment to changing environments and goals. Our main contribution is to introduce the Deep Averagers with Costs MDP (DAC-MDP) and to investigate its solutions for offline RL. DAC-MDPs are a non-parametric model that can leverage deep representations and account for limited data by introducing costs for exploiting under-represented parts of the model. In theory, we show conditions that allow for lower-bounding the performance of DAC-MDP solutions. We also investigate the empirical behavior in a number of environments, including those with image-based observations. Overall, the experiments demonstrate that the framework can work in practice and scale to large complex offline RL problems.",0
"Our research focuses on an offline reinforcement learning approach that involves optimally solving finitely-represented MDPs derived from a static dataset of experience. This method can be applied to any learned representation and can support multiple solution objectives, as well as adapt to changing environments and goals with ease. Our main contribution is the introduction of the Deep Averagers with Costs MDP (DAC-MDP) and its investigation for offline RL solutions. DAC-MDPs are non-parametric models that can utilize deep representations and account for limited data by imposing costs for exploiting under-represented parts of the model. In theory, we demonstrate conditions that allow for lower-bounding the performance of DAC-MDP solutions. We also examine the empirical behavior in various environments, including those with image-based observations. Our experiments show that the framework is practical and can handle large complex offline RL problems.",1
"Reinforcement Learning is proving a successful tool that can manage urban intersections with a fraction of the effort required to curate traditional traffic controllers. However, literature on the introduction and control of pedestrians to such intersections is scarce. Furthermore, it is unclear what traffic state variables should be used as reward to obtain the best agent performance. This paper robustly evaluates 30 different Reinforcement Learning reward functions for controlling intersections serving pedestrians and vehicles covering the main traffic state variables available via modern vision-based sensors. Some rewards proposed in previous literature solely for vehicular traffic are extended to pedestrians while new ones are introduced. We use a calibrated model in terms of demand, sensors, green times and other operational constraints of a real intersection in Greater Manchester, UK. The assessed rewards can be classified in 5 groups depending on the magnitudes used: queues, waiting time, delay, average speed and throughput in the junction. The performance of different agents, in terms of waiting time, is compared across different demand levels, from normal operation to saturation of traditional adaptive controllers. We find that those rewards maximising the speed of the network obtain the lowest waiting time for vehicles and pedestrians simultaneously, closely followed by queue minimisation, demonstrating better performance than other previously proposed methods.",0
"Reinforcement Learning has proven to be a successful tool for efficiently managing urban intersections, requiring much less effort than traditional traffic controllers. However, there is a lack of literature on how to introduce and control pedestrians at these intersections and it is unclear which traffic state variables should be used to reward the agent for optimal performance. This study evaluates 30 different Reinforcement Learning reward functions for controlling intersections that serve both vehicles and pedestrians. The rewards are based on traffic state variables obtained from modern vision-based sensors and are classified into 5 groups: queues, waiting time, delay, average speed, and throughput. A calibrated model of a real intersection in Greater Manchester, UK, is used to assess the rewards across different demand levels. The study finds that rewards that maximize the speed of the network achieve the lowest waiting time for both vehicles and pedestrians, followed closely by queue minimization, outperforming previously proposed methods.",1
"Efficient exploration remains a challenging problem in reinforcement learning, especially for tasks where extrinsic rewards from environments are sparse or even totally disregarded. Significant advances based on intrinsic motivation show promising results in simple environments but often get stuck in environments with multimodal and stochastic dynamics. In this work, we propose a variational dynamic model based on the conditional variational inference to model the multimodality and stochasticity. We consider the environmental state-action transition as a conditional generative process by generating the next-state prediction under the condition of the current state, action, and latent variable. We derive an upper bound of the negative log-likelihood of the environmental transition and use such an upper bound as the intrinsic reward for exploration, which allows the agent to learn skills by self-supervised exploration without observing extrinsic rewards. We evaluate the proposed method on several image-based simulation tasks and a real robotic manipulating task. Our method outperforms several state-of-the-art environment model-based exploration approaches.",0
"Exploring efficiently is a difficult task in reinforcement learning, particularly for tasks where rewards from the environment are scarce or ignored altogether. Current methods using intrinsic motivation have shown success in simple environments but struggle when dealing with complex and unpredictable dynamics. This study introduces a variational dynamic model that uses conditional variational inference to handle multimodality and stochasticity. The state-action transition is treated as a generative process, where the next-state prediction is generated based on the current state, action, and latent variable. The negative log-likelihood of the environmental transition is used as an intrinsic reward for exploration, allowing the agent to learn skills through self-supervised exploration without relying on extrinsic rewards. The proposed method is evaluated on various image-based simulation tasks and a real-world robotic manipulation task, and it outperforms other environment model-based exploration approaches.",1
"It is believed that a model-based approach for reinforcement learning (RL) is the key to reduce sample complexity. However, the understanding of the sample optimality of model-based RL is still largely missing, even for the linear case. This work considers sample complexity of finding an $\epsilon$-optimal policy in a Markov decision process (MDP) that admits a linear additive feature representation, given only access to a generative model. We solve this problem via a plug-in solver approach, which builds an empirical model and plans in this empirical model via an arbitrary plug-in solver. We prove that under the anchor-state assumption, which implies implicit non-negativity in the feature space, the minimax sample complexity of finding an $\epsilon$-optimal policy in a $\gamma$-discounted MDP is $O(K/(1-\gamma)^3\epsilon^2)$, which only depends on the dimensionality $K$ of the feature space and has no dependence on the state or action space. We further extend our results to a relaxed setting where anchor-states may not exist and show that a plug-in approach can be sample efficient as well, providing a flexible approach to design model-based algorithms for RL.",0
"Reducing sample complexity in reinforcement learning (RL) is thought to be achievable through a model-based approach. However, the sample optimality of this method, even in linear cases, is not well understood. This study examines the sample complexity of finding an $\epsilon$-optimal policy in a Markov decision process (MDP) that uses a linear additive feature representation with only access to a generative model. Our approach uses a plug-in solver, which creates an empirical model and plans within it using an arbitrary plug-in solver. We prove that under the anchor-state assumption, which implies implicit non-negativity in the feature space, the minimax sample complexity of finding an $\epsilon$-optimal policy in a $\gamma$-discounted MDP is $O(K/(1-\gamma)^3\epsilon^2)$, where $K$ is the feature space's dimensionality, with no dependence on the state or action space. We also extend our results to a relaxed setting where anchor-states may not exist and show that a plug-in approach can be sample efficient, providing a flexible method for designing model-based algorithms for RL.",1
"We present $\Gamma$-nets, a method for generalizing value function estimation over timescale. By using the timescale as one of the estimator's inputs we can estimate value for arbitrary timescales. As a result, the prediction target for any timescale is available and we are free to train on multiple timescales at each timestep. Here we empirically evaluate $\Gamma$-nets in the policy evaluation setting. We first demonstrate the approach on a square wave and then on a robot arm using linear function approximation. Next, we consider the deep reinforcement learning setting using several Atari video games. Our results show that $\Gamma$-nets can be effective for predicting arbitrary timescales, with only a small cost in accuracy as compared to learning estimators for fixed timescales. $\Gamma$-nets provide a method for compactly making predictions at many timescales without requiring a priori knowledge of the task, making it a valuable contribution to ongoing work on model-based planning, representation learning, and lifelong learning algorithms.",0
"Our method, called $\Gamma$-nets, offers a way to estimate value function over a range of timescales. By incorporating the timescale as an input, we can predict values for any timescale and train on multiple timescales simultaneously. Our study focuses on evaluating $\Gamma$-nets for policy evaluation. We demonstrate their effectiveness through experiments on a square wave and a robot arm using linear function approximation, as well as on several Atari video games in the deep reinforcement learning setting. Our findings indicate that $\Gamma$-nets provide accurate predictions for arbitrary timescales at a minor cost in accuracy compared to fixed timescale estimators. This approach is a valuable contribution to the current research on model-based planning, representation learning, and lifelong learning algorithms as it allows for predictions at various timescales without prior knowledge of the task.",1
"Recent advances in multi-agent reinforcement learning (MARL) have achieved super-human performance in games like Quake 3 and Dota 2. Unfortunately, these techniques require orders-of-magnitude more training rounds than humans and don't generalize to new agent configurations even on the same game. In this work, we propose Collaborative Q-learning (CollaQ) that achieves state-of-the-art performance in the StarCraft multi-agent challenge and supports ad hoc team play. We first formulate multi-agent collaboration as a joint optimization on reward assignment and show that each agent has an approximately optimal policy that decomposes into two parts: one part that only relies on the agent's own state, and the other part that is related to states of nearby agents. Following this novel finding, CollaQ decomposes the Q-function of each agent into a self term and an interactive term, with a Multi-Agent Reward Attribution (MARA) loss that regularizes the training. CollaQ is evaluated on various StarCraft maps and shows that it outperforms existing state-of-the-art techniques (i.e., QMIX, QTRAN, and VDN) by improving the win rate by 40% with the same number of samples. In the more challenging ad hoc team play setting (i.e., reweight/add/remove units without re-training or finetuning), CollaQ outperforms previous SoTA by over 30%.",0
"Advancements in multi-agent reinforcement learning (MARL) have resulted in extraordinary game performance, surpassing that of humans, in games like Quake 3 and Dota 2. However, these methods require significantly more training rounds than humans and are unable to generalize to new agent configurations even within the same game. This research proposes Collaborative Q-learning (CollaQ) as a solution to these challenges, achieving exceptional performance in the StarCraft multi-agent challenge and supporting ad hoc team play. The authors achieve this by optimizing reward assignment via multi-agent collaboration and demonstrating that each agent's policy can be divided into two parts: one reliant on the agent's state and one related to nearby agents' states. CollaQ decomposes the Q-function of each agent into a self and interactive term, with a Multi-Agent Reward Attribution (MARA) loss that regularizes the training. This approach outperforms existing state-of-the-art techniques (QMIX, QTRAN, and VDN) by 40% in terms of win rate, using the same number of samples, and over 30% in ad hoc team play settings, without any retraining or finetuning.",1
"Reinforcement learning considers the problem of finding policies that maximize an expected cumulative reward in a Markov decision process with unknown transition probabilities. In this paper we consider the problem of finding optimal policies assuming that they belong to a reproducing kernel Hilbert space (RKHS). To that end we compute unbiased stochastic gradients of the value function which we use as ascent directions to update the policy. A major drawback of policy gradient-type algorithms is that they are limited to episodic tasks unless stationarity assumptions are imposed. Hence preventing these algorithms to be fully implemented online, which is a desirable property for systems that need to adapt to new tasks and/or environments in deployment. The main requirement for a policy gradient algorithm to work is that the estimate of the gradient at any point in time is an ascent direction for the initial value function. In this work we establish that indeed this is the case which enables to show the convergence of the online algorithm to the critical points of the initial value function. A numerical example shows the ability of our online algorithm to learn to solve a navigation and surveillance problem, in which an agent must loop between to goal locations. This example corroborates our theoretical findings about the ascent directions of subsequent stochastic gradients. It also shows how the agent running our online algorithm succeeds in learning to navigate, following a continuing cyclic trajectory that does not comply with the standard stationarity assumptions in the literature for non episodic training.",0
"The problem of maximizing expected cumulative reward in a Markov decision process with unknown transition probabilities is tackled by reinforcement learning. This paper focuses on finding optimal policies by assuming they belong to a reproducing kernel Hilbert space (RKHS). Stochastic gradients of the value function are computed and used as ascent directions to update the policy. However, policy gradient-type algorithms are limited to episodic tasks unless stationarity assumptions are imposed, preventing them from being fully implemented online. This work establishes that the estimate of the gradient is an ascent direction for the initial value function, enabling the convergence of the online algorithm to the critical points. A numerical example shows the ability of the algorithm to learn to solve a navigation and surveillance problem. The example confirms the theoretical findings and demonstrates how the agent succeeds in learning to navigate, following a continuing cyclic trajectory that does not comply with standard stationarity assumptions for non-episodic training.",1
"While Deep Reinforcement Learning (DRL) has emerged as a promising approach to many complex tasks, it remains challenging to train a single DRL agent that is capable of undertaking multiple different continuous control tasks. In this paper, we present a Knowledge Transfer based Multi-task Deep Reinforcement Learning framework (KTM-DRL) for continuous control, which enables a single DRL agent to achieve expert-level performance in multiple different tasks by learning from task-specific teachers. In KTM-DRL, the multi-task agent first leverages an offline knowledge transfer algorithm designed particularly for the actor-critic architecture to quickly learn a control policy from the experience of task-specific teachers, and then it employs an online learning algorithm to further improve itself by learning from new online transition samples under the guidance of those teachers. We perform a comprehensive empirical study with two commonly-used benchmarks in the MuJoCo continuous control task suite. The experimental results well justify the effectiveness of KTM-DRL and its knowledge transfer and online learning algorithms, as well as its superiority over the state-of-the-art by a large margin.",0
"Despite being a promising approach to complex tasks, training a single Deep Reinforcement Learning (DRL) agent to undertake multiple continuous control tasks remains a challenge. This paper introduces the Knowledge Transfer based Multi-task Deep Reinforcement Learning (KTM-DRL) framework for continuous control, which allows a single DRL agent to achieve high-level performance in various tasks by learning from task-specific teachers. KTM-DRL utilizes an offline knowledge transfer algorithm to quickly learn a control policy from the experience of task-specific teachers, followed by an online learning algorithm to further improve performance using new transition samples. The effectiveness of KTM-DRL and its algorithms are evaluated using two common benchmarks in the MuJoCo continuous control task suite, with results indicating its superiority over state-of-the-art approaches.",1
"Few-shot adaptation is a challenging problem in the context of simulation-to-real transfer in robotics, requiring safe and informative data collection. In physical systems, additional challenge may be posed by domain noise, which is present in virtually all real-world applications. In this paper, we propose to perform few-shot adaptation of dynamics models in noisy conditions using an uncertainty-aware Kalman filter-based neural network architecture. We show that the proposed method, which explicitly addresses domain noise, improves few-shot adaptation error over a blackbox adaptation LSTM baseline, and over a model-free on-policy reinforcement learning approach, which tries to learn an adaptable and informative policy at the same time. The proposed method also allows for system analysis by analyzing hidden states of the model during and after adaptation.",0
"The problem of adapting few-shot simulations in robotics from simulation to real-life situations is difficult and requires safe and informative data collection. Domain noise is an added complication that is present in almost all real-world applications. This research proposes a solution to perform few-shot adaptation of dynamics models in noisy conditions by using an uncertainty-aware Kalman filter-based neural network architecture. The proposed method explicitly addresses domain noise and improves few-shot adaptation error over a blackbox adaptation LSTM baseline and a model-free on-policy reinforcement learning approach. Additionally, the method allows for system analysis by examining hidden states of the model during and after adaptation.",1
"The phase-ordering problem of modern compilers has received a lot of attention from the research community over the years, yet remains largely unsolved. Various optimization sequences exposed to the user are manually designed by compiler developers. In designing such a sequence developers have to choose the set of optimization passes, their parameters and ordering within a sequence. Resulting sequences usually fall short of achieving optimal runtime for a given source code and may sometimes even degrade the performance when compared to unoptimized version. In this paper, we employ a deep reinforcement learning approach to the phase-ordering problem. Provided with sub-sequences constituting LLVM's O3 sequence, our agent learns to outperform the O3 sequence on the set of source codes used for training and achieves competitive performance on the validation set, gaining up to 1.32x speedup on previously-unseen programs. Notably, our approach differs from autotuning methods by not depending on one or more test runs of the program for making successful optimization decisions. It has no dependence on any dynamic feature, but only on the statically-attainable intermediate representation of the source code. We believe that the models trained using our approach can be integrated into modern compilers as neural optimization agents, at first to complement, and eventually replace the hand-crafted optimization sequences.",0
"Despite receiving extensive attention from the research community, the phase-ordering problem in modern compilers remains largely unsolved. Compiler developers manually design optimization sequences, selecting the optimization passes, their parameters, and order within a sequence. However, these sequences often fall short of achieving optimal runtime and may even degrade performance compared to the unoptimized version. In this paper, we propose a deep reinforcement learning approach to the phase-ordering problem, which outperforms LLVM's O3 sequence on the training set and achieves competitive performance on the validation set, resulting in up to a 1.32x speedup on previously-unseen programs. Our approach is unique in that it does not rely on autotuning methods or dynamic features, but solely on the statically-attainable intermediate representation of the source code. We believe our approach can be integrated as neural optimization agents in modern compilers to eventually replace hand-crafted optimization sequences.",1
"Objective: To combine medical knowledge and medical data to interpretably predict the risk of disease. Methods: We formulated the disease prediction task as a random walk along a knowledge graph (KG). Specifically, we build a KG to record relationships between diseases and risk factors according to validated medical knowledge. Then, a mathematical object walks along the KG. It starts walking at a patient entity, which connects the KG based on the patient current diseases or risk factors and stops at a disease entity, which represents the predicted disease. The trajectory generated by the object represents an interpretable disease progression path of the given patient. The dynamics of the object are controlled by a policy-based reinforcement learning (RL) module, which is trained by electronic health records (EHRs). Experiments: We utilized two real-world EHR datasets to evaluate the performance of our model. In the disease prediction task, our model achieves 0.743 and 0.639 in terms of macro area under the curve (AUC) in predicting 53 circulation system diseases in the two datasets, respectively. This performance is comparable to the commonly used machine learning (ML) models in medical research. In qualitative analysis, our clinical collaborator reviewed the disease progression paths generated by our model and advocated their interpretability and reliability. Conclusion: Experimental results validate the proposed model in interpretably evaluating and optimizing disease prediction. Significance: Our work contributes to leveraging the potential of medical knowledge and medical data jointly for interpretable prediction tasks.",0
"The objective of our study is to predict the risk of disease by combining medical knowledge and medical data in an interpretable manner. We adopted a random walk approach on a knowledge graph (KG) to formulate the disease prediction task. The KG records the relationship between diseases and risk factors based on validated medical knowledge. The KG is traversed by a mathematical object that starts at a patient entity and stops at a disease entity, generating a disease progression path that is interpretable. We used a policy-based reinforcement learning (RL) module trained by electronic health records (EHRs) to control the dynamics of the object. We evaluated our model using two real-world EHR datasets and achieved comparable performance to commonly used machine learning (ML) models in medical research. Our clinical collaborator reviewed the disease progression paths generated by our model and confirmed their interpretability and reliability. Our approach demonstrates the potential of joint utilization of medical knowledge and medical data for interpretable disease prediction tasks.",1
"Consequential decisions are increasingly informed by sophisticated data-driven predictive models. However, to consistently learn accurate predictive models, one needs access to ground truth labels. Unfortunately, in practice, labels may only exist conditional on certain decisions---if a loan is denied, there is not even an option for the individual to pay back the loan. Hence, the observed data distribution depends on how decisions are being made. In this paper, we show that in this selective labels setting, learning a predictor directly only from available labeled data is suboptimal in terms of both fairness and utility. To avoid this undesirable behavior, we propose to directly learn decision policies that maximize utility under fairness constraints and thereby take into account how decisions affect which data is observed in the future. Our results suggest the need for a paradigm shift in the context of fair machine learning from the currently prevalent idea of simply building predictive models from a single static dataset via risk minimization, to a more interactive notion of ""learning to decide"". In particular, such policies should not entirely neglect part of the input space, drawing connections to explore/exploit tradeoffs in reinforcement learning, data missingness, and potential outcomes in causal inference. Experiments on synthetic and real-world data illustrate the favorable properties of learning to decide in terms of utility and fairness.",0
"Sophisticated data-driven predictive models are increasingly informing consequential decisions. However, accurate predictive models require access to ground truth labels, which may only exist under certain conditions. For example, if a loan is denied, there is no option for the individual to pay it back. Consequently, the observed data distribution depends on the decision-making process. This paper argues that directly learning a predictor from available labeled data is suboptimal in terms of fairness and utility in this selective labels setting. Instead, the authors propose learning decision policies that maximize utility while considering fairness constraints, taking into account how decisions affect observed data in the future. This approach represents a paradigm shift in fair machine learning, moving from static predictive models to interactive decision-making. These policies should consider the entire input space and explore/exploit tradeoffs in reinforcement learning, data missingness, and potential outcomes in causal inference. Synthetic and real-world experiments demonstrate the advantages of this approach in terms of utility and fairness.",1
"Effective and intelligent exploration has been an unresolved problem for reinforcement learning. Most contemporary reinforcement learning relies on simple heuristic strategies such as $\epsilon$-greedy exploration or adding Gaussian noise to actions. These heuristics, however, are unable to intelligently distinguish the well explored and the unexplored regions of state space, which can lead to inefficient use of training time. We introduce entropy-based exploration (EBE) that enables an agent to explore efficiently the unexplored regions of state space. EBE quantifies the agent's learning in a state using merely state-dependent action values and adaptively explores the state space, i.e. more exploration for the unexplored region of the state space. We perform experiments on a diverse set of environments and demonstrate that EBE enables efficient exploration that ultimately results in faster learning without having to tune any hyperparameter.   The code to reproduce the experiments is given at \url{https://github.com/Usama1002/EBE-Exploration} and the supplementary video is given at \url{https://youtu.be/nJggIjjzKic}.",0
"Reinforcement learning has yet to effectively address the issue of intelligent exploration. Current approaches rely on basic heuristic strategies, such as adding Gaussian noise or using $\epsilon$-greedy exploration, which do not differentiate between well-explored and unexplored areas of the state space. Consequently, these methods can be inefficient during training. To overcome this, we present entropy-based exploration (EBE), which uses state-dependent action values to quantify an agent's learning and dynamically explore the state space, placing greater emphasis on unexplored regions. Our experiments in various environments demonstrate that EBE enables efficient exploration and faster learning, without requiring hyperparameter tuning. To reproduce our results, please refer to our code at \url{https://github.com/Usama1002/EBE-Exploration}. Additionally, you can watch our supplementary video at \url{https://youtu.be/nJggIjjzKic}.",1
"Entropy regularization is an important idea in reinforcement learning, with great success in recent algorithms like Soft Q Network (SQN) and Soft Actor-Critic (SAC1). In this work, we extend this idea into the on-policy realm. We propose the soft policy gradient theorem (SPGT) for on-policy maximum entropy reinforcement learning. With SPGT, a series of new policy optimization algorithms are derived, such as SPG, SA2C, SA3C, SDDPG, STRPO, SPPO, SIMPALA and so on. We find that SDDPG is equivalent to SAC1. For policy gradient, the policy network is often represented as a Gaussian distribution with a global action variance, which damages the representation capacity. We introduce a local action variance for policy network and find it can work collaboratively with the idea of entropy regularization. Our method outperforms prior works on a range of benchmark tasks. Furthermore, our method can be easily extended to large scale experiment with great stability and parallelism.",0
"Reinforcement learning has seen success with the incorporation of entropy regularization, as seen in recent algorithms like Soft Q Network (SQN) and Soft Actor-Critic (SAC1). This paper expands on this concept for on-policy maximum entropy reinforcement learning by introducing the soft policy gradient theorem (SPGT), which leads to the development of new policy optimization algorithms including SPG, SA2C, SA3C, SDDPG, STRPO, SPPO, SIMPALA and more. It was discovered that SDDPG is equivalent to SAC1. A common issue with policy gradient is the representation capacity of the policy network, which is often represented as a Gaussian distribution with a global action variance. To overcome this, the paper introduces a local action variance for the policy network, which works together with entropy regularization. The proposed method outperforms previous works on various benchmark tasks and can be easily scaled up with stability and parallelism.",1
"Today, human operators primarily perform voltage control of the electric transmission system. As the complexity of the grid increases, so does its operation, suggesting additional automation could be beneficial. A subset of machine learning known as deep reinforcement learning (DRL) has recently shown promise in performing tasks typically performed by humans. This paper applies DRL to the transmission voltage control problem, presents open-source DRL environments for voltage control, proposes a novel modification to the ""deep Q network"" (DQN) algorithm, and performs experiments at scale with systems up to 500 buses. The promise of applying DRL to voltage control is demonstrated, though more research is needed to enable DRL-based techniques to consistently outperform conventional methods.",0
"At present, human operators are responsible for controlling the voltage of the electric transmission system. However, as the grid becomes more complex, there is a need for increased automation. Deep reinforcement learning (DRL), a type of machine learning, has shown potential in performing tasks that are typically done by humans. This study applies DRL to the transmission voltage control problem, introduces open-source DRL environments for voltage control, proposes a new modification to the ""deep Q network"" (DQN) algorithm, and conducts experiments on systems with up to 500 buses. While DRL has demonstrated promise in voltage control, further research is necessary to determine if DRL-based techniques can consistently outperform traditional methods.",1
"Consider a typical organization whose worker agents seek to collectively cooperate for its general betterment. However, each individual agent simultaneously seeks to act to secure a larger chunk than its co-workers of the annual increment in compensation, which usually comes from a {\em fixed} pot. As such, the individual agent in the organization must cooperate and compete. Another feature of many organizations is that a worker receives a bonus, which is often a fraction of previous year's total profit. As such, the agent derives a reward that is also partly dependent on historical performance. How should the individual agent decide to act in this context? Few methods for the mixed cooperative-competitive setting have been presented in recent years, but these are challenged by problem domains whose reward functions do not depend on the current state and action only. Recent deep multi-agent reinforcement learning (MARL) methods using long short-term memory (LSTM) may be used, but these adopt a joint perspective to the interaction or require explicit exchange of information among the agents to promote cooperation, which may not be possible under competition. In this paper, we first show that the agent's decision-making problem can be modeled as an interactive partially observable Markov decision process (I-POMDP) that captures the dynamic of a history-dependent reward. We present an interactive advantage actor-critic method (IA2C$^+$), which combines the independent advantage actor-critic network with a belief filter that maintains a belief distribution over other agents' models. Empirical results show that IA2C$^+$ learns the optimal policy faster and more robustly than several other baselines including one that uses a LSTM, even when attributed models are incorrect.",0
"In a typical organization, worker agents aim to cooperate for the betterment of the organization as a whole. However, each individual agent also seeks to secure a larger portion of the fixed pot of annual compensation compared to their co-workers. This creates a situation where the individual agent must both cooperate and compete. Additionally, many organizations offer bonuses that are a fraction of the previous year's total profit, meaning that the agent's reward is partly dependent on past performance. This context raises the question of how an individual agent should act. While some methods for mixed cooperative-competitive settings have been proposed, they struggle with reward functions that don't solely rely on the current state and action. Recent MARL methods using LSTM have also been explored, but these require agents to exchange information to promote cooperation, which may not be feasible in competitive situations. This paper proposes modeling the agent's decision-making problem as an I-POMDP that captures the dynamic nature of history-dependent rewards. An interactive advantage actor-critic method (IA2C$^+$) is presented, which combines an independent advantage actor-critic network with a belief filter that maintains a distribution over other agents' models. Empirical results show that IA2C$^+$ learns the optimal policy faster and more robustly than several other baselines, including one that uses LSTM, even when models are incorrect.",1
"For deep neural network accelerators, memory movement is both energetically expensive and can bound computation. Therefore, optimal mapping of tensors to memory hierarchies is critical to performance. The growing complexity of neural networks calls for automated memory mapping instead of manual heuristic approaches; yet the search space of neural network computational graphs have previously been prohibitively large. We introduce Evolutionary Graph Reinforcement Learning (EGRL), a method designed for large search spaces, that combines graph neural networks, reinforcement learning, and evolutionary search. A set of fast, stateless policies guide the evolutionary search to improve its sample-efficiency. We train and validate our approach directly on the Intel NNP-I chip for inference. EGRL outperforms policy-gradient, evolutionary search and dynamic programming baselines on BERT, ResNet-101 and ResNet-50. We additionally achieve 28-78\% speed-up compared to the native NNP-I compiler on all three workloads.",0
"In order to optimize performance for deep neural network accelerators, it is crucial to consider the cost and constraints of memory movement during computation. While manual heuristic approaches have been used in the past to map tensors to memory hierarchies, the increasing complexity of neural networks necessitates an automated approach. However, the large search space of neural network computational graphs has made this challenging. To address this issue, we propose Evolutionary Graph Reinforcement Learning (EGRL), which utilizes graph neural networks, reinforcement learning, and evolutionary search to handle large search spaces. Our approach includes a set of fast, stateless policies to guide the evolutionary search and improve its sample-efficiency. We train and evaluate EGRL on the Intel NNP-I chip for inference and demonstrate its superiority over policy-gradient, evolutionary search, and dynamic programming baselines on BERT, ResNet-101, and ResNet-50. Furthermore, we achieve a speed-up of 28-78\% compared to the native NNP-I compiler for all three workloads.",1
"Feed-forward neural networks consist of a sequence of layers, in which each layer performs some processing on the information from the previous layer. A downside to this approach is that each layer (or module, as multiple modules can operate in parallel) is tasked with processing the entire hidden state, rather than a particular part of the state which is most relevant for that module. Methods which only operate on a small number of input variables are an essential part of most programming languages, and they allow for improved modularity and code re-usability. Our proposed method, Neural Function Modules (NFM), aims to introduce the same structural capability into deep learning. Most of the work in the context of feed-forward networks combining top-down and bottom-up feedback is limited to classification problems. The key contribution of our work is to combine attention, sparsity, top-down and bottom-up feedback, in a flexible algorithm which, as we show, improves the results in standard classification, out-of-domain generalization, generative modeling, and learning representations in the context of reinforcement learning.",0
"Sequential layers make up feed-forward neural networks, with each layer processing information from the previous layer. However, this approach has a drawback in that each module must process the entire hidden state instead of only the relevant part. Programming languages rely on methods that operate on a limited number of input variables for better modularity and code re-usability. Our proposed method, Neural Function Modules (NFM), introduces this capability to deep learning. Most feed-forward network research combines top-down and bottom-up feedback for classification issues. Our work's significant contribution is a flexible algorithm that combines attention, sparsity, top-down and bottom-up feedback, improving standard classification, out-of-domain generalization, generative modeling, and reinforcement learning representation learning.",1
"We tackle tag-based query refinement as a mobile-friendly alternative to standard facet search. We approach the inference challenge with reinforcement learning, and propose a deep contextual bandit that can be efficiently scaled in a multi-tenant SaaS scenario.",0
Our approach to enhancing tag-based query refinement for mobile devices involves utilizing reinforcement learning to address the inference challenge. We suggest implementing a deep contextual bandit that can be easily scaled in a multi-tenant SaaS environment. This provides a more user-friendly alternative to traditional facet search.,1
"Meta-learning or few-shot learning, has been successfully applied in a wide range of domains from computer vision to reinforcement learning. Among the many frameworks proposed for meta-learning, bayesian methods are particularly favoured when accurate and calibrated uncertainty estimate is required. In this paper, we investigate the similarities and disparities among two recently published bayesian meta-learning methods: ALPaCA (Harrison et al. [2018]) and PACOH (Rothfuss et al. [2020]). We provide theoretical analysis as well as empirical benchmarks across synthetic and real-world dataset. While ALPaCA holds advantage in computation time by the usage of a linear kernel, general GP-based methods provide much more flexibility and achieves better result across datasets when using a common kernel such as SE (Squared Exponential) kernel. The influence of different loss function choice is also discussed.",0
"Few-shot learning, also known as meta-learning, has been applied successfully in various fields, including computer vision and reinforcement learning. Bayesian methods are highly preferred in meta-learning frameworks when there is a need for precise and calibrated uncertainty estimates. This study aims to explore the similarities and differences between two recently introduced Bayesian meta-learning methods, namely ALPaCA and PACOH. Through theoretical analysis and empirical benchmarks on both synthetic and real-world datasets, we demonstrate that while ALPaCA has an advantage in computation time due to its linear kernel, general Gaussian Process-based methods offer greater flexibility and better results across datasets when employing a common kernel like the Squared Exponential kernel. Additionally, we discuss the impact of different loss function choices.",1
"The ability to compose learned skills to solve new tasks is an important property of lifelong-learning agents. In this work, we formalise the logical composition of tasks as a Boolean algebra. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. We then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. We prove that by composing these value functions in specific ways, we immediately recover the optimal policies for all tasks expressible under the Boolean algebra. We verify our approach in two domains---including a high-dimensional video game environment requiring function approximation---where an agent first learns a set of base skills, and then composes them to solve a super-exponential number of new tasks.",0
"The capacity of lifelong-learning agents to apply acquired skills to tackle novel tasks is a crucial attribute. This study establishes a formal framework for logically combining tasks using Boolean algebra. This enables the creation of new tasks through the negation, disjunction, and conjunction of a set of fundamental tasks. By training goal-directed value functions and constraining the task's transition dynamics, an agent can solve these new tasks without further training. Our findings demonstrate that by combining these value functions in particular ways, optimal policies for all tasks that conform to the Boolean algebra can be obtained immediately. We validate our approach in two domains, including a video game environment with high-dimensional features that require function approximation. In this environment, an agent learns a set of fundamental skills before using them to solve an exponential number of new tasks.",1
"State-of-the-art deep learning algorithms mostly rely on gradient backpropagation to train a deep artificial neural network, which is generally regarded to be biologically implausible. For a network of stochastic units trained on a reinforcement learning task or a supervised learning task, one biologically plausible way of learning is to train each unit by REINFORCE. In this case, only a global reward signal has to be broadcast to all units, and the learning rule given is local, which can be interpreted as reward-modulated spike-timing-dependent plasticity (R-STDP) that is observed biologically. Although this learning rule follows the gradient of return in expectation, it suffers from high variance and cannot be used to train a deep network in practice. In this paper, we propose an algorithm called MAP propagation that can reduce this variance significantly while retaining the local property of learning rule. Different from prior works on local learning rules (e.g. Contrastive Divergence) which mostly applies to undirected models in unsupervised learning tasks, our proposed algorithm applies to directed models in reinforcement learning tasks. We show that the newly proposed algorithm can solve common reinforcement learning tasks at a speed similar to that of backpropagation when applied to an actor-critic network.",0
"The current advanced deep learning algorithms typically rely on gradient backpropagation to train a deep artificial neural network, which is not biologically plausible. One possible biologically plausible learning method for a network of stochastic units trained in a reinforcement or supervised learning task is to use the REINFORCE method to train each unit. In this approach, a global reward signal is broadcast to all units, and the learning rule is local, which is similar to reward-modulated spike-timing-dependent plasticity observed in biological systems. However, this learning rule has high variance and is not practical for training deep networks. In this study, we introduce a new algorithm called MAP propagation, which reduces the variance significantly while maintaining the local learning rule property. Unlike previous approaches that apply to undirected models in unsupervised learning tasks, our algorithm applies to directed models in reinforcement learning tasks. Our results show that the proposed algorithm can solve common reinforcement learning tasks at a similar speed to backpropagation when applied to an actor-critic network.",1
"We propose a general framework for policy representation for reinforcement learning tasks. This framework involves finding a low-dimensional embedding of the policy on a reproducing kernel Hilbert space (RKHS). The usage of RKHS based methods allows us to derive strong theoretical guarantees on the expected return of the reconstructed policy. Such guarantees are typically lacking in black-box models, but are very desirable in tasks requiring stability. We conduct several experiments on classic RL domains. The results confirm that the policies can be robustly embedded in a low-dimensional space while the embedded policy incurs almost no decrease in return.",0
"Our proposal is a comprehensive model for representing policies in reinforcement learning tasks. The model entails identifying a low-dimensional embedding of the policy on a reproducing kernel Hilbert space (RKHS). The application of RKHS techniques enables us to establish reliable theoretical assurances on the anticipated return of the reconstructed policy. These assurances are often absent in opaque models, but are highly coveted in tasks that demand stability. We conduct multiple experiments on traditional RL domains, which demonstrate that policies can be securely embedded in a low-dimensional space and the embedded policy experiences minimal reduction in return.",1
"Multi-agent reinforcement learning has recently shown great promise as an approach to networked system control. Arguably, one of the most difficult and important tasks for which large scale networked system control is applicable is common-pool resource management. Crucial common-pool resources include arable land, fresh water, wetlands, wildlife, fish stock, forests and the atmosphere, of which proper management is related to some of society's greatest challenges such as food security, inequality and climate change. Here we take inspiration from a recent research program investigating the game-theoretic incentives of humans in social dilemma situations such as the well-known tragedy of the commons. However, instead of focusing on biologically evolved human-like agents, our concern is rather to better understand the learning and operating behaviour of engineered networked systems comprising general-purpose reinforcement learning agents, subject only to nonbiological constraints such as memory, computation and communication bandwidth. Harnessing tools from empirical game-theoretic analysis, we analyse the differences in resulting solution concepts that stem from employing different information structures in the design of networked multi-agent systems. These information structures pertain to the type of information shared between agents as well as the employed communication protocol and network topology. Our analysis contributes new insights into the consequences associated with certain design choices and provides an additional dimension of comparison between systems beyond efficiency, robustness, scalability and mean control performance.",0
"The use of multi-agent reinforcement learning has emerged as a promising method for controlling networked systems. An essential and challenging task for networked system control is managing common-pool resources, including arable land, fresh water, wetlands, wildlife, fish stock, forests, and the atmosphere. Properly managing these resources is crucial for addressing significant societal issues such as climate change, inequality, and food security. Our research draws inspiration from recent studies on game-theoretic incentives of humans in social dilemma situations, such as the tragedy of the commons. However, our focus is on understanding the learning and operating behavior of engineered networked systems made up of general-purpose reinforcement learning agents, subject to nonbiological limitations like memory, computation, and communication bandwidth. Using empirical game-theoretic analysis tools, we examine the differences in solution concepts that arise from using different information structures in designing networked multi-agent systems. These information structures include the type of information shared between agents, communication protocol, and network topology. Our research provides new insights into the consequences of certain design choices and offers an additional dimension for comparing systems beyond efficiency, robustness, scalability, and mean control performance.",1
Deep Q-learning method is one of the most popularly used deep reinforcement learning algorithms which uses deep neural networks to approximate the estimation of the action-value function. Training of the deep Q-network (DQN) is usually restricted to first order gradient based methods. This paper attempts to accelerate the training of deep Q-networks by introducing a second order Nesterov's accelerated quasi-Newton method. We evaluate the performance of the proposed method on deep reinforcement learning using double DQNs for global routing. The results show that the proposed method can obtain better routing solutions compared to the DQNs trained with first order Adam and RMSprop methods.,0
"The widely-used deep reinforcement learning algorithm, Deep Q-learning method, leverages deep neural networks to approximate the action-value function. Typically, first-order gradient based methods constrain DQN training. This study aims to expedite DQN training through the introduction of a second-order Nesterov's accelerated quasi-Newton method. Our experiment demonstrates the performance of this method on double DQNs for global routing in deep reinforcement learning. The findings indicate that our proposed approach results in better routing solutions than DQNs trained with first-order Adam and RMSprop methods.",1
"We develop a data-driven, model-free approach for the optimal control of the dynamical system. The proposed approach relies on the Deep Neural Network (DNN) based learning of Koopman operator for the purpose of control. In particular, DNN is employed for the data-driven identification of basis function used in the linear lifting of nonlinear control system dynamics. The controller synthesis is purely data-driven and does not rely on a priori domain knowledge. The OpenAI Gym environment, employed for Reinforcement Learning-based control design, is used for data generation and learning of Koopman operator in control setting. The method is applied to two classic dynamical systems on OpenAI Gym environment to demonstrate the capability.",0
"We present a novel method for optimal control of a dynamical system, which is data-driven and model-free. Our proposed approach utilizes the Deep Neural Network (DNN) to learn the Koopman operator for control purposes. Specifically, we use the DNN to identify basis functions for the linear lifting of nonlinear control system dynamics. This data-driven approach does not require any prior knowledge of the domain and is solely based on the collected data. We use the OpenAI Gym environment for Reinforcement Learning-based control design, which generates data and trains the Koopman operator. We apply our approach to two classic dynamical systems in the OpenAI Gym environment to showcase its effectiveness.",1
"We consider reinforcement learning (RL) in episodic Markov decision processes (MDPs) with linear function approximation under drifting environment. Specifically, both the reward and state transition functions can evolve over time, as long as their respective total variations, quantified by suitable metrics, do not exceed certain \textit{variation budgets}. We first develop the $\texttt{LSVI-UCB-Restart}$ algorithm, an optimistic modification of least-squares value iteration combined with periodic restart, and establish its dynamic regret bound when variation budgets are known. We then propose a parameter-free algorithm, $\texttt{Ada-LSVI-UCB-Restart}$, that works without knowing the variation budgets, but with a slightly worse dynamic regret bound. We also derive the first minimax dynamic regret lower bound for nonstationary MDPs to show that our proposed algorithms are near-optimal. As a byproduct, we establish a minimax regret lower bound for linear MDPs, which is unsolved by \cite{jin2020provably}. In addition, we provide numerical experiments to demonstrate the effectiveness of our proposed algorithms. As far as we know, this is the first dynamic regret analysis in nonstationary reinforcement learning with function approximation.",0
"We examine reinforcement learning (RL) in episodic Markov decision processes (MDPs) with linear function approximation in a changing environment. The reward and state transition functions may vary over time, but their total variations must not surpass specific variation budgets, as measured by relevant metrics. We introduce the $\texttt{LSVI-UCB-Restart}$ algorithm that combines least-squares value iteration with periodic restart and has an optimistic approach. We establish its dynamic regret bound when the variation budgets are known. We then propose the $\texttt{Ada-LSVI-UCB-Restart}$ algorithm that operates without knowledge of the variation budgets but has a slightly worse dynamic regret bound. We demonstrate that our proposed algorithms are near-optimal by deriving the first minimax dynamic regret lower bound for nonstationary MDPs. We also establish a minimax regret lower bound for linear MDPs, which \cite{jin2020provably} has not yet solved. Finally, we provide numerical experiments to demonstrate the effectiveness of our proposed algorithms. This is the first dynamic regret analysis in nonstationary reinforcement learning with function approximation that we are aware of.",1
"In recent years, many spatial-temporal graph convolutional network (STGCN) models are proposed to deal with the spatial-temporal network data forecasting problem. These STGCN models have their own advantages, i.e., each of them puts forward many effective operations and achieves good prediction results in the real applications. If users can effectively utilize and combine these excellent operations integrating the advantages of existing models, then they may obtain more effective STGCN models thus create greater value using existing work. However, they fail to do so due to the lack of domain knowledge, and there is lack of automated system to help users to achieve this goal. In this paper, we fill this gap and propose Auto-STGCN algorithm, which makes use of existing models to automatically explore high-performance STGCN model for specific scenarios. Specifically, we design Unified-STGCN framework, which summarizes the operations of existing architectures, and use parameters to control the usage and characteristic attributes of each operation, so as to realize the parameterized representation of the STGCN architecture and the reorganization and fusion of advantages. Then, we present Auto-STGCN, an optimization method based on reinforcement learning, to quickly search the parameter search space provided by Unified-STGCN, and generate optimal STGCN models automatically. Extensive experiments on real-world benchmark datasets show that our Auto-STGCN can find STGCN models superior to existing STGCN models with heuristic parameters, which demonstrates the effectiveness of our proposed method.",0
"Over the years, various models of spatial-temporal graph convolutional networks (STGCN) have been introduced to address the issue of predicting spatial-temporal network data. Each of these models has its unique benefits and has been effective in real-world applications. By combining these different models and their operations, users can create more efficient STGCN models and generate more value from existing work. Unfortunately, users often lack the domain knowledge and automated systems needed to effectively integrate these models. To address this issue, we introduce the Auto-STGCN algorithm, which uses existing models to create high-performance STGCN models for specific scenarios. We present the Unified-STGCN framework, which summarizes the operations of existing architectures and uses parameters to control the usage and characteristics of each operation. This allows for the parameterized representation of the STGCN architecture and the reorganization and fusion of advantages. We also introduce Auto-STGCN, an optimization method based on reinforcement learning, to quickly search the parameter search space provided by Unified-STGCN and generate optimal STGCN models automatically. Our experiments on real-world benchmark datasets show that our Auto-STGCN can find STGCN models superior to existing STGCN models with heuristic parameters, demonstrating the effectiveness of our proposed method.",1
"Improving sample efficiency is a key research problem in reinforcement learning (RL), and CURL, which uses contrastive learning to extract high-level features from raw pixels of individual video frames, is an efficient algorithm~\citep{srinivas2020curl}. We observe that consecutive video frames in a game are highly correlated but CURL deals with them independently. To further improve data efficiency, we propose a new algorithm, masked contrastive representation learning for RL, that takes the correlation among consecutive inputs into consideration. In addition to the CNN encoder and the policy network in CURL, our method introduces an auxiliary Transformer module to leverage the correlations among video frames. During training, we randomly mask the features of several frames, and use the CNN encoder and Transformer to reconstruct them based on the context frames. The CNN encoder and Transformer are jointly trained via contrastive learning where the reconstructed features should be similar to the ground-truth ones while dissimilar to others. During inference, the CNN encoder and the policy network are used to take actions, and the Transformer module is discarded. Our method achieves consistent improvements over CURL on $14$ out of $16$ environments from DMControl suite and $21$ out of $26$ environments from Atari 2600 Games. The code is available at https://github.com/teslacool/m-curl.",0
"Reinforcement learning (RL) faces the challenge of enhancing sample efficiency, which is tackled by CURL, an algorithm that employs contrastive learning to extract high-level features from raw pixels of individual video frames. However, CURL treats consecutive video frames independently despite their high correlation in a game. In order to further enhance data efficiency, we propose a new algorithm, masked contrastive representation learning for RL, which takes into account the correlation among consecutive inputs. Our method introduces an auxiliary Transformer module in addition to the CNN encoder and the policy network in CURL, to leverage the correlations among video frames. During training, we randomly mask the features of several frames and use the CNN encoder and Transformer to reconstruct them based on the context frames. The CNN encoder and Transformer are jointly trained via contrastive learning to ensure that the reconstructed features are similar to the ground-truth ones while dissimilar to others. During inference, only the CNN encoder and the policy network are used to take actions, and the Transformer module is not utilized. Our approach demonstrates consistent improvements over CURL on $14$ out of $16$ environments from DMControl suite and $21$ out of $26$ environments from Atari 2600 Games. The code can be found at https://github.com/teslacool/m-curl.",1
"Policy gradient methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution or how they cope with approximation error due to using a restricted class of parametric policies. This work provides provable characterizations of the computational, approximation, and sample size properties of policy gradient methods in the context of discounted Markov Decision Processes (MDPs). We focus on both: ""tabular"" policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy; and parametric policy classes (considering both log-linear and neural policy classes), which may not contain the optimal policy and where we provide agnostic learning results. One central contribution of this work is in providing approximation guarantees that are average case -- which avoid explicit worst-case dependencies on the size of state space -- by making a formal connection to supervised learning under distribution shift. This characterization shows an important interplay between estimation error, approximation error, and exploration (as characterized through a precisely defined condition number).",0
"Policy gradient methods are effective in solving challenging reinforcement learning problems with large state and/or action spaces, but their basic theoretical convergence properties remain largely unknown. It is unclear whether they converge to a globally optimal solution or how they handle approximation error when using a restricted class of parametric policies. This study aims to provide provable characterizations of the computational, approximation, and sample size properties of policy gradient methods in discounted Markov Decision Processes. The focus is on both ""tabular"" policy parameterizations, where global convergence to the optimal policy is shown, and parametric policy classes, such as log-linear and neural policy classes, where agnostic learning results are provided. The study also presents average-case approximation guarantees that do not have explicit worst-case dependencies on the size of the state space by connecting formal supervised learning under distribution shift. The characterization reveals a significant interplay between estimation error, approximation error, and exploration through a precisely defined condition number.",1
"We consider reinforcement learning (RL) in Markov Decision Processes in which an agent repeatedly interacts with an environment that is modeled by a controlled Markov process. At each time step $t$, it earns a reward, and also incurs a cost-vector consisting of $M$ costs. We design learning algorithms that maximize the cumulative reward earned over a time horizon of $T$ time-steps, while simultaneously ensuring that the average values of the $M$ cost expenditures are bounded by agent-specified thresholds $c^{ub}_i,i=1,2,\ldots,M$. The considerations on the cumulative cost expenditures departs from the existing literature, in that the agent now additionally needs to balance the cost expenses in an online manner, while simultaneously performing the exploration-exploitation trade-off that is typically encountered in RL tasks.   In order to measure the performance of a reinforcement learning algorithm that satisfies the average cost constraints, we define an $M+1$ dimensional regret vector that is composed of its reward regret, and $M$ cost regrets. The reward regret measures the sub-optimality in the cumulative reward, while the $i$-th component of the cost regret vector is the difference between its $i$-th cumulative cost expense and the expected cost expenditures $Tc^{ub}_i$. We prove that with a high probablity, the regret vector of UCRL-CMDP is upper-bounded as $O\left( S\sqrt{AT^{1.5}\log(T)}\right)$, where $S$ is the number of states, $A$ is the number of actions, and $T$ is the time horizon. We further show how to reduce the regret of a desired subset of the $M$ costs, at the expense of increasing the regrets of rewards and the remaining costs. To the best of our knowledge, ours is the only work that considers non-episodic RL under average cost constraints, and derive algorithms that can~\emph{tune the regret vector} according to the agent's requirements on its cost regrets.",0
"Our focus is on reinforcement learning (RL) in Markov Decision Processes, where a controlled Markov process models the agent's interaction with the environment. Each time step, the agent earns a reward and incurs a cost-vector consisting of $M$ costs. Our aim is to develop learning algorithms that maximize the cumulative reward over $T$ time-steps while ensuring that the average values of the $M$ cost expenditures do not exceed a set of agent-specified thresholds $c^{ub}_i, i=1,2,\ldots,M$. This approach differs from existing literature as it requires the agent to balance cost expenses in an online manner while performing the exploration-exploitation trade-off typical in RL tasks. To evaluate the performance of such algorithms, we define an $M+1$ dimensional regret vector that measures reward and cost regrets. We prove that UCRL-CMDP has an upper-bound regret vector of $O\left( S\sqrt{AT^{1.5}\log(T)}\right)$, where $S$ and $A$ are the number of states and actions, respectively. We also show how to reduce the regret of a desired subset of costs while increasing the regrets of rewards and the remaining costs. Our work is the only one to consider non-episodic RL under average cost constraints and derive algorithms that can tune the regret vector to meet the agent's requirements for cost regrets.",1
"The beer game is a widely used in-class game that is played in supply chain management classes to demonstrate the bullwhip effect. The game is a decentralized, multi-agent, cooperative problem that can be modeled as a serial supply chain network in which agents cooperatively attempt to minimize the total cost of the network even though each agent can only observe its own local information. Each agent chooses order quantities to replenish its stock. Under some conditions, a base-stock replenishment policy is known to be optimal. However, in a decentralized supply chain in which some agents (stages) may act irrationally (as they do in the beer game), there is no known optimal policy for an agent wishing to act optimally.   We propose a machine learning algorithm, based on deep Q-networks, to optimize the replenishment decisions at a given stage. When playing alongside agents who follow a base-stock policy, our algorithm obtains near-optimal order quantities. It performs much better than a base-stock policy when the other agents use a more realistic model of human ordering behavior. Unlike most other algorithms in the literature, our algorithm does not have any limits on the beer game parameter values. Like any deep learning algorithm, training the algorithm can be computationally intensive, but this can be performed ahead of time; the algorithm executes in real time when the game is played. Moreover, we propose a transfer learning approach so that the training performed for one agent and one set of cost coefficients can be adapted quickly for other agents and costs. Our algorithm can be extended to other decentralized multi-agent cooperative games with partially observed information, which is a common type of situation in real-world supply chain problems.",0
"Supply chain management classes often use the beer game as a teaching tool to illustrate the bullwhip effect. This game involves a decentralized, multi-agent, cooperative problem where agents work together to minimize the network's total cost, even though they only have access to their own local information. Each agent decides on the order quantity to replenish their stock, and a base-stock replenishment policy is considered optimal. However, when dealing with irrational agents, as in the beer game, no known optimal policy exists. To address this, we propose a deep Q-network machine learning algorithm that optimizes replenishment decisions for a given stage. Our algorithm outperforms the base-stock policy when other agents follow a more realistic ordering model. It has no restrictions on beer game parameter values and can be trained ahead of time for real-time execution during gameplay. Additionally, we suggest a transfer learning approach to adapt the algorithm quickly for other agents and costs. This algorithm can be extended to other decentralized multi-agent cooperative games with partially observed information, which is common in real-world supply chain problems.",1
"Generative models are increasingly able to produce remarkably high quality images and text. The community has developed numerous evaluation metrics for comparing generative models. However, these metrics do not effectively quantify data diversity. We develop a new diversity metric that can readily be applied to data, both synthetic and natural, of any type. Our method employs random network distillation, a technique introduced in reinforcement learning. We validate and deploy this metric on both images and text. We further explore diversity in few-shot image generation, a setting which was previously difficult to evaluate.",0
"The quality of images and text produced by generative models has significantly improved, and the community has created various evaluation measures for comparing them. However, these measures do not accurately assess data diversity. To address this issue, we have devised a novel diversity metric that can be used on any type of data, synthetic or natural. Our approach utilizes random network distillation, which was first introduced in reinforcement learning. We have verified and implemented this metric on both images and text. Additionally, we have investigated diversity in few-shot image generation, an area that was previously challenging to evaluate.",1
"In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.",0
"In order to ensure safety, an agent's actions may be limited. If the environment is well-known, a safe controller can be created beforehand, but this is not possible with learning. Reinforcement learned (RL) controllers require exploration, which can be dangerous in critical situations. To incorporate safety methods from software engineering and controller synthesis, we investigate the advantages of structuring the constraints of a constrained Markov decision process using formal languages. Finite automata are used to quickly identify constraint violations, and constraint states are added to the MDP state to facilitate learning a dense cost function. We evaluate the impact of these techniques on various RL algorithms across multiple constraints in Safety Gym, MuJoCo, and Atari environments.",1
"Existing model-based reinforcement learning methods often study perception modeling and decision making separately. We introduce joint Perception and Control as Inference (PCI), a general framework to combine perception and control for partially observable environments through Bayesian inference. Based on the fact that object-level inductive biases are critical in human perceptual learning and reasoning, we propose Object-based Perception Control (OPC), an instantiation of PCI which manages to facilitate control using automatic discovered object-based representations. We develop an unsupervised end-to-end solution and analyze the convergence of the perception model update. Experiments in a high-dimensional pixel environment demonstrate the learning effectiveness of our object-based perception control approach. Specifically, we show that OPC achieves good perceptual grouping quality and outperforms several strong baselines in accumulated rewards.",0
"Typically, model-based reinforcement learning methods approach perception modeling and decision making as two distinct areas of study. However, we have introduced a novel approach called joint Perception and Control as Inference (PCI) that combines perception and control for partially observable environments using Bayesian inference. Our approach, Object-based Perception Control (OPC), is based on the notion that object-level inductive biases are crucial in human perceptual learning and reasoning. OPC leverages automatic discovered object-based representations to facilitate control. We have developed an unsupervised end-to-end solution and have analyzed the convergence of the perception model update. Our experiments in a high-dimensional pixel environment have demonstrated the effectiveness of our approach. OPC has achieved good perceptual grouping quality and has outperformed several strong baselines in accumulated rewards.",1
"In this paper we investigate the use of model-based reinforcement learning to assist people with Type 1 Diabetes with insulin dose decisions. The proposed architecture consists of multiple Echo State Networks to predict blood glucose levels combined with Model Predictive Controller for planning. Echo State Network is a version of recurrent neural networks which allows us to learn long term dependencies in the input of time series data in an online manner. Additionally, we address the quantification of uncertainty for a more robust control. Here, we used ensembles of Echo State Networks to capture model (epistemic) uncertainty. We evaluated the approach with the FDA-approved UVa/Padova Type 1 Diabetes simulator and compared the results against baseline algorithms such as Basal-Bolus controller and Deep Q-learning. The results suggest that the model-based reinforcement learning algorithm can perform equally or better than the baseline algorithms for the majority of virtual Type 1 Diabetes person profiles tested.",0
"The aim of this study is to explore the potential of model-based reinforcement learning as a tool to aid individuals with Type 1 Diabetes in making informed insulin dosage decisions. The proposed architecture incorporates multiple Echo State Networks to predict blood glucose levels and a Model Predictive Controller for planning. Echo State Network is a recurrent neural network that facilitates the learning of long-term dependencies in time series data in real-time. Moreover, the study addresses the issue of uncertainty quantification for more reliable control. To accomplish this, ensembles of Echo State Networks are utilized to capture model uncertainty. The FDA-approved UVa/Padova Type 1 Diabetes simulator was used to evaluate the effectiveness of the approach, and its performance was compared with that of established algorithms such as Basal-Bolus controller and Deep Q-learning. The results indicate that the model-based reinforcement learning algorithm can achieve similar or better outcomes than the baseline algorithms for most virtual Type 1 Diabetes cases examined.",1
"Deep reinforcement learning (DRL) is an emerging methodology that is transforming the way many complicated transportation decision-making problems are tackled. Researchers have been increasingly turning to this powerful learning-based methodology to solve challenging problems across transportation fields. While many promising applications have been reported in the literature, there remains a lack of comprehensive synthesis of the many DRL algorithms and their uses and adaptations. The objective of this paper is to fill this gap by conducting a comprehensive, synthesized review of DRL applications in transportation. We start by offering an overview of the DRL mathematical background, popular and promising DRL algorithms, and some highly effective DRL extensions. Building on this overview, a systematic investigation of about 150 DRL studies that have appeared in the transportation literature, divided into seven different categories, is performed. Building on this review, we continue to examine the applicability, strengths, shortcomings, and common and application-specific issues of DRL techniques with regard to their applications in transportation. In the end, we recommend directions for future research and present available resources for actually implementing DRL.",0
"The use of deep reinforcement learning (DRL) is revolutionizing the approach to complex transportation decision-making problems. Researchers increasingly rely on this powerful learning-based methodology to address challenging issues across various transportation fields. However, despite the promising outcomes reported in literature, a lack of synthesis exists regarding the numerous DRL algorithms and their uses and adaptations. This paper aims to address this gap by conducting a comprehensive review of DRL applications in transportation. The review begins with a presentation of the DRL mathematical background, popular and promising algorithms, and highly effective extensions. With this groundwork, the review systematically investigates about 150 DRL studies in transportation literature, organized into seven categories. The study then analyzes the applicability, strengths, shortcomings, and common and application-specific issues of DRL techniques in transportation. Finally, the paper recommends future research directions and provides available resources for DRL implementation.",1
"We propose AttendLight, an end-to-end Reinforcement Learning (RL) algorithm for the problem of traffic signal control. Previous approaches for this problem have the shortcoming that they require training for each new intersection with a different structure or traffic flow distribution. AttendLight solves this issue by training a single, universal model for intersections with any number of roads, lanes, phases (possible signals), and traffic flow. To this end, we propose a deep RL model which incorporates two attention models. The first attention model is introduced to handle different numbers of roads-lanes; and the second attention model is intended for enabling decision-making with any number of phases in an intersection. As a result, our proposed model works for any intersection configuration, as long as a similar configuration is represented in the training set. Experiments were conducted with both synthetic and real-world standard benchmark data-sets. The results we show cover intersections with three or four approaching roads; one-directional/bi-directional roads with one, two, and three lanes; different number of phases; and different traffic flows. We consider two regimes: (i) single-environment training, single-deployment, and (ii) multi-environment training, multi-deployment. AttendLight outperforms both classical and other RL-based approaches on all cases in both regimes.",0
"AttendLight is a novel end-to-end Reinforcement Learning (RL) algorithm proposed for traffic signal control. Prior methods for this task have a limitation in that they require training for each new intersection with a varying structure or traffic flow distribution. To overcome this issue, AttendLight utilizes a single, universal model that can handle intersections with any number of roads, lanes, phases (i.e., signals), and traffic flows. The proposed deep RL model incorporates two attention models: the first one handles varying numbers of roads-lanes, and the second one enables decision-making with any number of phases. Thus, the model can work with any intersection configuration, provided that a similar configuration is present in the training set. Experiments were conducted with both synthetic and real-world standard benchmark data-sets, covering various intersection configurations. AttendLight outperformed classical and other RL-based approaches on all cases in both single-environment training, single-deployment, and multi-environment training, multi-deployment regimes.",1
"Although Q-learning is one of the most successful algorithms for finding the best action-value function (and thus the optimal policy) in reinforcement learning, its implementation often suffers from large overestimation of Q-function values incurred by random sampling. The double Q-learning algorithm proposed in~\citet{hasselt2010double} overcomes such an overestimation issue by randomly switching the update between two Q-estimators, and has thus gained significant popularity in practice. However, the theoretical understanding of double Q-learning is rather limited. So far only the asymptotic convergence has been established, which does not characterize how fast the algorithm converges. In this paper, we provide the first non-asymptotic (i.e., finite-time) analysis for double Q-learning. We show that both synchronous and asynchronous double Q-learning are guaranteed to converge to an $\epsilon$-accurate neighborhood of the global optimum by taking $\tilde{\Omega}\left(\left( \frac{1}{(1-\gamma)^6\epsilon^2}\right)^{\frac{1}{\omega}} +\left(\frac{1}{1-\gamma}\right)^{\frac{1}{1-\omega}}\right)$ iterations, where $\omega\in(0,1)$ is the decay parameter of the learning rate, and $\gamma$ is the discount factor. Our analysis develops novel techniques to derive finite-time bounds on the difference between two inter-connected stochastic processes, which is new to the literature of stochastic approximation.",0
"Q-learning is a highly effective algorithm for discovering the optimal policy in reinforcement learning by identifying the best action-value function. However, the implementation of this algorithm often results in significant overestimation of Q-function values due to random sampling. The double Q-learning algorithm proposed by~\citet{hasselt2010double} avoids this issue by randomly alternating the update between two Q-estimators, making it a popular choice in practice. However, the theoretical understanding of double Q-learning is limited to asymptotic convergence, which fails to describe how fast the algorithm converges. In this paper, we present the first non-asymptotic analysis of double Q-learning. We demonstrate that both synchronous and asynchronous double Q-learning will converge to an $\epsilon$-accurate neighborhood of the global optimum after $\tilde{\Omega}\left(\left( \frac{1}{(1-\gamma)^6\epsilon^2}\right)^{\frac{1}{\omega}} +\left(\frac{1}{1-\gamma}\right)^{\frac{1}{1-\omega}}\right)$ iterations, where $\omega\in(0,1)$ is the decay parameter of the learning rate, and $\gamma$ is the discount factor. Our analysis introduces new techniques for deriving finite-time bounds on the difference between two connected stochastic processes, which is a new contribution to the literature on stochastic approximation.",1
"We present an algorithm for local, regularized, policy improvement in reinforcement learning (RL) that allows us to formulate model-based and model-free variants in a single framework. Our algorithm can be interpreted as a natural extension of work on KL-regularized RL and introduces a form of tree search for continuous action spaces. We demonstrate that additional computation spent on model-based policy improvement during learning can improve data efficiency, and confirm that model-based policy improvement during action selection can also be beneficial. Quantitatively, our algorithm improves data efficiency on several continuous control benchmarks (when a model is learned in parallel), and it provides significant improvements in wall-clock time in high-dimensional domains (when a ground truth model is available). The unified framework also helps us to better understand the space of model-based and model-free algorithms. In particular, we demonstrate that some benefits attributed to model-based RL can be obtained without a model, simply by utilizing more computation.",0
"A new algorithm is introduced for reinforcement learning (RL) that enables us to create both model-based and model-free variations within a single structure. This algorithm builds on previous work on KL-regularized RL and incorporates a type of tree search for continuous action spaces. The results show that spending more time on model-based policy improvement during learning can enhance data efficiency, while model-based policy improvement during action selection can also be advantageous. The algorithm improves data efficiency on various continuous control benchmarks and reduces wall-clock time in high-dimensional domains. Furthermore, the unified framework allows for a better understanding of the model-based and model-free algorithms, and it is found that certain benefits of model-based RL can be achieved without a model by using additional computation.",1
"Deep reinforcement learning (DRL) has been shown to be successful in many application domains. Combining recurrent neural networks (RNNs) and DRL further enables DRL to be applicable in non-Markovian environments by capturing temporal information. However, training of both DRL and RNNs is known to be challenging requiring a large amount of training data to achieve convergence. In many targeted applications, such as those used in the fifth generation (5G) cellular communication, the environment is highly dynamic while the available training data is very limited. Therefore, it is extremely important to develop DRL strategies that are capable of capturing the temporal correlation of the dynamic environment requiring limited training overhead. In this paper, we introduce the deep echo state Q-network (DEQN) that can adapt to the highly dynamic environment in a short period of time with limited training data. We evaluate the performance of the introduced DEQN method under the dynamic spectrum sharing (DSS) scenario, which is a promising technology in 5G and future 6G networks to increase the spectrum utilization. Compared to conventional spectrum management policy that grants a fixed spectrum band to a single system for exclusive access, DSS allows the secondary system to share the spectrum with the primary system. Our work sheds light on the application of an efficient DRL framework in highly dynamic environments with limited available training data.",0
"Many domains have found success with deep reinforcement learning (DRL), which can be further enhanced by combining it with recurrent neural networks (RNNs) to capture temporal information in non-Markovian environments. However, both DRL and RNNs are challenging to train and require a large amount of data for convergence. In applications like fifth generation (5G) cellular communication, where training data is limited and environments are highly dynamic, developing DRL strategies with limited training overhead is crucial. This paper introduces the deep echo state Q-network (DEQN) as a method that can adapt to dynamic environments with limited data. The DEQN is evaluated under the dynamic spectrum sharing (DSS) scenario, which allows secondary systems to share spectrum with primary systems, increasing spectrum utilization compared to conventional policies. This work demonstrates the potential of a efficient DRL framework in highly dynamic environments with limited training data.",1
"Active Traffic Management strategies are often adopted in real-time to address such sudden flow breakdowns. When queuing is imminent, Speed Harmonization (SH), which adjusts speeds in upstream traffic to mitigate traffic showckwaves downstream, can be applied. However, because SH depends on driver awareness and compliance, it may not always be effective in mitigating congestion. The use of multiagent reinforcement learning for collaborative learning, is a promising solution to this challenge. By incorporating this technique in the control algorithms of connected and autonomous vehicle (CAV), it may be possible to train the CAVs to make joint decisions that can mitigate highway bottleneck congestion without human driver compliance to altered speed limits. In this regard, we present an RL-based multi-agent CAV control model to operate in mixed traffic (both CAVs and human-driven vehicles (HDVs)). The results suggest that even at CAV percent share of corridor traffic as low as 10%, CAVs can significantly mitigate bottlenecks in highway traffic. Another objective was to assess the efficacy of the RL-based controller vis-\`a-vis that of the rule-based controller. In addressing this objective, we duly recognize that one of the main challenges of RL-based CAV controllers is the variety and complexity of inputs that exist in the real world, such as the information provided to the CAV by other connected entities and sensed information. These translate as dynamic length inputs which are difficult to process and learn from. For this reason, we propose the use of Graphical Convolution Networks (GCN), a specific RL technique, to preserve information network topology and corresponding dynamic length inputs. We then use this, combined with Deep Deterministic Policy Gradient (DDPG), to carry out multi-agent training for congestion mitigation using the CAV controllers.",0
"To address sudden traffic flow breakdowns, Active Traffic Management strategies are often employed in real-time. Speed Harmonization (SH) is a technique that can be used to mitigate traffic shockwaves by adjusting speeds in upstream traffic when queuing is imminent. However, SH's effectiveness depends on driver awareness and compliance. To address this challenge, multiagent reinforcement learning for collaborative learning is a promising solution. By incorporating this technique into the control algorithms of connected and autonomous vehicles (CAV), it may be possible to train them to make joint decisions that can mitigate highway bottleneck congestion. We present an RL-based multi-agent CAV control model that operates in mixed traffic (CAVs and human-driven vehicles (HDVs)). Our results suggest that even at low CAV percent share of corridor traffic, CAVs can significantly mitigate bottlenecks in highway traffic. We also assessed the efficacy of the RL-based controller compared to that of the rule-based controller. However, the complexity and variety of inputs in the real world, such as information from other connected entities and sensed information, pose a challenge. To address this, we propose the use of Graphical Convolution Networks (GCN), a specific RL technique that preserves information network topology and corresponding dynamic length inputs. We combine this with Deep Deterministic Policy Gradient (DDPG) to carry out multi-agent training for congestion mitigation using the CAV controllers.",1
"Meta-learning is an important approach to improve machine learning performance with a limited number of observations for target tasks. However, when observations are unbalancedly obtained, it is difficult to improve the performance even with meta-learning methods. In this paper, we propose an active learning method for meta-learning on node response prediction tasks in attributed graphs, where nodes to observe are selected to improve performance with as few observed nodes as possible. With the proposed method, we use models based on graph convolutional neural networks for both predicting node responses and selecting nodes, by which we can predict responses and select nodes even for graphs with unseen response variables. The response prediction model is trained by minimizing the expected test error. The node selection model is trained by maximizing the expected error reduction with reinforcement learning. We demonstrate the effectiveness of the proposed method with 11 types of road congestion prediction tasks.",0
"Improving machine learning performance with limited observations for target tasks is a crucial aspect of meta-learning. However, if observations are unevenly distributed, even meta-learning methods may not enhance performance. This paper proposes an active learning method for meta-learning on attributed graphs' node response prediction tasks, where a minimal number of nodes are observed to improve performance. Our approach utilizes graph convolutional neural networks for both node response prediction and selection, enabling us to predict responses and select nodes for graphs with unseen response variables. We train the response prediction model by minimizing expected test error and the node selection model by maximizing the expected error reduction with reinforcement learning. To showcase the effectiveness of our method, we conduct experiments on 11 types of road congestion prediction tasks.",1
"The accuracy of deep convolutional neural networks (CNNs) generally improves when fueled with high resolution images. However, this often comes at a high computational cost and high memory footprint. Inspired by the fact that not all regions in an image are task-relevant, we propose a novel framework that performs efficient image classification by processing a sequence of relatively small inputs, which are strategically selected from the original image with reinforcement learning. Such a dynamic decision process naturally facilitates adaptive inference at test time, i.e., it can be terminated once the model is sufficiently confident about its prediction and thus avoids further redundant computation. Notably, our framework is general and flexible as it is compatible with most of the state-of-the-art light-weighted CNNs (such as MobileNets, EfficientNets and RegNets), which can be conveniently deployed as the backbone feature extractor. Experiments on ImageNet show that our method consistently improves the computational efficiency of a wide variety of deep models. For example, it further reduces the average latency of the highly efficient MobileNet-V3 on an iPhone XS Max by 20% without sacrificing accuracy. Code and pre-trained models are available at https://github.com/blackfeather-wang/GFNet-Pytorch.",0
"The precision of deep convolutional neural networks (CNNs) can be enhanced by using high-resolution images, but this can be computationally expensive and take up a lot of memory. To address this issue, we have developed a new approach that allows for efficient image classification by processing a sequence of smaller inputs that are strategically chosen from the original image using reinforcement learning. This dynamic decision-making process enables adaptive inference during testing, so that computation can be terminated once the model is sufficiently certain of its prediction, avoiding redundant computation. Our method is versatile and can be used with many state-of-the-art, lightweight CNNs (such as MobileNets, EfficientNets, and RegNets), and can be easily integrated as the backbone feature extractor. Our experiments on ImageNet demonstrate that our approach consistently improves the computational efficiency of various deep models. For example, it reduces the average latency of the highly efficient MobileNet-V3 on an iPhone XS Max by 20% without compromising accuracy. Our code and pre-trained models are available at https://github.com/blackfeather-wang/GFNet-Pytorch.",1
"Realistic environments often provide agents with very limited feedback. When the environment is initially unknown, the feedback, in the beginning, can be completely absent, and the agents may first choose to devote all their effort on exploring efficiently. The exploration remains a challenge while it has been addressed with many hand-tuned heuristics with different levels of generality on one side, and a few theoretically-backed exploration strategies on the other. Many of them are incarnated by intrinsic motivation and in particular explorations bonuses. A common rule of thumb for exploration bonuses is to use $1/\sqrt{n}$ bonus that is added to the empirical estimates of the reward, where $n$ is a number of times this particular state (or a state-action pair) was visited. We show that, surprisingly, for a pure-exploration objective of reward-free exploration, bonuses that scale with $1/n$ bring faster learning rates, improving the known upper bounds with respect to the dependence on the horizon $H$. Furthermore, we show that with an improved analysis of the stopping time, we can improve by a factor $H$ the sample complexity in the best-policy identification setting, which is another pure-exploration objective, where the environment provides rewards but the agent is not penalized for its behavior during the exploration phase.",0
"Agents operating in realistic environments often receive limited feedback, especially when the environment is unfamiliar. At the outset, feedback may be entirely absent, and agents may prioritize efficient exploration. However, exploration in such environments remains challenging and has been tackled using various hand-tuned heuristics with varying levels of generality, as well as a few theoretically-backed exploration strategies, such as intrinsic motivation and exploration bonuses. Typically, exploration bonuses involve adding a $1/\sqrt{n}$ bonus to the empirical reward estimate, where $n$ represents the number of times a particular state or state-action pair has been visited. Surprisingly, we show that for a pure-exploration objective of reward-free exploration, bonuses that scale with $1/n$ result in faster learning rates and improve the known upper bounds in terms of the horizon $H$. Additionally, we demonstrate that an improved analysis of the stopping time can increase the sample complexity by a factor of $H$ in the best-policy identification setting, which is another pure-exploration objective where the environment provides rewards, but the agent is not penalized for behavior during the exploration phase.",1
"It is a popular belief that model-based Reinforcement Learning (RL) is more sample efficient than model-free RL, but in practice, it is not always true due to overweighed model errors. In complex and noisy settings, model-based RL tends to have trouble using the model if it does not know when to trust the model.   In this work, we find that better model usage can make a huge difference. We show theoretically that if the use of model-generated data is restricted to state-action pairs where the model error is small, the performance gap between model and real rollouts can be reduced. It motivates us to use model rollouts only when the model is confident about its predictions. We propose Masked Model-based Actor-Critic (M2AC), a novel policy optimization algorithm that maximizes a model-based lower-bound of the true value function. M2AC implements a masking mechanism based on the model's uncertainty to decide whether its prediction should be used or not. Consequently, the new algorithm tends to give robust policy improvements. Experiments on continuous control benchmarks demonstrate that M2AC has strong performance even when using long model rollouts in very noisy environments, and it significantly outperforms previous state-of-the-art methods.",0
"Although it is commonly believed that model-based Reinforcement Learning (RL) is more efficient with sample sizes than model-free RL, this is not always the case due to the possibility of model errors being overemphasized. In complex and noisy scenarios, using the model to its full potential can be difficult if there is no clear indication of when to trust it. This study discovered that better model utilization can lead to significant improvement. By limiting the use of model-generated data to state-action pairs where the model error is small, we can decrease the performance gap between model and real rollouts. This motivates the use of model rollouts only when the model is confident about its predictions. To achieve this, we propose a new policy optimization algorithm called Masked Model-based Actor-Critic (M2AC), which maximizes a model-based lower-bound of the true value function. M2AC includes a masking mechanism that uses the model's uncertainty to determine whether or not its prediction should be used. As a result, the new algorithm provides more robust policy improvements. Our experiments using continuous control benchmarks demonstrate that M2AC performs well even when using long model rollouts in noisy environments, outperforming previous state-of-the-art methods.",1
"Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the corner stones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has not been fully investigated. In this paper, our goal is to address the fundamental question about its sample complexity. We study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model. We show that model-based MARL achieves a sample complexity of $\tilde O(|S||A||B|(1-\gamma)^{-3}\epsilon^{-2})$ for finding the Nash equilibrium (NE) value up to some $\epsilon$ error, and the $\epsilon$-NE policies with a smooth planning oracle, where $\gamma$ is the discount factor, and $S,A,B$ denote the state space, and the action spaces for the two agents. We further show that such a sample bound is minimax-optimal (up to logarithmic factors) if the algorithm is reward-agnostic, where the algorithm queries state transition samples without reward knowledge, by establishing a matching lower bound. This is in contrast to the usual reward-aware setting, with a $\tilde\Omega(|S|(|A|+|B|)(1-\gamma)^{-3}\epsilon^{-2})$ lower bound, where this model-based approach is near-optimal with only a gap on the $|A|,|B|$ dependence. Our results not only demonstrate the sample-efficiency of this basic model-based approach in MARL, but also elaborate on the fundamental tradeoff between its power (easily handling the more challenging reward-agnostic case) and limitation (less adaptive and suboptimal in $|A|,|B|$), particularly arises in the multi-agent context.",0
"Model-based reinforcement learning (RL) is a well-established method for determining an optimal policy by using an empirical model. It is particularly effective in multi-agent RL (MARL) because it separates the learning and planning stages, thus avoiding non-stationary problems when all agents are simultaneously improving their policies using samples. However, the sample complexity of model-based MARL algorithms is not fully understood. This paper aims to investigate the sample complexity of model-based MARL algorithms. The study focuses on the two-player discounted zero-sum Markov games setting, with only access to a generative model. The results show that model-based MARL can achieve a sample complexity of $\tilde O(|S||A||B|(1-\gamma)^{-3}\epsilon^{-2})$ when finding the Nash equilibrium (NE) value with a smooth planning oracle, and can obtain the $\epsilon$-NE policies. It is revealed that such a sample bound is minimax-optimal (up to logarithmic factors) when the algorithm is reward-agnostic, where the algorithm queries state transition samples without reward knowledge, as a matching lower bound is established. In comparison, the reward-aware setting requires a $\tilde\Omega(|S|(|A|+|B|)(1-\gamma)^{-3}\epsilon^{-2})$ lower bound, where the model-based approach is nearly optimal except for the $|A|,|B|$ dependence. The results not only demonstrate the sample efficiency of this basic model-based approach in MARL but also explain the fundamental tradeoff between its power (easily handling the more challenging reward-agnostic case) and limitation (less adaptive and suboptimal in $|A|,|B|$), particularly in the multi-agent context.",1
"In this paper, we focus on the problem of robustifying reinforcement learning (RL) algorithms with respect to model uncertainties. Indeed, in the framework of model-based RL, we propose to merge the theory of constrained Markov decision process (CMDP), with the theory of robust Markov decision process (RMDP), leading to a formulation of robust constrained-MDPs (RCMDP). This formulation, simple in essence, allows us to design RL algorithms that are robust in performance, and provides constraint satisfaction guarantees, with respect to uncertainties in the system's states transition probabilities. The need for RCMPDs is important for real-life applications of RL. For instance, such formulation can play an important role for policy transfer from simulation to real world (Sim2Real) in safety critical applications, which would benefit from performance and safety guarantees which are robust w.r.t model uncertainty. We first propose the general problem formulation under the concept of RCMDP, and then propose a Lagrangian formulation of the optimal problem, leading to a robust-constrained policy gradient RL algorithm. We finally validate this concept on the inventory management problem.",0
"The focus of this paper is on enhancing the robustness of reinforcement learning (RL) algorithms when faced with model uncertainties. Our approach involves combining the theories of constrained Markov decision process (CMDP) and robust Markov decision process (RMDP) within the framework of model-based RL, resulting in the development of robust constrained-MDPs (RCMDP). This formulation is straightforward, allowing for the creation of RL algorithms that are both robust in performance and provide guarantees for satisfying constraints when faced with uncertainties in the system's state transition probabilities. The need for RCMDPs is particularly relevant for real-world applications of RL, such as Sim2Real transfer in safety-critical scenarios, where performance and safety guarantees that are robust with respect to model uncertainty are crucial. To demonstrate the concept, we propose a general problem formulation using RCMDP and a Lagrangian formulation for the optimal problem, which leads to a policy gradient RL algorithm that is robustly constrained. Finally, we validate the proposed approach using an inventory management problem.",1
"Despite ample motivation from costly exploration and limited trajectory data, rapidly adapting to new environments with few-shot reinforcement learning (RL) can remain a challenging task, especially with respect to personalized settings. Here, we consider the problem of recommending optimal policies to a set of multiple entities each with potentially different characteristics, such that individual entities may parameterize distinct environments with unique transition dynamics. Inspired by existing literature in meta-learning, we extend previous work by focusing on the notion that certain environments are more similar to each other than others in personalized settings, and propose a model-free meta-learning algorithm that prioritizes past experiences by relevance during gradient-based adaptation. Our algorithm involves characterizing past policy divergence through methods in inverse reinforcement learning, and we illustrate how such metrics are able to effectively distinguish past policy parameters by the environment they were deployed in, leading to more effective fast adaptation during test time. To study personalization more effectively we introduce a navigation testbed to specifically incorporate environment diversity across training episodes, and demonstrate that our approach outperforms meta-learning alternatives with respect to few-shot reinforcement learning in personalized settings.",0
"Although costly exploration and limited trajectory data may provide ample motivation, adapting rapidly to new environments with few-shot reinforcement learning (RL) can be difficult, particularly in personalized settings. In this study, we address the challenge of recommending optimal policies for multiple entities that may have different characteristics and may parameterize distinct environments with unique transition dynamics. Building on existing literature in meta-learning, we propose a model-free meta-learning algorithm that prioritizes past experiences by relevance during gradient-based adaptation. Our approach involves characterizing past policy divergence using methods from inverse reinforcement learning, which effectively distinguish past policy parameters by the environment they were deployed in, leading to more effective fast adaptation during test time. To evaluate our algorithm's effectiveness, we introduce a navigation testbed that incorporates environment diversity across training episodes and demonstrate that it outperforms other meta-learning alternatives in personalized settings with respect to few-shot reinforcement learning.",1
"Hindsight Experience Replay (HER) is one of the efficient algorithm to solve Reinforcement Learning tasks related to sparse rewarded environments.But due to its reduced sample efficiency and slower convergence HER fails to perform effectively. Natural gradients solves these challenges by converging the model parameters better. It avoids taking bad actions that collapse the training performance. However updating parameters in neural networks requires expensive computation and thus increase in training time. Our proposed method solves the above mentioned challenges with better sample efficiency and faster convergence with increased success rate. A common failure mode for DDPG is that the learned Q-function begins to dramatically overestimate Q-values, which then leads to the policy breaking, because it exploits the errors in the Q-function. We solve this issue by including Twin Delayed Deep Deterministic Policy Gradients(TD3) in HER. TD3 learns two Q-functions instead of one and it adds noise tothe target action, to make it harder for the policy to exploit Q-function errors. The experiments are done with the help of OpenAis Mujoco environments. Results on these environments show that our algorithm (TDHER+KFAC) performs better inmost of the scenarios",0
"HER is an effective algorithm for solving Reinforcement Learning tasks related to scarce rewards. However, it lacks sample efficiency and struggles with slow convergence, resulting in subpar performance. Natural gradients can address these issues by enhancing model parameter convergence and avoiding bad actions that damage training performance. Nevertheless, the computational expense of updating neural network parameters increases training time. Our proposed method improves sample efficiency and hastens convergence while increasing the success rate by overcoming these challenges. DDPG commonly fails due to Q-function overestimation, which causes policy breakdown by exploiting these errors. We address this problem by incorporating Twin Delayed Deep Deterministic Policy Gradients (TD3) into HER. TD3 learns two Q-functions and adds noise to the target action to make it harder for the policy to exploit Q-function errors. We conducted experiments using OpenAis Mujoco environments, and our algorithm (TDHER+KFAC) outperformed the majority of scenarios.",1
"Biological agents learn and act intelligently in spite of a highly limited capacity to process and store information. Many real-world problems involve continuous control, which represents a difficult task for artificial intelligence agents. In this paper we explore the potential learning advantages a natural constraint on information flow might confer onto artificial agents in continuous control tasks. We focus on the model-free reinforcement learning (RL) setting and formalize our approach in terms of an information-theoretic constraint on the complexity of learned policies. We show that our approach emerges in a principled fashion from the application of rate-distortion theory. We implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and Mutual Information Reinforcement Learning (MIRL) algorithm. Our experiments using continuous control tasks show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while displaying the high sample efficiency of similar methods.",0
"Despite their limited ability to process and store information, biological agents exhibit intelligent behavior. However, artificial intelligence agents face challenges in solving real-world problems that require continuous control. This paper explores how natural constraints on information flow can potentially benefit artificial agents in continuous control tasks, particularly in the model-free reinforcement learning (RL) setting. The authors propose an approach that uses an information-theoretic constraint to limit the complexity of learned policies. This approach is based on rate-distortion theory and is implemented in a new algorithm called the Capacity-Limited Actor-Critic (CLAC) algorithm, which is part of a family of RL algorithms including Soft Actor Critic (SAC) and Mutual Information Reinforcement Learning (MIRL) algorithm. The authors conduct experiments on continuous control tasks and demonstrate that the CLAC algorithm improves generalization in modified test environments compared to alternative methods, while maintaining high sample efficiency.",1
"Evolution strategies (ES), as a family of black-box optimization algorithms, recently emerge as a scalable alternative to reinforcement learning (RL) approaches such as Q-learning or policy gradient, and are much faster when many central processing units (CPUs) are available due to better parallelization. In this paper, we propose a systematic incremental learning method for ES in dynamic environments. The goal is to adjust previously learned policy to a new one incrementally whenever the environment changes. We incorporate an instance weighting mechanism with ES to facilitate its learning adaptation, while retaining scalability of ES. During parameter updating, higher weights are assigned to instances that contain more new knowledge, thus encouraging the search distribution to move towards new promising areas of parameter space. We propose two easy-to-implement metrics to calculate the weights: instance novelty and instance quality. Instance novelty measures an instance's difference from the previous optimum in the original environment, while instance quality corresponds to how well an instance performs in the new environment. The resulting algorithm, Instance Weighted Incremental Evolution Strategies (IW-IES), is verified to achieve significantly improved performance on a suite of robot navigation tasks. This paper thus introduces a family of scalable ES algorithms for RL domains that enables rapid learning adaptation to dynamic environments.",0
"Recently, Evolution Strategies (ES) have emerged as a scalable alternative to Reinforcement Learning (RL) approaches like Q-learning and policy gradient. They are particularly faster when many Central Processing Units (CPUs) are available due to better parallelization. In this paper, we propose a systematic incremental learning method for ES in dynamic environments. Our goal is to adapt previously learned policies to new ones incrementally whenever the environment changes. To facilitate learning adaptation while retaining scalability of ES, we incorporate an instance weighting mechanism. During parameter updating, we assign higher weights to instances containing more new knowledge, encouraging the search distribution to move towards new promising areas of parameter space. We introduce two easy-to-implement metrics to calculate the weights: instance novelty and instance quality. Instance novelty measures an instance's difference from the previous optimum in the original environment, while instance quality corresponds to its performance in the new environment. The resulting algorithm, Instance Weighted Incremental Evolution Strategies (IW-IES), achieves significantly improved performance on a suite of robot navigation tasks. This paper thus presents scalable ES algorithms for RL domains that enable rapid learning adaptation to dynamic environments.",1
"Epidemiologists model the dynamics of epidemics in order to propose control strategies based on pharmaceutical and non-pharmaceutical interventions (contact limitation, lock down, vaccination, etc). Hand-designing such strategies is not trivial because of the number of possible interventions and the difficulty to predict long-term effects. This task can be cast as an optimization problem where state-of-the-art machine learning algorithms such as deep reinforcement learning, might bring significant value. However, the specificity of each domain -- epidemic modelling or solving optimization problem -- requires strong collaborations between researchers from different fields of expertise.   This is why we introduce EpidemiOptim, a Python toolbox that facilitates collaborations between researchers in epidemiology and optimization. EpidemiOptim turns epidemiological models and cost functions into optimization problems via a standard interface commonly used by optimization practitioners (OpenAI Gym). Reinforcement learning algorithms based on Q-Learning with deep neural networks (DQN) and evolutionary algorithms (NSGA-II) are already implemented. We illustrate the use of EpidemiOptim to find optimal policies for dynamical on-off lock-down control under the optimization of death toll and economic recess using a Susceptible-Exposed-Infectious-Removed (SEIR) model for COVID-19. Using EpidemiOptim and its interactive visualization platform in Jupyter notebooks, epidemiologists, optimization practitioners and others (e.g. economists) can easily compare epidemiological models, costs functions and optimization algorithms to address important choices to be made by health decision-makers.",0
"To suggest effective control strategies for epidemics, epidemiologists model their dynamics with both pharmaceutical and non-pharmaceutical interventions like vaccination, contact limitation, and lock down. However, the task of designing these strategies is not simple due to the vast number of interventions available and the challenge of predicting their long-term effects. To address this, machine learning algorithms like deep reinforcement learning can be used to optimize these strategies. But, the specificity of each field necessitates strong collaboration between researchers from different domains. Therefore, we present EpidemiOptim, a Python toolbox that facilitates such collaborations between epidemiologists and optimization practitioners. It transforms epidemiological models and cost functions into optimization problems using a standard interface and incorporates reinforcement learning and evolutionary algorithms. We demonstrate the use of EpidemiOptim to find optimal policies for on-off lock-down control during COVID-19 using a SEIR model. Through its interactive visualization platform, EpidemiOptim enables researchers from various fields to compare models, costs functions, and optimization algorithms to make informed decisions for health policy-making.",1
"Continual learning could shift the machine learning paradigm from data centric to model centric. A continual learning model needs to scale efficiently to handle semantically different datasets, while avoiding unnecessary growth. We introduce hash-routed convolutional neural networks: a group of convolutional units where data flows dynamically. Feature maps are compared using feature hashing and similar data is routed to the same units. A hash-routed network provides excellent plasticity thanks to its routed nature, while generating stable features through the use of orthogonal feature hashing. Each unit evolves separately and new units can be added (to be used only when necessary). Hash-routed networks achieve excellent performance across a variety of typical continual learning benchmarks without storing raw data and train using only gradient descent. Besides providing a continual learning framework for supervised tasks with encouraging results, our model can be used for unsupervised or reinforcement learning.",0
"The machine learning paradigm could be transformed from being data centric to model centric through continual learning. In order for a continual learning model to be effective, it must be able to handle different datasets efficiently without unnecessary growth. We have developed hash-routed convolutional neural networks, which consist of dynamically flowing convolutional units. Similar data is routed to the same units through feature hashing. This model offers great plasticity due to its routed nature and generates stable features with orthogonal feature hashing. Units can evolve separately and new ones can be added when necessary. The hash-routed network performs well on various continual learning benchmarks without storing raw data and using only gradient descent for training. This model can also be applied to unsupervised or reinforcement learning, making it a versatile framework for continual learning.",1
"Learning effective policies for sparse objectives is a key challenge in Deep Reinforcement Learning (RL). A common approach is to design task-related dense rewards to improve task learnability. While such rewards are easily interpreted, they rely on heuristics and domain expertise. Alternate approaches that train neural networks to discover dense surrogate rewards avoid heuristics, but are high-dimensional, black-box solutions offering little interpretability. In this paper, we present a method that discovers dense rewards in the form of low-dimensional symbolic trees - thus making them more tractable for analysis. The trees use simple functional operators to map an agent's observations to a scalar reward, which then supervises the policy gradient learning of a neural network policy. We test our method on continuous action spaces in Mujoco and discrete action spaces in Atari and Pygame environments. We show that the discovered dense rewards are an effective signal for an RL policy to solve the benchmark tasks. Notably, we significantly outperform a widely used, contemporary neural-network based reward-discovery algorithm in all environments considered.",0
"Deep Reinforcement Learning (RL) faces a significant obstacle in learning efficient policies for sparse objectives. One prevalent technique is to create task-specific dense rewards, which help enhance the capacity to learn the task. However, these rewards heavily depend on heuristics and domain expertise, despite being easily understandable. In contrast, alternative approaches that use neural networks to identify dense surrogate rewards do not rely on heuristics, but they are high-dimensional and opaque, which hinders interpretability. In this article, we present a novel method that uncovers low-dimensional symbolic trees that represent dense rewards, making them easier to analyze. The trees employ basic functional operators that map an agent's observations to a scalar reward, which supervises the policy gradient learning of a neural network policy. We evaluate our approach on continuous action spaces in Mujoco and discrete action spaces in Atari and Pygame environments. Our results demonstrate that the found dense rewards are a valuable signal for RL policies to solve benchmark tasks. Notably, we significantly outperform a commonly used contemporary neural-network-based reward-discovery algorithm in all environments analyzed.",1
"Learning to locomote is one of the most common tasks in physics-based animation and deep reinforcement learning (RL). A learned policy is the product of the problem to be solved, as embodied by the RL environment, and the RL algorithm. While enormous attention has been devoted to RL algorithms, much less is known about the impact of design choices for the RL environment. In this paper, we show that environment design matters in significant ways and document how it can contribute to the brittle nature of many RL results. Specifically, we examine choices related to state representations, initial state distributions, reward structure, control frequency, episode termination procedures, curriculum usage, the action space, and the torque limits. We aim to stimulate discussion around such choices, which in practice strongly impact the success of RL when applied to continuous-action control problems of interest to animation, such as learning to locomote.",0
"The process of learning to move is a common task in physics-based animation and deep reinforcement learning (RL). The RL algorithm and RL environment, which represent the problem to be solved, are responsible for producing a learned policy. Although much attention has been given to the RL algorithm, little is known about the impact of design choices for the RL environment. This paper demonstrates that environmental design plays a significant role and can contribute to the fragility of many RL outcomes. The study focuses on various design choices, such as state representations, initial state distributions, reward structure, control frequency, episode termination procedures, curriculum usage, the action space, and the torque limits. The goal is to encourage discussion around these choices to enhance the success of RL in solving continuous-action control problems, including learning how to move.",1
"We propose a reinforcement learning algorithm for stationary mean-field games, where the goal is to learn a pair of mean-field state and stationary policy that constitutes the Nash equilibrium. When viewing the mean-field state and the policy as two players, we propose a fictitious play algorithm which alternatively updates the mean-field state and the policy via gradient-descent and proximal policy optimization, respectively. Our algorithm is in stark contrast with previous literature which solves each single-agent reinforcement learning problem induced by the iterates mean-field states to the optimum. Furthermore, we prove that our fictitious play algorithm converges to the Nash equilibrium at a sublinear rate. To the best of our knowledge, this seems the first provably convergent single-loop reinforcement learning algorithm for mean-field games based on iterative updates of both mean-field state and policy.",0
"Our proposal is an algorithm for stationary mean-field games that employs reinforcement learning. The algorithm aims to learn a Nash equilibrium by discovering a mean-field state and stationary policy pair. We treat the mean-field state and policy as two players and use a fictitious play algorithm to update them alternatively via gradient-descent and proximal policy optimization. Our approach differs from previous literature, which solves each single-agent reinforcement learning problem induced by the iterates mean-field states to the optimum. Moreover, we demonstrate that our fictitious play algorithm has sublinear convergence to the Nash equilibrium. As far as we know, this is the first single-loop reinforcement learning algorithm for mean-field games that is provably convergent and relies on iterative updates of both the mean-field state and policy.",1
"In order to make better use of deep reinforcement learning in the creation of sensing policies for resource-constrained IoT devices, we present and study a novel reward function based on the Fisher information value. This reward function enables IoT sensor devices to learn to spend available energy on measurements at otherwise unpredictable moments, while conserving energy at times when measurements would provide little new information. This is a highly general approach, which allows for a wide range of use cases without significant human design effort or hyper-parameter tuning. We illustrate the approach in a scenario of workplace noise monitoring, where results show that the learned behavior outperforms a uniform sampling strategy and comes close to a near-optimal oracle solution.",0
"A new reward function based on the Fisher information value is proposed and studied to enhance the use of deep reinforcement learning in creating sensing policies for IoT devices with limited resources. This reward function enables IoT sensors to optimize energy usage by performing measurements at unpredictable moments and conserving energy when measurements provide little new information. This approach is versatile and applicable to various use cases without extensive human design or hyper-parameter tuning. The approach is demonstrated in workplace noise monitoring, and results indicate that it outperforms uniform sampling and approximates a near-optimal solution.",1
"Reinforcement learning (RL) algorithms typically deal with maximizing the expected cumulative return (discounted or undiscounted, finite or infinite horizon). However, several crucial applications in the real world, such as drug discovery, do not fit within this framework because an RL agent only needs to identify states (molecules) that achieve the highest reward within a trajectory and does not need to optimize for the expected cumulative return. In this work, we formulate an objective function to maximize the expected maximum reward along a trajectory, derive a novel functional form of the Bellman equation, introduce the corresponding Bellman operators, and provide a proof of convergence. Using this formulation, we achieve state-of-the-art results on the task of molecule generation that mimics a real-world drug discovery pipeline.",0
"Reinforcement learning (RL) algorithms typically aim to maximize the expected cumulative return, whether it be discounted or undiscounted, finite or infinite horizon. However, certain crucial real-world applications, such as drug discovery, do not align with this framework as an RL agent only needs to identify the states (molecules) that result in the highest reward within a trajectory, rather than optimizing for the expected cumulative return. This study introduces an objective function to maximize the expected maximum reward along a trajectory, a novel form of the Bellman equation, corresponding Bellman operators, and a proof of convergence. By utilizing this formulation, the study achieves state-of-the-art outcomes in the task of molecule generation that simulates a real-world drug discovery pipeline.",1
"We develop a deep learning model to effectively solve high-dimensional nonlinear parabolic partial differential equations (PDE). We follow Feynman-Kac formula to reformulate PDE into the equivalent stochastic control problem governed by a Backward Stochastic Differential Equation (BSDE) system. The Markovian property of the BSDE is utilized in designing our neural network architecture, which is inspired by the Actor-Critic algorithm usually applied for deep Reinforcement Learning. Compared to the State-of-the-Art model, we make several improvements including 1) largely reduced trainable parameters, 2) faster convergence rate and 3) fewer hyperparameters to tune. We demonstrate those improvements by solving a few well-known classes of PDEs such as Hamilton-Jacobian-Bellman equation, Allen-Cahn equation and Black-Scholes equation with dimensions on the order of 100.",0
"Our team has created a powerful deep learning model that can effectively tackle high-dimensional nonlinear parabolic partial differential equations (PDEs). To accomplish this, we have utilized the Feynman-Kac formula to reformulate the PDE as a stochastic control problem that is governed by a Backward Stochastic Differential Equation (BSDE) system. Our neural network architecture takes advantage of the Markovian property of the BSDE and is inspired by the Actor-Critic algorithm that is commonly used for deep Reinforcement Learning. In comparison to the State-of-the-Art model, our approach boasts several key enhancements. First, we have significantly reduced the number of trainable parameters. Additionally, our model exhibits a faster convergence rate and requires fewer hyperparameters to be tuned. We have demonstrated the effectiveness of our approach by successfully solving a number of well-known PDEs, including the Hamilton-Jacobian-Bellman equation, Allen-Cahn equation, and Black-Scholes equation, all with dimensions on the order of 100.",1
"Recently, deep learning has been successfully applied to a variety of networking problems. A fundamental challenge is that when the operational environment for a learning-augmented system differs from its training environment, such systems often make badly informed decisions, leading to bad performance. We argue that safely deploying learning-driven systems requires being able to determine, in real time, whether system behavior is coherent, for the purpose of defaulting to a reasonable heuristic when this is not so. We term this the online safety assurance problem (OSAP). We present three approaches to quantifying decision uncertainty that differ in terms of the signal used to infer uncertainty. We illustrate the usefulness of online safety assurance in the context of the proposed deep reinforcement learning (RL) approach to video streaming. While deep RL for video streaming bests other approaches when the operational and training environments match, it is dominated by simple heuristics when the two differ. Our preliminary findings suggest that transitioning to a default policy when decision uncertainty is detected is key to enjoying the performance benefits afforded by leveraging ML without compromising on safety.",0
"A variety of networking problems have been successfully solved using deep learning. However, a major challenge is that learning-augmented systems often perform poorly when their operational environment is different from their training environment. To ensure safe deployment of such systems, it is important to determine in real time whether the system behavior is coherent. This is known as the online safety assurance problem (OSAP). To tackle this problem, we propose three approaches for quantifying decision uncertainty, each using a different signal to infer uncertainty. We demonstrate the usefulness of OSAP in the context of deep reinforcement learning (RL) for video streaming. While deep RL outperforms other approaches when the operational and training environments are the same, it falls short when they differ. Our preliminary findings suggest that defaulting to a simple heuristic policy when decision uncertainty is detected is crucial for achieving the performance benefits of machine learning while ensuring safety.",1
"Reward-free exploration is a reinforcement learning setting studied by Jin et al. (2020), who address it by running several algorithms with regret guarantees in parallel. In our work, we instead give a more natural adaptive approach for reward-free exploration which directly reduces upper bounds on the maximum MDP estimation error. We show that, interestingly, our reward-free UCRL algorithm can be seen as a variant of an algorithm of Fiechter from 1994, originally proposed for a different objective that we call best-policy identification. We prove that RF-UCRL needs of order $({SAH^4}/{\varepsilon^2})(\log(1/\delta) + S)$ episodes to output, with probability $1-\delta$, an $\varepsilon$-approximation of the optimal policy for any reward function. This bound improves over existing sample-complexity bounds in both the small $\varepsilon$ and the small $\delta$ regimes. We further investigate the relative complexities of reward-free exploration and best-policy identification.",0
"Jin et al. (2020) studied reward-free exploration in the context of reinforcement learning. They employed several algorithms with regret guarantees running in parallel. In contrast, our approach is more natural, as we directly reduce upper bounds on the maximum MDP estimation error. Our reward-free UCRL algorithm is interestingly a variant of an algorithm proposed by Fiechter in 1994 for a different objective, which we refer to as best-policy identification. We demonstrate that RF-UCRL requires approximately $({SAH^4}/{\varepsilon^2})(\log(1/\delta) + S)$ episodes to output an $\varepsilon$-approximation of the optimal policy for any reward function with probability $1-\delta$. Our sample-complexity bounds outperform existing bounds in both the small $\varepsilon$ and small $\delta$ regimes. Additionally, we delve into the complexities of reward-free exploration and best-policy identification.",1
"Exploration is a key problem in reinforcement learning, since agents can only learn from data they acquire in the environment. With that in mind, maintaining a population of agents is an attractive method, as it allows data be collected with a diverse set of behaviors. This behavioral diversity is often boosted via multi-objective loss functions. However, those approaches typically leverage mean field updates based on pairwise distances, which makes them susceptible to cycling behaviors and increased redundancy. In addition, explicitly boosting diversity often has a detrimental impact on optimizing already fruitful behaviors for rewards. As such, the reward-diversity trade off typically relies on heuristics. Finally, such methods require behavioral representations, often handcrafted and domain specific. In this paper, we introduce an approach to optimize all members of a population simultaneously. Rather than using pairwise distance, we measure the volume of the entire population in a behavioral manifold, defined by task-agnostic behavioral embeddings. In addition, our algorithm Diversity via Determinants (DvD), adapts the degree of diversity during training using online learning techniques. We introduce both evolutionary and gradient-based instantiations of DvD and show they effectively improve exploration without reducing performance when better exploration is not required.",0
"Reinforcement learning faces a challenge in exploration, as agents can only learn from the data they gather in their environment. A possible solution is to maintain a population of agents, which can collect data with a wide range of behaviors and boost behavioral diversity through multi-objective loss functions. However, such approaches often rely on mean field updates based on pairwise distances, which can lead to cycling behaviors and redundancy. Additionally, explicitly promoting diversity may hinder optimizing already productive behaviors for rewards, and the reward-diversity trade-off often involves heuristics. These methods also require handcrafted, domain-specific behavioral representations. This paper proposes a new approach that optimizes all members of a population simultaneously, using the volume of the entire population in a behavioral manifold defined by task-agnostic behavioral embeddings instead of pairwise distance. The proposed algorithm, Diversity via Determinants (DvD), employs online learning techniques to adapt the degree of diversity during training. Both evolutionary and gradient-based instantiations of DvD are introduced and shown to effectively improve exploration without reducing performance when better exploration is unnecessary.",1
"We describe a new approach for managing aleatoric uncertainty in the Reinforcement Learning (RL) paradigm. Instead of selecting actions according to a single statistic, we propose a distributional method based on the second-order stochastic dominance (SSD) relation. This compares the inherent dispersion of random returns induced by actions, producing a more comprehensive and robust evaluation of the environment's uncertainty. The necessary conditions for SSD require estimators to predict accurate second moments. To accommodate this, we map the distributional RL problem to a Wasserstein gradient flow, treating the distributional Bellman residual as a potential energy functional. We propose a particle-based algorithm for which we prove optimality and convergence. Our experiments characterize the algorithm performance and demonstrate how uncertainty and performance are better balanced using an \textsc{ssd} policy than with other risk measures.",0
"Our article presents a novel technique to handle aleatoric uncertainty in Reinforcement Learning (RL). Rather than relying on a single statistic to select actions, we introduce a distributional method based on second-order stochastic dominance (SSD) relation. This approach evaluates the environment's uncertainty more comprehensively and robustly by comparing the inherent dispersion of random returns induced by actions. Accurate prediction of second moments is necessary for SSD, which we achieve by mapping the distributional RL problem to a Wasserstein gradient flow and treating the distributional Bellman residual as a potential energy functional. Our particle-based algorithm is proven to be optimal and convergent. We conducted experiments that demonstrate the algorithm's performance, showing that it balances uncertainty and performance better than other risk measures.",1
"In the classical multi-armed bandit problem, instance-dependent algorithms attain improved performance on ""easy"" problems with a gap between the best and second-best arm. Are similar guarantees possible for contextual bandits? While positive results are known for certain special cases, there is no general theory characterizing when and how instance-dependent regret bounds for contextual bandits can be achieved for rich, general classes of policies. We introduce a family of complexity measures that are both sufficient and necessary to obtain instance-dependent regret bounds. We then introduce new oracle-efficient algorithms which adapt to the gap whenever possible, while also attaining the minimax rate in the worst case. Finally, we provide structural results that tie together a number of complexity measures previously proposed throughout contextual bandits, reinforcement learning, and active learning and elucidate their role in determining the optimal instance-dependent regret. In a large-scale empirical evaluation, we find that our approach often gives superior results for challenging exploration problems.   Turning our focus to reinforcement learning with function approximation, we develop new oracle-efficient algorithms for reinforcement learning with rich observations that obtain optimal gap-dependent sample complexity.",0
"Can contextual bandits achieve instance-dependent regret bounds similar to those attained by instance-dependent algorithms in the classical multi-armed bandit problem? Although positive results have been established for certain special cases, there is currently no general theory to characterize when and how instance-dependent regret bounds can be achieved for rich, general classes of policies. To address this issue, we propose a family of complexity measures that are both sufficient and necessary to obtain instance-dependent regret bounds, along with new oracle-efficient algorithms that adapt to the gap whenever possible while also attaining the minimax rate in the worst case. Moreover, we provide structural results that connect various complexity measures proposed in contextual bandits, reinforcement learning, and active learning, and clarify their role in determining the optimal instance-dependent regret. Our approach outperforms existing methods in challenging exploration problems, as demonstrated in a large-scale empirical evaluation. In addition, we develop new oracle-efficient algorithms for reinforcement learning with function approximation that achieve optimal gap-dependent sample complexity, using rich observations.",1
"We provide theoretical investigations into off-policy evaluation in reinforcement learning using function approximators for (marginalized) importance weights and value functions. Our contributions include: (1) A new estimator, MWL, that directly estimates importance ratios over the state-action distributions, removing the reliance on knowledge of the behavior policy as in prior work (Liu et al., 2018). (2) Another new estimator, MQL, obtained by swapping the roles of importance weights and value-functions in MWL. MQL has an intuitive interpretation of minimizing average Bellman errors and can be combined with MWL in a doubly robust manner. (3) Several additional results that offer further insights into these methods, including the sample complexity analyses of MWL and MQL, their asymptotic optimality in the tabular setting, how the learned importance weights depend the choice of the discriminator class, and how our methods provide a unified view of some old and new algorithms in RL.",0
"Our focus is on studying off-policy evaluation in reinforcement learning using function approximators for (marginalized) importance weights and value functions. Our research offers novel contributions: firstly, we introduce a new estimator, MWL, which directly calculates importance ratios over the state-action distributions, avoiding the need for knowledge of the behavior policy as in previous studies (Liu et al., 2018). Secondly, we develop another new estimator, MQL, which involves swapping the roles of importance weights and value functions in MWL. MQL is easily interpretable, minimizing average Bellman errors and can be used in combination with MWL in a doubly robust manner. Thirdly, we provide additional results that provide deeper insights into these methods, including analyzing the sample complexity of MWL and MQL, their asymptotic optimality in the tabular setting, how the learned importance weights are impacted by the choice of the discriminator class, and how our methods offer a unified view of some old and new algorithms in RL.",1
"We address the problem of learning reusable state representations from streaming high-dimensional observations. This is important for areas like Reinforcement Learning (RL), which yields non-stationary data distributions during training. We make two key contributions. First, we propose an evaluation suite that measures alignment between latent and true low-dimensional states. We benchmark several widely used unsupervised learning approaches. This uncovers the strengths and limitations of existing approaches that impose additional constraints/objectives on the latent space. Our second contribution is a unifying mathematical formulation for learning latent relations. We learn analytic relations on source domains, then use these relations to help structure the latent space when learning on target domains. This formulation enables a more general, flexible and principled way of shaping the latent space. It formalizes the notion of learning independent relations, without imposing restrictive simplifying assumptions or requiring domain-specific information. We present mathematical properties, concrete algorithms for implementation and experimental validation of successful learning and transfer of latent relations.",0
"Our focus is on developing a method for acquiring reusable state representations from high-dimensional observations in a streaming context. This is particularly pertinent in the field of Reinforcement Learning (RL), where the data distributions are non-stationary during training. We present two primary contributions. Firstly, we introduce an appraisal suite to assess the alignment between latent and actual low-dimensional states. We evaluate various unsupervised learning techniques to identify the strengths and weaknesses of existing models that enforce additional constraints/objectives on the latent space. Our second contribution is a comprehensive mathematical framework for learning latent relationships. We acquire analytic relationships on source domains, which we leverage to structure the latent space when learning on target domains. This approach allows for a more versatile, adaptable, and principled way of shaping the latent space. It formalizes the notion of acquiring independent relations without imposing limiting assumptions or requiring domain-specific information. We provide mathematical properties, specific algorithms for implementation, and experimental validation of successful learning and transfer of latent relationships.",1
"An important aspect of intelligence is the ability to adapt to a novel task without any direct experience (zero-shot), based on its relationship to previous tasks. Humans can exhibit this cognitive flexibility. By contrast, models that achieve superhuman performance in specific tasks often fail to adapt to even slight task alterations. To address this, we propose a general computational framework for adapting to novel tasks based on their relationship to prior tasks. We begin by learning vector representations of tasks. To adapt to new tasks, we propose meta-mappings, higher-order tasks that transform basic task representations. We demonstrate the effectiveness of this framework across a wide variety of tasks and computational paradigms, ranging from regression to image classification and reinforcement learning. We compare to both human adaptability and language-based approaches to zero-shot learning. Across these domains, meta-mapping is successful, often achieving 80-90% performance, without any data, on a novel task, even when the new task directly contradicts prior experience. We further show that meta-mapping can not only generalize to new tasks via learned relationships, but can also generalize using novel relationships unseen during training. Finally, using meta-mapping as a starting point can dramatically accelerate later learning on a new task, and reduce learning time and cumulative error substantially. Our results provide insight into a possible computational basis of intelligent adaptability and offer a possible framework for modeling cognitive flexibility and building more flexible artificial intelligence systems.",0
"The ability to adapt to new tasks without prior experience is a key component of intelligence, and humans excel at this cognitive skill. However, many models that perform well in specific tasks struggle to adapt to even minor changes. To address this issue, we propose a computational framework for adapting to new tasks based on their relationship to prior tasks. We first learn vector representations of tasks and then use meta-mappings, which are higher-order tasks that transform basic task representations, to adapt to new tasks. Our framework is effective across a variety of tasks and computational paradigms, and it outperforms both human adaptability and language-based approaches to zero-shot learning. Meta-mapping can successfully achieve up to 90% performance on a novel task, even when the task contradicts prior experience. Additionally, it can generalize to new tasks using both learned and novel relationships, and it can accelerate later learning and reduce cumulative error. Our results offer insight into the computational basis of adaptability and provide a framework for building more flexible artificial intelligence systems.",1
"As reinforcement learning agents become increasingly integrated into complex, real-world environments, designing for safety becomes a critical consideration. We specifically focus on researching scenarios where agents can cause undesired side effects while executing a policy on a primary task. Since one can define multiple tasks for a given environment dynamics, there are two important challenges. First, we need to abstract the concept of safety that applies broadly to that environment independent of the specific task being executed. Second, we need a mechanism for the abstracted notion of safety to modulate the actions of agents executing different policies to minimize their side-effects. In this work, we propose Safety Aware Reinforcement Learning (SARL) - a framework where a virtual safe agent modulates the actions of a main reward-based agent to minimize side effects. The safe agent learns a task-independent notion of safety for a given environment. The main agent is then trained with a regularization loss given by the distance between the native action probabilities of the two agents. Since the safe agent effectively abstracts a task-independent notion of safety via its action probabilities, it can be ported to modulate multiple policies solving different tasks within the given environment without further training. We contrast this with solutions that rely on task-specific regularization metrics and test our framework on the SafeLife Suite, based on Conway's Game of Life, comprising a number of complex tasks in dynamic environments. We show that our solution is able to match the performance of solutions that rely on task-specific side-effect penalties on both the primary and safety objectives while additionally providing the benefit of generalizability and portability.",0
"Incorporating reinforcement learning agents into intricate, practical settings raises the need to prioritize safety measures. Our research specifically concentrates on exploring scenarios where agents may produce undesired side effects while executing a policy on a primary task. Since a single environment may consist of multiple tasks, we face two significant challenges. Firstly, we must conceptualize safety in a way that applies universally to the environment, regardless of the specific task being executed. Secondly, we require a mechanism for the abstracted safety notion to regulate the actions of agents executing different policies to minimize their side-effects. Our proposed solution is Safety Aware Reinforcement Learning (SARL) - a framework in which a virtual safe agent modifies the actions of a primary reward-based agent to reduce side effects. The safe agent learns a task-independent safety concept for a given environment, and the primary agent is trained with a regularization loss based on the difference between the native action probabilities of the two agents. As the safe agent abstracts a task-independent safety notion via its action probabilities, it can be applied to regulate multiple policies solving various tasks within the environment without further training. We compare our approach with alternatives that rely on task-specific regularization metrics and evaluate our framework on the SafeLife Suite, a collection of complex tasks in dynamic environments based on Conway's Game of Life. Our results demonstrate that our solution can match the performance of alternatives that rely on task-specific side-effect penalties on both the primary and safety objectives while also offering the advantage of generalizability and portability.",1
"In this paper, we study the problem of autonomously discovering temporally abstracted actions, or options, for exploration in reinforcement learning. For learning diverse options suitable for exploration, we introduce the infomax termination objective defined as the mutual information between options and their corresponding state transitions. We derive a scalable optimization scheme for maximizing this objective via the termination condition of options, yielding the InfoMax Option Critic (IMOC) algorithm. Through illustrative experiments, we empirically show that IMOC learns diverse options and utilizes them for exploration. Moreover, we show that IMOC scales well to continuous control tasks.",0
"The focus of our research is on the self-directed identification of temporally abstracted actions, referred to as options, for the purpose of exploration in reinforcement learning. To achieve the goal of obtaining a variety of exploration-friendly options, we propose the infomax termination objective, which measures the mutual information between options and their corresponding state transitions. We present a scalable optimization technique to maximize this objective, which involves using the termination condition of options and results in the InfoMax Option Critic (IMOC) algorithm. Our experimental results demonstrate that IMOC successfully acquires diverse options and employs them for exploration, and furthermore, that IMOC is effective in continuous control tasks.",1
"Recent research has shown that map raw pixels from a single front-facing camera directly to steering commands are surprisingly powerful. This paper presents a convolutional neural network (CNN) to playing the CarRacing-v0 using imitation learning in OpenAI Gym. The dataset is generated by playing the game manually in Gym and used a data augmentation method to expand the dataset to 4 times larger than before. Also, we read the true speed, four ABS sensors, steering wheel position, and gyroscope for each image and designed a mixed model by combining the sensor input and image input. After training, this model can automatically detect the boundaries of road features and drive the robot like a human. By comparing with AlexNet and VGG16 using the average reward in CarRacing-v0, our model wins the maximum overall system performance.",0
"Recently, it has been discovered that utilizing raw pixels captured from a single front-facing camera and translating them into steering commands yields surprisingly effective results. In this research paper, we introduce a convolutional neural network (CNN) that utilizes imitation learning to navigate the CarRacing-v0 game within the OpenAI Gym environment. The dataset utilized was produced manually through gameplay in Gym, and we expanded its size by four times through the utilization of a data augmentation method. Additionally, we incorporated true speed, four ABS sensors, steering wheel position, and gyroscope data for each image and created a hybrid model by merging sensor input and image input. Following training, our model is capable of automatically identifying road features and controlling the vehicle in a manner similar to a human driver. We compared our model's performance to that of AlexNet and VGG16 using the average reward in CarRacing-v0, and our model exceeded both in terms of overall system performance.",1
"QTRAN is a multi-agent reinforcement learning (MARL) algorithm capable of learning the largest class of joint-action value functions up to date. However, despite its strong theoretical guarantee, it has shown poor empirical performance in complex environments, such as Starcraft Multi-Agent Challenge (SMAC). In this paper, we identify the performance bottleneck of QTRAN and propose a substantially improved version, coined QTRAN++. Our gains come from (i) stabilizing the training objective of QTRAN, (ii) removing the strict role separation between the action-value estimators of QTRAN, and (iii) introducing a multi-head mixing network for value transformation. Through extensive evaluation, we confirm that our diagnosis is correct, and QTRAN++ successfully bridges the gap between empirical performance and theoretical guarantee. In particular, QTRAN++ newly achieves state-of-the-art performance in the SMAC environment. The code will be released.",0
"The QTRAN algorithm has the ability to learn the largest class of joint-action value functions to date, making it a powerful multi-agent reinforcement learning (MARL) tool. However, it has been found to have poor performance in complex environments, such as the Starcraft Multi-Agent Challenge (SMAC), despite its strong theoretical guarantee. In this research, we have identified the performance bottleneck of QTRAN and have proposed a greatly improved version, QTRAN++. Our improvements have been achieved by stabilizing the training objective of QTRAN, removing the strict role separation between its action-value estimators, and introducing a multi-head mixing network for value transformation. Our evaluation has confirmed the correctness of our diagnosis, and QTRAN++ has successfully bridged the gap between empirical performance and theoretical guarantee. In fact, QTRAN++ has achieved state-of-the-art performance in the SMAC environment. The code for QTRAN++ will be released.",1
"Reinforcement learning (RL) methods usually treat reward functions as black boxes. As such, these methods must extensively interact with the environment in order to discover rewards and optimal policies. In most RL applications, however, users have to program the reward function and, hence, there is the opportunity to treat reward functions as white boxes instead -- to show the reward function's code to the RL agent so it can exploit its internal structures to learn optimal policies faster. In this paper, we show how to accomplish this idea in two steps. First, we propose reward machines (RMs), a type of finite state machine that supports the specification of reward functions while exposing reward function structure. We then describe different methodologies to exploit such structures, including automated reward shaping, task decomposition, and counterfactual reasoning for data augmentation. Experiments on tabular and continuous domains show the benefits of exploiting reward structure across different tasks and RL agents.",0
"Reward functions are typically treated as black boxes in reinforcement learning (RL) methods, requiring extensive interaction with the environment to discover rewards and optimal policies. However, in many RL applications, users program the reward function, providing an opportunity to treat it as a white box. By exposing the reward function's code to the RL agent, it can exploit its internal structures to learn optimal policies faster. This paper presents two steps to achieve this. First, it introduces reward machines (RMs), which are finite state machines that support the specification of reward functions while exposing their structure. Then, it describes several methodologies to exploit this structure, including automated reward shaping, task decomposition, and counterfactual reasoning for data augmentation. The experiments on tabular and continuous domains demonstrate the benefits of exploiting reward structure across different tasks and RL agents.",1
"We introduce CuLE (CUDA Learning Environment), a CUDA port of the Atari Learning Environment (ALE) which is used for the development of deep reinforcement algorithms. CuLE overcomes many limitations of existing CPU-based emulators and scales naturally to multiple GPUs. It leverages GPU parallelization to run thousands of games simultaneously and it renders frames directly on the GPU, to avoid the bottleneck arising from the limited CPU-GPU communication bandwidth. CuLE generates up to 155M frames per hour on a single GPU, a finding previously achieved only through a cluster of CPUs. Beyond highlighting the differences between CPU and GPU emulators in the context of reinforcement learning, we show how to leverage the high throughput of CuLE by effective batching of the training data, and show accelerated convergence for A2C+V-trace. CuLE is available at https://github.com/NVLabs/cule .",0
"CuLE (CUDA Learning Environment) is a new tool designed for developing deep reinforcement algorithms, which is a CUDA port of the Atari Learning Environment (ALE). It solves many limitations of existing CPU-based emulators and can naturally scale to multiple GPUs. By utilizing GPU parallelization, it can run thousands of games concurrently and render frames directly on the GPU, eliminating the bottleneck caused by limited CPU-GPU communication bandwidth. CuLE can generate up to 155M frames per hour on a single GPU, which was previously only possible with a cluster of CPUs. In addition to highlighting the differences between CPU and GPU emulators in the reinforcement learning context, we demonstrate how to take advantage of CuLE's high throughput by effectively batching the training data and show accelerated convergence for A2C+V-trace. CuLE is available at https://github.com/NVLabs/cule.",1
"Efficient design of biological sequences will have a great impact across many industrial and healthcare domains. However, discovering improved sequences requires solving a difficult optimization problem. Traditionally, this challenge was approached by biologists through a model-free method known as ""directed evolution"", the iterative process of random mutation and selection. As the ability to build models that capture the sequence-to-function map improves, such models can be used as oracles to screen sequences before running experiments. In recent years, interest in better algorithms that effectively use such oracles to outperform model-free approaches has intensified. These span from approaches based on Bayesian Optimization, to regularized generative models and adaptations of reinforcement learning. In this work, we implement an open-source Fitness Landscape EXploration Sandbox (FLEXS: github.com/samsinai/FLEXS) environment to test and evaluate these algorithms based on their optimality, consistency, and robustness. Using FLEXS, we develop an easy-to-implement, scalable, and robust evolutionary greedy algorithm (AdaLead). Despite its simplicity, we show that AdaLead is a remarkably strong benchmark that out-competes more complex state of the art approaches in a variety of biologically motivated sequence design challenges.",0
"The efficient design of biological sequences is essential for a wide range of industrial and healthcare applications, but it is a challenging optimization problem. Biologists have traditionally used a model-free approach called ""directed evolution"" to discover better sequences through random mutation and selection. However, as models that can predict sequence-to-function relationships improve, they can be used to screen sequences before conducting experiments. Recently, there has been a growing interest in developing algorithms that use these models to outperform model-free approaches. These algorithms include Bayesian Optimization, regularized generative models, and adaptations of reinforcement learning. This study uses an open-source Fitness Landscape EXploration Sandbox (FLEXS) to evaluate these algorithms based on their optimality, consistency, and robustness. The researchers develop a simple but effective evolutionary greedy algorithm called AdaLead, which outperforms more complex state-of-the-art approaches in various biologically motivated sequence design challenges.",1
"We explore value-based multi-agent reinforcement learning (MARL) in the popular paradigm of centralized training with decentralized execution (CTDE). CTDE has an important concept, Individual-Global-Max (IGM) principle, which requires the consistency between joint and local action selections to support efficient local decision-making. However, in order to achieve scalability, existing MARL methods either limit representation expressiveness of their value function classes or relax the IGM consistency, which may suffer from instability risk or lead to poor performance. This paper presents a novel MARL approach, called duPLEX dueling multi-agent Q-learning (QPLEX), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the IGM principle into the neural network architecture and thus enables efficient value function learning. Theoretical analysis shows that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX significantly outperforms state-of-the-art baselines in both online and offline data collection settings, and also reveal that QPLEX achieves high sample efficiency and can benefit from offline datasets without additional online exploration.",0
"The article examines multi-agent reinforcement learning (MARL) using a value-based approach in the widely-used centralized training with decentralized execution (CTDE) paradigm. CTDE involves the Individual-Global-Max (IGM) principle, which requires joint and local action selections to be consistent for effective local decision-making. However, current MARL methods either limit the expressiveness of value function classes or relax IGM consistency to ensure scalability, which can result in poor performance or instability. The paper introduces a new MARL method, called QPLEX, which utilizes a duplex dueling architecture to factorize the joint value function and encode the IGM principle into the neural network architecture for efficient value function learning. Theoretical analysis indicates that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX outperforms state-of-the-art methods in both online and offline data collection settings, with high sample efficiency and the ability to benefit from offline datasets without additional online exploration.",1
"We deal with the problem of generating textual captions from optical remote sensing (RS) images using the notion of deep reinforcement learning. Due to the high inter-class similarity in reference sentences describing remote sensing data, jointly encoding the sentences and images encourages prediction of captions that are semantically more precise than the ground truth in many cases. To this end, we introduce an Actor Dual-Critic training strategy where a second critic model is deployed in the form of an encoder-decoder RNN to encode the latent information corresponding to the original and generated captions. While all actor-critic methods use an actor to predict sentences for an image and a critic to provide rewards, our proposed encoder-decoder RNN guarantees high-level comprehension of images by sentence-to-image translation. We observe that the proposed model generates sentences on the test data highly similar to the ground truth and is successful in generating even better captions in many critical cases. Extensive experiments on the benchmark Remote Sensing Image Captioning Dataset (RSICD) and the UCM-captions dataset confirm the superiority of the proposed approach in comparison to the previous state-of-the-art where we obtain a gain of sharp increments in both the ROUGE-L and CIDEr measures.",0
"The problem of producing written descriptions from optical remote sensing images is addressed using deep reinforcement learning. Since remote sensing reference sentences often have similar meanings, combining the sentences and images improves the accuracy of the generated captions compared to the actual captions in many cases. To achieve this, an Actor Dual-Critic training method is introduced, which uses a second critic model in the form of an encoder-decoder RNN to encode the original and generated captions' latent information. While most actor-critic methods use an actor and critic for sentence prediction and rewards, respectively, our encoder-decoder RNN ensures that the images are understood at a high level through sentence-to-image translation. The proposed model produces sentences similar to the ground truth on test data and generates even better captions in critical scenarios. The effectiveness of this approach is demonstrated through extensive experiments on the Remote Sensing Image Captioning Dataset (RSICD) and the UCM-captions dataset, resulting in significant improvements in both ROUGE-L and CIDEr measures over the previous state-of-the-art.",1
"The training of autonomous agents often requires expensive and unsafe trial-and-error interactions with the environment. Nowadays several data sets containing recorded experiences of intelligent agents performing various tasks, spanning from the control of unmanned vehicles to human-robot interaction and medical applications are accessible on the internet. With the intention of limiting the costs of the learning procedure it is convenient to exploit the information that is already available rather than collecting new data. Nevertheless, the incapability to augment the batch can lead the autonomous agents to develop far from optimal behaviours when the sampled experiences do not allow for a good estimate of the true distribution of the environment. Offline learning is the area of machine learning concerned with efficiently obtaining an optimal policy with a batch of previously collected experiences without further interaction with the environment. In this paper we adumbrate the ideas motivating the development of the state-of-the-art offline learning baselines. The listed methods consist in the introduction of epistemic uncertainty dependent constraints during the classical resolution of a Markov Decision Process, with and without function approximators, that aims to alleviate the bad effects of the distributional mismatch between the available samples and real world. We provide comments on the practical utility of the theoretical bounds that justify the application of these algorithms and suggest the utilization of Generative Adversarial Networks to estimate the distributional shift that affects all of the proposed model-free and model-based approaches.",0
"The training process for autonomous agents often involves risky and expensive trial-and-error interactions with the environment. Nowadays, there are various datasets available on the internet that contain recorded experiences of intelligent agents performing different tasks, such as controlling unmanned vehicles, human-robot interaction, and medical applications. To reduce the cost of learning, it is beneficial to use the existing information rather than collecting new data. However, if the batch cannot be augmented, the autonomous agents may develop suboptimal behaviors when the sampled experiences do not accurately represent the environment. Offline learning is a machine learning field that aims to efficiently obtain an optimal policy using previously collected experiences without further interaction with the environment. This paper outlines the ideas that motivate the development of the latest offline learning baselines. These methods introduce epistemic uncertainty dependent constraints during the resolution of a Markov Decision Process, with or without function approximators, to mitigate the adverse effects of the distributional mismatch between the available samples and the real world. We discuss the practical usefulness of the theoretical bounds that justify the application of these algorithms and suggest using Generative Adversarial Networks to estimate the distributional shift that affects all proposed model-free and model-based approaches.",1
"Training agents using Reinforcement Learning in games with sparse rewards is a challenging problem, since large amounts of exploration are required to retrieve even the first reward. To tackle this problem, a common approach is to use reward shaping to help exploration. However, an important drawback of reward shaping is that agents sometimes learn to optimize the shaped reward instead of the true objective. In this paper, we present a novel technique that we call action guidance that successfully trains agents to eventually optimize the true objective in games with sparse rewards while maintaining most of the sample efficiency that comes with reward shaping. We evaluate our approach in a simplified real-time strategy (RTS) game simulator called $\mu$RTS.",0
"It is challenging to train agents using Reinforcement Learning in games that offer sparse rewards. The retrieval of the first reward requires extensive exploration. Reward shaping is a common approach to address this issue, but it has an important drawback - it causes agents to optimize the shaped reward instead of the true objective. In this paper, we introduce a new technique called action guidance that overcomes this problem. Our approach trains agents to optimize the true objective in games with sparse rewards while maintaining sample efficiency that comes with reward shaping. We tested our technique in $\mu$RTS, a simplified real-time strategy (RTS) game simulator.",1
"Conservation laws are considered to be fundamental laws of nature. It has broad applications in many fields, including physics, chemistry, biology, geology, and engineering. Solving the differential equations associated with conservation laws is a major branch in computational mathematics. The recent success of machine learning, especially deep learning in areas such as computer vision and natural language processing, has attracted a lot of attention from the community of computational mathematics and inspired many intriguing works in combining machine learning with traditional methods. In this paper, we are the first to view numerical PDE solvers as an MDP and to use (deep) RL to learn new solvers. As proof of concept, we focus on 1-dimensional scalar conservation laws. We deploy the machinery of deep reinforcement learning to train a policy network that can decide on how the numerical solutions should be approximated in a sequential and spatial-temporal adaptive manner. We will show that the problem of solving conservation laws can be naturally viewed as a sequential decision-making process, and the numerical schemes learned in such a way can easily enforce long-term accuracy. Furthermore, the learned policy network is carefully designed to determine a good local discrete approximation based on the current state of the solution, which essentially makes the proposed method a meta-learning approach. In other words, the proposed method is capable of learning how to discretize for a given situation mimicking human experts. Finally, we will provide details on how the policy network is trained, how well it performs compared with some state-of-the-art numerical solvers such as WENO schemes, and supervised learning based approach L3D and PINN, and how well it generalizes.",0
"The fundamental laws of nature are known as conservation laws and have extensive applications in various fields, including biology, chemistry, engineering, geology, and physics. One significant aspect of computational mathematics is solving differential equations related to conservation laws. The community of computational mathematics has shown a keen interest in combining traditional methods with machine learning, especially deep learning, which has been successful in computer vision and natural language processing. This paper proposes a novel approach by considering numerical PDE solvers as an MDP and using (deep) RL to train new solvers. The proposed method focuses on 1-dimensional scalar conservation laws and trains a policy network to determine the numerical solutions in a spatial-temporal adaptive manner, enforcing long-term accuracy. The policy network is designed to learn from the current state of the solution, making it a meta-learning approach capable of mimicking human experts. The paper includes details on the training process, performance comparison with state-of-the-art numerical solvers, and generalization ability.",1
"Reinforcement Learning (RL) agents typically learn memoryless policies---policies that only consider the last observation when selecting actions. Learning memoryless policies is efficient and optimal in fully observable environments. However, some form of memory is necessary when RL agents are faced with partial observability. In this paper, we study a lightweight approach to tackle partial observability in RL. We provide the agent with an external memory and additional actions to control what, if anything, is written to the memory. At every step, the current memory state is part of the agent's observation, and the agent selects a tuple of actions: one action that modifies the environment and another that modifies the memory. When the external memory is sufficiently expressive, optimal memoryless policies yield globally optimal solutions. Unfortunately, previous attempts to use external memory in the form of binary memory have produced poor results in practice. Here, we investigate alternative forms of memory in support of learning effective memoryless policies. Our novel forms of memory outperform binary and LSTM-based memory in well-established partially observable domains.",0
"In Reinforcement Learning (RL), agents typically acquire memoryless policies that only consider the most recent observation to select actions. This approach is efficient and optimal for fully observable environments, but inadequate when RL agents face partial observability. In this research, we explore a lightweight method to handle partial observability in RL by equipping the agent with an external memory and additional actions to control the writing of information to the memory. The current memory status becomes part of the agent's observation at every step, and the agent selects a tuple of actions comprising one action to modify the environment and another to modify the memory. Optimal memoryless policies produce globally optimal solutions when the external memory is expressive enough. However, binary memory has produced unfavorable outcomes in previous attempts to use external memory. Therefore, we investigate alternative memory forms that surpass binary and LSTM-based memory in well-established partially observable domains to facilitate the learning of effective memoryless policies.",1
"Real-world applications that involve missing values are often constrained by the cost to obtain data. Test-cost sensitive, or costly feature, methods additionally consider the cost of acquiring features. Such methods have been extensively studied in the problem of classification. In this paper, we study a related problem of test-cost sensitive methods to identify nearby points from a large set, given a new point with some unknown feature values. We present two models, one based on a tree and another based on Deep Reinforcement Learning. In our simulations, we show that the models outperform random agents on a set of five real-world data sets.",0
"When dealing with missing values in real-life applications, the cost of obtaining data can be a limiting factor. Costly feature or test-cost sensitive techniques take into account the expense of acquiring features and have been widely investigated in classification problems. This study focuses on test-cost sensitive methods for identifying neighboring points from a large set, when presented with a new point with unknown feature values. We introduce two models - one tree-based and another Deep Reinforcement Learning-based - and demonstrate through simulations that they outperform random agents using five real-world data sets.",1
"A key question in reinforcement learning is how an intelligent agent can generalize knowledge across different inputs. By generalizing across different inputs, information learned for one input can be immediately reused for improving predictions for another input. Reusing information allows an agent to compute an optimal decision-making strategy using less data. State representation is a key element of the generalization process, compressing a high-dimensional input space into a low-dimensional latent state space. This article analyzes properties of different latent state spaces, leading to new connections between model-based and model-free reinforcement learning. Successor features, which predict frequencies of future observations, form a link between model-based and model-free learning: Learning to predict future expected reward outcomes, a key characteristic of model-based agents, is equivalent to learning successor features. Learning successor features is a form of temporal difference learning and is equivalent to learning to predict a single policy's utility, which is a characteristic of model-free agents. Drawing on the connection between model-based reinforcement learning and successor features, we demonstrate that representations that are predictive of future reward outcomes generalize across variations in both transitions and rewards. This result extends previous work on successor features, which is constrained to fixed transitions and assumes re-learning of the transferred state representation.",0
"Reinforcement learning raises an important question regarding an intelligent agent's ability to apply knowledge across different inputs. By doing so, information learned for one input can be reused to enhance predictions for another input, thereby enabling the agent to make optimal decisions with less data. To achieve this, state representation plays a crucial role in compressing a high-dimensional input space into a low-dimensional latent state space. This article examines various latent state spaces and identifies connections between model-based and model-free reinforcement learning through the use of successor features. These features predict future observations and are a form of temporal difference learning. Learning successor features is equivalent to learning a single policy's utility, which is a characteristic of model-free agents. By applying this connection, we show that representations that predict future reward outcomes can generalize across variations in transitions and rewards, extending the previous work on successor features that was limited to fixed transitions and required re-learning of the transferred state representation.",1
"We present a method to automatically find security strategies for the use case of intrusion prevention. Following this method, we model the interaction between an attacker and a defender as a Markov game and let attack and defense strategies evolve through reinforcement learning and self-play without human intervention. Using a simple infrastructure configuration, we demonstrate that effective security strategies can emerge from self-play. This shows that self-play, which has been applied in other domains with great success, can be effective in the context of network security. Inspection of the converged policies show that the emerged policies reflect common-sense knowledge and are similar to strategies of humans. Moreover, we address known challenges of reinforcement learning in this domain and present an approach that uses function approximation, an opponent pool, and an autoregressive policy representation. Through evaluations we show that our method is superior to two baseline methods but that policy convergence in self-play remains a challenge.",0
"A technique to discover security measures for intrusion prevention is introduced in this study. The approach includes modeling the interaction between an attacker and a defender as a Markov game and allowing attack and defense strategies to develop through reinforcement learning and self-play without intervention from humans. The effectiveness of the security strategies derived from self-play is demonstrated using a basic infrastructure configuration. The study proves that self-play, which has been successful in other areas, can also be beneficial in network security. The policies that result from self-play show that they reflect common-sense knowledge and are similar to human strategies. Additionally, the study addresses the challenges of reinforcement learning in this area by presenting a technique that uses function approximation, an opponent pool, and an autoregressive policy representation. The results indicate that our method is more effective than two baseline methods, but policy convergence in self-play remains a challenge.",1
"Multi-agent policy gradient (MAPG) methods recently witness vigorous progress. However, there is a significant performance discrepancy between MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate causes that hinder the performance of MAPG algorithms and present a multi-agent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that DOP significantly outperforms both state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https://sites.google.com/view/dop-mapg/.",0
"MAPG methods have seen significant progress, but they do not perform as well as state-of-the-art multi-agent value-based approaches. To improve the performance of MAPG algorithms, this paper presents a new approach called multi-agent decomposed policy gradient (DOP). DOP introduces the idea of value function decomposition into the multi-agent actor-critic framework, which supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. The paper formally proves that DOP critics have sufficient representational capability to ensure convergence. Empirical evaluations on StarCraft II micromanagement benchmark and multi-agent particle environments show that DOP significantly outperforms both state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https://sites.google.com/view/dop-mapg/.",1
"In this paper, we investigate a computing task scheduling problem in space-air-ground integrated network (SAGIN) for delay-oriented Internet of Things (IoT) services. In the considered scenario, an unmanned aerial vehicle (UAV) collects computing tasks from IoT devices and then makes online offloading decisions, in which the tasks can be processed at the UAV or offloaded to the nearby base station or the remote satellite. Our objective is to design a task scheduling policy that minimizes offloading and computing delay of all tasks given the UAV energy capacity constraint. To this end, we first formulate the online scheduling problem as an energy-constrained Markov decision process (MDP). Then, considering the task arrival dynamics, we develop a novel deep risk-sensitive reinforcement learning algorithm. Specifically, the algorithm evaluates the risk, which measures the energy consumption that exceeds the constraint, for each state and searches the optimal parameter weighing the minimization of delay and risk while learning the optimal policy. Extensive simulation results demonstrate that the proposed algorithm can reduce the task processing delay by up to 30% compared to probabilistic configuration methods while satisfying the UAV energy capacity constraint.",0
"The aim of this study is to investigate a task scheduling problem in the SAGIN for delay-oriented IoT services. The scenario involves a UAV collecting computing tasks from IoT devices and making online offloading decisions. The tasks can be processed at the UAV or offloaded to a nearby base station or remote satellite. The objective is to minimize offloading and computing delay of all tasks while adhering to the UAV energy capacity constraint. The researchers formulated the scheduling problem as an energy-constrained MDP and developed a new deep risk-sensitive reinforcement learning algorithm to address it. The algorithm evaluates the risk of each state, which measures the energy consumption exceeding the constraint. It searches for the optimal parameter weighing the minimization of delay and risk while learning the optimal policy. Extensive simulation results showed that the proposed algorithm can reduce task processing delay by up to 30% compared to probabilistic configuration methods while maintaining the UAV energy capacity constraint.",1
"Intelligent robots need to achieve abstract objectives using concrete, spatiotemporally complex sensory information and motor control. Tabula rasa deep reinforcement learning (RL) has tackled demanding tasks in terms of either visual, abstract, or physical reasoning, but solving these jointly remains a formidable challenge. One recent, unsolved benchmark task that integrates these challenges is Mujoban, where a robot needs to arrange 3D warehouses generated from 2D Sokoban puzzles. We explore whether integrated tasks like Mujoban can be solved by composing RL modules together in a sense-plan-act hierarchy, where modules have well-defined roles similarly to classic robot architectures. Unlike classic architectures that are typically model-based, we use only model-free modules trained with RL or supervised learning. We find that our modular RL approach dramatically outperforms the state-of-the-art monolithic RL agent on Mujoban. Further, learned modules can be reused when, e.g., using a different robot platform to solve the same task. Together our results give strong evidence for the importance of research into modular RL designs. Project website: https://sites.google.com/view/modular-rl/",0
"The complex nature of spatiotemporally sensory information and motor control makes it necessary for intelligent robots to achieve abstract objectives. While recent advancements in Tabula rasa deep reinforcement learning (RL) have managed to handle tasks focused on visual, abstract, or physical reasoning, integrating these remains a challenge. Mujoban is an unsolved benchmark task that combines these challenges, requiring a robot to arrange 3D warehouses from 2D Sokoban puzzles. We investigate whether composing RL modules in a sense-plan-act hierarchy, similar to classic robot architectures, can solve integrated tasks like Mujoban. Unlike traditional architectures, we use only model-free modules trained with RL or supervised learning. Our modular RL approach outperforms the state-of-the-art monolithic RL agent on Mujoban, and learned modules can be reused for similar tasks using different robot platforms. These results highlight the significance of research into modular RL designs. For more details, visit our project website at https://sites.google.com/view/modular-rl/.",1
"Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical TD algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks.",0
"The use of function approximation in reinforcement learning can result in instability and divergence, especially when combined with off-policy learning and Bellman updates. However, deep reinforcement learning has addressed these issues through empirical adaptation and regularization of the representation, with auxiliary tasks playing a key role. This suggests that representation learning can ensure stability. This paper formally demonstrates the existence of nontrivial state representations that make the TD algorithm stable, even when learning off-policy. The authors analyze representation learning schemes based on policy transition matrices, such as proto-value functions, along three dimensions: approximation error, stability, and estimation ease. They find that a Schur basis provides convergence guarantees in the most general case, but it is challenging to estimate from samples. For a fixed reward function, an orthogonal basis of the corresponding Krylov subspace is an even better choice. The authors conclude by showing that these stable representations can be learned using stochastic gradient descent, offering improved techniques for representation learning with deep networks.",1
"Motivated by the episodic version of the classical inventory control problem, we propose a new Q-learning-based algorithm, Elimination-Based Half-Q-Learning (HQL), that enjoys improved efficiency over existing algorithms for a wide variety of problems in the one-sided-feedback setting. We also provide a simpler variant of the algorithm, Full-Q-Learning (FQL), for the full-feedback setting. We establish that HQL incurs $ \tilde{\mathcal{O}}(H^3\sqrt{ T})$ regret and FQL incurs $\tilde{\mathcal{O}}(H^2\sqrt{ T})$ regret, where $H$ is the length of each episode and $T$ is the total length of the horizon. The regret bounds are not affected by the possibly huge state and action space. Our numerical experiments demonstrate the superior efficiency of HQL and FQL, and the potential to combine reinforcement learning with richer feedback models.",0
"We introduce a new algorithm called Elimination-Based Half-Q-Learning (HQL) that is based on Q-learning and is designed to address the classical inventory control problem in an episodic setting. This algorithm offers improved efficiency compared to existing approaches in scenarios that involve one-sided feedback. We also present a simplified version of the algorithm called Full-Q-Learning (FQL), which is suitable for full-feedback settings. The regret incurred by HQL is approximately $\tilde{\mathcal{O}}(H^3\sqrt{T})$, while that of FQL is around $\tilde{\mathcal{O}}(H^2\sqrt{T})$, where $H$ represents the episode length and $T$ represents the total horizon length. These regret bounds are not affected by the size of the state and action space. Our experiments demonstrate the effectiveness of HQL and FQL and their potential for integration with more complex feedback models in reinforcement learning.",1
"Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a simple and stable alternative, it requires access to demonstrations from a human supervisor. In this paper, we study RL algorithms that use imitation learning to acquire goal reaching policies from scratch, without the need for expert demonstrations or a value function. In lieu of demonstrations, we leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. We propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal-reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. We formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal-reaching performance and robustness over current RL algorithms in several benchmark tasks.",0
"Reinforcement learning (RL) algorithms can be challenging to work with and may lack stability, particularly when it comes to learning goal-reaching behaviors from rewards that are few and far between. While supervised imitation learning offers a straightforward and stable alternative, it relies on demonstrations from a human supervisor. In this study, we examine RL algorithms that use imitation learning to acquire goal-reaching policies from scratch, without the need for expert demonstrations or a value function. Instead of demonstrations, we use the fact that any trajectory can serve as a successful demonstration for reaching the final state in that same trajectory. We present a simple algorithm in which an agent repeatedly labels and imitates the trajectories it generates to learn goal-reaching behaviors from scratch. At each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the actual goal reached to enhance the policy. We demonstrate formally that this supervised learning procedure optimizes a bound on the RL target, derive performance bounds of the learned policy, and show through empirical data that the approach offers improved goal-reaching performance and robustness compared to current RL algorithms in various benchmark tasks.",1
"We study the problem of balancing effectiveness and efficiency in automated feature selection. After exploring many feature selection methods, we observe a computational dilemma: 1) traditional feature selection is mostly efficient, but difficult to identify the best subset; 2) the emerging reinforced feature selection automatically navigates to the best subset, but is usually inefficient. Can we bridge the gap between effectiveness and efficiency under automation? Motivated by this dilemma, we aim to develop a novel feature space navigation method. In our preliminary work, we leveraged interactive reinforcement learning to accelerate feature selection by external trainer-agent interaction. In this journal version, we propose a novel interactive and closed-loop architecture to simultaneously model interactive reinforcement learning (IRL) and decision tree feedback (DTF). Specifically, IRL is to create an interactive feature selection loop and DTF is to feed structured feature knowledge back to the loop. First, the tree-structured feature hierarchy from decision tree is leveraged to improve state representation. In particular, we represent the selected feature subset as an undirected graph of feature-feature correlations and a directed tree of decision features. We propose a new embedding method capable of empowering graph convolutional network to jointly learn state representation from both the graph and the tree. Second, the tree-structured feature hierarchy is exploited to develop a new reward scheme. In particular, we personalize reward assignment of agents based on decision tree feature importance. In addition, observing agents' actions can be feedback, we devise another reward scheme, to weigh and assign reward based on the feature selected frequency ratio in historical action records. Finally, we present extensive experiments on real-world datasets to show the improved performance.",0
"The focus of our research is to find a balance between effectiveness and efficiency in automated feature selection. We have discovered a dilemma with traditional feature selection methods, which are efficient but struggle to identify the best subset, and reinforced feature selection methods, which can automatically navigate to the best subset but are often inefficient. Our goal is to develop a new feature space navigation method that can bridge this gap. Our preliminary work involved using interactive reinforcement learning to speed up feature selection through external trainer-agent interaction. In this journal version, we propose an interactive and closed-loop architecture that combines interactive reinforcement learning (IRL) with decision tree feedback (DTF). To improve state representation, we use the tree-structured feature hierarchy from decision trees to represent the selected feature subset as an undirected graph of feature-feature correlations and a directed tree of decision features. We also create a new reward scheme that is personalized based on decision tree feature importance and feature selection frequency ratio in historical action records. Our experiments on real-world datasets demonstrate improved performance.",1
"In this study, we address image retargeting, which is a task that adjusts input images to arbitrary sizes. In one of the best-performing methods called MULTIOP, multiple retargeting operators were combined and retargeted images at each stage were generated to find the optimal sequence of operators that minimized the distance between original and retargeted images. The limitation of this method is in its tremendous processing time, which severely prohibits its practical use. Therefore, the purpose of this study is to find the optimal combination of operators within a reasonable processing time; we propose a method of predicting the optimal operator for each step using a reinforcement learning agent. The technical contributions of this study are as follows. Firstly, we propose a reward based on self-play, which will be insensitive to the large variance in the content-dependent distance measured in MULTIOP. Secondly, we propose to dynamically change the loss weight for each action to prevent the algorithm from falling into a local optimum and from choosing only the most frequently used operator in its training. Our experiments showed that we achieved multi-operator image retargeting with less processing time by three orders of magnitude and the same quality as the original multi-operator-based method, which was the best-performing algorithm in retargeting tasks.",0
"The focus of our research is on image retargeting, a task that involves resizing input images to arbitrary dimensions. The MULTIOP method, considered one of the most effective techniques for this task, combines multiple retargeting operators and generates retargeted images at each stage to determine the optimal sequence of operators that minimizes the difference between the original and retargeted images. However, this method suffers from a significant drawback, namely, its excessive processing time, which limits its practical application. Therefore, our objective in this study is to identify the optimal combination of operators within a reasonable processing time. To accomplish this, we propose a method that utilizes a reinforcement learning agent to predict the optimal operator for each step. Our technical contributions include a reward system based on self-play, which is not influenced by the content-dependent distance measured in MULTIOP, and a dynamic loss weight that prevents the algorithm from choosing only the most frequently used operator during training. Our experiments demonstrate that our approach achieves multi-operator image retargeting with significantly less processing time (three orders of magnitude less) and the same level of quality as the original MULTIOP-based method, which was the best-performing algorithm for retargeting tasks.",1
"In this paper, we present an approach based on reinforcement learning for eye tracking data manipulation. It is based on two opposing agents, where one tries to classify the data correctly and the second agent looks for patterns in the data, which get manipulated to hide specific information. We show that our approach is successfully applicable to preserve the privacy of the subjects. For this purpose, we evaluate our approach iteratively to showcase the behavior of the reinforcement learning based approach. In addition, we evaluate the importance of temporal, as well as spatial, information of eye tracking data for specific classification goals. In the last part of our evaluation, we apply the procedure to further public data sets without re-training the autoencoder or the data manipulator. The results show that the learned manipulation is generalized and applicable to unseen data as well.",0
"This paper introduces a reinforcement learning approach to manipulate eye tracking data in order to preserve the privacy of subjects. The approach involves two agents with opposing goals: one aims to correctly classify the data, while the other seeks to identify patterns in the data and manipulate them to conceal specific information. Our evaluation of the approach is iterative, demonstrating its effectiveness in maintaining subject privacy. We also analyze the significance of temporal and spatial information in eye tracking data for specific classification objectives. Furthermore, we apply the technique to additional public data sets without retraining the autoencoder or data manipulator, and find that the learned manipulation is generalizable to unseen data.",1
"Reinforcement learning methods for traffic signal control has gained increasing interests recently and achieved better performances compared with traditional transportation methods. However, reinforcement learning based methods usually requires heavy training data and computational resources which largely limit its application in real-world traffic signal control. This makes meta-learning, which enables data-efficient and fast-adaptation training by leveraging the knowledge of previous learning experiences, catches attentions in traffic signal control. In this paper, we propose a novel value-based Bayesian meta-reinforcement learning framework BM-DQN to robustly speed up the learning process in new scenarios by utilizing well-trained prior knowledge learned from existing scenarios. This framework based on our proposed fast-adaptation variation to Gradient-EM Bayesian Meta-learning and the fast update advantage of DQN, which allows fast adaptation to new scenarios with continual learning ability and robustness to uncertainty. The experiments on 2D navigation and traffic signal control show that our proposed framework adapts more quickly and robustly in new scenarios than previous methods, and specifically, much better continual learning ability in heterogeneous scenarios.",0
"Recently, there has been growing interest in using reinforcement learning techniques for traffic signal control, which have shown better performance compared to traditional transportation methods. However, these methods typically require extensive training data and computational resources, limiting their real-world application. As a result, meta-learning has become a popular approach in traffic signal control to enable data-efficient and fast-adaptation training by leveraging prior learning experiences. In this study, we introduce a novel value-based Bayesian meta-reinforcement learning framework, BM-DQN, which utilizes well-trained prior knowledge from existing scenarios to robustly accelerate the learning process in new scenarios. Our framework is based on a fast-adaptation variation to Gradient-EM Bayesian Meta-learning and the fast update advantage of DQN, enabling fast adaptation to new scenarios with continual learning ability and robustness to uncertainty. Our experiments on 2D navigation and traffic signal control demonstrate that our framework adapts more quickly and robustly in new scenarios than previous methods, particularly exhibiting superior continual learning ability in heterogeneous scenarios.",1
"Visual object tracking was generally tackled by reasoning independently on fast processing algorithms, accurate online adaptation methods, and fusion of trackers. In this paper, we unify such goals by proposing a novel tracking methodology that takes advantage of other visual trackers, offline and online. A compact student model is trained via the marriage of knowledge distillation and reinforcement learning. The first allows to transfer and compress tracking knowledge of other trackers. The second enables the learning of evaluation measures which are then exploited online. After learning, the student can be ultimately used to build (i) a very fast single-shot tracker, (ii) a tracker with a simple and effective online adaptation mechanism, (iii) a tracker that performs fusion of other trackers. Extensive validation shows that the proposed algorithms compete with real-time state-of-the-art trackers.",0
"The conventional approach to visual object tracking involved developing fast processing algorithms, accurate online adaptation techniques, and combining different trackers. However, this paper introduces a new tracking technique that combines these objectives by utilizing both offline and online visual trackers. This is achieved through a novel methodology that employs knowledge distillation and reinforcement learning to train a compact student model. The former facilitates the transfer and compression of tracking knowledge from other trackers, while the latter enables the learning of evaluation measures which are subsequently utilized online. The resulting student model can be used to create a fast single-shot tracker, a tracker with a simple and effective online adaptation mechanism, or a tracker that integrates multiple trackers. Extensive validation demonstrates that the proposed algorithms are competitive with real-time state-of-the-art trackers.",1
"Finding the optimal signal timing strategy is a difficult task for the problem of large-scale traffic signal control (TSC). Multi-Agent Reinforcement Learning (MARL) is a promising method to solve this problem. However, there is still room for improvement in extending to large-scale problems and modeling the behaviors of other agents for each individual agent. In this paper, a new MARL, called Cooperative double Q-learning (Co-DQL), is proposed, which has several prominent features. It uses a highly scalable independent double Q-learning method based on double estimators and the UCB policy, which can eliminate the over-estimation problem existing in traditional independent Q-learning while ensuring exploration. It uses mean field approximation to model the interaction among agents, thereby making agents learn a better cooperative strategy. In order to improve the stability and robustness of the learning process, we introduce a new reward allocation mechanism and a local state sharing method. In addition, we analyze the convergence properties of the proposed algorithm. Co-DQL is applied on TSC and tested on a multi-traffic signal simulator. According to the results obtained on several traffic scenarios, Co- DQL outperforms several state-of-the-art decentralized MARL algorithms. It can effectively shorten the average waiting time of the vehicles in the whole road system.",0
"Developing an effective signal timing strategy for large-scale traffic signal control (TSC) is a challenging task. Multi-Agent Reinforcement Learning (MARL) is a promising solution, but further improvements are needed to better handle large-scale problems and model the behaviors of individual agents. This paper proposes a new MARL approach called Cooperative double Q-learning (Co-DQL), which has several key features. Co-DQL uses a scalable double Q-learning method based on double estimators and the UCB policy to eliminate over-estimation issues while ensuring exploration. It employs mean field approximation to model inter-agent interactions, enabling agents to learn better cooperative strategies. The algorithm also includes a new reward allocation mechanism and local state sharing method to enhance stability and robustness during learning. The proposed algorithm's convergence properties are analyzed, and Co-DQL is tested on TSC using a multi-traffic signal simulator. The results demonstrate that Co-DQL outperforms several state-of-the-art decentralized MARL algorithms and effectively reduces the average waiting time of vehicles across the road system.",1
"Graphs can be used to represent and reason about real world systems and a variety of metrics have been devised to quantify their global characteristics. An important property is robustness to failures and attacks, which is relevant for the infrastructure and communication networks that power modern society. Prior work on making topological modifications to a graph, e.g., adding edges, in order to increase robustness is typically based on local and spectral properties or a shallow search since robustness is expensive to compute directly. However, such strategies are necessarily suboptimal.   In this work, we present RNet-DQN, an approach for constructing networks that uses Reinforcement Learning to address improving the robustness of graphs to random and targeted removals of nodes. In particular, the approach relies on changes in the estimated robustness as a reward signal and Graph Neural Networks for representing states. Experiments on synthetic and real-world graphs show that this approach can deliver performance superior to existing methods while being much cheaper to evaluate and generalizing to out-of-sample graphs, as well as to larger out-of-distribution graphs in some cases. The approach is readily applicable to optimizing other global structural properties of graphs.",0
"The use of graphs to represent and analyze real world systems is common, with various metrics used to measure their overall characteristics. One important feature is their ability to withstand failures and attacks, which is particularly relevant for networks that support modern society. Previous attempts to modify graph topology for increased robustness have relied on local and spectral properties or shallow searches, as direct computation is costly. However, these strategies are not optimal. In this study, we introduce RNet-DQN, a network construction approach that uses Reinforcement Learning to improve graph robustness against both random and targeted node removals. Our method relies on estimated robustness changes as a reward signal and Graph Neural Networks for state representation. Tests on synthetic and real-world graphs demonstrate superior performance compared to existing methods, with lower evaluation costs and the ability to generalize to out-of-sample and larger out-of-distribution graphs. The approach is also applicable to optimizing other global structural properties of graphs.",1
"Reinforcement learning (RL) has achieved remarkable performance in numerous sequential decision making and control tasks. However, a common problem is that learned nearly optimal policy always overfits to the training environment and may not be extended to situations never encountered during training. For practical applications, the randomness of environment usually leads to some devastating events, which should be the focus of safety-critical systems such as autonomous driving. In this paper, we introduce the minimax formulation and distributional framework to improve the generalization ability of RL algorithms and develop the Minimax Distributional Soft Actor-Critic (Minimax DSAC) algorithm. Minimax formulation aims to seek optimal policy considering the most severe variations from environment, in which the protagonist policy maximizes action-value function while the adversary policy tries to minimize it. Distributional framework aims to learn a state-action return distribution, from which we can model the risk of different returns explicitly, thereby formulating a risk-averse protagonist policy and a risk-seeking adversarial policy. We implement our method on the decision-making tasks of autonomous vehicles at intersections and test the trained policy in distinct environments. Results demonstrate that our method can greatly improve the generalization ability of the protagonist agent to different environmental variations.",0
"In various sequential decision-making and control tasks, Reinforcement Learning (RL) has shown impressive performance. However, a common issue is that the nearly optimal policy learned tends to overfit the training environment and may not be applicable to situations that were not encountered during training. This poses a significant challenge for safety-critical systems like autonomous driving, where the randomness of the environment can result in catastrophic outcomes. To address this, we introduce the Minimax Distributional Soft Actor-Critic (Minimax DSAC) algorithm that utilizes the minimax formulation and distributional framework to enhance the generalization ability of RL algorithms. The minimax formulation aims to find the optimal policy by considering the most severe variations in the environment. This is done by maximizing the action-value function for the protagonist policy while minimizing it for the adversary policy. The distributional framework aims to learn a state-action distribution of returns that explicitly models the risk of different returns. This allows us to develop a risk-averse protagonist policy and a risk-seeking adversarial policy. We apply our method to decision-making tasks of autonomous vehicles at intersections and test the trained policy in diverse environments. Our results show that our approach significantly improves the generalization ability of the protagonist agent to different environmental variations.",1
"While researchers in the asset management industry have mostly focused on techniques based on financial and risk planning techniques like Markowitz efficient frontier, minimum variance, maximum diversification or equal risk parity, in parallel, another community in machine learning has started working on reinforcement learning and more particularly deep reinforcement learning to solve other decision making problems for challenging task like autonomous driving, robot learning, and on a more conceptual side games solving like Go. This paper aims to bridge the gap between these two approaches by showing Deep Reinforcement Learning (DRL) techniques can shed new lights on portfolio allocation thanks to a more general optimization setting that casts portfolio allocation as an optimal control problem that is not just a one-step optimization, but rather a continuous control optimization with a delayed reward. The advantages are numerous: (i) DRL maps directly market conditions to actions by design and hence should adapt to changing environment, (ii) DRL does not rely on any traditional financial risk assumptions like that risk is represented by variance, (iii) DRL can incorporate additional data and be a multi inputs method as opposed to more traditional optimization methods. We present on an experiment some encouraging results using convolution networks.",0
"The asset management industry's focus has been primarily on financial and risk planning techniques, such as Markowitz efficient frontier, minimum variance, maximum diversification, and equal risk parity. However, a community in machine learning has emerged, working on reinforcement learning, particularly deep reinforcement learning, to tackle decision-making problems such as autonomous driving, robot learning, and game-solving like Go. This paper seeks to bridge the gap between these approaches by demonstrating how Deep Reinforcement Learning (DRL) techniques can provide new insights into portfolio allocation. This is achieved through a more comprehensive optimization approach that treats portfolio allocation as an optimal control problem, involving continuous control optimization with delayed rewards rather than a single-step optimization. DRL offers various advantages, such as its ability to adapt to market conditions, not relying on traditional financial risk assumptions, and its incorporation of multiple inputs. We present an experiment that showcases the promising results of using convolution networks.",1
"Can an agent learn efficiently in a noisy and self adapting environment with sequential, non-stationary and non-homogeneous observations? Through trading bots, we illustrate how Deep Reinforcement Learning (DRL) can tackle this challenge. Our contributions are threefold: (i) the use of contextual information also referred to as augmented state in DRL, (ii) the impact of a one period lag between observations and actions that is more realistic for an asset management environment, (iii) the implementation of a new repetitive train test method called walk forward analysis, similar in spirit to cross validation for time series. Although our experiment is on trading bots, it can easily be translated to other bot environments that operate in sequential environment with regime changes and noisy data. Our experiment for an augmented asset manager interested in finding the best portfolio for hedging strategies shows that AAMDRL achieves superior returns and lower risk.",0
"Is it possible for an agent to efficiently learn in a self-adapting and noisy environment with non-stationary, non-homogeneous, and sequential observations? By utilizing trading bots, we demonstrate how Deep Reinforcement Learning (DRL) can overcome this challenge. Our contributions are threefold: (i) introducing contextual information, or augmented state, in DRL; (ii) implementing a one-period lag between observations and actions, which is more realistic for asset management; and (iii) developing a new repetitive train-test method called walk-forward analysis, similar to time series cross-validation. Although we conducted our experiment on trading bots, it is easily transferable to other bot environments that operate in a sequential environment with noisy data and regime changes. Our experiment shows that AAMDRL (augmented asset manager DRL) achieves superior returns and lower risk for an asset manager looking to find the best portfolio for hedging strategies.",1
"Existing image captioning models are usually trained by cross-entropy (XE) loss and reinforcement learning (RL), which set ground-truth words as hard targets and force the captioning model to learn from them. However, the widely adopted training strategies suffer from misalignment in XE training and inappropriate reward assignment in RL training. To tackle these problems, we introduce a teacher model that serves as a bridge between the ground-truth caption and the caption model by generating some easier-to-learn word proposals as soft targets. The teacher model is constructed by incorporating the ground-truth image attributes into the baseline caption model. To effectively learn from the teacher model, we propose Teacher-Critical Training Strategies (TCTS) for both XE and RL training to facilitate better learning processes for the caption model. Experimental evaluations of several widely adopted caption models on the benchmark MSCOCO dataset show the proposed TCTS comprehensively enhances most evaluation metrics, especially the Bleu and Rouge-L scores, in both training stages. TCTS is able to achieve to-date the best published single model Bleu-4 and Rouge-L performances of 40.2% and 59.4% on the MSCOCO Karpathy test split. Our codes and pre-trained models will be open-sourced.",0
"Typically, image captioning models are trained using cross-entropy (XE) loss and reinforcement learning (RL), which enforce the model to learn from hard targets set by ground-truth words. However, these commonly used training methods often encounter misalignment issues during XE training and inappropriate reward assignment in RL training. To address these issues, we propose a teacher model that generates easier-to-learn word proposals as soft targets to bridge the gap between the ground-truth caption and the caption model. The teacher model is developed by integrating the image attributes into the baseline caption model. We also introduce Teacher-Critical Training Strategies (TCTS) for both XE and RL training to enable better learning processes for the caption model. Experimental evaluations on the MSCOCO dataset show that TCTS significantly improves most evaluation metrics, especially the Bleu and Rouge-L scores, in both training stages. TCTS achieves the best published single model Bleu-4 and Rouge-L performances of 40.2% and 59.4%, respectively, on the MSCOCO Karpathy test split. Our codes and pre-trained models will be available as open-source.",1
"Change Captioning is a task that aims to describe the difference between images with natural language. Most existing methods treat this problem as a difference judgment without the existence of distractors, such as viewpoint changes. However, in practice, viewpoint changes happen often and can overwhelm the semantic difference to be described. In this paper, we propose a novel visual encoder to explicitly distinguish viewpoint changes from semantic changes in the change captioning task. Moreover, we further simulate the attention preference of humans and propose a novel reinforcement learning process to fine-tune the attention directly with language evaluation rewards. Extensive experimental results show that our method outperforms the state-of-the-art approaches by a large margin in both Spot-the-Diff and CLEVR-Change datasets.",0
"The task of Change Captioning involves using natural language to describe the differences between images. However, many current methods do not take into account the presence of distractors, such as changes in viewpoint, which can make it challenging to accurately describe semantic differences. To address this issue, we introduce a new visual encoder that is explicitly designed to differentiate between semantic and viewpoint changes. Additionally, we use a reinforcement learning process to fine-tune attention based on language evaluation rewards, mimicking human attention preferences. Our experimental results demonstrate that our approach significantly outperforms existing methods in both the Spot-the-Diff and CLEVR-Change datasets.",1
"This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.",0
"This article covers a wide range of techniques that are available for Monte Carlo gradient estimation in the field of machine learning and statistical sciences. The focus is on computing the gradient of a function's expectation with respect to distribution parameters, which is also known as the sensitivity analysis problem. In machine learning, this gradient problem is crucial for supervised, unsupervised, and reinforcement learning. To facilitate easy and efficient use and analysis, we aim to convert gradients into a form suitable for Monte Carlo estimation. We examine three methods: pathwise, score function, and measure-valued gradient estimators, tracing their historical development, derivation, and underlying assumptions. We also explore their application in other areas, demonstrate their interrelatedness and potential for combination, and discuss their potential for generalization. Monte Carlo gradient estimation has led to significant advancements in the past, and a deeper understanding of this problem is vital for further progress. Our goal is to promote these advancements.",1
"Reinforcement Learning algorithms require a large number of samples to solve complex tasks with sparse and delayed rewards. Complex tasks can often be hierarchically decomposed into sub-tasks. A step in the Q-function can be associated with solving a sub-task, where the expectation of the return increases. RUDDER has been introduced to identify these steps and then redistribute reward to them, thus immediately giving reward if sub-tasks are solved. Since the problem of delayed rewards is mitigated, learning is considerably sped up. However, for complex tasks, current exploration strategies as deployed in RUDDER struggle with discovering episodes with high rewards. Therefore, we assume that episodes with high rewards are given as demonstrations and do not have to be discovered by exploration. Typically the number of demonstrations is small and RUDDER's LSTM model as a deep learning method does not learn well. Hence, we introduce Align-RUDDER, which is RUDDER with two major modifications. First, Align-RUDDER assumes that episodes with high rewards are given as demonstrations, replacing RUDDER's safe exploration and lessons replay buffer. Second, we replace RUDDER's LSTM model by a profile model that is obtained from multiple sequence alignment of demonstrations. Profile models can be constructed from as few as two demonstrations as known from bioinformatics. Align-RUDDER inherits the concept of reward redistribution, which considerably reduces the delay of rewards, thus speeding up learning. Align-RUDDER outperforms competitors on complex artificial tasks with delayed reward and few demonstrations. On the MineCraft ObtainDiamond task, Align-RUDDER is able to mine a diamond, though not frequently. Github: https://github.com/ml-jku/align-rudder, YouTube: https://youtu.be/HO-_8ZUl-UY",0
"To solve intricate tasks with sparse and delayed rewards, Reinforcement Learning algorithms require a significant number of samples. These tasks can typically be broken down into sub-tasks, and solving them increases the expected return. RUDDER identifies these sub-tasks and redistributes rewards to them, thereby mitigating the problem of delayed rewards and speeding up learning. However, RUDDER's exploration strategies may struggle to discover episodes with high rewards, which are assumed to be given as demonstrations in Align-RUDDER. Align-RUDDER replaces RUDDER's safe exploration and lessons replay buffer with demonstrations and uses a profile model obtained from multiple sequence alignment. This modification considerably reduces reward delay and improves performance on complex tasks with few demonstrations. Align-RUDDER outperforms other methods on the MineCraft ObtainDiamond task. For more information, visit the Github and YouTube links.",1
"Model-based reinforcement learning approaches leverage a forward dynamics model to support planning and decision making, which, however, may fail catastrophically if the model is inaccurate. Although there are several existing methods dedicated to combating the model error, the potential of the single forward model is still limited. In this paper, we propose to additionally construct a backward dynamics model to reduce the reliance on accuracy in forward model predictions. We develop a novel method, called Bidirectional Model-based Policy Optimization (BMPO) to utilize both the forward model and backward model to generate short branched rollouts for policy optimization. Furthermore, we theoretically derive a tighter bound of return discrepancy, which shows the superiority of BMPO against the one using merely the forward model. Extensive experiments demonstrate that BMPO outperforms state-of-the-art model-based methods in terms of sample efficiency and asymptotic performance.",0
"Reinforcement learning methods that rely on forward dynamics models for planning and decision making can be highly vulnerable to catastrophic failures if the model is inaccurate. Although various techniques have been developed to address this issue, the potential of a single forward model remains limited. To overcome this limitation, we propose constructing a backward dynamics model to supplement the forward model in reducing reliance on accuracy. Our approach, called Bidirectional Model-based Policy Optimization (BMPO), utilizes both models to generate short branched rollouts for policy optimization. We also derive a tighter bound of return discrepancy, which demonstrates the superiority of BMPO over methods that rely solely on the forward model. Our extensive experiments confirm that BMPO outperforms existing state-of-the-art model-based methods in terms of sample efficiency and asymptotic performance.",1
"Today's intelligent traffic light control system is based on the current road traffic conditions for traffic regulation. However, these approaches cannot exploit the future traffic information in advance. In this paper, we propose GPlight, a deep reinforcement learning (DRL) algorithm integrated with graph neural network (GNN) , to relieve the traffic congestion for multi-intersection intelligent traffic control system. In GPlight, the graph neural network (GNN) is first used to predict the future short-term traffic flow at the intersections. Then, the results of traffic flow prediction are used in traffic light control, and the agent combines the predicted results with the observed current traffic conditions to dynamically control the phase and duration of the traffic lights at the intersection. Experiments on both synthetic and two real-world data-sets of Hangzhou and New-York verify the effectiveness and rationality of the GPlight algorithm.",0
"The current intelligent traffic light control system is designed to regulate traffic based on present road traffic conditions. However, it cannot make use of future traffic information. This paper introduces GPlight, an algorithm that combines deep reinforcement learning (DRL) with graph neural network (GNN) to alleviate traffic congestion in multi-intersection intelligent traffic control systems. GPlight utilizes GNN to predict short-term traffic flow at intersections. These predictions are then incorporated into traffic light control, where the agent dynamically controls the phase and duration of traffic lights based on current traffic conditions and predicted results. Experiments conducted on synthetic and real-world data-sets from Hangzhou and New York validate the effectiveness and rationality of the GPlight algorithm.",1
"Existing ineffective and inflexible traffic light control at urban intersections can often lead to congestion in traffic flows and cause numerous problems, such as long delay and waste of energy. How to find the optimal signal timing strategy is a significant challenge in urban traffic management. In this paper, we propose PDlight, a deep reinforcement learning (DRL) traffic light control algorithm with a novel reward as PRCOL (Pressure with Remaining Capacity of Outgoing Lane). Serving as an improvement over the pressure used in traffic control algorithms, PRCOL considers not only the number of vehicles on the incoming lane but also the remaining capacity of the outgoing lane. Simulation results using both synthetic and real-world data-sets show that the proposed PDlight yields lower average travel time compared with several state-of-the-art algorithms, PressLight and Colight, under both fixed and dynamic green light duration.",0
"The current traffic light control system in urban intersections is ineffective and inflexible, often resulting in traffic congestion, long delays, and energy wastage. Finding the best signal timing strategy is a significant challenge for urban traffic management. This paper proposes a traffic light control algorithm called PDlight, which uses deep reinforcement learning (DRL) and a unique reward system called PRCOL (Pressure with Remaining Capacity of Outgoing Lane). Unlike other traffic control algorithms, PRCOL considers both the number of incoming vehicles and the remaining capacity of the outgoing lane. The simulation results, based on synthetic and real-world data, demonstrate that PDlight outperforms other algorithms, such as PressLight and Colight, in terms of lower average travel time, regardless of the fixed or dynamic green light duration.",1
Hindsight Credit Assignment (HCA) refers to a recently proposed family of methods for producing more efficient credit assignment in reinforcement learning. These methods work by explicitly estimating the probability that certain actions were taken in the past given present information. Prior work has studied the properties of such methods and demonstrated their behaviour empirically. We extend this work by introducing a particular HCA algorithm which has provably lower variance than the conventional Monte-Carlo estimator when the necessary functions can be estimated exactly. This result provides a strong theoretical basis for how HCA could be broadly useful.,0
A new set of techniques called Hindsight Credit Assignment (HCA) has been introduced to enhance credit assignment in reinforcement learning. These techniques involve estimating the likelihood of previous actions based on current information to improve efficiency. Previous studies have investigated the characteristics of these methods and confirmed their effectiveness through practical tests. Our contribution is the introduction of a specific HCA algorithm that produces lower variance than the conventional Monte-Carlo estimator when exact function estimation is possible. This finding establishes a solid theoretical foundation for the broad applicability of HCA.,1
"Accurate detection of anatomical landmarks is an essential step in several medical imaging tasks. We propose a novel communicative multi-agent reinforcement learning (C-MARL) system to automatically detect landmarks in 3D brain images. C-MARL enables the agents to learn explicit communication channels, as well as implicit communication signals by sharing certain weights of the architecture among all the agents. The proposed approach is evaluated on two brain imaging datasets from adult magnetic resonance imaging (MRI) and fetal ultrasound scans. Our experiments show that involving multiple cooperating agents by learning their communication with each other outperforms previous approaches using single agents.",0
The precise identification of anatomical landmarks is crucial in various medical imaging procedures. Our research introduces a new and innovative system called communicative multi-agent reinforcement learning (C-MARL) that can detect landmarks in 3D brain images automatically. This system allows agents to learn communication channels explicitly and implicit signals by sharing specific architecture weights. We tested this method on two datasets of brain images from adult MRI and fetal ultrasound scans. Our experiments demonstrate that using multiple agents that collaborate by learning to communicate with each other is more effective than previous methods that use only single agents.,1
Finding a path free from obstacles that poses minimal risk is critical for safe navigation. People who are sighted and people who are visually impaired require navigation safety while walking on a sidewalk. In this research we developed an assistive navigation on a sidewalk by integrating sensory inputs using reinforcement learning. We trained a Sidewalk Obstacle Avoidance Agent (SOAA) through reinforcement learning in a simulated robotic environment. A Sidewalk Obstacle Conversational Agent (SOCA) is built by training a natural language conversation agent with real conversation data. The SOAA along with SOCA was integrated in a prototype device called augmented guide (AG). Empirical analysis showed that this prototype improved the obstacle avoidance experience about 5% from a base case of 81.29%,0
"Safe navigation is crucial for individuals who are sighted or visually impaired, and it is important to find a path that is obstacle-free and poses minimal risks. To address this concern, we conducted research that aimed to create an assistive navigation system for sidewalks. Our approach involved using reinforcement learning to integrate sensory inputs and train a Sidewalk Obstacle Avoidance Agent (SOAA) in a simulated robotic environment. We also developed a natural language conversation agent, called the Sidewalk Obstacle Conversational Agent (SOCA), by training it with real conversation data. These two agents were then integrated into a prototype device called the augmented guide (AG). Based on empirical analysis, we found that the AG improved the obstacle avoidance experience by approximately 5% from a base case of 81.29%.",1
"The models of n-ary cross sentence relation extraction based on distant supervision assume that consecutive sentences mentioning n entities describe the relation of these n entities. However, on one hand, this assumption introduces noisy labeled data and harms the models' performance. On the other hand, some non-consecutive sentences also describe one relation and these sentences cannot be labeled under this assumption. In this paper, we relax this strong assumption by a weaker distant supervision assumption to address the second issue and propose a novel sentence distribution estimator model to address the first problem. This estimator selects correctly labeled sentences to alleviate the effect of noisy data is a two-level agent reinforcement learning model. In addition, a novel universal relation extractor with a hybrid approach of attention mechanism and PCNN is proposed such that it can be deployed in any tasks, including consecutive and nonconsecutive sentences. Experiments demonstrate that the proposed model can reduce the impact of noisy data and achieve better performance on general n-ary cross sentence relation extraction task compared to baseline models.",0
"Distant supervision-based models for n-ary cross sentence relation extraction assume that consecutive sentences mentioning n entities describe the relationship between these entities. However, this assumption leads to noisy labeled data and affects the models' performance. Additionally, some non-consecutive sentences may describe a relation, but cannot be labeled under this assumption. To address these issues, we propose a weaker distant supervision assumption and a novel sentence distribution estimator model. This estimator selects correctly labeled sentences to mitigate the impact of noisy data through a two-level agent reinforcement learning model. Furthermore, we introduce a hybrid approach of attention mechanism and PCNN in our universal relation extractor, which can be used in any task involving consecutive and nonconsecutive sentences. Our experiments show that our proposed model performs better than baseline models in general n-ary cross sentence relation extraction tasks by reducing the impact of noisy data.",1
"Faults are endemic to all systems. Adaptive fault-tolerant control maintains degraded performance when faults occur as opposed to unsafe conditions or catastrophic events. In systems with abrupt faults and strict time constraints, it is imperative for control to adapt quickly to system changes to maintain system operations. We present a meta-reinforcement learning approach that quickly adapts its control policy to changing conditions. The approach builds upon model-agnostic meta learning (MAML). The controller maintains a complement of prior policies learned under system faults. This ""library"" is evaluated on a system after a new fault to initialize the new policy. This contrasts with MAML, where the controller derives intermediate policies anew, sampled from a distribution of similar systems, to initialize a new policy. Our approach improves sample efficiency of the reinforcement learning process. We evaluate our approach on an aircraft fuel transfer system under abrupt faults.",0
"All systems are prone to faults, but adaptive fault-tolerant control is able to maintain performance even when faults occur, preventing unsafe conditions or catastrophic events. When dealing with systems that have sudden faults and strict time constraints, it is crucial for control to quickly adapt to these changes in order to keep the system running smoothly. Our solution is a meta-reinforcement learning approach that rapidly adjusts its control policy to new conditions, using model-agnostic meta learning (MAML) as a foundation. The controller maintains a library of prior policies learned from previous system faults, which is evaluated to initialize new policies after a fault occurs. This differs from MAML, where intermediate policies are derived from a distribution of similar systems. Our approach enhances the efficiency of the reinforcement learning process and has been tested on an aircraft fuel transfer system during abrupt faults.",1
"Many reinforcement learning tasks can benefit from explicit planning based on an internal model of the environment. Previously, such planning components have been incorporated through a neural network that partially aligns with the computational graph of value iteration. Such network have so far been focused on restrictive environments (e.g. grid-worlds), and modelled the planning procedure only indirectly. We relax these constraints, proposing a graph neural network (GNN) that executes the value iteration (VI) algorithm, across arbitrary environment models, with direct supervision on the intermediate steps of VI. The results indicate that GNNs are able to model value iteration accurately, recovering favourable metrics and policies across a variety of out-of-distribution tests. This suggests that GNN executors with strong supervision are a viable component within deep reinforcement learning systems.",0
"An internal model of the environment can aid in explicit planning for many reinforcement learning tasks. Previously, neural networks were used to incorporate planning components that partially aligned with the computational graph of value iteration. However, these networks were mainly utilized in restrictive environments and indirectly modeled the planning procedure. We propose a graph neural network (GNN) that executes the value iteration (VI) algorithm with direct supervision on the intermediate steps of VI, irrespective of the environment models. The results demonstrate that GNNs can accurately model value iteration and generate favorable metrics and policies during out-of-distribution tests. This suggests that GNN executors with strong supervision are a feasible component for deep reinforcement learning systems.",1
"OpenSpiel is a collection of environments and algorithms for research in general reinforcement learning and search/planning in games. OpenSpiel supports n-player (single- and multi- agent) zero-sum, cooperative and general-sum, one-shot and sequential, strictly turn-taking and simultaneous-move, perfect and imperfect information games, as well as traditional multiagent environments such as (partially- and fully- observable) grid worlds and social dilemmas. OpenSpiel also includes tools to analyze learning dynamics and other common evaluation metrics. This document serves both as an overview of the code base and an introduction to the terminology, core concepts, and algorithms across the fields of reinforcement learning, computational game theory, and search.",0
"OpenSpiel is a versatile platform that offers a range of environments and algorithms for research in general reinforcement learning and search/planning in games. With OpenSpiel, users can explore a variety of game types, including n-player (single- and multi-agent) zero-sum, cooperative and general-sum, one-shot and sequential, strictly turn-taking and simultaneous-move, perfect and imperfect information games, as well as traditional multiagent environments like (partially- and fully- observable) grid worlds and social dilemmas. Additionally, OpenSpiel provides tools to analyze learning dynamics and other common evaluation metrics. This document serves as both an introduction to the terminology, core concepts, and algorithms across the fields of reinforcement learning, computational game theory, and search, as well as an overview of the code base.",1
"Efficient exploration is a long-standing problem in reinforcement learning. In this work, we introduce a self-supervised exploration policy by incentivizing the agent to maximize multisensory incongruity, which can be measured in two aspects: perception incongruity and action incongruity. The former represents the uncertainty in multisensory fusion model, while the latter represents the uncertainty in an agent's policy. Specifically, an alignment predictor is trained to detect whether multiple sensory inputs are aligned, the error of which is used to measure perception incongruity. The policy takes the multisensory observations with sensory-wise dropout as input and outputs actions for exploration. The variance of actions is further used to measure action incongruity. Our formulation allows the agent to learn skills by exploring in a self-supervised manner without any external rewards. Besides, our method enables the agent to learn a compact multimodal representation from hard examples, which further improves the sample efficiency of our policy learning. We demonstrate the efficacy of this formulation across a variety of benchmark environments including object manipulation and audio-visual games.",0
"The problem of efficient exploration in reinforcement learning has been a challenge for a long time. In this study, we propose a self-supervised exploration policy that encourages the agent to increase multisensory incongruity. This can be measured in two ways: perception incongruity, which represents the uncertainty in the multisensory fusion model, and action incongruity, which represents the uncertainty in the agent's policy. To achieve this, we train an alignment predictor to detect whether multiple sensory inputs are aligned, and use the error to measure perception incongruity. The policy uses multisensory observations with sensory-wise dropout as input and outputs actions for exploration, with the variance of actions used to measure action incongruity. Our approach enables the agent to learn skills through self-supervised exploration without external rewards, and also allows it to learn a concise multimodal representation from difficult examples, improving the efficiency of our policy learning. Our approach is demonstrated to be effective in a range of benchmark environments, including object manipulation and audio-visual games.",1
"Design of cyber-physical systems (CPSs) is a challenging task that involves searching over a large search space of various CPS configurations and possible values of components composing the system. Hence, there is a need for sample-efficient CPS design space exploration to select the system architecture and component values that meet the target system requirements. We address this challenge by formulating CPS design as a multi-objective optimization problem and propose DISPATCH, a two-step methodology for sample-efficient search over the design space. First, we use a genetic algorithm to search over discrete choices of system component values for architecture search and component selection or only component selection and terminate the algorithm even before meeting the system requirements, thus yielding a coarse design. In the second step, we use an inverse design to search over a continuous space to fine-tune the component values and meet the diverse set of system requirements. We use a neural network as a surrogate function for the inverse design of the system. The neural network, converted into a mixed-integer linear program, is used for active learning to sample component values efficiently in a continuous search space. We illustrate the efficacy of DISPATCH on electrical circuit benchmarks: two-stage and three-stage transimpedence amplifiers. Simulation results show that the proposed methodology improves sample efficiency by 5-14x compared to a prior synthesis method that relies on reinforcement learning. It also synthesizes circuits with the best performance (highest bandwidth/lowest area) compared to designs synthesized using reinforcement learning, Bayesian optimization, or humans.",0
"Developing cyber-physical systems (CPSs) is a complex undertaking that involves exploring a vast search space of various CPS configurations and potential component values that make up the system. Therefore, there is a demand for efficient CPS design space exploration to determine the appropriate system architecture and component values that fulfill the desired system requirements. To address this challenge, we present a two-step method called DISPATCH, which formulates CPS design as a multi-objective optimization problem. In the first step, we employ a genetic algorithm to explore discrete choices of system component values for architecture search and component selection, yielding a coarse design. In the second step, we use an inverse design to fine-tune the component values in a continuous search space to meet the diverse set of system requirements. A neural network serves as a surrogate function for the inverse design of the system, which is converted into a mixed-integer linear program for active learning to efficiently sample component values. We demonstrate the effectiveness of DISPATCH on electrical circuit benchmarks, with simulation results showing that it improves sample efficiency by 5-14x compared to a prior synthesis method relying on reinforcement learning. It also produces circuits with superior performance (highest bandwidth/lowest area) compared to designs synthesized using reinforcement learning, Bayesian optimization, or human expertise.",1
"We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization. Our method first trains a base model using Q-learning, which typically overfits the training games. The base model's action token distribution is used to perform observation pruning that removes irrelevant tokens. A second bootstrapped model is then retrained on the pruned observation text. Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art methods despite requiring less number of training episodes.",0
"Our study reveals that Reinforcement Learning (RL) techniques utilized to solve Text-Based Games (TBGs) often experience difficulty in generalizing on unobserved games, particularly when the data is limited. To tackle this issue, we propose Context Relevant Episodic State Truncation (CREST) to eliminate irrelevant tokens in observation text, thereby enabling enhanced generalization. Our approach initially trains a Q-learning-based base model, which usually overfits the training games. We then use the action token distribution of the base model to prune observation text of irrelevant tokens. A second bootstrapped model is subsequently trained on the pruned observation text. Our bootstrapped agent exhibits superior generalization in solving new TextWorld games, necessitating only a fraction of the training games used in previous state-of-the-art methods, despite requiring a lower number of training episodes.",1
"Deep reinforcement learning has been successful in a variety of tasks, such as game playing and robotic manipulation. However, attempting to learn \textit{tabula rasa} disregards the logical structure of many domains as well as the wealth of readily available knowledge from domain experts that could help ""warm start"" the learning process. We present a novel reinforcement learning technique that allows for intelligent initialization of a neural network weights and architecture. Our approach permits the encoding domain knowledge directly into a neural decision tree, and improves upon that knowledge with policy gradient updates. We empirically validate our approach on two OpenAI Gym tasks and two modified StarCraft 2 tasks, showing that our novel architecture outperforms multilayer-perceptron and recurrent architectures. Our knowledge-based framework finds superior policies compared to imitation learning-based and prior knowledge-based approaches. Importantly, we demonstrate that our approach can be used by untrained humans to initially provide >80% increase in expected reward relative to baselines prior to training (p < 0.001), which results in a >60% increase in expected reward after policy optimization (p = 0.011).",0
"Despite its success in tasks like game playing and robotic manipulation, deep reinforcement learning's attempt at learning from scratch ignores the logical structure of many domains and the vast knowledge available from domain experts that could enhance the learning process. To address this, we introduce a new reinforcement learning technique that enables intelligent initialization of a neural network's architecture and weights. Our approach incorporates domain knowledge directly into a neural decision tree and enhances it with policy gradient updates. We validate our approach on two OpenAI Gym tasks and two modified StarCraft 2 tasks, where it outperforms multilayer-perceptron and recurrent architectures. Our knowledge-based framework yields higher quality policies than imitation learning-based and prior knowledge-based methods. Importantly, untrained humans can use our approach to achieve an initial >80% increase in expected reward relative to baselines before training (p < 0.001), which results in a >60% increase in expected reward after policy optimization (p = 0.011).",1
"Surprise-based learning allows agents to rapidly adapt to non-stationary stochastic environments characterized by sudden changes. We show that exact Bayesian inference in a hierarchical model gives rise to a surprise-modulated trade-off between forgetting old observations and integrating them with the new ones. The modulation depends on a probability ratio, which we call ""Bayes Factor Surprise"", that tests the prior belief against the current belief. We demonstrate that in several existing approximate algorithms the Bayes Factor Surprise modulates the rate of adaptation to new observations. We derive three novel surprised-based algorithms, one in the family of particle filters, one in the family of variational learning, and the other in the family of message passing, that have constant scaling in observation sequence length and particularly simple update dynamics for any distribution in the exponential family. Empirical results show that these surprise-based algorithms estimate parameters better than alternative approximate approaches and reach levels of performance comparable to computationally more expensive algorithms. The Bayes Factor Surprise is related to but different from Shannon Surprise. In two hypothetical experiments, we make testable predictions for physiological indicators that dissociate the Bayes Factor Surprise from Shannon Surprise. The theoretical insight of casting various approaches as surprise-based learning, as well as the proposed online algorithms, may be applied to the analysis of animal and human behavior, and to reinforcement learning in non-stationary environments.",0
"Agents can adapt to unpredictable environments using surprise-based learning, which involves a trade-off between forgetting previous observations and incorporating new ones. This trade-off depends on a probability ratio called the ""Bayes Factor Surprise"", which compares prior beliefs to current beliefs. We found that this ratio affects the rate of adaptation in existing algorithms and developed three new algorithms that are simple and work well for any distribution in the exponential family. Our experiments showed that surprise-based algorithms perform better than other methods and can be used to study animal and human behavior as well as reinforcement learning in non-stationary environments. Additionally, we discovered that the Bayes Factor Surprise is distinct from Shannon Surprise and can be distinguished using physiological indicators in hypothetical experiments.",1
"Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.",0
"The utilization of machine learning is feasible in comprehending healthcare data. To achieve an all-encompassing understanding of healthcare data, probabilistic machine learning models are employed. This analysis delves into the potential of probabilistic machine learning in advancing healthcare. The challenges in constructing predictive models are scrutinized, and the advantages of using probabilistic models in addressing calibration and missing data are investigated. In addition to predictive models, the efficacy of probabilistic machine learning models is examined in phenotyping, generative models for clinical use cases, and reinforcement learning.",1
"This paper demonstrates the application of reinforcement learning (RL) to process synthesis by presenting Distillation Gym, a set of RL environments in which an RL agent is tasked with designing a distillation train, given a user defined multi-component feed stream. Distillation Gym interfaces with a process simulator (COCO and ChemSep) to simulate the environment. A demonstration of two distillation problem examples are discussed in this paper (a Benzene, Toluene, P-xylene separation problem and a hydrocarbon separation problem), in which a deep RL agent is successfully able to learn within Distillation Gym to produce reasonable designs. Finally, this paper proposes the creation of Chemical Engineering Gym, an all-purpose reinforcement learning software toolkit for chemical engineering process synthesis.",0
"This paper showcases how reinforcement learning (RL) can be utilized in process synthesis through the introduction of Distillation Gym. This comprises a series of RL environments that assign an RL agent with the responsibility of creating a distillation train based on a user-defined multi-component feed stream. Distillation Gym interacts with a process simulator (COCO and ChemSep) to imitate the environment. The paper illustrates two distillation problems (a Benzene, Toluene, P-xylene separation problem and a hydrocarbon separation problem) in which a deep RL agent effectively learned to create reasonable designs within Distillation Gym. Finally, the paper suggests the development of Chemical Engineering Gym, a comprehensive RL software toolkit for chemical engineering process synthesis.",1
"Analyzing the handwriting generation process is an important issue and has been tackled by various generation models, such as kinematics based models and stochastic models. In this study, we use a reinforcement learning (RL) framework to realize handwriting generation with the careful future planning ability. In fact, the handwriting process of human beings is also supported by their future planning ability; for example, the ability is necessary to generate a closed trajectory like '0' because any shortsighted model, such as a Markovian model, cannot generate it. For the algorithm, we employ generative adversarial imitation learning (GAIL). Typical RL algorithms require the manual definition of the reward function, which is very crucial to control the generation process. In contrast, GAIL trains the reward function along with the other modules of the framework. In other words, through GAIL, we can understand the reward of the handwriting generation process from handwriting examples. Our experimental results qualitatively and quantitatively show that the learned reward catches the trends in handwriting generation and thus GAIL is well suited for the acquisition of handwriting behavior.",0
"Various generation models, including kinematics based and stochastic models, have tackled the important issue of analyzing the handwriting generation process. This study utilizes a reinforcement learning (RL) framework to achieve handwriting generation with careful future planning ability. Human handwriting is supported by future planning ability, as evidenced by the need for this ability to generate a closed trajectory like '0,' which cannot be generated by shortsighted models like Markovian models. The algorithm used in this study is generative adversarial imitation learning (GAIL), which trains the reward function with the other modules of the framework. Unlike typical RL algorithms that require manual definition of the reward function, GAIL understands the reward of the handwriting generation process from handwriting examples. The experimental results demonstrate that the learned reward captures the trends in handwriting generation, indicating that GAIL is suitable for acquiring handwriting behavior.",1
"Contextual bandits provide an effective way to model the dynamic data problem in ML by leveraging online (incremental) learning to continuously adjust the predictions based on changing environment. We explore details on contextual bandits, an extension to the traditional reinforcement learning (RL) problem and build a novel algorithm to solve this problem using an array of action-based learners. We apply this approach to model an article recommendation system using an array of stochastic gradient descent (SGD) learners to make predictions on rewards based on actions taken. We then extend the approach to a publicly available MovieLens dataset and explore the findings. First, we make available a simplified simulated dataset showing varying user preferences over time and how this can be evaluated with static and dynamic learning algorithms. This dataset made available as part of this research is intentionally simulated with limited number of features and can be used to evaluate different problem-solving strategies. We will build a classifier using static dataset and evaluate its performance on this dataset. We show limitations of static learner due to fixed context at a point of time and how changing that context brings down the accuracy. Next we develop a novel algorithm for solving the contextual bandit problem. Similar to the linear bandits, this algorithm maps the reward as a function of context vector but uses an array of learners to capture variation between actions/arms. We develop a bandit algorithm using an array of stochastic gradient descent (SGD) learners, with separate learner per arm. Finally, we will apply this contextual bandit algorithm to predicting movie ratings over time by different users from the standard Movie Lens dataset and demonstrate the results.",0
"Contextual bandits offer a viable solution to the dynamic data problem in machine learning by using online (incremental) learning to adjust predictions based on changes in the environment. In this study, we examine the intricacies of contextual bandits, an expansion of the traditional reinforcement learning (RL) problem. We devise a unique algorithm that utilizes an assortment of action-based learners to resolve the issue. Our approach involves utilizing a range of stochastic gradient descent (SGD) learners to predict rewards based on actions taken in an article recommendation system. We then extend this technique to a public MovieLens dataset and report our findings. Initially, we present a simplified simulated dataset that exhibits varying user preferences over time. The dataset is designed to evaluate different problem-solving strategies and offers a static and dynamic learning algorithm. We demonstrate the limitations of the static learner when the context changes, resulting in a decrease in accuracy. Our algorithm for solving the contextual bandit problem, similar to the linear bandits, maps the reward as a context vector function, employing an array of learners to capture the variation between actions/arms. We utilize an array of SGD learners, one for each arm, to develop a bandit algorithm. Finally, we apply our contextual bandit algorithm to forecast movie ratings over time by different users from the standard MovieLens dataset and report the outcomes.",1
"Proximal Policy Optimization (PPO) is a popular deep policy gradient algorithm. In standard implementations, PPO regularizes policy updates with clipped probability ratios, and parameterizes policies with either continuous Gaussian distributions or discrete Softmax distributions. These design choices are widely accepted, and motivated by empirical performance comparisons on MuJoCo and Atari benchmarks.   We revisit these practices outside the regime of current benchmarks, and expose three failure modes of standard PPO. We explain why standard design choices are problematic in these cases, and show that alternative choices of surrogate objectives and policy parameterizations can prevent the failure modes. We hope that our work serves as a reminder that many algorithmic design choices in reinforcement learning are tied to specific simulation environments. We should not implicitly accept these choices as a standard part of a more general algorithm.",0
"The Proximal Policy Optimization (PPO) algorithm is a deep policy gradient technique that is widely used. In typical implementations, PPO employs clipped probability ratios to regulate policy updates, and either continuous Gaussian distributions or discrete Softmax distributions to parameterize policies. These design decisions are generally accepted and have been demonstrated to lead to improved performance on MuJoCo and Atari benchmarks. However, we have discovered three instances where standard PPO fails outside of these benchmarks. We have identified the issues with the typical design choices in these situations and have demonstrated that alternative surrogate objectives and policy parameterizations can prevent these failures. Our findings emphasize the importance of recognizing that many reinforcement learning algorithms are associated with specific simulation environments, and that we should not accept these design choices as universal components of the algorithm.",1
"In this paper, we study the problem of obtaining a control policy that can mimic and then outperform expert demonstrations in Markov decision processes where the reward function is unknown to the learning agent. One main relevant approach is the inverse reinforcement learning (IRL), which mainly focuses on inferring a reward function from expert demonstrations. The obtained control policy by IRL and the associated algorithms, however, can hardly outperform expert demonstrations. To overcome this limitation, we propose a novel method that enables the learning agent to outperform the demonstrator via a new concurrent reward and action policy learning approach. In particular, we first propose a new stereo utility definition that aims to address the bias in the interpretation of expert demonstrations. We then propose a loss function for the learning agent to learn reward and action policies concurrently such that the learning agent can outperform expert demonstrations. The performance of the proposed method is first demonstrated in OpenAI environments. Further efforts are conducted to experimentally validate the proposed method via an indoor drone flight scenario.",0
"The aim of this paper is to investigate the issue of obtaining a control policy that can surpass expert demonstrations in Markov decision processes where the learning agent has no knowledge of the reward function. Prior research has mainly focused on inverse reinforcement learning (IRL), which involves deducing a reward function from expert demonstrations. However, despite its effectiveness in obtaining a control policy, the IRL method falls short in outperforming expert demonstrations. To address this limitation, we propose a new method that enables the learning agent to surpass the expert via a concurrent reward and action policy learning approach. We introduce a new stereo utility definition to address the bias in the interpretation of expert demonstrations and propose a loss function for the learning agent to learn reward and action policies concurrently. We demonstrate the effectiveness of our proposed method in OpenAI environments and further validate it through an indoor drone flight scenario.",1
"Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across robotics benchmarks that the use of an adversarial population results in a more robust policy that also improves out-of-distribution generalization. Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.",0
"Although Reinforcement Learning (RL) is a useful tool for designing controllers, it can encounter robustness issues and fail catastrophically when underlying system dynamics are disrupted. To address this, the Robust RL formulation introduces worst-case adversarial noise to the dynamics and creates the noise distribution as the solution to a zero-sum minimax game. However, current studies on learning solutions to Robust RL have focused mainly on training a single RL agent against a single adversary. Our research shows that using a single adversary does not consistently provide robustness to dynamics variations under standard parametrizations of the adversary, leading to a highly exploitable policy by new adversaries. To address this, we suggest a population-based augmentation to the Robust RL formulation, where we randomly initialize a group of adversaries and uniformly sample from the population during training. We validate our approach across robotics benchmarks, demonstrating that the use of an adversarial population results in a more robust policy that enhances out-of-distribution generalization. Finally, we show that this method provides comparable robustness and generalization to domain randomization on these benchmarks while avoiding a common domain randomization failure mode.",1
"A standard metric used to measure the approximate optimality of policies in imperfect information games is exploitability, i.e. the performance of a policy against its worst-case opponent. However, exploitability is intractable to compute in large games as it requires a full traversal of the game tree to calculate a best response to the given policy. We introduce a new metric, approximate exploitability, that calculates an analogous metric using an approximate best response; the approximation is done by using search and reinforcement learning. This is a generalization of local best response, a domain specific evaluation metric used in poker. We provide empirical results for a specific instance of the method, demonstrating that our method converges to exploitability in the tabular and function approximation settings for small games. In large games, our method learns to exploit both strong and weak agents, learning to exploit an AlphaZero agent.",0
"The measure commonly used to determine how effective policies are in imperfect information games is exploitability. This refers to the performance of a policy when faced with its worst-case opponent. However, calculating exploitability is difficult in large games because it requires a complete traversal of the game tree to find the best response to a given policy. To address this challenge, we propose a new metric called approximate exploitability. This metric calculates a similar value using an approximate best response, which we obtain through a combination of search and reinforcement learning. Our approach is a more general version of the domain-specific evaluation metric used in poker called local best response. We present empirical evidence that demonstrates the effectiveness of our method. Specifically, our approach converges to exploitability in small games, both in the tabular and function approximation settings. Additionally, we show that our method can learn to exploit both strong and weak agents, including an AlphaZero agent.",1
"Reinforcement learning is a promising model-free and adaptive controller for demand side management, as part of the future smart grid, at the district level. This paper presents the results of the algorithm that was submitted for the CityLearn Challenge, which was hosted in early 2020 with the aim of designing and tuning a reinforcement learning agent to flatten and smooth the aggregated curve of electrical demand of a district of diverse buildings. The proposed solution secured second place in the challenge using a centralised 'Soft Actor Critic' deep reinforcement learning agent that was able to handle continuous action spaces. The controller was able to achieve an averaged score of 0.967 on the challenge dataset comprising of different buildings and climates. This highlights the potential application of deep reinforcement learning as a plug-and-play style controller, that is capable of handling different climates and a heterogenous building stock, for district demand side management of buildings.",0
"The use of reinforcement learning as a controller for demand side management at the district level in the smart grid is a promising approach. In this paper, the algorithm submitted for the CityLearn Challenge in early 2020 is discussed. The aim of the challenge was to design and fine-tune a reinforcement learning agent to smooth the electrical demand curve of a district with a variety of buildings. The proposed solution, a centralised 'Soft Actor Critic' deep reinforcement learning agent, achieved second place in the challenge with the ability to handle continuous action spaces. The controller attained an average score of 0.967 on the challenge dataset, demonstrating the potential of deep reinforcement learning as a plug-and-play style controller for district demand side management of buildings in diverse climates and with a heterogenous building stock.",1
This work extends the analysis of the theoretical results presented within the paper Is Q-Learning Provably Efficient? by Jin et al. We include a survey of related research to contextualize the need for strengthening the theoretical guarantees related to perhaps the most important threads of model-free reinforcement learning. We also expound upon the reasoning used in the proofs to highlight the critical steps leading to the main result showing that Q-learning with UCB exploration achieves a sample efficiency that matches the optimal regret that can be achieved by any model-based approach.,0
"The analysis of the theoretical findings in the paper ""Is Q-Learning Provably Efficient?"" by Jin et al. is expanded upon in this work. A survey of related research is included to provide a context for the necessity of reinforcing the theoretical assurances related to the most vital aspects of model-free reinforcement learning. Additionally, the reasoning utilized in the proofs is elaborated upon to emphasize the crucial stages that lead to the primary outcome, demonstrating that Q-learning with UCB exploration attains a sample efficiency that is on par with the optimal regret achievable through any model-based approach.",1
"Most reinforcement learning methods are based upon the key assumption that the transition dynamics and reward functions are fixed, that is, the underlying Markov decision process is stationary. However, in many real-world applications, this assumption is violated, and using existing algorithms may result in a performance lag. To proactively search for a good future policy, we present a policy gradient algorithm that maximizes a forecast of future performance. This forecast is obtained by fitting a curve to the counter-factual estimates of policy performance over time, without explicitly modeling the underlying non-stationarity. The resulting algorithm amounts to a non-uniform reweighting of past data, and we observe that minimizing performance over some of the data from past episodes can be beneficial when searching for a policy that maximizes future performance. We show that our algorithm, called Prognosticator, is more robust to non-stationarity than two online adaptation techniques, on three simulated problems motivated by real-world applications.",0
"The majority of reinforcement learning methods assume that the transition dynamics and reward functions remain constant, meaning the Markov decision process remains stationary. However, this assumption is not always true in real-world scenarios, and using existing algorithms may lead to poor performance. To address this issue, we introduce a policy gradient algorithm that seeks to maximize future performance by utilizing a forecast obtained from fitting a curve to counter-factual estimates of policy performance over time. This algorithm involves non-uniform reweighting of past data and shows improved performance compared to two online adaptation techniques on three simulated problems inspired by real-world applications. We call this algorithm Prognosticator. Additionally, minimizing performance over some of the data from past episodes can be advantageous when searching for a policy that maximizes future performance.",1
"Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain.",0
"Sequential decision making tasks involve an agent interacting with an environment and receiving rewards as feedback. Reinforcement Learning algorithms can learn complex behavioral patterns for these tasks. Traditionally, such algorithms make decisions at each time step. However, we propose a new framework called Fine Grained Action Repetition (FiGAR), which allows the agent to decide both the action and the time scale of repeating it. FiGAR can be used with Deep Reinforcement Learning algorithms that maintain an explicit policy estimate and enable temporal abstractions in the action space. We conducted experiments and found that our framework improved performance on three different policy search algorithms in various domains, namely Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in the Mujoco domain, and Deep Deterministic Policy Gradients in the TORCS car racing domain.",1
"Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain. One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate. It decides the granularity at which agents can control game play. A frame skip value of $k$ allows the agent to repeat a selected action $k$ number of times. The current state of the art architectures like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state. In this paper, we propose a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable parameter. This allows us to choose the number of times an action is to be repeated based on the current state. We show empirically that such a setting improves the performance on relatively harder games like Seaquest.",0
"State-of-the-art performance has been achieved by Deep Reinforcement Learning techniques in developing control policies for games within the Atari 2600 domain. The Arcade Learning Environment (ALE) has an important parameter, the frame skip rate, which determines the level of granularity at which agents can control gameplay. With a frame skip value of $k$, the agent can repeat a selected action $k$ times. The current leading architectures, such as Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN), utilize a static frame skip rate, where the action output from the network is repeated for a fixed number of frames, irrespective of the current state. In this research, we introduce a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN), which transforms the frame skip rate into a dynamic and learnable parameter. This enables us to choose the number of times an action is repeated based on the current state. We demonstrate through empirical evidence that this approach enhances performance on more challenging games such as Seaquest.",1
This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.,0
"In this paper, a framework for automated skill acquisition in reinforcement learning is introduced. This involves identifying abstract states and extended actions between them to simplify and speed up the learning process. This approach also allows for generalization over multiple tasks without starting from scratch. Metastable regions in the state space are identified using ideas from dynamical systems, and the PCCA+ algorithm is used to find suitable abstractions. Skills are defined as sequences of actions that lead to transitions between abstract states, which are generated using the connectivity information from PCCA+. These skills can be reused across different tasks without the need for an exact model of the environment. The framework is scalable to complex tasks with large state spaces by performing state aggregation using a representation learned from an action conditional video prediction network.",1
"We propose a novel hybrid stochastic policy gradient estimator by combining an unbiased policy gradient estimator, the REINFORCE estimator, with another biased one, an adapted SARAH estimator for policy optimization. The hybrid policy gradient estimator is shown to be biased, but has variance reduced property. Using this estimator, we develop a new Proximal Hybrid Stochastic Policy Gradient Algorithm (ProxHSPGA) to solve a composite policy optimization problem that allows us to handle constraints or regularizers on the policy parameters. We first propose a single-looped algorithm then introduce a more practical restarting variant. We prove that both algorithms can achieve the best-known trajectory complexity $\mathcal{O}\left(\varepsilon^{-3}\right)$ to attain a first-order stationary point for the composite problem which is better than existing REINFORCE/GPOMDP $\mathcal{O}\left(\varepsilon^{-4}\right)$ and SVRPG $\mathcal{O}\left(\varepsilon^{-10/3}\right)$ in the non-composite setting. We evaluate the performance of our algorithm on several well-known examples in reinforcement learning. Numerical results show that our algorithm outperforms two existing methods on these examples. Moreover, the composite settings indeed have some advantages compared to the non-composite ones on certain problems.",0
"Our proposal is to create a new type of stochastic policy gradient estimator by merging the REINFORCE estimator, an unbiased policy gradient estimator, with the adapted SARAH estimator, a biased one that optimizes policy. Although the hybrid policy gradient estimator is biased, it can reduce variance. With this estimator, we have developed a Proximal Hybrid Stochastic Policy Gradient Algorithm (ProxHSPGA) that can solve a composite policy optimization problem, which allows constraints or regularizers on the policy parameters. We have introduced two algorithms, a single-looped one and a more practical restarting variant, that can both achieve the best-known trajectory complexity of $\mathcal{O}\left(\varepsilon^{-3}\right)$ to attain a first-order stationary point for the composite problem. This complexity is superior to the existing REINFORCE/GPOMDP and SVRPG complexities in the non-composite setting. Our algorithm has been tested on several reinforcement learning examples, and the numerical results have shown that it outperforms two existing methods. Additionally, the composite settings provide certain advantages over the non-composite ones on certain problems.",1
"We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://github.com/MishaLaskin/curl.",0
"In this article, we introduce CURL: a novel approach to Reinforcement Learning that utilizes Contrastive Unsupervised Representations. This technique allows CURL to extract high-level features from raw pixels and subsequently perform off-policy control based on these features. Our experiments demonstrate that CURL outperforms previous pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games. Specifically, we achieve 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. Notably, CURL is the first image-based algorithm to approach the sample-efficiency of methods that use state-based features in the DeepMind Control Suite. Interested readers can access our open-sourced code at https://github.com/MishaLaskin/curl.",1
"Experience replay enables online reinforcement learning agents to store and reuse the previous experiences of interacting with the environment. In the original method, the experiences are sampled and replayed uniformly at random. A prior work called prioritized experience replay was developed where experiences are prioritized, so as to replay experiences seeming to be more important more frequently. In this paper, we develop a method called double-prioritized state-recycled (DPSR) experience replay, prioritizing the experiences in both training stage and storing stage, as well as replacing the experiences in the memory with state recycling to make the best of experiences that seem to have low priorities temporarily. We used this method in Deep Q-Networks (DQN), and achieved a state-of-the-art result, outperforming the original method and prioritized experience replay on many Atari games.",0
"The utilization of experience replay allows reinforcement learning agents to save and reuse past experiences when interacting with the environment. Typically, experiences are randomly sampled and replayed. However, prioritized experience replay was introduced where more important experiences are given priority for more frequent replay. This paper proposes a new approach called double-prioritized state-recycled (DPSR) experience replay, which prioritizes experiences during both training and storage stages, and recycles low-priority experiences to maximize their potential. The DPSR method was applied to Deep Q-Networks (DQN) and demonstrated superior performance compared to the original and prioritized experience replay methods on multiple Atari games.",1
"Reinforcement learning (RL) is a promising field to enhance robotic autonomy and decision making capabilities for space robotics, something which is challenging with traditional techniques due to stochasticity and uncertainty within the environment. RL can be used to enable lunar cave exploration with infrequent human feedback, faster and safer lunar surface locomotion or the coordination and collaboration of multi-robot systems. However, there are many hurdles making research challenging for space robotic applications using RL and machine learning, particularly due to insufficient resources for traditional robotics simulators like CoppeliaSim. Our solution to this is an open source modular platform called Reinforcement Learning for Simulation based Training of Robots, or RL STaR, that helps to simplify and accelerate the application of RL to the space robotics research field. This paper introduces the RL STaR platform, and how researchers can use it through a demonstration.",0
"The field of reinforcement learning (RL) holds great potential for improving the decision-making and autonomy of space robotics, which can be difficult to achieve with conventional methods due to the unpredictable nature of the environment. RL can facilitate infrequent human feedback for lunar cave exploration, expedite and ensure safe lunar surface locomotion, and enable coordination between multi-robot systems. Nonetheless, the application of machine learning and RL to space robotics research poses several challenges, including the lack of resources for traditional robotics simulators such as CoppeliaSim. To address this issue, we have developed an open-source modular platform known as Reinforcement Learning for Simulation-based Training of Robots (RL STaR). This platform streamlines and expedites the application of RL to the space robotics research field. In this paper, we present the RL STaR platform and provide a demonstration of how researchers can utilize it.",1
"Existing model-based value expansion methods typically leverage a world model for value estimation with a fixed rollout horizon to assist policy learning. However, the fixed rollout with an inaccurate model has a potential to harm the learning process. In this paper, we investigate the idea of using the model knowledge for value expansion adaptively. We propose a novel method called Dynamic-horizon Model-based Value Expansion (DMVE) to adjust the world model usage with different rollout horizons. Inspired by reconstruction-based techniques that can be applied for visual data novelty detection, we utilize a world model with a reconstruction module for image feature extraction, in order to acquire more precise value estimation. The raw and the reconstructed images are both used to determine the appropriate horizon for adaptive value expansion. On several benchmark visual control tasks, experimental results show that DMVE outperforms all baselines in sample efficiency and final performance, indicating that DMVE can achieve more effective and accurate value estimation than state-of-the-art model-based methods.",0
"Current model-based value expansion techniques rely on a fixed rollout horizon and a world model to estimate value and aid policy learning. However, if the world model is inaccurate, the fixed rollout can impede the learning process. This paper proposes an adaptive approach to using model knowledge for value expansion. The Dynamic-horizon Model-based Value Expansion (DMVE) method adjusts the world model usage according to the chosen rollout horizon. The world model incorporates a reconstruction module for image feature extraction, allowing for more precise value estimation. Both raw and reconstructed images are used to determine the appropriate horizon for adaptive value expansion. Experimental results on various visual control tasks demonstrate that DMVE outperforms all baselines in terms of sample efficiency and final performance, indicating its superior effectiveness and accuracy compared to state-of-the-art model-based methods.",1
"EXP-based algorithms are often used for exploration in multi-armed bandit. We revisit the EXP3.P algorithm and establish both the lower and upper bounds of regret in the Gaussian multi-armed bandit setting, as well as a more general distribution option. The analyses do not require bounded rewards compared to classical regret assumptions. We also extend EXP4 from multi-armed bandit to reinforcement learning to incentivize exploration by multiple agents. The resulting algorithm has been tested on hard-to-explore games and it shows an improvement on exploration compared to state-of-the-art.",0
"Multi-armed bandit exploration often uses EXP-based algorithms. In this study, we examine the EXP3.P algorithm and determine the lower and upper bounds of regret in the Gaussian multi-armed bandit setting, as well as a more general distribution option. Unlike classical regret assumptions, our analyses do not require bounded rewards. We also expand EXP4 from multi-armed bandit to reinforcement learning to encourage exploration among multiple agents. We tested the resulting algorithm on challenging games and found that it outperformed state-of-the-art methods in terms of exploration.",1
"Feature selection aims to select a subset of features to optimize the performances of downstream predictive tasks. Recently, multi-agent reinforced feature selection (MARFS) has been introduced to automate feature selection, by creating agents for each feature to select or deselect corresponding features. Although MARFS enjoys the automation of the selection process, MARFS suffers from not just the data complexity in terms of contents and dimensionality, but also the exponentially-increasing computational costs with regard to the number of agents. The raised concern leads to a new research question: Can we simplify the selection process of agents under reinforcement learning context so as to improve the efficiency and costs of feature selection? To address the question, we develop a single-agent reinforced feature selection approach integrated with restructured choice strategy. Specifically, the restructured choice strategy includes: 1) we exploit only one single agent to handle the selection task of multiple features, instead of using multiple agents. 2) we develop a scanning method to empower the single agent to make multiple selection/deselection decisions in each round of scanning. 3) we exploit the relevance to predictive labels of features to prioritize the scanning orders of the agent for multiple features. 4) we propose a convolutional auto-encoder algorithm, integrated with the encoded index information of features, to improve state representation. 5) we design a reward scheme that take into account both prediction accuracy and feature redundancy to facilitate the exploration process. Finally, we present extensive experimental results to demonstrate the efficiency and effectiveness of the proposed method.",0
"The objective of feature selection is to choose a subset of features that can enhance the performance of predictive tasks. Recently, the use of multi-agent reinforced feature selection (MARFS) has been introduced to automate this selection process by assigning agents to each feature to choose or reject them. However, MARFS suffers from a high degree of complexity in terms of data content and dimensionality, as well as exponential computational costs when dealing with a large number of agents. This has prompted researchers to ask if there is a way to simplify the selection process of agents under a reinforcement learning context to enhance the efficiency and reduce costs. To address this question, we developed a single-agent reinforced feature selection approach that incorporates a restructured choice strategy. This strategy includes using only one agent to handle the selection task of multiple features, developing a scanning method to enable the agent to make multiple selection/deselection decisions, prioritizing the scanning orders of the agent for multiple features based on their relevance to predictive labels, using a convolutional auto-encoder algorithm integrated with the encoded index information of features to improve state representation, and designing a reward scheme that considers both prediction accuracy and feature redundancy to facilitate the exploration process. Our experimental results demonstrate the effectiveness and efficiency of the proposed method.",1
"Motivated by high-stakes decision-making domains like personalized medicine where user information is inherently sensitive, we design privacy preserving exploration policies for episodic reinforcement learning (RL). We first provide a meaningful privacy formulation using the notion of joint differential privacy (JDP)--a strong variant of differential privacy for settings where each user receives their own sets of output (e.g., policy recommendations). We then develop a private optimism-based learning algorithm that simultaneously achieves strong PAC and regret bounds, and enjoys a JDP guarantee. Our algorithm only pays for a moderate privacy cost on exploration: in comparison to the non-private bounds, the privacy parameter only appears in lower-order terms. Finally, we present lower bounds on sample complexity and regret for reinforcement learning subject to JDP.",0
"Our focus is on developing privacy-preserving exploration policies for episodic reinforcement learning, particularly in domains such as personalized medicine where user data is sensitive. To achieve this, we utilize joint differential privacy (JDP), a robust form of differential privacy that applies to individual user output sets. We introduce an algorithm that combines private optimism-based learning with strong PAC and regret bounds, all while maintaining JDP. The algorithm incurs only a modest privacy cost on exploration, with the privacy parameter appearing only in lower-order terms. Lastly, we provide lower bounds for sample complexity and regret in reinforcement learning under JDP.",1
"People with type 1 diabetes (T1D) lack the ability to produce the insulin their bodies need. As a result, they must continually make decisions about how much insulin to self-administer to adequately control their blood glucose levels. Longitudinal data streams captured from wearables, like continuous glucose monitors, can help these individuals manage their health, but currently the majority of the decision burden remains on the user. To relieve this burden, researchers are working on closed-loop solutions that combine a continuous glucose monitor and an insulin pump with a control algorithm in an `artificial pancreas.' Such systems aim to estimate and deliver the appropriate amount of insulin. Here, we develop reinforcement learning (RL) techniques for automated blood glucose control. Through a series of experiments, we compare the performance of different deep RL approaches to non-RL approaches. We highlight the flexibility of RL approaches, demonstrating how they can adapt to new individuals with little additional data. On over 2.1 million hours of data from 30 simulated patients, our RL approach outperforms baseline control algorithms: leading to a decrease in median glycemic risk of nearly 50% from 8.34 to 4.24 and a decrease in total time hypoglycemic of 99.8%, from 4,610 days to 6. Moreover, these approaches are able to adapt to predictable meal times (decreasing average risk by an additional 24% as meals increase in predictability). This work demonstrates the potential of deep RL to help people with T1D manage their blood glucose levels without requiring expert knowledge. All of our code is publicly available, allowing for replication and extension.",0
"Individuals with type 1 diabetes (T1D) do not possess the necessary insulin production in their bodies, thus they must constantly make decisions about how much insulin to take to maintain their blood glucose levels. Although wearable devices such as continuous glucose monitors can aid in managing their health, the majority of the decision-making remains with the user. To alleviate this burden, researchers are developing closed-loop systems that combine a glucose monitor and insulin pump with a control algorithm to form an ""artificial pancreas."" Our study focuses on using reinforcement learning (RL) techniques to automate blood glucose control. We compare the performance of various deep RL approaches to non-RL methods through a series of experiments. Our findings show that the flexibility of RL approaches allows them to adapt to new individuals with minimal additional data. Using over 2.1 million hours of data from 30 simulated patients, our RL approach outperforms baseline control algorithms. It leads to a nearly 50% decrease in median glycemic risk and a 99.8% decrease in total time spent hypoglycemic. Furthermore, our approaches can adapt to predictable meal times, reducing average risk by an additional 24% as meal predictability increases. Our work illustrates the potential of deep RL in helping individuals with T1D manage their blood glucose levels without the need for expert knowledge. Our code is publicly available for replication and extension.",1
"Building Reinforcement Learning (RL) algorithms which are able to adapt to continuously evolving tasks is an open research challenge. One technology that is known to inherently handle such non-stationary input patterns well is Hierarchical Temporal Memory (HTM), a general and biologically plausible computational model for the human neocortex. As the RL paradigm is inspired by human learning, HTM is a natural framework for an RL algorithm supporting non-stationary environments. In this paper, we present HTMRL, the first strictly HTM-based RL algorithm. We empirically and statistically show that HTMRL scales to many states and actions, and demonstrate that HTM's ability for adapting to changing patterns extends to RL. Specifically, HTMRL performs well on a 10-armed bandit after 750 steps, but only needs a third of that to adapt to the bandit suddenly shuffling its arms. HTMRL is the first iteration of a novel RL approach, with the potential of extending to a capable algorithm for Meta-RL.",0
"Developing Reinforcement Learning (RL) algorithms that can adjust to dynamic tasks is a research challenge. Hierarchical Temporal Memory (HTM), a computational model for the human neocortex, is a technology that can effectively handle non-stationary input patterns. As RL is inspired by human learning, HTM serves as a natural framework for an RL algorithm that supports dynamic environments. This paper introduces HTMRL, the first RL algorithm that is strictly based on HTM. Our empirical analysis demonstrates that HTMRL is scalable for numerous states and actions, and that HTM's adaptability to changing patterns is beneficial for RL. Specifically, HTMRL performs well on a 10-armed bandit after 750 steps and adapts to the sudden shuffling of arms in only a third of that time. HTMRL is an innovative RL approach with the potential to become a proficient Meta-RL algorithm.",1
"Inspired by the philosophy employed by human beings to determine whether a presented face example is genuine or not, i.e., to glance at the example globally first and then carefully observe the local regions to gain more discriminative information, for the face anti-spoofing problem, we propose a novel framework based on the Convolutional Neural Network (CNN) and the Recurrent Neural Network (RNN). In particular, we model the behavior of exploring face-spoofing-related information from image sub-patches by leveraging deep reinforcement learning. We further introduce a recurrent mechanism to learn representations of local information sequentially from the explored sub-patches with an RNN. Finally, for the classification purpose, we fuse the local information with the global one, which can be learned from the original input image through a CNN. Moreover, we conduct extensive experiments, including ablation study and visualization analysis, to evaluate our proposed framework on various public databases. The experiment results show that our method can generally achieve state-of-the-art performance among all scenarios, demonstrating its effectiveness.",0
"Our proposed framework for the face anti-spoofing problem is inspired by human behavior when determining the authenticity of presented faces. We first globally glance at the example and then carefully observe local regions for more discriminative information. Our novel framework is based on the Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). We use deep reinforcement learning to model exploring face-spoofing-related information from image sub-patches. We introduce a recurrent mechanism to learn representations of local information sequentially from the explored sub-patches with an RNN. For classification, we fuse local information with global information learned from the input image through a CNN. We evaluate our proposed framework on various public databases through extensive experiments, including ablation study and visualization analysis. Our method achieves state-of-the-art performance in all scenarios, demonstrating its effectiveness.",1
"Temporal grounding of natural language in untrimmed videos is a fundamental yet challenging multimedia task facilitating cross-media visual content retrieval. We focus on the weakly supervised setting of this task that merely accesses to coarse video-level language description annotation without temporal boundary, which is more consistent with reality as such weak labels are more readily available in practice. In this paper, we propose a \emph{Boundary Adaptive Refinement} (BAR) framework that resorts to reinforcement learning (RL) to guide the process of progressively refining the temporal boundary. To the best of our knowledge, we offer the first attempt to extend RL to temporal localization task with weak supervision. As it is non-trivial to obtain a straightforward reward function in the absence of pairwise granular boundary-query annotations, a cross-modal alignment evaluator is crafted to measure the alignment degree of segment-query pair to provide tailor-designed rewards. This refinement scheme completely abandons traditional sliding window based solution pattern and contributes to acquiring more efficient, boundary-flexible and content-aware grounding results. Extensive experiments on two public benchmarks Charades-STA and ActivityNet demonstrate that BAR outperforms the state-of-the-art weakly-supervised method and even beats some competitive fully-supervised ones.",0
"The task of grounding natural language in untrimmed videos is difficult but necessary for cross-media visual content retrieval. We are focusing on a weakly supervised approach to this task, which relies on coarse video-level language description annotations without temporal boundaries. This approach is more practical as it is easier to obtain weak labels in this manner. Our proposed solution is the Boundary Adaptive Refinement (BAR) framework, which uses reinforcement learning to progressively refine the temporal boundary. We believe this is the first attempt to extend RL to temporal localization tasks with weak supervision. Since obtaining a straightforward reward function is not possible without pairwise granular boundary-query annotations, we have created a cross-modal alignment evaluator to measure the alignment degree of segment-query pairs and provide tailor-made rewards. This refinement scheme does not rely on traditional sliding window-based solutions, which makes it more efficient, boundary-flexible, and content-aware. Our experiments on two public benchmarks, Charades-STA and ActivityNet, show that BAR outperforms the state-of-the-art weakly-supervised methods and even beats some fully-supervised ones.",1
"Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.",0
"Sequential decision tasks with limited environmental feedback can be addressed using reinforcement learning (RL), a popular paradigm. However, learning in many domains still requires significant interaction with the environment, which can be expensive in realistic scenarios. To overcome this challenge, transfer learning has been applied to RL, allowing experience from one task to be utilized when learning a harder subsequent task. Recently, research has explored curriculum learning (CL) as a way to sequence tasks or data samples into a learning framework, making it possible to learn a problem that may have otherwise been too difficult to learn from scratch. This article presents a framework for CL in RL, surveys and classifies existing CL methods based on their assumptions, capabilities, and goals, and identifies open problems and suggests future directions for RL curriculum learning research.",1
"Energy-aware control for multiple unmanned aerial vehicles (UAVs) is one of the major research interests in UAV based networking. Yet few existing works have focused on how the network should react around the timing when the UAV lineup is changed. In this work, we study proactive self-remedy of energy-constrained UAV networks when one or more UAVs are short of energy and about to quit for charging. We target at an energy-aware optimal UAV control policy which proactively relocates the UAVs when any UAV is about to quit the network, rather than passively dispatches the remaining UAVs after the quit. Specifically, a deep reinforcement learning (DRL)-based self remedy approach, named SREC-DRL, is proposed to maximize the accumulated user satisfaction scores for a certain period within which at least one UAV will quit the network. To handle the continuous state and action space in the problem, the state-of-the-art algorithm of the actor-critic DRL, i.e., deep deterministic policy gradient (DDPG), is applied with better convergence stability. Numerical results demonstrate that compared with the passive reaction method, the proposed SREC-DRL approach shows a $12.12\%$ gain in accumulative user satisfaction score during the remedy period.",0
"UAV based networking research has a strong focus on energy-aware control for multiple unmanned aerial vehicles (UAVs). However, there is limited research on how the network should react when the UAV lineup changes. This study aims to address this issue by exploring proactive self-remedy of energy-constrained UAV networks. The proposed energy-aware optimal UAV control policy relocates UAVs when any UAV is about to quit the network, rather than dispatching remaining UAVs after a quit. A DRL-based self remedy approach called SREC-DRL is proposed to maximize user satisfaction scores for a certain period when at least one UAV will quit the network. To handle the continuous state and action space, the state-of-the-art algorithm of the actor-critic DRL, i.e., deep deterministic policy gradient (DDPG), is applied for better convergence stability. Numerical results show that compared to the passive reaction method, the proposed SREC-DRL approach results in a $12.12\%$ gain in accumulative user satisfaction score during the remedy period.",1
"We explore how to enable machines to model 3D shapes like human modelers using deep reinforcement learning (RL). In 3D modeling software like Maya, a modeler usually creates a mesh model in two steps: (1) approximating the shape using a set of primitives; (2) editing the meshes of the primitives to create detailed geometry. Inspired by such artist-based modeling, we propose a two-step neural framework based on RL to learn 3D modeling policies. By taking actions and collecting rewards in an interactive environment, the agents first learn to parse a target shape into primitives and then to edit the geometry. To effectively train the modeling agents, we introduce a novel training algorithm that combines heuristic policy, imitation learning and reinforcement learning. Our experiments show that the agents can learn good policies to produce regular and structure-aware mesh models, which demonstrates the feasibility and effectiveness of the proposed RL framework.",0
"Our focus is on teaching machines to replicate the abilities of human 3D modelers by employing deep reinforcement learning (RL). In typical 3D modeling software, such as Maya, a modeler will create a mesh model by first approximating its shape with a set of primitives, and then editing the meshes of these primitives to create more detailed geometry. Drawing inspiration from this artist-based modeling approach, we have developed a two-step neural framework based on RL that enables the machine to learn 3D modeling policies. By taking actions and collecting rewards within an interactive environment, the agents learn to parse a target shape into primitives and then to edit the geometry. To ensure effective training of the modeling agents, we have introduced a novel training algorithm that combines heuristic policy, imitation learning, and reinforcement learning. Our experiments have shown that the agents can learn good policies to produce regular and structure-aware mesh models, highlighting the practicality and efficiency of the proposed RL framework.",1
"The heavy traffic congestion problem has always been a concern for modern cities. To alleviate traffic congestion, researchers use reinforcement learning (RL) to develop better traffic signal control (TSC) algorithms in recent years. However, most RL models are trained and tested in the same traffic flow environment, which results in a serious overfitting problem. Since the traffic flow environment in the real world keeps varying, these models can hardly be applied due to the lack of generalization ability. Besides, the limited number of accessible traffic flow data brings extra difficulty in testing the generalization ability of the models. In this paper, we design a novel traffic flow generator based on Wasserstein generative adversarial network to generate sufficient diverse and quality traffic flows and use them to build proper training and testing environments. Then we propose a meta-RL TSC framework GeneraLight to improve the generalization ability of TSC models. GeneraLight boosts the generalization performance by combining the idea of flow clustering and model-agnostic meta-learning. We conduct extensive experiments on multiple real-world datasets to show the superior performance of GeneraLight on generalizing to different traffic flows.",0
"Modern cities have always been concerned about the problem of heavy traffic congestion. To address this issue, researchers have recently used reinforcement learning (RL) to develop better traffic signal control (TSC) algorithms. However, most RL models are trained and tested in the same traffic flow environment, resulting in serious overfitting problems. Since the traffic flow environment in the real world is constantly changing, these models lack the ability to generalize and cannot be applied widely. Additionally, the limited availability of traffic flow data makes it difficult to test the models' generalization ability. To address these issues, we present a novel traffic flow generator based on Wasserstein generative adversarial network. This generator produces diverse and high-quality traffic flows, which we use to build proper training and testing environments. We also propose a meta-RL TSC framework called GeneraLight, which uses flow clustering and model-agnostic meta-learning to improve the generalization ability of TSC models. Our experiments on multiple real-world datasets demonstrate that GeneraLight outperforms other methods in generalizing to different traffic flows.",1
"In this paper, we study the problem of balancing effectiveness and efficiency in automated feature selection. Feature selection is a fundamental intelligence for machine learning and predictive analysis. After exploring many feature selection methods, we observe a computational dilemma: 1) traditional feature selection methods (e.g., mRMR) are mostly efficient, but difficult to identify the best subset; 2) the emerging reinforced feature selection methods automatically navigate feature space to explore the best subset, but are usually inefficient. Are automation and efficiency always apart from each other? Can we bridge the gap between effectiveness and efficiency under automation? Motivated by such a computational dilemma, this study is to develop a novel feature space navigation method. To that end, we propose an Interactive Reinforced Feature Selection (IRFS) framework that guides agents by not just self-exploration experience, but also diverse external skilled trainers to accelerate learning for feature exploration. Specifically, we formulate the feature selection problem into an interactive reinforcement learning framework. In this framework, we first model two trainers skilled at different searching strategies: (1) KBest based trainer; (2) Decision Tree based trainer. We then develop two strategies: (1) to identify assertive and hesitant agents to diversify agent training, and (2) to enable the two trainers to take the teaching role in different stages to fuse the experiences of the trainers and diversify teaching process. Such a hybrid teaching strategy can help agents to learn broader knowledge, and, thereafter, be more effective. Finally, we present extensive experiments on real-world datasets to demonstrate the improved performances of our method: more efficient than existing reinforced selection and more effective than classic selection.",0
"The focus of this paper is to explore the challenge of achieving both effectiveness and efficiency in automated feature selection. Feature selection is a crucial aspect of machine learning and predictive analysis. We discovered a computational issue during our investigation, where traditional feature selection methods, like mRMR, are often efficient but lack the ability to identify the best subset, while emerging reinforced feature selection methods can navigate feature space automatically but are typically inefficient. Can automation and efficiency be combined? To address this dilemma, we propose a new feature space navigation method called Interactive Reinforced Feature Selection (IRFS). Our framework guides agents through self-exploration and external trainer experiences to accelerate feature exploration. We model two trainers who are skilled at different searching strategies and develop assertive and hesitant agent identification strategies to diversify training. Furthermore, we enable the trainers to take on different teaching roles to fuse experiences and diversify the teaching process. Our hybrid teaching strategy helps agents to gain a broader understanding and become more effective. We conducted extensive experiments on real-world datasets to demonstrate that our method is more efficient than existing reinforced selection and more effective than classic selection.",1
"Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of ""X-former"" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored ""X-former"" models, providing an organized and comprehensive overview of existing work and models across multiple domains.",0
"Lately, Transformer models have gained significant interest due to their effectiveness in various domains such as language, vision, and reinforcement learning. In the area of natural language processing, they have become an essential component of modern deep learning. Recently, a plethora of ""X-former"" models have been introduced, including Reformer, Linformer, Performer, and Longformer, which enhance the original Transformer architecture by improving computational and memory efficiency. To assist researchers in navigating this abundance of models, this paper presents a detailed and comprehensive overview of recent ""X-former"" models that prioritize efficiency, covering various domains.",1
"High false-positive rate is a long-standing challenge for anomaly detection algorithms, especially in high-stake applications. To identify the true anomalies, in practice, analysts or domain experts will be employed to investigate the top instances one by one in a ranked list of anomalies identified by an anomaly detection system. This verification procedure generates informative labels that can be leveraged to re-rank the anomalies so as to help the analyst to discover more true anomalies given a time budget. Some re-ranking strategies have been proposed to approximate the above sequential decision process. Specifically, existing strategies have been focused on making the top instances more likely to be anomalous based on the feedback. Then they greedily select the top-1 instance for query. However, these greedy strategies could be sub-optimal since some low-ranked instances could be more helpful in the long-term. In this work, we propose Active Anomaly Detection with Meta-Policy (Meta-AAD), a novel framework that learns a meta-policy for query selection. Specifically, Meta-AAD leverages deep reinforcement learning to train the meta-policy to select the most proper instance to explicitly optimize the number of discovered anomalies throughout the querying process. Meta-AAD is easy to deploy since a trained meta-policy can be directly applied to any new datasets without further tuning. Extensive experiments on 24 benchmark datasets demonstrate that Meta-AAD significantly outperforms the state-of-the-art re-ranking strategies and the unsupervised baseline. The empirical analysis shows that the trained meta-policy is transferable and inherently achieves a balance between long-term and short-term rewards.",0
"Detecting anomalies accurately is a significant challenge for anomaly detection algorithms, particularly in high-risk applications, due to the high occurrence of false positives. To identify genuine anomalies, experts or analysts are employed to investigate the top-ranked instances on a list generated by the anomaly detection system. This verification method provides informative labels that can help to re-rank the anomalies, leading to the discovery of more actual anomalies within a given time frame. However, previous re-ranking strategies have been sub-optimal as they prioritize the top-ranked instances, neglecting potentially helpful low-ranked instances. To address this issue, we introduce a novel framework called Meta-AAD (Active Anomaly Detection with Meta-Policy) that employs deep reinforcement learning to train a meta-policy for query selection. The trained meta-policy can be applied directly to new datasets without further tuning and achieves a balance between long-term and short-term rewards. Extensive experiments on 24 benchmark datasets demonstrate that Meta-AAD outperforms the state-of-the-art re-ranking strategies and the unsupervised baseline.",1
"Strategic recommendations (SR) refer to the problem where an intelligent agent observes the sequential behaviors and activities of users and decides when and how to interact with them to optimize some long-term objectives, both for the user and the business. These systems are in their infancy in the industry and in need of practical solutions to some fundamental research challenges. At Adobe research, we have been implementing such systems for various use-cases, including points of interest recommendations, tutorial recommendations, next step guidance in multi-media editing software, and ad recommendation for optimizing lifetime value. There are many research challenges when building these systems, such as modeling the sequential behavior of users, deciding when to intervene and offer recommendations without annoying the user, evaluating policies offline with high confidence, safe deployment, non-stationarity, building systems from passive data that do not contain past recommendations, resource constraint optimization in multi-user systems, scaling to large and dynamic actions spaces, and handling and incorporating human cognitive biases. In this paper we cover various use-cases and research challenges we solved to make these systems practical.",0
"The concept of Strategic Recommendations (SR) involves an intelligent agent observing users' sequential behaviors and activities to determine the optimal timing and method of interaction for both user and business long-term objectives. These systems are still in their early stages and require solutions to fundamental research challenges. Adobe Research has successfully implemented SR systems for various purposes, such as recommending points of interest, tutorials, next steps in multimedia editing, and optimizing ad recommendations. Building such systems presents various challenges, including modeling user behavior, determining the right time to intervene without irritating the user, offline policy evaluation, safe deployment, non-stationarity, creating systems from passive data, optimizing limited resources in multi-user systems, scaling for large action spaces, and dealing with human cognitive biases. This paper discusses how we solved these challenges and implemented practical SR systems for different use-cases.",1
"Markov reward processes (MRPs) are used to model stochastic phenomena arising in operations research, control engineering, robotics, and artificial intelligence, as well as communication and transportation networks. In many of these cases, such as in the policy evaluation problem encountered in reinforcement learning, the goal is to estimate the long-term value function of such a process without access to the underlying population transition and reward functions. Working with samples generated under the synchronous model, we study the problem of estimating the value function of an infinite-horizon, discounted MRP on finitely many states in the $\ell_\infty$-norm. We analyze both the standard plug-in approach to this problem and a more robust variant, and establish non-asymptotic bounds that depend on the (unknown) problem instance, as well as data-dependent bounds that can be evaluated based on the observations of state-transitions and rewards. We show that these approaches are minimax-optimal up to constant factors over natural sub-classes of MRPs. Our analysis makes use of a leave-one-out decoupling argument tailored to the policy evaluation problem, one which may be of independent interest.",0
"Markov reward processes (MRPs) are utilized to model random occurrences found in various fields, including operations research, control engineering, robotics, artificial intelligence, communication, and transportation networks. In certain scenarios, such as in reinforcement learning, the objective is to estimate the long-term value function of an MRP without access to the underlying population transition and reward functions. We explore the problem of computing the value function of an infinite-horizon, discounted MRP on a finite number of states in the $\ell_\infty$-norm, using samples generated under the synchronous model. We investigate both the standard plug-in approach and a more robust variant, and establish non-asymptotic bounds that rely on the problem instance (which is unknown) and data-dependent bounds that can be evaluated using state-transitions and rewards observations. We demonstrate that these approaches are minimax-optimal up to constant factors over natural sub-classes of MRPs. Our analysis employs a leave-one-out decoupling argument, which we tailor to the policy evaluation problem, which may be of independent interest.",1
"As discussed in previous studies, the efficacy of evolutionary or reinforcement learning algorithms for continuous control optimization can be enhanced by including a neural module dedicated to feature extraction trained through self-supervised methods. In this paper we report additional experiments supporting this hypothesis and we demonstrate how the advantage provided by feature extraction is not limited to problems that benefit from dimensionality reduction or that involve agents operating on the basis of allocentric perception. We introduce a method that permits to continue the training of the feature-extraction module during the training of the policy network and that increases the efficacy of feature extraction. Finally, we compare alternative feature-extracting methods and we show that sequence-to-sequence learning yields better results than the methods considered in previous studies.",0
"Previous studies have shown that the effectiveness of evolutionary or reinforcement learning algorithms for continuous control optimization can be improved by incorporating a neural module dedicated to feature extraction, which is trained using self-supervised methods. This article presents additional experiments that support this theory and demonstrates that the benefits of feature extraction are not limited to problems that require dimensionality reduction or involve agents operating on allocentric perception. The article also introduces a method for enhancing feature extraction by continuing training during the policy network training process. Finally, alternative feature-extraction methods are compared, and the results indicate that sequence-to-sequence learning outperforms previously considered methods.",1
"We introduce a novel framework to account for sensitivity to rewards uncertainty in sequential decision-making problems. While risk-sensitive formulations for Markov decision processes studied so far focus on the distribution of the cumulative reward as a whole, we aim at learning policies sensitive to the uncertain/stochastic nature of the rewards, which has the advantage of being conceptually more meaningful in some cases. To this end, we present a new decomposition of the randomness contained in the cumulative reward based on the Doob decomposition of a stochastic process, and introduce a new conceptual tool - the \textit{chaotic variation} - which can rigorously be interpreted as the risk measure of the martingale component associated to the cumulative reward process. We innovate on the reinforcement learning side by incorporating this new risk-sensitive approach into model-free algorithms, both policy gradient and value function based, and illustrate its relevance on grid world and portfolio optimization problems.",0
"Our framework offers a unique approach to address the impact of uncertain rewards on sequential decision-making problems. Unlike previous risk-sensitive formulations for Markov decision processes that focus on the overall distribution of cumulative rewards, our aim is to develop policies that account for the stochastic nature of rewards in a more meaningful way. We achieve this by introducing a new way to decompose the randomness in cumulative rewards, using the Doob decomposition of a stochastic process. Additionally, we introduce the concept of ""chaotic variation"" as a rigorous measure of risk for the martingale component of the cumulative reward process. We integrate this novel risk-sensitive approach into model-free algorithms for reinforcement learning, including both policy gradient and value function-based methods. Our approach is demonstrated to be effective in solving problems related to grid world and portfolio optimization.",1
"In reinforcement learning (RL), we always expect the agent to explore as many states as possible in the initial stage of training and exploit the explored information in the subsequent stage to discover the most returnable trajectory. Based on this principle, in this paper, we soften the proximal policy optimization by introducing the entropy and dynamically setting the temperature coefficient to balance the opportunity of exploration and exploitation. While maximizing the expected reward, the agent will also seek other trajectories to avoid the local optimal policy. Nevertheless, the increase of randomness induced by entropy will reduce the train speed in the early stage. Integrating the temporal-difference (TD) method and the general advantage estimator (GAE), we propose the dual-track advantage estimator (DTAE) to accelerate the convergence of value functions and further enhance the performance of the algorithm. Compared with other on-policy RL algorithms on the Mujoco environment, the proposed method not only significantly speeds up the training but also achieves the most advanced results in cumulative return.",0
"The initial stage of training in reinforcement learning (RL) requires the agent to explore as many states as possible and then exploit the information gathered to determine the most lucrative trajectory. To achieve this, we introduce entropy and dynamically adjust the temperature coefficient to balance exploration and exploitation in the proximal policy optimization. While maximizing rewards, the agent will also look for alternative trajectories to avoid local optima. However, the increase in randomness from entropy will slow down training in the early stages. To speed up convergence of value functions and improve algorithm performance, we propose the dual-track advantage estimator (DTAE), which combines the temporal-difference (TD) method and the general advantage estimator (GAE). Our proposed method achieves faster training and the highest cumulative return compared to other on-policy RL algorithms on the Mujoco environment.",1
"Off-policy policy optimization is a challenging problem in reinforcement learning (RL). The algorithms designed for this problem often suffer from high variance in their estimators, which results in poor sample efficiency, and have issues with convergence. A few variance-reduced on-policy policy gradient algorithms have been recently proposed that use methods from stochastic optimization to reduce the variance of the gradient estimate in the REINFORCE algorithm. However, these algorithms are not designed for the off-policy setting and are memory-inefficient, since they need to collect and store a large ``reference'' batch of samples from time to time. To achieve variance-reduced off-policy-stable policy optimization, we propose an algorithm family that is memory-efficient, stochastically variance-reduced, and capable of learning from off-policy samples. Empirical studies validate the effectiveness of the proposed approaches.",0
"Reinforcement learning faces a challenging problem with off-policy policy optimization. Existing algorithms for this task suffer from high variance and poor sample efficiency, as well as convergence issues. While a few on-policy policy gradient algorithms have been developed to reduce variance using stochastic optimization methods, these are not suitable for off-policy settings and require storing large batches of reference samples, making them memory-inefficient. To address these problems, we present a novel algorithm family that is both memory-efficient and capable of learning from off-policy samples while reducing variance. Empirical studies demonstrate the effectiveness of our algorithms.",1
"Recent works using deep learning to solve the Traveling Salesman Problem (TSP) have focused on learning construction heuristics. Such approaches find TSP solutions of good quality but require additional procedures such as beam search and sampling to improve solutions and achieve state-of-the-art performance. However, few studies have focused on improvement heuristics, where a given solution is improved until reaching a near-optimal one. In this work, we propose to learn a local search heuristic based on 2-opt operators via deep reinforcement learning. We propose a policy gradient algorithm to learn a stochastic policy that selects 2-opt operations given a current solution. Moreover, we introduce a policy neural network that leverages a pointing attention mechanism, which unlike previous works, can be easily extended to more general k-opt moves. Our results show that the learned policies can improve even over random initial solutions and approach near-optimal solutions at a faster rate than previous state-of-the-art deep learning methods.",0
"Current research on applying deep learning to solve the Traveling Salesman Problem (TSP) has mainly concentrated on learning construction heuristics. Although such approaches produce reasonable TSP solutions, various supplementary processes such as beam search and sampling are necessary to refine the solutions and achieve top-notch performance. Nonetheless, there have been hardly any studies that have focused on improving heuristics, which involve enhancing a given solution until it approaches an optimal one. This research proposes using deep reinforcement learning to learn a local search heuristic that employs 2-opt operators. A policy gradient algorithm is proposed to learn a stochastic policy that selects 2-opt operations based on the current solution. Additionally, a policy neural network that utilizes a pointing attention mechanism is presented, which, unlike prior research, can be easily expanded to more general k-opt moves. The obtained results indicate that the learned policies can enhance even random initial solutions and approach near-optimal solutions more rapidly than previously established state-of-the-art deep learning methods.",1
"In many real world applications, reinforcement learning agents have to optimize multiple objectives while following certain rules or satisfying a list of constraints. Classical methods based on reward shaping, i.e. a weighted combination of different objectives in the reward signal, or Lagrangian methods, including constraints in the loss function, have no guarantees that the agent satisfies the constraints at all points in time and can lead to undesired behavior. When a discrete policy is extracted from an action-value function, safe actions can be ensured by restricting the action space at maximization, but can lead to sub-optimal solutions among feasible alternatives. In this work, we propose Constrained Q-learning, a novel off-policy reinforcement learning framework restricting the action space directly in the Q-update to learn the optimal Q-function for the induced constrained MDP and the corresponding safe policy. In addition to single-step constraints referring only to the next action, we introduce a formulation for approximate multi-step constraints under the current target policy based on truncated value-functions. We analyze the advantages of Constrained Q-learning in the tabular case and compare Constrained DQN to reward shaping and Lagrangian methods in the application of high-level decision making in autonomous driving, considering constraints for safety, keeping right and comfort. We train our agent in the open-source simulator SUMO and on the real HighD data set.",0
"Reinforcement learning agents in real world scenarios often need to optimize multiple objectives while adhering to rules or satisfying a set of constraints. Traditional approaches such as reward shaping and Lagrangian methods do not guarantee that constraints are met at all times and can result in undesired actions. While restricting the action space can ensure safe actions, it may not lead to optimal solutions. This study proposes a new approach called Constrained Q-learning, which restricts the action space directly in the Q-update to learn the optimal Q-function for the induced constrained MDP and corresponding safe policy. This approach also includes a formulation for approximate multi-step constraints based on truncated value-functions. The advantages of Constrained Q-learning are analyzed in the tabular case and compared to reward shaping and Lagrangian methods in the application of high-level decision making in autonomous driving, considering safety, keeping right, and comfort constraints. The agent is trained in the open-source simulator SUMO and on the real HighD data set.",1
"Reinforcement learning from self-play has recently reported many successes. Self-play, where the agents compete with themselves, is often used to generate training data for iterative policy improvement. In previous work, heuristic rules are designed to choose an opponent for the current learner. Typical rules include choosing the latest agent, the best agent, or a random historical agent. However, these rules may be inefficient in practice and sometimes do not guarantee convergence even in the simplest matrix games. In this paper, we propose a new algorithmic framework for competitive self-play reinforcement learning in two-player zero-sum games. We recognize the fact that the Nash equilibrium coincides with the saddle point of the stochastic payoff function, which motivates us to borrow ideas from classical saddle point optimization literature. Our method trains several agents simultaneously, and intelligently takes each other as opponent based on simple adversarial rules derived from a principled perturbation-based saddle optimization method. We prove theoretically that our algorithm converges to an approximate equilibrium with high probability in convex-concave games under standard assumptions. Beyond the theory, we further show the empirical superiority of our method over baseline methods relying on the aforementioned opponent-selection heuristics in matrix games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net policy function approximators.",0
"Recently, reinforcement learning through self-play has yielded numerous successes. Self-play involves agents competing with themselves, commonly used to generate training data for iterative policy improvement. Previous research has designed heuristic rules to select an opponent for the current learner, such as choosing the latest, best, or a random historical agent. However, these rules may prove inefficient and not guarantee convergence, even in simple matrix games. This paper presents a new algorithmic framework for competitive self-play reinforcement learning in two-player zero-sum games. It acknowledges that the Nash equilibrium coincides with the saddle point of the stochastic payoff function, prompting the authors to borrow from classical saddle point optimization literature. The proposed method trains several agents simultaneously and intelligently selects each other as opponents based on simple adversarial rules derived from a principled perturbation-based saddle optimization method. The authors theoretically prove the algorithm's convergence to an approximate equilibrium with high probability in convex-concave games under standard assumptions. Furthermore, they demonstrate the empirical superiority of their method over baseline methods that rely on the aforementioned opponent-selection heuristics in matrix games, grid-world soccer, Gomoku, and simulated robot sumo, with neural net policy function approximators.",1
"Severe infectious diseases such as the novel coronavirus (COVID-19) pose a huge threat to public health. Stringent control measures, such as school closures and stay-at-home orders, while having significant effects, also bring huge economic losses. A crucial question for policymakers around the world is how to make the trade-off and implement the appropriate interventions. In this work, we propose a Multi-Objective Reinforcement Learning framework to facilitate the data-driven decision making and minimize the long-term overall cost. Specifically, at each decision point, a Bayesian epidemiological model is first learned as the environment model, and then we use the proposed model-based multi-objective planning algorithm to find a set of Pareto-optimal policies. This framework, combined with the prediction bands for each policy, provides a real-time decision support tool for policymakers. The application is demonstrated with the spread of COVID-19 in China.",0
"Public health is threatened by severe infectious diseases, such as COVID-19, which necessitate strict control measures like school closures and stay-at-home orders. However, these measures result in significant economic losses, and policymakers worldwide must determine the appropriate interventions. To aid in data-driven decision-making and minimize long-term overall expenses, we suggest a Multi-Objective Reinforcement Learning framework. At each decision point, a Bayesian epidemiological model acts as the environment model, and our proposed model-based multi-objective planning algorithm discovers a set of Pareto-optimal policies. This framework, combined with prediction bands for each policy, provides a real-time decision support tool. The spread of COVID-19 in China is utilized to demonstrate the application.",1
"In this paper we apply guided policy search (GPS) based reinforcement learning framework for a high dimensional optimal control problem arising in an additive manufacturing process. The problem comprises of controlling the process parameters so that layer-wise deposition of material leads to desired geometric characteristics of the resulting part surface while minimizing the material deposited. A realistic simulation model of the deposition process along with carefully selected set of guiding distributions generated based on iterative Linear Quadratic Regulator is used to train a neural network policy using GPS. A closed loop control based on the trained policy and in-situ measurement of the deposition profile is tested experimentally, and shows promising performance.",0
"This paper utilizes a guided policy search (GPS) framework for reinforcement learning to address a high-dimensional optimal control problem that arises in additive manufacturing. Specifically, the problem involves controlling the process parameters to achieve desired geometric features of a part surface while minimizing material usage during layer-wise deposition. To train a neural network policy using GPS, a realistic simulation model of the deposition process and a carefully selected set of guiding distributions generated from iterative Linear Quadratic Regulator are employed. Finally, a closed-loop control based on the trained policy and in-situ measurement of the deposition profile is experimentally tested and demonstrates encouraging results.",1
"Few sample learning (FSL) is significant and challenging in the field of machine learning. The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence since humans can readily establish their cognition to novelty from just a single or a handful of examples whereas machine learning algorithms typically entail hundreds or thousands of supervised samples to guarantee generalization ability. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning technologies, little surveys or reviews for FSL are available until now. In this context, we extensively review 300+ papers of FSL spanning from the 2000s to 2019 and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history as well as the current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review the latest advances on these topics. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.",0
"Learning from few samples (FSL) is a significant challenge in the field of machine learning. The ability to learn and generalize from very few examples distinguishes human intelligence from artificial intelligence, as humans can quickly adapt to new situations with just a single or a handful of examples. In contrast, machine learning algorithms typically require hundreds or thousands of supervised samples to achieve generalization. Despite being a topic of interest since the early 2000s and the recent surge of deep learning technologies, there are few surveys or reviews available on FSL. Therefore, we conducted an extensive review of over 300 FSL papers from 2000 to 2019, providing a timely and comprehensive survey. Our review covers the evolution and current progress of FSL, categorizing approaches into generative and discriminative models, with an emphasis on meta-learning approaches. We also highlight emerging topics and recent advances, as well as FSL applications in computer vision, natural language processing, audio and speech, reinforcement learning, robotic, and data analysis. Finally, we discuss promising trends as guidance for future research.",1
"Imitation learning in a high-dimensional environment is challenging. Most inverse reinforcement learning (IRL) methods fail to outperform the demonstrator in such a high-dimensional environment, e.g., Atari domain. To address this challenge, we propose a novel reward learning module to generate intrinsic reward signals via a generative model. Our generative method can perform better forward state transition and backward action encoding, which improves the module's dynamics modeling ability in the environment. Thus, our module provides the imitation agent both the intrinsic intention of the demonstrator and a better exploration ability, which is critical for the agent to outperform the demonstrator. Empirical results show that our method outperforms state-of-the-art IRL methods on multiple Atari games, even with one-life demonstration. Remarkably, our method achieves performance that is up to 5 times the performance of the demonstration.",0
"It is difficult to achieve imitation learning in a high-dimensional environment. Inverse reinforcement learning (IRL) methods typically fail to surpass the performance of the demonstrator in such environments, such as the Atari domain. To overcome this challenge, we present a fresh reward learning module that uses a generative model to create intrinsic reward signals. Our generative approach facilitates better forward state transition and backward action encoding, improving the module's ability to model the environment's dynamics. As a result, the module equips the imitation agent with both the intrinsic intention of the demonstrator and enhanced exploration abilities, which are crucial for outperforming the demonstrator. Our empirical results demonstrate that our method surpasses state-of-the-art IRL methods in multiple Atari games, even with just one-life demonstration. Notably, our method achieves performance up to five times greater than that of the demonstration.",1
"The goal of the inverse reinforcement learning (IRL) problem is to recover the reward functions from expert demonstrations. However, the IRL problem like any ill-posed inverse problem suffers the congenital defect that the policy may be optimal for many reward functions, and expert demonstrations may be optimal for many policies. In this work, we generalize the IRL problem to a well-posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions. We adopt the Monte Carlo expectation-maximization (MCEM) method to estimate the parameter of the probability distribution as the first solution to the SIRL problem. The solution is succinct, robust, and transferable for a learning task and can generate alternative solutions to the IRL problem. Through our formulation, it is possible to observe the intrinsic property for the IRL problem from a global viewpoint, and our approach achieves a considerable performance on the objectworld.",0
"The aim of inverse reinforcement learning (IRL) is to retrieve the reward functions from expert demonstrations. However, this problem is inherently flawed, as the policy may be optimal for various reward functions, and expert demonstrations may be optimal for multiple policies. In this study, we introduce stochastic inverse reinforcement learning (SIRL), a well-defined expectation optimization problem that recovers the probability distribution over reward functions. Our solution employs the Monte Carlo expectation-maximization (MCEM) method to estimate the probability distribution parameter as the first step. It is concise, resilient, and adaptable to learning tasks, and can provide alternative solutions to the IRL problem. Our approach provides a global perspective on the intrinsic characteristics of the IRL problem and performs well on the objectworld.",1
"Counterfactual regret minimization (CFR) is a popular method to deal with decision-making problems of two-player zero-sum games with imperfect information. Unlike existing studies that mostly explore for solving larger scale problems or accelerating solution efficiency, we propose a framework, RLCFR, which aims at improving the generalization ability of the CFR method. In the RLCFR, the game strategy is solved by the CFR in a reinforcement learning framework. And the dynamic procedure of iterative interactive strategy updating is modeled as a Markov decision process (MDP). Our method, RLCFR, then learns a policy to select the appropriate way of regret updating in the process of iteration. In addition, a stepwise reward function is formulated to learn the action policy, which is proportional to how well the iteration strategy is at each step. Extensive experimental results on various games have shown that the generalization ability of our method is significantly improved compared with existing state-of-the-art methods.",0
"The Counterfactual regret minimization (CFR) method is commonly used to address decision-making issues in imperfect information two-player zero-sum games. While prior research has focused on solving larger problems or improving solution efficiency, our study introduces a framework called RLCFR that enhances the generalization capabilities of CFR. In RLCFR, the CFR solves game strategies within a reinforcement learning framework, with iterative interactive strategy updates modeled as a Markov decision process (MDP). Our approach, RLCFR, learns a policy for selecting the appropriate regret updating method during iteration, using a stepwise reward function to determine the action policy based on how effective the iteration strategy is at each step. Extensive experiments on various games have demonstrated that our method significantly enhances generalization compared to current state-of-the-art techniques.",1
"Latent factor models are the driving forces of the state-of-the-art recommender systems, with an important insight of vectorizing raw input features into dense embeddings. The dimensions of different feature embeddings are often set to a same value empirically, which limits the predictive performance of latent factor models. Existing works have proposed heuristic or reinforcement learning-based methods to search for mixed feature embedding dimensions. For efficiency concern, these methods typically choose embedding dimensions from a restricted set of candidate dimensions. However, this restriction will hurt the flexibility of dimension selection, leading to suboptimal performance of search results. In this paper, we propose Differentiable Neural Input Search (DNIS), a method that searches for mixed feature embedding dimensions in a more flexible space through continuous relaxation and differentiable optimization. The key idea is to introduce a soft selection layer that controls the significance of each embedding dimension, and optimize this layer according to model's validation performance. DNIS is model-agnostic and thus can be seamlessly incorporated with existing latent factor models for recommendation. We conduct experiments with various architectures of latent factor models on three public real-world datasets for rating prediction, Click-Through-Rate (CTR) prediction, and top-k item recommendation. The results demonstrate that our method achieves the best predictive performance compared with existing neural input search approaches with fewer embedding parameters and less time cost.",0
"The current state-of-the-art recommender systems are powered by latent factor models, which involve transforming raw input features into dense embeddings. However, the dimensions of these feature embeddings are typically set to the same value based on empirical observations, which can limit the predictive capabilities of the models. Previous research has attempted to address this issue through heuristic or reinforcement learning-based methods, but they often restrict the candidate dimensions for efficiency reasons, which can result in suboptimal performance. To overcome these limitations, this paper proposes a new method called Differentiable Neural Input Search (DNIS), which uses continuous relaxation and differentiable optimization to search for mixed feature embedding dimensions in a more flexible space. This approach includes a soft selection layer that controls the significance of each embedding dimension and optimizes it based on the model's validation performance. DNIS is a model-agnostic technique, which can be easily integrated with existing latent factor models. The experiments conducted on three public datasets for rating prediction, Click-Through-Rate (CTR) prediction, and top-k item recommendation show that DNIS outperforms existing neural input search approaches with fewer embedding parameters and less time cost.",1
"The subject of this paper is reinforcement learning. Policies are considered here that produce actions based on states and random elements autocorrelated in subsequent time instants. Consequently, an agent learns from experiments that are distributed over time and potentially give better clues to policy improvement. Also, physical implementation of such policies, e.g. in robotics, is less problematic, as it avoids making robots shake. This is in opposition to most RL algorithms which add white noise to control causing unwanted shaking of the robots. An algorithm is introduced here that approximately optimizes the aforementioned policy. Its efficiency is verified for four simulated learning control problems (Ant, HalfCheetah, Hopper, and Walker2D) against three other methods (PPO, SAC, ACER). The algorithm outperforms others in three of these problems.",0
"The focus of this paper is reinforcement learning, which involves using policies that generate actions based on states and autocorrelated random elements in subsequent time periods. This enables an agent to learn from experiments that are spread out over time and provide more insightful information for policy improvement. Additionally, implementing these policies in physical systems, like robotics, is less problematic as it eliminates the issue of shaking that occurs with most RL algorithms that add white noise to control. The paper introduces an algorithm that optimizes this policy and demonstrates its effectiveness by comparing it to three other methods (PPO, SAC, ACER) in four simulated learning control problems (Ant, HalfCheetah, Hopper, and Walker2D). The algorithm outperforms the others in three of these problems.",1
"This work examines the use of reinforcement learning (RL) to optimize cyclic lockdowns, which is one of the methods available for control of the COVID-19 pandemic. The problem is structured as an optimal control system for tracking a reference value, corresponding to the maximum usage level of a critical resource, such as ICU beds. However, instead of using conventional optimal control methods, RL is used to find optimal control policies. A framework was developed to calculate optimal cyclic lockdown timings using an RL-based on-off controller. The RL-based controller is implemented as an RL agent that interacts with an epidemic simulator, implemented as an extended SEIR epidemic model. The RL agent learns a policy function that produces an optimal sequence of open/lockdown decisions such that goals specified in the RL reward function are optimized. Two concurrent goals were used: the first one is a public health goal that minimizes overshoots of ICU bed usage above an ICU bed threshold, and the second one is a socio-economic goal that minimizes the time spent under lockdowns. It is assumed that cyclic lockdowns are considered as a temporary alternative to extended lockdowns when a region faces imminent danger of overpassing resource capacity limits and when imposing an extended lockdown would cause severe social and economic consequences due to lack of necessary economic resources to support its affected population during an extended lockdown.",0
"This study investigates the use of reinforcement learning (RL) as a means of optimizing cyclic lockdowns, which are among the strategies available for controlling the COVID-19 pandemic. The approach involves structuring the problem as an optimal control system that tracks a reference value representing the maximum utilization of a critical resource, such as ICU beds. However, instead of relying on conventional optimal control methods, the study employs RL to determine optimal control policies. A framework is developed for computing the optimal timing of cyclic lockdowns using an RL-based on-off controller. The controller is implemented as an RL agent that interacts with an epidemic simulator based on an extended SEIR model. The RL agent learns a policy function that generates the best sequence of open/lockdown decisions to optimize goals specified in the RL reward function. The study employs two concurrent goals: a public health goal that minimizes overshoots of ICU bed usage above a threshold, and a socio-economic goal that minimizes the duration of lockdowns. The study assumes that cyclic lockdowns are a temporary solution to extended lockdowns when a region faces the imminent risk of exceeding resource capacity limits, and when imposing an extended lockdown would have severe social and economic ramifications due to a lack of necessary resources to support the affected population during an extended lockdown.",1
"We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework which modifies traditional on-policy actor-critic methods by separating policy and value function training into distinct phases. In prior methods, one must choose between using a shared network or separate networks to represent the policy and value function. Using separate networks avoids interference between objectives, while using a shared network allows useful features to be shared. PPG is able to achieve the best of both worlds by splitting optimization into two phases, one that advances training and one that distills features. PPG also enables the value function to be more aggressively optimized with a higher level of sample reuse. Compared to PPO, we find that PPG significantly improves sample efficiency on the challenging Procgen Benchmark.",0
"The Phasic Policy Gradient (PPG) is a reinforcement learning framework that modifies traditional on-policy actor-critic methods by separating the training of policy and value function into different phases. In previous methods, choosing between separate or shared networks to represent policy and value function was necessary. Although using separate networks prevents interference between objectives, using shared networks allows feature sharing. PPG splits optimization into two phases, one for advancing training, and the other for distilling features, achieving the benefits of both methods. PPG also enables aggressive optimization of value function with increased sample reuse. PPG improves sample efficiency on the challenging Procgen Benchmark significantly more than PPO.",1
"The goal of meta-learning is to train a model on a variety of learning tasks, such that it can adapt to new problems within only a few iterations. Here we propose a principled information-theoretic model that optimally partitions the underlying problem space such that specialized expert decision-makers solve the resulting sub-problems. To drive this specialization we impose the same kind of information processing constraints both on the partitioning and the expert decision-makers. We argue that this specialization leads to efficient adaptation to new tasks. To demonstrate the generality of our approach we evaluate three meta-learning domains: image classification, regression, and reinforcement learning.",0
"Meta-learning aims to develop a model that can quickly adapt to new problems by being trained on various learning tasks. Our proposal is an information-theoretic model that divides the problem space into sub-problems, each solved by specialized experts. We apply information processing constraints to both the partitioning and experts to promote specialization and efficient adaptation to new tasks. We test our approach in three meta-learning domains: image classification, regression, and reinforcement learning to demonstrate its versatility.",1
"Reinforcement learning agents are faced with two types of uncertainty. Epistemic uncertainty stems from limited data and is useful for exploration, whereas aleatoric uncertainty arises from stochastic environments and must be accounted for in risk-sensitive applications. We highlight the challenges involved in simultaneously estimating both of them, and propose a framework for disentangling and estimating these uncertainties on learned Q-values. We derive unbiased estimators of these uncertainties and introduce an uncertainty-aware DQN algorithm, which we show exhibits safe learning behavior and outperforms other DQN variants on the MinAtar testbed.",0
"Agents that use reinforcement learning encounter two forms of uncertainty. The first is epistemic uncertainty, which results from insufficient data and is beneficial for exploration. The second type is aleatoric uncertainty, which emerges from stochastic environments and is essential in risk-sensitive applications. Our focus is on addressing the difficulties associated with the simultaneous estimation of these two types of uncertainty. We propose a framework for separating and evaluating these uncertainties on learned Q-values and present unbiased estimators for them. Furthermore, we present an uncertainty-aware DQN algorithm that demonstrates safe learning behavior and outperforms other DQN variants on the MinAtar testbed.",1
"Reinforcement Learning (RL) can be used to fit a mapping from patient state to a medication regimen. Prior studies have used deterministic and value-based tabular learning to learn a propofol dose from an observed anesthetic state. Deep RL replaces the table with a deep neural network and has been used to learn medication regimens from registry databases. Here we perform the first application of deep RL to closed-loop control of anesthetic dosing in a simulated environment. We use the cross-entropy method to train a deep neural network to map an observed anesthetic state to a probability of infusing a fixed propofol dosage. During testing, we implement a deterministic policy that transforms the probability of infusion to a continuous infusion rate. The model is trained and tested on simulated pharmacokinetic/pharmacodynamic models with randomized parameters to ensure robustness to patient variability. The deep RL agent significantly outperformed a proportional-integral-derivative controller (median absolute performance error 1.7% +/- 0.6 and 3.4% +/- 1.2). Modeling continuous input variables instead of a table affords more robust pattern recognition and utilizes our prior domain knowledge. Deep RL learned a smooth policy with a natural interpretation to data scientists and anesthesia care providers alike.",0
"The utilization of Reinforcement Learning (RL) in fitting a mapping from a patient's state to a medication regimen has been studied in the past. Previous studies have employed deterministic and value-based tabular learning to learn the propofol dose from an observed anesthetic state. However, Deep RL replaces the table with a deep neural network and has been used to learn medication regimens from registry databases. In this research, we present the first application of Deep RL to closed-loop control of anesthetic dosing in a simulated environment. We employ the cross-entropy method to train a deep neural network to map an observed anesthetic state to a probability of infusing a fixed propofol dosage. During testing, we implement a deterministic policy that converts the probability of infusion to a continuous infusion rate. The model is trained and tested on simulated pharmacokinetic/pharmacodynamic models with randomized parameters to ensure robustness to patient variability. The deep RL agent significantly outperformed a proportional-integral-derivative controller, with a median absolute performance error of 1.7% +/- 0.6 compared to 3.4% +/- 1.2. By modeling continuous input variables instead of a table, we achieve more robust pattern recognition and utilize our prior domain knowledge. The Deep RL learned a smooth policy that data scientists and anesthesia care providers could interpret naturally.",1
"We present a novel approach (DyNODE) that captures the underlying dynamics of a system by incorporating control in a neural ordinary differential equation framework. We conduct a systematic evaluation and comparison of our method and standard neural network architectures for dynamics modeling. Our results indicate that a simple DyNODE architecture when combined with an actor-critic reinforcement learning (RL) algorithm that uses model predictions to improve the critic's target values, outperforms canonical neural networks, both in sample efficiency and predictive performance across a diverse range of continuous tasks that are frequently used to benchmark RL algorithms. This approach provides a new avenue for the development of models that are more suited to learn the evolution of dynamical systems, particularly useful in the context of model-based reinforcement learning. To assist related work, we have made code available at https://github.com/vmartinezalvarez/DyNODE .",0
"Our paper introduces DyNODE, a unique method that utilizes control within a neural ordinary differential equation framework to capture a system's underlying dynamics. We conducted a thorough evaluation and comparison of our approach with standard neural network architectures for dynamics modeling. Our findings suggest that a basic DyNODE structure, combined with an actor-critic reinforcement learning algorithm, outperforms traditional neural networks in both sample efficiency and predictive performance across a variety of continuous tasks that are commonly used to evaluate RL algorithms. This innovative technique provides a new avenue for developing models that can better learn the evolution of dynamical systems, particularly in the context of model-based reinforcement learning. To aid researchers in related fields, we have made our code accessible at https://github.com/vmartinezalvarez/DyNODE.",1
"Black-box optimization is primarily important for many compute-intensive applications, including reinforcement learning (RL), robot control, etc. This paper presents a novel theoretical framework for black-box optimization, in which our method performs stochastic update with the implicit natural gradient of an exponential-family distribution. Theoretically, we prove the convergence rate of our framework with full matrix update for convex functions. Our theoretical results also hold for continuous non-differentiable black-box functions. Our methods are very simple and contain less hyper-parameters than CMA-ES \cite{hansen2006cma}. Empirically, our method with full matrix update achieves competitive performance compared with one of the state-of-the-art method CMA-ES on benchmark test problems. Moreover, our methods can achieve high optimization precision on some challenging test functions (e.g., $l_1$-norm ellipsoid test problem and Levy test problem), while methods with explicit natural gradient, i.e., IGO \cite{ollivier2017information} with full matrix update can not. This shows the efficiency of our methods.",0
"Black-box optimization is crucial for various compute-intensive applications such as robot control and reinforcement learning (RL). In this paper, we introduce a new theoretical framework for black-box optimization. Our method employs a stochastic update with the implicit natural gradient of an exponential-family distribution. We demonstrate the convergence rate of our framework with full matrix update for convex functions and continuous non-differentiable black-box functions. Compared to CMA-ES, our methods have fewer hyper-parameters and are straightforward. Our empirical results show that our method with full matrix update performs competitively with CMA-ES on benchmark test problems. Additionally, our methods achieve high optimization precision on challenging test functions such as the $l_1$-norm ellipsoid test problem and Levy test problem, unlike IGO with full matrix update, which employs explicit natural gradient. This highlights the effectiveness of our methods.",1
"Unmanned Aerial Systems (UAS) are being increasingly deployed for commercial, civilian, and military applications. The current UAS state-of-the-art still depends on a remote human controller with robust wireless links to perform several of these applications. The lack of autonomy restricts the domains of application and tasks for which a UAS can be deployed. Enabling autonomy and intelligence to the UAS will help overcome this hurdle and expand its use improving safety and efficiency. The exponential increase in computing resources and the availability of large amount of data in this digital era has led to the resurgence of machine learning from its last winter. Therefore, in this chapter, we discuss how some of the advances in machine learning, specifically deep learning and reinforcement learning can be leveraged to develop next-generation autonomous UAS. We first begin motivating this chapter by discussing the application, challenges, and opportunities of the current UAS in the introductory section. We then provide an overview of some of the key deep learning and reinforcement learning techniques discussed throughout this chapter. A key area of focus that will be essential to enable autonomy to UAS is computer vision. Accordingly, we discuss how deep learning approaches have been used to accomplish some of the basic tasks that contribute to providing UAS autonomy. Then we discuss how reinforcement learning is explored for using this information to provide autonomous control and navigation for UAS. Next, we provide the reader with directions to choose appropriate simulation suites and hardware platforms that will help to rapidly prototype novel machine learning based solutions for UAS. We additionally discuss the open problems and challenges pertaining to each aspect of developing autonomous UAS solutions to shine light on potential research areas.",0
"Unmanned Aerial Systems (UAS) are being increasingly utilized for a variety of applications, including commercial, civilian, and military. However, current UAS technology is limited by the need for a remote human controller with reliable wireless connections to perform certain tasks. This lack of autonomy restricts the range of applications and tasks for which UAS can be deployed. To overcome this limitation and improve safety and efficiency, it is important to enable autonomy and intelligence in UAS. With the exponential growth of computing resources and data availability, machine learning has seen a resurgence in recent years. This chapter explores how advances in machine learning, specifically deep learning and reinforcement learning, can be utilized to develop the next generation of autonomous UAS. The chapter begins by discussing the current state of UAS, its challenges, and opportunities. It then provides an overview of key machine learning techniques and their application to UAS, with a focus on computer vision. The chapter then discusses how reinforcement learning can be used for autonomous control and navigation of UAS. The reader is provided with guidance on selecting appropriate simulation suites and hardware platforms for rapid prototyping of machine learning-based solutions. Finally, open problems and challenges are discussed to highlight potential research areas for the development of autonomous UAS solutions.",1
"Agent-based methods allow for defining simple rules that generate complex group behaviors. The governing rules of such models are typically set a priori and parameters are tuned from observed behavior trajectories. Instead of making simplifying assumptions across all anticipated scenarios, inverse reinforcement learning provides inference on the short-term (local) rules governing long term behavior policies by using properties of a Markov decision process. We use the computationally efficient linearly-solvable Markov decision process to learn the local rules governing collective movement for a simulation of the self propelled-particle (SPP) model and a data application for a captive guppy population. The estimation of the behavioral decision costs is done in a Bayesian framework with basis function smoothing. We recover the true costs in the SPP simulation and find the guppies value collective movement more than targeted movement toward shelter.",0
"By utilizing agent-based methods, it becomes possible to establish uncomplicated regulations that result in intricate group behaviors. The rules that govern these models are typically predetermined, with parameters being adjusted based on observed behavior patterns. In contrast to making generalized assumptions that apply to all possible scenarios, inverse reinforcement learning employs the characteristics of a Markov decision process to make inferences regarding the short-term (local) rules that dictate long-term behavioral policies. For our simulations of the self-propelled particle (SPP) model and our data application involving a captive guppy population, we utilize the computationally efficient linearly-solvable Markov decision process to learn the local rules that guide collective movement. Our estimation of the decision costs associated with these behaviors takes place within a Bayesian framework, utilizing basis function smoothing. Our results show that we were able to accurately determine the true costs in the SPP simulation, and we found that the guppies placed a higher value on collective movement than they did on targeted movement towards shelter.",1
"This paper presents a novel approach for job shop scheduling problems using deep reinforcement learning. To account for the complexity of production environment, we employ graph neural networks to model the various relations within production environments. Furthermore, we cast the JSSP as a distributed optimization problem in which learning agents are individually assigned to resources which allows for higher flexibility with respect to changing production environments. The proposed distributed RL agents used to optimize production schedules for single resources are running together with a co-simulation framework of the production environment to obtain the required amount of data. The approach is applied to a multi-robot environment and a complex production scheduling benchmark environment. The initial results underline the applicability and performance of the proposed method.",0
"A new technique is introduced in this article for addressing job shop scheduling issues through the use of deep reinforcement learning. To handle the intricacies of the production environment, we utilize graph neural networks to represent the various connections within the environment. Additionally, we frame the JSSP as a distributed optimization problem, assigning learning agents to individual resources for greater adaptability in response to changes in the production environment. To gather the necessary data, the distributed RL agents used to optimize production schedules for single resources are run in conjunction with a co-simulation framework of the production environment. The method is tested in both a multi-robot environment and a complicated production scheduling benchmark environment, with preliminary outcomes demonstrating the practicality and efficacy of the proposed approach.",1
"Deep active inference has been proposed as a scalable approach to perception and action that deals with large policy and state spaces. However, current models are limited to fully observable domains. In this paper, we describe a deep active inference model that can learn successful policies directly from high-dimensional sensory inputs. The deep learning architecture optimizes a variant of the expected free energy and encodes the continuous state representation by means of a variational autoencoder. We show, in the OpenAI benchmark, that our approach has comparable or better performance than deep Q-learning, a state-of-the-art deep reinforcement learning algorithm.",0
"A scalable method for perception and action, named deep active inference, has been proposed to manage large policy and state spaces. Nevertheless, existing models are constrained to completely observable domains. In this article, we present a deep active inference model that can acquire effective policies straight from high-dimensional sensory inputs. The deep learning structure optimizes a modified version of the expected free energy and decodes the continuous state representation via a variational autoencoder. Our results in the OpenAI benchmark display that our technique performs similarly or better than deep Q-learning, a state-of-the-art deep reinforcement learning algorithm.",1
"Probabilistic Boolean Networks (PBNs) were introduced as a computational model for the study of complex dynamical systems, such as Gene Regulatory Networks (GRNs). Controllability in this context is the process of making strategic interventions to the state of a network in order to drive it towards some other state that exhibits favourable biological properties. In this paper we study the ability of a Double Deep Q-Network with Prioritized Experience Replay in learning control strategies within a finite number of time steps that drive a PBN towards a target state, typically an attractor. The control method is model-free and does not require knowledge of the network's underlying dynamics, making it suitable for applications where inference of such dynamics is intractable. We present extensive experiment results on two synthetic PBNs and the PBN model constructed directly from gene-expression data of a study on metastatic-melanoma.",0
"The aim of this paper is to investigate the potential of a Double Deep Q-Network with Prioritized Experience Replay to learn control strategies that can guide a Probabilistic Boolean Network (PBN) towards a specific target state. PBNs were originally developed as a computational model for analyzing complex dynamic systems, such as Gene Regulatory Networks (GRNs), and controllability is a crucial process for driving these networks towards states that exhibit favorable biological properties. The proposed control method is model-free and can be applied in scenarios where it is difficult to infer the underlying dynamics of the network. The study includes experiments on two synthetic PBNs and a PBN model derived from gene-expression data of a metastatic-melanoma study.",1
"Gaussian Processes (GPs) are widely employed in control and learning because of their principled treatment of uncertainty. However, tracking uncertainty for iterative, multi-step predictions in general leads to an analytically intractable problem. While approximation methods exist, they do not come with guarantees, making it difficult to estimate their reliability and to trust their predictions. In this work, we derive formal probability error bounds for iterative prediction and planning with GPs. Building on GP properties, we bound the probability that random trajectories lie in specific regions around the predicted values. Namely, given a tolerance $\epsilon > 0 $, we compute regions around the predicted trajectory values, such that GP trajectories are guaranteed to lie inside them with probability at least $1-\epsilon$. We verify experimentally that our method tracks the predictive uncertainty correctly, even when current approximation techniques fail. Furthermore, we show how the proposed bounds can be employed within a safe reinforcement learning framework to verify the safety of candidate control policies, guiding the synthesis of provably safe controllers.",0
"Due to their rigorous handling of uncertainty, Gaussian Processes (GPs) are widely utilized in both control and learning. However, when it comes to iterative, multi-step predictions, tracking uncertainty typically results in a problem that is analytically intractable. Although there are approximation methods available, their reliability cannot be guaranteed, making it challenging to trust their predictions. In this study, we establish formal probability error bounds for iterative prediction and planning using GPs. By leveraging GP properties, we can determine the probability that random trajectories will fall within certain regions surrounding predicted values. Specifically, we compute regions around predicted trajectory values that guarantee GP trajectories will remain within them with a probability of at least $1-\epsilon$ given a tolerance of $\epsilon > 0$. We demonstrate through experimentation that our approach accurately tracks predictive uncertainty, even in cases where existing approximation methods fall short. Additionally, we illustrate how the proposed bounds can be integrated into a safe reinforcement learning framework to verify the safety of potential control policies and guide the development of controllers that are provably safe.",1
"We compare the model-free reinforcement learning with the model-based approaches through the lens of the expressive power of neural networks for policies, $Q$-functions, and dynamics. We show, theoretically and empirically, that even for one-dimensional continuous state space, there are many MDPs whose optimal $Q$-functions and policies are much more complex than the dynamics. We hypothesize many real-world MDPs also have a similar property. For these MDPs, model-based planning is a favorable algorithm, because the resulting policies can approximate the optimal policy significantly better than a neural network parameterization can, and model-free or model-based policy optimization rely on policy parameterization. Motivated by the theory, we apply a simple multi-step model-based bootstrapping planner (BOOTS) to bootstrap a weak $Q$-function into a stronger policy. Empirical results show that applying BOOTS on top of model-based or model-free policy optimization algorithms at the test time improves the performance on MuJoCo benchmark tasks.",0
"By examining the capabilities of neural networks for policies, $Q$-functions, and dynamics, we compare model-free reinforcement learning to model-based approaches. Our findings, both theoretical and empirical, reveal that even in a one-dimensional continuous state space, there exist numerous MDPs with significantly more complex optimal $Q$-functions and policies than dynamics. We contend that many real-world MDPs share this characteristic, favoring model-based planning as a superior algorithm due to its ability to approximate optimal policies more effectively than neural network parameterization, which is relied upon by model-free or model-based policy optimization. Based on this theory, we utilize a straightforward multi-step model-based bootstrapping planner (BOOTS) to strengthen a weak $Q$-function into a more robust policy. Our empirical results demonstrate that implementing BOOTS in conjunction with model-based or model-free policy optimization algorithms during testing enhances performance on MuJoCo benchmark tasks.",1
"Recent years have witnessed the popularity of Graph Neural Networks (GNN) in various scenarios. To obtain optimal data-specific GNN architectures, researchers turn to neural architecture search (NAS) methods, which have made impressive progress in discovering effective architectures in convolutional neural networks. Two preliminary works, GraphNAS and Auto-GNN, have made first attempt to apply NAS methods to GNN. Despite the promising results, there are several drawbacks in expressive capability and search efficiency of GraphNAS and Auto-GNN due to the designed search space. To overcome these drawbacks, we propose the SNAG framework (Simplified Neural Architecture search for Graph neural networks), consisting of a novel search space and a reinforcement learning based search algorithm. Extensive experiments on real-world datasets demonstrate the effectiveness of the SNAG framework compared to human-designed GNNs and NAS methods, including GraphNAS and Auto-GNN.",0
"In recent times, Graph Neural Networks (GNNs) have become popular in various scenarios. Researchers seeking optimal data-specific GNN architectures have adopted neural architecture search (NAS) methods, which have been successful in discovering effective architectures in convolutional neural networks. The first attempts to apply NAS methods to GNN were made by GraphNAS and Auto-GNN. However, these approaches had drawbacks in expressive capability and search efficiency due to their designed search space. To overcome these limitations, we propose a new framework called SNAG (Simplified Neural Architecture search for Graph neural networks), which includes a novel search space and a reinforcement learning-based search algorithm. Our experiments on real-world datasets demonstrate the superior effectiveness of the SNAG framework compared to both human-designed GNNs and NAS methods, including GraphNAS and Auto-GNN.",1
"Continuous control is a widely applicable area of reinforcement learning. The main players of this area are actor-critic methods that utilize policy gradients of neural approximators as a common practice. The focus of our study is to show the characteristics of the actor loss function which is the essential part of the optimization. We exploit low dimensional visualizations of the loss function and provide comparisons for loss landscapes of various algorithms. Furthermore, we apply our approach to multi-store dynamic inventory control, a notoriously difficult problem in supply chain operations, and explore the shape of the loss function associated with the optimal policy. We modelled and solved the problem using reinforcement learning while having a loss landscape in favor of optimality.",0
"Reinforcement learning has a broad range of applications, including continuous control. Actor-critic methods are the primary techniques used in this field, utilizing policy gradients of neural approximators. Our research focuses on the actor loss function, which is a critical component of the optimization process. To do this, we use low-dimensional visualizations of the loss function and compare loss landscapes of different algorithms. Additionally, we apply our approach to multi-store dynamic inventory control, a challenging problem in supply chain operations. We examine the shape of the loss function linked to the optimal policy. Using reinforcement learning, we modeled and solved the problem while ensuring a favorable loss landscape for achieving optimality.",1
"Learning classifier systems (LCSs) are population-based predictive systems that were originally envisioned as agents to act in reinforcement learning (RL) environments. These systems can suffer from population bloat and so are amenable to compaction techniques that try to strike a balance between population size and performance. A well-studied LCS architecture is XCSF, which in the RL setting acts as a Q-function approximator. We apply XCSF to a deterministic and stochastic variant of the FrozenLake8x8 environment from OpenAI Gym, with its performance compared in terms of function approximation error and policy accuracy to the optimal Q-functions and policies produced by solving the environments via dynamic programming. We then introduce a novel compaction algorithm (Greedy Niche Mass Compaction - GNMC) and study its operation on XCSF's trained populations. Results show that given a suitable parametrisation, GNMC preserves or even slightly improves function approximation error while yielding a significant reduction in population size. Reasonable preservation of policy accuracy also occurs, and we link this metric to the commonly used steps-to-goal metric in maze-like environments, illustrating how the metrics are complementary rather than competitive.",0
"Initially designed for use in reinforcement learning (RL) environments, learning classifier systems (LCSs) are predictive systems that rely on populations. However, these populations can become bloated, leading to a decline in performance. To address this issue, compaction techniques have been developed to balance population size and performance. One example of an LCS architecture is XCSF, which acts as a Q-function approximator in RL settings. In this study, we use XCSF to analyze the FrozenLake8x8 environment from OpenAI Gym, comparing its performance in terms of function approximation error and policy accuracy to optimal Q-functions and policies produced through dynamic programming. We also introduce a new compaction algorithm, GNMC, which is applied to XCSF's trained populations. Our results show that, with appropriate parametrization, GNMC can maintain or slightly improve function approximation error while significantly reducing population size. Policy accuracy is reasonably preserved, and we demonstrate how this metric relates to the steps-to-goal metric in maze-like environments, showing that the two metrics complement each other instead of competing.",1
"We investigate the sample efficiency of reinforcement learning in a $\gamma$-discounted infinite-horizon Markov decision process (MDP) with state space $\mathcal{S}$ and action space $\mathcal{A}$, assuming access to a generative model. Despite a number of prior work tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy is yet to be determined. In particular, prior results suffer from a sample size barrier, in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$ (up to some log factor). The current paper overcomes this barrier by certifying the minimax optimality of model-based reinforcement learning as soon as the sample size exceeds the order of $\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo some log factor). More specifically, a perturbed model-based planning algorithm provably finds an $\varepsilon$-optimal policy with an order of $\frac{|\mathcal{S}||\mathcal{A}| }{(1-\gamma)^3\varepsilon^2}\log\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)\varepsilon}$ samples for any $\varepsilon \in (0, \frac{1}{1-\gamma}]$. Along the way, we derive improved (instance-dependent) guarantees for model-based policy evaluation. To the best of our knowledge, this work provides the first minimax-optimal guarantee in a generative model that accommodates the entire range of sample sizes (beyond which finding a meaningful policy is information theoretically impossible).",0
"Our study examines the efficiency of reinforcement learning on a Markov decision process (MDP) with infinite horizons and a discount factor of $\gamma$, where we assume a model is available, and the state and action spaces are denoted $\mathcal{S}$ and $\mathcal{A}$, respectively. Although previous research has addressed this issue, a comprehensive understanding of the trade-offs between sample complexity and statistical accuracy remains elusive. Prior results are constrained by a minimum sample size requirement of at least $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$ (plus a logarithmic factor) for their statistical guarantees to be valid. In contrast, our paper surmounts this limitation by certifying that model-based reinforcement learning achieves minimax optimality when the sample size exceeds roughly $\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo a logarithmic factor). Specifically, we prove that a perturbed model-based planning algorithm can discover an $\varepsilon$-optimal policy with a sample size of approximately $\frac{|\mathcal{S}||\mathcal{A}| }{(1-\gamma)^3\varepsilon^2}\log\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)\varepsilon}$ for any $\varepsilon \in (0, \frac{1}{1-\gamma}]$. Additionally, we establish improved (instance-dependent) guarantees for model-based policy evaluation. To our knowledge, this is the first research to provide a minimax-optimal guarantee in a generative model that accommodates the entire range of sample sizes, beyond which finding a meaningful policy is information theoretically impossible.",1
"Meta learning algorithms have been widely applied in many tasks for efficient learning, such as few-shot image classification and fast reinforcement learning. During meta training, the meta learner develops a common learning strategy, or experience, from a variety of learning tasks. Therefore, during meta test, the meta learner can use the learned strategy to quickly adapt to new tasks even with a few training samples. However, there is still a dark side about meta learning in terms of reliability and robustness. In particular, is meta learning vulnerable to adversarial attacks? In other words, would a well-trained meta learner utilize its learned experience to build wrong or likely useless knowledge, if an adversary unnoticeably manipulates the given training set? Without the understanding of this problem, it is extremely risky to apply meta learning in safety-critical applications. Thus, in this paper, we perform the initial study about adversarial attacks on meta learning under the few-shot classification problem. In particular, we formally define key elements of adversarial attacks unique to meta learning and propose the first attacking algorithm against meta learning under various settings. We evaluate the effectiveness of the proposed attacking strategy as well as the robustness of several representative meta learning algorithms. Experimental results demonstrate that the proposed attacking strategy can easily break the meta learner and meta learning is vulnerable to adversarial attacks. The implementation of the proposed framework will be released upon the acceptance of this paper.",0
"Meta learning algorithms have been widely used in various tasks to enhance learning efficiency, such as few-shot image classification and fast reinforcement learning. During the meta training phase, the meta learner acquires a general learning strategy or experience from multiple learning tasks. Consequently, during the meta test phase, the meta learner can swiftly adapt to new tasks, even with limited training samples, using the learned strategy. However, meta learning has a downside in terms of reliability and robustness. The question remains whether adversarial attacks can compromise the effectiveness of meta learning. In other words, can a well-trained meta learner acquire erroneous or ineffective knowledge if an adversary manipulates the training set undetected? Thus, it is crucial to investigate this issue before applying meta learning in safety-critical applications. This paper undertakes an initial study on adversarial attacks on meta learning, particularly in the few-shot classification problem. We formally define key elements of adversarial attacks specific to meta learning and present the first attacking algorithm against meta learning under various settings. We evaluate the effectiveness of the proposed attacking strategy and the robustness of several representative meta learning algorithms. Our experimental results demonstrate that the proposed attacking strategy can easily compromise the meta learner and that meta learning is vulnerable to adversarial attacks. We will release the implementation of our proposed framework upon the acceptance of this paper.",1
"Motivated by the widespread use of temporal-difference (TD-) and Q-learning algorithms in reinforcement learning, this paper studies a class of biased stochastic approximation (SA) procedures under a mild ""ergodic-like"" assumption on the underlying stochastic noise sequence. Building upon a carefully designed multistep Lyapunov function that looks ahead to several future updates to accommodate the stochastic perturbations (for control of the gradient bias), we prove a general result on the convergence of the iterates, and use it to derive non-asymptotic bounds on the mean-square error in the case of constant stepsizes. This novel looking-ahead viewpoint renders finite-time analysis of biased SA algorithms under a large family of stochastic perturbations possible. For direct comparison with existing contributions, we also demonstrate these bounds by applying them to TD- and Q-learning with linear function approximation, under the practical Markov chain observation model. The resultant finite-time error bound for both the TD- as well as the Q-learning algorithms is the first of its kind, in the sense that it holds i) for the unmodified versions (i.e., without making any modifications to the parameter updates) using even nonlinear function approximators; as well as for Markov chains ii) under general mixing conditions and iii) starting from any initial distribution, at least one of which has to be violated for existing results to be applicable.",0
"In this paper, the focus is on studying a specific type of biased stochastic approximation (SA) procedures in the context of temporal-difference (TD-) and Q-learning algorithms in reinforcement learning. The authors use a multistep Lyapunov function to control the gradient bias caused by stochastic perturbations and prove a general result on the convergence of iterates. They use this result to derive non-asymptotic bounds on the mean-square error for constant stepsizes. This approach allows for finite-time analysis of biased SA algorithms under a range of stochastic perturbations. The authors apply their bounds to TD- and Q-learning with linear function approximation under the Markov chain observation model. Their finite-time error bounds are the first of their kind, as they hold for unmodified versions of the algorithms using nonlinear function approximators and under general mixing conditions starting from any initial distribution. Existing results require at least one of these conditions to be violated.",1
"The ubiquitous adoption of Internet-of-Things (IoT) based applications has resulted in the emergence of the Fog computing paradigm, which allows seamlessly harnessing both mobile-edge and cloud resources. Efficient scheduling of application tasks in such environments is challenging due to constrained resource capabilities, mobility factors in IoT, resource heterogeneity, network hierarchy, and stochastic behaviors. xisting heuristics and Reinforcement Learning based approaches lack generalizability and quick adaptability, thus failing to tackle this problem optimally. They are also unable to utilize the temporal workload patterns and are suitable only for centralized setups. However, Asynchronous-Advantage-Actor-Critic (A3C) learning is known to quickly adapt to dynamic scenarios with less data and Residual Recurrent Neural Network (R2N2) to quickly update model parameters. Thus, we propose an A3C based real-time scheduler for stochastic Edge-Cloud environments allowing decentralized learning, concurrently across multiple agents. We use the R2N2 architecture to capture a large number of host and task parameters together with temporal patterns to provide efficient scheduling decisions. The proposed model is adaptive and able to tune different hyper-parameters based on the application requirements. We explicate our choice of hyper-parameters through sensitivity analysis. The experiments conducted on real-world data set show a significant improvement in terms of energy consumption, response time, Service-Level-Agreement and running cost by 14.4%, 7.74%, 31.9%, and 4.64%, respectively when compared to the state-of-the-art algorithms.",0
"The emergence of Fog computing, a result of the widespread use of IoT-based applications, enables the seamless utilization of mobile-edge and cloud resources. However, scheduling application tasks in such environments is challenging due to resource constraints, IoT mobility factors, heterogeneous resources, network hierarchy, and stochastic behaviors. Existing heuristics and Reinforcement Learning based approaches are not optimal as they lack generalizability and quick adaptability, and cannot utilize temporal workload patterns, making them only suitable for centralized setups. To address this, we propose an A3C-based real-time scheduler for decentralized learning across multiple agents in stochastic Edge-Cloud environments. Our model is adaptive and can tune hyper-parameters based on application requirements, which we explain through sensitivity analysis. We use the R2N2 architecture to capture host and task parameters and temporal patterns for efficient scheduling decisions. Experiments on real-world data show a significant improvement in energy consumption, response time, Service-Level-Agreement, and running costs compared to state-of-the-art algorithms.",1
"A novel approach of applying deep reinforcement learning to an RF pulse design is introduced. This method, which is referred to as DeepRF_SLR, is designed to minimize the peak amplitude or, equivalently, minimize the pulse duration of a multiband refocusing pulse generated by the Shinar Le-Roux (SLR) algorithm. In the method, the root pattern of SLR polynomial, which determines the RF pulse shape, is optimized by iterative applications of deep reinforcement learning and greedy tree search. When tested for the designs of the multiband factors of three and seven RFs, DeepRF_SLR demonstrated improved performance compared to conventional methods, generating shorter duration RF pulses in shorter computational time. In the experiments, the RF pulse from DeepRF_SLR produced a slice profile similar to the minimum-phase SLR RF pulse and the profiles matched to that of the computer simulation. Our approach suggests a new way of designing an RF by applying a machine learning algorithm, demonstrating a machine-designed MRI sequence.",0
"This article introduces a new approach for designing RF pulses using deep reinforcement learning called DeepRF_SLR. The aim of this method is to reduce the peak amplitude or pulse duration of a multiband refocusing pulse produced by the SLR algorithm. The root pattern of the SLR polynomial, which determines the shape of the RF pulse, is optimized using a combination of deep reinforcement learning and greedy tree search. In tests, DeepRF_SLR proved to be more effective than conventional methods, generating shorter RF pulses in less time. The resulting RF pulse produced a slice profile similar to that of the minimum-phase SLR RF pulse and matched the computer simulation. This approach offers a novel way of designing RF pulses using machine learning and demonstrates the potential of machine-designed MRI sequences.",1
"Monocular 3D object detection aims to extract the 3D position and properties of objects from a 2D input image. This is an ill-posed problem with a major difficulty lying in the information loss by depth-agnostic cameras. Conventional approaches sample 3D bounding boxes from the space and infer the relationship between the target object and each of them, however, the probability of effective samples is relatively small in the 3D space. To improve the efficiency of sampling, we propose to start with an initial prediction and refine it gradually towards the ground truth, with only one 3d parameter changed in each step. This requires designing a policy which gets a reward after several steps, and thus we adopt reinforcement learning to optimize it. The proposed framework, Reinforced Axial Refinement Network (RAR-Net), serves as a post-processing stage which can be freely integrated into existing monocular 3D detection methods, and improve the performance on the KITTI dataset with small extra computational costs.",0
"The objective of monocular 3D object detection is to determine the position and characteristics of objects in a 2D image. The challenge posed by this task is that cameras that lack depth perception result in loss of information. Traditional methods involve selecting 3D bounding boxes and analyzing their relationship with the target object, but this approach is limited by the low probability of obtaining effective samples in 3D space. To address this issue, we propose an approach that involves gradually refining an initial prediction towards the ground truth by changing only one 3D parameter in each step. To optimize this process, we use reinforcement learning to develop a policy that receives a reward after several steps. Our proposed Reinforced Axial Refinement Network (RAR-Net) can be integrated into existing monocular 3D detection methods as a post-processing stage, improving performance on the KITTI dataset with minimal computational overhead.",1
"Policies trained via Reinforcement Learning (RL) are often needlessly complex, making them more difficult to analyse and interpret. In a run with $n$ time steps, a policy will decide $n$ times on an action to take, even when only a tiny subset of these decisions deliver value over selecting a simple default action. Given a pre-trained policy, we propose a black-box method based on statistical fault localisation that ranks the states of the environment according to the importance of decisions made in those states. We evaluate our ranking method by creating new, simpler policies by pruning decisions identified as unimportant, and measure the impact on performance. Our experimental results on a diverse set of standard benchmarks (gridworld, CartPole, Atari games) show that in some cases less than half of the decisions made contribute to the expected reward. We furthermore show that the decisions made in the most frequently visited states are not the most important for the expected reward.",0
"Policies developed using Reinforcement Learning (RL) can be excessively complicated, making them challenging to analyze and interpret. During a run of n time steps, a policy will make n decisions on actions to take, even though only a small portion of these decisions yield value over choosing a simple default action. To address this, we suggest a black-box approach that uses statistical fault localization to rank the states of the environment based on the importance of the decisions made in those states using a pre-trained policy. We assess the effectiveness of our ranking method by removing unimportant decisions identified through pruning and measuring the impact on performance. Our experiments on various standard benchmarks (gridworld, CartPole, Atari games) reveal that in some cases, less than half of the decisions made contribute to the expected reward. Additionally, we find that the most frequently visited states do not have the most critical decisions for the expected reward.",1
"Recent successes combine reinforcement learning algorithms and deep neural networks, despite reinforcement learning not being widely applied to robotics and real world scenarios. This can be attributed to the fact that current state-of-the-art, end-to-end reinforcement learning approaches still require thousands or millions of data samples to converge to a satisfactory policy and are subject to catastrophic failures during training. Conversely, in real world scenarios and after just a few data samples, humans are able to either provide demonstrations of the task, intervene to prevent catastrophic actions, or simply evaluate if the policy is performing correctly. This research investigates how to integrate these human interaction modalities to the reinforcement learning loop, increasing sample efficiency and enabling real-time reinforcement learning in robotics and real world scenarios. This novel theoretical foundation is called Cycle-of-Learning, a reference to how different human interaction modalities, namely, task demonstration, intervention, and evaluation, are cycled and combined to reinforcement learning algorithms. Results presented in this work show that the reward signal that is learned based upon human interaction accelerates the rate of learning of reinforcement learning algorithms and that learning from a combination of human demonstrations and interventions is faster and more sample efficient when compared to traditional supervised learning algorithms. Finally, Cycle-of-Learning develops an effective transition between policies learned using human demonstrations and interventions to reinforcement learning. The theoretical foundation developed by this research opens new research paths to human-agent teaming scenarios where autonomous agents are able to learn from human teammates and adapt to mission performance metrics in real-time and in real world scenarios.",0
"Despite the limited application of reinforcement learning in robotics and real world scenarios, recent successes have been achieved by combining reinforcement learning algorithms with deep neural networks. However, end-to-end reinforcement learning approaches currently require thousands or millions of data samples to reach a satisfactory policy and can experience catastrophic failures during training. In contrast, humans can provide task demonstrations, intervene to prevent catastrophic actions, or evaluate policy performance after just a few data samples in real world scenarios. This research explores the integration of these human interaction modalities into the reinforcement learning loop, increasing efficiency and enabling real-time learning. This theoretical foundation, known as Cycle-of-Learning, cycles and combines task demonstration, intervention, and evaluation to reinforce learning algorithms. Results demonstrate that learning from both human demonstrations and interventions is faster and more sample efficient than traditional supervised learning algorithms. Cycle-of-Learning also facilitates an effective transition between policies learned through human demonstrations and interventions to reinforcement learning. The theoretical foundation developed by this research opens new paths to human-agent teaming scenarios where autonomous agents can learn from human teammates and adapt to mission performance metrics in real-time and in real world scenarios.",1
"Spiking neuron networks have been used successfully to solve simple reinforcement learning tasks with continuous action set applying learning rules based on spike-timing-dependent plasticity (STDP). However, most of these models cannot be applied to reinforcement learning tasks with discrete action set since they assume that the selected action is a deterministic function of firing rate of neurons, which is continuous. In this paper, we propose a new STDP-based learning rule for spiking neuron networks which contains feedback modulation. We show that the STDP-based learning rule can be used to solve reinforcement learning tasks with discrete action set at a speed similar to standard reinforcement learning algorithms when applied to the CartPole and LunarLander tasks. Moreover, we demonstrate that the agent is unable to solve these tasks if feedback modulation is omitted from the learning rule. We conclude that feedback modulation allows better credit assignment when only the units contributing to the executed action and TD error participate in learning.",0
"Using spike-timing-dependent plasticity (STDP) learning rules, spiking neuron networks have been successful in solving uncomplicated reinforcement learning tasks with a continuous action set. However, these models are not suitable for tasks with a discrete action set as they assume that the chosen action is a deterministic function of the neurons' firing rate, which is continuous. In this study, a novel STDP-based learning rule with feedback modulation is proposed, which can efficiently solve reinforcement learning tasks with a discrete action set, similar to conventional reinforcement learning algorithms applied to the CartPole and LunarLander tasks. Furthermore, the study demonstrates that the agent cannot solve these tasks without feedback modulation in the learning rule. The study concludes that feedback modulation enhances credit assignment when only the units contributing to the executed action and TD error participate in learning.",1
"Existing algorithms aiming to learn a binary classifier from positive (P) and unlabeled (U) data generally require estimating the class prior or label noises ahead of building a classification model. However, the estimation and classifier learning are normally conducted in a pipeline instead of being jointly optimized. In this paper, we propose to alternatively train the two steps using reinforcement learning. Our proposal adopts a policy network to adaptively make assumptions on the labels of unlabeled data, while a classifier is built upon the output of the policy network and provides rewards to learn a better strategy. The dynamic and interactive training between the policy maker and the classifier can exploit the unlabeled data in a more effective manner and yield a significant improvement on the classification performance. Furthermore, we present two different approaches to represent the actions sampled from the policy. The first approach considers continuous actions as soft labels, while the other uses discrete actions as hard assignment of labels for unlabeled examples.We validate the effectiveness of the proposed method on two benchmark datasets as well as one e-commerce dataset. The result shows the proposed method is able to consistently outperform state-of-the-art methods in various settings.",0
"Typically, algorithms aimed at learning a binary classifier from positive (P) and unlabeled (U) data require the estimation of the class prior or label noises before building the classification model. However, this process is often conducted in a pipeline rather than being jointly optimized. This paper proposes an alternative method that uses reinforcement learning to train the two steps separately. A policy network is used to make assumptions on the labels of unlabeled data, while a classifier is built upon the output of the policy network to learn a better strategy. This dynamic training approach can exploit unlabeled data more effectively and significantly improve classification performance. The paper also presents two different approaches to represent the actions sampled from the policy. The effectiveness of this proposed method is validated on two benchmark datasets and one e-commerce dataset, consistently outperforming state-of-the-art methods in various settings.",1
"With the rise of online e-commerce platforms, more and more customers prefer to shop online. To sell more products, online platforms introduce various modules to recommend items with different properties such as huge discounts. A web page often consists of different independent modules. The ranking policies of these modules are decided by different teams and optimized individually without cooperation, which might result in competition between modules. Thus, the global policy of the whole page could be sub-optimal. In this paper, we propose a novel multi-agent cooperative reinforcement learning approach with the restriction that different modules cannot communicate. Our contributions are three-fold. Firstly, inspired by a solution concept in game theory named correlated equilibrium, we design a signal network to promote cooperation of all modules by generating signals (vectors) for different modules. Secondly, an entropy-regularized version of the signal network is proposed to coordinate agents' exploration of the optimal global policy. Furthermore, experiments based on real-world e-commerce data demonstrate that our algorithm obtains superior performance over baselines.",0
"The increasing popularity of online shopping has resulted in e-commerce platforms introducing various recommendation modules, including those with substantial discounts, to increase sales. However, these modules are often independent, with each team optimizing their ranking policies without collaboration. This can lead to competition between modules and a sub-optimal global policy for the entire web page. To address this, we propose a new approach using multi-agent cooperative reinforcement learning, where modules cannot communicate. Our approach includes a signal network that generates signals for different modules, promoting cooperation inspired by correlated equilibrium in game theory. We also propose an entropy-regularized version of the signal network to coordinate exploration of the optimal global policy. Our experiments using real-world e-commerce data demonstrate superior performance over existing approaches.",1
"Action and observation delays exist prevalently in the real-world cyber-physical systems which may pose challenges in reinforcement learning design. It is particularly an arduous task when handling multi-agent systems where the delay of one agent could spread to other agents. To resolve this problem, this paper proposes a novel framework to deal with delays as well as the non-stationary training issue of multi-agent tasks with model-free deep reinforcement learning. We formally define the Delay-Aware Markov Game that incorporates the delays of all agents in the environment. To solve Delay-Aware Markov Games, we apply centralized training and decentralized execution that allows agents to use extra information to ease the non-stationarity issue of the multi-agent systems during training, without the need of a centralized controller during execution. Experiments are conducted in multi-agent particle environments including cooperative communication, cooperative navigation, and competitive experiments. We also test the proposed algorithm in traffic scenarios that require coordination of all autonomous vehicles to show the practical value of delay-awareness. Results show that the proposed delay-aware multi-agent reinforcement learning algorithm greatly alleviates the performance degradation introduced by delay. Codes and demo videos are available at: https://github.com/baimingc/delay-aware-MARL.",0
"In cyber-physical systems, action and observation delays are common and can create challenges for reinforcement learning design. This is especially true for multi-agent systems where one agent's delay can impact others. To address this problem, a new framework is proposed in this paper that uses model-free deep reinforcement learning to handle delays and non-stationary training for multi-agent tasks. The Delay-Aware Markov Game is defined formally to take into account the delays of all agents in the environment. To solve this game, centralized training and decentralized execution are used, allowing agents to use additional information during training to mitigate non-stationarity without a central controller during execution. The proposed algorithm is tested in various multi-agent particle environments and traffic scenarios, demonstrating its practical value in delay-awareness. The delay-aware multi-agent reinforcement learning algorithm proves effective in mitigating the performance degradation caused by delays. Codes and demo videos are available at: https://github.com/baimingc/delay-aware-MARL.",1
"The domain of Embodied AI, in which agents learn to complete tasks through interaction with their environment from egocentric observations, has experienced substantial growth with the advent of deep reinforcement learning and increased interest from the computer vision, NLP, and robotics communities. This growth has been facilitated by the creation of a large number of simulated environments (such as AI2-THOR, Habitat and CARLA), tasks (like point navigation, instruction following, and embodied question answering), and associated leaderboards. While this diversity has been beneficial and organic, it has also fragmented the community: a huge amount of effort is required to do something as simple as taking a model trained in one environment and testing it in another. This discourages good science. We introduce AllenAct, a modular and flexible learning framework designed with a focus on the unique requirements of Embodied AI research. AllenAct provides first-class support for a growing collection of embodied environments, tasks and algorithms, provides reproductions of state-of-the-art models and includes extensive documentation, tutorials, start-up code, and pre-trained models. We hope that our framework makes Embodied AI more accessible and encourages new researchers to join this exciting area. The framework can be accessed at: https://allenact.org/",0
"The Embodied AI domain has experienced a significant expansion thanks to the increased interest from the computer vision, NLP, and robotics communities, as well as the development of deep reinforcement learning. Embodied AI agents learn to complete tasks by interacting with their surroundings through egocentric observations. This growth has been facilitated by the creation of a multitude of simulated environments, tasks, and leaderboards, such as AI2-THOR, Habitat, and CARLA. However, this diversity has also caused fragmentation within the community, making it challenging to test models trained in one environment in another. To address this issue, we introduce AllenAct, a modular and flexible learning framework designed to meet the unique requirements of Embodied AI research. AllenAct supports a growing collection of embodied environments, tasks, and algorithms, offers reproductions of state-of-the-art models, and includes extensive documentation, tutorials, start-up code, and pre-trained models. Our hope is that AllenAct will make Embodied AI more accessible and encourage new researchers to join this exciting area. To access the framework, please visit: https://allenact.org/",1
"Sparse rewards present a difficult problem in reinforcement learning and may be inevitable in certain domains with complex dynamics such as real-world robotics. Hindsight Experience Replay (HER) is a recent replay memory development that allows agents to learn in sparse settings by altering memories to show them as successful even though they may not be. While, empirically, HER has shown some success, it does not provide guarantees around the makeup of samples drawn from an agent's replay memory. This may result in minibatches that contain only memories with zero-valued rewards or agents learning an undesirable policy that completes HER-adjusted goals instead of the actual goal.   In this paper, we introduce Or Your Money Back (OYMB), a replay memory sampler designed to work with HER. OYMB improves training efficiency in sparse settings by providing a direct interface to the agent's replay memory that allows for control over minibatch makeup, as well as a preferential lookup scheme that prioritizes real-goal memories before HER-adjusted memories. We test our approach on five tasks across three unique environments. Our results show that using HER in combination with OYMB outperforms using HER alone and leads to agents that learn to complete the real goal more quickly.",0
"Reinforcement learning faces a challenging issue when rewards are sparse, especially in complex domains such as real-world robotics. Hindsight Experience Replay (HER) has been developed as a solution to enable agents to learn in sparse settings by modifying memories to appear successful, even if they are not. However, HER lacks guarantees regarding the samples drawn from an agent's replay memory and may cause agents to learn an undesirable policy. To address this problem, we propose a replay memory sampler called Or Your Money Back (OYMB) that works in conjunction with HER to enhance training efficiency in sparse settings. OYMB offers direct control over minibatch makeup and prioritizes real-goal memories over HER-adjusted memories. We test our approach on five tasks across three environments and demonstrate that using HER with OYMB improves performance and enables agents to learn to complete the real goal more quickly.",1
"Meta-learning researchers face two fundamental issues in their empirical work: prototyping and reproducibility. Researchers are prone to make mistakes when prototyping new algorithms and tasks because modern meta-learning methods rely on unconventional functionalities of machine learning frameworks. In turn, reproducing existing results becomes a tedious endeavour -- a situation exacerbated by the lack of standardized implementations and benchmarks. As a result, researchers spend inordinate amounts of time on implementing software rather than understanding and developing new ideas.   This manuscript introduces learn2learn, a library for meta-learning research focused on solving those prototyping and reproducibility issues. learn2learn provides low-level routines common across a wide-range of meta-learning techniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning), and builds standardized interfaces to algorithms and benchmarks on top of them. In releasing learn2learn under a free and open source license, we hope to foster a community around standardized software for meta-learning research.",0
"In their empirical work, meta-learning researchers encounter two primary challenges: prototyping and reproducibility. When developing new algorithms and tasks, researchers may make errors due to the unconventional functionalities of machine learning frameworks utilized by modern meta-learning techniques. This, in turn, makes reproducing existing outcomes a tedious endeavor, further complicated by the absence of standardized implementations and benchmarks. Consequently, researchers spend an excessive amount of time implementing software instead of exploring and generating novel ideas. To address these concerns, this paper presents the learn2learn library, designed specifically for meta-learning research and aimed at resolving the issues of prototyping and reproducibility. Utilizing common low-level routines across a broad range of meta-learning techniques (including meta-descent, meta-reinforcement learning, and few-shot learning), learn2learn creates standardized interfaces to algorithms and benchmarks. By releasing learn2learn under a free and open source license, the authors aim to encourage the development of a community focused on standardized software for meta-learning research.",1
"Recent advances in supervised learning and reinforcement learning have provided new opportunities to apply related methodologies to automated driving. However, there are still challenges to achieve automated driving maneuvers in dynamically changing environments. Supervised learning algorithms such as imitation learning can generalize to new environments by training on a large amount of labeled data, however, it can be often impractical or cost-prohibitive to obtain sufficient data for each new environment. Although reinforcement learning methods can mitigate this data-dependency issue by training the agent in a trial-and-error way, they still need to re-train policies from scratch when adapting to new environments. In this paper, we thus propose a meta reinforcement learning (MRL) method to improve the agent's generalization capabilities to make automated lane-changing maneuvers at different traffic environments, which are formulated as different traffic congestion levels. Specifically, we train the model at light to moderate traffic densities and test it at a new heavy traffic density condition. We use both collision rate and success rate to quantify the safety and effectiveness of the proposed model. A benchmark model is developed based on a pretraining method, which uses the same network structure and training tasks as our proposed model for fair comparison. The simulation results shows that the proposed method achieves an overall success rate up to 20% higher than the benchmark model when it is generalized to the new environment of heavy traffic density. The collision rate is also reduced by up to 18% than the benchmark model. Finally, the proposed model shows more stable and efficient generalization capabilities adapting to the new environment, and it can achieve 100% successful rate and 0% collision rate with only a few steps of gradient updates.",0
"Supervised learning and reinforcement learning have advanced and present new possibilities for automated driving. However, there remain difficulties in executing automated driving maneuvers in environments that frequently change. Although supervised learning algorithms, like imitation learning, can generalize to new environments by training on a vast amount of labeled data, obtaining sufficient data for each new environment may be impractical or costly. Although reinforcement learning methods can mitigate this data-dependency issue by training the agent in a trial-and-error manner, they still need to retrain policies from the beginning when adjusting to new environments. This paper proposes a meta reinforcement learning (MRL) method to enhance the agent's generalization abilities for automated lane-changing maneuvers in various traffic environments. The simulation results demonstrate that the proposed model achieves a higher overall success rate and a reduced collision rate when compared to a benchmark model. The proposed model also shows more efficient generalization capabilities, achieving 100% success and 0% collision rates with only a few gradient updates.",1
"A novel deep multi-agent reinforcement learning framework is proposed to identify and resolve conflicts among a variable number of aircraft in a high-density, stochastic, and dynamic sector. Currently the sector capacity is constrained by human air traffic controller's cognitive limitation. We investigate the feasibility of a new concept (autonomous separation assurance) and a new approach to push the sector capacity above human cognitive limitation. We propose the concept of using distributed vehicle autonomy to ensure separation, instead of a centralized sector air traffic controller. Our proposed framework utilizes Proximal Policy Optimization (PPO) that we modify to incorporate an attention network. This allows the agents to have access to variable aircraft information in the sector in a scalable, efficient approach to achieve high traffic throughput under uncertainty. Agents are trained using a centralized learning, decentralized execution scheme where one neural network is learned and shared by all agents. The proposed framework is validated on three challenging case studies in the BlueSky air traffic control environment. Numerical results show the proposed framework significantly reduces offline training time, increases performance, and results in a more efficient policy.",0
"To overcome the limitations of human air traffic controllers in a highly dynamic and stochastic sector with varying aircraft numbers, a new deep multi-agent reinforcement learning framework has been introduced. The proposed framework aims to achieve autonomous separation assurance, which can surpass the capacity limits of human cognition. Instead of a centralized air traffic controller, a distributed vehicle autonomy approach is utilized to ensure separation. The framework employs Proximal Policy Optimization (PPO) with an attention network to provide scalable and efficient access to variable aircraft information, enabling high traffic throughput under uncertainty. The agents are trained using a centralized learning and decentralized execution scheme with a shared neural network. Three challenging case studies in the BlueSky air traffic control environment were used for validation. The proposed framework reduces offline training time, increases performance, and results in a more efficient policy.",1
"The potential of Reinforcement Learning (RL) has been demonstrated through successful applications to games such as Go and Atari. However, while it is straightforward to evaluate the performance of an RL algorithm in a game setting by simply using it to play the game, evaluation is a major challenge in clinical settings where it could be unsafe to follow RL policies in practice. Thus, understanding sensitivity of RL policies to the host of decisions made during implementation is an important step toward building the type of trust in RL required for eventual clinical uptake. In this work, we perform a sensitivity analysis on a state-of-the-art RL algorithm (Dueling Double Deep Q-Networks)applied to hemodynamic stabilization treatment strategies for septic patients in the ICU. We consider sensitivity of learned policies to input features, embedding model architecture, time discretization, reward function, and random seeds. We find that varying these settings can significantly impact learned policies, which suggests a need for caution when interpreting RL agent output.",0
"Reinforcement Learning (RL) has shown promise with successful applications to games like Go and Atari. However, evaluating the performance of RL algorithms in clinical settings is challenging since following RL policies may not be safe in practice. Therefore, determining the sensitivity of RL policies to implementation decisions is crucial for establishing trust in RL for clinical use. This study analyzes the sensitivity of a state-of-the-art RL algorithm, Dueling Double Deep Q-Networks, applied to hemodynamic stabilization treatment strategies for septic patients in the ICU. Various settings, including input features, embedding model architecture, time discretization, reward function, and random seeds were varied to assess their impact on learned policies. Results indicate that changes to these settings significantly affect learned policies, highlighting the need for caution when interpreting RL agent output.",1
"Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world, undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight the computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks.",0
"Despite the success of Reinforcement Learning (RL) algorithms in simulated domains, their direct application to physical systems is often hindered by hard constraints such as safety or resources. Standard RL incentivizes the agent to explore any behavior that maximizes rewards, but in reality, unintended behavior can harm the system or agent, hampering the learning process. This study proposes a Constrained Markov Decision Process and an on-policy formulation to address learning with constraints. The approach translates cumulative cost constraints into state-based constraints, allowing for safe policy improvement that maximizes returns while ensuring constraint satisfaction at every step. The study provides theoretical guarantees for agent convergence and safety during training and highlights the computational advantages of the approach. The effectiveness of the method is demonstrated on safe navigation tasks and safety-constrained versions of MuJoCo environments, using deep neural networks.",1
"Recent work has shown that the structure of deep convolutional neural networks can be used as a structured image prior for solving various inverse image restoration tasks. Instead of using hand-designed architectures, we propose to search for neural architectures that capture stronger image priors. Building upon a generic U-Net architecture, our core contribution lies in designing new search spaces for (1) an upsampling cell and (2) a pattern of cross-scale residual connections. We search for an improved network by leveraging an existing neural architecture search algorithm (using reinforcement learning with a recurrent neural network controller). We validate the effectiveness of our method via a wide variety of applications, including image restoration, dehazing, image-to-image translation, and matrix factorization. Extensive experimental results show that our algorithm performs favorably against state-of-the-art learning-free approaches and reaches competitive performance with existing learning-based methods in some cases.",0
"Recent research has discovered that the construction of deep convolutional neural networks can be employed as a structured image prior to solve diverse inverse image restoration challenges. Rather than utilizing manually designed architectures, our proposition is to explore neural architectures that capture more robust image priors. We have developed new search spaces for an upsampling cell and a pattern of cross-scale residual connections, building upon a generic U-Net architecture. To discover an enhanced network, we employ an existing neural architecture search algorithm that employs reinforcement learning with a recurrent neural network controller. Our approach has been validated through a wide range of applications, including image restoration, dehazing, image-to-image translation, and matrix factorization. Comprehensive experimental results have revealed that our algorithm performs better than state-of-the-art learning-free approaches and reaches comparable performance with existing learning-based methods in some cases.",1
"Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate fictitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.",0
"Progress in deep reinforcement learning (RL) has been significant in two-player zero-sum games, including Go, Poker, and Starcraft. These games are advantageous for RL methods due to their adversarial nature. However, the real world presents many-agent interactions that are a mixture of competitive and common-interest aspects. Diplomacy, a 7-player board game, is designed to enhance the dilemmas resulting from such interactions and presents simultaneous moves and a large combinatorial action space, posing a challenge for RL algorithms. To address this, we propose an approximate best response operator and a family of policy iteration methods that approximate fictitious play. Our approach outperforms the previous state-of-the-art, and game theoretic equilibrium analysis confirms consistent improvements.",1
"Synthesizing realistic medical images provides a feasible solution to the shortage of training data in deep learning based medical image recognition systems. However, the quality control of synthetic images for data augmentation purposes is under-investigated, and some of the generated images are not realistic and may contain misleading features that distort data distribution when mixed with real images. Thus, the effectiveness of those synthetic images in medical image recognition systems cannot be guaranteed when they are being added randomly without quality assurance. In this work, we propose a reinforcement learning (RL) based synthetic sample selection method that learns to choose synthetic images containing reliable and informative features. A transformer based controller is trained via proximal policy optimization (PPO) using the validation classification accuracy as the reward. The selected images are mixed with the original training data for improved training of image recognition systems. To validate our method, we take the pathology image recognition as an example and conduct extensive experiments on two histopathology image datasets. In experiments on a cervical dataset and a lymph node dataset, the image classification performance is improved by 8.1% and 2.3%, respectively, when utilizing high-quality synthetic images selected by our RL framework. Our proposed synthetic sample selection method is general and has great potential to boost the performance of various medical image recognition systems given limited annotation.",0
"The scarcity of training data in deep learning-based medical image recognition systems can be addressed by generating synthetic medical images that closely resemble real ones. However, the reliability of these synthetic images for data augmentation purposes has not been fully investigated. Some of the generated images may not be realistic and could contain misleading features that distort data distribution when combined with real images. Therefore, the effectiveness of these synthetic images in improving medical image recognition systems cannot be guaranteed without quality assurance. To address this issue, we introduce a reinforcement learning-based synthetic sample selection method that learns to identify synthetic images that contain reliable and informative features. Our method utilizes a transformer-based controller trained using proximal policy optimization, with validation classification accuracy as the reward. The selected synthetic images are then mixed with the original training data to improve the training of medical image recognition systems. Our approach is validated using two histopathology image datasets, where we demonstrate an 8.1% and 2.3% improvement in image classification performance for a cervical and lymph node dataset, respectively. Our proposed synthetic sample selection method is general and has the potential to enhance the performance of various medical image recognition systems with limited annotation.",1
"Value-based methods for reinforcement learning lack generally applicable ways to derive behavior from a value function. Many approaches involve approximate value iteration (e.g., $Q$-learning), and acting greedily with respect to the estimates with an arbitrary degree of entropy to ensure that the state-space is sufficiently explored. Behavior based on explicit greedification assumes that the values reflect those of \textit{some} policy, over which the greedy policy will be an improvement. However, value-iteration can produce value functions that do not correspond to \textit{any} policy. This is especially relevant in the function-approximation regime, when the true value function can't be perfectly represented. In this work, we explore the use of \textit{inverse policy evaluation}, the process of solving for a likely policy given a value function, for deriving behavior from a value function. We provide theoretical and empirical results to show that inverse policy evaluation, combined with an approximate value iteration algorithm, is a feasible method for value-based control.",0
"There is a lack of generally applicable methods to derive behavior from a value function in value-based reinforcement learning. Many approaches involve approximate value iteration, such as $Q$-learning, and acting greedily based on estimated values with some level of entropy to ensure sufficient exploration of the state-space. However, behavior based on explicit greedification assumes that the values reflect those of at least one policy, which may not always be the case, especially in the function-approximation regime. In this study, we propose the use of inverse policy evaluation to derive behavior from a value function. This involves solving for a likely policy given a value function and combining it with an approximate value iteration algorithm. Our theoretical and empirical results demonstrate that this method is a feasible approach for value-based control.",1
"In mobile crowdsourcing (MCS), the platform selects participants to complete location-aware tasks from the recruiters aiming to achieve multiple goals (e.g., profit maximization, energy efficiency, and fairness). However, different MCS systems have different goals and there are possibly conflicting goals even in one MCS system. Therefore, it is crucial to design a participant selection algorithm that applies to different MCS systems to achieve multiple goals. To deal with this issue, we formulate the participant selection problem as a reinforcement learning problem and propose to solve it with a novel method, which we call auxiliary-task based deep reinforcement learning (ADRL). We use transformers to extract representations from the context of the MCS system and a pointer network to deal with the combinatorial optimization problem. To improve the sample efficiency, we adopt an auxiliary-task training process that trains the network to predict the imminent tasks from the recruiters, which facilitates the embedding learning of the deep learning model. Additionally, we release a simulated environment on a specific MCS task, the ride-sharing task, and conduct extensive performance evaluations in this environment. The experimental results demonstrate that ADRL outperforms and improves sample efficiency over other well-recognized baselines in various settings.",0
"The platform in mobile crowdsourcing (MCS) selects participants from recruiters who have multiple goals such as profit maximization, energy efficiency, and fairness. However, different MCS systems may have conflicting goals, which makes it important to design a participant selection algorithm that can work for various MCS systems and achieve multiple goals. To address this issue, we propose a new method called auxiliary-task based deep reinforcement learning (ADRL) that formulates the participant selection problem as a reinforcement learning problem. We use transformers to extract representations from the MCS system's context and a pointer network to solve the combinatorial optimization problem. To improve sample efficiency, we use an auxiliary-task training process that trains the network to predict upcoming tasks from recruiters, which facilitates the deep learning model's embedding learning. We also provide a simulated environment for the ride-sharing task and conduct extensive performance evaluations. The experimental results show that ADRL outperforms other baselines and improves sample efficiency in various settings.",1
"Hierarchical models for deep reinforcement learning (RL) have emerged as powerful methods for generating meaningful control strategies in difficult long time horizon tasks. Training of said hierarchical models, however, continue to suffer from instabilities that limit their applicability. In this paper, we address instabilities that arise from the concurrent optimization of goal-assignment and goal-achievement policies. Drawing connections between this concurrent optimization scheme and communication and cooperation in multi-agent RL, we redefine the standard optimization procedure to explicitly promote cooperation between these disparate tasks. Our method is demonstrated to achieve superior results to existing techniques in a set of difficult long time horizon tasks, and serves to expand the scope of solvable tasks by hierarchical reinforcement learning. Videos of the results are available at: https://sites.google.com/berkeley.edu/cooperative-hrl.",0
"Deep reinforcement learning has introduced hierarchical models that are effective in generating control strategies for challenging long-term tasks. The training of these models, however, is plagued by instabilities that limit their application. This study addresses the instability issue that arises from the concurrent optimization of goal-assignment and goal-achievement policies. By drawing connections between this and cooperation in multi-agent RL, the standard optimization process is redefined to promote cooperation between these tasks. The results demonstrate that our approach outperforms existing methods in complex long-term tasks, expanding the range of solvable tasks in hierarchical reinforcement learning. Watch videos of the results at https://sites.google.com/berkeley.edu/cooperative-hrl.",1
"This paper aims to establish an entropy-regularized value-based reinforcement learning method that can ensure the monotonic improvement of policies at each policy update. Unlike previously proposed lower-bounds on policy improvement in general infinite-horizon MDPs, we derive an entropy-regularization aware lower bound. Since our bound only requires the expected policy advantage function to be estimated, it is scalable to large-scale (continuous) state-space problems. We propose a novel reinforcement learning algorithm that exploits this lower-bound as a criterion for adjusting the degree of a policy update for alleviating policy oscillation. We demonstrate the effectiveness of our approach in both discrete-state maze and continuous-state inverted pendulum tasks using a linear function approximator for value estimation.",0
"The goal of this paper is to introduce a method for value-based reinforcement learning that incorporates entropy regularization to ensure policy improvement at each update. Instead of relying on previously suggested lower-bounds for policy improvement in general infinite-horizon MDPs, we have developed a new lower-bound that takes into account entropy regularization. This lower-bound is scalable to large-scale problems as it only requires estimation of the expected policy advantage function. We have created a unique reinforcement learning algorithm that uses this lower-bound as a criterion for adjusting policy updates to reduce policy oscillation. Our approach was tested on both discrete-state maze and continuous-state inverted pendulum tasks using a linear function approximator for value estimation, and proved to be effective.",1
"Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from \emph{raw} video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.",0
"Although adversarial learning methods have been proposed for various applications, the instability of adversarial model training is a known issue. The key to achieving effective performance in these models is to balance the generator and discriminator, as a highly accurate discriminator produces less informative gradients. To resolve this issue, we present a straightforward and universal approach to limiting information flow in the discriminator using an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can regulate the accuracy of the discriminator and maintain informative gradients. Our proposed method, the variational discriminator bottleneck (VDB), is shown to be highly effective in three distinct areas of adversarial learning algorithms. Our primary evaluation focuses on the VDB's ability to learn dynamic continuous control skills (e.g., running) from raw video demonstrations, outperforming previous adversarial imitation learning methods. Additionally, the VDB can be combined with adversarial inverse reinforcement learning to develop parsimonious reward functions that can be adapted and optimized for new settings. Lastly, we demonstrate that the VDB can train GANs more effectively for image generation, surpassing several previous stabilization methods.",1
"Dynamic dispatching is one of the core problems for operation optimization in traditional industries such as mining, as it is about how to smartly allocate the right resources to the right place at the right time. Conventionally, the industry relies on heuristics or even human intuitions which are often short-sighted and sub-optimal solutions. Leveraging the power of AI and Internet of Things (IoT), data-driven automation is reshaping this area. However, facing its own challenges such as large-scale and heterogenous trucks running in a highly dynamic environment, it can barely adopt methods developed in other domains (e.g., ride-sharing). In this paper, we propose a novel Deep Reinforcement Learning approach to solve the dynamic dispatching problem in mining. We first develop an event-based mining simulator with parameters calibrated in real mines. Then we propose an experience-sharing Deep Q Network with a novel abstract state/action representation to learn memories from heterogeneous agents altogether and realizes learning in a centralized way. We demonstrate that the proposed methods significantly outperform the most widely adopted approaches in the industry by $5.56\%$ in terms of productivity. The proposed approach has great potential in a broader range of industries (e.g., manufacturing, logistics) which have a large-scale of heterogenous equipment working in a highly dynamic environment, as a general framework for dynamic resource allocation.",0
"In traditional industries like mining, dynamic dispatching presents a challenge for optimizing operations. The main issue is how to allocate resources efficiently, which has traditionally relied on heuristics and human intuition, resulting in sub-optimal solutions. With the advent of AI and IoT, data-driven automation is transforming this area. However, given the unique challenges of mining, such as large-scale and heterogeneous trucks operating in a dynamic environment, existing methods from other domains like ride-sharing are not suitable. To address this, we present a new Deep Reinforcement Learning approach to solve the dynamic dispatching problem in mining. We develop an event-based mining simulator and propose an experience-sharing Deep Q Network with a novel abstract state/action representation to learn memories from heterogeneous agents in a centralized way. Our methods outperform the most widely adopted approaches in the industry by 5.56% in terms of productivity. This approach has the potential to be applied in other industries, such as manufacturing and logistics, with large-scale and heterogeneous equipment operating in a dynamic environment. It can serve as a general framework for dynamic resource allocation.",1
"We develop a generic data-driven method for estimator selection in off-policy policy evaluation settings. We establish a strong performance guarantee for the method, showing that it is competitive with the oracle estimator, up to a constant factor. Via in-depth case studies in contextual bandits and reinforcement learning, we demonstrate the generality and applicability of the method. We also perform comprehensive experiments, demonstrating the empirical efficacy of our approach and comparing with related approaches. In both case studies, our method compares favorably with existing methods.",0
"Our research introduces a data-driven approach for selecting estimators in off-policy policy evaluation scenarios. We prove that this method performs well and can compete with the oracle estimator, with only a constant difference. To showcase the versatility of our approach, we conduct extensive case studies in both reinforcement learning and contextual bandits. Additionally, we conduct comprehensive experiments to demonstrate the effectiveness of our approach, and compare it with other related methods. In both case studies, our method outperforms the existing methods.",1
"Unsupervised domain adaptation without consuming annotation process for unlabeled target data attracts appealing interests in semantic segmentation. However, 1) existing methods neglect that not all semantic representations across domains are transferable, which cripples domain-wise transfer with untransferable knowledge; 2) they fail to narrow category-wise distribution shift due to category-agnostic feature alignment. To address above challenges, we develop a new Critical Semantic-Consistent Learning (CSCL) model, which mitigates the discrepancy of both domain-wise and category-wise distributions. Specifically, a critical transfer based adversarial framework is designed to highlight transferable domain-wise knowledge while neglecting untransferable knowledge. Transferability-critic guides transferability-quantizer to maximize positive transfer gain under reinforcement learning manner, although negative transfer of untransferable knowledge occurs. Meanwhile, with the help of confidence-guided pseudo labels generator of target samples, a symmetric soft divergence loss is presented to explore inter-class relationships and facilitate category-wise distribution alignment. Experiments on several datasets demonstrate the superiority of our model.",0
"Semantic segmentation is a field that is interested in unsupervised domain adaptation without annotation for unlabeled target data. However, current methods do not consider that not all semantic representations can be transferred across domains, which limits the domain-wise transfer of knowledge. Additionally, these methods fail to address category-wise distribution shifts due to category-agnostic feature alignment. To overcome these challenges, we propose a new approach called Critical Semantic-Consistent Learning (CSCL) that mitigates both domain-wise and category-wise distribution discrepancies. Our approach employs a critical transfer-based adversarial framework to highlight transferable domain-wise knowledge and ignore untransferable knowledge. Additionally, we use a transferability-critic to guide transferability-quantizer to maximize positive transfer gain while minimizing negative transfer of untransferable knowledge. We also use a symmetric soft divergence loss to explore inter-class relationships and align category-wise distribution. Our experiments show that our model outperforms existing methods on several datasets.",1
"We propose Improved Memories Learning (IMeL), a novel algorithm that turns reinforcement learning (RL) into a supervised learning (SL) problem and delimits the role of neural networks (NN) to interpolation. IMeL consists of two components. The first is a reservoir of experiences. Each experience is updated based on a non-parametric procedural improvement of the policy, computed as a bounded one-sample Monte Carlo estimate. The second is a NN regressor, which receives as input improved experiences from the reservoir (context points) and computes the policy by interpolation. The NN learns to measure the similarity between states in order to compute long-term forecasts by averaging experiences, rather than by encoding the problem structure in the NN parameters. We present preliminary results and propose IMeL as a baseline method for assessing the merits of more complex models and inductive biases.",0
"A new algorithm called Improved Memories Learning (IMeL) is suggested to convert reinforcement learning (RL) into a supervised learning (SL) problem, using neural networks (NN) for interpolation. IMeL comprises of two elements: a reservoir of experiences that is updated using a Monte Carlo estimate to improve the policy, and a NN regressor that uses the updated experiences to interpolate and compute the policy. The NN is trained to measure the similarity between states and forecast long-term outcomes by averaging experiences, instead of encoding the problem structure in the NN parameters. IMeL is proposed as a benchmark for evaluating more sophisticated models and inductive biases, and preliminary results are presented.",1
"We prove performance guarantees of two algorithms for approximating $Q^\star$ in batch reinforcement learning. Compared to classical iterative methods such as Fitted Q-Iteration---whose performance loss incurs quadratic dependence on horizon---these methods estimate (some forms of) the Bellman error and enjoy linear-in-horizon error propagation, a property established for the first time for algorithms that rely solely on batch data and output stationary policies. One of the algorithms uses a novel and explicit importance-weighting correction to overcome the infamous ""double sampling"" difficulty in Bellman error estimation, and does not use any squared losses. Our analyses reveal its distinct characteristics and potential advantages compared to classical algorithms.",0
"We demonstrate the effectiveness of two algorithms in approximating $Q^\star$ in batch reinforcement learning by providing performance guarantees. These algorithms differ from traditional iterative methods such as Fitted Q-Iteration, which suffer from a quadratic dependence on horizon resulting in significant performance loss. Instead, our methods estimate the Bellman error and exhibit linear-in-horizon error propagation, a unique feature among algorithms that rely solely on batch data and output stationary policies. One of our algorithms utilizes a novel and explicit importance-weighting correction to overcome the ""double sampling"" problem associated with Bellman error estimation, and avoids the use of squared losses. Our analyses highlight the distinct characteristics and potential benefits of this algorithm compared to traditional approaches.",1
"Temporal-Difference (TD) learning with nonlinear smooth function approximation for policy evaluation has achieved great success in modern reinforcement learning. It is shown that such a problem can be reformulated as a stochastic nonconvex-strongly-concave optimization problem, which is challenging as naive stochastic gradient descent-ascent algorithm suffers from slow convergence. Existing approaches for this problem are based on two-timescale or double-loop stochastic gradient algorithms, which may also require sampling large-batch data. However, in practice, a single-timescale single-loop stochastic algorithm is preferred due to its simplicity and also because its step-size is easier to tune. In this paper, we propose two single-timescale single-loop algorithms which require only one data point each step. Our first algorithm implements momentum updates on both primal and dual variables achieving an $O(\varepsilon^{-4})$ sample complexity, which shows the important role of momentum in obtaining a single-timescale algorithm. Our second algorithm improves upon the first one by applying variance reduction on top of momentum, which matches the best known $O(\varepsilon^{-3})$ sample complexity in existing works. Furthermore, our variance-reduction algorithm does not require a large-batch checkpoint. Moreover, our theoretical results for both algorithms are expressed in a tighter form of simultaneous primal and dual side convergence.",0
"Modern reinforcement learning has experienced significant success with Temporal-Difference (TD) learning utilizing nonlinear smooth function approximation for policy evaluation. However, this method presents a challenging stochastic nonconvex-strongly-concave optimization problem, which is complicated by the slow convergence of naive stochastic gradient descent-ascent algorithms. Existing approaches rely on two-timescale or double-loop stochastic gradient algorithms that often require large-batch data sampling. Despite the preference for single-timescale single-loop stochastic algorithms in practice due to their simplicity and ease of tuning step-size, they have not been able to match the sample complexity of existing methods. This paper addresses this challenge by proposing two single-timescale single-loop algorithms that only require one data point per step. The first algorithm employs momentum updates on both primal and dual variables, achieving an $O(\varepsilon^{-4})$ sample complexity. The second algorithm improves on the first by incorporating variance reduction on top of momentum, matching the best known $O(\varepsilon^{-3})$ sample complexity in existing works without requiring a large-batch checkpoint. Additionally, our theoretical results demonstrate simultaneous primal and dual side convergence in a tighter form for both algorithms.",1
"A large portion of passenger requests is reportedly unserviced, partially due to vacant for-hire drivers' cruising behavior during the passenger seeking process. This paper aims to model the multi-driver repositioning task through a mean field multi-agent reinforcement learning (MARL) approach that captures competition among multiple agents. Because the direct application of MARL to the multi-driver system under a given reward mechanism will likely yield a suboptimal equilibrium due to the selfishness of drivers, this study proposes a reward design scheme with which a more desired equilibrium can be reached. To effectively solve the bilevel optimization problem with upper level as the reward design and the lower level as a multi-agent system, a Bayesian optimization (BO) algorithm is adopted to speed up the learning process. We then apply the bilevel optimization model to two case studies, namely, e-hailing driver repositioning under service charge and multiclass taxi driver repositioning under NYC congestion pricing. In the first case study, the model is validated by the agreement between the derived optimal control from BO and that from an analytical solution. With a simple piecewise linear service charge, the objective of the e-hailing platform can be increased by 8.4%. In the second case study, an optimal toll charge of $5.1 is solved using BO, which improves the objective of city planners by 7.9%, compared to that without any toll charge. Under this optimal toll charge, the number of taxis in the NYC central business district is decreased, indicating a better traffic condition, without substantially increasing the crowdedness of the subway system.",0
"There are many passenger requests that go unfulfilled because for-hire drivers tend to cruise around instead of actively seeking passengers. This study presents a mean field multi-agent reinforcement learning (MARL) approach to model the multi-driver repositioning task and account for competition among drivers. However, the direct application of MARL may not yield an optimal equilibrium due to drivers' selfishness, so a reward design scheme is proposed to reach a more desirable equilibrium. To solve the bilevel optimization problem with reward design as the upper level and the multi-agent system as the lower level, a Bayesian optimization (BO) algorithm is utilized. Two case studies are conducted, one on e-hailing driver repositioning and the other on multiclass taxi driver repositioning under NYC congestion pricing. The models are validated using BO and analytical solutions, respectively. The results show that the proposed models can improve the objectives of e-hailing platforms and city planners while reducing traffic congestion in the NYC central business district without significantly increasing subway crowdedness.",1
"Inverse Distance Weighted models (IDW) have been widely used for predicting and modeling multidimensional space in multimodal industrial processes. However, the more complex the structure of multidimensional space, the lower the performance of IDW models, and real industrial datasets tend to have more complex spatial structure. To solve this problem, a new framework for spatial prediction and modeling based on deep reinforcement learning network is proposed. In the proposed framework, the internal relationship between state and action is enhanced by reusing the state values in the Q network, and the convergence rate and stability of the deep reinforcement learning network are improved. The improved deep reinforcement learning network is then used to search for and learn the hyperparameters of each sample point in the inverse distance weighted model. These hyperparameters can reflect the spatial structure of the current industrial dataset to some extent. Then a spatial distribution of hyperparameters is constructed based on the learned hyperparameters. Each interpolation point obtains corresponding hyperparameters from the hyperparametric spatial distribution and brings them into the classical IDW models for prediction, thus achieving differential spatial prediction and modeling. The simulation results show that the proposed framework is suitable for real industrial datasets with complex spatial structure characteristics and is more accurate than current IDW models in spatial prediction.",0
"The use of Inverse Distance Weighted models (IDW) is prevalent in the prediction and modeling of multidimensional space in multimodal industrial processes. However, as the complexity of the spatial structure increases, the performance of IDW models decreases, and real industrial datasets tend to have more intricate spatial structures. To address this issue, a novel framework for spatial prediction and modeling is proposed, which is based on a deep reinforcement learning network. The proposed framework enhances the internal relationship between state and action by reusing the state values in the Q network, which improves the convergence rate and stability of the deep reinforcement learning network. The improved deep reinforcement learning network is then utilized to search for and learn the hyperparameters of each sample point in the inverse distance weighted model. These hyperparameters reflect the spatial structure of the current industrial dataset. A spatial distribution of hyperparameters is then constructed based on the learned hyperparameters, and each interpolation point obtains corresponding hyperparameters from the hyperparametric spatial distribution. These hyperparameters are then used in the classical IDW models for prediction, resulting in differential spatial prediction and modeling. Simulation results demonstrate that the proposed framework is suitable for real industrial datasets with complex spatial structure characteristics and achieves greater accuracy in spatial prediction than current IDW models.",1
"Learning about many things can provide numerous benefits to a reinforcement learning system. For example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. The question we tackle in this paper is how to sculpt the stream of experience---how to adapt the learning system's behavior---to optimize the learning of a collection of value functions. A simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. Unfortunately, implementing this simple idea has proven difficult, and thus has been the focus of decades of study. It remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. In this paper, we investigate and compare different intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. We discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. We provide a comprehensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement learning and active learning. Our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behavior, if each individual learner is introspective.",0
"A reinforcement learning system can benefit greatly from learning about a variety of subjects. For instance, incorporating multiple auxiliary value functions into the system can enhance exploration and representation learning in addition to optimizing environmental rewards. The aim of this paper is to explore how to customize the learning system's behavior to optimize the learning of a collection of value functions. One solution is to calculate an intrinsic reward based on the statistics of each auxiliary learner and use reinforcement learning to maximize it. However, implementing this solution has been challenging and the subject of study for decades. It is unclear which measures of learning would be effective for parallel learning in situations where environmental rewards are sparse or non-existent. In this paper, we compare and investigate various intrinsic reward mechanisms using a new bandit-like parallel-learning testbed. We emphasize the significance of introspective prediction learners that increase their learning rate when progress is possible and decrease it when it is not. We conduct a comprehensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement and active learning. Our findings reveal that intrinsic rewards based on the amount of learning can produce useful behavior if each individual learner is introspective.",1
"Episodic control provides a highly sample-efficient method for reinforcement learning while enforcing high memory and computational requirements. This work proposes a simple heuristic for reducing these requirements, and an application to Model-Free Episodic Control (MFEC) is presented. Experiments on Atari games show that this heuristic successfully reduces MFEC computational demands while producing no significant loss of performance when conservative choices of hyperparameters are used. Consequently, episodic control becomes a more feasible option when dealing with reinforcement learning tasks.",0
"The use of episodic control in reinforcement learning is efficient in terms of sample size, but it requires significant memory and computational resources. To address this issue, this study suggests a straightforward approach to decrease these requirements, with a focus on Model-Free Episodic Control (MFEC). The results of experiments conducted on Atari games demonstrate that this approach reduces the computational demands of MFEC without significantly compromising performance, especially when conservative hyperparameters are employed. As a result, using episodic control in reinforcement learning tasks becomes a more practical choice.",1
"This paper highlights several properties of large urban networks that can have an impact on machine learning methods applied to traffic signal control. In particular, we show that the average network flow tends to be independent of the signal control policy as density increases. This property, which so far has remained under the radar, implies that deep reinforcement learning (DRL) methods becomes ineffective when trained under congested conditions, and might explain DRL's limited success for traffic signal control. Our results apply to all possible grid networks thanks to a parametrization based on two network parameters: the ratio of the expected distance between consecutive traffic lights to the expected green time, and the turning probability at intersections. Networks with different parameters exhibit very different responses to traffic signal control. Notably, we found that no control (i.e. random policy) can be an effective control strategy for a surprisingly large family of networks. The impact of the turning probability turned out to be very significant both for baseline and for DRL policies. It also explains the loss of symmetry observed for these policies, which is not captured by existing theories that rely on corridor approximations without turns. Our findings also suggest that supervised learning methods have enormous potential as they require very little examples to produce excellent policies.",0
"This article discusses how certain characteristics of large urban networks can affect the effectiveness of machine learning techniques used for traffic signal control. Specifically, it demonstrates that as network density increases, the average flow of traffic tends to be unaffected by signal control policies. This previously overlooked property suggests that deep reinforcement learning (DRL) methods may not perform well when trained to deal with congestion, which may explain why they have not been very successful for traffic signal control. The study's findings apply to all kinds of grid networks and are based on two key network parameters: the ratio of the expected distance between consecutive traffic lights to the expected green time and the likelihood of turns at intersections. Networks with different parameters respond differently to traffic signal control, and the article reveals that a random policy can be a surprisingly effective control strategy for a large family of networks. The study also highlights the significant impact of turning probability on both baseline and DRL policies and suggests that supervised learning methods could be highly effective with minimal examples. Finally, the research explains the asymmetry observed in these policies, which is not accounted for in existing theories that rely on corridor approximations without turns.",1
"Moving around in the world is naturally a multisensory experience, but today's embodied agents are deaf---restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to a sounding object. We propose a multi-modal deep reinforcement learning approach to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces, and our work lays groundwork for new research in embodied AI with audio-visual perception.",0
"Natural movement in the world involves using all our senses, but current agents are limited to only their visual perception. To address this, we present a method for audio-visual navigation in realistic 3D environments. Our approach utilizes deep reinforcement learning to train agents to navigate using both visual and auditory information. This enables agents to discover the geometry of the space through sound and follow sound-emitting targets. To support this method, we introduce SoundSpaces, a unique dataset of audio renderings based on geometrical acoustic simulations for two publicly available 3D environments, and we modify Habitat to support this new sensor. Our results demonstrate that incorporating audio significantly improves visual navigation in 3D spaces, and we believe this work opens up new avenues for research in embodied AI with audio-visual perception.",1
"Developing agents that can perform challenging complex tasks is the goal of reinforcement learning. The model-free reinforcement learning has been considered as a feasible solution. However, the state of the art research has been to develop increasingly complicated techniques. This increasing complexity makes the reconstruction difficult. Furthermore, the problem of reward dependency is still exists. As a result, research on imitation learning, which learns policy from a demonstration of experts, has begun to attract attention. Imitation learning directly learns policy based on data on the behavior of the experts without the explicit reward signal provided by the environment. However, imitation learning tries to optimize policies based on deep reinforcement learning such as trust region policy optimization. As a result, deep reinforcement learning based imitation learning also poses a crisis of reproducibility. The issue of complex model-free model has received considerable critical attention. A derivative-free optimization based reinforcement learning and the simplification on policies obtain competitive performance on the dynamic complex tasks. The simplified policies and derivative free methods make algorithm be simple. The reconfiguration of research demo becomes easy. In this paper, we propose an imitation learning method that takes advantage of the derivative-free optimization with simple linear policies. The proposed method performs simple random search in the parameter space of policies and shows computational efficiency. Experiments in this paper show that the proposed model, without a direct reward signal from the environment, obtains competitive performance on the MuJoCo locomotion tasks.",0
"The objective of reinforcement learning is to develop agents that can tackle challenging complex tasks. While model-free reinforcement learning has been deemed a possible solution, recent research has focused on developing more intricate techniques. This has made reconstruction a difficult task, and the problem of reward dependency still persists. As a result, imitation learning has emerged as an alternative approach that learns policy from expert demonstrations without the explicit reward signal provided by the environment. However, even deep reinforcement learning-based imitation learning poses a challenge to reproducibility. To address this issue, researchers have explored derivative-free optimization-based reinforcement learning and simplified policies to achieve competitive performance on dynamic complex tasks. In this paper, we propose an imitation learning method that leverages derivative-free optimization with simple linear policies. Our proposed method employs simple random search in the parameter space of policies and demonstrates computational efficiency. Our experiments show that the proposed model obtains competitive performance on MuJoCo locomotion tasks without a direct reward signal from the environment.",1
"We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments.",0
"The issue of Online Continual Compression is explored in this study, which involves compressing and storing a representative dataset from a non i.i.d data stream while only observing each sample once. Applying auto-encoders in this context presents a significant obstacle, as representations derived from earlier encoder states must be compatible with later decoder states. To overcome this challenge, discrete auto-encoders are used, and Adaptive Quantization Modules (AQM) are introduced to regulate the compression ability of the module during the learning process. This enables the selection of an appropriate compression for incoming samples, factoring in memory constraints and the current state of the learned compression. Notably, our approach does not require pretraining, even on complex datasets, and outperforms previous methods in continual learning benchmarks. The effectiveness of AQM is demonstrated on larger images, LiDAR, and reinforcement learning environments.",1
"Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods. Project page: https://nv-tlabs.github.io/meta-sim-structure/.",0
"Procedural models are extensively utilized in generating scenes for graphics, gaming, and creating labeled synthetic datasets for ML. Skilled experts must carefully adjust numerous parameters to produce a diverse and realistic scene. These parameters govern the structure of the generated scenes, such as the number of cars, and ensure objects are placed in valid configurations. Meta-Sim aims to automatically fine-tune parameters unsupervised by using a target collection of real images. In Meta-Sim2, we aim to solve the challenging problem of learning the scene structure in addition to parameters. We use Reinforcement Learning to train our model due to the discrete nature of the problem and design a feature space divergence to ensure successful training. Our experiments on a real driving dataset demonstrate that we can learn to generate data that captures the discrete structural statistics of objects without any supervision. Additionally, our generated dataset leads to improved object detector performance compared to other baseline simulation methods. Visit our project page at https://nv-tlabs.github.io/meta-sim-structure/.",1
"Ensemble models are powerful model building tools that are developed with a focus to improve the accuracy of model predictions. They find applications in time series forecasting in varied scenarios including but not limited to process industries, health care, and economics where a single model might not provide optimal performance. It is known that if models selected for data modelling are distinct (linear/non-linear, static/dynamic) and independent (minimally correlated models), the accuracy of the predictions is improved. Various approaches suggested in the literature to weigh the ensemble models use a static set of weights. Due to this limitation, approaches using a static set of weights for weighing ensemble models cannot capture the dynamic changes or local features of the data effectively. To address this issue, a Reinforcement Learning (RL) approach to dynamically assign and update weights of each of the models at different time instants depending on the nature of data and the individual model predictions is proposed in this work. The RL method implemented online, essentially learns to update the weights and reduce the errors as the time progresses. Simulation studies on time series data showed that the dynamic weighted approach using RL learns the weight better than existing approaches. The accuracy of the proposed method is compared with an existing approach of online Neural Network tuning quantitatively through normalized mean square error(NMSE) values.",0
"Ensemble models are effective tools for building models and improving the accuracy of predictions. They are utilized in various areas such as process industries, health care, and economics where a single model may not perform optimally. To enhance the accuracy of predictions, it is recommended to select distinct and independent models. However, using a static set of weights to weigh ensemble models has limitations as it cannot capture dynamic changes or local features of the data effectively. To address this issue, this work proposes a Reinforcement Learning (RL) approach that dynamically assigns and updates weights for each model based on the nature of the data and individual predictions. The RL method learns and updates weights online to reduce errors as time progresses. Simulation studies on time series data show that the dynamic weighted approach using RL outperforms existing approaches. The proposed method's accuracy is compared quantitatively with an existing approach of online Neural Network tuning using normalized mean square error (NMSE) values.",1
"Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.",0
"A major challenge for large-scale real-world applications of reinforcement learning (RL) is effectively using large, previously collected datasets. Offline RL algorithms offer a solution by allowing effective policies to be learned from static datasets without further interaction. However, offline RL presents significant challenges in practice, particularly with complex and multi-modal data distributions, where standard off-policy RL methods can fail due to overestimation of values caused by the distributional shift between the dataset and the learned policy. To address these limitations, we introduce conservative Q-learning (CQL), which learns a conservative Q-function that lower-bounds the true value of a policy under this Q-function. We prove that CQL produces a lower bound on the value of the current policy and can be integrated into a policy learning procedure with theoretical improvement guarantees. CQL is implemented by augmenting the standard Bellman error objective with a simple Q-value regularizer, which is easy to implement on top of existing deep Q-learning and actor-critic implementations. We demonstrate that CQL outperforms existing offline RL methods on discrete and continuous control domains, particularly when learning from complex and multi-modal data distributions, often achieving policies with 2-5 times higher final return.",1
"This paper investigates the use of Reinforcement Learning for the robust design of low-thrust interplanetary trajectories in presence of severe disturbances, modeled alternatively as Gaussian additive process noise, observation noise, control actuation errors on thrust magnitude and direction, and possibly multiple missed thrust events. The optimal control problem is recast as a time-discrete Markov Decision Process to comply with the standard formulation of reinforcement learning. An open-source implementation of the state-of-the-art algorithm Proximal Policy Optimization is adopted to carry out the training process of a deep neural network, used to map the spacecraft (observed) states to the optimal control policy. The resulting Guidance and Control Network provides both a robust nominal trajectory and the associated closed-loop guidance law. Numerical results are presented for a typical Earth-Mars mission. First, in order to validate the proposed approach, the solution found in a (deterministic) unperturbed scenario is compared with the optimal one provided by an indirect technique. Then, the robustness and optimality of the obtained closed-loop guidance laws is assessed by means of Monte Carlo campaigns performed in the considered uncertain scenarios. These preliminary results open up new horizons for the use of reinforcement learning in the robust design of interplanetary missions.",0
"The aim of this study is to explore the potential of Reinforcement Learning in designing reliable interplanetary trajectories with low-thrust propulsion systems, even when there are severe disturbances. These disturbances can be modeled as Gaussian additive process noise, observation noise, control actuation errors on thrust magnitude and direction, and multiple missed thrust events. To solve this optimal control problem, the study uses a Markov Decision Process that adheres to the standard formulation of reinforcement learning. The Proximal Policy Optimization algorithm is used to implement a deep neural network that maps spacecraft states to an optimal control policy. The resulting Guidance and Control Network generates a robust nominal trajectory and a corresponding closed-loop guidance law. The study presents numerical results for an Earth-Mars mission, where the solution found in an unperturbed scenario is compared to the optimal solution provided by an indirect technique. Monte Carlo campaigns are performed to evaluate the robustness and optimality of the closed-loop guidance laws in uncertain scenarios. These preliminary findings highlight the potential of reinforcement learning in the robust design of interplanetary missions.",1
"Options have been shown to be an effective tool in reinforcement learning, facilitating improved exploration and learning. In this paper, we present an approach based on spectral graph theory and derive an algorithm that systematically discovers options without access to a specific reward or task assignment. As opposed to the common practice used in previous methods, our algorithm makes full use of the spectrum of the graph Laplacian. Incorporating modes associated with higher graph frequencies unravels domain subtleties, which are shown to be useful for option discovery. Using geometric and manifold-based analysis, we present a theoretical justification for the algorithm. In addition, we showcase its performance in several domains, demonstrating clear improvements compared to competing methods.",0
"Reinforcement learning has been found to benefit from the use of options as they aid in better exploration and learning. This research paper introduces a new approach that utilizes spectral graph theory to develop an algorithm that can discover options without the need for a specific reward or task assignment. Unlike other methods that only use a limited number of graph frequencies, our algorithm leverages the full spectrum of the graph Laplacian, which uncovers domain intricacies that can be helpful for option identification. A theoretical explanation for the algorithm is provided via geometric and manifold-based analysis. Furthermore, the algorithm's effectiveness is demonstrated in various environments, with noticeable improvements over other existing methods.",1
"Second-order information has proven to be very effective in determining the redundancy of neural network weights and activations. Recent paper proposes to use Hessian traces of weights and activations for mixed-precision quantization and achieves state-of-the-art results. However, prior works only focus on selecting bits for each layer while the redundancy of different channels within a layer also differ a lot. This is mainly because the complexity of determining bits for each channel is too high for original methods. Here, we introduce Channel-wise Hessian Aware trace-Weighted Quantization (CW-HAWQ). CW-HAWQ uses Hessian trace to determine the relative sensitivity order of different channels of activations and weights. What's more, CW-HAWQ proposes to use deep Reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agent to find the optimal ratios of different quantization bits and assign bits to channels according to the Hessian trace order. The number of states in CW-HAWQ is much smaller compared with traditional AutoML based mix-precision methods since we only need to search ratios for the quantization bits. Compare CW-HAWQ with state-of-the-art shows that we can achieve better results for multiple networks.",0
"The effectiveness of second-order information in determining neural network weight and activation redundancy is well-established. A recent study has suggested using Hessian trace of weights and activations for mixed-precision quantization, which has resulted in state-of-the-art outcomes. However, previous research has only focused on selecting bits for each layer and has not considered the differences in redundancy among different channels. This is due to the complex nature of determining bits for each channel, which has made it challenging to use conventional methods. To overcome this limitation, we introduce Channel-wise Hessian Aware trace-Weighted Quantization (CW-HAWQ). CW-HAWQ utilizes Hessian trace to determine the relative sensitivity order of different channels of activations and weights. Additionally, CW-HAWQ uses a deep Reinforcement Learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agent to determine the optimal ratios of different quantization bits and assign bits to channels according to the Hessian trace order. Compared to traditional AutoML-based mixed-precision methods, CW-HAWQ has a smaller number of states, as it only searches for ratios for the quantization bits. Our results show that CW-HAWQ outperforms state-of-the-art methods for multiple networks.",1
"We propose an explainable reinforcement learning (XRL) framework that analyzes an agent's history of interaction with the environment to extract interestingness elements that help explain its behavior. The framework relies on data readily available from standard RL algorithms, augmented with data that can easily be collected by the agent while learning. We describe how to create visual summaries of an agent's behavior in the form of short video-clips highlighting key interaction moments, based on the proposed elements. We also report on a user study where we evaluated the ability of humans to correctly perceive the aptitude of agents with different characteristics, including their capabilities and limitations, given visual summaries automatically generated by our framework. The results show that the diversity of aspects captured by the different interestingness elements is crucial to help humans correctly understand an agent's strengths and limitations in performing a task, and determine when it might need adjustments to improve its performance.",0
"Our proposed framework for explainable reinforcement learning (XRL) involves analyzing an agent's history of interaction with its environment to identify interesting elements that can help explain its behavior. This framework utilizes data from standard RL algorithms and additional data that can be easily collected by the learning agent. We demonstrate how visual summaries of an agent's behavior can be created using short video-clips that highlight key moments of interaction based on the identified interestingness elements. Additionally, we conducted a user study to evaluate the effectiveness of our framework in enabling humans to accurately perceive an agent's capabilities and limitations based on automatically generated visual summaries. Our findings indicate that a diverse set of interestingness elements is crucial in helping humans understand an agent's strengths and weaknesses in performing a task and identifying areas for improvement.",1
"We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes average reward in undiscounted infinite-horizon problem settings. The attacker can manipulate the rewards or the transition dynamics in the learning environment at training-time and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an \emph{optimal stealthy attack} for different measures of attack cost. We provide sufficient technical conditions under which the attack is feasible and provide lower/upper bounds on the attack cost. We instantiate our attacks in two settings: (i) an \emph{offline} setting where the agent is doing planning in the poisoned environment, and (ii) an \emph{online} setting where the agent is learning a policy using a regret-minimization framework with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.",0
"The focus of our study is a security threat to reinforcement learning, which involves an attacker manipulating the learning environment so that the agent is compelled to execute a target policy selected by the attacker. Our examination is centered on RL agents whose objective is to discover a policy that maximizes the average reward in an undiscounted infinite-horizon problem setting. The attacker can tamper with the rewards or transition dynamics during training, with a preference for a stealthy approach. We propose an optimization framework for uncovering the ideal stealthy attack for various measures of attack cost, and we establish technical criteria for determining the feasibility of the attack and providing lower/upper bounds on the attack cost. We implement our attacks in two settings: (i) an offline setting, where the agent is planning in a poisoned environment, and (ii) an online setting, where the agent is learning a policy via a regret-minimization framework using poisoned feedback. Our findings indicate that under mild conditions, the attacker can effortlessly teach the victim any target policy, and this poses a significant security threat to reinforcement learning agents in practical situations.",1
"Visualization tools for supervised learning allow users to interpret, introspect, and gain an intuition for the successes and failures of their models. While reinforcement learning practitioners ask many of the same questions, existing tools are not applicable to the RL setting as these tools address challenges typically found in the supervised learning regime. In this work, we design and implement an interactive visualization tool for debugging and interpreting RL algorithms. Our system addresses many features missing from previous tools such as (1) tools for supervised learning often are not interactive; (2) while debugging RL policies researchers use state representations that are different from those seen by the agent; (3) a framework designed to make the debugging RL policies more conducive. We provide an example workflow of how this system could be used, along with ideas for future extensions.",0
"The use of visualization tools in supervised learning enables users to gain a better understanding of their models by interpreting and analyzing their successes and failures. However, these tools are not suitable for reinforcement learning, as they do not address the specific challenges associated with this type of learning. In this study, we have created an interactive visualization tool that is specifically designed for debugging and interpreting RL algorithms. Our system offers several features that were previously missing from other tools, including interactivity, the ability to use state representations that are different from those seen by the agent, and a framework that is optimized for debugging RL policies. We present an example workflow for using our tool and suggest potential areas for future development.",1
"Localizing moments in untrimmed videos via language queries is a new and interesting task that requires the ability to accurately ground language into video. Previous works have approached this task by processing the entire video, often more than once, to localize relevant activities. In the real world applications of this approach, such as video surveillance, efficiency is a key system requirement. In this paper, we present TripNet, an end-to-end system that uses a gated attention architecture to model fine-grained textual and visual representations in order to align text and video content. Furthermore, TripNet uses reinforcement learning to efficiently localize relevant activity clips in long videos, by learning how to intelligently skip around the video. It extracts visual features for few frames to perform activity classification. In our evaluation over Charades-STA, ActivityNet Captions and the TACoS dataset, we find that TripNet achieves high accuracy and saves processing time by only looking at 32-41% of the entire video.",0
"The task of localizing moments in untrimmed videos through language queries is intriguing and requires accurate grounding of language into video. Prior approaches have involved processing the entire video multiple times to identify relevant activities, but efficiency is crucial in real-world applications like video surveillance. In this paper, we introduce TripNet, an end-to-end system that employs gated attention architecture to model fine-grained textual and visual representations to align text and video content. Additionally, TripNet uses reinforcement learning to efficiently locate relevant activity clips in long videos by intelligently skipping around the video. It extracts visual features for a few frames for activity classification. Based on our evaluation of Charades-STA, ActivityNet Captions and the TACoS dataset, TripNet proves to be highly accurate and saves processing time by only analyzing 32-41% of the video.",1
"The performance of a trained object detection neural network depends a lot on the image quality. Generally, images are pre-processed before feeding them into the neural network and domain knowledge about the image dataset is used to choose the pre-processing techniques. In this paper, we introduce an algorithm called ObjectRL to choose the amount of a particular pre-processing to be applied to improve the object detection performances of pre-trained networks. The main motivation for ObjectRL is that an image which looks good to a human eye may not necessarily be the optimal one for a pre-trained object detector to detect objects.",0
"The effectiveness of a trained neural network for object detection heavily relies on the quality of the image. Typically, images undergo pre-processing before being inputted into the neural network, and knowledge of the image dataset is utilized to select appropriate pre-processing techniques. This paper presents ObjectRL, an algorithm designed to determine the appropriate level of a specific pre-processing technique to enhance the performance of pre-trained networks for object detection. The primary driving force behind ObjectRL is the recognition that an image that appears satisfactory to humans may not necessarily be the most ideal for a pre-trained object detector to accurately detect objects.",1
"Current research directions in deep reinforcement learning include bridging the simulation-reality gap, improving sample efficiency of experiences in distributed multi-agent reinforcement learning, together with the development of robust methods against adversarial agents in distributed learning, among many others. In this work, we are particularly interested in analyzing how multi-agent reinforcement learning can bridge the gap to reality in distributed multi-robot systems where the operation of the different robots is not necessarily homogeneous. These variations can happen due to sensing mismatches, inherent errors in terms of calibration of the mechanical joints, or simple differences in accuracy. While our results are simulation-based, we introduce the effect of sensing, calibration, and accuracy mismatches in distributed reinforcement learning with proximal policy optimization (PPO). We discuss on how both the different types of perturbances and how the number of agents experiencing those perturbances affect the collaborative learning effort. The simulations are carried out using a Kuka arm model in the Bullet physics engine. This is, to the best of our knowledge, the first work exploring the limitations of PPO in multi-robot systems when considering that different robots might be exposed to different environments where their sensors or actuators have induced errors. With the conclusions of this work, we set the initial point for future work on designing and developing methods to achieve robust reinforcement learning on the presence of real-world perturbances that might differ within a multi-robot system.",0
"Deep reinforcement learning is a field of research that aims to improve the efficiency of experiences in distributed multi-agent reinforcement learning and develop robust methods against adversarial agents. Our focus is on exploring how multi-agent reinforcement learning can bridge the gap to reality in distributed multi-robot systems, even when the operation of the robots is not homogeneous. This can occur due to sensing mismatches, calibration errors, or differences in accuracy. We use proximal policy optimization (PPO) in our simulations, introducing the effect of sensing, calibration, and accuracy mismatches to discuss the impact on collaborative learning efforts. Our simulations use a Kuka arm model in the Bullet physics engine. This is the first work to explore the limitations of PPO in multi-robot systems when considering that different robots may be exposed to varying environments with induced errors. Our conclusions set the initial point for future work on designing and developing methods to achieve robust reinforcement learning in the presence of real-world perturbances that may differ within a multi-robot system.",1
"We consider the problem of learning from demonstrated trajectories with inverse reinforcement learning (IRL). Motivated by a limitation of the classical maximum entropy model in capturing the structure of the network of states, we propose an IRL model based on a generalized version of the causal entropy maximization problem, which allows us to generate a class of maximum entropy IRL models. Our generalized model has an advantage of being able to recover, in addition to a reward function, another expert's function that would (partially) capture the impact of the connecting structure of the states on experts' decisions. Empirical evaluation on a real-world dataset and a grid-world dataset shows that our generalized model outperforms the classical ones, in terms of recovering reward functions and demonstrated trajectories.",0
"The focus of our study is on using inverse reinforcement learning (IRL) to learn from demonstrated trajectories. Our research is motivated by the limitations of the classical maximum entropy model, which struggles to capture the state network's structure. To address this issue, we propose a new IRL model that is based on a generalized version of the causal entropy maximization problem. This approach allows us to produce a range of maximum entropy IRL models. Our generalized model has the added benefit of recovering expert functions that take into account the connecting structure of states and how it influences expert decision-making. We evaluated our model on a real-world dataset and a grid-world dataset, demonstrating that our approach outperforms classical models when it comes to recovering reward functions and demonstrated trajectories.",1
"We consider the generic approach of using an experience memory to help exploration by adapting a restart distribution. That is, given the capacity to reset the state with those corresponding to the agent's past observations, we help exploration by promoting faster state-space coverage via restarting the agent from a more diverse set of initial states, as well as allowing it to restart in states associated with significant past experiences. This approach is compatible with both on-policy and off-policy methods. However, a caveat is that altering the distribution of initial states could change the optimal policies when searching within a restricted class of policies. To reduce this unsought learning bias, we evaluate our approach in deep reinforcement learning which benefits from the high representational capacity of deep neural networks. We instantiate three variants of our approach, each inspired by an idea in the context of experience replay. Using these variants, we show that performance gains can be achieved, especially in hard exploration problems.",0
"Our approach involves utilizing an experience memory to aid exploration by adjusting a restart distribution, which can be applied to both on-policy and off-policy methods. The concept is to enable faster coverage of the state-space by allowing the agent to start from a wider range of initial states, including those linked to significant past experiences. However, altering the initial state distribution may affect optimal policies when searching within a restricted policy class, resulting in unwanted learning bias. To minimize this bias, we employ deep reinforcement learning that takes advantage of the high representational capacity of deep neural networks. We develop three variants of our approach, inspired by concepts in experience replay, and demonstrate that they can enhance performance, particularly in challenging exploration scenarios.",1
"Studying the set of exact solutions of a system of polynomial equations largely depends on a single iterative algorithm, known as Buchberger's algorithm. Optimized versions of this algorithm are crucial for many computer algebra systems (e.g., Mathematica, Maple, Sage). We introduce a new approach to Buchberger's algorithm that uses reinforcement learning agents to perform S-pair selection, a key step in the algorithm. We then study how the difficulty of the problem depends on the choices of domain and distribution of polynomials, about which little is known. Finally, we train a policy model using proximal policy optimization (PPO) to learn S-pair selection strategies for random systems of binomial equations. In certain domains, the trained model outperforms state-of-the-art selection heuristics in total number of polynomial additions performed, which provides a proof-of-concept that recent developments in machine learning have the potential to improve performance of algorithms in symbolic computation.",0
"The identification of the exact solutions of a polynomial equation system is mainly reliant on Buchberger's algorithm, a single iterative algorithm that is critical for various computer algebra systems like Mathematica, Maple, and Sage. Our study presents a new approach that employs reinforcement learning agents in S-pair selection, a crucial phase in the algorithm, to improve its performance. We also explore how the complexity of the problem is influenced by the selection of polynomials' domain and distribution, which is not yet fully understood. Lastly, we employ proximal policy optimization (PPO) to train a policy model for S-pair selection strategies in random binomial equation systems. Our experiments show that in specific domains, the trained model surpasses the state-of-the-art selection heuristics in terms of total polynomial additions executed. This demonstrates the potential of machine learning advancements in enhancing the output of symbolic computation algorithms.",1
"Despite the wide applications of Adam in reinforcement learning (RL), the theoretical convergence of Adam-type RL algorithms has not been established. This paper provides the first such convergence analysis for two fundamental RL algorithms of policy gradient (PG) and temporal difference (TD) learning that incorporate AMSGrad updates (a standard alternative of Adam in theoretical analysis), referred to as PG-AMSGrad and TD-AMSGrad, respectively. Moreover, our analysis focuses on Markovian sampling for both algorithms. We show that under general nonlinear function approximation, PG-AMSGrad with a constant stepsize converges to a neighborhood of a stationary point at the rate of $\mathcal{O}(1/T)$ (where $T$ denotes the number of iterations), and with a diminishing stepsize converges exactly to a stationary point at the rate of $\mathcal{O}(\log^2 T/\sqrt{T})$. Furthermore, under linear function approximation, TD-AMSGrad with a constant stepsize converges to a neighborhood of the global optimum at the rate of $\mathcal{O}(1/T)$, and with a diminishing stepsize converges exactly to the global optimum at the rate of $\mathcal{O}(\log T/\sqrt{T})$. Our study develops new techniques for analyzing the Adam-type RL algorithms under Markovian sampling.",0
"Although Adam is widely used in reinforcement learning (RL), there is currently no established theoretical convergence for Adam-type RL algorithms. This paper presents the first convergence analysis for two fundamental RL algorithms: policy gradient (PG) and temporal difference (TD) learning. These algorithms incorporate AMSGrad updates, which is a standard alternative to Adam in theoretical analysis. We focus on Markovian sampling for both algorithms. Our analysis shows that PG-AMSGrad with a constant stepsize converges to a stationary point at $\mathcal{O}(1/T)$ rate, while with a diminishing stepsize, it converges exactly to a stationary point at $\mathcal{O}(\log^2 T/\sqrt{T})$ rate. Under linear function approximation, TD-AMSGrad with a constant stepsize converges to the global optimum at $\mathcal{O}(1/T)$ rate, and with a diminishing stepsize, it converges exactly to the global optimum at $\mathcal{O}(\log T/\sqrt{T})$ rate. This study develops new techniques to analyze Adam-type RL algorithms under Markovian sampling.",1
"We study the optimal sample complexity in large-scale Reinforcement Learning (RL) problems with policy space generalization, i.e. the agent has a prior knowledge that the optimal policy lies in a known policy space. Existing results show that without a generalization model, the sample complexity of an RL algorithm will inevitably depend on the cardinalities of state space and action space, which are intractably large in many practical problems.   To avoid such undesirable dependence on the state and action space sizes, this paper proposes a new notion of eluder dimension for the policy space, which characterizes the intrinsic complexity of policy learning in an arbitrary Markov Decision Process (MDP). Using a simulator oracle, we prove a near-optimal sample complexity upper bound that only depends linearly on the eluder dimension. We further prove a similar regret bound in deterministic systems without the simulator.",0
"Our research focuses on determining the ideal sample complexity for Reinforcement Learning (RL) in large-scale problems with policy space generalization. This means that the agent already possesses knowledge that the optimal policy can be found within a specific policy space. Previous studies have revealed that RL algorithms lack generalization models, resulting in sample complexities that rely on the extensive cardinalities of state and action spaces. This poses a challenge for practical problems, where these spaces are often too large to handle. To address this issue, our study introduces a novel concept known as the eluder dimension, which measures the inherent complexity of policy learning in any Markov Decision Process (MDP). By utilizing a simulator oracle, we demonstrate a sample complexity upper bound that is almost optimal and only relies on the eluder dimension. Furthermore, we establish a comparable regret bound for deterministic systems that do not have a simulator.",1
"This paper proposes Entropy-Regularized Imitation Learning (ERIL), which is a combination of forward and inverse reinforcement learning under the framework of the entropy-regularized Markov decision process. ERIL minimizes the reverse Kullback-Leibler (KL) divergence between two probability distributions induced by a learner and an expert. Inverse reinforcement learning (RL) in ERIL evaluates the log-ratio between two distributions using the density ratio trick, which is widely used in generative adversarial networks. More specifically, the log-ratio is estimated by building two binary discriminators. The first discriminator is a state-only function, and it tries to distinguish the state generated by the forward RL step from the expert's state. The second discriminator is a function of current state, action, and transitioned state, and it distinguishes the generated experiences from the ones provided by the expert. Since the second discriminator has the same hyperparameters of the forward RL step, it can be used to control the discriminator's ability. The forward RL minimizes the reverse KL estimated by the inverse RL. We show that minimizing the reverse KL divergence is equivalent to finding an optimal policy under entropy regularization. Consequently, a new policy is derived from an algorithm that resembles Dynamic Policy Programming and Soft Actor-Critic. Our experimental results on MuJoCo-simulated environments show that ERIL is more sample-efficient than such previous methods. We further apply the method to human behaviors in performing a pole-balancing task and show that the estimated reward functions show how every subject achieves the goal.",0
"The proposed method in this paper is called Entropy-Regularized Imitation Learning (ERIL). ERIL combines forward and inverse reinforcement learning within the entropy-regularized Markov decision process framework. The goal of ERIL is to minimize the reverse Kullback-Leibler (KL) divergence between the probability distributions induced by the expert and learner. Inverse reinforcement learning in ERIL uses the density ratio trick to evaluate the log-ratio between the two distributions with the help of two binary discriminators. The first discriminator is a state-only function that distinguishes between the forward RL step and the expert's state, while the second discriminator is a function of current state, action, and transitioned state that distinguishes between generated experiences and those provided by the expert. The second discriminator has the same hyperparameters as the forward RL step, enabling control over the discriminator's ability. The forward RL minimizes the reverse KL estimated by the inverse RL, which is equivalent to finding an optimal policy under entropy regularization. A new policy is derived from an algorithm resembling Dynamic Policy Programming and Soft Actor-Critic. The experimental results on MuJoCo-simulated environments demonstrate that ERIL is more sample-efficient than previous methods. The method is further applied to human behaviors in a pole-balancing task, and the estimated reward functions reveal how each subject achieves the goal.",1
"Many popular reinforcement learning problems (e.g., navigation in a maze, some Atari games, mountain car) are instances of the episodic setting under its stochastic shortest path (SSP) formulation, where an agent has to achieve a goal state while minimizing the cumulative cost. Despite the popularity of this setting, the exploration-exploitation dilemma has been sparsely studied in general SSP problems, with most of the theoretical literature focusing on different problems (i.e., fixed-horizon and infinite-horizon) or making the restrictive loop-free SSP assumption (i.e., no state can be visited twice during an episode). In this paper, we study the general SSP problem with no assumption on its dynamics (some policies may actually never reach the goal). We introduce UC-SSP, the first no-regret algorithm in this setting, and prove a regret bound scaling as $\displaystyle \widetilde{\mathcal{O}}( D S \sqrt{ A D K})$ after $K$ episodes for any unknown SSP with $S$ states, $A$ actions, positive costs and SSP-diameter $D$, defined as the smallest expected hitting time from any starting state to the goal. We achieve this result by crafting a novel stopping rule, such that UC-SSP may interrupt the current policy if it is taking too long to achieve the goal and switch to alternative policies that are designed to rapidly terminate the episode.",0
"Episodic reinforcement learning problems, such as navigation in a maze, some Atari games, and mountain car, fall under the stochastic shortest path (SSP) formulation where the agent strives to reach a goal state while minimizing cumulative cost. Despite the popularity of this setting, exploration-exploitation dilemma has been poorly studied in general SSP problems, with most theoretical literature focusing on different problems or making restrictive assumptions. In this paper, we investigate the general SSP problem with no dynamics assumption and introduce UC-SSP, the first no-regret algorithm in this setting, with a regret bound scaling as $\displaystyle \widetilde{\mathcal{O}}( D S \sqrt{ A D K})$ after $K$ episodes for any unknown SSP. We achieve this result by developing a new stopping rule that enables UC-SSP to switch to alternative policies if the current one takes too long to achieve the goal, thereby rapidly terminating the episode.",1
"Hierarchical abstraction and curiosity-driven exploration are two common paradigms in current reinforcement learning approaches to break down difficult problems into a sequence of simpler ones and to overcome reward sparsity. However, there is a lack of approaches that combine these paradigms, and it is currently unknown whether curiosity also helps to perform the hierarchical abstraction. As a novelty and scientific contribution, we tackle this issue and develop a method that combines hierarchical reinforcement learning with curiosity. Herein, we extend a contemporary hierarchical actor-critic approach with a forward model to develop a hierarchical notion of curiosity. We demonstrate in several continuous-space environments that curiosity can more than double the learning performance and success rates for most of the investigated benchmarking problems. We also provide our source code and a supplementary video.",0
"There are two common approaches in reinforcement learning: hierarchical abstraction and curiosity-driven exploration. These methods break down complex problems into simpler ones and overcome reward sparsity respectively. However, there is a lack of research on how these paradigms can be combined and whether curiosity aids in hierarchical abstraction. Therefore, we present a novel method that combines hierarchical reinforcement learning with curiosity. Our approach extends a contemporary hierarchical actor-critic approach with a forward model to develop a hierarchical notion of curiosity. Through experiments in various continuous-space environments, we show that incorporating curiosity can significantly improve learning performance and success rates in benchmarking problems. Additionally, we provide our source code and a supplementary video.",1
"Catan is a strategic board game having interesting properties, including multi-player, imperfect information, stochastic, complex state space structure (hexagonal board where each vertex, edge and face has its own features, cards for each player, etc), and a large action space (including negotiation). Therefore, it is challenging to build AI agents by Reinforcement Learning (RL for short), without domain knowledge nor heuristics. In this paper, we introduce cross-dimensional neural networks to handle a mixture of information sources and a wide variety of outputs, and empirically demonstrate that the network dramatically improves RL in Catan. We also show that, for the first time, a RL agent can outperform jsettler, the best heuristic agent available.",0
"Catan is a board game that poses a challenge for creating AI agents through Reinforcement Learning due to its multi-player nature, complex state space structure, and diverse action space. The game's hexagonal board has distinct features on each vertex, edge, and face, alongside unique cards for each player, and negotiation options. However, this paper introduces cross-dimensional neural networks that handle various outputs and information sources, leading to a significant improvement in Catan's RL. Moreover, the research shows that the RL agent can even outperform jsettler, the game's best heuristic agent, for the first time.",1
"Recent studies have demonstrated that reinforcement learning (RL) agents are susceptible to adversarial manipulation, similar to vulnerabilities previously demonstrated in the supervised learning setting. While most existing work studies the problem in the context of computer vision or console games, this paper focuses on reinforcement learning in autonomous cyber defence under partial observability. We demonstrate that under the black-box setting, where the attacker has no direct access to the target RL model, causative attacks---attacks that target the training process---can poison RL agents even if the attacker only has partial observability of the environment. In addition, we propose an inversion defence method that aims to apply the opposite perturbation to that which an attacker might use to generate their adversarial samples. Our experimental results illustrate that the countermeasure can effectively reduce the impact of the causative attack, while not significantly affecting the training process in non-attack scenarios.",0
"Recent research has shown that reinforcement learning (RL) agents are vulnerable to adversarial manipulation, similar to the weaknesses previously observed in supervised learning. While prior studies have typically examined this issue in the context of computer vision or console games, this paper focuses on the application of RL in autonomous cyber defense with partial observability. Our investigation reveals that, even when the attacker has only limited access to the environment, causative attacks targeting the training process can corrupt RL agents under the black-box setting. Furthermore, we introduce an inversion defense technique that seeks to counteract the perturbations used by attackers to generate their adversarial samples. Our experimental findings indicate that this countermeasure can effectively mitigate the impact of causative attacks without significantly affecting the training process in non-attack scenarios.",1
"In reinforcement learning, wrappers are universally used to transform the information that passes between a model and an environment. Despite their ubiquity, no library exists with reasonable implementations of all popular preprocessing methods. This leads to unnecessary bugs, code inefficiencies, and wasted developer time. Accordingly we introduce SuperSuit, a Python library that includes all popular wrappers, and wrappers that can easily apply lambda functions to the observations/actions/reward. It's compatible with the standard Gym environment specification, as well as the PettingZoo specification for multi-agent environments. The library is available at https://github.com/PettingZoo-Team/SuperSuit,and can be installed via pip.",0
"Wrappers are widely employed in reinforcement learning to convert information exchanged between a model and an environment. Despite their extensive usage, there is no library that provides reasonable implementations of all the prevalent preprocessing methods. Consequently, this results in unnecessary errors, suboptimal code, and wasted developer time. To address this issue, we present SuperSuit, a Python library that encompasses all the popular wrappers, and also allows the application of lambda functions to observations, actions, and rewards. The library conforms to the standard Gym environment specification as well as the PettingZoo specification for multi-agent environments. You can access the library at https://github.com/PettingZoo-Team/SuperSuit and install it via pip.",1
"We present AutoPose, a novel neural architecture search(NAS) framework that is capable of automatically discovering multiple parallel branches of cross-scale connections towards accurate and high-resolution 2D human pose estimation. Recently, high-performance hand-crafted convolutional networks for pose estimation show growing demands on multi-scale fusion and high-resolution representations. However, current NAS works exhibit limited flexibility on scale searching, they dominantly adopt simplified search spaces of single-branch architectures. Such simplification limits the fusion of information at different scales and fails to maintain high-resolution representations. The presentedAutoPose framework is able to search for multi-branch scales and network depth, in addition to the cell-level microstructure. Motivated by the search space, a novel bi-level optimization method is presented, where the network-level architecture is searched via reinforcement learning, and the cell-level search is conducted by the gradient-based method. Within 2.5 GPU days, AutoPose is able to find very competitive architectures on the MS COCO dataset, that are also transferable to the MPII dataset. Our code is available at https://github.com/VITA-Group/AutoPose.",0
"AutoPose is a new framework for neural architecture search that can automatically identify several parallel branches of cross-scale connections for accurate and high-resolution 2D human pose estimation. While hand-crafted convolutional networks have demonstrated high performance in pose estimation, they require multi-scale fusion and high-resolution representations. However, current NAS methods have limited flexibility in scale searching and predominantly employ simplified search spaces of single-branch architectures, which results in poor information fusion at different scales and low-resolution representations. To address this issue, AutoPose can search for multi-branch scales and network depth, as well as cell-level microstructure. A novel bi-level optimization technique is employed, wherein reinforcement learning is used to search for the network-level architecture, and the gradient-based method is used for the cell-level search. Within 2.5 GPU days, AutoPose found highly competitive architectures on the MS COCO dataset, which were transferable to the MPII dataset. Our code is available at https://github.com/VITA-Group/AutoPose.",1
"Deep Q-Networks (DQN) is one of the most well-known methods of deep reinforcement learning, which uses deep learning to approximate the action-value function. Solving numerous Deep reinforcement learning challenges such as moving targets problem and the correlation between samples are the main advantages of this model. Although there have been various extensions of DQN in recent years, they all use a similar method to DQN to overcome the problem of moving targets. Despite the advantages mentioned, synchronizing the network weight in a fixed step size, independent of the agent's behavior, may in some cases cause the loss of some properly learned networks. These lost networks may lead to states with more rewards, hence better samples stored in the replay memory for future training. In this paper, we address this problem from the DQN family and provide an adaptive approach for the synchronization of the neural weights used in DQN. In this method, the synchronization of weights is done based on the recent behavior of the agent, which is measured by a criterion at the end of the intervals. To test this method, we adjusted the DQN and rainbow methods with the proposed adaptive synchronization method. We compared these adjusted methods with their standard form on well-known games, which results confirm the quality of our synchronization methods.",0
"DQN is a renowned deep reinforcement learning technique that employs deep learning to estimate the action-value function. DQN effectively tackles various challenges encountered in deep reinforcement learning, including addressing moving targets and managing sample correlation. Although there have been several DQN extensions over the years, all of them rely on similar methods to overcome the moving targets challenge. Despite its advantages, the fixed step size used for synchronizing network weights in DQN may result in the loss of appropriately learned networks. This loss can cause states with more rewards, leading to better samples stored in the replay memory for future training. This paper proposes an adaptive synchronization approach for neural weights used in DQN to address this issue. The synchronization is based on the agent's recent behavior, measured using a criterion at the end of intervals. We tested this method by adjusting DQN and rainbow methods and compared them with their standard forms on well-known games. The results confirm the effectiveness of our synchronization methods.",1
"We present a holistic data-driven approach to the problem of productivity increase on the example of a metallurgical pickling line. The proposed approach combines mathematical modeling as a base algorithm and a cooperative Multi-Agent Reinforcement Learning (MARL) system implemented such as to enhance the performance by multiple criteria while also meeting safety and reliability requirements and taking into account the unexpected volatility of certain technological processes. We demonstrate how Deep Q-Learning can be applied to a real-life task in a heavy industry, resulting in significant improvement of previously existing automation systems.The problem of input data scarcity is solved by a two-step combination of LSTM and CGAN, which helps to embrace both the tabular representation of the data and its sequential properties. Offline RL training, a necessity in this setting, has become possible through the sophisticated probabilistic kinematic environment.",0
"In this article, we introduce a comprehensive data-driven strategy for boosting productivity in a metallurgical pickling line. Our approach involves using mathematical modeling as a foundation algorithm and a cooperative Multi-Agent Reinforcement Learning (MARL) system to enhance performance across multiple criteria, while prioritizing safety and reliability and accounting for unexpected fluctuations in technological processes. We demonstrate how Deep Q-Learning can be applied to an actual task in the heavy industry, leading to substantial improvements in pre-existing automation systems. Additionally, we address the issue of insufficient input data by combining LSTM and CGAN in a two-step process, which accommodates both the tabular form of the data and its sequential nature. Finally, we discuss how the advanced probabilistic kinematic environment allows for offline RL training in this context.",1
"Temporal difference (TD) learning is one of the main foundations of modern reinforcement learning. This paper studies the use of TD(0), a canonical TD algorithm, to estimate the value function of a given policy from a batch of data. In this batch setting, we show that TD(0) may converge to an inaccurate value function because the update following an action is weighted according to the number of times that action occurred in the batch -- not the true probability of the action under the given policy. To address this limitation, we introduce \textit{policy sampling error corrected}-TD(0) (PSEC-TD(0)). PSEC-TD(0) first estimates the empirical distribution of actions in each state in the batch and then uses importance sampling to correct for the mismatch between the empirical weighting and the correct weighting for updates following each action. We refine the concept of a certainty-equivalence estimate and argue that PSEC-TD(0) is a more data efficient estimator than TD(0) for a fixed batch of data. Finally, we conduct an empirical evaluation of PSEC-TD(0) on three batch value function learning tasks, with a hyperparameter sensitivity analysis, and show that PSEC-TD(0) produces value function estimates with lower mean squared error than TD(0).",0
"The study focuses on TD learning, a crucial aspect of modern reinforcement learning, and examines the use of TD(0) algorithm to calculate the value function of a policy from a set of data. However, in a batch scenario, the accuracy of the TD(0) algorithm is questionable because the weighting of updates following an action is determined by the frequency of the action in the batch, not by its actual probability in the given policy. To overcome this limitation, the paper introduces PSEC-TD(0), which estimates the empirical distribution of actions in each state in the batch and uses importance sampling to correct the weightings for updates after each action. The concept of certainty-equivalence estimate is refined, and PSEC-TD(0) is shown to be a more efficient estimator than TD(0) for a fixed batch of data. Empirical evaluations of PSEC-TD(0) on three batch value function learning tasks, with a hyperparameter sensitivity analysis, demonstrate that PSEC-TD(0) produces value function estimates with lower mean squared error than TD(0).",1
"Minimax optimization has found extensive applications in modern machine learning, in settings such as generative adversarial networks (GANs), adversarial training and multi-agent reinforcement learning. As most of these applications involve continuous nonconvex-nonconcave formulations, a very basic question arises---""what is a proper definition of local optima?""   Most previous work answers this question using classical notions of equilibria from simultaneous games, where the min-player and the max-player act simultaneously. In contrast, most applications in machine learning, including GANs and adversarial training, correspond to sequential games, where the order of which player acts first is crucial (since minimax is in general not equal to maximin due to the nonconvex-nonconcave nature of the problems). The main contribution of this paper is to propose a proper mathematical definition of local optimality for this sequential setting---local minimax, as well as to present its properties and existence results. Finally, we establish a strong connection to a basic local search algorithm---gradient descent ascent (GDA): under mild conditions, all stable limit points of GDA are exactly local minimax points up to some degenerate points.",0
"In modern machine learning, minimax optimization is widely used in various applications such as generative adversarial networks (GANs), adversarial training, and multi-agent reinforcement learning. However, due to the continuous nonconvex-nonconcave nature of these problems, a crucial question arises regarding the appropriate definition of local optima. While previous research has used classical notions of equilibria from simultaneous games, most machine learning applications involve sequential games where player order affects the outcome. Therefore, in this paper, we propose a proper mathematical definition of local optimality for this sequential setting, known as local minimax, and explore its properties and existence results. Additionally, we establish a strong connection between local minimax and the gradient descent ascent (GDA) algorithm, showing that all stable limit points of GDA are local minimax points up to some degenerate points, given mild conditions.",1
"We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the $Q$-function, and the Reward function by deep learning. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.",0
"Our framework proposes the estimation of a reward function for inverse reinforcement learning, utilizing deep energy-based policies. Our method is called PQR, which sequentially estimates the Policy, the $Q$-function, and the Reward function through deep learning. Unlike other methods, PQR does not assume that the reward is solely dependent on the state, but can account for the action taken as well. It also includes stochastic state transitions. To achieve this, we assume the existence of an anchor action that yields no reward, typically the action of doing nothing. We provide estimators and algorithms for the PQR, which we prove can uniquely recover the true reward when the environment transition is known. In cases where the transitions are unknown, we bound the estimation error of PQR. The effectiveness of PQR is demonstrated through both synthetic and real-world datasets.",1
"Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a stepwise approach for optimizing safety and cumulative reward. In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openly-available environment named GP-SAFETY-GYM, and the other simulates Mars surface exploration by using real observation data.",0
"The use of safe reinforcement learning has shown potential in improving the policy of agents that operate in safety-critical applications. In this paper, we introduce the SNO-MDP algorithm, which optimizes Markov decision processes when faced with unknown safety constraints. Our approach involves a stepwise procedure that prioritizes safety and cumulative reward optimization. Initially, the agent gains knowledge of safety constraints by expanding the safe region, before proceeding to optimize cumulative reward within the certified safe region. We provide theoretical guarantees that ensure both the satisfaction of the safety constraint and the near-optimality of the cumulative reward, provided certain regularity assumptions are met. To demonstrate the effectiveness of SNO-MDP, we conduct experiments in two scenarios. The first experiment involves synthetic data from the newly available GP-SAFETY-GYM environment, while the second simulates Mars surface exploration using real observation data.",1
"Deep reinforcement learning (RL) has achieved great empirical successes in various domains. However, the large search space of neural networks requires a large amount of data, which makes the current RL algorithms not sample efficient. Motivated by the fact that many environments with continuous state space have smooth transitions, we propose to learn a smooth policy that behaves smoothly with respect to states. We develop a new framework -- \textbf{S}mooth \textbf{R}egularized \textbf{R}einforcement \textbf{L}earning ($\textbf{SR}^2\textbf{L}$), where the policy is trained with smoothness-inducing regularization. Such regularization effectively constrains the search space, and enforces smoothness in the learned policy. Moreover, our proposed framework can also improve the robustness of policy against measurement error in the state space, and can be naturally extended to distribubutionally robust setting. We apply the proposed framework to both on-policy (TRPO) and off-policy algorithm (DDPG). Through extensive experiments, we demonstrate that our method achieves improved sample efficiency and robustness.",0
"Various domains have seen great empirical success with deep reinforcement learning (RL). However, the large search space of neural networks means that current RL algorithms are not sample efficient due to the need for a large amount of data. In light of the fact that many environments with continuous state space have smooth transitions, we suggest learning a smooth policy that behaves smoothly with respect to states. Our solution is the Smooth Regularized Reinforcement Learning (SR RL) framework, which trains the policy with smoothness-inducing regularization to effectively constrain the search space and enforce smoothness in the learned policy. Our framework also improves the policy's robustness against measurement error in the state space and can be naturally extended to distributionally robust settings. We apply the SR RL framework to both on-policy (TRPO) and off-policy algorithms (DDPG) and demonstrate through extensive experiments that our method achieves improved sample efficiency and robustness.",1
"Reinforcement learning (RL) in real-world safety-critical target settings like urban driving is hazardous, imperiling the RL agent, other agents, and the environment. To overcome this difficulty, we propose a ""safety-critical adaptation"" task setting: an agent first trains in non-safety-critical ""source"" environments such as in a simulator, before it adapts to the target environment where failures carry heavy costs. We propose a solution approach, CARL, that builds on the intuition that prior experience in diverse environments equips an agent to estimate risk, which in turn enables relative safety through risk-averse, cautious adaptation. CARL first employs model-based RL to train a probabilistic model to capture uncertainty about transition dynamics and catastrophic states across varied source environments. Then, when exploring a new safety-critical environment with unknown dynamics, the CARL agent plans to avoid actions that could lead to catastrophic states. In experiments on car driving, cartpole balancing, half-cheetah locomotion, and robotic object manipulation, CARL successfully acquires cautious exploration behaviors, yielding higher rewards with fewer failures than strong RL adaptation baselines. Website at https://sites.google.com/berkeley.edu/carl.",0
"RL in safety-critical settings, such as urban driving, poses risks to both the RL agent and its surroundings. To address this challenge, we introduce a ""safety-critical adaptation"" task setting whereby the agent is first trained in non-safety-critical environments before adapting to the target environment. To achieve this, we propose CARL, which leverages prior experience in diverse environments to estimate risk and facilitate cautious adaptation. CARL employs model-based RL to train a probabilistic model that captures uncertainty about transition dynamics and catastrophic states across various source environments. The CARL agent then avoids actions that could lead to catastrophic states when exploring new safety-critical environments. In experiments involving car driving, cartpole balancing, half-cheetah locomotion, and robotic object manipulation, CARL successfully acquires cautious exploration behaviors, leading to higher rewards with fewer failures than strong RL adaptation baselines. Visit https://sites.google.com/berkeley.edu/carl for more information.",1
"Bayesian optimization has demonstrated impressive success in finding the optimum input x* and output f* = f(x*) = max f(x) of a black-box function f. In some applications, however, the optimum output f* is known in advance and the goal is to find the corresponding optimum input x*. In this paper, we consider a new setting in BO in which the knowledge of the optimum output f* is available. Our goal is to exploit the knowledge about f* to search for the input x* efficiently. To achieve this goal, we first transform the Gaussian process surrogate using the information about the optimum output. Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization. We show that our approaches work intuitively and give quantitatively better performance against standard BO methods. We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available.",0
"Bayesian optimization has been successful in finding the best input x* and output f* = f(x*) = max f(x) for a black-box function f. However, in some cases, the desired output f* is already known, and the goal is to find the corresponding input x*. This paper introduces a new approach to BO, where the knowledge of f* is available. Our objective is to use this knowledge efficiently to search for x*. To achieve this, we modify the Gaussian process surrogate using the information about f*. We also propose two acquisition functions: confidence bound minimization and expected regret minimization. Our methods outperform standard BO techniques both intuitively and quantitatively, as shown in real-world applications such as tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on the Skin Segmentation dataset where the optimum values are publicly available.",1
"Trajectory optimizers for model-based reinforcement learning, such as the Cross-Entropy Method (CEM), can yield compelling results even in high-dimensional control tasks and sparse-reward environments. However, their sampling inefficiency prevents them from being used for real-time planning and control. We propose an improved version of the CEM algorithm for fast planning, with novel additions including temporally-correlated actions and memory, requiring 2.7-22x less samples and yielding a performance increase of 1.2-10x in high-dimensional control problems.",0
"Model-based reinforcement learning trajectory optimizers, like the Cross-Entropy Method (CEM), have proven effective in high-dimensional control tasks and sparse-reward environments. Despite this, their sampling inefficiency hinders their use in real-time planning and control. Our solution is an enhanced version of the CEM algorithm that enables fast planning. This improved algorithm includes novel elements such as temporally-correlated actions and memory, that reduce the required number of samples by 2.7-22x. As a result, it boosts performance by 1.2-10x in high-dimensional control problems.",1
"In the past few years, off-policy reinforcement learning methods have shown promising results in their application for robot control. Deep Q-learning, however, still suffers from poor data-efficiency and is susceptible to stochasticity in the environment or reward functions which is limiting with regard to real-world applications. We alleviate these problems by proposing two novel off-policy Temporal-Difference formulations: (1) Truncated Q-functions which represent the return for the first n steps of a target-policy rollout w.r.t. the full action-value and (2) Shifted Q-functions, acting as the farsighted return after this truncated rollout. This decomposition allows us to optimize both parts with their individual learning rates, achieving significant learning speedup. We prove that the combination of these short- and long-term predictions is a representation of the full return, leading to the Composite Q-learning algorithm. We show the efficacy of Composite Q-learning in the tabular case and compare Deep Composite Q-learning with TD3 and TD3(Delta), which we introduce as an off-policy variant of TD(Delta). Moreover, we show that Composite TD3 outperforms TD3 as well as state-of-the-art compositional Q-learning approaches significantly in terms of data-efficiency in multiple simulated robot tasks and that Composite Q-learning is robust to stochastic environments and reward functions.",0
"Off-policy reinforcement learning methods have displayed promise for robot control in recent years. However, Deep Q-learning suffers from low data-efficiency and is vulnerable to stochasticity in the environment or reward functions. This limits its applicability in real-world scenarios. Our proposed solution consists of two innovative off-policy Temporal-Difference formulations: Truncated Q-functions and Shifted Q-functions. The Truncated Q-functions represent the return for the first n steps of a target-policy rollout, while the Shifted Q-functions act as the farsighted return after this truncated rollout. By optimizing both parts with their individual learning rates, we achieve significant learning speedup. The combination of these short- and long-term predictions is a representation of the full return, leading to the Composite Q-learning algorithm. We demonstrate the effectiveness of Composite Q-learning in the tabular case and compare it with TD3 and TD3(Delta), which is an off-policy variant of TD(Delta). We also show that Composite TD3 performs better than TD3 and state-of-the-art compositional Q-learning approaches in terms of data-efficiency in various simulated robot tasks. Additionally, Composite Q-learning is robust to stochastic environments and reward functions.",1
"We consider the challenging problem of zero-shot video object segmentation (VOS). That is, segmenting and tracking multiple moving objects within a video fully automatically, without any manual initialization. We treat this as a grouping problem by exploiting object proposals and making a joint inference about grouping over both space and time. We propose a network architecture for tractably performing proposal selection and joint grouping. Crucially, we then show how to train this network with reinforcement learning so that it learns to perform the optimal non-myopic sequence of grouping decisions to segment the whole video. Unlike standard supervised techniques, this also enables us to directly optimize for the non-differentiable overlap-based metrics used to evaluate VOS. We show that the proposed method, which we call ALBA outperforms the previous stateof-the-art on three benchmarks: DAVIS 2017 [2], FBMS [20] and Youtube-VOS [27].",0
"In this study, we focus on the complex task of zero-shot video object segmentation (VOS), which involves automatically segmenting and tracking multiple moving objects within a video without any manual intervention. To address this issue, we approach it as a grouping problem by utilizing object proposals and making joint inferences about grouping across both space and time. We introduce a network architecture that can perform proposal selection and joint grouping in a feasible manner. Additionally, we demonstrate how to use reinforcement learning to train this network, enabling it to learn the optimal sequence of grouping decisions for segmenting the entire video. Unlike traditional supervised techniques, this approach allows us to directly optimize for the non-differentiable overlap-based metrics used to evaluate VOS. Our approach, known as ALBA, surpasses the previous state-of-the-art on three benchmarks: DAVIS 2017, FBMS, and Youtube-VOS.",1
"This paper seeks to establish a framework for directing a society of simple, specialized, self-interested agents to solve what traditionally are posed as monolithic single-agent sequential decision problems. What makes it challenging to use a decentralized approach to collectively optimize a central objective is the difficulty in characterizing the equilibrium strategy profile of non-cooperative games. To overcome this challenge, we design a mechanism for defining the learning environment of each agent for which we know that the optimal solution for the global objective coincides with a Nash equilibrium strategy profile of the agents optimizing their own local objectives. The society functions as an economy of agents that learn the credit assignment process itself by buying and selling to each other the right to operate on the environment state. We derive a class of decentralized reinforcement learning algorithms that are broadly applicable not only to standard reinforcement learning but also for selecting options in semi-MDPs and dynamically composing computation graphs. Lastly, we demonstrate the potential advantages of a society's inherent modular structure for more efficient transfer learning.",0
"The aim of this paper is to create a framework that guides a group of self-serving, specialized agents to solve complex decision-making problems that are typically handled by a single agent. The challenge of using a decentralized approach to collectively optimize a central objective lies in defining the equilibrium strategy profile of non-cooperative games. In order to overcome this obstacle, we have designed a mechanism that defines the learning environment for each agent, which leads to a Nash equilibrium strategy profile and ultimately an optimal solution for the global objective. The agents within the society function as an economy that learns the credit assignment process by buying and selling the right to operate on the environment state. We have developed a range of decentralized reinforcement learning algorithms that are widely applicable, including for semi-MDPs and dynamically composing computation graphs. Finally, we demonstrate how the modular structure of a society can lead to more efficient transfer learning.",1
"Direct policy gradient methods for reinforcement learning are a successful approach for a variety of reasons: they are model free, they directly optimize the performance metric of interest, and they allow for richly parameterized policies. Their primary drawback is that, by being local in nature, they fail to adequately explore the environment. In contrast, while model-based approaches and Q-learning directly handle exploration through the use of optimism, their ability to handle model misspecification and function approximation is far less evident. This work introduces the the Policy Cover-Policy Gradient (PC-PG) algorithm, which provably balances the exploration vs. exploitation tradeoff using an ensemble of learned policies (the policy cover). PC-PG enjoys polynomial sample complexity and run time for both tabular MDPs and, more generally, linear MDPs in an infinite dimensional RKHS. Furthermore, PC-PG also has strong guarantees under model misspecification that go beyond the standard worst case $\ell_{\infty}$ assumptions; this includes approximation guarantees for state aggregation under an average case error assumption, along with guarantees under a more general assumption where the approximation error under distribution shift is controlled. We complement the theory with empirical evaluation across a variety of domains in both reward-free and reward-driven settings.",0
"Reinforcement learning has successfully implemented direct policy gradient methods due to their various advantages. These methods are model-free, optimize the performance metric of interest, and allow for richly parameterized policies. However, their limitation is that they fail to explore the environment sufficiently due to their local nature. In contrast, model-based approaches and Q-learning handle exploration through the use of optimism, but their ability to handle model misspecification and function approximation is less evident. The Policy Cover-Policy Gradient (PC-PG) algorithm is introduced in this work to balance the exploration vs. exploitation tradeoff using an ensemble of learned policies (the policy cover). PC-PG has polynomial sample complexity and run time for both tabular MDPs and linear MDPs in an infinite dimensional RKHS. Moreover, PC-PG has strong guarantees under model misspecification, which includes approximation guarantees for state aggregation under an average case error assumption and guarantees under a more general assumption where the approximation error under distribution shift is controlled. The theory is complemented with empirical evaluation across a variety of domains in both reward-free and reward-driven settings.",1
"Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difficult to replicate. In this work, we argue that the inconsistency of performance stems from the use of flawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks.",0
"Reinforcement learning algorithmic advancements heavily rely on performance evaluations. However, recent studies have revealed that the reported results are frequently inconsistent and challenging to reproduce. We assert that this inconsistency is due to flawed evaluation metrics. To address this problem, we introduce a comprehensive evaluation methodology for reinforcement learning algorithms that generates reliable performance measurements for individual and aggregated environments. To validate this approach, we apply it to a diverse array of reinforcement learning algorithms on standard benchmark tasks.",1
"The use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain.",0
"The application of Reinforcement Learning (RL) is presently confined to simulations or improving human-operated systems through recommendations. Actual environments, such as industrial robots or power grids, are designed with safety restrictions in mind, which are enforced through valid actions masks or contingency controllers. For instance, a robot's motor range and angles may be limited to physical boundaries. Any breach of these restrictions leads to rejected actions or a safe mode triggered by an external controller, thereby preventing RL agents from learning from their errors. The purpose of this paper is to propose a straightforward adjustment to a state-of-the-art deep RL algorithm (DQN) that allows learning from forbidden actions. To achieve this, an additional safety loss inspired by structured classification is incorporated into the traditional Q-learning update. Our empirical results demonstrate that this strategy reduces the number of constraint violations during the learning phase and accelerates convergence to nearly optimal policies compared to standard DQN. The experiments were conducted in a Visual Grid World Environment and Text-World domain.",1
"Requirement of large annotated datasets restrict the use of deep convolutional neural networks (CNNs) for many practical applications. The problem can be mitigated by using active learning (AL) techniques which, under a given annotation budget, allow to select a subset of data that yields maximum accuracy upon fine tuning. State of the art AL approaches typically rely on measures of visual diversity or prediction uncertainty, which are unable to effectively capture the variations in spatial context. On the other hand, modern CNN architectures make heavy use of spatial context for achieving highly accurate predictions. Since the context is difficult to evaluate in the absence of ground-truth labels, we introduce the notion of contextual diversity that captures the confusion associated with spatially co-occurring classes. Contextual Diversity (CD) hinges on a crucial observation that the probability vector predicted by a CNN for a region of interest typically contains information from a larger receptive field. Exploiting this observation, we use the proposed CD measure within two AL frameworks: (1) a core-set based strategy and (2) a reinforcement learning based policy, for active frame selection. Our extensive empirical evaluation establish state of the art results for active learning on benchmark datasets of Semantic Segmentation, Object Detection and Image Classification. Our ablation studies show clear advantages of using contextual diversity for active learning. The source code and additional results are available at https://github.com/sharat29ag/CDAL.",0
"Deep convolutional neural networks (CNNs) are often limited in practical applications due to the requirement of large annotated datasets. To address this, active learning (AL) techniques can be used to select a subset of data that yields maximum accuracy upon fine tuning within a given annotation budget. However, state of the art AL approaches rely on measures of visual diversity or prediction uncertainty, which are unable to effectively capture variations in spatial context. This is problematic as modern CNN architectures rely heavily on spatial context for accurate predictions. To address this limitation, we propose the use of contextual diversity (CD) to capture the confusion associated with spatially co-occurring classes. CD is based on the observation that the probability vector predicted by a CNN for a region of interest typically contains information from a larger receptive field. We use the proposed CD measure within two AL frameworks, a core-set based strategy and a reinforcement learning based policy, for active frame selection. Our empirical evaluation shows that our approach outperforms state of the art results for active learning on benchmark datasets of semantic segmentation, object detection and image classification. Our ablation studies demonstrate clear advantages of using contextual diversity for active learning. The source code and additional results are available at https://github.com/sharat29ag/CDAL.",1
"Estimating dense correspondences between images is a long-standing image under-standing task. Recent works introduce convolutional neural networks (CNNs) to extract high-level feature maps and find correspondences through feature matching. However,high-level feature maps are in low spatial resolution and therefore insufficient to provide accurate and fine-grained features to distinguish intra-class variations for correspondence matching. To address this problem, we generate robust features by dynamically selecting features at different scales. To resolve two critical issues in feature selection,i.e.,how many and which scales of features to be selected, we frame the feature selection process as a sequential Markov decision-making process (MDP) and introduce an optimal selection strategy using reinforcement learning (RL). We define an RL environment for image matching in which each individual action either requires new features or terminates the selection episode by referring a matching score. Deep neural networks are incorporated into our method and trained for decision making. Experimental results show that our method achieves comparable/superior performance with state-of-the-art methods on three benchmarks, demonstrating the effectiveness of our feature selection strategy.",0
"The task of estimating dense correspondences between images has been a longstanding challenge in image understanding. Convolutional neural networks (CNNs) have recently been introduced to extract high-level feature maps and facilitate feature matching. However, these feature maps are of low spatial resolution and cannot provide precise and detailed features to distinguish intra-class variations for correspondence matching. To overcome this limitation, we have developed a method that dynamically selects features at multiple scales to generate robust features. To address the challenges of feature selection, we have formulated it as a sequential Markov decision-making process (MDP) and introduced an optimal selection strategy using reinforcement learning (RL). Our approach uses deep neural networks for decision making and incorporates them into our method. We have defined an RL environment for image matching where each action either requires new features or terminates the selection episode by referring to a matching score. Our experimental results show that our method achieves comparable or superior performance with state-of-the-art techniques on three benchmarks, demonstrating the effectiveness of our feature selection strategy.",1
"We investigate how reinforcement learning can be used to train level-designing agents. This represents a new approach to procedural content generation in games, where level design is framed as a game, and the content generator itself is learned. By seeing the design problem as a sequential task, we can use reinforcement learning to learn how to take the next action so that the expected final level quality is maximized. This approach can be used when few or no examples exist to train from, and the trained generator is very fast. We investigate three different ways of transforming two-dimensional level design problems into Markov decision processes and apply these to three game environments.",0
"Our research delves into the utilization of reinforcement learning for training agents that design levels. This marks a novel method for generating procedural content in games, as it formulates level design as a game and teaches the content generator itself. By viewing the design challenge as a sequential task, we can apply reinforcement learning to learn how to undertake the subsequent action to optimize the level's final quality. This approach is fitting for situations where minimal or no examples are available for training and the generator is highly efficient. We examine three distinct methods of converting two-dimensional level design predicaments into Markov decision processes and utilize them across three game environments.",1
"Training a multi-agent reinforcement learning (MARL) model is generally difficult because there are numerous combinations of complex interactions among agents that induce certain reward signals. Especially when there is a sparse reward signal, the training becomes more difficult. Previous studies have tried to resolve this issue by employing an intrinsic reward, which is a signal specifically designed for inducing the interactions among agents, to boost the MARL model training. However, this approach requires extensive prior knowledge to design an intrinsic reward. To optimize the training of an MARL model, we propose a learning-based exploration strategy to generate the initial states of a game. The proposed method adopts a variational graph autoencoder to represent a state of a game such that (1) the state can be compactly encoded to the latent representation by considering the relationship among agents, and (2) the latent representation can be used as an effective input to the surrogate model predicting the exploration score. The proposed method determines the latent representations that maximize the surrogate model and decodes these representations to generate the initial states from which the MARL model starts training. Empirically, we demonstrate that the generated states improve the training and performance of MARL more than the existing exploration methods.",0
"Training a MARL model is challenging due to the complex interactions among agents that produce reward signals, particularly when rewards are sparse. Previous approaches have employed intrinsic rewards to stimulate agent interactions, but they require considerable prior knowledge to design. To optimize training, we propose a learning-based exploration strategy that generates initial game states. Our method employs a variational graph autoencoder to represent game states compactly, considering agent relationships. The latent representation is used as an input to the surrogate model that predicts exploration scores. Our approach identifies latent representations that maximize the surrogate model and decodes them to generate initial states for MARL training. Empirical results show that our generated states improve training and performance compared to existing exploration methods.",1
"The severe on-chip memory limitations are currently preventing the deployment of the most accurate Deep Neural Network (DNN) models on tiny MicroController Units (MCUs), even if leveraging an effective 8-bit quantization scheme. To tackle this issue, in this paper we present an automated mixed-precision quantization flow based on the HAQ framework but tailored for the memory and computational characteristics of MCU devices. Specifically, a Reinforcement Learning agent searches for the best uniform quantization levels, among 2, 4, 8 bits, of individual weight and activation tensors, under the tight constraints on RAM and FLASH embedded memory sizes. We conduct an experimental analysis on MobileNetV1, MobileNetV2 and MNasNet models for Imagenet classification. Concerning the quantization policy search, the RL agent selects quantization policies that maximize the memory utilization. Given an MCU-class memory bound of 2MB for weight-only quantization, the compressed models produced by the mixed-precision engine result as accurate as the state-of-the-art solutions quantized with a non-uniform function, which is not tailored for CPUs featuring integer-only arithmetic. This denotes the viability of uniform quantization, required for MCU deployments, for deep weights compression. When also limiting the activation memory budget to 512kB, the best MobileNetV1 model scores up to 68.4% on Imagenet thanks to the found quantization policy, resulting to be 4% more accurate than the other 8-bit networks fitting the same memory constraints.",0
"At present, the limited memory capacity of on-chip systems is preventing the deployment of highly accurate Deep Neural Network (DNN) models on small MicroController Units (MCUs), even with the use of effective 8-bit quantization schemes. To address this problem, we propose an automated mixed-precision quantization process based on the HAQ framework but customized for the memory and computational properties of MCU devices. Our method employs a Reinforcement Learning agent to identify the optimal uniform quantization levels, ranging from 2 to 8 bits, for individual weight and activation tensors while adhering to strict constraints on RAM and FLASH memory sizes. We evaluate our approach on Imagenet classification using the MobileNetV1, MobileNetV2, and MNasNet models. Our RL agent selects quantization policies that maximize memory utilization and achieves compression results as accurate as state-of-the-art solutions that use non-uniform functions not suited for CPUs with integer-only arithmetic. This validates the effectiveness of uniform quantization for deep weight compression in MCU deployments. By also limiting the activation memory budget to 512kB, our proposed quantization policy results in a 4% increase in accuracy for the best MobileNetV1 model, achieving a score of 68.4% on Imagenet while adhering to the same memory constraints as other 8-bit networks.",1
"Reinforcement learning has been applied to human movement through physiologically-based biomechanical models to add insights into the neural control of these movements; it is also useful in the design of prosthetics and robotics. In this paper, we extend the use of reinforcement learning into controlling an ocular biomechanical system to perform saccades, which is one of the fastest eye movement systems. We describe an ocular environment and an agent trained using Deep Deterministic Policy Gradients method to perform saccades. The agent was able to match the desired eye position with a mean deviation angle of 3:5+/-1:25 degrees. The proposed framework is a first step towards using the capabilities of deep reinforcement learning to enhance our understanding of ocular biomechanics.",0
"The utilization of reinforcement learning has been expanded beyond human movement analysis to include the design of prosthetics and robotics, allowing for insights into the neural control of these movements. This paper takes it a step further by implementing reinforcement learning to control an ocular biomechanical system for the purpose of performing saccades. The study includes the creation of an ocular environment and an agent that was trained using the Deep Deterministic Policy Gradients method. The agent was able to successfully match the desired eye position with a mean deviation angle of 3:5+/-1:25 degrees. This framework represents a crucial first step in utilizing the power of deep reinforcement learning to advance our understanding of ocular biomechanics.",1
"Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support mixed precision (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy, and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.",0
"The use of model quantization in compressing and accelerating deep neural network (DNN) inference is widespread. As DNN hardware accelerators increasingly support mixed precision (1-8 bits), finding the optimal bitwidth for each layer becomes challenging. This requires domain experts to explore the vast design space, taking into account accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. The conventional quantization algorithm quantizes all layers uniformly, ignoring hardware architectures. In this study, we introduce the Hardware-Aware Automated Quantization (HAQ) framework, which leverages reinforcement learning to determine the quantization policy automatically. We also take into account the hardware accelerator's feedback in the design loop. Instead of relying on proxy signals, we use a hardware simulator to generate direct feedback signals (latency and energy) to the RL agent. Our framework specializes the quantization policy for different neural network architectures and hardware architectures, effectively reducing latency by 1.4-1.95x and energy consumption by 1.9x with negligible loss of accuracy compared to fixed bitwidth (8 bits) quantization. Our framework reveals that optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy, and model size) are drastically different. We interpret the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design.",1
Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated methods of pruning. We also are abstaining from any heuristic information about Virtual Connections or Semi Virtual Connections which were previously used in all previous known computer versions of the game. The H-search algorithm which was the basis of finding such connections and had been used with success in previous Hex playing agents has been forgone. Instead what we use is reinforcement learning through self play and approximations through neural networks to by pass the problem of high branching factor and maintaining large tables for state-action evaluations. Our code is based primarily on NeuroHex. The inspiration is drawn from the recent success of AlphaGo Zero.,0
"Hex is a challenging game that presents a high degree of complexity due to its branching factor. In a groundbreaking attempt to solve the game, we have chosen to forgo traditional game tree structures and pruning methods. Furthermore, we have opted to exclude heuristic information regarding Virtual Connections and Semi Virtual Connections, which were previously used in all known computer versions of Hex. Instead, we have developed a reinforcement learning approach that utilizes self-play and neural networks to overcome the issue of the game's high branching factor and the need for large tables for state-action evaluations. Our approach is heavily influenced by NeuroHex and inspired by the success of AlphaGo Zero.",1
"Off-policy evaluation in reinforcement learning offers the chance of using observational data to improve future outcomes in domains such as healthcare and education, but safe deployment in high stakes settings requires ways of assessing its validity. Traditional measures such as confidence intervals may be insufficient due to noise, limited data and confounding. In this paper we develop a method that could serve as a hybrid human-AI system, to enable human experts to analyze the validity of policy evaluation estimates. This is accomplished by highlighting observations in the data whose removal will have a large effect on the OPE estimate, and formulating a set of rules for choosing which ones to present to domain experts for validation. We develop methods to compute exactly the influence functions for fitted Q-evaluation with two different function classes: kernel-based and linear least squares, as well as importance sampling methods. Experiments on medical simulations and real-world intensive care unit data demonstrate that our method can be used to identify limitations in the evaluation process and make evaluation more robust.",0
"Reinforcement learning's off-policy evaluation presents a promising opportunity for using observational data to improve outcomes in fields like healthcare and education. However, implementing it in high stakes situations requires a way to assess its validity beyond traditional measures like confidence intervals, which may be insufficient due to noise, limited data, and confounding factors. To address this, our paper proposes a hybrid human-AI system that allows human experts to analyze the validity of policy evaluation estimates. This is achieved by identifying observations in the data that have a significant impact on the OPE estimate and creating a set of rules to select which ones to present to domain experts for validation. We also developed methods to compute the influence functions for fitted Q-evaluation using kernel-based and linear least squares function classes, as well as importance sampling techniques. Our experiments on medical simulations and real-world ICU data demonstrate that our method can pinpoint limitations in the evaluation process and improve its robustness.",1
"Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is exacerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning.",0
"Artificial agents can learn to perform various tasks with less supervision by utilizing learned dynamics models and planning and policy learning algorithms. However, a significant challenge when using a learned forward dynamics model is the inconsistency between the model's objective of future state reconstruction and that of the downstream planner or policy, which aims to complete a specific task. This issue is more complex in vision-based control tasks in diverse real-world environments where the model's capacity is limited. This paper proposes a solution to this problem by directing prediction towards task-relevant information. This approach enables the model to be aware of the current task, model only relevant state space quantities, and closely match the downstream task's learning objective. Moreover, this method is entirely self-supervised, eliminating the need for a reward function or image labels. The results show that this method effectively models the relevant parts of the scene conditioned on the goal, outperforming standard task-agnostic dynamics models and model-free reinforcement learning.",1
"In many intelligent systems, a network of agents collaboratively perceives the environment for better and more efficient situation awareness. As these agents often have limited resources, it could be greatly beneficial to identify the content overlapping among camera views from different agents and leverage it for reducing the processing, transmission and storage of redundant/unimportant video frames. This paper presents a consensus-based distributed multi-agent video fast-forwarding framework, named DMVF, that fast-forwards multi-view video streams collaboratively and adaptively. In our framework, each camera view is addressed by a reinforcement learning based fast-forwarding agent, which periodically chooses from multiple strategies to selectively process video frames and transmits the selected frames at adjustable paces. During every adaptation period, each agent communicates with a number of neighboring agents, evaluates the importance of the selected frames from itself and those from its neighbors, refines such evaluation together with other agents via a system-wide consensus algorithm, and uses such evaluation to decide their strategy for the next period. Compared with approaches in the literature on a real-world surveillance video dataset VideoWeb, our method significantly improves the coverage of important frames and also reduces the number of frames processed in the system.",0
"A network of agents working together to perceive their environment for improved situation awareness is common in many intelligent systems. However, since these agents usually operate with limited resources, it would be advantageous to identify overlapping content in camera views from different agents and use it to reduce processing, transmission, and storage of redundant or unimportant video frames. This research paper introduces a distributed multi-agent video fast-forwarding framework, called DMVF, which collaboratively and adaptively fast-forwards multi-view video streams. Each camera view is handled by a fast-forwarding agent based on reinforcement learning, which selects from various strategies to process video frames selectively and transmit them at adjustable speeds. During each adaptation period, agents communicate with neighboring agents, evaluate the importance of selected frames, refine the evaluation via a system-wide consensus algorithm, and use it to determine their strategy for the next period. In a comparison with existing approaches on a real-world surveillance video dataset VideoWeb, our method notably enhances important frame coverage while minimizing the number of frames processed in the system.",1
"We present a simple yet efficient Hybrid Classifier based on Deep Learning and Reinforcement Learning. Q-Learning is used with two Q-states and four actions. Conventional techniques use feature maps extracted from Convolutional Neural Networks (CNNs) and include them in the Qstates along with past history. This leads to difficulties with these approaches as the number of states is very large number due to high dimensions of the feature maps. Since our method uses only two Q-states it is simple and has much lesser number of parameters to optimize and also thus has a straightforward reward function. Also, the approach uses unexplored actions for image processing vis-a-vis other contemporary techniques. Three datasets have been used for benchmarking of the approach. These are the MNIST Digit Image Dataset, the USPS Digit Image Dataset and the MATLAB Digit Image Dataset. The performance of the proposed hybrid classifier has been compared with other contemporary techniques like a well-established Reinforcement Learning Technique, AlexNet, CNN-Nearest Neighbor Classifier and CNNSupport Vector Machine Classifier. Our approach outperforms these contemporary hybrid classifiers on all the three datasets used.",0
"A Hybrid Classifier that combines Deep Learning and Reinforcement Learning is introduced in this study. The classifier employs Q-Learning with four actions and two Q-states. Unlike conventional methods that use feature maps from Convolutional Neural Networks (CNNs), our approach only employs past history in the Q-states, which simplifies the model and reduces the number of parameters to optimize. Additionally, our method employs new actions for image processing that are not used in other contemporary techniques. To evaluate the performance of the proposed approach, three datasets, namely the MNIST Digit Image Dataset, the USPS Digit Image Dataset, and the MATLAB Digit Image Dataset, were used for benchmarking. The proposed hybrid classifier outperforms other contemporary hybrid classifiers, such as AlexNet, CNN-Nearest Neighbor Classifier, and CNN-Support Vector Machine Classifier, on all three datasets.",1
"Deep hashing has shown promising results in image retrieval and recognition. Despite its success, most existing deep hashing approaches are rather similar: either multi-layer perceptron or CNN is applied to extract image feature, followed by different binarization activation functions such as sigmoid, tanh or autoencoder to generate binary code. In this work, we introduce a novel decision-making approach for deep supervised hashing. We formulate the hashing problem as travelling across the vertices in the binary code space, and learn a deep Q-network with a novel label embedding reward defined by Bose-Chaudhuri-Hocquenghem (BCH) codes to explore the best path. Extensive experiments and analysis on the CIFAR-10 and NUS-WIDE dataset show that our approach outperforms state-of-the-art supervised hashing methods under various code lengths.",0
"The success of deep hashing in image retrieval and recognition is well-documented, but current approaches are quite similar. They either employ a multi-layer perceptron or a CNN to extract image features, then use various binarization activation functions such as sigmoid, tanh, or autoencoder to create binary code. Our work introduces a new decision-making approach for deep supervised hashing. We view the hashing problem as a journey through the vertices of the binary code space and train a deep Q-network using a novel label embedding reward based on Bose-Chaudhuri-Hocquenghem (BCH) codes to find the best path. Our experiments on the CIFAR-10 and NUS-WIDE datasets demonstrate that our method outperforms other supervised hashing methods in various code lengths.",1
"Coagent policy gradient algorithms (CPGAs) are reinforcement learning algorithms for training a class of stochastic neural networks called coagent networks. In this work, we prove that CPGAs converge to locally optimal policies. Additionally, we extend prior theory to encompass asynchronous and recurrent coagent networks. These extensions facilitate the straightforward design and analysis of hierarchical reinforcement learning algorithms like the option-critic, and eliminate the need for complex derivations of customized learning rules for these algorithms.",0
"The reinforcement learning algorithms known as coagent policy gradient algorithms (CPGAs) are utilized for training stochastic neural networks, specifically coagent networks. Our research has established that CPGAs result in the attainment of policies that are locally optimal. Furthermore, we have expanded previous theory to encompass asynchronous and recurrent coagent networks. These extensions simplify the design and analysis of hierarchical reinforcement learning algorithms such as option-critic and eliminate the necessity for intricate derivations of personalized learning rules for those algorithms.",1
"The task of searching certain people in videos has seen increasing potential in real-world applications, such as video organization and editing. Most existing approaches are devised to work in an offline manner, where identities can only be inferred after an entire video is examined. This working manner precludes such methods from being applied to online services or those applications that require real-time responses. In this paper, we propose an online person search framework, which can recognize people in a video on the fly. This framework maintains a multimodal memory bank at its heart as the basis for person recognition, and updates it dynamically with a policy obtained by reinforcement learning. Our experiments on a large movie dataset show that the proposed method is effective, not only achieving remarkable improvements over online schemes but also outperforming offline methods.",0
"The potential of searching for specific individuals in videos has expanded in practical applications, including video editing and organization. However, most current approaches operate offline, meaning identities are only identified after the entire video has been analyzed. This limits their use in real-time applications or online services. This study presents an online person search framework that can recognize individuals in a video as it plays. The framework relies on a multimodal memory bank for person recognition and updates it dynamically using reinforcement learning. Experiments on a large movie dataset demonstrated the effectiveness of the proposed method, surpassing both offline and online schemes.",1
"Many modern methods for imitation learning and inverse reinforcement learning, such as GAIL or AIRL, are based on an adversarial formulation. These methods apply GANs to match the expert's distribution over states and actions with the implicit state-action distribution induced by the agent's policy. However, by framing imitation learning as a saddle point problem, adversarial methods can suffer from unstable optimization, and convergence can only be shown for small policy updates. We address these problems by proposing a framework for non-adversarial imitation learning. The resulting algorithms are similar to their adversarial counterparts and, thus, provide insights for adversarial imitation learning methods. Most notably, we show that AIRL is an instance of our non-adversarial formulation, which enables us to greatly simplify its derivations and obtain stronger convergence guarantees. We also show that our non-adversarial formulation can be used to derive novel algorithms by presenting a method for offline imitation learning that is inspired by the recent ValueDice algorithm, but does not rely on small policy updates for convergence. In our simulated robot experiments, our offline method for non-adversarial imitation learning seems to perform best when using many updates for policy and discriminator at each iteration and outperforms behavioral cloning and ValueDice.",0
"Numerous current approaches to imitation learning and inverse reinforcement learning, such as GAIL or AIRL, utilize an adversarial approach that employs GANs to align the expert's distribution over states and actions with the agent's policy. However, this methodology can encounter unstable optimization due to the way imitation learning is framed as a saddle point problem, and convergence can only be demonstrated for minor policy updates. To address these issues, we propose a non-adversarial imitation learning framework that is comparable to its adversarial counterparts and offers valuable insights for adversarial imitation learning methods. Our non-adversarial formulation is demonstrated to be an instance of AIRL, with stronger convergence guarantees and simplified derivations. Furthermore, we present a novel algorithm for offline imitation learning that is inspired by ValueDice but does not necessitate minor policy updates for convergence. Our simulated robot experiments exhibit that our offline non-adversarial imitation learning method performs best when numerous updates for policy and discriminator are employed at each iteration, and it surpasses behavioral cloning and ValueDice.",1
"Dexterous manipulation of objects in virtual environments with our bare hands, by using only a depth sensor and a state-of-the-art 3D hand pose estimator (HPE), is challenging. While virtual environments are ruled by physics, e.g. object weights and surface frictions, the absence of force feedback makes the task challenging, as even slight inaccuracies on finger tips or contact points from HPE may make the interactions fail. Prior arts simply generate contact forces in the direction of the fingers' closures, when finger joints penetrate virtual objects. Although useful for simple grasping scenarios, they cannot be applied to dexterous manipulations such as in-hand manipulation. Existing reinforcement learning (RL) and imitation learning (IL) approaches train agents that learn skills by using task-specific rewards, without considering any online user input. In this work, we propose to learn a model that maps noisy input hand poses to target virtual poses, which introduces the needed contacts to accomplish the tasks on a physics simulator. The agent is trained in a residual setting by using a model-free hybrid RL+IL approach. A 3D hand pose estimation reward is introduced leading to an improvement on HPE accuracy when the physics-guided corrected target poses are remapped to the input space. As the model corrects HPE errors by applying minor but crucial joint displacements for contacts, this helps to keep the generated motion visually close to the user input. Since HPE sequences performing successful virtual interactions do not exist, a data generation scheme to train and evaluate the system is proposed. We test our framework in two applications that use hand pose estimates for dexterous manipulations: hand-object interactions in VR and hand-object motion reconstruction in-the-wild.",0
"Performing precise object manipulation in virtual environments using only a depth sensor and a state-of-the-art 3D hand pose estimator (HPE) is a difficult task due to the lack of force feedback. Although virtual environments follow the rules of physics, the absence of force feedback can lead to interaction failures if HPE inaccuracies occur. Previous methods generated contact forces in the direction of finger closures, which limited their application to simple grasping scenarios. To address this, we propose a model that maps noisy input hand poses to target virtual poses, allowing the necessary contacts to be established on a physics simulator. Our agent is trained using a hybrid RL+IL approach and introduces a 3D hand pose estimation reward, improving HPE accuracy. Since HPE sequences that successfully interact in virtual environments are unavailable, we propose a data generation scheme to train and evaluate our system. We test our framework in two applications: hand-object interactions in VR and hand-object motion reconstruction in-the-wild.",1
"SafePILCO is a software tool for safe and data-efficient policy search with reinforcement learning. It extends the known PILCO algorithm, originally written in MATLAB, to support safe learning. We provide a Python implementation and leverage existing libraries that allow the codebase to remain short and modular, which is appropriate for wider use by the verification, reinforcement learning, and control communities.",0
"SafePILCO is a software tool that facilitates secure and resourceful policy search using reinforcement learning. It builds upon the PILCO algorithm, which was initially developed in MATLAB, to enable safe learning. Our Python implementation of the tool is designed to be concise and modular, and we utilize pre-existing libraries to achieve this, making it suitable for use across verification, reinforcement learning, and control communities.",1
"Deep Q-Network (DQN) based multi-agent systems (MAS) for reinforcement learning (RL) use various schemes where in the agents have to learn and communicate. The learning is however specific to each agent and communication may be satisfactorily designed for the agents. As more complex Deep QNetworks come to the fore, the overall complexity of the multi-agent system increases leading to issues like difficulty in training, need for higher resources and more training time, difficulty in fine-tuning, etc. To address these issues we propose a simple but efficient DQN based MAS for RL which uses shared state and rewards, but agent-specific actions, for updation of the experience replay pool of the DQNs, where each agent is a DQN. The benefits of the approach are overall simplicity, faster convergence and better performance as compared to conventional DQN based approaches. It should be noted that the method can be extended to any DQN. As such we use simple DQN and DDQN (Double Q-learning) respectively on three separate tasks i.e. Cartpole-v1 (OpenAI Gym environment) , LunarLander-v2 (OpenAI Gym environment) and Maze Traversal (customized environment). The proposed approach outperforms the baseline on these tasks by decent margins respectively.",0
"Multi-agent systems (MAS) for reinforcement learning (RL) using Deep Q-Networks (DQN) typically involve agents that need to learn and communicate with each other. However, the learning process is specific to each individual agent, and the communication design may not be optimal. As DQNs become more complex, the overall complexity of the system increases, resulting in difficulties such as longer training times, resource demands, and the need for fine-tuning. To address these challenges, we propose a straightforward yet effective DQN-based MAS for RL that uses shared state and rewards, but agent-specific actions for updating the experience replay pool of the DQNs. Each agent is a DQN, and this approach offers benefits such as simplicity, faster convergence, and better performance compared to traditional DQN-based methods. Moreover, this method can be extended to any DQN. We demonstrate the efficacy of our approach using simple DQN and DDQN on three distinct tasks, namely Cartpole-v1, LunarLander-v2, and Maze Traversal. Our proposed approach outperforms the baseline by a considerable margin in each of these tasks.",1
"Generating accurate descriptions for online fashion items is important not only for enhancing customers' shopping experiences, but also for the increase of online sales. Besides the need of correctly presenting the attributes of items, the expressions in an enchanting style could better attract customer interests. The goal of this work is to develop a novel learning framework for accurate and expressive fashion captioning. Different from popular work on image captioning, it is hard to identify and describe the rich attributes of fashion items. We seed the description of an item by first identifying its attributes, and introduce attribute-level semantic (ALS) reward and sentence-level semantic (SLS) reward as metrics to improve the quality of text descriptions. We further integrate the training of our model with maximum likelihood estimation (MLE), attribute embedding, and Reinforcement Learning (RL). To facilitate the learning, we build a new FAshion CAptioning Dataset (FACAD), which contains 993K images and 130K corresponding enchanting and diverse descriptions. Experiments on FACAD demonstrate the effectiveness of our model.",0
"It is crucial to provide precise descriptions for fashion items online to enhance customers' shopping experiences and increase sales. Captivating language is also essential to attract customers. This study aims to create a new learning framework for fashion captioning that emphasizes accuracy and expressiveness. Unlike existing image captioning work, it is challenging to identify and describe the complex attributes of fashion items. The proposed approach starts by identifying item attributes and uses attribute-level semantic reward and sentence-level semantic reward to improve description quality. The model integrates maximum likelihood estimation, attribute embedding, and Reinforcement Learning to enhance training. To support learning, the team created the FAshion CAptioning Dataset (FACAD) with 993K images and 130K varied and engaging descriptions. Results from experiments on FACAD indicate that the model is effective.",1
"In the paper, we propose a class of efficient momentum-based policy gradient methods for the model-free reinforcement learning, which use adaptive learning rates and do not require any large batches. Specifically, we propose a fast important-sampling momentum-based policy gradient (IS-MBPG) method based on a new momentum-based variance reduced technique and the importance sampling technique. We also propose a fast Hessian-aided momentum-based policy gradient (HA-MBPG) method based on the momentum-based variance reduced technique and the Hessian-aided technique. Moreover, we prove that both the IS-MBPG and HA-MBPG methods reach the best known sample complexity of $O(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of the non-concave performance function, which only require one trajectory at each iteration. In particular, we present a non-adaptive version of IS-MBPG method, i.e., IS-MBPG*, which also reaches the best known sample complexity of $O(\epsilon^{-3})$ without any large batches. In the experiments, we apply four benchmark tasks to demonstrate the effectiveness of our algorithms.",0
"We propose an efficient class of momentum-based policy gradient methods for model-free reinforcement learning in our paper. These methods use adaptive learning rates and do not necessitate large batches. One such method is the fast important-sampling momentum-based policy gradient (IS-MBPG) technique that employs a novel momentum-based variance reduced approach along with importance sampling. Another method we propose is the fast Hessian-aided momentum-based policy gradient (HA-MBPG), which uses the momentum-based variance reduced technique and the Hessian-aided method. We establish that both IS-MBPG and HA-MBPG methods can reach the best-known sample complexity of $O(\epsilon^{-3})$ to find an $\epsilon$-stationary point of the non-concave performance function, requiring only one trajectory per iteration. Additionally, we present a non-adaptive version of IS-MBPG, called IS-MBPG*, which reaches the best-known sample complexity of $O(\epsilon^{-3})$ without large batches. We conduct experiments on four benchmark tasks to showcase the effectiveness of our algorithms.",1
"This note aims to provide a basic intuition on the concept of filtrations as used in the context of reinforcement learning (RL). Filtrations are often used to formally define RL problems, yet their implications might not be eminent for those without a background in measure theory. Essentially, a filtration is a construct that captures partial knowledge up to time $t$, without revealing any future information that has already been simulated, yet not revealed to the decision-maker. We illustrate this with simple examples from the finance domain on both discrete and continuous outcome spaces. Furthermore, we show that the notion of filtration is not needed, as basing decisions solely on the current problem state (which is possible due to the Markovian property) suffices to eliminate future knowledge from the decision-making process.",0
"The purpose of this note is to provide a basic understanding of filtrations in the context of reinforcement learning (RL). Filtrations are commonly used in the formal definition of RL problems, but their significance may not be immediately apparent to those without a background in measure theory. In essence, filtrations capture partial knowledge up to a certain time without disclosing any future information that has been simulated but not yet revealed to the decision-maker. We demonstrate this through finance examples on discrete and continuous outcome spaces. Additionally, we demonstrate that the use of filtrations is unnecessary because basing decisions solely on the current problem state, which is possible due to the Markovian property, is sufficient to remove future knowledge from the decision-making process.",1
"Recurrent Mixture Density Networks (RMDNs) are consisted of two main parts: a Recurrent Neural Network (RNN) and a Gaussian Mixture Model (GMM), in which a kind of RNN (almost LSTM) is used to find the parameters of a GMM in every time step. While available RMDNs have been faced with different difficulties. The most important of them is high$-$dimensional problems. Since estimating the covariance matrix for the high$-$dimensional problems is more difficult, due to existing correlation between dimensions and satisfying the positive definition condition. Consequently, the available methods have usually used RMDN with a diagonal covariance matrix for high$-$dimensional problems by supposing independence among dimensions. Hence, in this paper with inspiring a common approach in the literature of GMM, we consider a tied configuration for each precision matrix (inverse of the covariance matrix) in RMDN as $(\(\Sigma _k^{ - 1} = U{D_k}U\))$ to enrich GMM rather than considering a diagonal form for it. But due to simplicity, we assume $\(U\)$ be an Identity matrix and $\(D_k\)$ is a specific diagonal matrix for $\(k^{th}\)$ component. Until now, we only have a diagonal matrix and it does not differ with available diagonal RMDNs. Besides, Flow$-$based neural networks are a new group of generative models that are able to transform a distribution to a simpler distribution and vice versa, through a sequence of invertible functions. Therefore, we applied a diagonal GMM on transformed observations. At every time step, the next observation, $\({y_{t + 1}}\)$, has been passed through a flow$-$based neural network to obtain a much simpler distribution. Experimental results for a reinforcement learning problem verify the superiority of the proposed method to the base$-$line method in terms of Negative Log$-$Likelihood (NLL) for RMDN and the cumulative reward for a controller with fewer population size.",0
"Recurrent Mixture Density Networks (RMDNs) consist of a Recurrent Neural Network (RNN) and a Gaussian Mixture Model (GMM). An RNN is used to find the parameters of a GMM in each time step. However, high-dimensional problems pose a challenge as estimating the covariance matrix is difficult due to correlations between dimensions and the positive definition condition. To overcome this, available methods use a diagonal covariance matrix, assuming independence among dimensions. In this paper, we propose a tied configuration for each precision matrix in RMDN to enrich GMM by considering a specific diagonal matrix for the kth component. We apply a diagonal GMM on transformed observations using Flow-based neural networks, which transform a distribution to a simpler distribution and vice versa through invertible functions. Experimental results show that our proposed method outperforms the baseline method in terms of Negative Log-Likelihood for RMDN and cumulative reward for a controller with fewer population size in a reinforcement learning problem.",1
"Learning robot manipulation through deep reinforcement learning in environments with sparse rewards is a challenging task. In this paper we address this problem by introducing a notion of imaginary object goals. For a given manipulation task, the object of interest is first trained to reach a desired target position on its own, without being manipulated, through physically realistic simulations. The object policy is then leveraged to build a predictive model of plausible object trajectories providing the robot with a curriculum of incrementally more difficult object goals to reach during training. The proposed algorithm, Follow the Object (FO), has been evaluated on 7 MuJoCo environments requiring increasing degree of exploration, and has achieved higher success rates compared to alternative algorithms. In particularly challenging learning scenarios, e.g. where the object's initial and target positions are far apart, our approach can still learn a policy whereas competing methods currently fail.",0
"The process of teaching a robot how to manipulate objects using deep reinforcement learning in environments that have limited rewards is a difficult undertaking. To overcome this obstacle, we have developed a concept known as imaginary object goals. Initially, the object of focus is trained to reach a desired position without any manipulation, employing realistic simulations. This technique is then utilized to create a model that predicts potential object trajectories, gradually increasing the level of difficulty for the robot during training. Our approach, Follow the Object (FO), was tested in seven MuJoCo environments with varying degrees of exploration and resulted in higher success rates when compared to other methods. Even in challenging situations, where the object's initial and target positions are distant, our algorithm was able to learn a policy while competitors failed.",1
"Industrial sponsored search system (SSS) can be logically divided into three modules: keywords matching, ad retrieving, and ranking. During ad retrieving, the ad candidates grow exponentially. A query with high commercial value might retrieve a great deal of ad candidates such that the ranking module could not afford. Due to limited latency and computing resources, the candidates have to be pruned earlier. Suppose we set a pruning line to cut SSS into two parts: upstream and downstream. The problem we are going to address is: how to pick out the best $K$ items from $N$ candidates provided by the upstream to maximize the total system's revenue. Since the industrial downstream is very complicated and updated quickly, a crucial restriction in this problem is that the selection scheme should get adapted to the downstream. In this paper, we propose a novel model-free reinforcement learning approach to fixing this problem. Our approach considers downstream as a black-box environment, and the agent sequentially selects items and finally feeds into the downstream, where revenue would be estimated and used as a reward to improve the selection policy. To the best of our knowledge, this is first time to consider the system optimization from a downstream adaption view. It is also the first time to use reinforcement learning techniques to tackle this problem. The idea has been successfully realized in Baidu's sponsored search system, and online long time A/B test shows remarkable improvements on revenue.",0
"The Industrial sponsored search system (SSS) comprises of three modules, namely, keywords matching, ad retrieving, and ranking. While retrieving ads, the number of ad candidates increases exponentially. This poses a challenge for the ranking module, which may not be able to handle the large number of ad candidates for queries with high commercial value. Due to limited computing resources and latency, the ad candidates must be pruned earlier. To address this issue, the SSS is divided into upstream and downstream by setting a pruning line. The objective is to choose the best $K$ items from $N$ candidates provided by the upstream to maximize the total system's revenue. The selection scheme should adapt to the downstream, which is a complex and rapidly changing environment. To solve this problem, we propose a novel model-free reinforcement learning approach that considers the downstream as a black-box environment. The agent selects items sequentially and feeds them into the downstream, where revenue is estimated and used as a reward to improve the selection policy. This approach is the first to consider system optimization from a downstream adaption view and to use reinforcement learning techniques to tackle the problem. We have successfully implemented this idea in Baidu's sponsored search system, which has shown remarkable improvements in revenue, as demonstrated by the online long time A/B test.",1
"Deep neural networks, including reinforcement learning agents, have been proven vulnerable to small adversarial changes in the input, thus making deploying such networks in the real world problematic. In this paper, we propose RADIAL-RL, a method to train reinforcement learning agents with improved robustness against any $l_p$-bounded adversarial attack. By simply minimizing an upper bound of the loss functions under worst case adversarial perturbation derived from efficient robustness verification methods, we significantly improve robustness of RL-agents trained on Atari-2600 games and show that RADIAL-RL can beat state-of-the-art robust training algorithms when evaluated against PGD-attacks. We also propose a new evaluation method, Greedy Worst-Case Reward (GWC), for measuring attack agnostic robustness of RL agents. GWC can be evaluated efficiently and it serves as a good estimate of the reward under the worst possible sequence of adversarial attacks; in particular, GWC accounts for the importance of each action and their temporal dependency, improving upon previous approaches that only evaluate whether each single action can change under input perturbations. Our code is available at https://github.com/tuomaso/radial_rl.",0
"The susceptibility of deep neural networks, including reinforcement learning agents, to minor adversarial changes in input poses a problem for their deployment in the real world. To address this issue, we introduce RADIAL-RL, a method for training reinforcement learning agents that enhances their resilience against any $l_p$-bounded adversarial attack. By minimizing the upper bound of loss functions through worst-case adversarial perturbation derived from efficient robustness verification methods, we considerably improve the robustness of RL-agents trained on Atari-2600 games. Our results demonstrate that RADIAL-RL surpasses state-of-the-art robust training algorithms when evaluated against PGD-attacks. Additionally, we present a new evaluation technique called Greedy Worst-Case Reward (GWC), which efficiently measures attack agnostic robustness of RL agents. GWC offers a more accurate estimate of the reward under the worst possible sequence of adversarial attacks, accounting for the significance of each action and their temporal dependency. This approach improves on earlier methods that only evaluate the likelihood of each single action changing under input perturbations. We have made our code available at https://github.com/tuomaso/radial_rl.",1
"This paper demonstrates that continual relearning of control policies using incremental deep reinforcement learning (RL) can improve policy learning for non-stationary processes. We demonstrate this approach for a data-driven 'smart building environment' that we use as a test-bed for developing HVAC controllers for reducing energy consumption of large buildings on our university campus. The non-stationarity in building operations and weather patterns makes it imperative to develop control strategies that are adaptive to changing conditions. On-policy RL algorithms, such as Proximal Policy Optimization (PPO) represent an approach for addressing this non-stationarity, but exploration on the actual system is not an option for safety-critical systems. As an alternative, we develop an incremental RL technique that simultaneously reduces building energy consumption without sacrificing overall comfort. We compare the performance of our incremental RL controller to that of a static RL controller that does not implement the relearning function. The performance of the static controller diminishes significantly over time, but the relearning controller adjusts to changing conditions while ensuring comfort and optimal energy performance.",0
"The aim of this study is to showcase the benefits of incremental deep reinforcement learning (RL) for improving policy learning in systems that are subject to change. The authors apply this approach to a ""smart building environment"" on their university campus, where they develop HVAC controllers for reducing energy consumption. Given the non-stationarity of building operations and weather patterns, it is crucial to develop adaptive control strategies. While on-policy RL algorithms like Proximal Policy Optimization (PPO) can address this issue, exploration on the actual system may not be safe. Therefore, the authors propose an incremental RL technique that achieves optimal energy performance while maintaining comfort. They compare the performance of their technique to that of a static RL controller, which shows significant degradation over time. In contrast, the incremental RL controller continually adjusts to changing conditions and ensures both comfort and energy efficiency.",1
"Popular Maximum Entropy Inverse Reinforcement Learning approaches require the computation of expected state visitation frequencies for the optimal policy under an estimate of the reward function. This usually requires intermediate value estimation in the inner loop of the algorithm, slowing down convergence considerably. In this work, we introduce a novel class of algorithms that only needs to solve the MDP underlying the demonstrated behavior once to recover the expert policy. This is possible through a formulation that exploits a probabilistic behavior assumption for the demonstrations within the structure of Q-learning. We propose Inverse Action-value Iteration which is able to fully recover an underlying reward of an external agent in closed-form analytically. We further provide an accompanying class of sampling-based variants which do not depend on a model of the environment. We show how to extend this class of algorithms to continuous state-spaces via function approximation and how to estimate a corresponding action-value function, leading to a policy as close as possible to the policy of the external agent, while optionally satisfying a list of predefined hard constraints. We evaluate the resulting algorithms called Inverse Action-value Iteration, Inverse Q-learning and Deep Inverse Q-learning on the Objectworld benchmark, showing a speedup of up to several orders of magnitude compared to (Deep) Max-Entropy algorithms. We further apply Deep Constrained Inverse Q-learning on the task of learning autonomous lane-changes in the open-source simulator SUMO achieving competent driving after training on data corresponding to 30 minutes of demonstrations.",0
"The current methods of Maximum Entropy Inverse Reinforcement Learning that are popular require the computation of expected state visitation frequencies for the optimal policy using an estimated reward function. However, this process can be slow as it requires intermediate value estimation in the inner loop of the algorithm. To address this issue, we present a new set of algorithms that only require solving the MDP underlying the demonstrated behavior once to recover the expert policy. This is achieved through a formulation that takes advantage of a probabilistic behavior assumption for the demonstrations within the structure of Q-learning. Our proposed Inverse Action-value Iteration can fully recover an underlying reward of an external agent in closed-form analytically, and we also provide sampling-based variants that do not rely on a model of the environment. We demonstrate how to extend these algorithms to continuous state-spaces via function approximation and estimate a corresponding action-value function, leading to a policy that is close to the external agent's policy while optionally satisfying predefined hard constraints. Our evaluation of the resulting algorithms, including Inverse Action-value Iteration, Inverse Q-learning, and Deep Inverse Q-learning on the Objectworld benchmark, shows a speedup of up to several orders of magnitude compared to (Deep) Max-Entropy algorithms. Additionally, we apply Deep Constrained Inverse Q-learning to the task of learning autonomous lane-changes in the open-source simulator SUMO, achieving competent driving after training on data corresponding to 30 minutes of demonstrations.",1
"This paper introduces an algorithm for discovering implicit and delayed causal relations between events observed by a robot at arbitrary times, with the objective of improving data-efficiency and interpretability of model-based reinforcement learning (RL) techniques. The proposed algorithm initially predicts observations with the Markov assumption, and incrementally introduces new hidden variables to explain and reduce the stochasticity of the observations. The hidden variables are memory units that keep track of pertinent past events. Such events are systematically identified by their information gains. The learned transition and reward models are then used for planning. Experiments on simulated and real robotic tasks show that this method significantly improves over current RL techniques.",0
"The aim of this paper is to present an algorithm that can identify implicit and delayed causal relationships between events observed by a robot at any time. The purpose of this is to enhance the efficiency of data and the comprehensibility of model-based reinforcement learning (RL) methods. The algorithm utilizes the Markov assumption to initially predict observations and gradually introduces hidden variables to explain and decrease the unpredictability of the observations. These hidden variables function as memory units that store important past events, which are identified based on their informational value. The resulting transition and reward models are then utilized for planning. The algorithm was tested on both simulated and actual robotic tasks and was found to be substantially more effective than current RL approaches.",1
"Due to its low storage cost and fast query speed, hashing has been widely used in large-scale image retrieval tasks. Hash bucket search returns data points within a given Hamming radius to each query, which can enable search at a constant or sub-linear time cost. However, existing hashing methods cannot achieve satisfactory retrieval performance for hash bucket search in complex scenarios, since they learn only one hash code for each image. More specifically, by using one hash code to represent one image, existing methods might fail to put similar image pairs to the buckets with a small Hamming distance to the query when the semantic information of images is complex. As a result, a large number of hash buckets need to be visited for retrieving similar images, based on the learned codes. This will deteriorate the efficiency of hash bucket search. In this paper, we propose a novel hashing framework, called multiple code hashing (MCH), to improve the performance of hash bucket search. The main idea of MCH is to learn multiple hash codes for each image, with each code representing a different region of the image. Furthermore, we propose a deep reinforcement learning algorithm to learn the parameters in MCH. To the best of our knowledge, this is the first work that proposes to learn multiple hash codes for each image in image retrieval. Experiments demonstrate that MCH can achieve a significant improvement in hash bucket search, compared with existing methods that learn only one hash code for each image.",0
"Hashing has become a popular technique for large-scale image retrieval tasks due to its low storage cost and fast query speed. Hash bucket search is a method that returns data points within a specific Hamming radius to a query, allowing for search at a constant or sub-linear time cost. However, current hashing methods fail to achieve satisfactory retrieval performance in complex scenarios since they learn only one hash code for each image. This results in similar image pairs being placed in buckets with a large Hamming distance to the query, leading to a decrease in efficiency. To address this problem, we propose a novel hashing framework called multiple code hashing (MCH), which learns multiple hash codes for each image, with each code representing a different region of the image. Additionally, we introduce a deep reinforcement learning algorithm to learn the parameters in MCH. Our experiments demonstrate that MCH significantly improves hash bucket search performance compared to existing methods that only learn one hash code for each image. This is the first work that proposes to learn multiple hash codes for each image in image retrieval.",1
"Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing goal reaching performance together with the entropy of the goal distribution, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. We prove that, under regularity conditions, Skew-Fit converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that combining Skew-Fit for learning goal distributions with existing goal-reaching methods outperforms a variety of prior methods on open-sourced visual goal-reaching tasks. Moreover, we demonstrate that Skew-Fit enables a real-world robot to learn to open a door, entirely from scratch, from pixels, and without any manually-designed reward function.",0
"To equip autonomous agents with a wide range of skills, it is necessary to avoid limiting their repertoire with manually-designed reward functions, which can also be time-consuming. Self-supervised agents that set their own goals can automate this process but may require heuristic design decisions for appropriate goal setting. In this paper, we propose a formal exploration objective that maximizes state coverage for goal-reaching policies. We prove that this objective is equivalent to maximizing goal reaching performance and the entropy of the goal distribution, where goals correspond to full state observations. To implement this principle, we introduce Skew-Fit, an algorithm that learns maximum-entropy goal distributions. We demonstrate that Skew-Fit, combined with existing goal-reaching methods, outperforms various prior methods on visual goal-reaching tasks. Furthermore, we show that Skew-Fit enables a real-world robot to learn to open a door from scratch, using only pixels and without a manually-designed reward function.",1
"Medical automatic diagnosis (MAD) aims to learn an agent that mimics the behavior of human doctors, i.e. inquiring symptoms and informing diseases. Due to medical ethics concerns, it is impractical to directly apply reinforcement learning techniques to MAD, e.g., training a reinforced agent with human patients. Developing a patient simulator by using the collected patient-doctor dialogue records has been proposed as a promising workaround to MAD. However, most of these existing works overlook the causal relationship between patient symptoms and diseases. For example, these simulators simply generate the ""not-sure"" response to the symptom inquiry if the symptom was not observed in the dialogue record. Consequently, the MAD agent is usually trained without exploiting the counterfactual reasoning beyond the factual observations. To address this problem, this paper presents a propensity-based patient simulator (PBPS), which is capable of facilitating the training of MAD agents by generating informative counterfactual answers along with the disease diagnosis. Specifically, our PBPS estimates the propensity score of each record with the patient-doctor dialogue reasoning, and can thus generate the counterfactual answers by searching across records. That is, the unrecorded symptom for one patient can be found in the records of other patients according to the propensity score matching. The informative and causal-aware responses from PBPS are beneficial for modeling diagnostic confidence. To this end, we also propose a progressive assurance agent~(P2A) trained with PBPS, which includes two separate yet cooperative branches accounting for the execution of symptom-inquiry and disease-diagnosis actions, respectively.",0
"The objective of Medical Automatic Diagnosis (MAD) is to train an agent that can imitate the behavior of human doctors by questioning symptoms and identifying diseases. However, using reinforcement learning techniques directly on MAD, such as training an agent with human patients, is not practical due to ethical concerns. As a workaround, a patient simulator has been proposed using collected patient-doctor dialogue records. However, current simulators overlook the causal relationship between patient symptoms and diseases, resulting in the MAD agent being trained without counterfactual reasoning beyond factual observations. This paper introduces a Propensity-Based Patient Simulator (PBPS) that can generate informative counterfactual answers along with disease diagnosis, which is useful for modeling diagnostic confidence. PBPS estimates the propensity score of each record by analyzing patient-doctor dialogue reasoning and can generate counterfactual answers by searching across records. The paper also proposes a Progressive Assurance Agent (P2A) that is trained with PBPS. P2A includes two separate branches for executing symptom-inquiry and disease-diagnosis actions, respectively, that work cooperatively.",1
"Reinforcement learning (RL) algorithms still suffer from high sample complexity despite outstanding recent successes. The need for intensive interactions with the environment is especially observed in many widely popular policy gradient algorithms that perform updates using on-policy samples. The price of such inefficiency becomes evident in real-world scenarios such as interaction-driven robot learning, where the success of RL has been rather limited. We address this issue by building on the general sample efficiency of off-policy algorithms. With nonparametric regression and density estimation methods we construct a nonparametric Bellman equation in a principled manner, which allows us to obtain closed-form estimates of the value function, and to analytically express the full policy gradient. We provide a theoretical analysis of our estimate to show that it is consistent under mild smoothness assumptions and empirically show that our approach has better sample efficiency than state-of-the-art policy gradient methods.",0
"Despite recent impressive achievements, reinforcement learning (RL) algorithms still face the challenge of high sample complexity. This is particularly true for policy gradient algorithms that rely on on-policy samples and require extensive interactions with the environment. The inefficiency of such methods becomes apparent in real-world scenarios, such as interaction-driven robot learning, where RL has not been particularly successful. To address this issue, we leverage the sample efficiency of off-policy algorithms and employ nonparametric regression and density estimation techniques to construct a nonparametric Bellman equation in a principled manner. This enables us to obtain closed-form estimates of the value function and express the full policy gradient analytically. We demonstrate that our approach has better sample efficiency than state-of-the-art policy gradient methods through both theoretical analysis and empirical evaluation.",1
"Acquiring abilities in the absence of a task-oriented reward function is at the frontier of reinforcement learning research. This problem has been studied through the lens of empowerment, which draws a connection between option discovery and information theory. Information-theoretic skill discovery methods have garnered much interest from the community, but little research has been conducted in understanding their limitations. Through theoretical analysis and empirical evidence, we show that existing algorithms suffer from a common limitation -- they discover options that provide a poor coverage of the state space. In light of this, we propose 'Explore, Discover and Learn' (EDL), an alternative approach to information-theoretic skill discovery. Crucially, EDL optimizes the same information-theoretic objective derived from the empowerment literature, but addresses the optimization problem using different machinery. We perform an extensive evaluation of skill discovery methods on controlled environments and show that EDL offers significant advantages, such as overcoming the coverage problem, reducing the dependence of learned skills on the initial state, and allowing the user to define a prior over which behaviors should be learned. Code is publicly available at https://github.com/victorcampos7/edl.",0
"The current focus of research in reinforcement learning is on acquiring abilities without the incentive of a task-based reward system. Empowerment is a concept that has been used to investigate this issue, linking option discovery to information theory. Although information-theoretic skill discovery techniques have attracted a lot of attention, there has been limited exploration of their limitations. Our research explores this issue through theoretical analysis and empirical evidence, revealing that existing algorithms suffer from a common problem - they discover options that do not fully cover the state space. To address this, we propose 'Explore, Discover and Learn' (EDL), an alternative approach to information-theoretic skill discovery that optimizes the same information-theoretic objective derived from the empowerment literature, but uses different methods. We conducted a comprehensive evaluation of skill discovery techniques on controlled environments and found that EDL offers significant advantages, including overcoming the coverage problem, reducing the impact of the initial state on learned skills, and enabling users to define a prior for which behaviors to learn. Our code is available publicly at https://github.com/victorcampos7/edl.",1
"Learning-based approaches for solving large sequential decision making problems have become popular in recent years. The resulting agents perform differently and their characteristics depend on those of the underlying learning approach. Here, we consider a benchmark planning problem from the reinforcement learning domain, the Racetrack, to investigate the properties of agents derived from different deep (reinforcement) learning approaches. We compare the performance of deep supervised learning, in particular imitation learning, to reinforcement learning for the Racetrack model. We find that imitation learning yields agents that follow more risky paths. In contrast, the decisions of deep reinforcement learning are more foresighted, i.e., avoid states in which fatal decisions are more likely. Our evaluations show that for this sequential decision making problem, deep reinforcement learning performs best in many aspects even though for imitation learning optimal decisions are considered.",0
"In recent times, learning-based techniques have gained popularity for solving large sequential decision making problems. The agents produced by these methods exhibit varying characteristics, which are contingent on the underlying learning approach. To investigate the attributes of agents derived from different deep (reinforcement) learning techniques, we employ a benchmark planning problem from the reinforcement learning domain known as Racetrack. Specifically, we compare the performance of deep supervised learning, particularly imitation learning, with reinforcement learning for the Racetrack model. Our findings reveal that imitation learning leads to agents that take more hazardous routes, whereas deep reinforcement learning produces more prudent decisions, avoiding states where fatal consequences are more probable. Despite optimal decisions being considered for imitation learning, our evaluations demonstrate that for this sequential decision making problem, deep reinforcement learning outperforms imitation learning in several aspects.",1
"This paper introduces two simple techniques to improve off-policy Reinforcement Learning (RL) algorithms. First, we formulate off-policy RL as a stochastic proximal point iteration. The target network plays the role of the variable of optimization and the value network computes the proximal operator. Second, we exploits the two value functions commonly employed in state-of-the-art off-policy algorithms to provide an improved action value estimate through bootstrapping with limited increase of computational resources. Further, we demonstrate significant performance improvement over state-of-the-art algorithms on standard continuous-control RL benchmarks.",0
"In this article, two uncomplicated methods are presented to enhance the effectiveness of off-policy Reinforcement Learning (RL) algorithms. Initially, we define off-policy RL as a stochastic proximal point iteration, where the target network functions as the optimization variable and the value network performs the proximal operator. Secondly, we take advantage of the two value functions typically used in top-performing off-policy algorithms to enhance the action value estimation through bootstrapping while keeping computational resources to a minimum. Additionally, we provide evidence of significant performance enhancement compared to state-of-the-art algorithms on standard continuous-control RL benchmarks.",1
"Despite the significant progress of deep reinforcement learning (RL) in solving sequential decision making problems, RL agents often overfit to training environments and struggle to adapt to new, unseen environments. This prevents robust applications of RL in real world situations, where system dynamics may deviate wildly from the training settings. In this work, our primary contribution is to propose an information theoretic regularization objective and an annealing-based optimization method to achieve better generalization ability in RL agents. We demonstrate the extreme generalization benefits of our approach in different domains ranging from maze navigation to robotic tasks; for the first time, we show that agents can generalize to test parameters more than 10 standard deviations away from the training parameter distribution. This work provides a principled way to improve generalization in RL by gradually removing information that is redundant for task-solving; it opens doors for the systematic study of generalization from training to extremely different testing settings, focusing on the established connections between information theory and machine learning.",0
"Although deep reinforcement learning (RL) has made significant strides in addressing sequential decision making problems, RL agents tend to overfit to training environments and experience difficulty adapting to new and unfamiliar environments. This hinders the practical application of RL in real-world scenarios, where system dynamics may differ greatly from the training settings. Our main contribution is proposing an information theoretic regularization objective and an annealing-based optimization method to enhance the generalization ability of RL agents. We showcase the remarkable generalization advantages of our approach across various domains, from maze navigation to robotic tasks. Our work demonstrates, for the first time, that agents can generalize to test parameters over 10 standard deviations away from the training parameter distribution. This work offers a systematic approach to improving generalization in RL by gradually eliminating redundant information for task-solving. Additionally, it enables the systematic examination of generalization from training to vastly different testing settings, focusing on the well-established links between information theory and machine learning.",1
"While recent progress in deep reinforcement learning has enabled robots to learn complex behaviors, tasks with long horizons and sparse rewards remain an ongoing challenge. In this work, we propose an effective reward shaping method through predictive coding to tackle sparse reward problems. By learning predictive representations offline and using these representations for reward shaping, we gain access to reward signals that understand the structure and dynamics of the environment. In particular, our method achieves better learning by providing reward signals that 1) understand environment dynamics 2) emphasize on features most useful for learning 3) resist noise in learned representations through reward accumulation. We demonstrate the usefulness of this approach in different domains ranging from robotic manipulation to navigation, and we show that reward signals produced through predictive coding are as effective for learning as hand-crafted rewards.",0
"Despite recent advancements in deep reinforcement learning that have enabled robots to acquire complex skills, sparse rewards and tasks with long horizons remain challenging. This study proposes a reward shaping technique utilizing predictive coding to address sparse reward issues. By acquiring predictive representations offline and applying them to reward shaping, we access reward signals that comprehend the environment's structure and dynamics. Our method achieves superior learning by providing reward signals that 1) understand environment dynamics, 2) stress on the most useful features for learning, and 3) withstand noise in learned representations through reward accumulation. We demonstrate the efficacy of this approach in a variety of domains, including robotics manipulation and navigation, and show that the reward signals produced through predictive coding are as effective for learning as manually crafted rewards.",1
"Imitation learning can reproduce policies by observing experts, which poses a problem regarding policy privacy. Policies, such as human, or policies on deployed robots, can all be cloned without consent from the owners. How can we protect against external observers cloning our proprietary policies? To answer this question we introduce a new reinforcement learning framework, where we train an ensemble of near-optimal policies, whose demonstrations are guaranteed to be useless for an external observer. We formulate this idea by a constrained optimization problem, where the objective is to improve proprietary policies, and at the same time deteriorate the virtual policy of an eventual external observer. We design a tractable algorithm to solve this new optimization problem by modifying the standard policy gradient algorithm. Our formulation can be interpreted in lenses of confidentiality and adversarial behaviour, which enables a broader perspective of this work. We demonstrate the existence of ""non-clonable"" ensembles, providing a solution to the above optimization problem, which is calculated by our modified policy gradient algorithm. To our knowledge, this is the first work regarding the protection of policies in Reinforcement Learning.",0
"The use of imitation learning to replicate expert policies raises a concern about policy privacy. Unauthorized cloning of policies, whether they be human or robot policies, can occur without the owners' consent. To address this issue, we propose a new reinforcement learning framework that trains a set of near-optimal policies. The demonstrations produced by these policies are rendered useless for external observers, safeguarding proprietary policies. We achieve this by formulating a constrained optimization problem that improves proprietary policies while degrading the virtual policy of external observers. We have developed a practical algorithm to solve this optimization problem, modifying the standard policy gradient algorithm. Our approach considers confidentiality and adversarial behavior, providing a broader perspective. We have demonstrated the existence of ""non-clonable"" ensembles, which solve the optimization problem using our modified policy gradient algorithm. This is the first work that addresses policy protection in Reinforcement Learning.",1
"Mutual Information between agent Actions and environment States (MIAS) quantifies the influence of agent on its environment. Recently, it was found that the maximization of MIAS can be used as an intrinsic motivation for artificial agents. In literature, the term empowerment is used to represent the maximum of MIAS at a certain state. While empowerment has been shown to solve a broad range of reinforcement learning problems, its calculation in arbitrary dynamics is a challenging problem because it relies on the estimation of mutual information. Existing approaches, which rely on sampling, are limited to low dimensional spaces, because high-confidence distribution-free lower bounds for mutual information require exponential number of samples. In this work, we develop a novel approach for the estimation of empowerment in unknown dynamics from visual observation only, without the need to sample for MIAS. The core idea is to represent the relation between action sequences and future states using a stochastic dynamic model in latent space with a specific form. This allows us to efficiently compute empowerment with the ""Water-Filling"" algorithm from information theory. We construct this embedding with deep neural networks trained on a sophisticated objective function. Our experimental results show that the designed embedding preserves information-theoretic properties of the original dynamics.",0
"The Mutual Information between agent Actions and environment States (MIAS) measures how much impact the agent has on its surroundings. Recently, researchers have discovered that maximizing MIAS can motivate artificial agents. The term ""empowerment"" is used to describe the highest MIAS value at a given state. Empowerment has been successful in solving many reinforcement learning problems; however, calculating it in arbitrary dynamics is difficult because it relies on mutual information estimation. Current methods that use sampling are restricted to low-dimensional spaces because high-confidence distribution-free lower bounds for mutual information require an exponential number of samples. In this study, we introduce a new approach to estimating empowerment in unknown dynamics using only visual observations. We do not need to sample for MIAS. The core idea is to use a stochastic dynamic model in latent space with a specific structure to represent the relationship between action sequences and future states. This allows us to efficiently calculate empowerment using the ""Water-Filling"" algorithm from information theory. We use deep neural networks trained on a sophisticated objective function to create this embedding. Our experimental results demonstrate that the designed embedding preserves the information-theoretic properties of the original dynamics.",1
"Curriculum Learning for Reinforcement Learning is an increasingly popular technique that involves training an agent on a defined sequence of intermediate tasks, called a Curriculum, to increase the agent's performance and learning speed. This paper introduces a novel paradigm for automatic curriculum generation based on a progression of task complexity. Different progression functions are introduced, including an autonomous online task progression based on the performance of the agent. The progression function also determines how long the agent should train on each intermediate task, which is an open problem in other task-based curriculum approaches. The benefits and wide applicability of our approach are shown by empirically comparing its performance to two state-of-the-art Curriculum Learning algorithms on a grid world and on a complex simulated navigation domain.",0
"The practice of Curriculum Learning for Reinforcement Learning has gained popularity in recent times. It involves teaching an agent a set of intermediate tasks, known as a Curriculum, in a specific order to enhance its learning speed and performance. This research presents a unique method for automatically generating a Curriculum by following a progression of increasing task complexity. Various progression functions are proposed, such as an independent online task progression that relies on the agent's performance. This function also decides the duration of training required for each intermediate task, which has been a challenge for other task-based Curriculum Learning methods. The study demonstrates the usefulness of our approach and its broad applicability by conducting empirical comparisons with two cutting-edge Curriculum Learning techniques in a grid world and a complex simulated navigation domain.",1
"Distributional Reinforcement Learning (RL) differs from traditional RL in that, rather than the expectation of total returns, it estimates distributions and has achieved state-of-the-art performance on Atari Games. The key challenge in practical distributional RL algorithms lies in how to parameterize estimated distributions so as to better approximate the true continuous distribution. Existing distributional RL algorithms parameterize either the probability side or the return value side of the distribution function, leaving the other side uniformly fixed as in C51, QR-DQN or randomly sampled as in IQN. In this paper, we propose fully parameterized quantile function that parameterizes both the quantile fraction axis (i.e., the x-axis) and the value axis (i.e., y-axis) for distributional RL. Our algorithm contains a fraction proposal network that generates a discrete set of quantile fractions and a quantile value network that gives corresponding quantile values. The two networks are jointly trained to find the best approximation of the true distribution. Experiments on 55 Atari Games show that our algorithm significantly outperforms existing distributional RL algorithms and creates a new record for the Atari Learning Environment for non-distributed agents.",0
"Distributional Reinforcement Learning (RL) diverges from conventional RL by estimating distributions instead of relying on total return expectations. This method has shown outstanding performance in Atari Games. However, implementing practical distributional RL algorithms presents a challenge in parameterizing estimated distributions to approximate the true continuous distribution. Current distributional RL algorithms parameterize either the probability or return value side of the distribution function and leave the other side uniformly fixed or randomly sampled. In this paper, we propose a fully parameterized quantile function that parameterizes both the quantile fraction and value axes for distributional RL. Our algorithm includes a fraction proposal network and a quantile value network that are jointly trained to discover the best approximation of the true distribution. Results from experiments on 55 Atari Games demonstrate that our algorithm outperforms existing distributional RL algorithms and sets a new record for the Atari Learning Environment for non-distributed agents.",1
"Connected and automated vehicles (CAVs) have attracted more and more attention recently. The fast actuation time allows them having the potential to promote the efficiency and safety of the whole transportation system. Due to technical challenges, there will be a proportion of vehicles that can be equipped with automation while other vehicles are without automation. Instead of learning a reliable behavior for ego automated vehicle, we focus on how to improve the outcomes of the total transportation system by allowing each automated vehicle to learn cooperation with each other and regulate human-driven traffic flow. One of state of the art method is using reinforcement learning to learn intelligent decision making policy. However, direct reinforcement learning framework cannot improve the performance of the whole system. In this article, we demonstrate that considering the problem in multi-agent setting with shared policy can help achieve better system performance than non-shared policy in single-agent setting. Furthermore, we find that utilization of attention mechanism on interaction features can capture the interplay between each agent in order to boost cooperation. To the best of our knowledge, while previous automated driving studies mainly focus on enhancing individual's driving performance, this work serves as a starting point for research on system-level multi-agent cooperation performance using graph information sharing. We conduct extensive experiments in car-following and unsignalized intersection settings. The results demonstrate that CAVs controlled by our method can achieve the best performance against several state of the art baselines.",0
"Recently, there has been an increasing interest in Connected and Automated Vehicles (CAVs). The quick response time of CAVs has the potential to improve the safety and efficiency of the transportation system. However, due to technical limitations, not all vehicles can be equipped with automation. Instead of focusing on improving the behavior of individual automated vehicles, this article explores how cooperation and regulation between automated and human-driven vehicles can improve the transportation system's overall outcomes. Reinforcement learning is a state-of-the-art method for learning intelligent decision-making policies. However, a direct reinforcement learning framework cannot improve the performance of the entire system. This article proposes a multi-agent setting with a shared policy that can achieve better system performance than a non-shared policy in a single-agent setting. Moreover, using an attention mechanism on interaction features can capture the interplay between agents to improve cooperation. This work is a starting point for research on system-level multi-agent cooperation performance using graph information sharing. Extensive experiments were conducted in car-following and unsignalized intersection settings, and the results show that CAVs controlled by this method outperformed several state-of-the-art baselines.",1
"Network pruning has been the driving force for the acceleration of neural networks and the alleviation of model storage/transmission burden. With the advent of AutoML and neural architecture search (NAS), pruning has become topical with automatic mechanism and searching based architecture optimization. Yet, current automatic designs rely on either reinforcement learning or evolutionary algorithm. Due to the non-differentiability of those algorithms, the pruning algorithm needs a long searching stage before reaching the convergence.   To circumvent this problem, this paper introduces a differentiable pruning method via hypernetworks for automatic network pruning. The specifically designed hypernetworks take latent vectors as input and generate the weight parameters of the backbone network. The latent vectors control the output channels of the convolutional layers in the backbone network and act as a handle for the pruning of the layers. By enforcing $\ell_1$ sparsity regularization to the latent vectors and utilizing proximal gradient solver, sparse latent vectors can be obtained. Passing the sparsified latent vectors through the hypernetworks, the corresponding slices of the generated weight parameters can be removed, achieving the effect of network pruning. The latent vectors of all the layers are pruned together, resulting in an automatic layer configuration. Extensive experiments are conducted on various networks for image classification, single image super-resolution, and denoising. And the experimental results validate the proposed method.",0
"The acceleration of neural networks and reduction of model storage/transmission burden has been driven by network pruning. Recently, pruning has gained attention with automatic mechanisms and architecture optimization based on neural architecture search (NAS) and AutoML. However, current automatic designs rely on reinforcement learning or evolutionary algorithms. Due to their non-differentiability, the pruning algorithm takes a long time to converge after a searching phase. To overcome this challenge, this paper introduces a differentiable pruning method through hypernetworks for automatic network pruning. The designed hypernetworks take latent vectors as input and generate weight parameters for the backbone network. The latent vectors control output channels of convolutional layers to act as a handle for layer pruning. By enforcing $\ell_1$ sparsity regularization on latent vectors and using proximal gradient solvers, sparse latent vectors are obtained. Corresponding slices of generated weight parameters can be removed, achieving network pruning. The latent vectors of all layers are pruned together, resulting in automatic layer configuration. The proposed method is validated through extensive experiments on various networks for image classification, single image super-resolution, and denoising.",1
"One of the great promises of robot learning systems is that they will be able to learn from their mistakes and continuously adapt to ever-changing environments. Despite this potential, most of the robot learning systems today are deployed as a fixed policy and they are not being adapted after their deployment. Can we efficiently adapt previously learned behaviors to new environments, objects and percepts in the real world? In this paper, we present a method and empirical evidence towards a robot learning framework that facilitates continuous adaption. In particular, we demonstrate how to adapt vision-based robotic manipulation policies to new variations by fine-tuning via off-policy reinforcement learning, including changes in background, object shape and appearance, lighting conditions, and robot morphology. Further, this adaptation uses less than 0.2% of the data necessary to learn the task from scratch. We find that our approach of adapting pre-trained policies leads to substantial performance gains over the course of fine-tuning, and that pre-training via RL is essential: training from scratch or adapting from supervised ImageNet features are both unsuccessful with such small amounts of data. We also find that these positive results hold in a limited continual learning setting, in which we repeatedly fine-tune a single lineage of policies using data from a succession of new tasks. Our empirical conclusions are consistently supported by experiments on simulated manipulation tasks, and by 52 unique fine-tuning experiments on a real robotic grasping system pre-trained on 580,000 grasps.",0
"Although robot learning systems have the potential to learn from their mistakes and adapt to changing environments, most current systems are used as a fixed policy and are not adjusted after deployment. This paper explores the possibility of efficiently adapting previously learned behaviors to new environments, objects, and percepts in the real world. The authors present a method for a robot learning framework that enables continuous adaptation, demonstrating how vision-based robotic manipulation policies can be fine-tuned via off-policy reinforcement learning to accommodate changes in background, object shape and appearance, lighting conditions, and robot morphology. This approach requires less than 0.2% of the data needed to learn the task from scratch, resulting in substantial performance gains over the course of fine-tuning. The authors also find that pre-training via RL is essential and that training from scratch or adapting from supervised ImageNet features with small amounts of data is unsuccessful. The results are supported by experiments on simulated manipulation tasks and 52 unique fine-tuning experiments on a real robotic grasping system pre-trained on 580,000 grasps, demonstrating the potential for continual learning in the future.",1
"Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning of hyperparameters to successfully resolve problems. One of the most influential parameters in optimization procedures based on stochastic gradient descent (SGD) is the learning rate. We investigate cyclical learning and propose a method for defining a general cyclical learning rate for various DRL problems. In this paper we present a method for cyclical learning applied to complex DRL problems. Our experiments show that, utilizing cyclical learning achieves similar or even better results than highly tuned fixed learning rates. This paper presents the first application of cyclical learning rates in DRL settings and is a step towards overcoming manual hyperparameter tuning.",0
DRL methods often require careful adjustment of hyperparameters to effectively solve problems. Among the critical parameters for optimization procedures that employ stochastic gradient descent (SGD) is the learning rate. We examine cyclical learning and put forward a technique to establish a universal cyclical learning rate for various DRL problems. This study introduces a cyclical learning approach for challenging DRL problems. Our tests demonstrate that using cyclical learning produces outcomes that are comparable or superior to those obtained by extensively fine-tuning fixed learning rates. This article marks the first implementation of cyclical learning rates in DRL contexts and represents a stride forward in the quest to eliminate manual hyperparameter adjustment.,1
"Exploration-exploitation dilemma has long been a crucial issue in reinforcement learning. In this paper, we propose a new approach to automatically balance between these two. Our method is built upon the Soft Actor-Critic (SAC) algorithm, which uses an ""entropy temperature"" that balances the original task reward and the policy entropy, and hence controls the trade-off between exploitation and exploration. It is empirically shown that SAC is very sensitive to this hyperparameter, and the follow-up work (SAC-v2), which uses constrained optimization for automatic adjustment, has some limitations. The core of our method, namely Meta-SAC, is to use metagradient along with a novel meta objective to automatically tune the entropy temperature in SAC. We show that Meta-SAC achieves promising performances on several of the Mujoco benchmarking tasks, and outperforms SAC-v2 over 10% in one of the most challenging tasks, humanoid-v2.",0
"For a long time, the exploration-exploitation dilemma has been a significant concern in reinforcement learning. In this article, we suggest a fresh approach to balance these two factors automatically. Our technique employs the Soft Actor-Critic (SAC) algorithm, which uses an ""entropy temperature"" to balance the original task reward and the policy entropy. This approach helps to control the trade-off between exploitation and exploration. Empirical evidence indicates that SAC is highly sensitive to this hyperparameter, and the subsequent SAC-v2 method that uses constrained optimization to automatically adjust the entropy temperature has some restrictions. The essence of our method, called Meta-SAC, is to utilize metagradient together with a unique meta objective to automatically align the entropy temperature in SAC. We demonstrate that Meta-SAC performs well on several of the Mujoco benchmarking tasks and surpasses SAC-v2 by over 10% in one of the most challenging tasks, humanoid-v2.",1
"When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than starting from randomly initialized weights. Adaptation can be useful in cases when training data is scarce, when a single learner needs to perform multiple tasks, or when one wishes to encode priors in the network. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others.   In this paper, we propose a straightforward alternative: side-tuning. Side-tuning adapts a pre-trained network by training a lightweight ""side"" network that is fused with the (unchanged) pre-trained network via summation. This simple method works as well as or better than existing solutions and it resolves some of the basic issues with fine-tuning, fixed features, and other common approaches. In particular, side-tuning is less prone to overfitting, is asymptotically consistent, and does not suffer from catastrophic forgetting in incremental learning. We demonstrate the performance of side-tuning under a diverse set of scenarios, including incremental learning (iCIFAR, iTaskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), with consistently promising results.",0
"When training a neural network for a specific task, using a pre-trained network may be preferable to starting from scratch with randomly initialized weights. This approach is especially useful when training data is limited, when a single learner is expected to perform multiple tasks, or when prior knowledge needs to be encoded in the network. The most common methods for adapting a pre-trained network are fine-tuning and using the pre-trained network as a fixed feature extractor. However, in this paper, we propose an alternative approach called side-tuning. Side-tuning involves training a lightweight ""side"" network that is combined with the pre-trained network through summation. This straightforward method has been shown to be as effective as, or even better than, other existing solutions. Additionally, side-tuning addresses some of the limitations of fine-tuning and fixed features, including reduced overfitting, asymptotic consistency, and avoidance of catastrophic forgetting in incremental learning. Our experiments demonstrate the effectiveness of side-tuning in a variety of scenarios, including reinforcement learning, imitation learning, and transfer learning, among others.",1
"Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved efficiency, but their optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.",0
"The goal of data augmentation (DA) techniques is to enhance the variability of data, ultimately improving the ability of deep networks to generalize. AutoAugment was the first to use reinforcement learning to automate the search for optimal DA policies, but its high computational cost limits its wide usage. Subsequent works such as Population Based Augmentation (PBA) and Fast AutoAugment have improved efficiency, but optimization speed remains a problem. To address this issue, we propose Differentiable Automatic Data Augmentation (DADA), which significantly reduces costs by relaxing the discrete DA policy selection into a differentiable optimization problem using Gumbel-Softmax. Additionally, we introduce an unbiased gradient estimator, RELAX, which leads to an efficient and effective one-pass optimization strategy for learning an efficient and accurate DA policy. We conducted extensive experiments using CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets, and we demonstrated the value of Auto DA in pre-training for downstream detection problems. Our results show that DADA is at least one order of magnitude faster than the state-of-the-art while achieving comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.",1
"Designing the decision-making processes of artificial agents that are involved in competitive interactions is a challenging task. In a competitive scenario, the agent does not only have a dynamic environment but also is directly affected by the opponents' actions. Observing the Q-values of the agent is usually a way of explaining its behavior, however, do not show the temporal-relation between the selected actions. We address this problem by proposing the \emph{Moody framework}. We evaluate our model by performing a series of experiments using the competitive multiplayer Chef's Hat card game and discuss how our model allows the agents' to obtain a holistic representation of the competitive dynamics within the game.",0
"It is difficult to design decision-making processes for artificial agents engaged in competitive interactions. These agents face a constantly changing environment and are impacted by their opponents' actions. While observing an agent's Q-values can explain its behavior, it does not reveal the relationship between its chosen actions over time. To overcome this issue, we introduce the Moody framework. Our framework is tested through a series of experiments using the Chef's Hat card game, and we demonstrate how it allows agents to gain a comprehensive understanding of the competitive dynamics in play.",1
"A great variety of off-policy learning algorithms exist in the literature, and new breakthroughs in this area continue to be made, improving theoretical understanding and yielding state-of-the-art reinforcement learning algorithms. In this paper, we take a unifying view of this space of algorithms, and consider their trade-offs of three fundamental quantities: update variance, fixed-point bias, and contraction rate. This leads to new perspectives of existing methods, and also naturally yields novel algorithms for off-policy evaluation and control. We develop one such algorithm, C-trace, demonstrating that it is able to more efficiently make these trade-offs than existing methods in use, and that it can be scaled to yield state-of-the-art performance in large-scale environments.",0
"The literature presents a wide range of off-policy learning algorithms, with ongoing advancements improving theoretical comprehension and producing advanced reinforcement learning algorithms. This paper takes a holistic approach to these algorithms, evaluating their trade-offs in three critical areas: update variance, fixed-point bias, and contraction rate. This approach offers new insights into established methods and generates original algorithms for both evaluation and control. We introduce the C-trace algorithm as an example, which more effectively balances these trade-offs than current methods and can be scaled to achieve exceptional performance in expansive environments.",1
"The principal contribution of this paper is a conceptual framework for off-policy reinforcement learning, based on conditional expectations of importance sampling ratios. This framework yields new perspectives and understanding of existing off-policy algorithms, and reveals a broad space of unexplored algorithms. We theoretically analyse this space, and concretely investigate several algorithms that arise from this framework.",0
"This paper presents a novel conceptual framework for off-policy reinforcement learning that centers around conditional expectations of importance sampling ratios. By utilizing this framework, we are able to gain fresh insights and comprehension into current off-policy algorithms, while also uncovering a wide range of unexplored algorithms. Furthermore, we conduct a theoretical analysis of this expansive algorithmic space and perform practical investigations of various algorithms derived from this framework.",1
"3D ultrasound (US) is widely used due to its rich diagnostic information, portability and low cost. Automated standard plane (SP) localization in US volume not only improves efficiency and reduces user-dependence, but also boosts 3D US interpretation. In this study, we propose a novel Multi-Agent Reinforcement Learning (MARL) framework to localize multiple uterine SPs in 3D US simultaneously. Our contribution is two-fold. First, we equip the MARL with a one-shot neural architecture search (NAS) module to obtain the optimal agent for each plane. Specifically, Gradient-based search using Differentiable Architecture Sampler (GDAS) is employed to accelerate and stabilize the training process. Second, we propose a novel collaborative strategy to strengthen agents' communication. Our strategy uses recurrent neural network (RNN) to learn the spatial relationship among SPs effectively. Extensively validated on a large dataset, our approach achieves the accuracy of 7.05 degree/2.21mm, 8.62 degree/2.36mm and 5.93 degree/0.89mm for the mid-sagittal, transverse and coronal plane localization, respectively. The proposed MARL framework can significantly increase the plane localization accuracy and reduce the computational cost and model size.",0
"Due to its diagnostic information, portability, and low cost, 3D ultrasound (US) is widely utilized. Automated standard plane (SP) localization within US volume enhances efficiency, reduces user-dependence, and improves 3D US interpretation. This study introduces a novel Multi-Agent Reinforcement Learning (MARL) framework to simultaneously localize multiple uterine SPs in 3D US. Our approach has two contributions. Firstly, we use a one-shot neural architecture search (NAS) module to obtain the optimal agent for each plane, employing Gradient-based search using Differentiable Architecture Sampler (GDAS) to expedite and stabilize the training process. Secondly, we propose a novel collaborative strategy that leverages recurrent neural network (RNN) to effectively learn the spatial relationship among SPs. Validated extensively on a large dataset, our approach achieves high accuracy for mid-sagittal, transverse, and coronal plane localizations. The proposed MARL framework enhances plane localization accuracy while reducing computational cost and model size.",1
"Reinforcement learning algorithms can show strong variation in performance between training runs with different random seeds. In this paper we explore how this affects hyperparameter optimization when the goal is to find hyperparameter settings that perform well across random seeds. In particular, we benchmark whether it is better to explore a large quantity of hyperparameter settings via pruning of bad performers, or if it is better to aim for quality of collected results by using repetitions. For this we consider the Successive Halving, Random Search, and Bayesian Optimization algorithms, the latter two with and without repetitions. We apply these to tuning the PPO2 algorithm on the Cartpole balancing task and the Inverted Pendulum Swing-up task. We demonstrate that pruning may negatively affect the optimization and that repeated sampling does not help in finding hyperparameter settings that perform better across random seeds. From our experiments we conclude that Bayesian optimization with a noise robust acquisition function is the best choice for hyperparameter optimization in reinforcement learning tasks.",0
"The performance of reinforcement learning algorithms can vary greatly depending on the random seed used in training. This study examines how this variation affects hyperparameter optimization when seeking to find settings that perform well across random seeds. The study compares the effectiveness of exploring a large number of hyperparameter settings through pruning bad performers versus aiming for quality results through repetitions, using the Successive Halving, Random Search, and Bayesian Optimization algorithms (with and without repetitions) to tune the PPO2 algorithm on the Cartpole balancing and Inverted Pendulum Swing-up tasks. The results show that pruning may have a negative impact on optimization, and that repeated sampling does not improve the search for hyperparameter settings that perform better across random seeds. The study concludes that Bayesian optimization with a noise robust acquisition function is the best option for hyperparameter optimization in reinforcement learning tasks.",1
"Distribution and sample models are two popular model choices in model-based reinforcement learning (MBRL). However, learning these models can be intractable, particularly when the state and action spaces are large. Expectation models, on the other hand, are relatively easier to learn due to their compactness and have also been widely used for deterministic environments. For stochastic environments, it is not obvious how expectation models can be used for planning as they only partially characterize a distribution. In this paper, we propose a sound way of using approximate expectation models for MBRL. In particular, we 1) show that planning with an expectation model is equivalent to planning with a distribution model if the state value function is linear in state features, 2) analyze two common parametrization choices for approximating the expectation: linear and non-linear expectation models, 3) propose a sound model-based policy evaluation algorithm and present its convergence results, and 4) empirically demonstrate the effectiveness of the proposed planning algorithm.",0
"Model-based reinforcement learning (MBRL) offers two model choices: distribution models and sample models. However, learning these models can be difficult, especially in cases where the state and action spaces are large. Expectation models are a more manageable option, as they are compact and are commonly used in deterministic environments. However, they are not as useful in stochastic environments as they only provide a partial characterization of the distribution. This paper proposes a way to use approximate expectation models for MBRL. Specifically, the paper presents: 1) an equivalence between planning with an expectation model and planning with a distribution model if the state value function is linear in state features, 2) an analysis of two parametrization choices for approximating the expectation: linear and non-linear expectation models, 3) a model-based policy evaluation algorithm with convergence results, and 4) empirical evidence demonstrating the effectiveness of the proposed planning algorithm.",1
"There has been considerable and growing interest in applying machine learning for cyber defenses. One promising approach has been to apply natural language processing techniques to analyze logs data for suspicious behavior. A natural question arises to how robust these systems are to adversarial attacks. Defense against sophisticated attack is of particular concern for cyber defenses. In this paper, we develop a testing framework to evaluate adversarial robustness of machine learning cyber defenses, particularly those focused on log data. Our framework uses techniques from deep reinforcement learning and adversarial natural language processing. We validate our framework using a publicly available dataset and demonstrate that our adversarial attack does succeed against the target systems, revealing a potential vulnerability. We apply our framework to analyze the influence of different levels of dropout regularization and find that higher dropout levels increases robustness. Moreover 90% dropout probability exhibited the highest level of robustness by a significant margin, which suggests unusually high dropout may be necessary to properly protect against adversarial attacks.",0
"Machine learning for cyber defenses has garnered significant interest, with a promising technique being the application of natural language processing to analyze logs data for suspicious activity. However, concerns have arisen regarding the ability of these systems to withstand adversarial attacks, particularly in the face of sophisticated attacks. In this study, we establish a testing framework that utilizes deep reinforcement learning and adversarial natural language processing to evaluate the robustness of machine learning cyber defenses, focusing on log data. Our framework successfully executes an adversarial attack on the target systems, revealing a potential vulnerability. We also analyze the impact of varying levels of dropout regularization and discover that higher dropout levels increase robustness. Notably, a dropout probability of 90% exhibits the highest level of robustness, indicating that unusually high dropout may be necessary for proper protection against adversarial attacks.",1
"Video summarization aims at generating concise video summaries from the lengthy videos, to achieve better user watching experience. Due to the subjectivity, purely supervised methods for video summarization may bring the inherent errors from the annotations. To solve the subjectivity problem, we study the general user summarization process. General users usually watch the whole video, compare interesting clips and select some clips to form a final summary. Inspired by the general user behaviours, we formulate the summarization process as multiple sequential decision-making processes, and propose Comparison-Selection Network (CoSNet) based on multi-agent reinforcement learning. Each agent focuses on a video clip and constantly changes its focus during the iterations, and the final focus clips of all agents form the summary. The comparison network provides the agent with the visual feature from clips and the chronological feature from the past round, while the selection network of the agent makes decisions on the change of its focus clip. The specially designed unsupervised reward and supervised reward together contribute to the policy advancement, each containing local and global parts. Extensive experiments on two benchmark datasets show that CoSNet outperforms state-of-the-art unsupervised methods with the unsupervised reward and surpasses most supervised methods with the complete reward.",0
"The goal of video summarization is to create brief summaries of lengthy videos in order to enhance the viewing experience for users. The use of purely supervised methods for video summarization can lead to errors due to subjectivity. To address this, we have studied the general user summarization process, which involves watching the entire video, comparing interesting clips, and selecting clips to include in the final summary. We have formulated the summarization process as multiple sequential decision-making processes, and introduced the Comparison-Selection Network (CoSNet) based on multi-agent reinforcement learning. Each agent focuses on a different video clip and changes focus during the process to create the final summary. The comparison network provides visual and chronological features, while the selection network determines which clip to focus on. The unsupervised and supervised rewards contribute to policy advancement. Our experiments on benchmark datasets demonstrate that CoSNet outperforms unsupervised methods with the unsupervised reward and most supervised methods with the complete reward.",1
"Off-policy evaluation (OPE) in reinforcement learning is an important problem in settings where experimentation is limited, such as education and healthcare. But, in these very same settings, observed actions are often confounded by unobserved variables making OPE even more difficult. We study an OPE problem in an infinite-horizon, ergodic Markov decision process with unobserved confounders, where states and actions can act as proxies for the unobserved confounders. We show how, given only a latent variable model for states and actions, policy value can be identified from off-policy data. Our method involves two stages. In the first, we show how to use proxies to estimate stationary distribution ratios, extending recent work on breaking the curse of horizon to the confounded setting. In the second, we show optimal balancing can be combined with such learned ratios to obtain policy value while avoiding direct modeling of reward functions. We establish theoretical guarantees of consistency, and benchmark our method empirically.",0
"Reinforcement learning faces a significant challenge with off-policy evaluation (OPE) in fields like healthcare and education, where experimentation is limited. This challenge is further complicated by unobserved variables that confound observed actions. In this study, we examine an OPE problem in an infinite-horizon, ergodic Markov decision process that contains unobserved confounders, where states and actions can serve as proxies for the unobserved variables. Our approach involves two stages. In the first stage, we use proxies to estimate stationary distribution ratios, extending the previous work on breaking the curse of horizon to the confounded setting. In the second stage, we show how optimal balancing can be combined with the learned ratios to achieve policy value without direct modeling of reward functions. We establish theoretical guarantees of consistency and empirically benchmark our method.",1
"The goal of neural-symbolic computation is to integrate the connectionist and symbolist paradigms. Prior methods learn the neural-symbolic models using reinforcement learning (RL) approaches, which ignore the error propagation in the symbolic reasoning module and thus converge slowly with sparse rewards. In this paper, we address these issues and close the loop of neural-symbolic learning by (1) introducing the \textbf{grammar} model as a \textit{symbolic prior} to bridge neural perception and symbolic reasoning, and (2) proposing a novel \textbf{back-search} algorithm which mimics the top-down human-like learning procedure to propagate the error through the symbolic reasoning module efficiently. We further interpret the proposed learning framework as maximum likelihood estimation using Markov chain Monte Carlo sampling and the back-search algorithm as a Metropolis-Hastings sampler. The experiments are conducted on two weakly-supervised neural-symbolic tasks: (1) handwritten formula recognition on the newly introduced HWF dataset; (2) visual question answering on the CLEVR dataset. The results show that our approach significantly outperforms the RL methods in terms of performance, converging speed, and data efficiency. Our code and data are released at \url{https://liqing-ustc.github.io/NGS}.",0
"The aim of neural-symbolic computation is to combine the connectionist and symbolist paradigms. Previous techniques utilized reinforcement learning (RL) to train neural-symbolic models, but this approach disregards error propagation in the symbolic reasoning component, resulting in slow convergence and sparse rewards. This paper addresses these challenges by (1) introducing a \textbf{grammar} model as a \textit{symbolic prior} to link neural perception and symbolic reasoning, and (2) proposing a novel \textbf{back-search} algorithm that imitates the top-down human-like learning process to transmit errors through the symbolic reasoning module efficiently. We interpret the proposed learning framework as maximum likelihood estimation employing Markov chain Monte Carlo sampling and the back-search algorithm as a Metropolis-Hastings sampler. We evaluate our approach on two weakly-supervised neural-symbolic tasks, namely handwritten formula recognition on the newly introduced HWF dataset and visual question answering on the CLEVR dataset. The outcomes demonstrate that our method surpasses RL approaches in terms of performance, convergence speed, and data efficiency. Our code and data are available at \url{https://liqing-ustc.github.io/NGS}.",1
"Humans integrate multiple sensory modalities (e.g. visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. Experimental results on Atari games show that our new intrinsic motivation significantly outperforms several state-of-the-art baselines. We further visualize our noisy agents' behavior in a physics environment and demonstrate that our newly designed intrinsic reward leads to the emergence of physical interaction behaviors (e.g. contact with objects).",0
"To comprehend the physical world, humans utilize multiple sensory modalities, such as visual and audio. In this study, we suggest a unique form of intrinsic motivation for Reinforcement Learning (RL) that stimulates the agent to comprehend the causal impact of its actions through forecasting auditory events. Initially, the agent collects a small amount of acoustic data and employs K-means to detect fundamental auditory event clusters. Next, a neural network is trained to anticipate the auditory events, and the prediction errors are utilized as intrinsic rewards to govern RL exploration. The experimental findings on Atari games indicate that our new intrinsic motivation surpasses several existing state-of-the-art baselines. Additionally, we illustrate the conduct of our noisy agents in a physics environment and demonstrate that our newly developed intrinsic reward leads to the emergence of physical interaction behaviors, such as contact with objects.",1
"Bayesian strategies for contextual bandits have proved promising in single-state reinforcement learning tasks by modeling uncertainty using context information from the environment. In this paper, we propose Greedy Bandits with Sampled Context (GB-SC), a method for contextual multi-armed bandits to develop the prior from the context information using Thompson Sampling, and arm selection using an epsilon-greedy policy. The framework GB-SC allows for evaluation of context-reward dependency, as well as providing robustness for partially observable context vectors by leveraging the prior developed. Our experimental results show competitive performance on the Mushroom environment in terms of expected regret and expected cumulative regret, as well as insights on how each context subset affects decision-making.",0
"The utilization of Bayesian approaches in contextual bandits has shown potential in reinforcement learning tasks that involve a single state. This is achieved by incorporating context information from the environment to model uncertainty. In this paper, we introduce a method called Greedy Bandits with Sampled Context (GB-SC) for contextual multi-armed bandits. GB-SC utilizes Thompson Sampling to establish a prior from context information and an epsilon-greedy policy for arm selection. The GB-SC framework enables the assessment of the relationship between context and reward, and enhances the robustness of partially observable context vectors by leveraging the developed prior. Our empirical findings demonstrate GB-SC's competitive performance in the Mushroom environment in terms of expected regret and expected cumulative regret. Additionally, we provide insights on how each context subset influences decision-making.",1
"In reinforcement learning, it is typical to use the empirically observed transitions and rewards to estimate the value of a policy via either model-based or Q-fitting approaches. Although straightforward, these techniques in general yield biased estimates of the true value of the policy. In this work, we investigate the potential for statistical bootstrapping to be used as a way to take these biased estimates and produce calibrated confidence intervals for the true value of the policy. We identify conditions - specifically, sufficient data size and sufficient coverage - under which statistical bootstrapping in this setting is guaranteed to yield correct confidence intervals. In practical situations, these conditions often do not hold, and so we discuss and propose mechanisms that can be employed to mitigate their effects. We evaluate our proposed method and show that it can yield accurate confidence intervals in a variety of conditions, including challenging continuous control environments and small data regimes.",0
"Typically, in reinforcement learning, observed transitions and rewards are utilized to estimate the value of a policy. This is done through model-based or Q-fitting approaches, which often yield biased estimations of the policy's true value. In this study, we examine the possibility of using statistical bootstrapping to create calibrated confidence intervals for the true value of the policy from these biased estimations. We identify the conditions necessary for statistical bootstrapping to yield correct confidence intervals, including adequate data size and coverage. However, in practical scenarios, these conditions are frequently not met, and we propose mechanisms to minimize their impact. Our method is evaluated and proven to produce accurate confidence intervals in various settings, including challenging continuous control environments and situations with limited data.",1
"We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.",0
"Procgen Benchmark is a collection of 16 game-like environments that have been procedurally generated to evaluate the sample efficiency and generalization of reinforcement learning. We anticipate that the availability of top-notch training environments will benefit the community, and we have provided precise experimental procedures for utilizing this benchmark. Through empirical evidence, we have established that a broad range of environment distributions is necessary to train and assess RL agents effectively, thereby promoting the widespread use of procedural content generation. Subsequently, we employed this benchmark to examine the impact of increasing model size and observed that larger models improve both sample efficiency and generalization significantly.",1
"There exists a Classification accuracy gap of about 20% between our best methods of generating Unsupervisedly Learned Representations and the accuracy rates achieved by (naturally Unsupervisedly Learning) humans. We are at our fourth decade at least in search of this class of paradigms. It thus may well be that we are looking in the wrong direction. We present in this paper a possible solution to this puzzle. We demonstrate that Reinforcement Learning schemes can learn representations, which may be used for Pattern Recognition tasks such as Classification, achieving practically the same accuracy as that of humans. Our main modest contribution lies in the observations that: a. when applied to a real world environment (e.g. nature itself) Reinforcement Learning does not require labels, and thus may be considered a natural candidate for the long sought, accuracy competitive Unsupervised Learning method, and b. in contrast, when Reinforcement Learning is applied in a simulated or symbolic processing environment (e.g. a computer program) it does inherently require labels and should thus be generally classified, with some exceptions, as Supervised Learning. The corollary of these observations is that further search for Unsupervised Learning competitive paradigms which may be trained in simulated environments like many of those found in research and applications may be futile.",0
"Our current methods of generating Unsupervisedly Learned Representations have a Classification accuracy gap of approximately 20% compared to humans who naturally Unsupervisedly Learn. Despite searching for decades, we may have been looking in the wrong direction. Our paper proposes a solution to this problem by demonstrating that Reinforcement Learning schemes can learn representations and achieve comparable accuracy to humans in Pattern Recognition tasks such as Classification. Our observation is that Reinforcement Learning in a real-world environment does not require labels, making it a promising candidate for an accuracy competitive Unsupervised Learning method. In contrast, when applied in a simulated or symbolic processing environment, Reinforcement Learning inherently requires labels and should be classified as Supervised Learning, with some exceptions. Therefore, the continued search for Unsupervised Learning competitive paradigms trained in simulated environments may be futile.",1
"Reinforcement learning is showing great potentials in robotics applications, including autonomous driving, robot manipulation and locomotion. However, with complex uncertainties in the real-world environment, it is difficult to guarantee the successful generalization and sim-to-real transfer of learned policies theoretically. In this paper, we introduce and extend the idea of robust stability and $H_\infty$ control to design policies with both stability and robustness guarantee. Specifically, a sample-based approach for analyzing the Lyapunov stability and performance robustness of a learning-based control system is proposed. Based on the theoretical results, a maximum entropy algorithm is developed for searching Lyapunov function and designing a policy with provable robust stability guarantee. Without any specific domain knowledge, our method can find a policy that is robust to various uncertainties and generalizes well to different test environments. In our experiments, we show that our method achieves better robustness to both large impulsive disturbances and parametric variations in the environment than the state-of-art results in both robust and generic RL, as well as classic control. Anonymous code is available to reproduce the experimental results at https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL.",0
"Robotics applications such as autonomous driving, robot manipulation, and locomotion have great potential for reinforcement learning. However, the real-world environment is complex and uncertain, making it difficult to guarantee successful generalization and transfer of learned policies. This paper presents an extension of the idea of robust stability and $H_\infty$ control to design policies that offer both stability and robustness guarantees. A sample-based approach for analyzing the Lyapunov stability and performance robustness of a learning-based control system is proposed. Using this approach, a maximum entropy algorithm is developed to search for Lyapunov function and design a policy with a provable robust stability guarantee. Our method does not require specific domain knowledge, and it can find a policy that is robust to various uncertainties and generalizes well to different test environments. Our experiments show that our method achieves better robustness than state-of-the-art results in both robust and generic RL, as well as classic control, with reproducible code available at https://github.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL.",1
"Although duality is used extensively in certain fields, such as supervised learning in machine learning, it has been much less explored in others, such as reinforcement learning (RL). In this paper, we show how duality is involved in a variety of RL work, from that which spearheaded the field, such as Richard Bellman's value iteration, to that which was done within just the past few years yet has already had significant impact, such as TRPO, A3C, and GAIL. We show that duality is not uncommon in reinforcement learning, especially when value iteration, or dynamic programming, is used or when first or second order approximations are made to transform initially intractable problems into tractable convex programs.",0
"Although duality has been extensively used in certain fields, like supervised learning in machine learning, it has not been explored as much in other fields, such as reinforcement learning. This paper demonstrates how duality is involved in various reinforcement learning works, ranging from Richard Bellman's value iteration, which pioneered the field, to more recent works like TRPO, A3C, and GAIL, which have already shown significant impact. The paper highlights that duality is prevalent in reinforcement learning, particularly when dynamic programming or value iteration is utilized, or when first or second order approximations are employed to convert initially complex problems into manageable convex programs.",1
"Deep Q-learning algorithms often suffer from poor gradient estimations with an excessive variance, resulting in unstable training and poor sampling efficiency. Stochastic variance-reduced gradient methods such as SVRG have been applied to reduce the estimation variance (Zhao et al. 2019). However, due to the online instance generation nature of reinforcement learning, directly applying SVRG to deep Q-learning is facing the problem of the inaccurate estimation of the anchor points, which dramatically limits the potentials of SVRG. To address this issue and inspired by the recursive gradient variance reduction algorithm SARAH (Nguyen et al. 2017), this paper proposes to introduce the recursive framework for updating the stochastic gradient estimates in deep Q-learning, achieving a novel algorithm called SRG-DQN. Unlike the SVRG-based algorithms, SRG-DQN designs a recursive update of the stochastic gradient estimate. The parameter update is along an accumulated direction using the past stochastic gradient information, and therefore can get rid of the estimation of the full gradients as the anchors. Additionally, SRG-DQN involves the Adam process for further accelerating the training process. Theoretical analysis and the experimental results on well-known reinforcement learning tasks demonstrate the efficiency and effectiveness of the proposed SRG-DQN algorithm.",0
"The variance in gradient estimations often leads to unstable training and poor sampling efficiency in Deep Q-learning algorithms. Researchers have tried to solve this issue by using stochastic variance-reduced gradient methods like SVRG. However, applying SVRG directly to deep Q-learning is challenging due to the inaccurate estimation of anchor points caused by online instance generation. To address this problem, this study proposes a recursive framework inspired by SARAH to update stochastic gradient estimates in deep Q-learning, creating a new algorithm called SRG-DQN. This algorithm uses a recursive update of stochastic gradient estimates along an accumulated direction using past information, eliminating the need for full gradient estimation as anchors. Additionally, SRG-DQN incorporates the Adam process to speed up the training process. Theoretical analysis and experiments on well-known reinforcement learning tasks confirm the efficiency and effectiveness of SRG-DQN.",1
"Standard reinforcement learning (RL) aims to find an optimal policy that identifies the best action for each state. However, in healthcare settings, many actions may be near-equivalent with respect to the reward (e.g., survival). We consider an alternative objective -- learning set-valued policies to capture near-equivalent actions that lead to similar cumulative rewards. We propose a model-free algorithm based on temporal difference learning and a near-greedy heuristic for action selection. We analyze the theoretical properties of the proposed algorithm, providing optimality guarantees and demonstrate our approach on simulated environments and a real clinical task. Empirically, the proposed algorithm exhibits good convergence properties and discovers meaningful near-equivalent actions. Our work provides theoretical, as well as practical, foundations for clinician/human-in-the-loop decision making, in which humans (e.g., clinicians, patients) can incorporate additional knowledge (e.g., side effects, patient preference) when selecting among near-equivalent actions.",0
"The typical goal of reinforcement learning (RL) is to locate the most effective action for each state, but this may not be feasible in healthcare settings where multiple actions offer similar rewards, such as survival. To address this issue, we suggest an alternative approach that involves learning set-valued policies that capture near-equivalent actions leading to comparable cumulative rewards. We have developed a model-free algorithm that utilizes temporal difference learning and a near-greedy heuristic for action selection. Our algorithm has been analyzed theoretically, and we guarantee optimality while demonstrating its effectiveness on simulated environments and a genuine clinical task. It has strong convergence properties and can identify significant near-equivalent actions. Our findings provide theoretical and practical support for clinician and human-in-the-loop decision-making processes, allowing humans to incorporate additional knowledge such as side effects and patient preferences when selecting among similar actions.",1
"The combination of Monte-Carlo tree search (MCTS) with deep reinforcement learning has led to significant advances in artificial intelligence. However, AlphaZero, the current state-of-the-art MCTS algorithm, still relies on handcrafted heuristics that are only partially understood. In this paper, we show that AlphaZero's search heuristics, along with other common ones such as UCT, are an approximation to the solution of a specific regularized policy optimization problem. With this insight, we propose a variant of AlphaZero which uses the exact solution to this policy optimization problem, and show experimentally that it reliably outperforms the original algorithm in multiple domains.",0
"Artificial intelligence has made notable progress due to the integration of Monte-Carlo tree search (MCTS) and deep reinforcement learning. However, AlphaZero, which is currently considered the most advanced MCTS algorithm, still depends on incompletely understood heuristics created by hand. This study demonstrates that AlphaZero's search heuristics, as well as other common ones like UCT, are an approximation of a specific regularized policy optimization problem's solution. Based on this finding, we propose a modified version of AlphaZero that employs the precise solution to this policy optimization problem. Our experimental results indicate that our approach surpasses the original algorithm's performance in numerous domains.",1
"Stabilizing the unknown dynamics of a control system and minimizing regret in control of an unknown system are among the main goals in control theory and reinforcement learning. In this work, we pursue both these goals for adaptive control of linear quadratic regulators (LQR). Prior works accomplish either one of these goals at the cost of the other one. The algorithms that are guaranteed to find a stabilizing controller suffer from high regret, whereas algorithms that focus on achieving low regret assume the presence of a stabilizing controller at the early stages of agent-environment interaction. In the absence of such a stabilizing controller, at the early stages, the lack of reasonable model estimates needed for (i) strategic exploration and (ii) design of controllers that stabilize the system, results in regret that scales exponentially in the problem dimensions. We propose a framework for adaptive control that exploits the characteristics of linear dynamical systems and deploys additional exploration in the early stages of agent-environment interaction to guarantee sooner design of stabilizing controllers. We show that for the classes of controllable and stabilizable LQRs, where the latter is a generalization of prior work, these methods achieve $\tilde{\mathcal{O}}(\sqrt{T})$ regret with a polynomial dependence in the problem dimensions.",0
"The main objectives in control theory and reinforcement learning are to stabilize unknown dynamics of a control system and minimize regret in controlling an unknown system. This study aims to achieve both goals for adaptive control of linear quadratic regulators (LQR). Previous studies have achieved one of these goals at the expense of the other. Algorithms that promise to find a stabilizing controller come with high regret, while those that focus on low regret assume the presence of a stabilizing controller at the initial stages of agent-environment interaction. Early on, if such a stabilizing controller is absent, the lack of model estimates needed for strategic exploration and controller design leads to regret that increases exponentially with the problem's dimensions. To address this, the authors propose a framework for adaptive control that utilizes the features of linear dynamical systems and additional exploration during the early stages of agent-environment interaction to design stabilizing controllers sooner. The methods employed achieve $\tilde{\mathcal{O}}(\sqrt{T})$ regret, with a polynomial dependence on the problem dimensions, for the controllable and stabilizable LQR classes, which is an extension of prior work.",1
"Sequential decision making, commonly formalized as Markov Decision Process optimization, is a key challenge in artificial intelligence. Two successful approaches to MDP optimization are planning and reinforcement learning. Both research fields largely have their own research communities. However, if both research fields solve the same problem, then we should be able to disentangle the common factors in their solution approaches. Therefore, this paper presents a unifying framework for reinforcement learning and planning (FRAP), which identifies the underlying dimensions on which any planning or learning algorithm has to decide. At the end of the paper, we compare - in a single table - a variety of well-known planning, model-free and model-based RL algorithms along the dimensions of our framework, illustrating the validity of the framework. Altogether, FRAP provides deeper insight into the algorithmic space of planning and reinforcement learning, and also suggests new approaches to integration of both fields.",0
"Artificial intelligence faces a significant challenge in the optimization of Markov Decision Processes, which involve sequential decision making. Planning and reinforcement learning are two successful approaches to MDP optimization, with distinct research communities. However, as both fields aim to solve the same problem, it may be possible to identify the common factors in their solution approaches. To this end, we introduce a unified framework for planning and reinforcement learning (FRAP), which highlights the underlying dimensions that all planning and learning algorithms must consider. We evaluate a range of planning, model-free, and model-based RL algorithms using our framework, and present the results in a single table. Overall, FRAP provides a deeper understanding of the algorithmic space of planning and reinforcement learning and suggests new avenues for integrating both fields.",1
"Patients with diabetes who are self-monitoring have to decide right before each meal how much insulin they should take. A standard bolus advisor exists, but has never actually been proven to be optimal in any sense. We challenged this rule applying Reinforcement Learning techniques on data simulated with T1DM, an FDA-approved simulator developed by Kovatchev et al. modeling the gluco-insulin interaction. Results show that the optimal bolus rule is fairly different from the standard bolus advisor, and if followed can actually avoid hypoglycemia episodes.",0
"Individuals with diabetes who are monitoring their own condition must determine the appropriate amount of insulin to take immediately prior to each meal. Although a standard bolus advisor is available, it has not been shown to be ideal in any way. To test this assumption, we utilized Reinforcement Learning techniques on data that was simulated using T1DM, an FDA-approved simulator created by Kovatchev et al. which models the interaction between glucose and insulin. The outcomes indicate that the best bolus rule varies significantly from the standard bolus advisor, and if implemented, can prevent episodes of hypoglycemia.",1
"In order to deal with the curse of dimensionality in reinforcement learning (RL), it is common practice to make parametric assumptions where values or policies are functions of some low dimensional feature space. This work focuses on the representation learning question: how can we learn such features? Under the assumption that the underlying (unknown) dynamics correspond to a low rank transition matrix, we show how the representation learning question is related to a particular non-linear matrix decomposition problem. Structurally, we make precise connections between these low rank MDPs and latent variable models, showing how they significantly generalize prior formulations for representation learning in RL. Algorithmically, we develop FLAMBE, which engages in exploration and representation learning for provably efficient RL in low rank transition models.",0
"To tackle the problem of the curse of dimensionality in reinforcement learning, it is common to use parametric assumptions where values or policies are expressed as functions of a low dimensional feature space. This study focuses on the question of how to learn these features, and proposes a solution based on the assumption that the underlying dynamics of the system can be represented by a low rank transition matrix. By linking this assumption to a non-linear matrix decomposition problem, the study establishes a connection between low rank Markov decision processes (MDPs) and latent variable models, which is a significant improvement on previous approaches to representation learning in RL. The study also introduces an algorithm called FLAMBE, which combines exploration and representation learning for efficient RL in low rank transition models.",1
"Batch reinforcement learning (RL) is important to apply RL algorithms to many high stakes tasks. Doing batch RL in a way that yields a reliable new policy in large domains is challenging: a new decision policy may visit states and actions outside the support of the batch data, and function approximation and optimization with limited samples can further increase the potential of learning policies with overly optimistic estimates of their future performance. Recent algorithms have shown promise but can still be overly optimistic in their expected outcomes. Theoretical work that provides strong guarantees on the performance of the output policy relies on a strong concentrability assumption, that makes it unsuitable for cases where the ratio between state-action distributions of behavior policy and some candidate policies is large. This is because in the traditional analysis, the error bound scales up with this ratio. We show that a small modification to Bellman optimality and evaluation back-up to take a more conservative update can have much stronger guarantees. In certain settings, they can find the approximately best policy within the state-action space explored by the batch data, without requiring a priori assumptions of concentrability. We highlight the necessity of our conservative update and the limitations of previous algorithms and analyses by illustrative MDP examples, and demonstrate an empirical comparison of our algorithm and other state-of-the-art batch RL baselines in standard benchmarks.",0
"The use of batch reinforcement learning (RL) is crucial for implementing RL algorithms in high-risk tasks. However, generating a dependable new policy in extensive domains through batch RL can be difficult. This is because the new decision policy may explore states and actions that are not covered by the batch data, and function approximation and optimization with limited samples can lead to overestimation of future performance. Although recent algorithms have shown promise, they can still be overly optimistic in their expected results. Theoretical work that guarantees strong performance of the output policy assumes a high concentrability, which is unsuitable for situations where the ratio between state-action distributions of the behavior policy and candidate policies is large. This is because, in conventional analysis, the error bound increases with this ratio. However, we demonstrate that modifying the Bellman optimality and evaluation back-up to include a more cautious update can provide much stronger guarantees. In certain scenarios, this approach can identify the best policy within the state-action space explored by the batch data without assuming concentrability. By using illustrative MDP examples, we emphasize the importance of the conservative update and the limitations of previous algorithms and analyses. We also compare our algorithm with other state-of-the-art batch RL baselines in standard benchmarks.",1
"Neural Architecture Search (NAS) achieved many breakthroughs in recent years. In spite of its remarkable progress, many algorithms are restricted to particular search spaces. They also lack efficient mechanisms to reuse knowledge when confronting multiple tasks. These challenges preclude their applicability, and motivate our proposal of CATCH, a novel Context-bAsed meTa reinforcement learning (RL) algorithm for transferrable arChitecture searcH. The combination of meta-learning and RL allows CATCH to efficiently adapt to new tasks while being agnostic to search spaces. CATCH utilizes a probabilistic encoder to encode task properties into latent context variables, which then guide CATCH's controller to quickly ""catch"" top-performing networks. The contexts also assist a network evaluator in filtering inferior candidates and speed up learning. Extensive experiments demonstrate CATCH's universality and search efficiency over many other widely-recognized algorithms. It is also capable of handling cross-domain architecture search as competitive networks on ImageNet, COCO, and Cityscapes are identified. This is the first work to our knowledge that proposes an efficient transferrable NAS solution while maintaining robustness across various settings.",0
"Recent years have seen numerous advancements in Neural Architecture Search (NAS). Despite this progress, many NAS algorithms are confined to specific search spaces and lack efficient mechanisms for reusing knowledge when tackling multiple tasks. These limitations hinder their practicality and inspired the development of a new algorithm called CATCH. CATCH is a Context-bAsed meTa reinforcement learning (RL) approach to architecture search that combines meta-learning and RL to adapt to new tasks efficiently and remain independent of search spaces. CATCH utilizes a probabilistic encoder to encode task properties into latent context variables, guiding the controller to identify top-performing networks quickly. The contexts also assist the network evaluator in filtering out inferior candidates and accelerate learning. Extensive experiments validate CATCH's effectiveness over many other widely-recognized algorithms, demonstrating its capability of handling cross-domain architecture search. CATCH is the first efficient transferrable NAS solution that maintains robustness across multiple settings.",1
"Augmenting reinforcement learning with imitation learning is often hailed as a method by which to improve upon learning from scratch. However, most existing methods for integrating these two techniques are subject to several strong assumptions---chief among them that information about demonstrator actions is available. In this paper, we investigate the extent to which this assumption is necessary by introducing and evaluating reinforced inverse dynamics modeling (RIDM), a novel paradigm for combining imitation from observation (IfO) and reinforcement learning with no dependence on demonstrator action information. Moreover, RIDM requires only a single demonstration trajectory and is able to operate directly on raw (unaugmented) state features. We find experimentally that RIDM performs favorably compared to a baseline approach for several tasks in simulation as well as for tasks on a real UR5 robot arm. Experiment videos can be found at https://sites.google.com/view/ridm-reinforced-inverse-dynami.",0
"The combination of reinforcement learning and imitation learning is commonly recognized as a way to enhance learning from scratch. Nonetheless, many current methods that integrate these techniques rely on strong assumptions, primarily the availability of information regarding demonstrator actions. This study investigates the necessity of this assumption by presenting and evaluating a new approach called reinforced inverse dynamics modeling (RIDM). RIDM combines imitation from observation (IfO) and reinforcement learning without depending on demonstrator action information. Additionally, RIDM only requires a single demonstration trajectory and can work directly on unaugmented state features. The experimental results demonstrate that RIDM performs better than a baseline method for several simulated tasks and real UR5 robot arm tasks. Videos of the experiments can be viewed at https://sites.google.com/view/ridm-reinforced-inverse-dynami.",1
"In this work, we study adaptive data-guided traffic planning and control using Reinforcement Learning (RL). We shift from the plain use of classic methods towards state-of-the-art in deep RL community. We embed several recent techniques in our algorithm that improve the original Deep Q-Networks (DQN) for discrete control and discuss the traffic-related interpretations that follow. We propose a novel DQN-based algorithm for Traffic Control (called TC-DQN+) as a tool for fast and more reliable traffic decision-making. We introduce a new form of reward function which is further discussed using illustrative examples with comparisons to traditional traffic control methods.",0
"Our research focuses on utilizing Reinforcement Learning (RL) for adaptive traffic planning and control. Instead of relying solely on traditional methods, we incorporate cutting-edge techniques from the deep RL field. By enhancing the Deep Q-Networks (DQN) approach for discrete control, we present the TC-DQN+ algorithm for more efficient and dependable traffic decision-making. To further illustrate our findings, we introduce a unique reward function and compare it to conventional traffic control methods through various examples.",1
"Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert's behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.",0
"To navigate an unfamiliar environment using natural language instructions, an agent must utilize Vision-and-Language Navigation (VLN). However, current dominant methods that rely on supervised learning tend to replicate expert behavior and struggle with unseen environments. Reinforcement Learning (RL) models offer better generalization but require significant manual reward engineering. To address these challenges, we present the Soft Expert Reward Learning (SERL) model for VLN. Our approach includes two components: the Soft Expert Distillation (SED) module that encourages expert-like behavior in a gentle manner, and the Self-Perceiving (SP) module that prioritizes reaching the destination efficiently. Our model surpasses existing methods in performance across VLN seen, unseen, and test splits, indicating its effectiveness in overcoming reward engineering and generalization issues.",1
"Deep convolutional neural networks demonstrate impressive results in the super-resolution domain. A series of studies concentrate on improving peak signal noise ratio (PSNR) by using much deeper layers, which are not friendly to constrained resources. Pursuing a trade-off between the restoration capacity and the simplicity of models is still non-trivial. Recent contributions are struggling to manually maximize this balance, while our work achieves the same goal automatically with neural architecture search. Specifically, we handle super-resolution with a multi-objective approach. We also propose an elastic search tactic at both micro and macro level, based on a hybrid controller that profits from evolutionary computation and reinforcement learning. Quantitative experiments help us to draw a conclusion that our generated models dominate most of the state-of-the-art methods with respect to the individual FLOPS.",0
"The super-resolution field has seen remarkable results from deep convolutional neural networks. However, many studies focus on enhancing peak signal noise ratio (PSNR) by utilizing deeper layers, which is not ideal for limited resources. Achieving a balance between model simplicity and restoration capacity remains a challenge. While recent efforts strive to manually achieve this equilibrium, our work utilizes neural architecture search to automatically attain the same objective. Our approach employs a multi-objective strategy to handle super-resolution and introduces an elastic search tactic at both micro and macro levels. This is facilitated by a hybrid controller that utilizes evolutionary computation and reinforcement learning. Our quantitative experiments reveal that the models generated by our approach surpass most of the current state-of-the-art methods in terms of individual FLOPS.",1
"Integer programming (IP) is a general optimization framework widely applicable to a variety of unstructured and structured problems arising in, e.g., scheduling, production planning, and graph optimization. As IP models many provably hard to solve problems, modern IP solvers rely on many heuristics. These heuristics are usually human-designed, and naturally prone to suboptimality. The goal of this work is to show that the performance of those solvers can be greatly enhanced using reinforcement learning (RL). In particular, we investigate a specific methodology for solving IPs, known as the Cutting Plane Method. This method is employed as a subroutine by all modern IP solvers. We present a deep RL formulation, network architecture, and algorithms for intelligent adaptive selection of cutting planes (aka cuts). Across a wide range of IP tasks, we show that the trained RL agent significantly outperforms human-designed heuristics, and effectively generalizes to 10X larger instances and across IP problem classes. The trained agent is also demonstrated to benefit the popular downstream application of cutting plane methods in Branch-and-Cut algorithm, which is the backbone of state-of-the-art commercial IP solvers.",0
"The optimization framework known as Integer Programming (IP) has broad applicability to both structured and unstructured problems in various fields, such as scheduling, production planning, and graph optimization. However, due to the complexity of many of these problems, IP solvers often rely on human-designed heuristics that can be suboptimal. To address this, this study proposes using reinforcement learning (RL) to enhance IP solver performance, specifically focusing on the Cutting Plane Method that is utilized by all modern IP solvers. The study presents a deep RL formulation, network architecture, and algorithms for adaptive selection of cutting planes. The results show that the trained RL agent significantly outperforms human-designed heuristics and generalizes well to larger instances and problem classes. Additionally, the trained agent is shown to benefit the Branch-and-Cut algorithm, which is used in commercial IP solvers.",1
"Background and motivation: Combining Deep Reinforcement Learning (Deep RL) and Health Systems Simulations has significant potential, for both research into improving Deep RL performance and safety, and in operational practice. While individual toolkits exist for Deep RL and Health Systems Simulations, no framework to integrate the two has been established.   Aim: Provide a framework for integrating Deep RL Networks with Health System Simulations, and to ensure this framework is compatible with Deep RL agents that have been developed and tested using OpenAI Gym.   Methods: We developed our framework based on the OpenAI Gym framework, and demonstrate its use on a simple hospital bed capacity model. We built the Deep RL agents using PyTorch, and the Hospital Simulatation using SimPy.   Results: We demonstrate example models using a Double Deep Q Network or a Duelling Double Deep Q Network as the Deep RL agent.   Conclusion: SimPy may be used to create Health System Simulations that are compatible with agents developed and tested on OpenAI Gym environments.   GitHub repository of code: https://github.com/MichaelAllen1966/learninghospital",0
"The combination of Deep Reinforcement Learning (Deep RL) and Health Systems Simulations has enormous potential for advancing both research into enhancing Deep RL performance and safety, as well as practical applications. Although separate toolkits exist for each, no established framework currently integrates the two. The objective is to create a framework that can combine the Deep RL Networks with Health System Simulations, ensuring it is compatible with Deep RL agents developed and tested using OpenAI Gym. The framework was developed using the OpenAI Gym framework and tested on a basic hospital bed capacity model, with PyTorch for Deep RL agents and SimPy for the Hospital Simulation. Results illustrate example models using either a Double Deep Q Network or a Duelling Double Deep Q Network as the Deep RL agent. In summary, SimPy can be utilized to construct Health System Simulations that are compatible with agents developed and tested on OpenAI Gym environments. Access to the code is available through the GitHub repository at https://github.com/MichaelAllen1966/learninghospital.",1
"This chapter studies emerging cyber-attacks on reinforcement learning (RL) and introduces a quantitative approach to analyze the vulnerabilities of RL. Focusing on adversarial manipulation on the cost signals, we analyze the performance degradation of TD($\lambda$) and $Q$-learning algorithms under the manipulation. For TD($\lambda$), the approximation learned from the manipulated costs has an approximation error bound proportional to the magnitude of the attack. The effect of the adversarial attacks on the bound does not depend on the choice of $\lambda$. In $Q$-learning, we show that $Q$-learning algorithms converge under stealthy attacks and bounded falsifications on cost signals. We characterize the relation between the falsified cost and the $Q$-factors as well as the policy learned by the learning agent which provides fundamental limits for feasible offensive and defensive moves. We propose a robust region in terms of the cost within which the adversary can never achieve the targeted policy. We provide conditions on the falsified cost which can mislead the agent to learn an adversary's favored policy. A case study of TD($\lambda$) learning is provided to corroborate the results.",0
"In this chapter, the focus is on examining the growing cyber-attacks on reinforcement learning (RL) and presenting a quantitative method to evaluate the vulnerabilities of RL. The primary area of focus is the manipulation of cost signals and the resulting impact on TD($\lambda$) and $Q$-learning algorithms. For TD($\lambda$), the approximation error bound of manipulated costs is proportional to the magnitude of the attack, while the attack's effect on the bound is independent of the choice of $\lambda$. The study shows that $Q$-learning algorithms converge under bounded falsifications and stealthy attacks on cost signals. The research characterizes the relationship between the falsified cost, the $Q$-factors, and the policy learned by the learning agent, providing the basis for feasible offensive and defensive tactics. Furthermore, a robust cost region is proposed, where an adversary cannot achieve the desired policy. The study also outlines the conditions under which falsified costs can mislead the agent into learning an adversary's favored policy. Finally, a case study of TD($\lambda$) learning is presented to validate the findings.",1
"This is a short communication on a Lyapunov function argument for softmax in bandit problems. There are a number of excellent papers coming out using differential equations for policy gradient algorithms in reinforcement learning \cite{agarwal2019optimality,bhandari2019global,mei2020global}. We give a short argument that gives a regret bound for the soft-max ordinary differential equation for bandit problems. We derive a similar result for a different policy gradient algorithm, again for bandit problems. For this second algorithm, it is possible to prove regret bounds in the stochastic case \cite{DW20}. At the end, we summarize some ideas and issues on deriving stochastic regret bounds for policy gradients.",0
"This brief message presents a Lyapunov function approach to the use of softmax in bandit problems. While some papers have utilized differential equations for policy gradient algorithms in reinforcement learning with great success, we offer a concise argument for the regret bound of the soft-max ordinary differential equation in bandit problems. Additionally, we provide a comparable outcome for a distinct policy gradient algorithm in the context of bandit problems, where stochastic regret bounds can be proven. Finally, we summarize some concepts and challenges related to deriving stochastic regret bounds for policy gradients.",1
"The Obstacle Tower Challenge is the task to master a procedurally generated chain of levels that subsequently get harder to complete. Whereas the most top performing entries of last year's competition used human demonstrations or reward shaping to learn how to cope with the challenge, we present an approach that performed competitively (placed 7th) but starts completely from scratch by means of Deep Reinforcement Learning with a relatively simple feed-forward deep network structure. We especially look at the generalization performance of the taken approach concerning different seeds and various visual themes that have become available after the competition, and investigate where the agent fails and why. Note that our approach does not possess a short-term memory like employing recurrent hidden states. With this work, we hope to contribute to a better understanding of what is possible with a relatively simple, flexible solution that can be applied to learning in environments featuring complex 3D visual input where the abstract task structure itself is still fairly simple.",0
"The challenge of the Obstacle Tower is to conquer a series of levels that increase in difficulty and are generated procedurally. In contrast to the previous year's top-performing entries, which employed techniques such as human demonstrations or reward shaping to surpass the challenge, we propose an approach that achieved competitive results (ranked 7th) without any prior knowledge, using Deep Reinforcement Learning and a basic feed-forward deep network structure. Our focus is on examining the generalizability of this approach under different seeds and varied visual themes that were introduced after the competition, as well as analyzing its weaknesses and causes of failure. It should be noted that our method does not use short-term memory, such as recurrent hidden states. Through this study, we aim to contribute to a better understanding of the potential of a simple and adaptable solution that can be utilized for learning in environments with complex 3D visual input, where the fundamental task structure remains relatively straightforward.",1
"The state-of-the-art machine learning approaches are based on classical von Neumann computing architectures and have been widely used in many industrial and academic domains. With the recent development of quantum computing, researchers and tech-giants have attempted new quantum circuits for machine learning tasks. However, the existing quantum computing platforms are hard to simulate classical deep learning models or problems because of the intractability of deep quantum circuits. Thus, it is necessary to design feasible quantum algorithms for quantum machine learning for noisy intermediate scale quantum (NISQ) devices. This work explores variational quantum circuits for deep reinforcement learning. Specifically, we reshape classical deep reinforcement learning algorithms like experience replay and target network into a representation of variational quantum circuits. Moreover, we use a quantum information encoding scheme to reduce the number of model parameters compared to classical neural networks. To the best of our knowledge, this work is the first proof-of-principle demonstration of variational quantum circuits to approximate the deep $Q$-value function for decision-making and policy-selection reinforcement learning with experience replay and target network. Besides, our variational quantum circuits can be deployed in many near-term NISQ machines.",0
"Previously, classical von Neumann computing architectures were used for state-of-the-art machine learning approaches that were implemented in various industrial and academic sectors. However, with the advancement of quantum computing, researchers and tech-giants have been experimenting with new quantum circuits for machine learning tasks. Unfortunately, the existing quantum computing platforms face difficulties in simulating classical deep learning models or problems because of the intractability of deep quantum circuits. Therefore, it is necessary to develop feasible quantum algorithms for quantum machine learning that can be executed on noisy intermediate scale quantum (NISQ) devices. This study focuses on exploring variational quantum circuits for deep reinforcement learning by reshaping classical deep reinforcement learning algorithms such as experience replay and target network into a representation of variational quantum circuits. Additionally, we use a quantum information encoding scheme to reduce the number of model parameters compared to classical neural networks. Our study is the first proof-of-principle demonstration of variational quantum circuits that can approximate the deep $Q$-value function for decision-making and policy-selection reinforcement learning with experience replay and target network. Furthermore, our variational quantum circuits can be implemented in many near-term NISQ machines.",1
"In this paper, three recently introduced reinforcement learning (RL) methods are used to generate human-interpretable policies for the cart-pole balancing benchmark. The novel RL methods learn human-interpretable policies in the form of compact fuzzy controllers and simple algebraic equations. The representations as well as the achieved control performances are compared with two classical controller design methods and three non-interpretable RL methods. All eight methods utilize the same previously generated data batch and produce their controller offline - without interaction with the real benchmark dynamics. The experiments show that the novel RL methods are able to automatically generate well-performing policies which are at the same time human-interpretable. Furthermore, one of the methods is applied to automatically learn an equation-based policy for a hardware cart-pole demonstrator by using only human-player-generated batch data. The solution generated in the first attempt already represents a successful balancing policy, which demonstrates the methods applicability to real-world problems.",0
"This paper utilizes three recently introduced reinforcement learning (RL) techniques to create policies for the cart-pole balancing benchmark that are easily interpretable by humans. The new RL methods develop human-interpretable policies through compact fuzzy controllers and simple algebraic equations, and their representations and control performances are compared to two classical controller design methods and three non-interpretable RL methods. All eight methods use the same previously gathered data batch and construct their controllers offline without real benchmark dynamics interaction. The experiments demonstrate that the novel RL methods can autonomously create well-performing policies that are also human-interpretable. Moreover, one of the methods is employed to automatically learn an equation-based policy for a hardware cart-pole demonstrator by using only human-player-generated batch data. The first attempt's solution already displays a successful balancing policy, indicating the method's practicality for real-world challenges.",1
"We present a novel algorithm -- convex natural evolutionary strategies (CoNES) -- for optimizing high-dimensional blackbox functions by leveraging tools from convex optimization and information geometry. CoNES is formulated as an efficiently-solvable convex program that adapts the evolutionary strategies (ES) gradient estimate to promote rapid convergence. The resulting algorithm is invariant to the parameterization of the belief distribution. Our numerical results demonstrate that CoNES vastly outperforms conventional blackbox optimization methods on a suite of functions used for benchmarking blackbox optimizers. Furthermore, CoNES demonstrates the ability to converge faster than conventional blackbox methods on a selection of OpenAI's MuJoCo reinforcement learning tasks for locomotion.",0
"A new algorithm called CoNES (convex natural evolutionary strategies) is introduced in this study for optimizing high-dimensional blackbox functions by incorporating tools from convex optimization and information geometry. CoNES is designed as an easily-solvable convex program that adjusts the evolutionary strategies (ES) gradient estimate to enhance rapid convergence. The algorithm remains unaffected by the parameterization of the belief distribution. Numerical experiments show that CoNES significantly outperforms traditional blackbox optimization techniques on a range of functions utilized for benchmarking blackbox optimizers. Additionally, CoNES has been found to converge more rapidly than conventional blackbox methods on a selection of OpenAI's MuJoCo reinforcement learning tasks for locomotion.",1
"Reinforcement Learning (RL) methods have been proven successful in solving manipulation tasks autonomously. However, RL is still not widely adopted on real robotic systems because working with real hardware entails additional challenges, especially when using rigid position-controlled manipulators. These challenges include the need for a robust controller to avoid undesired behavior, that risk damaging the robot and its environment, and constant supervision from a human operator. The main contributions of this work are, first, we proposed a learning-based force control framework combining RL techniques with traditional force control. Within said control scheme, we implemented two different conventional approaches to achieve force control with position-controlled robots; one is a modified parallel position/force control, and the other is an admittance control. Secondly, we empirically study both control schemes when used as the action space of the RL agent. Thirdly, we developed a fail-safe mechanism for safely training an RL agent on manipulation tasks using a real rigid robot manipulator. The proposed methods are validated on simulation and a real robot, an UR3 e-series robotic arm.",0
"Although Reinforcement Learning (RL) methods have been effective in solving manipulation tasks independently, they are not commonly used on real robotic systems due to additional challenges when working with actual hardware, particularly with rigid position-controlled manipulators. These challenges include the necessity for a robust controller to prevent undesired behavior that could damage the robot and its surroundings, as well as constant supervision from a human operator. This study's primary contributions include proposing a learning-based force control framework that combines RL techniques with traditional force control, implementing two conventional approaches for force control with position-controlled robots, studying both control schemes empirically as the action space of the RL agent, and developing a fail-safe mechanism for safely training an RL agent on manipulation tasks using a real rigid robot manipulator. The methods were tested and validated on both simulation and an UR3 e-series robotic arm.",1
"Patch-based attacks introduce a perceptible but localized change to the input that induces misclassification. A limitation of current patch-based black-box attacks is that they perform poorly for targeted attacks, and even for the less challenging non-targeted scenarios, they require a large number of queries. Our proposed PatchAttack is query efficient and can break models for both targeted and non-targeted attacks. PatchAttack induces misclassifications by superimposing small textured patches on the input image. We parametrize the appearance of these patches by a dictionary of class-specific textures. This texture dictionary is learned by clustering Gram matrices of feature activations from a VGG backbone. PatchAttack optimizes the position and texture parameters of each patch using reinforcement learning. Our experiments show that PatchAttack achieves > 99% success rate on ImageNet for a wide range of architectures, while only manipulating 3% of the image for non-targeted attacks and 10% on average for targeted attacks. Furthermore, we show that PatchAttack circumvents state-of-the-art adversarial defense methods successfully.",0
"The introduction of patch-based attacks causes a noticeable alteration in a specific area of the input, leading to incorrect classification. However, current patch-based black-box attacks have limitations in their effectiveness for targeted attacks and require a significant number of queries even for non-targeted scenarios. Our newly proposed PatchAttack is much more efficient in terms of queries and can successfully disrupt models for both targeted and non-targeted attacks. This is achieved by adding small textured patches onto the input image, which are parametrized by a dictionary of class-specific textures. The texture dictionary is created through clustering Gram matrices from a VGG backbone. PatchAttack then optimizes the position and texture parameters of each patch by using reinforcement learning. Our experiments demonstrate that PatchAttack can achieve a success rate of over 99% on ImageNet for various architectures, while only affecting 3% of the image for non-targeted attacks and on average 10% for targeted attacks. Additionally, PatchAttack has shown to surpass the latest adversarial defense methods.",1
"Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. However, existing methods have been focusing on learning fully decentralized value functions, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable Q-functions (NDQ) via communication minimization, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers are maximizing mutual information between agents' action selection and communication messages while minimizing the entropy of messages between agents. We show how to optimize these regularizers in a way that is easily integrated with existing value function factorization methods such as QMIX. Finally, we demonstrate that, on the StarCraft unit micromanagement benchmark, our framework significantly outperforms baseline methods and allows us to cut off more than $80\%$ of communication without sacrificing the performance. The videos of our experiments are available at https://sites.google.com/view/ndq.",0
"Multi-agent settings pose significant challenges for reinforcement learning, including scalability and non-stationarity. To overcome these challenges in collaborative multi-agent systems, value function factorization learning has emerged as a promising approach. However, current methods focus solely on fully decentralized value functions, which are not efficient for tasks requiring communication. This study presents a novel framework for learning nearly decomposable Q-functions (NDQ) through communication minimization. The agents act independently most of the time but periodically send messages to coordinate effectively. The framework combines value function factorization learning and communication learning by introducing two information-theoretic regularizers. These regularizers maximize the mutual information between agents' action selection and communication messages while minimizing the entropy of messages. The regularizers can be optimized alongside existing value function factorization methods such as QMIX. The framework significantly outperforms baseline methods on the StarCraft unit micromanagement benchmark, allowing for over 80% communication reduction without sacrificing performance. Experiment videos can be found at https://sites.google.com/view/ndq.",1
"The demand for same-day delivery (SDD) has increased rapidly in the last few years and has particularly boomed during the COVID-19 pandemic. Existing literature on the problem has focused on maximizing the utility, represented as the total number of expected requests served. However, a utility-driven solution results in unequal opportunities for customers to receive delivery service, raising questions about fairness. In this paper, we study the problem of achieving fairness in SDD. We construct a regional-level fairness constraint that ensures customers from different regions have an equal chance of being served. We develop a reinforcement learning model to learn policies that focus on both overall utility and fairness. Experimental results demonstrate the ability of our approach to mitigate the unfairness caused by geographic differences and constraints of resources, at both coarser and finer-grained level and with a small cost to utility. In addition, we simulate a real-world situation where the system is suddenly overwhelmed by a surge of requests, mimicking the COVID-19 scenario. Our model is robust to the systematic pressure and is able to maintain fairness with little compromise to the utility.",0
"Over the past few years, there has been a significant increase in the demand for same-day delivery (SDD), which has been further accelerated by the COVID-19 pandemic. However, existing literature has primarily focused on maximizing utility, which can lead to unequal opportunities for customers and raise concerns about fairness. To address this issue, we investigate achieving fairness in SDD in this study. Specifically, we propose a regional-level fairness constraint to ensure equal chances of being served for customers from different regions. Our approach uses a reinforcement learning model that balances overall utility and fairness and effectively reduces geographic disparities and resource constraints with minimal impact on utility. Furthermore, we simulate a COVID-19 scenario where sudden surges in requests occur, and our model proves to be resilient and maintains fairness while only slightly impacting utility.",1
Human beings learn causal models and constantly use them to transfer knowledge between similar environments. We use this intuition to design a transfer-learning framework using object-oriented representations to learn the causal relationships between objects. A learned causal dynamics model can be used to transfer between variants of an environment with exchangeable perceptual features among objects but with the same underlying causal dynamics. We adapt continuous optimization for structure learning techniques to explicitly learn the cause and effects of the actions in an interactive environment and transfer to the target domain by categorization of the objects based on causal knowledge. We demonstrate the advantages of our approach in a gridworld setting by combining causal model-based approach with model-free approach in reinforcement learning.,0
"Causal models are acquired by humans and are frequently employed to transfer knowledge across comparable situations. To learn the causal relationships between objects, we utilize this intuition and establish a transfer-learning framework using object-oriented representations. A learned causal dynamics model can be applied to different versions of an environment that have interchangeable perceptual features, but the same fundamental causal dynamics. We modify continuous optimization for structure learning methods to explicitly learn the cause and effect of actions in an interactive environment and transfer the knowledge to the target domain by categorizing objects based on causal understanding. We exhibit the benefits of our method in a gridworld scenario by integrating a causal model-based approach with a model-free approach in reinforcement learning.",1
"In this thesis, I propose a family of fully decentralized deep multi-agent reinforcement learning (MARL) algorithms to achieve high, real-time performance in network-level traffic signal control. In this approach, each intersection is modeled as an agent that plays a Markovian Game against the other intersection nodes in a traffic signal network modeled as an undirected graph, to approach the optimal reduction in delay. Following Partially Observable Markov Decision Processes (POMDPs), there are 3 levels of communication schemes between adjacent learning agents: independent deep Q-leaning (IDQL), shared states reinforcement learning (S2RL) and a shared states & rewards version of S2RL--S2R2L. In these 3 variants of decentralized MARL schemes, individual agent trains its local deep Q network (DQN) separately, enhanced by convergence-guaranteed techniques like double DQN, prioritized experience replay, multi-step bootstrapping, etc. To test the performance of the proposed three MARL algorithms, a SUMO-based simulation platform is developed to mimic the traffic evolution of the real world. Fed with random traffic demand between permitted OD pairs, a 4x4 Manhattan-style grid network is set up as the testbed, two different vehicle arrival rates are generated for model training and testing. The experiment results show that S2R2L has a quicker convergence rate and better convergent performance than IDQL and S2RL in the training process. Moreover, three MARL schemes all reveal exceptional generalization abilities. Their testing results surpass the benchmark Max Pressure (MP) algorithm, under the criteria of average vehicle delay, network-level queue length and fuel consumption rate. Notably, S2R2L has the best testing performance of reducing 34.55% traffic delay and dissipating 10.91% queue length compared with MP.",0
"This thesis proposes a set of fully decentralized deep multi-agent reinforcement learning (MARL) algorithms for achieving real-time, high performance in network-level traffic signal control. The approach models each intersection as an agent playing a Markovian Game against other intersection nodes in an undirected graph to minimize delay. The communication schemes between adjacent learning agents follow Partially Observable Markov Decision Processes (POMDPs) and include independent deep Q-leaning (IDQL), shared states reinforcement learning (S2RL), and a shared states & rewards version of S2RL called S2R2L. Each agent trains its local deep Q network (DQN) separately using convergence-guaranteed techniques. A SUMO-based simulation platform is used to test the performance of the proposed algorithms on a 4x4 Manhattan-style grid network with random traffic demand and two different vehicle arrival rates. The results show that S2R2L has the best testing performance, reducing traffic delay by 34.55% and dissipating 10.91% queue length compared to the benchmark Max Pressure (MP) algorithm. All three MARL schemes demonstrate exceptional generalization abilities and outperform MP in terms of average vehicle delay, network-level queue length, and fuel consumption rate.",1
