"Deep Neural Networks (DNNs) have transformed the field of machine learning and are widely deployed in many applications involving image, video, speech and natural language processing. The increasing compute demands of DNNs have been widely addressed through Graphics Processing Units (GPUs) and specialized accelerators. However, as model sizes grow, these von Neumann architectures require very high memory bandwidth to keep the processing elements utilized as a majority of the data resides in the main memory. Processing in memory has been proposed as a promising solution for the memory wall bottleneck for ML workloads. In this work, we propose a new DRAM-based processing-in-memory (PIM) multiplication primitive coupled with intra-bank accumulation to accelerate matrix vector operations in ML workloads. The proposed multiplication primitive adds  1% area overhead and does not require any change in the DRAM peripherals. Therefore, the proposed multiplication can be easily adopted in commodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture, data mapping scheme and dataflow for executing DNNs within DRAM. System evaluations performed on networks like AlexNet, VGG16 and ResNet18 show that the proposed architecture, mapping, and data flow can provide up to 19.5x speedup over an NVIDIA Titan Xp GPU highlighting the need to overcome the memory bottleneck in future generations of DNN hardware.",0
"As organizations increasingly rely on data-driven decision making, machine learning workloads have become more prevalent. However, traditional computer systems struggle to meet the performance demands of these workloads due to their reliance on slow disks and limited memory bandwidth. To address this issue, researchers propose utilizing processing in commodity dynamic random access memory (PIM-DRAM), which enables computation within the memory itself. This approach has shown promise in recent studies, but there remains room for improvement. In this paper, we aim to explore how PIM-DRAM can accelerate machine learning workloads by conducting experiments on real hardware platforms. We evaluate several state-of-the-art frameworks for training and inference across multiple benchmarks, including popular models such as ResNet-56 and BERT. Our results demonstrate that PIM-DRAM significantly reduces latency while maintaining high levels of accuracy compared to conventional methods. Additionally, our findings suggest that PIM-DRAM could potentially enable new use cases where machine learning was previously impractical given computational constraints. Overall, our study provides insights into the benefits and limitations of PIM-DRAM for acceleration in large-scale machine learning tasks and encourages further development in this area.",1
"Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation --- describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.",0
"This paper presents a new method called DeepCAD that can generate high quality three dimensional models suitable for use in computer-aided design (CAD). Our approach builds on recent advances in deep generative networks by training a neural network to predict 3D shapes given a set of input parameters. We show that our model can accurately predict detailed geometric features such as corners, curves, and chamfers, allowing us to generate complex shapes that are difficult to create manually. Additionally, we demonstrate how our method can be used to improve existing CAD systems by generating templates and guiding manual construction of designs. Finally, we compare our results against other state-of-the-art methods and demonstrate significant improvements across several metrics. Overall, DeepCAD represents a major step towards fully automating the process of creating accurate and detailed 3D models for engineering applications.",1
"Vision-language Navigation (VLN) tasks require an agent to navigate step-by-step while perceiving the visual observations and comprehending a natural language instruction. Large data bias, which is caused by the disparity ratio between the small data scale and large navigation space, makes the VLN task challenging. Previous works have proposed various data augmentation methods to reduce data bias. However, these works do not explicitly reduce the data bias across different house scenes. Therefore, the agent would overfit to the seen scenes and achieve poor navigation performance in the unseen scenes. To tackle this problem, we propose the Random Environmental Mixup (REM) method, which generates cross-connected house scenes as augmented data via mixuping environment. Specifically, we first select key viewpoints according to the room connection graph for each scene. Then, we cross-connect the key views of different scenes to construct augmented scenes. Finally, we generate augmented instruction-path pairs in the cross-connected scenes. The experimental results on benchmark datasets demonstrate that our augmentation data via REM help the agent reduce its performance gap between the seen and unseen environment and improve the overall performance, making our model the best existing approach on the standard VLN benchmark.",0
"This is an abstract for a research paper on developing artificial intelligence algorithms that can navigate through different environments using visual inputs and natural language commands from humans. The proposed approach mixes up randomly selected environmental factors, such as lighting conditions and object appearances, to train the algorithm so that it can better handle real-world variations. Experiments show improved performance compared to traditional methods without randomization, demonstrating the effectiveness of our method. We discuss potential applications for these vision-language navigation algorithms in fields like robotics and augmented reality. ----  For more complex tasks you may provide me text templates and ask them for specific modifications, but note I am unable to provide you with the full final product without your creative input.",1
"Image caption generation is one of the most challenging problems at the intersection of visual recognition and natural language modeling domains. In this work, we propose and study a practically important variant of this problem where test images may contain visual objects with no corresponding visual or textual training examples. For this problem, we propose a detection-driven approach based on a generalized zero-shot detection model and a template-based sentence generation model. In order to improve the detection component, we jointly define a class-to-class similarity based class representation and a practical score calibration mechanism. We also propose a novel evaluation metric that provides complimentary insights to the captioning outputs, by separately handling the visual and non-visual components of the captions. Our experiments show that the proposed zero-shot detection model obtains state-of-the-art performance on the MS-COCO dataset and the zero-shot captioning approach yields promising results.",0
"Detecting objects from images is one of the most fundamental tasks in computer vision, which has applications ranging from autonomous robots to content based image retrieval systems. Most existing works have been focusing on detectors that can handle common object classes such as cars, dogs, cats, etc., but there still remain a large number of uncommon or rare objects, e.g., animals living only in remote areas, plants specific to certain regions, newly designed vehicles or gadgets, etc. As we want our detection models to work well on all kinds of objects without additional training data for each category, we need ways to deal with unseen objects automatically during testing time. In this paper, instead of proposing another detector architecture like YOLOv8 or Tiny-DSOD, we concentrate on exploring several post-processing techniques to tackle this problem. Specifically: (a) We create “chimeric” datasets by fusing objects across different images into single examples with new identities (e.g., cat + dog = chimera). This method simulates arbitrary combinations of unseen objects, leading to improved generalization ability. (b) During inference, we apply a conditional prior network to infer object properties before classification. By doing so, ambiguity is reduced and detection accuracy increases, especially under low confidence scores caused by poor representations in feature maps. Besides boosting base performance via these components, our overall system outperforms other state-of-the-art methods on both COCO and VOC benchmarks using less computation budget than them on similar backbones. To further evaluate generalizability over novel categories, we introduce Stuff-Inception where we feed randomly cropped sub-regions (instead of full images) to pretrained models to observe if they can generate satisfactory captions fo",1
"Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",0
"This survey covers recent developments in vision transformers (ViTs), which are deep learning models that achieve state-of-the-art results across many computer vision tasks using only self-attention mechanisms without convolutional neural networks or recurrent layers. We discuss important ViT architectures like BERT, GPT, Big Bird, DALL•E2, TATTLE, Cairn-x, ConViT, Twins, and many others. For each architecture we provide details such as model size and training dataset. Additionally, we compare different variants of these models in terms of their accuracy, speed, efficiency, flexibility, scalability, interpretability, generalizability, stability, transferability, and robustness. Our survey provides readers with clear descriptions, comparisons, and analyses of current research trends surrounding vision transformers, along with future directions and challenges. Finally, we present several open problems and opportunities related to developing more advanced versions of ViTs using novel techniques from other domains.",1
"Data processing and analytics are fundamental and pervasive. Algorithms play a vital role in data processing and analytics where many algorithm designs have incorporated heuristics and general rules from human knowledge and experience to improve their effectiveness. Recently, reinforcement learning, deep reinforcement learning (DRL) in particular, is increasingly explored and exploited in many areas because it can learn better strategies in complicated environments it is interacting with than statically designed algorithms. Motivated by this trend, we provide a comprehensive review of recent works focusing on utilizing deep reinforcement learning to improve data processing and analytics. First, we present an introduction to key concepts, theories, and methods in deep reinforcement learning. Next, we discuss deep reinforcement learning deployment on database systems, facilitating data processing and analytics in various aspects, including data organization, scheduling, tuning, and indexing. Then, we survey the application of deep reinforcement learning in data processing and analytics, ranging from data preparation, natural language interface to healthcare, fintech, etc. Finally, we discuss important open challenges and future research directions of using deep reinforcement learning in data processing and analytics.",0
"""Data processing and analytics have become essential tasks across many industries, and advancements in machine learning have greatly improved the efficiency and accuracy of these processes. One area that has gained significant attention recently is deep reinforcement learning (DRL), which uses artificial neural networks to learn from trial and error in complex environments. In this survey, we aim to provide a comprehensive overview of recent developments in DRL applied to data processing and analytics tasks. We discuss how DRL algorithms can be used in areas such as web search, recommender systems, natural language processing, image classification, fraud detection, network intrusion detection, and others. We analyze the strengths and limitations of different approaches and highlight challenges and opportunities for future research. By summarizing state-of-the-art methods and identifying open issues in the field, our work serves as a reference guide for both practitioners and researchers interested in applying DRL techniques to data processing and analytics problems.""",1
"Recently proposed fine-grained 3D visual grounding is an essential and challenging task, whose goal is to identify the 3D object referred by a natural language sentence from other distractive objects of the same category. Existing works usually adopt dynamic graph networks to indirectly model the intra/inter-modal interactions, making the model difficult to distinguish the referred object from distractors due to the monolithic representations of visual and linguistic contents. In this work, we exploit Transformer for its natural suitability on permutation-invariant 3D point clouds data and propose a TransRefer3D network to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. Concretely, we devise an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module to conduct fine-grained cross-modal feature matching. Facilitated by co-attention operation, our EA module matches visual entity features with linguistic entity features while RA module matches pair-wise visual relation features with linguistic relation features, respectively. We further integrate EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Extensive experiments on both Nr3D and Sr3D datasets demonstrate that our proposed model significantly outperforms existing approaches by up to 10.6% and claims the new state-of-the-art. To the best of our knowledge, this is the first work investigating Transformer architecture for fine-grained 3D visual grounding task.",0
"This paper presents a novel transformer model called ""TransRefer3D"" that addresses fine-grained visual grounding tasks by explicitly modeling entity relationships within a scene. In contrast to previous approaches which only focus on referring expressions within a single image, our approach models cross-modal dependencies between language queries and spatial entities across multiple images. Our method uses multi-head attention to weight different contextual views of referential expressions, allowing for better disambiguation of ambiguous references. Experimental results show significant improvements over state-of-the-art methods on two benchmark datasets, demonstrating the effectiveness of our proposed approach for fine-grained 3D visual grounding.",1
"Video captioning aims to automatically generate natural language sentences that can describe the visual contents of a given video. Existing generative models like encoder-decoder frameworks cannot explicitly explore the object-level interactions and frame-level information from complex spatio-temporal data to generate semantic-rich captions. Our main contribution is to identify three key problems in a joint framework for future video summarization tasks. 1) Enhanced Object Proposal: we propose a novel Conditional Graph that can fuse spatio-temporal information into latent object proposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to dynamically extract visual words with higher semantic levels. 3) Sentence Validation: A novel Discriminative Language Validator is proposed to verify generated captions so that key semantic concepts can be effectively preserved. Our experiments on two public datasets (MVSD and MSR-VTT) manifest significant improvements over state-of-the-art approaches on all metrics, especially for BLEU-4 and CIDEr. Our code is available at https://github.com/baiyang4/D-LSG-Video-Caption.",0
"This paper proposes a novel approach for video caption generation which combines both discriminative and latent semantic aspects for improved performance. We develop a framework that integrates multiple modalities such as audio, visual features and temporal dependencies. Our method outperforms state-of-the art approaches by utilizing a discriminative latent semantic graph, incorporating diverse contextual information while preserving intrinsic relationships among different time-steps in the generated captions. Experimental results demonstrate the effectiveness of our proposed model on standard benchmarks achieving improvements over strong baselines across metrics. This work sets a new milestone towards high quality automatic video description for a wide range of applications including accessibility, surveillance and content creation.",1
"Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN), and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High-Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Readers will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this book; however, familiarity with at least one programming language is assumed.",0
"Artificial neural networks have shown remarkable success across diverse applications in recent years. In particular, deep learning has revolutionized several fields by enabling powerful and efficient algorithms that learn directly from raw data. Recent advances in parallel computing, as well as large collections of labeled training data, are allowing these models to obtain state-of-the-art results on difficult tasks such as image classification, object detection, speech recognition, natural language processing, generative modelling and control. This work surveys important applications of this exciting new development, highlighting opportunities where neural network technology may bring significant impact within industry and society. Results indicate promise for a broad range of domains including health care, finance, education, creativity tools, robotics and gaming. As both researchers and users become familiar with this rapidly evolving area, we look forward to further transformational impacts of these systems.",1
"We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval.",0
"The task of image retrieval involves finding images that match a given textual description. Recent advances in pre-trained vision-and-language models have enabled significant improvements in this area. In this work, we evaluate several state-of-the-art pre-trained models on real-world image datasets and compare their performance against traditional feature extraction methods. Our results show that pre-trained models outperform traditional methods by a large margin, achieving substantial gains in accuracy and speed. We also explore the effectiveness of different model architectures and training objectives on image retrieval tasks. Finally, we discuss potential applications and future directions for research in this field. Overall, our findings demonstrate the power of pre-trained models in solving complex computer vision problems and highlight their promising potential for a wide range of real-world applications.",1
"Video-and-Language Inference is a recently proposed task for joint video-and-language understanding. This new task requires a model to draw inference on whether a natural language statement entails or contradicts a given video clip. In this paper, we study how to address three critical challenges for this task: judging the global correctness of the statement involved multiple semantic meanings, joint reasoning over video and subtitles, and modeling long-range relationships and complex social interactions. First, we propose an adaptive hierarchical graph network that achieves in-depth understanding of the video over complex interactions. Specifically, it performs joint reasoning over video and subtitles in three hierarchies, where the graph structure is adaptively adjusted according to the semantic structures of the statement. Secondly, we introduce semantic coherence learning to explicitly encourage the semantic coherence of the adaptive hierarchical graph network from three hierarchies. The semantic coherence learning can further improve the alignment between vision and linguistics, and the coherence across a sequence of video segments. Experimental results show that our method significantly outperforms the baseline by a large margin.",0
"This abstract describes how semantic coherence can be used to improve video-and-language inference by using adaptive hierarchical graph reasoning (AHGR). AHGR is a method that combines bottom-up and top-down processing to build meaningful representations from raw data such as video frames or textual descriptions. By incorporating semantic coherence into AHGR, we can more effectively identify relationships between different elements within the video and language inputs, leading to more accurate inferences across both modalities. To evaluate our approach, we conducted experiments on two challenging datasets: TVQA and MovieQA. Our results demonstrate that our AHGR model outperforms several strong baselines, showing the effectiveness of our proposed method.",1
"To continuously improve quality and reflect changes in data, machine learning applications have to regularly retrain and update their core models. We show that a differential analysis of language model snapshots before and after an update can reveal a surprising amount of detailed information about changes in the training data. We propose two new metrics---\emph{differential score} and \emph{differential rank}---for analyzing the leakage due to updates of natural language models. We perform leakage analysis using these metrics across models trained on several different datasets using different methods and configurations. We discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.",0
"This paper examines how natural language models can leak sensitive data through their updates. Despite recent advances in privacy regulations such as GDPR, there remains little understanding of the potential vulnerabilities introduced by these systems. Through empirical analysis on popular NLP datasets, we demonstrate that updates made to pretrained models can unintentionally reveal information about the users who contributed training examples, including personal characteristics and potentially even their identities. These findings highlight the need for more stringent controls over the use of large-scale machine learning datasets and suggest future research directions aimed at minimizing unwanted information leakage from model updates without sacrificing performance.",1
"Recently, there have been breakthroughs in computer vision (""CV"") models that are more generalizable with the advent of models such as CLIP and ALIGN. In this paper, we analyze CLIP and highlight some of the challenges such models pose. CLIP reduces the need for task specific training data, potentially opening up many niche tasks to automation. CLIP also allows its users to flexibly specify image classification classes in natural language, which we find can shift how biases manifest. Additionally, through some preliminary probes we find that CLIP can inherit biases found in prior computer vision systems. Given the wide and unpredictable domain of uses for such models, this raises questions regarding what sufficiently safe behaviour for such systems may look like. These results add evidence to the growing body of work calling for a change in the notion of a 'better' model--to move beyond simply looking at higher accuracy at task-oriented capability evaluations, and towards a broader 'better' that takes into account deployment-critical features such as different use contexts, and people who interact with the model when thinking about model deployment.",0
"""This research seeks to evaluate Clip (Contrastive Langevin Inference Pipeline) which is one of the most powerful generative models on human generated data yet created, however while some have investigated how well it works none have examined what it can do beyond making images or even if those images are accurate representations. Our analysis shows that although there is significant uncertainty in Clip predictions we can still classify them as more likely correct than incorrect. We also find that images made by clip lack many common features found in real world objects, such as depth or specular reflections from metallic materials and plastic lenses. Lastly our work establishes methods capable of evaluating other GANs against clip using a consistent metric.""",1
"In many sequence learning tasks, such as program synthesis and document summarization, a key problem is searching over a large space of possible output sequences. We propose to learn representations of the outputs that are specifically meant for search: rich enough to specify the desired output but compact enough to make search more efficient. Discrete latent codes are appealing for this purpose, as they naturally allow sophisticated combinatorial search strategies. The latent codes are learned using a self-supervised learning principle, in which first a discrete autoencoder is trained on the output sequences, and then the resulting latent codes are used as intermediate targets for the end-to-end sequence prediction task. Based on these insights, we introduce the \emph{Latent Programmer}, a program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language. We evaluate the Latent Programmer on two domains: synthesis of string transformation programs, and generation of programs from natural language descriptions. We demonstrate that the discrete latent representation significantly improves synthesis accuracy.",0
"Abstract: In the field of program synthesis, finding efficient solutions that satisfy complex specifications remains a challenging task. One approach towards solving this problem involves learning a latent representation of code using deep neural networks (DNNs). However, most current methods suffer from several limitations such as high computational cost, poor scalability, and limited expressiveness of the learned representations. To address these issues, we propose a new method called ""Latent Programmer"" which leverages discrete latent codes to represent programs, allowing us to learn more concise and interpretable models while significantly reducing computation time and memory usage. Our experimental results on multiple programming tasks demonstrate the effectiveness of our proposed method compared to state-of-the-art techniques, achieving better performance while requiring fewer resources. This work opens up exciting opportunities for improving existing software development practices by enabling developers to leverage machine learning algorithms to automatically generate high-quality code. Overall, this research makes important contributions to both the fields of artificial intelligence and software engineering.",1
"Inspired by how the human brain employs a higher number of neural pathways when describing a highly focused subject, we show that deep attentive models used for the main vision-language task of image captioning, could be extended to achieve better performance. Image captioning bridges a gap between computer vision and natural language processing. Automated image captioning is used as a tool to eliminate the need for human agent for creating descriptive captions for unseen images.Automated image captioning is challenging and yet interesting. One reason is that AI based systems capable of generating sentences that describe an input image could be used in a wide variety of tasks beyond generating captions for unseen images found on web or uploaded to social media. For example, in biology and medical sciences, these systems could provide researchers and physicians with a brief linguistic description of relevant images, potentially expediting their work.",0
"This abstract provides a brief overview of two approaches to natural language processing (NLP) that have shown promising results: Neural Twins and alternative calculations. Both methods aim to improve NLP by enhancing the understanding and generation of human-like responses. The Neural Twins approach involves training multiple models on different subsets of data so they can better capture the variability inherent in human language. These trained models then work together as ""twins"" to solve complex tasks. On the other hand, alternative calculation techniques focus more explicitly on reasoning and planning and try to model both linguistically-specific and world knowledge within a single system. They involve designing architectures where there are separate modules responsible for each component of computation but all working closely together as one network. By combining these distinct approaches, researchers hope to achieve even greater advancements in the field of NLP.",1
"While convolutional neural networks have shown a tremendous impact on various computer vision tasks, they generally demonstrate limitations in explicitly modeling long-range dependencies due to the intrinsic locality of the convolution operation. Initially designed for natural language processing tasks, Transformers have emerged as alternative architectures with innate global self-attention mechanisms to capture long-range dependencies. In this paper, we propose TransDepth, an architecture that benefits from both convolutional neural networks and transformers. To avoid the network losing its ability to capture local-level details due to the adoption of transformers, we propose a novel decoder that employs attention mechanisms based on gates. Notably, this is the first paper that applies transformers to pixel-wise prediction problems involving continuous labels (i.e., monocular depth prediction and surface normal estimation). Extensive experiments demonstrate that the proposed TransDepth achieves state-of-the-art performance on three challenging datasets. Our code is available at: https://github.com/ygjwd12345/TransDepth.",0
"Artificial intelligence has made significant strides in recent years due to advances in deep learning techniques such as convolutional neural networks (CNNs). However, these models often struggle to produce high quality outputs for tasks that require pixel-wise predictions, particularly on datasets with limited annotations. To address this challenge, we propose the use of transformer-based attention networks, which have proven effective in natural language processing tasks. Our approach leverages self-attention mechanisms to capture global dependencies between pixels while allowing for efficient computation. We evaluate our method on several benchmark datasets and demonstrate improved performance compared to state-of-the-art CNN-based methods. Additionally, we provide qualitative examples illustrating the advantages of our model in terms of accuracy and visual fidelity. Our work represents an important step towards realizing the full potential of artificial intelligence for continuous pixel-wise prediction tasks.",1
"Neural agents trained in reinforcement learning settings can learn to communicate among themselves via discrete tokens, accomplishing as a team what agents would be unable to do alone. However, the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero-shot understanding. Inspired by word embedding techniques from natural language processing, we propose neural agent architectures that enables them to communicate via discrete tokens derived from a learned, continuous space. We show in a decision theoretic framework that our technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, we validate that our trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, we demonstrate both that agents using our method can effectively respond to novel human communication and that humans can understand unlabeled emergent agent communication, outperforming the use of one-hot communication.",0
"In recent years, there has been growing interest in understanding how discrete representations can emerge from distributed neural networks. This paper presents a novel approach to studying this phenomenon by examining the relationship between semantic spaces and communication dynamics. Our results show that as agents interact in these semantic spaces, they form a shared representation that allows them to communicate more effectively. Furthermore, we find that this emergent representation is discrete rather than continuous, challenging previous assumptions about how neural networks process information. These findings have important implications for our understanding of cognitive processes such as language acquisition and social learning. Overall, this work represents a significant contribution to the field of artificial intelligence and neuroscience.",1
"Meter-level load forecasting is crucial for efficient energy management and power system planning for Smart Grids (SGs), in tasks associated with regulation, dispatching, scheduling, and unit commitment of power grids. Although a variety of algorithms have been proposed and applied on the field, more accurate and robust models are still required: the overall utility cost of operations in SGs increases 10 million currency units if the load forecasting error increases 1%, and the mean absolute percentage error (MAPE) in forecasting is still much higher than 1%. Transformers have become the new state-of-the-art in a variety of tasks, including the ones in computer vision, natural language processing and time series forecasting, surpassing alternative neural models such as convolutional and recurrent neural networks. In this letter, we present a new state-of-the-art Transformer-based algorithm for the meter-level load forecasting task, which has surpassed the former state-of-the-art, LSTM, and the traditional benchmark, vanilla RNN, in all experiments by a margin of at least 13% in MAPE.",0
"This paper presents a novel transformer-based approach for load forecasting in smart grid systems. Our model utilizes time-series data from the electrical network and machine learning techniques to accurately predict future energy consumption patterns. We evaluate our method against state-of-the-art benchmarks using real world datasets and demonstrate significant improvements in both accuracy and efficiency. The results indicate that our transformer-based load forecaster outperforms existing methods by up to 20% in mean absolute error (MAE) metrics and can effectively handle high dimensionality input data. In conclusion, our work provides a new powerful tool for electric utility companies to optimize their operations through accurate short-term load prediction.",1
"Deep learning on graphs has attracted significant interests recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from SSL on other domains like computer vision and natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of graph self-supervised learning, we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We construct a unified framework that mathematically formalizes the paradigm of graph SSL. According to the objectives of pretext tasks, we divide these approaches into four categories: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. We further conclude the applications of graph SSL across various research fields and summarize the commonly used datasets, evaluation benchmark, performance comparison and open-source codes of graph SSL. Finally, we discuss the remaining challenges and potential future directions in this research field.",0
"Graph self-supervised learning (GraphSSL) has emerged as a promising approach for leveraging large amounts of unlabeled graph data to learn meaningful representations that can be used in downstream tasks. In recent years, there has been growing interest in this field due to the increasing availability of large graphs from diverse domains such as social networks, bioinformatics, recommender systems, and knowledge bases. This survey provides a comprehensive overview of the current state of graph SSL research, including the key challenges faced by researchers, the most common techniques employed, and the major applications of graph SSL. We begin by discussing the motivation behind graph SSL and highlighting the unique characteristics of graph data that make traditional supervised learning approaches less effective. Next, we review several popular graph SSL methods, including autoencoders, contrastive learning, and generative models. For each method, we provide details on how they work, their advantages and limitations, and their applications in real-world scenarios. Finally, we conclude by summarizing the main findings of our survey, identifying key areas for future research, and suggesting directions for potential extensions. Overall, this survey aims to serve as a reference resource for both newcomers and experts in the field seeking to understand the latest advances in graph SSL.",1
"Progress of machine learning in critical care has been difficult to track, in part due to absence of public benchmarks. Other fields of research (such as computer vision and natural language processing) have established various competitions and public benchmarks. Recent availability of large clinical datasets has enabled the possibility of establishing public benchmarks. Taking advantage of this opportunity, we propose a public benchmark suite to address four areas of critical care, namely mortality prediction, estimation of length of stay, patient phenotyping and risk of decompensation. We define each task and compare the performance of both clinical models as well as baseline and deep learning models using eICU critical care dataset of around 73,000 patients. This is the first public benchmark on a multi-centre critical care dataset, comparing the performance of clinical gold standard with our predictive model. We also investigate the impact of numerical variables as well as handling of categorical variables on each of the defined tasks. The source code, detailing our methods and experiments is publicly available such that anyone can replicate our results and build upon our work.",0
"Machine Learning (ML) has been increasingly used in healthcare domains such as critical care to improve patient outcomes by predicting potential adverse events, mortality risk assessment, and optimizing treatment plans. In recent years, the availability of large electronic intensive care unit (eICU) datasets across multiple centers has enabled researchers to benchmark their ML models against standard clinical practices. However, comparing different eICU ML models can be challenging due to variations in data quality, feature engineering methods, model architectures, evaluation metrics, and implementation details. Therefore, establishing comprehensive benchmarks using multicenter eICU data plays a crucial role in advancing knowledge in the field of critical care informatics and improving patient outcomes. This study aimed to design rigorous methodologies for evaluating and benchmarking eICU ML models developed from diverse eICU datasets across different hospitals worldwide. Our approach involved developing standardized feature extraction pipelines, harmonization techniques, and robust evaluation protocols that were validated through cross-validation experiments involving multiple eICU datasets. We compared the performance of several state-of-the-art deep neural networks (DNNs), traditional machine learning algorithms, and other well-established prediction models used widely in clinical practice. We observed significant improvements over existing models and identified novel techniques for enhancing the generalizability and interpretability of DNNs. These findings will enable future comparisons of eICU ML models across multiple sites, ultimately leading to better care management strategies and improved outcomes for critically ill patients. Overall, our work serves as a benchmark for future studies seeking to optimize ML models in the context of high-stakes medical applications such as eICUs.",1
"Interventional causal models describe several joint distributions over some variables used to describe a system, one for each intervention setting. They provide a formal recipe for how to move between the different joint distributions and make predictions about the variables upon intervening on the system. Yet, it is difficult to formalise how we may change the underlying variables used to describe the system, say moving from fine-grained to coarse-grained variables. Here, we argue that compositionality is a desideratum for such model transformations and the associated errors: When abstracting a reference model M iteratively, first obtaining M' and then further simplifying that to obtain M'', we expect the composite transformation from M to M'' to exist and its error to be bounded by the errors incurred by each individual transformation step. Category theory, the study of mathematical objects via compositional transformations between them, offers a natural language to develop our framework for model transformations and abstractions. We introduce a category of finite interventional causal models and, leveraging theory of enriched categories, prove the desired compositionality properties for our framework.",0
This should be a good resource for me - thank you! I am writing an article on this topic as well - if there is any other specific questions I have that would benefit from your expertise please feel free to ask away!,1
"Video captioning is an essential technology to understand scenes and describe events in natural language. To apply it to real-time monitoring, a system needs not only to describe events accurately but also to produce the captions as soon as possible. Low-latency captioning is needed to realize such functionality, but this research area for online video captioning has not been pursued yet. This paper proposes a novel approach to optimize each caption's output timing based on a trade-off between latency and caption quality. An audio-visual Trans-former is trained to generate ground-truth captions using only a small portion of all video frames, and to mimic outputs of a pre-trained Transformer to which all the frames are given. A CNN-based timing detector is also trained to detect a proper output timing, where the captions generated by the two Trans-formers become sufficiently close to each other. With the jointly trained Transformer and timing detector, a caption can be generated in the early stages of an event-triggered video clip, as soon as an event happens or when it can be forecasted. Experiments with the ActivityNet Captions dataset show that our approach achieves 94% of the caption quality of the upper bound given by the pre-trained Transformer using the entire video clips, using only 28% of frames from the beginning.",0
"This abstract aims to describe a new method for optimizing latency in online video captioning using audio-visual transformers (AVTs). With the increasing popularity of streaming platforms such as Twitch and YouTube, there has been a growing demand for real-time captions that can keep up with live broadcasts. However, current methods for generating captions often result in high latencies due to their reliance on complex model architectures and data processing steps. Our approach addresses these issues by leveraging AVTs, which have demonstrated state-of-the-art performance in tasks involving audio and visual signals. By integrating an attention mechanism into our framework, we enable the models to focus on relevant segments of the input audio stream and generate more accurate predictions. Additionally, we show that incorporating spatial features derived from frames helps further reduce latency without sacrificing accuracy. Experimental results demonstrate that our approach achieves significant improvements over existing systems across multiple evaluation metrics while maintaining low latencies. Overall, our research represents a promising step towards developing efficient solutions for online video captioning.",1
"Most existing neural architecture search (NAS) algorithms are dedicated to the downstream tasks, e.g., image classification in computer vision. However, extensive experiments have shown that, prominent neural architectures, such as ResNet in computer vision and LSTM in natural language processing, are generally good at extracting patterns from the input data and perform well on different downstream tasks. These observations inspire us to ask: Is it necessary to use the performance of specific downstream tasks to evaluate and search for good neural architectures? Can we perform NAS effectively and efficiently while being agnostic to the downstream task? In this work, we attempt to affirmatively answer the above two questions and improve the state-of-the-art NAS solution by proposing a novel and generic NAS framework, termed Generic NAS (GenNAS). GenNAS does not use task-specific labels but instead adopts \textit{regression} on a set of manually designed synthetic signal bases for architecture evaluation. Such a self-supervised regression task can effectively evaluate the intrinsic power of an architecture to capture and transform the input signal patterns, and allow more sufficient usage of training samples. We then propose an automatic task search to optimize the combination of synthetic signals using limited downstream-task-specific labels, further improving the performance of GenNAS. We also thoroughly evaluate GenNAS's generality and end-to-end NAS performance on all search spaces, which outperforms almost all existing works with significant speedup.",0
"In neural architecture search (NAS), evaluating many models can become computationally expensive since they must be trained from scratch every time. Recent work has used transfer learning to amortize NAS evaluation cost by reusing pretrained weights; however, these methods still suffer from computational overhead due to the need to fine-tune all architectures on each dataset separately. We introduce GenericNeuroarXiv, a method that uses regression to predict accuracy directly without training any model at all. Our approach requires fewer than four GPU hours per dataset, is faster than previous nonlinear meta learners, does not assume access to any pretraining data, and outperforms other NAS baselines across seven benchmark datasets. Additionally, we show how our approach can even handle more challenging scenarios such as one-shot NAS, where only a single query is made to evaluate an entire distribution of architectures. Code will be released upon acceptance.",1
"Attention-based transformer networks have demonstrated promising potential as their applications extend from natural language processing to vision. However, despite the recent improvements, such as sub-quadratic attention approximation and various training enhancements, the compact vision transformers to date using the regular attention still fall short in comparison with its convnet counterparts, in terms of \textit{accuracy,} \textit{model size}, \textit{and} \textit{throughput}. This paper introduces a compact self-attention mechanism that is fundamental and highly generalizable. The proposed method reduces redundancy and improves efficiency on top of the existing attention optimizations. We show its drop-in applicability for both the regular attention mechanism and some most recent variants in vision transformers. As a result, we produced smaller and faster models with the same or better accuracies.",0
"In recent years, transformer architectures have revolutionized natural language processing tasks by providing state-of-the-art results on various benchmarks. However, these models often suffer from high computational complexity due to their attention mechanism, making them impractical for many real-world applications such as image classification, object detection, and video analysis that require efficient computation times. To overcome this challenge, we propose Armour, which combines novel design choices aimed at reducing the quadratic scaling of self-attention with a new lightweight convolutional layer variant. Our approach significantly improves efficiency while maintaining competitive accuracy across several vision tasks compared against strong baselines like ViT, DeiT, T2T-ViT, CoaT, and StreamTransformer. Additionally, our model offers better scalability with respect to sequence length, enabling potential use cases beyond typical sequence data like images, videos, and graph-structured inputs where local dependencies play an important role. We hope our work opens doors towards developing more powerful yet faster attention mechanisms suitable for broader adoption in machine learning practice beyond textual domains.",1
"Deep learning has achieved great success in a wide spectrum of multimedia applications such as image classification, natural language processing and multimodal data analysis. Recent years have seen the development of many deep learning frameworks that provide a high-level programming interface for users to design models, conduct training and deploy inference. However, it remains challenging to build an efficient end-to-end multimedia application with most existing frameworks. Specifically, in terms of usability, it is demanding for non-experts to implement deep learning models, obtain the right settings for the entire machine learning pipeline, manage models and datasets, and exploit external data sources all together. Further, in terms of adaptability, elastic computation solutions are much needed as the actual serving workload fluctuates constantly, and scaling the hardware resources to handle the fluctuating workload is typically infeasible. To address these challenges, we introduce SINGA-Easy, a new deep learning framework that provides distributed hyper-parameter tuning at the training stage, dynamic computational cost control at the inference stage, and intuitive user interactions with multimedia contents facilitated by model explanation. Our experiments on the training and deployment of multi-modality data analysis applications show that the framework is both usable and adaptable to dynamic inference loads. We implement SINGA-Easy on top of Apache SINGA and demonstrate our system with the entire machine learning life cycle.",0
"Title: ""SINGA-Easy: A Framework for MultiModal Analysis""  Abstract: This paper presents SINGA-Easy, a framework designed specifically for multi-modal analysis that offers easy integration and use for researchers across various domains. The proposed approach addresses some of the challenges posed by current state-of-the-art tools such as limited scalability, high computational demands, and difficulty integrating new modalities into existing workflows. By leveraging the power of the fastai library along with a modular architecture, we aim to provide a versatile solution suitable for both experts and novices alike. We demonstrate the effectiveness of our methodology through multiple case studies spanning different modalities such as natural language processing (NLP), computer vision (CV), robotics, among others. Our results show significant improvements over baseline methods on standard benchmark datasets while maintaining simplicity and efficiency. Overall, SINGA-Easy has immense potential for advancing research in the multimodal domain, providing practitioners with a one-stop shop for their analytical needs.",1
"The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.",0
"In recent years, deep learning has made significant progress in many domains by leveraging large amounts of data and powerful model architectures. However, one key challenge remains: creating models that can generate meaningful outputs given structured inputs. To address this limitation, we propose Perceiver IO, a general architecture designed for processing structured inputs and producing structured outputs. We demonstrate the effectiveness of our approach on several challenging tasks, including sentiment analysis, machine translation, and question answering. Our results show that Perceiver IO outperforms state-of-the-art methods while offering greater flexibility and interpretability. This work has important implications for natural language processing research, as well as practical applications across industries.",1
"Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained blindly? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image from those domains. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.",0
"This work focuses on domain adaptation for image generators using a novel technique called ""CLIP-guidance"". We explore the effectiveness of utilizing external guidance from CLIP (Contrastive Language Pretrained Model) embeddings as regularization signals during training for improved performance across different domains. By guiding the generator towards CLIP-consistent outputs, we can improve generalizability while maintaining high visual quality. Our experiments demonstrate that our method leads to state-of-the-art results across multiple benchmark datasets under both quantitative evaluation metrics and qualitative human assessments. Overall, this work provides new insights into effective domain adaptation techniques and their impacts on image generation tasks.",1
"Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish some tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize major approaches in the field, along with their technical strengths/ weaknesses, we have simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and motivates interested readers to consider approaches for designing effective human-in-the-loop solutions.",0
"This survey papers reviews recent developments in human-machine collaboration, discussing how machine learning (ML) models can benefit from interacting with human decision makers. In order to achieve good results on real-world problems, ML systems must make decisions based on vast amounts of data that they collect over time. Although these algorithms may yield very precise recommendations and predictions by themselves, incorporating human feedback at different stages of their design and use leads to more accurate outcomes overall. Furthermore, involving humans throughout the training process improves model interpretability, ensures fairness, reduces risk, and enhances trust. By leveraging human judgment as well as automation, ML applications gain in scalability and robustness, ultimately producing better outcomes in less time. Thus, we conclude that there exists immense potential in cooperative efforts among machines and human agents, provided these interactions are designed thoughtfully and ethically.",1
"Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.",0
"In recent years, deep learning techniques have become increasingly popular due to their ability to learn from large amounts of data. One such technique that has gained significant attention is recurrent neural networks (RNNs). RNNs are capable of processing sequential data and have been successfully applied to a variety of tasks including natural language processing, speech recognition, and time series prediction. However, training these models can be computationally expensive and often requires specialized hardware. To address this issue, we propose a novel approach called continual learning for RNNs which allows them to adapt incrementally without retraining on the entire dataset. We evaluate our method using several benchmark datasets and demonstrate that it achieves state-of-the-art performance while requiring significantly less computational resources compared to traditional RNN methods. Our results indicate that continual learning for RNNs represents a promising new direction for improving their efficiency and scalability.",1
"""How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?"" ""How unstructured and complex can we make a sentence and still generate plausible movements from it?"" These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for generating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion -- one each for the upper body and the lower body movements. Our model can generate plausible pose sequences for short sentences describing single actions as well as long compositional sentences describing multiple sequential and superimposed actions. We evaluate our proposed model on the publicly available KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study indicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.",0
"This paper presents a method for synthesizing animations directly from natural language descriptions using deep learning techniques. Our system uses a generative model trained on large amounts of animation data to predict the next frame given a text description of the current one. We evaluate our approach on several benchmark datasets and demonstrate that our method can generate plausible images and videos even when trained without ground truth alignment data. Furthermore, we show that our system can generalize well across different domains and styles, producing results comparable to those obtained by hand-crafted systems. Finally, we explore applications of our technique, such as generating novel animations from summaries and enabling non-experts to create animations easily through written input alone.",1
"Despite the progress in automatic detection of radiologic findings from chest X-ray (CXR) images in recent years, a quantitative evaluation of the explainability of these models is hampered by the lack of locally labeled datasets for different findings. With the exception of a few expert-labeled small-scale datasets for specific findings, such as pneumonia and pneumothorax, most of the CXR deep learning models to date are trained on global ""weak"" labels extracted from text reports, or trained via a joint image and unstructured text learning strategy. Inspired by the Visual Genome effort in the computer vision community, we constructed the first Chest ImaGenome dataset with a scene graph data structure to describe $242,072$ images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$ combinations of relation annotations between $29$ CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over $670,000$ localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from $500$ unique patients.",0
"""The lack of large radiographic image datasets has been a significant barrier to developing deep learning models that can reliably detect diseases from chest x-rays (CXR). To address this issue, we present the largest publicly available CXR dataset with annotated pathologies to date – the Imagenet Medical Image Collection (Imagenet), which contains over 8 million images. In this study, we focus on the subset of the collection containing frontal-view adult human chest x-rays, which includes 267,498 unique exams. This subset, called the Chest ImaGenome Dataset (ChID) represents an order of magnitude more data than all previously published datasets combined. We provide detailed descriptions of the imaging techniques utilized along with demographics and important metadata such as age, sex, smoking status and any known drug reactions that may impact the interpretation of lung disease patterns seen within the x-rays themselves. Given the amount of pathology seen in this dataset, our work could potentially lead to improvements in deep learning-based diagnostic decision support tools that perform competitively against board certified radiologists.""",1
"Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.",0
"This should be an overview of your research results without giving away the conclusion. The survey investigates different methods that have been developed for image caption generation by analyzing a number of state-of-the-art papers from top conferences and journals. By comparing these approaches we aimed to identify which techniques work best across both quantitative metrics as well as human evaluation. We identified two main categories of recent works - those that focus on describing images via natural language descriptions (Show), and those that extract salient features using visual representations to generate captions directly from raw pixels (Tell). Our analysis shows that while some hybrid models can outperform simpler baselines, there exists no universal champion method capable of consistently producing high quality captions. Given the wide variety of tasks evaluated during our study, the fact remains that all current algorithms still fall short relative to the capability of humans who consistently provide superior captions. Ultimately it may require fundamentally new advances in areas such as computer vision, deep learning, natural language processing, cognitive psychology and more before machines can truly match humans at image captioning.",1
"Gradient quantization is an emerging technique in reducing communication costs in distributed learning. Existing gradient quantization algorithms often rely on engineering heuristics or empirical observations, lacking a systematic approach to dynamically quantize gradients. This paper addresses this issue by proposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to dynamically adjust the quantization scheme for each gradient descent step by exploring the trade-off between communication cost and convergence error. We derive an upper bound, tight in some cases, of the convergence error for a restricted family of quantization schemes and loss functions. We design our DQ-SGD algorithm via minimizing the communication cost under the convergence error constraints. Finally, through extensive experiments on large-scale natural language processing and computer vision tasks on AG-News, CIFAR-10, and CIFAR-100 datasets, we demonstrate that our quantization scheme achieves better tradeoffs between the communication cost and learning performance than other state-of-the-art gradient quantization methods.",0
"Here we present DQ-SGD, a novel method that leverages dynamic quantization during distributed training to reduce communication overhead while maintaining high model accuracy. Traditional distributed learning approaches rely on gradient compression techniques such as quantization to balance limited bandwidth resources with increasingly larger datasets. However, these methods often struggle to find a suitable trade-off between efficiency and accuracy due to their static nature. By introducing dynamic quantization into the equation, our proposed algorithm can adaptively adjust the level of precision necessary at each stage of training, striking a more effective balance overall. Our experimental results demonstrate the superiority of DQ-SGD compared to other state-of-the-art strategies across multiple applications, including image classification, natural language processing, and reinforcement learning. With this approach, practitioners can enjoy the benefits of faster and cheaper distributed learning without sacrificing model quality. Overall, DQ-SGD represents a significant step forward in enabling wider adoption of large-scale machine learning models by addressing one of the key challenges facing modern deep learning research - efficient communication amid growing dataset sizes.",1
"Vision-and-Language Navigation (VLN) is a challenging task in which an agent needs to follow a language-specified path to reach a target destination. The goal gets even harder as the actions available to the agent get simpler and move towards low-level, atomic interactions with the environment. This setting takes the name of low-level VLN. In this paper, we strive for the creation of an agent able to tackle three key issues: multi-modality, long-term dependencies, and adaptability towards different locomotive settings. To that end, we devise ""Perceive, Transform, and Act"" (PTA): a fully-attentive VLN architecture that leaves the recurrent approach behind and the first Transformer-like architecture incorporating three different modalities - natural language, images, and low-level actions for the agent control. In particular, we adopt an early fusion strategy to merge lingual and visual information efficiently in our encoder. We then propose to refine the decoding phase with a late fusion extension between the agent's history of actions and the perceptual modalities. We experimentally validate our model on two datasets: PTA achieves promising results in low-level VLN on R2R and achieves good performance in the recently proposed R4R benchmark. Our code is publicly available at https://github.com/aimagelab/perceive-transform-and-act.",0
"In recent years, advances in deep learning have led to significant progress in computer vision tasks such as object detection, segmentation, and image classification. However, despite these accomplishments, there remains a large gap between state-of-the-art models trained on specific modalities (e.g., vision only) and more generalizable human level intelligence which can perform well across multiple sensory inputs simultaneously. This paper addresses that issue by proposing novel neural network architectures capable of seamlessly integrating visual perception with natural language understanding. To achieve this goal, we introduce multimodal attention networks, whose core component consists of dynamic weighted connections between different streams of data, allowing them to adaptively attend to relevant features at each moment in time. We demonstrate through extensive experiments on challenging real world navigation scenarios, that our approach outperforms existing methods by achieving unprecedented performance in terms of accuracy, robustness, and computational efficiency. Our results suggest the potential of using multimodal representations for developing advanced artificial intelligence systems which can effectively interact with their surroundings under highly uncertain conditions.",1
"Relative position encoding (RPE) is important for transformer to capture sequence ordering of input tokens. General efficacy has been proven in natural language processing. However, in computer vision, its efficacy is not well studied and even remains controversial, e.g., whether relative position encoding can work equally well as absolute position? In order to clarify this, we first review existing relative position encoding methods and analyze their pros and cons when applied in vision transformers. We then propose new relative position encoding methods dedicated to 2D images, called image RPE (iRPE). Our methods consider directional relative distance modeling as well as the interactions between queries and relative position embeddings in self-attention mechanism. The proposed iRPE methods are simple and lightweight. They can be easily plugged into transformer blocks. Experiments demonstrate that solely due to the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable improvements over their original versions on ImageNet and COCO respectively, without tuning any extra hyperparameters such as learning rate and weight decay. Our ablation and analysis also yield interesting findings, some of which run counter to previous understanding. Code and models are open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.",0
"This paper proposes novel approaches to relative position encoding (RPE) that improve performance on visual transformer models compared to previous methods. We analyze common limitations and pitfalls associated with existing RPE techniques and present innovations that address these issues effectively. Our method improves performance across various benchmark datasets while maintaining competitive computational costs. Furthermore, we provide detailed analyses of our model’s behavior by performing extensive studies to demonstrate its effectiveness. These findings contribute significant advancements towards understanding and utilizing efficient relative position encoders in vision transformers. Overall, the contributions of this work broaden our knowledge base regarding how relative positions can influence the learning process within transformers, enabling more informed future research.",1
"Compared with the visual grounding on 2D images, the natural-language-guided 3D object localization on point clouds is more challenging. In this paper, we propose a new model, named InstanceRefer, to achieve a superior 3D visual grounding through the grounding-by-matching strategy. In practice, our model first predicts the target category from the language descriptions using a simple language classification model. Then, based on the category, our model sifts out a small number of instance candidates (usually less than 20) from the panoptic segmentation of point clouds. Thus, the non-trivial 3D visual grounding task has been effectively re-formulated as a simplified instance-matching problem, considering that instance-level candidates are more rational than the redundant 3D object proposals. Subsequently, for each candidate, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception, respectively. Eventually, the most relevant candidate is selected and localized by ranking confidence scores, which are obtained by the cooperative holistic visual-language feature matching. Experiments confirm that our method outperforms previous state-of-the-arts on ScanRefer online benchmark and Nr3D/Sr3D datasets.",0
"This work presents a cooperative holistic understanding approach towards visual grounding on point clouds using instance multi-level contextual referring (InstanceRefer). With the rapid development of autonomous driving technology and robotics, there has been increasing interest in real-time semantic segmentation and object detection from 3D LiDAR scans. However, current methods face significant challenges due to the high variability present in point cloud data caused by factors such as viewpoint changes, occlusions, and different sensor configurations. To address these issues, our proposed method utilizes multi-scale context information provided by instance-based object representations to enhance local feature extraction and grouping. We then use a global context module based on graph convolutional networks to refine initial predictions and aggregate results across multiple levels of abstraction. Experimental results on three publicly available datasets demonstrate that our framework achieves state-of-the-art performance in both accuracy and speed, while also providing explainability via attention maps generated at each stage of reasoning. Our study provides important insights into improving the robustness and flexibility of existing models for accurate scene analysis under complex conditions.",1
"Video generation is one of the most challenging tasks in Machine Learning and Computer Vision fields of study. In this paper, we tackle the text to video generation problem, which is a conditional form of video generation. Humans can listen/read natural language sentences, and can imagine or visualize what is being described; therefore, we believe that video generation from natural language sentences will have an important impact on Artificial Intelligence. Video generation is relatively a new field of study in Computer Vision, which is far from being solved. The majority of recent works deal with synthetic datasets or real datasets with very limited types of objects, scenes, and emotions. To the best of our knowledge, this is the very first work on the text (free-form sentences) to video generation on more realistic video datasets like Actor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of video generation by regressing the latent representations of the first and last frames and employing a context-aware interpolation method to build the latent representations of in-between frames. We propose a stacking ``upPooling'' block to sequentially generate RGB frames out of each latent representation and progressively increase the resolution. Moreover, our proposed Discriminator encodes videos based on single and multiple frames. We provide quantitative and qualitative results to support our arguments and show the superiority of our method over well-known baselines like Recurrent Neural Network (RNN) and Deconvolution (as known as Convolutional Transpose) based video generation methods.",0
"This paper presents a generative model that utilizes latent path construction to improve temporal coherence between frames generated from natural language text descriptions. By incorporating a decoder that generates paths through continuous representations, our method enables longer and more diverse video generation than traditional approaches based on frame predictions alone. The resulting videos exhibit improved coherency, variability, and overall quality compared to those produced by alternative generative models. We demonstrate the effectiveness of our approach via extensive experiments, including user studies evaluating both objective metrics such as visual fidelity and subjective human judgments regarding video plausibility and realism.",1
"Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full softmax is costly from the computational and energy perspective. There have been various sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there is no sampling scheme that is provably adaptive and samples the negative classes efficiently. Therefore, alternative heuristics like random sampling, static frequency-based sampling, or learning-based biased sampling, which primarily trade either the sampling cost or the adaptivity of samples per iteration are adopted. In this paper, we show two classes of distributions where the sampling scheme is truly adaptive and provably generates negative samples in near-constant time. Our implementation in C++ on CPU is significantly superior, both in terms of wall-clock time and accuracy, compared to the most optimized TensorFlow implementations of other popular negative sampling approaches on powerful NVIDIA V100 GPU.",0
"In recent years there has been increased interest in training data augmentation methods that increase efficiency without compromising quality of generated samples. This has led to considerable advances in model performance across multiple tasks and benchmark datasets. One such method that has gained popularity is negative sampling (NS), which involves augmenting the training set by adding artificially generated negative examples that have low similarity to real instances from the same class. NS has proven effective at increasing dataset size while preserving sample quality. However, different types of negative distributions have been proposed, each offering unique benefits depending on the application scenario. In particular, two recently introduced approaches - UniformNoise and GuidedBacktranslationSampler - offer competitive tradeoffs between speed, informativeness, effectiveness and scalability. UniformNoise adds uniform noise to existing positive exemplars to generate new negatives, while Guided Back Translation Sampler generates synthetic negatives guided by parallel text translation models and pretrained language models. While both approaches have shown strong results compared to conventional techniques, they differ significantly in terms of computational overhead and potential biases. This work provides a comprehensive comparison study evaluating these two efficient and informative negative sampling distribution strategies, highlighting their strengths, weaknesses, and suitability under different circumstances. We aim to provide guidance for practitioners wishing to apply these advanced sampling methods to improve large scale supervised learning applications, particularly those leveraging pre-training with transformer architectures as backbone models, where scaling and quality remain critical factors. Our experiments consider multiple configurations and stateof-the art architectures appliedto eight challenging classification tasks including GLUE benchmark datasets. By conducting extensive ablation studies on individual components and hyperparameters",1
"Image Captioning is a task that combines computer vision and natural language processing, where it aims to generate descriptive legends for images. It is a two-fold process relying on accurate image understanding and correct language understanding both syntactically and semantically. It is becoming increasingly difficult to keep up with the latest research and findings in the field of image captioning due to the growing amount of knowledge available on the topic. There is not, however, enough coverage of those findings in the available review papers. We perform in this paper a run-through of the current techniques, datasets, benchmarks and evaluation metrics used in image captioning. The current research on the field is mostly focused on deep learning-based methods, where attention mechanisms along with deep reinforcement and adversarial learning appear to be in the forefront of this research topic. In this paper, we review recent methodologies such as UpDown, OSCAR, VIVO, Meta Learning and a model that uses conditional generative adversarial nets. Although the GAN-based model achieves the highest score, UpDown represents an important basis for image captioning and OSCAR and VIVO are more useful as they use novel object captioning. This review paper serves as a roadmap for researchers to keep up to date with the latest contributions made in the field of image caption generation.",0
"This paper presents a comprehensive overview of recent advances in deep learning methodologies for image captioning. Our study focuses on recent developments in convolutional neural networks (CNNs) and their applications to the task of generating descriptive natural language captions from images. We analyze popular approaches used by researchers to address challenges in image captioning such as encoder design, decoder architecture, attention mechanisms, loss functions, and evaluation metrics. In addition, we discuss open issues and future directions that hold promise for further improving the performance of image captioning systems. Overall, our aim is to provide insights into state-of-the-art methods in deep learning for image captioning and encourage further exploration in this exciting field.",1
"Image captioning is a task in the field of Artificial Intelligence that merges between computer vision and natural language processing. It is responsible for generating legends that describe images, and has various applications like descriptions used by assistive technology or indexing images (for search engines for instance). This makes it a crucial topic in AI that is undergoing a lot of research. This task however, like many others, is trained on large images labeled via human annotation, which can be very cumbersome: it needs manual effort, both financial and temporal costs, it is error-prone and potentially difficult to execute in some cases (e.g. medical images). To mitigate the need for labels, we attempt to use self-supervised learning, a type of learning where models use the data contained within the images themselves as labels. It is challenging to accomplish though, since the task is two-fold: the images and captions come from two different modalities and usually handled by different types of networks. It is thus not obvious what a completely self-supervised solution would look like. How it would achieve captioning in a comparable way to how self-supervision is applied today on image recognition tasks is still an ongoing research topic. In this project, we are using an encoder-decoder architecture where the encoder is a convolutional neural network (CNN) trained on OpenImages dataset and learns image features in a self-supervised fashion using the rotation pretext task. The decoder is a Long Short-Term Memory (LSTM), and it is trained, along within the image captioning model, on MS COCO dataset and is responsible of generating captions. Our GitHub repository can be found: https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction",0
"This study examines the application of self-supervised learning techniques to image caption generation using rotation prediction as a pretext task. The authors first introduce the core components and architecture of their proposed model and provide an overview of related work in the field of self-supervised learning for vision tasks. They then describe their experimental setup and evaluation metrics before presenting results from several ablation studies that compare different variants of the model. Finally, they conclude by discussing the limitations of the current approach and potential future directions for research. Overall, this work contributes to our understanding of how self-supervision can improve performance on complex image captioning tasks.",1
"Recent advances in the areas of multimodal machine learning and artificial intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Embodied AI. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment.",0
"In recent years, there has been significant progress in developing models that can perform tasks related to natural language processing (NLP) such as machine translation, sentiment analysis, and question answering. However, despite these advancements, embodied vision-language planning still poses many challenges that need to be addressed. This work focuses on identifying some of the core challenges in this area, including understanding the relationship between perception and action, integrating visual data with textual descriptions, dealing with uncertainty, and achieving robustness under real-world conditions. By analyzing these issues and discussing potential solutions, we aim to further advance the state-of-the-art in embodied NLP and contribute towards creating intelligent systems capable of effectively navigating and interacting with complex environments.",1
"Image manipulation with natural language, which aims to manipulate images with the guidance of language descriptions, has been a challenging problem in the fields of computer vision and natural language processing (NLP). Currently, a number of efforts have been made for this task, but their performances are still distant away from generating realistic and text-conformed manipulated images. Therefore, in this paper, we propose a memory-based Image Manipulation Network (MIM-Net), where a set of memories learned from images is introduced to synthesize the texture information with the guidance of the textual description. We propose a two-stage network with an additional reconstruction stage to learn the latent memories efficiently. To avoid the unnecessary background changes, we propose a Target Localization Unit (TLU) to focus on the manipulation of the region mentioned by the text. Moreover, to learn a robust memory, we further propose a novel randomized memory training loss. Experiments on the four popular datasets show the better performance of our method compared to the existing ones.",0
"This paper presents an innovative approach to semantic image manipulation using memory. By leveraging memory models such as transformers and convolutional neural networks (CNNs), we can generate images that adhere to specific semantic constraints while preserving their visual fidelity. We evaluate our method on multiple datasets and demonstrate its effectiveness compared to state-of-the-art methods in generating visually coherent and semantically meaningful outputs. Our work has applications in computer vision and graphics, including style transfer, editing, and content creation. Overall, this research contributes to the development of advanced artificial intelligence techniques for creative tasks, enabling new forms of artistic expression through technology.",1
"We introduce a method that allows to automatically segment images into semantically meaningful regions without human supervision. Derived regions are consistent across different images and coincide with human-defined semantic classes on some datasets. In cases where semantic regions might be hard for human to define and consistently label, our method is still able to find meaningful and consistent semantic classes. In our work, we use pretrained StyleGAN2~\cite{karras2020analyzing} generative model: clustering in the feature space of the generative model allows to discover semantic classes. Once classes are discovered, a synthetic dataset with generated images and corresponding segmentation masks can be created. After that a segmentation model is trained on the synthetic dataset and is able to generalize to real images. Additionally, by using CLIP~\cite{radford2021learning} we are able to use prompts defined in a natural language to discover some desired semantic classes. We test our method on publicly available datasets and show state-of-the-art results.",0
"In recent years there have been significant advances in computer vision tasks such as image segmentation using both traditional and deep learning approaches. However, these methods often require large amounts of labeled data and can still suffer from limited performance. Recently, unsupervised semantic image segmentation has emerged as a promising approach that enables fine-grained scene understanding without relying on extensive training datasets. This study proposes a novel unsupervised framework that utilizes Styled GANs (StyLEGAN) combined with Contrastive Language Pretraining (CLIP). Our method takes advantage of the generative capabilities of StyLEGAN to produce diverse and realistic semantic segmentations conditioned on arbitrary textual descriptions. These semantic labels are then used as supervision signals to train a feature extractor which captures relevant features for downstream segmentation tasks. By jointly optimizing the generator and discriminator objectives, we obtain high-quality, task-specific representations that generalize well across different domains and variations within each domain. We evaluate our approach on several benchmark datasets and demonstrate its effectiveness compared to state-of-the-art baseline models. We provide visualizations and ablation studies to showcase the superiority of our method in terms of generating accurate, diverse, and interpretable semantic segmentations. Additionally, we demonstrate how our model successfully transfers learned knowledge from one domain to another by performing zero-shot semantic segmentation experiments. Overall, our proposed technique offers an exciting opportunity towards developing powerful unsupervised semantic segmentation frameworks for applications in a wide range of fields including robotics, auton",1
"Self-supervised pre-training of large-scale transformer models on text corpora followed by finetuning has achieved state-of-the-art on a number of natural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247) claimed that frozen pretrained transformers (FPTs) match or outperform training from scratch as well as unfrozen (fine-tuned) pretrained transformers in a set of transfer tasks to other modalities. In our work, we find that this result is, in fact, an artifact of not tuning the learning rates. After carefully redesigning the empirical setup, we find that when tuning learning rates properly, pretrained transformers do outperform or match training from scratch in all of our tasks, but only as long as the entire model is finetuned. Thus, while transfer from pretrained language models to other modalities does indeed provide gains and hints at exciting possibilities for future work, properly tuning hyperparameters is important for arriving at robust findings.",0
"Here is an example of what I am looking for:  Paper Title: What are the Impacts of Autonomous Cars on Society?  Abstract: This paper examines the potential impacts that autonomous cars may have on society once they become widely adopted. The authors begin by discussing the current state of development of self-driving vehicles, including their capabilities and limitations. They then explore how these technologies might affect issues such as safety, congestion, environmental sustainability, and public transportation. Finally, the paper considers potential policy implications related to regulating and integrating autonomous cars into existing infrastructure. Overall, the research presented here provides valuable insights into the complexities surrounding the implementation of autonomous vehicles and highlights important areas where further research is necessary.  This could easily be adapted to fit the specific topic and style you require while still meeting length guidelines (without including the paper title). Let me know if there is any other guidance required!  ---  This paper investigates the effects of cross-modal transfer on pretrained transformer models when fine-tuning for downstream tasks. Previous studies have shown mixed results regarding whether pretraining on one task can benefit performance on another unrelated task, but little attention has been given to understanding the extent and mechanism of cross-modal transfer. To address this gap, we present a thorough analysis of three commonly used pretraining techniques - image classification, masked language modelling, and machine translation - using several established benchmark datasets from different domains. Our findings reveal interesting patterns indicating that different combinations of pretraining objectives lead to varying levels of transfer. In particular, certain types of pretraining appear more effective than others in promoting positive transfer across modalities and tasks. We hope our work encourages future exploration of cross-modal transfer and inspires new methods for designing versatile pretraining algorithms suitable for diverse NLP applications.",1
"Transformer with self-attention has led to the revolutionizing of natural language processing field, and recently inspires the emergence of Transformer-style architecture design with competitive results in numerous computer vision tasks. Nevertheless, most of existing designs directly employ self-attention over a 2D feature map to obtain the attention matrix based on pairs of isolated queries and keys at each spatial location, but leave the rich contexts among neighbor keys under-exploited. In this work, we design a novel Transformer-style module, i.e., Contextual Transformer (CoT) block, for visual recognition. Such design fully capitalizes on the contextual information among input keys to guide the learning of dynamic attention matrix and thus strengthens the capacity of visual representation. Technically, CoT block first contextually encodes input keys via a $3\times3$ convolution, leading to a static contextual representation of inputs. We further concatenate the encoded keys with input queries to learn the dynamic multi-head attention matrix through two consecutive $1\times1$ convolutions. The learnt attention matrix is multiplied by input values to achieve the dynamic contextual representation of inputs. The fusion of the static and dynamic contextual representations are finally taken as outputs. Our CoT block is appealing in the view that it can readily replace each $3\times3$ convolution in ResNet architectures, yielding a Transformer-style backbone named as Contextual Transformer Networks (CoTNet). Through extensive experiments over a wide range of applications (e.g., image recognition, object detection and instance segmentation), we validate the superiority of CoTNet as a stronger backbone. Source code is available at \url{https://github.com/JDAI-CV/CoTNet}.",0
"This paper introduces Contextual Transformer Networks (CTN), which achieve state-of-the-art results on multiple challenging visual recognition tasks by building upon efficient transformer architectures but adapting them to operate within image feature spaces rather than sequence data such as text. We show that CTNs effectively capture both spatial dependencies across pixels as well as semantic relationships across objects, enabling improved representation learning without explicit supervision or increased model complexity. CTNs significantly outperform existing methods including standard CNNs and vision transformers. Further, we discuss how attention operations can be better regularized under limited computational resources through sparsity inducing mechanisms. Our findings highlight the great potential of contextually adaptive representations based on selfattention mechanisms even in the domain of computer vision where classical convolutional models have been dominant so far. This work contributes new developments towards generalization of transformer architectures beyond text processing problems by designing principled solutions addressing structural differences between natural language and vision data. These advances build on strong theoretical grounds that bridge insights from deep learning theory with techniques exploiting hierarchical structure, novel normalization schemes, or computational efficiency measures specifically tailored for image classification and object detection scenarios. As one key takeaway message, our study reveals that merely replacing dense selfattention blocks with sparse ones is generally suboptimal whereas carefully integrating different interaction primitives at each level of scale allows leveraging taskspecific prior knowledge without losing expressiveness required by large scale datasets. In practice, these observations translate into flexible network designs providing more concise ways of approximati",1
"When humans solve complex problems, they rarely come up with a decision right-away. Instead, they start with an intuitive decision, reflect upon it, spot mistakes, resolve contradictions and jump between different hypotheses. Thus, they create a sequence of ideas and follow a train of thought that ultimately reaches a conclusive decision. Contrary to this, today's neural classification models are mostly trained to map an input to one single and fixed output. In this paper, we investigate how we can give models the opportunity of a second, third and $k$-th thought. We take inspiration from Hegel's dialectics and propose a method that turns an existing classifier's class prediction (such as the image class forest) into a sequence of predictions (such as forest $\rightarrow$ tree $\rightarrow$ mushroom). Concretely, we propose a correction module that is trained to estimate the model's correctness as well as an iterative prediction update based on the prediction's gradient. Our approach results in a dynamic system over class probability distributions $\unicode{x2014}$ the thought flow. We evaluate our method on diverse datasets and tasks from computer vision and natural language processing. We observe surprisingly complex but intuitive behavior and demonstrate that our method (i) can correct misclassifications, (ii) strengthens model performance, (iii) is robust to high levels of adversarial attacks, (iv) can increase accuracy up to 4% in a label-distribution-shift setting and (iv) provides a tool for model interpretability that uncovers model knowledge which otherwise remains invisible in a single distribution prediction.",0
"In ""Thought Flow Nets,"" we introduce the concept of thought flow nets as a novel framework that can generate and manipulate multiple models in sequence to perform complex tasks, including image generation, text translation, question answering, and more. We demonstrate how our approach can surpass single model results on several benchmarks by leveraging different types of pretrained LLMs (Large Language Models) without any fine-tuning. Our work offers new opportunities for researchers to create generative agents capable of understanding language and performing complex cognitive reasoning through human-like trains of thoughts.  The framework uses simple but powerful mechanisms, like attention pooling, to merge predictions from independent thought sequences into one coherent output. This allows us to handle multiple source domains at once while ensuring alignment of internal states between them. Our contributions consist of introducing the thought flow net idea, designing training algorithms, discussing analysis techniques, releasing code to the public domain for replication, outperforming strong baselines across datasets, reporting analyses on generated outputs and hidden activations, and presenting error diagnosis experiments to gain insights into model performance. This research has far-reaching implications in artificial intelligence, computer science, natural language processing, linguistics, philosophy, cognition, psychology, neuroscience, biological systems, engineering, data science, robotics, anthropology, sociology, education, ethics, futurology, economy, law, entertainment, media, arts, writing, design, communication, among others.",1
"We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.",0
This is the one I just came up from copying paste on the internet. Let me know if you would like me to rewrite a new version for you.,1
"This paper is a presentation of a new method for denoising images using Haralick features and further segmenting the characters using artificial neural networks. The image is divided into kernels, each of which is converted to a GLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation function is called, the result of which is an array with fourteen elements corresponding to fourteen features The Haralick values and the corresponding noise/text classification form a dictionary, which is then used to de-noise the image through kernel comparison. Segmentation is the process of extracting characters from a document and can be used when letters are separated by white space, which is an explicit boundary marker. Segmentation is the first step in many Natural Language Processing problems. This paper explores the process of segmentation using Neural Networks. While there have been numerous methods to segment characters of a document, this paper is only concerned with the accuracy of doing so using neural networks. It is imperative that the characters be segmented correctly, for failing to do so will lead to incorrect recognition by Natural language processing tools. Artificial Neural Networks was used to attain accuracy of upto 89%. This method is suitable for languages where the characters are delimited by white space. However, this method will fail to provide acceptable results when the language heavily uses connected letters. An example would be the Devanagari script, which is predominantly used in northern India.",0
"This research presents a new methodology for denoising and segmenting epigraphical scripts using machine learning techniques. We address two main challenges faced by previous methods: noise removal without losing important details, and accurate contour detection under varying image conditions. Our proposed model uses convolutional neural networks (CNN) pre-trained on synthetic data generated from real images. The network learns complex features that discriminate the script from background elements, enabling improved accuracy over traditional threshold-based approaches. In addition, we introduce a novel post-processing step that combines local edge detection and morphological operations to obtain more precise contours. Experimental results show significant improvements in both denoising and segmentation compared to current state-of-the-art techniques, particularly for degraded or low quality inscriptions. Overall, our approach represents a promising tool for digital humanities scholars working with historical documents, providing better accessibility and legibility for further analysis.",1
"Vision and Language Navigation (VLN) requires an agent to navigate to a target location by following natural language instructions. Most of existing works represent a navigation candidate by the feature of the corresponding single view where the candidate lies in. However, an instruction may mention landmarks out of the single view as references, which might lead to failures of textual-visual matching of existing methods. In this work, we propose a multi-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate visual contexts from neighbor views for better textual-visual matching. Specifically, our NvEM utilizes a subject module and a reference module to collect contexts from neighbor views. The subject module fuses neighbor views at a global level, and the reference module fuses neighbor objects at a local level. Subjects and references are adaptively determined via attention me'chanisms. Our model also includes an action module to utilize the strong orientation guidance (e.g., ""turn left"") in instructions. Each module predicts navigation action separately and their weighted sum is used for predicting the final action. Extensive experimental results demonstrate the effectiveness of the proposed method on the R2R and R4R benchmarks against several state-of-the-art navigators, and NvEM even beats some pre-training ones. Our code is available at https://github.com/MarSaKi/NvEM.",0
"Artificial intelligence systems have been rapidly improving their ability to navigate through visual scenes by using natural language instructions. However, these systems often struggle with understanding contextual relationships among objects within those scenes, such as spatial or temporal dependencies. This can lead to suboptimal performance on tasks that require reasoning about complex situations or interactions among multiple entities in the scene. To address this limitation, we propose a novel approach called ""Neighbor-View Enhanced Model"" (NVEM), which leverages object relationships from neighboring views to enhance traditional vision-and-language navigation models. Our method significantly outperforms previous state-of-the-art approaches on challenging benchmarks and demonstrates robustness under variations in environment configurations and text prompts. We evaluate our model's effectiveness in zero-shot generalization across unseen environments and discuss potential application scenarios in virtual reality and robotics. Overall, NVEM represents a significant step towards more advanced artificial agents capable of interacting with complex environments guided solely by human language direction.",1
"Language instruction plays an essential role in the natural language grounded navigation tasks. However, navigators trained with limited human-annotated instructions may have difficulties in accurately capturing key information from the complicated instruction at different timesteps, leading to poor navigation performance. In this paper, we exploit to train a more robust navigator which is capable of dynamically extracting crucial factors from the long instruction, by using an adversarial attacking paradigm. Specifically, we propose a Dynamic Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the navigator to move to the wrong target by destroying the most instructive information in instructions at different timesteps. By formulating the perturbation generation as a Markov Decision Process, DR-Attacker is optimized by the reinforcement learning algorithm to generate perturbed instructions sequentially during the navigation, according to a learnable attack score. Then, the perturbed instructions, which serve as hard samples, are used for improving the robustness of the navigator with an effective adversarial training strategy and an auxiliary self-supervised reasoning task. Experimental results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks show the superiority of our proposed method over state-of-the-art methods. Moreover, the visualization analysis shows the effectiveness of the proposed DR-Attacker, which can successfully attack crucial information in the instructions at different timesteps. Code is available at https://github.com/expectorlin/DR-Attacker.",0
"Recent advancements in natural language processing have led to the development of powerful vision-language navigation systems that enable agents to interact with their environment using natural language instructions. However, these systems can still be susceptible to attacks from adversaries who may provide malicious input designed to manipulate the behavior of the agent. In this work, we propose a new framework called ""Adversarial Reinforced Instruction Attacker"" (ARIA) which enables attackers to generate adversarial examples tailored specifically towards vision-language navigation tasks. Our approach uses reinforcement learning techniques to optimize the instruction perturbations according to different objectives such as misguiding the agent into taking dangerous actions or maximizing the number of steps taken by the agent before reaching its goal. Experimental results demonstrate the effectiveness of our method in generating successful attacks against state-of-the-art navigation models on benchmark datasets while requiring minimal computational resources. Our work highlights the importance of robustness evaluation in the field of vision-language navigation and encourages future research in developing defenses against adversarial attacks in this domain.",1
"We define disentanglement as how far class-different data points from each other are, relative to the distances among class-similar data points. When maximizing disentanglement during representation learning, we obtain a transformed feature representation where the class memberships of the data points are preserved. If the class memberships of the data points are preserved, we would have a feature representation space in which a nearest neighbour classifier or a clustering algorithm would perform well. We take advantage of this method to learn better natural language representation, and employ it on text classification and text clustering tasks. Through disentanglement, we obtain text representations with better-defined clusters and improve text classification performance. Our approach had a test classification accuracy of as high as 90.11% and test clustering accuracy of 88% on the AG News dataset, outperforming our baseline models -- without any other training tricks or regularization.",0
"This paper presents a novel approach to text classification and clustering using annealing soft nearest neighbor loss (ASNNL). ASNNL extends traditional nearest neighbor methods by allowing the distance between samples to be relaxed during training, enabling more flexible representations that capture higher order relationships between data points. We demonstrate the effectiveness of our method on several benchmark datasets, showing consistent improvements over baseline models. Our results suggest that ASNNL has strong potential as a general purpose framework for unsupervised learning tasks.",1
"We present a new four-pronged approach to build firefighter's situational awareness for the first time in the literature. We construct a series of deep learning frameworks built on top of one another to enhance the safety, efficiency, and successful completion of rescue missions conducted by firefighters in emergency first response settings. First, we used a deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real-time. Next, we extended this CNN framework for object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing(NLP) framework. Third, we built a deep Q-learning-based agent, immune to stress-induced disorientation and anxiety, capable of making clear navigation decisions based on the observed and stored facts in live-fire environments. Finally, we used a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for anomaly detection in real-time. With these ad-hoc deep learning structures, we built the artificial intelligence system's backbone for firefighters' situational awareness. To bring the designed system into usage by firefighters, we designed a physical structure where the processed results are used as inputs in the creation of an augmented reality capable of advising firefighters of their location and key features around them, which are vital to the rescue operation at hand, as well as a path planning feature that acts as a virtual guide to assist disoriented first responders in getting back to safety. When combined, these four approaches present a novel approach to information understanding, transfer, and synthesis that could dramatically improve firefighter response and efficacy and reduce life loss.",0
"This research explores how deep learning can enhance situational awareness during firefights by integrating augmented reality technologies into training simulations. We propose using artificial neural networks (ANNs) and convolutional neural network (CNNs) algorithms to process video footage from helmet cameras worn by firefighters and detect key features such as smoke density, heat sources, flame heights, and structural hazards. We then feed these outputs into an augmented reality headset, which provides real-time visualization of the inferred data overlaid onto the actual environment seen through the user’s field of view. Our approach seeks to provide greater insight into complex environments while reducing cognitive load on firefighters who must navigate dangerous situations quickly and efficiently. Results indicate our methodology improves decision making speed by up to 49% while increasing accuracy by up to 27%. By equipping first responders with advanced tools for enhanced perception, we improve their safety and increase chances of successful missions in high stakes emergency scenarios. Is there a particular aspect of the abstract that you would like me to focus on? The main objective of this study was to explore the potential benefits of integrating deep learning techniques with AR technology for use in firefighting operations. Helmet camera footage captured from firefighters was analyzed with artificial neural networks and CNNs, allowing for real-time identification of critical factors affecting fire behavior including smoke density, heat sources, flame height, and structural conditions. These results were projected through an AR display viewed by the firefighter in order to gain additional insights into the unfolding situation without diverting attention away from other important tasks. The findings showed that this integrated approach could greatly benefit firefighters by providing them more accurate and timely information, potentially improving response times and minimizing risk exposure in challenging circumstances. Ultimately, the goal of this research is to better equip those serving in harm's way to protect lives and property.",1
"Deep learning (DL) techniques have achieved great success in predictive accuracy in a variety of tasks, but deep neural networks (DNNs) are shown to produce highly overconfident scores for even abnormal samples. Well-defined uncertainty indicates whether a model's output should (or should not) be trusted and thus becomes critical in real-world scenarios which typically involves shifted input distributions due to many factors. Existing uncertainty approaches assume that testing samples from a different data distribution would induce unreliable model predictions thus have higher uncertainty scores. They quantify model uncertainty by calibrating DL model's confidence of a given input and evaluate the effectiveness in computer vision (CV) and natural language processing (NLP)-related tasks. However, their methodologies' reliability may be compromised under programming tasks due to difference in data representations and shift patterns. In this paper, we first define three different types of distribution shift in program data and build a large-scale shifted Java dataset. We implement two common programming language tasks on our dataset to study the effect of each distribution shift on DL model performance. We also propose a large-scale benchmark of existing state-of-the-art predictive uncertainty on programming tasks and investigate their effectiveness under data distribution shift. Experiments show that program distribution shift does degrade the DL model performance to varying degrees and that existing uncertainty methods all present certain limitations in quantifying uncertainty on program dataset.",0
"In many real-world applications, predicting outcomes based on data can be challenging due to changes in the distribution of input variables over time. This phenomenon, known as program data distribution shift (PDDS), can lead to degraded performance of machine learning models and reduced confidence in their predictions. To address these issues, we propose a novel methodology for estimating predictive uncertainty under PDDS conditions. Our approach uses a combination of Bayesian methods and domain knowledge to quantify the impact of shifting distributions on model accuracy and reliability. We evaluate our framework using simulation studies and demonstrate its effectiveness through case studies involving financial forecasting and medical diagnosis tasks. Our findings show that our proposed method leads to significantly improved prediction intervals compared to existing approaches, resulting in more robust decision making processes even under uncertain conditions. Overall, our work contributes towards building more resilient predictive systems capable of handling unforeseen changes in data characteristics, which is crucial for ensuring trustworthy artificial intelligence (AI) systems.",1
"Faces generated using generative adversarial networks (GANs) have reached unprecedented realism. These faces, also known as ""Deep Fakes"", appear as realistic photographs with very little pixel-level distortions. While some work has enabled the training of models that lead to the generation of specific properties of the subject, generating a facial image based on a natural language description has not been fully explored. For security and criminal identification, the ability to provide a GAN-based system that works like a sketch artist would be incredibly useful. In this paper, we present a novel approach to generate facial images from semantic text descriptions. The learned model is provided with a text description and an outline of the type of face, which the model uses to sketch the features. Our models are trained using an Affine Combination Module (ACM) mechanism to combine the text embedding from BERT and the GAN latent space using a self-attention matrix. This avoids the loss of features due to inadequate ""attention"", which may happen if text embedding and latent vector are simply concatenated. Our approach is capable of generating images that are very accurately aligned to the exhaustive textual descriptions of faces with many fine detail features of the face and helps in generating better images. The proposed method is also capable of making incremental changes to a previously generated image if it is provided with additional textual descriptions or sentences.",0
"Introduce any terminology that needs explaining before using it in your abstract! Also provide details on who might find it relevant. --  In recent years, Generative Adversarial Networks (GAN) have become increasingly popular for generating realistic synthetic data such as images, videos, and audio signals. One major limitation of these models, however, is their lack of interpretability. In other words, while they can generate visually appealing outputs, they struggle to explain how they achieve those results. This has limited their adoption in fields where there is a need for explicit meaning extraction from multimedia content. To address this challenge, we propose a novel model called ""Semantic Text-to-Face GAN"" (or ST^2FG). Our model combines the strengths of GANs with textual semantic representations, enabling it to produce more interpretable results by mapping input text descriptions into corresponding face images. We demonstrate the effectiveness of our method through extensive experiments on two challenging benchmark datasets: Flickr-Faces-HQ and CelebA-HQ. Overall, our work has important implications across many domains ranging from computer graphics to facial recognition systems and natural language processing.",1
"Deep learning's success has been widely recognized in a variety of machine learning tasks, including image classification, audio recognition, and natural language processing. As an extension of deep learning beyond these domains, graph neural networks (GNNs) are designed to handle the non-Euclidean graph-structure which is intractable to previous deep learning techniques. Existing GNNs are presented using various techniques, making direct comparison and cross-reference more complex. Although existing studies categorize GNNs into spatial-based and spectral-based techniques, there hasn't been a thorough examination of their relationship. To close this gap, this study presents a single framework that systematically incorporates most GNNs. We organize existing GNNs into spatial and spectral domains, as well as expose the connections within each domain. A review of spectral graph theory and approximation theory builds a strong relationship across the spatial and spectral domains in further investigation.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for processing data represented as graphs. While many studies focus on bridging the gap between spatial and spectral domains using GNNs, there remains a significant research gap. To address this gap, we present a comprehensive survey that explores state-of-the-art approaches for merging topological information from both domains. Our review highlights key insights into advances made in designing efficient algorithms and models capable of exploiting both domain representations, including convolutional graph neural networks, message passing methods, graph attention mechanisms, and multi-channel techniques. We further discuss how these innovations can improve applications such as node classification, link prediction, graph clustering, and anomaly detection in diverse fields ranging from computer vision to natural language processing. Finally, we outline future directions aimed at promoting further progress toward achieving seamless integration across multiple domains in GNNs.",1
"Communication between agents in collaborative multi-agent settings is in general implicit or a direct data stream. This paper considers text-based natural language as a novel form of communication between multiple agents trained with reinforcement learning. This could be considered first steps toward a truly autonomous communication without the need to define a limited set of instructions, and natural collaboration between humans and robots. Inspired by the game of Blind Leads, we propose an environment where one agent uses natural language instructions to guide another through a maze. We test the ability of reinforcement learning agents to effectively communicate through discrete word-level symbols and show that the agents are able to sufficiently communicate through natural language with a limited vocabulary. Although the communication is not always perfect English, the agents are still able to navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of 0.61 over randomly generated sequences while maintaining a 100% maze completion rate. This is a 3.5 times the performance of the random baseline using our reference set.",0
"In recent years there has been increased interest in developing artificial intelligence agents capable of learning from experience, commonly referred to as reinforcement learning (RL) algorithms. These methods have shown impressive results in domains such as video games and robotics control tasks. However, many RL algorithms lack interpretability due to their complex nature and black box design. As RL becomes more widely adopted across various industries, explainability is becoming increasingly important. To address these shortcomings, we propose using natural language to communicate with collaborative reinforcement learning agents. By doing so, humans can provide feedback on agent behavior, ensuring that they act according to desired objectives and constraints. This study presents two text communication frameworks suitable for collaboration: one based on simple prompts, and another using a pre-trained GPT model for generating instructions from human feedback through interactive reward augmentation. Our experimental evaluation shows consistent improvement over standard RL policies using human feedback provided via text messages. These contributions towards creating interpretable RL agents with enhanced performance set the stage for future research into multiagent environments where effective collaboration is necessary for successful decision making.",1
"Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as ""one big sequence modeling"" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.",0
"In natural language processing (NLP), sequence modeling deals with predicting or generating sequences of tokens such as text or speech. Recurrent neural networks (RNNs) were once popular models used to solve these problems due to their ability to retain information over time by using hidden states that carry context across iterations. However, RNNs suffer from vanishing gradient issues which makes them difficult to train on tasks with long dependencies. This issue led researchers to propose alternatives like Gated Recurrent Units (GRUs) and Long Short-Term Memory Networks (LSTMs). These solutions have improved upon some shortcomings of traditional RNN architectures but still face challenges related to scalability, efficiency, and explainability. In recent years, transformer architectures have emerged as a new paradigm for NLP and have largely replaced recurrent models owing to their superior performance in terms of accuracy and training speed, especially when dealing with large datasets. Transformers use self attention mechanisms that allow parallel computation without losing sequential information unlike previous approaches. They operate exclusively on attention rather than relying on recurrence making them more powerful in certain scenarios. Despite this progress, there remains room for improvement to enhance interpretability and reduce computational overhead when dealing with limited data or small models. Reinforcement learning provides one approach towards addressing these limitations. The objective of this work is to explore sequence modeling through reinforcement learning where we formulate each token prediction step as a Markov decision process",1
"Developing video understanding intelligence is quite challenging because it requires holistic integration of images, scripts, and sounds based on natural language processing, temporal dependency, and reasoning. Recently, substantial attempts have been made on several video datasets with associated question answering (QA) on a large scale. However, existing evaluation metrics for video question answering (VideoQA) do not provide meaningful analysis. To make progress, we argue that a well-made framework, established on the way humans understand, is required to explain and evaluate the performance of understanding in detail. Then we propose a top-down evaluation system for VideoQA, based on the cognitive process of humans and story elements: Cognitive Modules for Evaluation (CogME). CogME is composed of three cognitive modules: targets, contents, and thinking. The interaction among the modules in the understanding procedure can be expressed in one sentence as follows: ""I understand the CONTENT of the TARGET through a way of THINKING."" Each module has sub-components derived from the story elements. We can specify the required aspects of understanding by annotating the sub-components to individual questions. CogME thus provides a framework for an elaborated specification of VideoQA datasets. To examine the suitability of a VideoQA dataset for validating video understanding intelligence, we evaluated the baseline model of the DramaQA dataset by applying CogME. The evaluation reveals that story elements are unevenly reflected in the existing dataset, and the model based on the dataset may cause biased predictions. Although this study has only been able to grasp a narrow range of stories, we expect that it offers the first step in considering the cognitive process of humans on the video understanding intelligence of humans and AI.",0
"Effective evaluation metrics play a crucial role in assessing the performance of video understanding models. In recent years, several metrics have been proposed that aim to measure different aspects of video intelligence such as object detection accuracy, action recognition, temporal segmentation, and spatio-temporal reasoning. However, these existing metrics often suffer from limitations like overfitting to specific datasets, poor correlation with human judgments, or high computational complexity. To address these challenges, we propose a new metric called Cognitive Measures of Excellence (CogME) which incorporates both quantitative and qualitative measures to evaluate multiple facets of video intelligence including visual fidelity, semantic meaning, narrative structure, attention control, causality modeling, memory recall, and context awareness. Our approach utilizes state-of-the-art deep learning techniques along with cognitively inspired frameworks to ensure robustness, interpretability, and generalization across diverse datasets. We empirically validate our method on benchmark datasets for action recognition, event localization, activity detection, and storyboard generation tasks, showing significant improvements compared to existing metrics. Overall, CogME provides a more comprehensive and accurate evaluation framework that aligns better with human perception and expectations of video understanding systems. This work paves the way for developing smarter AI systems with advanced cognitive abilities that can perform complex real-world tasks with greater efficiency and effectiveness.",1
"Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHighlights) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr",0
"Abstract: In this paper, we present QVHighlights, a system that enables users to easily navigate through videos by specifying natural language queries. Our approach uses state-of-the-art deep learning techniques to detect moments and highlights in videos based on these queries. We demonstrate the effectiveness of our method using real-world datasets and showcase how QVHighlights can provide more accurate results than traditional methods such as keyword-based retrieval. Additionally, we evaluate user feedback and engagement with QVHighlights to confirm its potential for improving video search and navigation systems. Overall, our work has important implications for multimedia processing research and practice, enabling efficient access to key parts of videos through intuitive and flexible human-computer interaction.",1
"We develop a novel approach to conformal prediction when the target task has limited data available for training. Conformal prediction identifies a small set of promising output candidates in place of a single prediction, with guarantees that the set contains the correct answer with high probability. When training data is limited, however, the predicted set can easily become unusably large. In this work, we obtain substantially tighter prediction sets while maintaining desirable marginal guarantees by casting conformal prediction as a meta-learning paradigm over exchangeable collections of auxiliary tasks. Our conformalization algorithm is simple, fast, and agnostic to the choice of underlying model, learning algorithm, or dataset. We demonstrate the effectiveness of this approach across a number of few-shot classification and regression tasks in natural language processing, computer vision, and computational chemistry for drug discovery.",0
"In recent years, few-shot learning has emerged as a promising approach to enable machines to learn from small amounts of data. One challenge that remains in many applications is how to adapt these methods to real world settings where there may be noise, missing values, and other types of uncertainty. To address this issue, we propose a novel method called ""Few-Shot Conformal Prediction with Auxiliary Tasks"" (FSCP). Our framework uses auxiliary tasks in conjunction with conformal prediction techniques to improve model robustness and provide confidence estimates for predictions made under uncertain conditions. We evaluate our approach on several benchmark datasets across different domains and show that it outperforms state-of-the-art baselines in terms of accuracy and reliability. Overall, our work demonstrates that combining conformal prediction with auxiliary tasks can effectively solve challenges associated with making accurate predictions in high-dimensional spaces even when faced with limited training data and significant uncertainty.",1
"Transformers have been successful for many natural language processing tasks. However, applying transformers to the video domain for tasks such as long-term video generation and scene understanding has remained elusive due to the high computational complexity and the lack of natural tokenization. In this paper, we propose the Object-Centric Video Transformer (OCVT) which utilizes an object-centric approach for decomposing scenes into tokens suitable for use in a generative video transformer. By factoring the video into objects, our fully unsupervised model is able to learn complex spatio-temporal dynamics of multiple interacting objects in a scene and generate future frames of the video. Our model is also significantly more memory-efficient than pixel-based models and thus able to train on videos of length up to 70 frames with a single 48GB GPU. We compare our model with previous RNN-based approaches as well as other possible video transformer baselines. We demonstrate OCVT performs well when compared to baselines in generating future frames. OCVT also develops useful representations for video reasoning, achieving start-of-the-art performance on the CATER task.",0
"This paper proposes a novel approach to video generation using transformers that allows objects within the scene to serve as ""words"" in a sentence. By treating each object as a discrete unit, we can generate new sequences by simply rearranging these objects according to desired grammatical rules. Our model takes advantage of recent advances in generative pretraining and image synthesis to create high quality results that capture both visual fidelity and semantic coherence. We evaluate our method on several challenging benchmarks and demonstrate significant improvements over baseline methods. In summary, our work shows that using objects as building blocks for video generation is a promising direction with exciting potential applications in areas such as computer graphics, animation, and virtual reality.",1
"Video captioning, i.e. the task of generating captions from video sequences creates a bridge between the Natural Language Processing and Computer Vision domains of computer science. The task of generating a semantically accurate description of a video is quite complex. Considering the complexity, of the problem, the results obtained in recent research works are praiseworthy. However, there is plenty of scope for further investigation. This paper addresses this scope and proposes a novel solution. Most video captioning models comprise two sequential/recurrent layers - one as a video-to-context encoder and the other as a context-to-caption decoder. This paper proposes a novel architecture, namely Semantically Sensible Video Captioning (SSVC) which modifies the context generation mechanism by using two novel approaches - ""stacked attention"" and ""spatial hard pull"". As there are no exclusive metrics for evaluating video captioning models, we emphasize both quantitative and qualitative analysis of our model. Hence, we have used the BLEU scoring metric for quantitative analysis and have proposed a human evaluation metric for qualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS Score overcomes the shortcomings of common automated scoring metrics. This paper reports that the use of the aforementioned novelties improves the performance of state-of-the-art architectures.",0
"Title: ""Stacked Attention and Semantic Hard Pull for Improved Video Captioning""  This paper presents a novel approach to video captioning that combines stacked attention and semantic hard pull techniques to improve the accuracy and coherence of generated captions. In traditional approaches to video captioning, attention mechanisms are used to weigh the importance of different input frames when predicting each output word. However, these methods can still suffer from semantic ambiguity and incoherency in the resulting captions. To address these issues, our proposed method utilizes a two-stage process involving both local and global context. Firstly, a local context module uses stacked attention to weight the contribution of individual frames within short time intervals (local windows). This helps capture fine-grained relationships among temporally adjacent features and improves the model’s ability to resolve ambiguous semantics. Secondly, a global context module employs semantic hard pull, which forces the model to attend to high-level concepts present throughout the entire video sequence. By incorporating both local and global context, our model generates more accurate and semantically meaningful captions than state-of-the-art video captioning systems. Our experiments on benchmark datasets demonstrate the effectiveness of our proposed method in terms of both quantitative metrics and human evaluations. Overall, this work represents a significant step forward in advancing the field of automatic video description and has promising applications in areas such as accessibility, entertainment, and surveillance.",1
"Academic advances of AI models in high-precision domains, like healthcare, need to be made explainable in order to enhance real-world adoption. Our past studies and ongoing interactions indicate that medical experts can use AI systems with greater trust if there are ways to connect the model inferences about patients to explanations that are tied back to the context of use. Specifically, risk prediction is a complex problem of diagnostic and interventional importance to clinicians wherein they consult different sources to make decisions. To enable the adoption of the ever improving AI risk prediction models in practice, we have begun to explore techniques to contextualize such models along three dimensions of interest: the patients' clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We validate the importance of these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes (T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a common T2DM comorbidity. Within the POC, we include risk prediction models for CKD, post-hoc explainers of the predictions, and other natural-language modules which operationalize domain knowledge and CPGs to provide context. With primary care physicians (PCP) as our end-users, we present our initial results and clinician feedback in this paper. Our POC approach covers multiple knowledge sources and clinical scenarios, blends knowledge to explain data and predictions to PCPs, and received an enthusiastic response from our medical expert.",0
"One possible approach for creating an effective AI language model is to use data from a wide range of domains and tasks, allowing the model to learn patterns that are relevant across different contexts and applications. This can make the resulting system more flexible and adaptive, able to handle a wider variety of inputs and questions. Additionally, utilizing diverse training data allows the AI to capture relationships between concepts and entities that may not have been explicitly defined by human developers, potentially leading to enhanced performance on novel problems. Incorporating clinical records into conversational AI models has shown promising results in improving understanding of health related conditions and treatments as well as personalized recommendations based on individual patient histories. Our research explores user-centered explainability of diabetes treatment plans using a large corpus of medical knowledge derived from academic literature and structured electronic health record data collected during routine care. We then evaluate how different levels of explanatory detail impact user trust, perceived expertise and confidence in accepting automated treatment advice. Our findings suggest that providing tailored explanations can increase acceptance rates for automated treatment plans while maintaining high levels of perceived expertise and trustworthiness. Overall, our study suggests that leveraging large scale datasets and incorporating them within NLP systems can drive significant advancements towards more human like interactions and ultimately lead to improved patient outcomes via automation.",1
"Black-box machine learning learning methods are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Distribution-free uncertainty quantification (distribution-free UQ) is a user-friendly paradigm for creating statistically rigorous confidence intervals/sets for such predictions. Critically, the intervals/sets are valid without distributional assumptions or model assumptions, with explicit guarantees with finitely many datapoints. Moreover, they adapt to the difficulty of the input; when the input example is difficult, the uncertainty intervals/sets are large, signaling that the model might be wrong. Without much work, one can use distribution-free methods on any underlying algorithm, such as a neural network, to produce confidence sets guaranteed to contain the ground truth with a user-specified probability, such as 90%. Indeed, the methods are easy-to-understand and general, applying to many modern prediction problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed at a reader interested in the practical implementation of distribution-free UQ, including conformal prediction and related methods, who is not necessarily a statistician. We will include many explanatory illustrations, examples, and code samples in Python, with PyTorch syntax. The goal is to provide the reader a working understanding of distribution-free UQ, allowing them to put confidence intervals on their algorithms, with one self-contained document.",0
"Abstract: In machine learning, uncertainty quantification is an important task that involves estimating how confident we can be about predictions made by our models. One popular method for uncertainty quantification is conformal prediction, which allows us to compute guaranteed error rates (also known as ""credible regions"") without any assumptions about the underlying distribution of the data. This makes conformal prediction particularly useful when little is known about the data, such as when dealing with outliers or noisy data. In this work, we provide a gentle introduction to conformal prediction and show how it can be used for distribution-free uncertainty quantification. We start by reviewing some basic concepts from probability theory and statistical inference, then introduce conformal prediction and explain how it works. Finally, we illustrate the use of conformal prediction through several examples using both simulated and real datasets, demonstrating its effectiveness and versatility. Overall, this paper provides a concise yet comprehensive overview of conformal prediction and highlights its potential applications in various fields where uncertainty quantification is crucial.",1
"Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-of-the-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. In this paper, we investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of KL divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling and dialog-response generation tasks demonstrate the winning effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.",0
"The hyperboloid model has been used successfully as a generalization of the vanilla VAE (Variational Autoencoder) architecture that can generate images. This work extends the use of hyperbolic space to text generation by introducing the hyperbolically constrained flowing autoencoder or Apo-VAE. We show on benchmark datasets how our approach outperforms other models for inference and sampling tasks. Additionally we evaluate the importance of using a hyperbolic latent distribution over a spherical one, emphasizing their effectiveness in generating coherent and diverse text outputs while controlling the level of hallucination present within generated sentences. Our method thus provides an alternative to traditional VAEs for natural language processing applications and offers interesting perspectives on the understanding of texts through non-Euclidean spaces.",1
"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of ""a benchmark lottery"" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.",0
"""This"" (The) Benchmark Lottery: A Comprehensive Guide for Choosing Effective Datasets =====================================================================================  The choice of benchmark datasets has become increasingly important as machine learning models have been integrated into many facets of our lives. Despite their widespread use, there remains no systematic way to choose effective benchmark datasets. In response, we propose the ""Benchmark Lottery,"" a framework that allows practitioners to make informed decisions about which benchmark dataset to use based on specific needs and characteristics. We describe how to create lotteries using two different approaches - randomization and non-randomization methods - as well as how to evaluate them by measuring both effectiveness and efficiency metrics. Through case studies, we demonstrate the utility of the Benchmark Lottery framework in real-world scenarios such as selecting the most appropriate benchmarks for image classification tasks in wildlife monitoring applications or choosing suitable benchmark datasets for developing sentiment analysis models for social media platforms. Overall, the Benchmark Lottery provides an accessible methodology for making better benchmark choices while minimizing human bias, promoting diversity in model evaluations, and fostering trustworthy Artificial Intelligence systems. # Full text without ""this"":  Introduction ------------  The choice of benchmark datasets has become increasingly critical due to the rapid integration of machine learning models into various aspects of society. However, a standardized approach to selecting effective benchmark datasets is lacking, leading to suboptimal evaluations of these models. This deficiency poses substantial risks to users, developers, organizations, and societies at large, since inferior evaluations can result in deceptive claims, harmful biases, unintended consequences, and other unwanted impacts. To address this challenge, we introduce the concept of a ""Benchmark Lottery"" in order to assist decision makers in selecting appropriate benchmark datasets according to predefined requirements and constraints. Our approach incorporates tw",1
"Visual Question Answering (VQA) is concerned with answering free-form questions about an image. Since it requires a deep semantic and linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires multi-modal reasoning from both computer vision and natural language processing. We propose Graphhopper, a novel method that approaches the task by integrating knowledge graph reasoning, computer vision, and natural language processing techniques. Concretely, our method is based on performing context-driven, sequential reasoning based on the scene entities and their semantic and spatial relationships. As a first step, we derive a scene graph that describes the objects in the image, as well as their attributes and their mutual relationships. Subsequently, a reinforcement learning agent is trained to autonomously navigate in a multi-hop manner over the extracted scene graph to generate reasoning paths, which are the basis for deriving answers. We conduct an experimental study on the challenging dataset GQA, based on both manually curated and automatically generated scene graphs. Our results show that we keep up with a human performance on manually curated scene graphs. Moreover, we find that Graphhopper outperforms another state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin.",0
"This paper presents Graphhopper, a novel multi-hop scene graph reasoning method for visual question answering (VQA). VQA is a challenging task that involves understanding images and natural language queries to generate answers. Existing methods have focused on either local feature analysis or global context modeling but lack effective ways to integrate these two levels of knowledge. Graphhopper addresses this issue by proposing a hierarchical scene representation where nodes represent image regions, edges indicate spatial relationships, and each edge can correspond to multiple attributes. This allows us to perform multi-hop reasoning over scenes through iteratively updating the node features using messages from neighboring regions. We then use a neural network to read out the final answer based on the updated node features and their dependencies within the whole scene graph structure. Extensive experiments demonstrate that our approach significantly improves over state-of-the-art results on major benchmarks while providing clear insights into how scene graphs guide the reasoning process. By bridging local perception and holistic comprehension, our work takes a step towards more human-like VQA models capable of complex inference and decision making.",1
"We address the problem of text-guided video temporal grounding, which aims to identify the time interval of certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain event, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.",0
"In recent years, multi-modal video temporal grounding has become an important research area due to its applications in fields such as autonomous vehicles, robotics, and human-computer interaction. One approach to address this problem is by using end-to-end deep learning models that can jointly model visual and textual modalities without requiring hand-engineered features or explicit supervision for alignment. This paper proposes a novel end-to-end multi-modal video temporal grounding method based on transformers, which have recently achieved state-of-the-art results in natural language processing tasks. Our proposed method leverages self-attention mechanisms to capture global dependencies across modalities, allowing the model to accurately align temporal boundaries across different granularity levels. We conduct comprehensive experiments on three challenging benchmark datasets (ActivityNet Captions, MSVD, and TACoS) and demonstrate that our method outperforms existing state-of-the-art methods. Additionally, we perform ablation studies to analyze the effectiveness of each component in our system, further validating the importance of cross-modality attention modules for successful end-to-end multi-modal video temporal grounding. Overall, our work represents a significant step forward towards achieving efficient and accurate multi-modal video understanding systems.",1
"With the rise of voice chat rooms, a gigantic resource of data can be exposed to the research community for natural language processing tasks. Moderators in voice chat rooms actively monitor the discussions and remove the participants with offensive language. However, it makes the hate speech detection even more difficult since some participants try to find creative ways to articulate hate speech. This makes the hate speech detection challenging in new social media like Clubhouse. To the best of our knowledge all the hate speech datasets have been collected from text resources like Twitter. In this paper, we take the first step to collect a significant dataset from Clubhouse as the rising star in social media industry. We analyze the collected instances from statistical point of view using the Google Perspective Scores. Our experiments show that, the Perspective Scores can outperform Bag of Words and Word2Vec as high level text features.",0
"Title: Detecting Hateful Language on GitHub using Machine Learning  GitHub has implemented several tools designed to prevent harrassment on their platform. This study examines one such tool - Clubhouse - which allows users to report inappropriate language by labeling them as ""Uncivil"" messages. The authors aim to create a machine learning model capable of identifying uncivil messages accurately without relying heavily on supervised training data. To achieve this goal, they use a combination of preprocessing techniques, including tokenization, stopword removal, stemming, and Part Of Speech (POS) tagging. They then train their model using different features extracted from each message and evaluate its performance through precision, recall, F1 score, and accuracy metrics. Results show that their proposed method achieves high accuracy in detecting civil vs uncivil messages compared to traditional rule-based methods. The results provide promising evidence towards building intelligent platforms like Clubhouse more efficiently.  This paper presents a novel approach for detecting hate speech on GitHub's platform. Specifically, we focus on the Clubhouse feature, which allows users to flag messages containing uncivil language. Our work seeks to develop a machine learning algorithm capable of recognizing uncivil messages without requiring extensive amounts of labeled data. We achieve this through careful preprocessing steps, including text cleanup, POS tagging, and feature extraction. Our experimental evaluation demonstrates significant improvements over traditional rule-based methods across multiple metrics, suggesting that our model can effectively identify incidents of hate speech. Overall, our findings have implications for building more efficient online communities and mitigating harassment on social media platforms.",1
"Compared to consumer lending, Micro, Small and Medium Enterprise (mSME) credit risk modelling is particularly challenging, as, often, the same sources of information are not available. Therefore, it is standard policy for a loan officer to provide a textual loan assessment to mitigate limited data availability. In turn, this statement is analysed by a credit expert alongside any available standard credit data. In our paper, we exploit recent advances from the field of Deep Learning and Natural Language Processing (NLP), including the BERT (Bidirectional Encoder Representations from Transformers) model, to extract information from 60 000 textual assessments provided by a lender. We consider the performance in terms of the AUC (Area Under the receiver operating characteristic Curve) and Brier Score metrics and find that the text alone is surprisingly effective for predicting default. However, when combined with traditional data, it yields no additional predictive capability, with performance dependent on the text's length. Our proposed deep learning model does, however, appear to be robust to the quality of the text and therefore suitable for partly automating the mSME lending process. We also demonstrate how the content of loan assessments influences performance, leading us to a series of recommendations on a new strategy for collecting future mSME loan assessments.",0
"This paper investigates the use of natural language processing techniques to create accurate models predicting which individuals or companies are likely to go bankrupt. We analyze records from multiple industries, finding common patterns across all data sets which can be used as features for prediction models. Our models achieve higher accuracy than traditional approaches using simple financial indicators like debt ratio or liquidity metrics alone. Furthermore, we show that our models generalize well by testing them on previously unseen industry datasets. By combining large amounts of textual data with powerful machine learning algorithms, we demonstrate that NLP can effectively forecast defaults even without access to sensitive financial statements or customer credit scores. Our results have implications for risk management, fraud detection, and credit underwriting, among other applications.",1
"Medical Visual Question Answering (VQA) is a multi-modal challenging task widely considered by research communities of the computer vision and natural language processing. Since most current medical VQA models focus on visual content, ignoring the importance of text, this paper proposes a multi-view attention-based model(MuVAM) for medical visual question answering which integrates the high-level semantics of medical images on the basis of text description. Firstly, different methods are utilized to extract the features of the image and the question for the two modalities of vision and text. Secondly, this paper proposes a multi-view attention mechanism that include Image-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view attention can correlate the question with image and word in order to better analyze the question and get an accurate answer. Thirdly, a composite loss is presented to predict the answer accurately after multi-modal feature fusion and improve the similarity between visual and textual cross-modal features. It consists of classification loss and image-question complementary (IQC) loss. Finally, for data errors and missing labels in the VQA-RAD dataset, we collaborate with medical experts to correct and complete this dataset and then construct an enhanced dataset, VQA-RADPh. The experiments on these two datasets show that the effectiveness of MuVAM surpasses the state-of-the-art method.",0
"In recent years, visual question answering (VQA) has emerged as a challenging task that requires both computer vision and natural language processing capabilities. VQA systems aim to provide answers to questions asked about images by accurately understanding the content of those images and how they relate to the textual queries posed. However, most existing VQA models rely heavily on convolutional neural networks (CNNs), which can suffer from limited attention mechanisms, resulting in suboptimal performance.  This study presents a new multi-view attention-based model called MuVAM for medical VQA tasks, addressing these limitations. Our proposed model employs multiple view modules, each focusing on different aspects of image representation such as object detection, region feature extraction, and scene classification. These views are then fused using attention mechanisms based on spatial relations among objects detected in the image. Additionally, we introduce a novel attention mechanism that considers the relevance of individual CNN layers when aggregating information from different views.  We evaluate our approach on two benchmark datasets, namely MCQA and NDSToxDB. Experimental results show that our model achieves significant improvements over several state-of-the-art methods, demonstrating its effectiveness at providing accurate answers to complex medical VQA tasks. Furthermore, ablation studies reveal the importance of incorporating different types of attention mechanisms in our model.  In summary, our work advances current research in VQA by introducing a multi-view attention-based model specifically tailored for medical applications. By utilizing multiple sources of information from the input image and leveraging innovative attention mechanisms, we achieve superior performance compared to existing approaches. This contribution paves the way for more advanced VQA systems capable of handling diverse domains and improving healthcare informatics practices.",1
"Attribute extrapolation in sample generation is challenging for deep neural networks operating beyond the training distribution. We formulate a new task for extrapolation in sequence generation, focusing on natural language and proteins, and propose GENhance, a generative framework that enhances attributes through a learned latent space. Trained on movie reviews and a computed protein stability dataset, GENhance can generate strongly-positive text reviews and highly stable protein sequences without being exposed to similar data during training. We release our benchmark tasks and models to contribute to the study of generative modeling extrapolation and data-driven design in biology and chemistry.",0
"AI language models have made significant strides in recent years due to advances in deep learning techniques and access to large datasets. However, most existing approaches rely on generation based solely on local patterns rather than exploiting high-level attributes that provide meaningful interpretations of complex scenes and interactions within them. Our approach addresses these limitations by proposing a novel methodology that utilizes attribute embedding spaces to capture rich descriptions of scene content and generate natural responses accordingly. In our experiments we demonstrate significantly improved results over baseline methods across multiple benchmark tasks while maintaining low computational overhead. This research has far reaching implications for a variety of applications including image and video synthesis as well as dialogue systems and virtual environments. Further work may expand upon this initial framework through integration into larger model architectures such as transformers which could potentially benefit from incorporating more global knowledge during inference.",1
"In this article, we present a Shell Language Preprocessing (SLP) library, which implements tokenization and encoding directed on the parsing of Unix and Linux shell commands. We describe the rationale behind the need for a new approach with specific examples when conventional Natural Language Processing (NLP) pipelines fail. Furthermore, we evaluate our methodology on a security classification task against widely accepted information and communications technology (ICT) tokenization techniques and achieve significant improvement of an F1-score from 0.392 to 0.874.",0
"In ""Shell language processing: Unix command parsing for machine learning,"" we present a novel methodology that leverages natural language programming (NLP) techniques to parse and interpret Unix commands for machine learning applications. We argue that traditional NLP approaches have failed to fully exploit the unique characteristics of shell languages - characterized by their conciseness, flexibility, and extensibility - due to their focus on text documents rather than executable code. To address this shortcoming, our approach introduces a dedicated module responsible for handling the specific grammar, contexts, and constraints of shell scripts. This enables us to more accurately capture the semantics and intent behind each line of code while respecting its unique syntax. Our experimental evaluation demonstrates the effectiveness of our proposed solution in terms of accuracy and efficiency, making it a promising foundation for future work in automating script development and maintenance tasks. By bridging the gap between shell languages and ML research, our study contributes to advancing the state of art in both areas while enabling new possibilities and use cases.",1
"Humans and other intelligent animals evolved highly sophisticated perception systems that combine multiple sensory modalities. On the other hand, state-of-the-art artificial agents rely mostly on visual inputs or structured low-dimensional observations provided by instrumented environments. Learning to act based on combined visual and auditory inputs is still a new topic of research that has not been explored beyond simple scenarios. To facilitate progress in this area we introduce a new version of VizDoom simulator to create a highly efficient learning environment that provides raw audio observations. We study the performance of different model architectures in a series of tasks that require the agent to recognize sounds and execute instructions given in natural language. Finally, we train our agent to play the full game of Doom and find that it can consistently defeat a traditional vision-based adversary. We are currently in the process of merging the augmented simulator with the main ViZDoom code repository. Video demonstrations and experiment code can be found at https://sites.google.com/view/sound-rl.",0
"In recent years there has been growing interest in developing agents capable of effectively utilizing multiple sensory systems in order to perform complex tasks. While traditional reinforcement learning algorithms have been successful in single-sensory environments, they often struggle when confronted with high degrees of multisensory complexity. This work proposes a novel approach to addressing this challenge by introducing high-throughput reinforcement learning with multiple sensory integration. By leveraging advanced techniques from computer vision, auditory processing, and natural language understanding, our proposed method enables agents to efficiently process vast amounts of sensory data while maintaining robustness across different domains and modalities. Experimental results demonstrate the effectiveness of our approach in achieving state-of-the-art performance on several benchmark datasets spanning diverse environments such as video games, robotics, and autonomous driving simulations. These findings contribute to broader efforts aimed at building intelligent agents able to tackle real-world problems involving rich and dynamic perceptual inputs.",1
"In recent years, artificial intelligence (AI) systems have come to the forefront. These systems, mostly based on Deep learning (DL), achieve excellent results in areas such as image processing, natural language processing, or speech recognition. Despite the statistically high accuracy of deep learning models, their output is often a decision of ""black box"". Thus, Interpretability methods have become a popular way to gain insight into the decision-making process of deep learning models. Explanation of a deep learning model is desirable in the medical domain since the experts have to justify their judgments to the patient. In this work, we proposed a method for explanation-guided training that uses a Layer-wise relevance propagation (LRP) technique to force the model to focus only on the relevant part of the image. We experimentally verified our method on a convolutional neural network (CNN) model for low-grade and high-grade glioma classification problems. Our experiments show promising results in a way to use interpretation techniques in the model training process.",0
"Abstarct: This paper explores the use of explanation-guided training (EGT) to improve the performance of neural networks for the task of classifying gliomas in MRI brain scans. Traditional machine learning approaches have had limited success due to difficulties in obtaining large amounts of labeled data and accurately representing complex visual features of gliomas using traditional handcrafted image descriptors. However, deep learning methods such as convolutional neural networks (CNNs) show great promise but often struggle to provide interpretability and understanding of their decision making process. EGT combines explainability techniques with standard backpropagation in order to optimize the parameters of CNN models so that they can make predictions which are both accurate and interpretable. In our experiments we demonstrate how EGT significantly improves accuracy over baseline models while providing more meaningful explanations for decision boundaries than gradient-based saliency maps alone. We believe these findings offer significant potential for clinical translation towards improved patient outcomes.",1
"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve.   The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations.   Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",0
"Abstract:  The field of reinforcement learning has seen significant advancements in recent years due to the development of new algorithms and datasets that allow agents to learn complex tasks efficiently. However, most current RL environments provide little opportunity for human feedback during training. In contrast, humans often benefit from guidance as they develop their skills, whether through mentorship, teaching, or self-reflection. This human capacity to learn from feedback may inform the design of more effective agent learning systems. We present MineRL BASALT, a competition focusing on building such learning systems by leveraging human knowledge to improve the performance of learned agents. Participants developed agents using novel techniques ranging from imitating demonstrations to optimizing based on human preferences and feedback. Our analysis of the submissions reveals insights into how different approaches can effectively leverage human feedback for improved results. Overall, our work contributes to the growing understanding of how artificial intelligence can benefit from incorporating human knowledge.",1
"Adaptive gradient methods such as RMSProp and Adam use exponential moving estimate of the squared gradient to compute adaptive step sizes, achieving better convergence than SGD in face of noisy objectives. However, Adam can have undesirable convergence behaviors due to unstable or extreme adaptive learning rates. Methods such as AMSGrad and AdaBound have been proposed to stabilize the adaptive learning rates of Adam in the later stage of training, but they do not outperform Adam in some practical tasks such as training Transformers \cite{transformer}. In this paper, we propose an adaptive learning rate principle, in which the running mean of squared gradient in Adam is replaced by a weighted mean, with weights chosen to maximize the estimated variance of each coordinate. This results in a faster adaptation to the local gradient variance, which leads to more desirable empirical convergence behaviors than Adam. We prove the proposed algorithm converges under mild assumptions for nonconvex stochastic optimization problems, and demonstrate the improved efficacy of our adaptive averaging approach on machine translation, natural language understanding and large-batch pretraining of BERT. The code is available at https://github.com/zhuchen03/MaxVA.",0
"In the field of machine learning, it has become increasingly important to optimize step sizes during training as they can greatly impact model performance. However, finding the optimal step size remains challenging due to the high computational cost required for hyperparameter tuning. To address this issue, we propose a new algorithm called MaxVA that fast adapts step sizes based on the maximization of observed variance of gradients (MaxVA). By using the maximum observed variance of gradients at each epoch, our method provides a simple yet effective approach to rapidly adjusting step sizes without requiring extensive computations. Our experiments show that MaxVA significantly improves accuracy across several benchmark datasets compared to state-of-the-art algorithms while reducing computational overhead. This work represents a significant advancement in the development of efficient methods for optimizing step sizes and enhancing the robustness of deep neural networks.",1
"Deep learning provides a promising way to extract effective representations from raw data in an end-to-end fashion and has proven its effectiveness in various domains such as computer vision, natural language processing, etc. However, in domains such as content/product recommendation and risk management, where sequence of event data is the most used raw data form and experts derived features are more commonly used, deep learning models struggle to dominate the game. In this paper, we propose a symbolic testing framework that helps to answer the question of what kinds of expert-derived features could be learned by a neural network. Inspired by this testing framework, we introduce an efficient architecture named SHORING, which contains two components: \textit{event network} and \textit{sequence network}. The \textit{event} network learns arbitrarily yet efficiently high-order \textit{event-level} embeddings via a provable reparameterization trick, the \textit{sequence} network aggregates from sequence of \textit{event-level} embeddings. We argue that SHORING is capable of learning certain standard symbolic expressions which the standard multi-head self-attention network fails to learn, and conduct comprehensive experiments and ablation studies on four synthetic datasets and three real-world datasets. The results show that SHORING empirically outperforms the state-of-the-art methods.",0
"Title: Proving Correctness of Conditionally Dependent Programs Using Symbolic Execution and Testing  Higher-order programming allows programs to call other programs as functions, enabling complex abstractions that lead to concise and reusable code. However, higher-order interactions, where one function calls another that has access to global state, pose significant challenges for proving correctness due to conditional dependencies. Existing methods struggle with high-order interaction networks (HONs) because they lack formal underpinnings or rely on incomplete tests or synthetic inputs. We present Shoring, a systematic approach that designs provably correct HONs using symbolic execution and testing techniques based on program logic. Our method can infer and check both logical properties and performance constraints for all branches of conditionals within HONs. Through evaluation on benchmark examples from various domains, we demonstrate how Shoring automatically infers conditions that eliminate spurious errors, generates efficient proof terms, handles recursive calls without increasing blowup, and discovers new bugs in existing implementations. Overall, our work advances the state of the art in automating correctness proofs for higher-order interactions and opens up promising directions for future research in verification and compiler optimizations.",1
"Braille has empowered visually challenged community to read and write. But at the same time, it has created a gap due to widespread inability of non-Braille users to understand Braille scripts. This gap has fuelled researchers to propose Optical Braille Recognition techniques to convert Braille documents to natural language. The main motivation of this work is to cement the communication gap at academic institutions by translating personal documents of blind students. This has been accomplished by proposing an economical and effective technique which digitizes Braille documents using a smartphone camera. For any given Braille image, a dot detection mechanism based on Hough transform is proposed which is invariant to skewness, noise and other deterrents. The detected dots are then clustered into Braille cells using distance-based clustering algorithm. In succession, the standard physical parameters of each Braille cells are estimated for feature extraction and classification as natural language characters. The comprehensive evaluation of this technique on the proposed dataset of 54 Braille scripts has yielded into accuracy of 98.71%.",0
"This paper presents a method for recognizing braille text using computer vision techniques. The proposed approach utilizes circular Hough transform to detect circles from the image of the braille code. These circles correspond to the bumps on the surface of the braille symbol. By extracting these circles, we can determine the corresponding character. Experimental results show that our method achieves high accuracy compared to state-of-the-art methods. Additionally, our approach is robust to variations in lighting conditions and camera angles. We believe that optical recognition of braille has the potential to greatly improve accessibility for visually impaired individuals by providing fast and accurate identification of written material. Our work contributes towards realizing this goal by proposing an effective and efficient solution for recognizing braille characters through computer vision techniques.",1
"A generic video summary is an abridged version of a video that conveys the whole story and features the most important scenes. Yet the importance of scenes in a video is often subjective, and users should have the option of customizing the summary by using natural language to specify what is important to them. Further, existing models for fully automatic generic summarization have not exploited available language models, which can serve as an effective prior for saliency. This work introduces CLIP-It, a single framework for addressing both generic and query-focused video summarization, typically approached separately in the literature. We propose a language-guided multimodal transformer that learns to score frames in a video based on their importance relative to one another and their correlation with a user-defined query (for query-focused summarization) or an automatically generated dense video caption (for generic video summarization). Our model can be extended to the unsupervised setting by training without ground-truth supervision. We outperform baselines and prior work by a significant margin on both standard video summarization datasets (TVSum and SumMe) and a query-focused video summarization dataset (QFVS). Particularly, we achieve large improvements in the transfer setting, attesting to our method's strong generalization capabilities.",0
"In order to summarize video content, prior work has relied heavily on manual annotations which is laborious and time consuming. To overcome these limitations, we propose a novel method called CLIP-It!, that utilizes natural language guidance from human annotators along with pre-trained representations of image and text data. Our framework integrates state-of-the art techniques like transformers, attention mechanisms, reinforcement learning, and variational autoencoders. By incorporating linguistic knowledge into our model, we can improve the performance of the summary generation process by allowing models to better understand relationships between visual events and their corresponding natural language descriptions. Results show that our approach outperforms competitive baselines across several metrics.",1
"Transformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.",0
"This can make your summary more readable. Also use active voice instead of passive voice which would again give better readability. Try to have at least one sentence which summarizes the overall conclusion that reader may take away from the work you present in your abstract. Finally, keep in mind that any abbreviation used should be spelled out fully the first time they appear. --- SparseBERT is a method that rethinks the importance analysis in self-attention models by using a principled approach based on randomized masking and marginalization. Unlike other methods, such as Longformer, SparseBERT maintains high levels of performance while reducing memory usage during inference by up to 75%. Additionally, SparseBERT has been found to yield higher interpretability through attention heatmap visualizations compared to dense self-attention and other sparse alternatives. Overall, these findings suggest that SparseBERT could lead to improved efficiency in natural language processing tasks without sacrificing model quality.",1
"We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",0
"Abstract: This paper explores the possibility of using pretrained transformer models as universal computation engines, capable of performing a wide range of tasks beyond their original intended purpose of natural language processing. We demonstrate that these models can effectively learn and adapt to new task requirements without fine-tuning, outperforming traditional baseline methods in many cases. Our results suggest that pretrained transformers may have significant potential as general-purpose problem solvers, with applications across multiple domains. Finally we discuss some limitations and future directions for research on this topic.",1
"This paper addresses the problem of media retrieval using a multimodal query (a query which combines visual input with additional semantic information in natural language feedback). We propose a SynthTriplet GAN framework which resolves this task by expanding the multimodal query with a synthetically generated image that captures semantic information from both image and text input. We introduce a novel triplet mining method that uses a synthetic image as an anchor to directly optimize for embedding distances of generated and target images. We demonstrate that apart from the added value of retrieval illustration with synthetic image with the focus on customization and user feedback, the proposed method greatly surpasses other multimodal generation methods and achieves state of the art results in the multimodal retrieval task. We also show that in contrast to other retrieval methods, our method provides explainable embeddings.",0
"Title: Enhancing Search Engines through Multimodal Retrieval with Synthetic Query Expansion  Abstract: Modern search engines struggle to accurately capture user intent due to limitations in natural language processing (NLP) techniques and the inherent ambiguity of textual queries. To address these issues, we propose a novel approach that utilizes multimodal data sources, such as images and videos, along with NLP methods to generate synthesized queries. These synthesized queries expand upon the original query by incorporating additional contextual information from both visual content and linguistic analysis. Our system then employs these expanded queries to perform retrieval from both image and video databases. We evaluate our method using two distinct datasets consisting of textual queries and multimedia results, demonstrating significant improvements over traditional search engine approaches. Our findings highlight the potential benefits of incorporating multimodal strategies into modern information retrieval systems.",1
"Transformers provide promising accuracy and have become popular and used in various domains such as natural language processing and computer vision. However, due to their massive number of model parameters, memory and computation requirements, they are not suitable for resource-constrained low-power devices. Even with high-performance and specialized devices, the memory bandwidth can become a performance-limiting bottleneck. In this paper, we present a performance analysis of state-of-the-art vision transformers on several devices. We propose to reduce the overall memory footprint and memory transfers by clustering the model parameters. We show that by using only 64 clusters to represent model parameters, it is possible to reduce the data transfer from the main memory by more than 4x, achieve up to 22% speedup and 39% energy savings on mobile devices with less than 0.1% accuracy loss.",0
"In recent years, transformer models have emerged as one of the most successful architectures in natural language processing tasks, achieving state-of-the-art performance on a variety of challenging problems such as machine translation, question answering, and text classification. However, these models often require large amounts of computational resources to train and deploy, making them impractical for use on resource-constrained devices such as smartphones and embedded systems. This work proposes several techniques aimed at improving the efficiency of transformers for resource-constrained devices without sacrificing their accuracy. We first analyze the factors that contribute to slow training times and high memory usage in transformer models, including self attention mechanisms, parallelization, and optimizations for modern hardware. Based on our analysis, we propose a set of modifications to the transformer architecture that improve its efficiency while maintaining comparable performance. Our experimental results show that these modifications significantly reduce the time required to train and evaluate transformer models on real-world datasets while requiring less memory and computing power compared to previous approaches. Overall, this research provides valuable insights into the design and optimization of transformer models for resource-constrained devices, opening up new opportunities for natural language processing applications in areas such as edge computing and mobile computing.",1
"Transformer attention architectures, similar to those developed for natural language processing, have recently proved efficient also in vision, either in conjunction with or as a replacement for convolutional layers. Typically, visual attention is inserted in the network architecture as a (series of) feedforward self-attention module(s), with mutual key-query agreement as the main selection and routing operation. However efficient, this strategy is only vaguely compatible with the way that attention is implemented in biological brains: as a separate and unified network of attentional selection regions, receiving inputs from and exerting modulatory influence on the entire hierarchy of visual regions. Here, we report experiments with a simple such attention system that can improve the performance of standard convolutional networks, with relatively few additional parameters. Each spatial position in each layer of the network produces a key-query vector pair; all queries are then pooled into a global attention query. On the next iteration, the match between each key and the global attention query modulates the network's activations -- emphasizing or silencing the locations that agree or disagree (respectively) with the global attention system. We demonstrate the usefulness of this brain-inspired Global Attention Agreement network (GAttANet) for various convolutional backbones (from a simple 5-layer toy model to a standard ResNet50 architecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our global attention system improves accuracy over the corresponding baseline.",0
In this study we present a novel network architecture that enables global attention agreements. These can be used as part of larger models including transformers which take into account both local and global context. We show how our method improves performance on several benchmark tasks by allowing for the integration of more complex spatio temporal relationships. Our experiments verify the effectiveness of incorporating global attention mechanisms in convolutional neural networks.,1
"Deep neural network (DNN) is a popular model implemented in many systems to handle complex tasks such as image classification, object recognition, natural language processing etc. Consequently DNN structural vulnerabilities become part of the security vulnerabilities in those systems. In this paper we study the root cause of DNN adversarial examples. We examine the DNN response surface to understand its classification boundary. Our study reveals the structural problem of DNN classification boundary that leads to the adversarial examples. Existing attack algorithms can generate from a handful to a few hundred adversarial examples given one clean image. We show there are infinitely many adversarial images given one clean sample, all within a small neighborhood of the clean sample. We then define DNN uncertainty regions and show transferability of adversarial examples is not universal. We also argue that generalization error, the large sample theoretical guarantee established for DNN, cannot adequately capture the phenomenon of adversarial examples. We need new theory to measure DNN robustness.",0
"Abstract: This research explores adversarial examples in deep neural networks by analyzing the response surface and uncertainty regions. The goal is to gain insight into how these types of inputs can cause misclassifications and why they are challenging to detect. By studying the behavior of the network on both benign and adversarial examples, we hope to develop methods that can better defend against attacks and improve model robustness. Our work demonstrates how the response surface and uncertainty regions differ between benign and adversarial examples and provides new perspectives on understanding their impact on deep learning systems. We discuss our findings and propose potential solutions to mitigate adversarial vulnerabilities in neural networks. Overall, this study contributes to the growing field of adversarial machine learning and emphasizes the importance of addressing security concerns in deep learning models.",1
"Deep Learning (DL) is considered the state-of-the-art in computer vision, speech recognition and natural language processing. Until recently, it was also widely accepted that DL is irrelevant for learning tasks on tabular data, especially in the small sample regime where ensemble methods are acknowledged as the gold standard. We present a new end-to-end differentiable method to train a standard FFNN. Our method, \textbf{Muddling labels for Regularization} (\texttt{MLR}), penalizes memorization through the generation of uninformative labels and the application of a differentiable close-form regularization scheme on the last hidden layer during training. \texttt{MLR} outperforms classical NN and the gold standard (GBDT, RF) for regression and classification tasks on several datasets from the UCI database and Kaggle covering a large range of sample sizes and feature to sample ratios. Researchers and practitioners can use \texttt{MLR} on its own as an off-the-shelf \DL{} solution or integrate it into the most advanced ML pipelines.",0
"""Tabular datasets are commonly used in many fields such as finance, medicine, and ecology. Traditional machine learning methods have been applied on these tabular data but deep learning has proven to provide better performance than traditional techniques like Random Forest and Logistic Regression. However, there are challenges associated with applying deep learning models to these high dimensional tables. One major challenge arises from the limited number of samples available for training due to which overfitting occurs, leading to poor generalization of the model. In order to tackle this issue, we propose a method called Muddling Label Regularization (MLR). MLR employs two techniques: the first technique involves adding noise to the labels while regularizing the activation functions of each layer based on their ability to resist noise propagation throughout the network. This approach leads to less pruning of weights during backpropagation and allows the model to learn more robust representations that can generalize better under small sample sizes. We demonstrate our proposed framework by experimenting with several benchmark datasets and achieve state-of-the art results compared to other existing approaches.""",1
"Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world tabular datasets. We propose SCARF, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that SCARF complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.",0
"In recent years, self-supervised learning has emerged as a powerful approach for training deep neural networks without relying on annotated data. One popular method is contrastive learning, which involves maximizing agreement between positive pairs (e.g., two views of the same image) while minimizing agreement between negative pairs (two different images). However, existing methods still have limitations, such as vulnerability to trivial solutions like color jittering and insufficient robustness against random corruptions.  To address these issues, we propose a new method called SCARF (Self-Supervised Contrastive Representation Fine-Tuning), based on contrastive learning and feature augmentations that are specifically designed to create challenging but meaningful perturbations to input data. Our key contributions can be summarized as follows:  * We introduce random feature corruption techniques for generating difficult yet informative augmentations, ensuring that both the pretraining step and downstream task benefit from stronger generalization ability. Specifically, we investigate three types of corruptions (additive noise, blur, and aspect ratio changes) and develop principles guiding their proper use during both pretraining and fine-tuning stages. * Experimentally, our proposed SCARF method surpasses state-of-the-art results across multiple benchmark datasets and architectures (including ImageNet, ClutteredCOCO, and Vistas) for tasks involving image classification and object detection. Furthermore, ablation studies demonstrate the effectiveness and complementarity of each component within our framework. * Finally, by utilizing real-world applications (semantic segmentation on PASCAL Context, panoptic segmentation on Cityscapes, and COCO object detection) along with synthetic datasets commonly used in academic evaluations, we offer comprehensive evidence that SCARF indeed learns superior representations enabling strong performance on diverse evaluation protocols beyond the simple image classification setting.  Overall, our work pushes forward the frontier of self-supervi",1
"This work presents CLIPDraw, an algorithm that synthesizes novel drawings based on natural language input. CLIPDraw does not require any training; rather a pre-trained CLIP language-image encoder is used as a metric for maximizing similarity between the given description and a generated drawing. Crucially, CLIPDraw operates over vector strokes rather than pixel images, a constraint that biases drawings towards simpler human-recognizable shapes. Results compare between CLIPDraw and other synthesis-through-optimization methods, as well as highlight various interesting behaviors of CLIPDraw, such as satisfying ambiguous text in multiple ways, reliably producing drawings in diverse artistic styles, and scaling from simple to complex visual representations as stroke count is increased. Code for experimenting with the method is available at: https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb",0
"This paper presents a new method for text-to-drawing synthesis using language-image encoders. We introduce CLIPDraw, which combines Contrastive Language-Image Pre-training (CLIP) with a generative model that can produce realistic drawings from natural language descriptions. Our approach uses contrastive learning to encode image features into a shared space with text embeddings, enabling efficient zero-shot inference on novel concepts. Experiments demonstrate that our model outperforms prior methods on challenging benchmarks for sketching and chart generation, establishing state-of-the-art performance. Additionally, we showcase the versatility of our system by generating images of diverse domains such as scientific illustration and visual storytelling. By bridging language and vision models, we pave the way towards more generalizable and deployable artificial intelligence systems.",1
"In large technology companies, the requirements for managing and organizing technical documents created by engineers and managers in supporting relevant decision making have increased dramatically in recent years, which has led to a higher demand for more scalable, accurate, and automated document classification. Prior studies have primarily focused on processing text for classification and small-scale databases. This paper describes a novel multimodal deep learning architecture, called TechDoc, for technical document classification, which utilizes both natural language and descriptive images to train hierarchical classifiers. The architecture synthesizes convolutional neural networks and recurrent neural networks through an integrated training process. We applied the architecture to a large multimodal technical document database and trained the model for classifying documents based on the hierarchical International Patent Classification system. Our results show that the trained neural network presents a greater classification accuracy than those using a single modality and several earlier text classification methods. The trained model can potentially be scaled to millions of real-world technical documents with both text and figures, which is useful for data and knowledge management in large technology companies and organizations.",0
"Technical document classification has been an area of interest for researchers due to its numerous applications such as automating document management systems, customer support through chatbots, and web search among others. With the rise of deep learning techniques, there have been several attempts at solving technical document classification problem using neural networks with promising results. In this paper we survey existing work on this topic along with our contributions towards improving state-of-the art performance in two ways: 1) by proposing better preprocessing methodologies for text data which enhances feature extraction capabilities leading to higher accuracy; 2) designing more advanced models that can effectively capture complex features hidden in large datasets without overfitting leading to improved performance measures such as F1 score, Precision and Recall . Our experiments show consistent improvement across all metrics compared to benchmarks on real world datasets. This work provides new insights into understanding how deep learning techniques can solve difficult NLP problems like document classification and opens up directions for future improvements.",1
"In this paper, we propose an architecture to solve a novel problem statement that has stemmed more so in recent times with an increase in demand for virtual content delivery due to the COVID-19 pandemic. All educational institutions, workplaces, research centers, etc. are trying to bridge the gap of communication during these socially distanced times with the use of online content delivery. The trend now is to create presentations, and then subsequently deliver the same using various virtual meeting platforms. The time being spent in such creation of presentations and delivering is what we try to reduce and eliminate through this paper which aims to use Machine Learning (ML) algorithms and Natural Language Processing (NLP) modules to automate the process of creating a slides-based presentation from a document, and then use state-of-the-art voice cloning models to deliver the content in the desired author's voice. We consider a structured document such as a research paper to be the content that has to be presented. The research paper is first summarized using BERT summarization techniques and condensed into bullet points that go into the slides. Tacotron inspired architecture with Encoder, Synthesizer, and a Generative Adversarial Network (GAN) based vocoder, is used to convey the contents of the slides in the author's voice (or any customized voice). Almost all learning has now been shifted to online mode, and professionals are now working from the comfort of their homes. Due to the current situation, teachers and professionals have shifted to presentations to help them in imparting information. In this paper, we aim to reduce the considerable amount of time that is taken in creating a presentation by automating this process and subsequently delivering this presentation in a customized voice, using a content delivery mechanism that can clone any voice using a short audio clip.",0
"This paper presents an AI based presentation creator that utilizes customizable audio content delivery. Our system uses natural language processing techniques to automatically generate spoken audio recordings from text input, enabling users to create engaging presentations without the need for manual recording. We evaluate our approach using user studies and demonstrate improvements over baseline systems both quantitatively and qualitatively. Furthermore, we provide insight into future directions and applications of our technology, such as personalizing educational material for different learning styles and delivering dynamic presentations in virtual reality environments. Overall, our work shows great potential for advancing the state-of-the-art in multimedia presentation creation and delivery.",1
"Transformer, which can benefit from global (long-range) information modeling using self-attention mechanisms, has been successful in natural language processing and 2D image classification recently. However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper, we for the first time exploit Transformer in 3D CNN for MRI Brain Tumor Segmentation and propose a novel network named TransBTS based on the encoder-decoder structure. To capture the local 3D context information, the encoder first utilizes 3D CNN to extract the volumetric spatial feature maps. Meanwhile, the feature maps are reformed elaborately for tokens that are fed into Transformer for global feature modeling. The decoder leverages the features embedded by Transformer and performs progressive upsampling to predict the detailed segmentation map. Extensive experimental results on both BraTS 2019 and 2020 datasets show that TransBTS achieves comparable or higher results than previous state-of-the-art 3D methods for brain tumor segmentation on 3D MRI scans. The source code is available at https://github.com/Wenxuan-1119/TransBTS",0
"In recent years, deep learning has proven to be highly effective in image segmentation tasks due to its ability to process large amounts of data and learn complex representations. However, few methods have been proposed that can effectively integrate multiple types of modalities such as MRI and CT scans which often provide complementary information for brain tumour segmentation. In this work we propose TransBTS, a novel multimodal network architecture based on the attention mechanism in transformers, which allows for efficient integration of multi-modal images while capturing both local and global contextual information necessary for accurate segmentation. We evaluate our method using two publicly available datasets and show that TransBTS outperforms state-of-the-art algorithms by achieving higher Dice scores and lower HD95 distances. Our results demonstrate the effectiveness of utilizing transformer networks for brain tumour segmentation, especially when dealing with multimodal medical imaging data. The code for TransBTS has been made publicly available at <https://github.com/SIIM2023/TransBTS>.",1
"Current vision and language tasks usually take complete visual data (e.g., raw images or videos) as input, however, practical scenarios may often consist the situations where part of the visual information becomes inaccessible due to various reasons e.g., restricted view with fixed camera or intentional vision block for security concerns. As a step towards the more practical application scenarios, we introduce a novel task that aims to describe a video using the natural language dialog between two agents as a supplementary information source given incomplete visual data. Different from most existing vision-language tasks where AI systems have full access to images or video clips, which may reveal sensitive information such as recognizable human faces or voices, we intentionally limit the visual input for AI systems and seek a more secure and transparent information medium, i.e., the natural language dialog, to supplement the missing visual information. Specifically, one of the intelligent agents - Q-BOT - is given two semantic segmented frames from the beginning and the end of the video, as well as a finite number of opportunities to ask relevant natural language questions before describing the unseen video. A-BOT, the other agent who has access to the entire video, assists Q-BOT to accomplish the goal by answering the asked questions. We introduce two different experimental settings with either a generative (i.e., agents generate questions and answers freely) or a discriminative (i.e., agents select the questions and answers from candidates) internal dialog generation process. With the proposed unified QA-Cooperative networks, we experimentally demonstrate the knowledge transfer process between the two dialog agents and the effectiveness of using the natural language dialog as a supplement for incomplete implicit visions.",0
"This paper introduces the concept of using natural language processing (NLP) techniques and machine learning algorithms to create artificial intelligence that can generate video descriptions for blind individuals. We propose a novel approach called ""dialog agents"" which involves training these systems on large datasets of human generated descriptive text. Our experiments show that our method produces highly accurate descriptions that accurately convey key details such as objects, actions, emotions, and context. In addition, we demonstrate how the use of NLP enables more complex reasoning tasks such as identifying and describing multiple events happening simultaneously in a scene, interpreting and explaining metaphors, similes and figurative expressions commonly found in descriptive narratives. Overall, our work represents a major step forward towards creating intelligent assistants capable of generating high quality descriptions of videos and enhancing accessibility for millions of visually impaired users worldwide.",1
"When tuning the architecture and hyperparameters of large machine learning models for on-device deployment, it is desirable to understand the optimal trade-offs between on-device latency and model accuracy. In this work, we leverage recent methodological advances in Bayesian optimization over high-dimensional search spaces and multi-objective Bayesian optimization to efficiently explore these trade-offs for a production-scale on-device natural language understanding model at Facebook.",0
"As artificial intelligence (AI) systems become increasingly prevalent in our daily lives, there is growing interest in developing techniques that can optimize their performance across multiple dimensions. One important aspect of AI system performance is latency, or how quickly they respond to inputs and produce outputs. In this paper, we propose a novel approach called ""Latency-aware neural architecture search with multi-objective Bayesian optimization"" which addresses this challenge by leveraging recent advances in probabilistic programming languages like PyMC3 and Probabilistic Programming Language(PPL). Our method uses these tools to model the relationship between hyperparameters controlling different components of the AI system, such as number of neurons per layer, depth of network stacks and width of networks, and predicted performance metrics including accuracy and latency. This allows us to perform efficient global searches over this high-dimensional parameter space using Bayesian optimization methods that balance the competing objectives of minimizing latency while maximizing performance. We demonstrate the effectiveness of our method through experiments on several benchmark datasets and show that it consistently finds models with better tradeoffs between latency and accuracy compared to prior state-of-the art approaches. Overall, this work represents an important step towards enabling AI developers to build more responsive and efficient systems that meet the diverse needs of users in real world settings.",1
"This work improves the quality of automated machine learning (AutoML) systems by using dataset and function descriptions while significantly decreasing computation time from minutes to milliseconds by using a zero-shot approach. Given a new dataset and a well-defined machine learning task, humans begin by reading a description of the dataset and documentation for the algorithms to be used. This work is the first to use these textual descriptions, which we call privileged information, for AutoML. We use a pre-trained Transformer model to process the privileged text and demonstrate that using this information improves AutoML performance. Thus, our approach leverages the progress of unsupervised representation learning in natural language processing to provide a significant boost to AutoML. We demonstrate that using only textual descriptions of the data and functions achieves reasonable classification performance, and adding textual descriptions to data meta-features improves classification across tabular datasets. To achieve zero-shot AutoML we train a graph neural network with these description embeddings and the data meta-features. Each node represents a training dataset, which we use to predict the best machine learning pipeline for a new test dataset in a zero-shot fashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a supervised learning task and dataset. In contrast, most AutoML systems require tens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces running and prediction times from minutes to milliseconds, consistently across datasets. By speeding up AutoML by orders of magnitude this work demonstrates real-time AutoML.",0
"Machine Learning (ML) automation tools make the model training process more efficient by taking care of several tasks such as data preprocessing, model selection and hyperparameter tuning. Although there have been significant advances in Automatic ML (AutoML), most current approaches still require considerable human intervention in order to set up the infrastructure, design the pipelines and interpret their results. In our work, we propose a new methodology called ""Privileged Zero-shot AutoML"" that combines unsupervised machine learning techniques with existing AutoML frameworks to achieve zero- shot automatic deployment on any dataset without prior knowledge or fine-tuning. Our approach leverages transfer learning from natural language processing to automatically infer informative feature representations which further allows it to perform well across multiple datasets. We evaluate our approach on a range of classification tasks and demonstrate state-of-the-art performance compared to other widely used AutoML methods while requiring significantly less computational resources. With the growth of big data, it has become increasingly important to develop scalable solutions for handling large scale ML tasks. Our work takes us one step closer towards fully autonomous ML systems, where even end users can generate models without extensive domain expertise.",1
"Recently, transformers have shown great superiority in solving computer vision tasks by modeling images as a sequence of manually-split patches with self-attention mechanism. However, current architectures of vision transformers (ViTs) are simply inherited from natural language processing (NLP) tasks and have not been sufficiently investigated and optimized. In this paper, we make a further step by examining the intrinsic structure of transformers for vision tasks and propose an architecture search method, dubbed ViTAS, to search for the optimal architecture with similar hardware budgets. Concretely, we design a new effective yet efficient weight sharing paradigm for ViTs, such that architectures with different token embedding, sequence size, number of heads, width, and depth can be derived from a single super-transformer. Moreover, to cater for the variance of distinct architectures, we introduce \textit{private} class token and self-attention maps in the super-transformer. In addition, to adapt the searching for different budgets, we propose to search the sampling probability of identity operation. Experimental results show that our ViTAS attains excellent results compared to existing pure transformer architectures. For example, with $1.3$G FLOPs budget, our searched architecture achieves $74.7\%$ top-$1$ accuracy on ImageNet and is $2.5\%$ superior than the current baseline ViT architecture. Code is available at \url{https://github.com/xiusu/ViTAS}.",0
"In recent years, there has been significant progress made by computer vision models like ResNet (He et al., 2016) , DenseNet (Huang et al., 2017), U-Net (Ronneberger et al ., 2015 ), SegNet (Badrinarayanan et al., 2017 ) etc which can handle large scale datasets such as Image Net (Deng et al., 2009) but they still suffer from slow inference speed due to their complex architectures. To address these issues Vision Transformers have been proposed which show promise in terms of both accuracy and computational efficiency (ViT(Vaswani et al., 2020)). Various works exploring Vision Transformer architecture search (Chen et al. , 2021; Wong et al. ,2021 ; Zhuge et al. , 2021) demonstrate that it's possible to find new ViT variants superior in performance than those hand designed ones. They use Neural Network architecture search methods like Random Search(RS)( Bauer et al . , 2019 ) , Evolutionary Strategies (ES)(Real et al . , 2018 ; Li et al. , 2021a) etc in order to conduct extensive studies on architecture space of Vision Transformers however it remains unclear what insights we gain if these approaches continue to explore larger search spaces. Additionally despite recent advances the process of searching for novel network topologies remain computationally expensive. This paper contributes a more effective search methodology which addresses these challenges with respect to efficient exploration in larger search spaces. We focus primarily on Evolutionary Methods due to their success seen in previous research (Yeung at al. , 2021). Since the model population size scales quadratically with the number of parameters , direct scaling up of existing methods is often prohibitive which leads us to propose ""Progressive Population Size Reduction"" (PPSR) - a pruning based technique used to reduce the population size throughout evolution while preserving the necessary representational capacity to maintain good performance . Secondly to accelerate the search process in large search spac",1
"Breakthroughs in machine learning in the last decade have led to `digital intelligence', i.e. machine learning models capable of learning from vast amounts of labeled data to perform several digital tasks such as speech recognition, face recognition, machine translation and so on. The goal of this thesis is to make progress towards designing algorithms capable of `physical intelligence', i.e. building intelligent autonomous navigation agents capable of learning to perform complex navigation tasks in the physical world involving visual perception, natural language understanding, reasoning, planning, and sequential decision making. Despite several advances in classical navigation methods in the last few decades, current navigation agents struggle at long-term semantic navigation tasks. In the first part of the thesis, we discuss our work on short-term navigation using end-to-end reinforcement learning to tackle challenges such as obstacle avoidance, semantic perception, language grounding, and reasoning. In the second part, we present a new class of navigation methods based on modular learning and structured explicit map representations, which leverage the strengths of both classical and end-to-end learning methods, to tackle long-term navigation tasks. We show that these methods are able to effectively tackle challenges such as localization, mapping, long-term planning, exploration and learning semantic priors. These modular learning methods are capable of long-term spatial and semantic understanding and achieve state-of-the-art results on various navigation tasks.",0
"Abstract: This paper presents a new approach to building intelligent autonomous navigation agents. Current methods rely heavily on machine learning techniques such as deep reinforcement learning and computer vision algorithms which often require large amounts of data to train accurate models. These approaches can be computationally expensive and may not generalize well across different environments. Our proposed method uses a symbolic reasoning system that allows the agent to plan and reason about its environment using high level representations. This results in a more flexible and interpretable solution compared to traditional black box neural network based systems. We evaluate our approach on several benchmark scenarios and show that our system outperforms existing state-of-the-art methods in terms of accuracy, efficiency, and interpretability. ---  Introduction: Autonomous navigation has become an increasingly important problem over recent years due to advances in robotics, drones, and self driving cars. These applications require highly precise and efficient motion planning solutions to navigate through complex real world environments safely and efficiently. Recently there have been significant advances in artificial intelligence (AI) especially within deep learning and machine learning subfields which provide promising solutions. However these approaches suffer from some limitations in term of explainability and lack of generalization capabilities across different domains, tasks, and environments. In this work we introduce an alternative AI based approach for solving intelligent navigation problems. Instead of relying solely on classical machine learning paradigms like supervised or unsupervised learning; we present a hybrid symbolic reasoning and planning framework inspired by philosophy of mind literature known as cognitive architectures. Cognitive architecture provides principled ways for understanding how human minds process information, solve problems, and make decisions. By modeling after human cognition principles we aim to create adaptive agents capable of tackling novel situations without explicitly requiring vast amount of training data or fine tuned hyperparameters. Additionally we aspire to design agents whose decision making processes are transparent to engineers enabling better control and explanation. To validate this approach w",1
"Twitter is a useful resource to analyze peoples' opinions on various topics. Often these topics are correlated or associated with locations from where these Tweet posts are made. For example, restaurant owners may need to know where their target customers eat with respect to the sentiment of the posts made related to food, policy planners may need to analyze citizens' opinion on relevant issues such as crime, safety, congestion, etc. with respect to specific parts of the city, or county or state. As promising as this is, less than $1\%$ of the crawled Tweet posts come with geolocation tags. That makes accurate prediction of Tweet posts for the non geo-tagged tweets very critical to analyze data in various domains. In this research, we utilized millions of Twitter posts and end-users domain expertise to build a set of deep neural network models using natural language processing (NLP) techniques, that predicts the geolocation of non geo-tagged Tweet posts at various level of granularities such as neighborhood, zipcode, and longitude with latitudes. With multiple neural architecture experiments, and a collaborative human-machine workflow design, our ongoing work on geolocation detection shows promising results that empower end-users to correlate relationship between variables of choice with the location information.",0
"Title: Enhancing Twitter Geolocation Accuracy through Collaborative Approaches  Twitter has become an essential platform for sharing real-time updates on events, news, opinions, and experiences worldwide. With billions of tweets posted daily, extracting location data from user posts can provide valuable insights into emerging trends, public sentiments, and social behavior patterns across different regions. Despite advances in natural language processing (NLP) techniques that support geolocation prediction, their accuracy remains limited by varying factors such as user ambiguity, contextual discrepancies, and dynamic human behaviors. This study presents a novel approach towards enhancing fine-grained geolocation prediction accuracy using human-machine collaboration. Our methodology integrates machine learning algorithms with crowdsourced feedback from human annotators who validate and refine predicted locations based on their local knowledge, cultural understanding, and domain expertise. We propose a hybrid model that balances automation efficiency with human intuition, addressing both recall and precision issues in conventional NLP systems. Results obtained demonstrate significant improvements over baseline methods and other state-of-the-art solutions while highlighting the advantages of our proposed collaborative framework. This work provides new perspectives and potential applications in areas such as social media analytics, crisis response, urban planning, market research, and many more. By merging the complementary strengths of machines and humans, we pave the way for enhanced geolocation analysis and improved decision making in diverse fields.",1
"Causality knowledge is vital to building robust AI systems. Deep learning models often perform poorly on tasks that require causal reasoning, which is often derived using some form of commonsense knowledge not immediately available in the input but implicitly inferred by humans. Prior work has unraveled spurious observational biases that models fall prey to in the absence of causality. While language representation models preserve contextual knowledge within learned embeddings, they do not factor in causal relationships during training. By blending causal relationships with the input features to an existing model that performs visual cognition tasks (such as scene understanding, video captioning, video question-answering, etc.), better performance can be achieved owing to the insight causal relationships bring about. Recently, several models have been proposed that have tackled the task of mining causal data from either the visual or textual modality. However, there does not exist widespread research that mines causal relationships by juxtaposing the visual and language modalities. While images offer a rich and easy-to-process resource for us to mine causality knowledge from, videos are denser and consist of naturally time-ordered events. Also, textual information offers details that could be implicit in videos. We propose iReason, a framework that infers visual-semantic commonsense knowledge using both videos and natural language captions. Furthermore, iReason's architecture integrates a causal rationalization module to aid the process of interpretability, error analysis and bias detection. We demonstrate the effectiveness of iReason using a two-pronged comparative analysis with language representation learning models (BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",0
"In today's world, artificial intelligence (AI) systems have made significant strides in tasks such as image recognition and natural language processing. However, there remains a critical gap in their ability to reason and make decisions based on commonsense knowledge. This paper presents ""iReason,"" a novel approach that utilizes videos and natural language data along with interpretability techniques to enable multimodal commonsense reasoning. Our framework uses a combination of generative models, deep learning architectures, and attention mechanisms to encode visual features from video frames and align them with natural language descriptions. We then leverage these aligned representations to perform various types of reasoning tasks such as question answering, inference making, and decision support. Finally, we provide analysis and insights into our model architecture and performance through interpretability methods like feature attribution and layerwise relevance propagation. Our experiments demonstrate the effectiveness of our approach in achieving state-of-the-art results across multiple benchmark datasets while providing explainability and transparency. With iReason, we aim to bridge the gap between computer vision and natural language processing, enabling AI systems to comprehend complex scenarios and interact more naturally with humans.",1
"Initially developed for natural language processing (NLP), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model.",0
"This should summarize the main points of your paper. Your reader can always access more details from your paper if necessary. Make sure you cover: goal, methodology, results, conclusions drawn, implications (if any) The goal of the paper was to study transformer models used in natural language processing tasks such as machine translation, question answering, named entity recognition, and sentiment analysis. We conducted experiments on several datasets using different variants of transformer models such as BERT, GPT-2, and RoBERTa to compare their performance across different NLP tasks. Our findings indicate that transformer models achieve state-of-the-art accuracy on most benchmarks tested. Moreover, we observed that RoBERTa yields consistently better results compared to other models for code generation tasks. Finally, our work highlights the potential applicability of pre-trained transformer models for improving software development processes through automation, which has significant implications for reducing time and costs associated with manual code reviews.",1
"Transformers recently are adapted from the community of natural language processing as a promising substitute of convolution-based neural networks for visual learning tasks. However, its supremacy degenerates given an insufficient amount of training data (e.g., ImageNet). To make it into practical utility, we propose a novel distillation-based method to train vision transformers. Unlike previous works, where merely heavy convolution-based teachers are provided, we introduce lightweight teachers with different architectural inductive biases (e.g., convolution and involution) to co-advise the student transformer. The key is that teachers with different inductive biases attain different knowledge despite that they are trained on the same dataset, and such different knowledge compounds and boosts the student's performance during distillation. Equipped with this cross inductive bias distillation method, our vision transformers (termed as CivT) outperform all previous transformers of the same architecture on ImageNet.",0
"In a world where machine learning models have become increasingly complex, there has been growing interest in developing new techniques that can distil knowledge from large scale language data so as to improve model performance across different languages and tasks without overfitting on any one specific dataset. Our work introduces co-advising—a framework that exploits the interaction between multiple student models to create a teacher model capable of effectively guiding their training. We demonstrate how to explicitly encourage cross inductive bias distillation among students during co-training by leveraging their complementary strengths and limitations while mitigating potential negative effects through selectively applied weight penalties. Experiments show significant improvements compared to traditional single-student distillation settings, including stronger zero-shot generalization capabilities across unseen languages. We hope our findings inspire future research into more advanced methods of multi-task learning beyond standard setups involving only pairs of teachers and students, since such systems may prove crucial for advancing state-of-the-art NLP models towards real-world applicability on diverse linguistic domains.",1
"Automatic code synthesis from natural language descriptions is a challenging task. We witness massive progress in developing code generation systems for domain-specific languages (DSLs) employing sequence-to-sequence deep learning techniques in the recent past. In this paper, we specifically experiment with \textsc{AlgoLisp} DSL-based generative models and showcase the existence of significant dataset bias through different classes of adversarial examples. We also experiment with two variants of Transformer-based models that outperform all existing \textsc{AlgoLisp} DSL-based code generation baselines. Consistent with the current state-of-the-art systems, our proposed models, too, achieve poor performance under adversarial settings. Therefore, we propose several dataset augmentation techniques to reduce bias and showcase their efficacy using robust experimentation.",0
"Automatic code generation has become increasingly popular as more companies seek out ways to reduce manual coding efforts while maintaining high quality outputs. However, one major concern associated with using automatic code generators is their susceptibility to adversarial attacks. In recent years, researchers have demonstrated that malicious inputs can easily fool many state-of-the-art code generation models into producing harmful code. This work seeks to address this issue by proposing novel techniques for improving the robustness of synthetic code generation models against such attacks. We begin by presenting a comprehensive analysis of existing approaches used to augment code generators' resilience to attacks, highlighting their limitations and drawbacks. Next, we introduce our new methods based on deep learning architectures that effectively detect and mitigate input perturbations, ensuring safe and reliable synthesized code. Our extensive evaluation results demonstrate the superior performance of our approach compared to other well-known systems. Overall, this work represents a significant step forward towards making automated code generation technology safer and more secure for real-world applications.",1
"Hierarchical Agglomerative Clustering (HAC) algorithms are extensively utilized in modern data science, and seek to partition the dataset into clusters while generating a hierarchical relationship between the data samples. HAC algorithms are employed in many applications, such as biology, natural language processing, and recommender systems. Thus, it is imperative to ensure that these algorithms are fair -- even if the dataset contains biases against certain protected groups, the cluster outputs generated should not discriminate against samples from any of these groups. However, recent work in clustering fairness has mostly focused on center-based clustering algorithms, such as k-median and k-means clustering. In this paper, we propose fair algorithms for performing HAC that enforce fairness constraints 1) irrespective of the distance linkage criteria used, 2) generalize to any natural measures of clustering fairness for HAC, 3) work for multiple protected groups, and 4) have competitive running times to vanilla HAC. Through extensive experiments on multiple real-world UCI datasets, we show that our proposed algorithm finds fairer clusterings compared to vanilla HAC as well as other state-of-the-art fair clustering approaches.",0
"One of the fundamental challenges facing machine learning is creating algorithms that can learn from data while remaining fair and unbiased. This task becomes even more difficult when dealing with hierarchical agglomerative clustering, where the goal is to group similar items together based on their similarity. Traditional methods often struggle with accurately identifying clusters that are both statistically significant and inclusive of underrepresented groups. In our work, we present a novel approach to hierarchical agglomerative clustering using fairness constraints. We demonstrate how these constraints lead to improved cluster validity and reduced bias by evaluating them against several benchmark datasets. Our findings show that our method outperforms existing techniques in terms of accuracy, diversity, and equitability. Ultimately, our work provides a new framework for constructing fairer and more representative models for clustering analysis.",1
"Pruning is an effective method to reduce the memory footprint and FLOPs associated with neural network models. However, existing structured-pruning methods often result in significant accuracy degradation for moderate pruning levels. To address this problem, we introduce a new Hessian Aware Pruning (HAP) method coupled with a Neural Implant approach that uses second-order sensitivity as a metric for structured pruning. The basic idea is to prune insensitive components and to use a Neural Implant for moderately sensitive components, instead of completely pruning them. For the latter approach, the moderately sensitive components are replaced with with a low rank implant that is smaller and less computationally expensive than the original component. We use the relative Hessian trace to measure sensitivity, as opposed to the magnitude based sensitivity metric commonly used in the literature. We test HAP for both computer vision tasks and natural language tasks, and we achieve new state-of-the-art results. Specifically, HAP achieves less than $0.1\%$/$0.5\%$ degradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than 70\%/50\% of parameters pruned. Meanwhile, HAP also achieves significantly better performance (up to 0.8\% with 60\% of parameters pruned) as compared to gradient based method for head pruning on transformer-based models. The framework has been open sourced and available online.",0
"In recent years, there has been significant interest in developing efficient pruning techniques that can remove redundant or unimportant neurons from deep neural networks (DNNs) while minimizing loss in accuracy. One promising approach to achieve this goal involves identifying critical connections or neurons using Hessians, which provide a measure of how sensitive the network output is to changes in input patterns. However, current state-of-the-art methods primarily focus on using fixed threshold values or heuristics to select important connections based on their corresponding Hessian values. These approaches often result in suboptimal performance as they do not take into account the relationship between different neurons in a DNN nor optimize pruning directly. This work proposes a novel algorithm that performs optimal neural implant by jointly optimizing connection selection and retraining the remaining network after pruning, using dynamic programming to search over all possible combinations of retained neurons. Experimental results demonstrate that our proposed method significantly outperforms existing methods in terms of accuracy recovery and model size reduction across a range of benchmark datasets and architectures. Our findings showcase the effectiveness of Hessian-aware optimization in creating more compact and accurate DNN models with minimal human intervention.",1
"As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.",0
"Recent advances in deep learning have led to a significant increase in model complexity, which has resulted in longer inference times and higher computational requirements. To address these issues, researchers have proposed quantization methods that enable efficient neural network inference on devices with limited hardware resources. This survey provides a comprehensive overview of existing quantization techniques used for accelerating neural network inferencing while minimizing loss in accuracy. We focus on several aspects such as bitwidth reduction techniques, tensor factorizations, pruning approaches, knowledge distillation, mixed precision, and depth scaling. Moreover, we analyze the trade-offs involved in employing different quantization strategies according to various performance metrics such as latency, energy consumption, memory footprint, and quality of service. Our evaluation shows that appropriate selection of quantization schemes can achieve up to two orders of magnitude speedup without significantly sacrificing model fidelity for a wide range of applications like image classification, object detection, speech recognition, and natural language processing. Finally, we discuss future directions in quantization research by highlighting promising areas including dynamic quantization, meta learning, and continual quantization adaptation. By providing detailed insights into current state-of-the art quantization techniques, our work serves as a valuable resource for both practitioners and researchers working in the field of machine learning deployment on edge computing devices.",1
"Transfer learning with pre-training on large-scale datasets has played an increasingly significant role in computer vision and natural language processing recently. However, as there exist numerous application scenarios that have distinctive demands such as certain latency constraints and specialized data distributions, it is prohibitively expensive to take advantage of large-scale pre-training for per-task requirements. In this paper, we focus on the area of object detection and present a transfer learning system named GAIA, which could automatically and efficiently give birth to customized solutions according to heterogeneous downstream needs. GAIA is capable of providing powerful pre-trained weights, selecting models that conform to downstream demands such as latency constraints and specified data domains, and collecting relevant data for practitioners who have very few datapoints for their tasks. With GAIA, we achieve promising results on COCO, Objects365, Open Images, Caltech, CityPersons, and UODB which is a collection of datasets including KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as an example, GAIA is able to efficiently produce models covering a wide range of latency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and bells. To benefit every practitioner in the community of object detection, GAIA is released at https://github.com/GAIA-vision.",0
"Increasingly, artificial intelligence (AI) systems aimed at object detection are becoming more specialized and tailored toward specific applications, rather than attempting to perform well across all use cases. However, developing these highly customized models can be time-consuming and computationally expensive, making them less accessible to small teams and individuals without extensive resources. To address this challenge, we propose GAIA, a transfer learning system designed to rapidly generate high-quality object detectors that meet user-specified requirements while minimizing computational costs. By leveraging existing knowledge from pre-trained deep neural networks and allowing for fine-grained control over model architecture, training data, and hyperparameters, GAIA enables researchers and developers to create accurate object detectors suited to their unique needs. Our comprehensive evaluation demonstrates the effectiveness and flexibility of our approach across various domains, making GAIA an attractive option for anyone seeking efficient, personalized object detection solutions. Ultimately, by bridging the gap between off-the-shelf methods and fully-custom approaches, GAIA empowers users to tackle complex tasks without sacrificing performance or affordability.",1
"Unsupervised domain adaptation is used in many machine learning applications where, during training, a model has access to unlabeled data in the target domain, and a related labeled dataset. In this paper, we introduce a novel and general domain-adversarial framework. Specifically, we derive a novel generalization bound for domain adaptation that exploits a new measure of discrepancy between distributions based on a variational characterization of f-divergences. It recovers the theoretical results from Ben-David et al. (2010a) as a special case and supports divergences used in practice. Based on this bound, we derive a new algorithmic framework that introduces a key correction in the original adversarial training method of Ganin et al. (2016). We show that many regularizers and ad-hoc objectives introduced over the last years in this framework are then not required to achieve performance comparable to (if not better than) state-of-the-art domain-adversarial methods. Experimental analysis conducted on real-world natural language and computer vision datasets show that our framework outperforms existing baselines, and obtains the best results for f-divergences that were not considered previously in domain-adversarial learning.",0
"This paper presents a comprehensive study on f-domain adversarial learning (fDAL), addressing both theoretical foundations and algorithmic developments. We begin by introducing the concept of fDAL as a novel framework that incorporates domain knowledge into adversarial training, enabling models to learn more robust representations under distribution shifts. Our analysis demonstrates how fDAL bridges the gap between distribution shift detection and mitigation, leading to state-of-the-art performance across benchmark datasets and real-world applications. The main contributions of our work can be summarized as follows: (i) we present a detailed theoretical treatment of fDAL, clarifying its relationship with existing methods from the literature; (ii) we devise efficient algorithms for solving the key optimization problems underlying fDAL; (iii) we conduct extensive experimental evaluations comparing fDAL against several baseline approaches using challenging benchmarks, reporting consistent improvements over competitors. Collectively, these results establish fDAL as an effective methodology for building resilient machine learning systems able to operate reliably in dynamic environments. With this work, we aim to encourage further research efforts towards uncovering new insights into domain adaptation, shedding light onto longstanding open questions at the heart of Artificial Intelligence.",1
"Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model configurations complicate the easy reuse of important innovations by a wide audience. Though there have been on-going efforts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces layoutparser, an open-source library for streamlining the usage of DL in DIA research and applications. The core layoutparser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout detection, character recognition, and many other document processing tasks. To promote extensibility, layoutparser also incorporates a community platform for sharing both pre-trained models and full document digitization pipelines. We demonstrate that layoutparser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io/.",0
"Abstract: This paper presents LayoutParser, a comprehensive toolkit for document image analysis using deep learning techniques that can effectively recognize and extract layout elements from complex documents such as books, magazines, and newspapers. The proposed solution addresses key challenges in this domain by leveraging state-of-the-art computer vision methods and algorithms tailored specifically for processing scanned images containing both textual and graphical content. To achieve high accuracy and robustness across varying conditions and data distributions, we introduce novel approaches to preprocessing, feature extraction, and post-processing that enhance performance on standard benchmark datasets while providing versatility for adapting to custom use cases. With its modular design, easy integration into existing systems, and scalability to large datasets, LayoutParser offers significant benefits over existing solutions, including reduced manual effort, improved reliability, and better support for real-world applications such as digitization projects or document archiving initiatives. Ultimately, our framework represents a powerful resource for researchers and practitioners alike who seek advanced capabilities in document image understanding. Keywords: deep learning, document image analysis, computer vision, object detection, natural language processing (NLP).",1
"Recently introduced self-supervised methods for image representation learning provide on par or superior results to their fully supervised competitors, yet the corresponding efforts to explain the self-supervised approaches lag behind. Motivated by this observation, we introduce a novel visual probing framework for explaining the self-supervised models by leveraging probing tasks employed previously in natural language processing. The probing tasks require knowledge about semantic relationships between image parts. Hence, we propose a systematic approach to obtain analogs of natural language in vision, such as visual words, context, and taxonomy. Our proposal is grounded in Marr's computational theory of vision and concerns features like textures, shapes, and lines. We show the effectiveness and applicability of those analogs in the context of explaining self-supervised representations. Our key findings emphasize that relations between language and vision can serve as an effective yet intuitive tool for discovering how machine learning models work, independently of data modality. Our work opens a plethora of research pathways towards more explainable and transparent AI.",0
"Title: ""Visual Probing: Cognitive Framework for Explainin... moreWrite an abstract around 150 to 300 words long for a paper titled ""Visual Probing: Cognitive Framework for Explaining Self-Supervised Image Representations."" Do not include the paper title in the abstract. Do not start with the word ""This"". Title: ""Visual Probing: Cognitive Framework for Explaining Self-Supervised Image Representations""  Abstract: In recent years, self-supervised learning has emerged as a popular approach for training deep neural networks on large amounts of unlabeled data. However, one major challenge facing these models is their lack of interpretability - we often don’t know why they make certain predictions, or how their representations encode meaningful concepts. This gap between model performance and human understanding can limit applications in critical areas such as healthcare and autonomous systems. Our work addresses this problem by introducing Visual Probing (VP), a framework that enables us to query pretrained image representations using natural language prompts, generating real-time explanations that highlight relevant features across layers and regions. We show through extensive experiments that VP provides insights into state-of-the-art models like CLIP and DALL-E2, facilitating comparison against other methods, interpretation of visualizations produced by them, generation of novel examples and outlier detection. Overall, our contributions aim towards developing practical tools and techniques that bring transparency to complex models trained on millions of images. Our goal is to empower researchers and practitioners working with vision technologies, enabling better decision making grounded in human cognition. Less",1
"Deep Learning has revolutionized the fields of computer vision, natural language understanding, speech recognition, information retrieval and more. However, with the progressive improvements in deep learning models, their number of parameters, latency, resources required to train, etc. have all have increased significantly. Consequently, it has become important to pay attention to these footprint metrics of a model as well, not just its quality. We present and motivate the problem of efficiency in deep learning, followed by a thorough survey of the five core areas of model efficiency (spanning modeling techniques, infrastructure, and hardware) and the seminal work there. We also present an experiment-based guide along with code, for practitioners to optimize their model training and deployment. We believe this is the first comprehensive survey in the efficient deep learning space that covers the landscape of model efficiency from modeling techniques to hardware support. Our hope is that this survey would provide the reader with the mental model and the necessary understanding of the field to apply generic efficiency techniques to immediately get significant improvements, and also equip them with ideas for further research and experimentation to achieve additional gains.",0
"In recent years, deep learning has become increasingly popular due to its ability to achieve state-of-the-art results in numerous domains such as computer vision, natural language processing, speech recognition, and robotics. However, one major challenge facing the field is the large computational requirements and time constraints associated with training these models, particularly for larger and more complex models. To address this issue, there have been several efforts aimed at making deep learning models smaller (by reducing model complexity), faster (through improved optimization techniques) and better (in terms of accuracy). This survey provides a comprehensive overview of the current research landscape on efficient deep learning, including advancements in network architecture design, regularization methods, transfer learning, distributed computing, mixed precision and hardware acceleration. We highlight key challenges faced by practitioners working in the area, along with their corresponding solutions, and provide insights into promising future directions for research in this rapidly evolving domain.",1
"We propose a method for efficiently incorporating constraints into a stochastic gradient Langevin framework for the training of deep neural networks. Constraints allow direct control of the parameter space of the model. Appropriately designed, they reduce the vanishing/exploding gradient problem, control weight magnitudes and stabilize deep neural networks and thus improve the robustness of training algorithms and the generalization capabilities of the trained neural network. We present examples of constrained training methods motivated by orthogonality preservation for weight matrices and explicit weight normalizations. We describe the methods in the overdamped formulation of Langevin dynamics and the underdamped form, in which momenta help to improve sampling efficiency. The methods are explored in test examples in image classification and natural language processing.",0
"In order to address overfitting issues prevalent in deep neural networks, several methods such as dropout and weight decay have been proposed. However, these methods lack interpretability and fail to leverage domain knowledge available from constraints such as monotonicity and non-negativity. To fill this gap, we propose Constraint-Based Regularization (CBR), which explicitly enforces known physical or mathematical properties using first derivative regularization terms integrated into the optimization process. CBR yields better generalization performance than traditional methods on benchmark datasets without any architectural modifications and can even surpass the accuracy achieved by DropConnect and Randomized Gaussians with equivalent parameters and computational cost. Moreover, CBR exhibits higher robustness against input perturbations compared to state-of-the-art alternatives, making it more suitable for deployment in safety-critical applications such as medical imaging analysis and autonomous driving perception systems. Finally, our analysis shows that CBR leads to solutions closer to a globally optimal decision boundary while preserving their smoothness, thus providing more meaningful insights into the predictions of neural networks.",1
"Compositional generalization is the ability to generalize systematically to a new data distribution by combining known components. Although humans seem to have a great ability to generalize compositionally, state-of-the-art neural models struggle to do so. In this work, we study compositional generalization in classification tasks and present two main contributions. First, we study ways to convert a natural language sequence-to-sequence dataset to a classification dataset that also requires compositional generalization. Second, we show that providing structural hints (specifically, providing parse trees and entity links as attention masks for a Transformer model) helps compositional generalization.",0
"In recent years, deep learning has shown great success in solving complex tasks such as image classification. However, many models still suffer from poor compositional generalization, which means they struggle to recognize objects that appear in novel combinations or arrangements. One possible solution to this problem is to use structure annotations, which provide explicit information about object relationships within images. This paper investigates the effectiveness of using structure annotations on two widely used benchmark datasets: CUB200-2011 and Pascal VOC. Our results show that incorporating these annotations leads to significant improvements in model performance, outperforming state-of-the-art methods without annotations by large margins. Additionally, we explore different ways of integrating these annotations into the training process and propose new evaluation metrics tailored specifically for assessing compositional generalization. Overall, our work demonstrates the potential impact of structured knowledge on improving computer vision systems and highlights promising directions for future research.",1
"Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a ""transposed"" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.",0
"Image transformer architectures have become increasingly popular due to their ability to model long range dependencies in sequence data, resulting in state-of-the-art results across many computer vision tasks. However, these models suffer from two major drawbacks: they lack direct support for cross-correlation modelling and require large amounts of computational resources due to their sequential nature. To address these issues, we introduce XCiT (Cross-covariance Image Transformer), a novel architecture that enables efficient computation of cross-correlations within images by factorizing them into smaller local correlations that can be computed using self-attention mechanisms. We demonstrate the effectiveness of our approach on several benchmark datasets and show that our method outperforms prior methods while requiring less computational overhead. Our contributions provide new insights into how to design efficient image transformation systems that leverage modern deep learning tools and techniques.",1
"Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, robust, and generalizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an efficient search strategy for finding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-the-art library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search efficiency and generalization on three domains -- string editing, image composition, and abstract reasoning about scenes -- even when no natural language hints are available at test time.",0
"This paper presents new methods that leverage language input/output pairs (e.g., API calls) from programmers as well as natural language descriptions of code snippets to learn models that predict good search heuristics for fixing bugs as well as generating code. Our approach uses these datasets together with recent advances in deep learning to train neural network models capable of encoding contexts into compact vector representations and decoding back human-readable strings capturing the key concepts involved. We evaluate our methodology using several bug fixes and code generation tasks and show that we can generate higher quality results compared to strong baseline approaches that do not exploit linguistic features. Our work has important implications for improving software development tools by enabling machines to better understand the intent behind programming actions and hence guide them towards more effective solutions. In future work, we plan on extending our approach to further applications such as developer collaboration support systems and interactive programming tutorials.",1
"Predicting gender by the name is not a simple task. In many applications, especially in the natural language processing (NLP) field, this task may be necessary, mainly when considering foreign names. Some machine learning algorithms can satisfactorily perform the prediction. In this paper, we examined and implemented feedforward and recurrent deep neural network models, such as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first name. A dataset of Brazilian names is used to train and evaluate the models. We analyzed the accuracy, recall, precision, and confusion matrix to measure the models' performances. The results indicate that the gender prediction can be performed from the feature extraction strategy looking at the names as a set of strings. Some models accurately predict the gender in more than 90% of the cases. The recurrent models overcome the feedforward models in this binary classification problem.",0
"""This paper presents a novel approach to predicting the gender of Brazilian names using deep learning techniques. We developed a neural network model that takes the name as input and outputs whether the name belongs to a male or female person. Our approach was trained on a dataset of over one million names extracted from public sources in Brazil. The results show that our model achieves high accuracy, outperforming other traditional methods such as rules-based systems and frequency analysis. This work has implications for natural language processing, information retrieval, and data mining applications.""",1
"This work introduces World-GAN, the first method to perform data-driven Procedural Content Generation via Machine Learning in Minecraft from a single example. Based on a 3D Generative Adversarial Network (GAN) architecture, we are able to create arbitrarily sized world snippets from a given sample. We evaluate our approach on creations from the community as well as structures generated with the Minecraft World Generator. Our method is motivated by the dense representations used in Natural Language Processing (NLP) introduced with word2vec [1]. The proposed block2vec representations make World-GAN independent from the number of different blocks, which can vary a lot in Minecraft, and enable the generation of larger levels. Finally, we demonstrate that changing this new representation space allows us to change the generated style of an already trained generator. World-GAN enables its users to generate Minecraft worlds based on parts of their creations.",0
"Abstract:  The video game world of Minecraft has become extremely popular due to its blocky graphics, open-world exploration, and endless possibilities for players to create their own content within the game. As such, there has been increasing interest in generating new Minecraft worlds that offer exciting environments for players to explore and build upon. In this paper, we introduce World-GAN (Generative Adversarial Network), a generative model capable of creating realistic and diverse Minecraft worlds at scale. Our approach utilizes a deep neural network architecture inspired by GANs (Generative Adversarial Networks) which have proven effective in other domains. We evaluate our generator on a dataset of existing Minecraft worlds and demonstrate its ability to generate highly detailed and varied worlds that closely match the statistical properties of these existing datasets. Additionally, we provide visualizations to showcase the fidelity and diversity of the generated worlds. Overall, this work represents a significant advancement towards automating the generation of high quality Minecraft worlds.",1
"Combining Natural Language with Vision represents a unique and interesting challenge in the domain of Artificial Intelligence. The AI City Challenge Track 5 for Natural Language-Based Vehicle Retrieval focuses on the problem of combining visual and textual information, applied to a smart-city use case. In this paper, we present All You Can Embed (AYCE), a modular solution to correlate single-vehicle tracking sequences with natural language. The main building blocks of the proposed architecture are (i) BERT to provide an embedding of the textual descriptions, (ii) a convolutional backbone along with a Transformer model to embed the visual information. For the training of the retrieval model, a variation of the Triplet Margin Loss is proposed to learn a distance measure between the visual and language embeddings. The code is publicly available at https://github.com/cscribano/AYCE_2021.",0
"This research explores the use of natural language processing (NLP) techniques for vehicle retrieval tasks in spatio-temporal settings. We present All You Can Embed (AYCE), a NLP-based approach that leverages spatio-temporal transformers to embed both spatial and temporal contexts into a unified framework, allowing for efficient querying of vehicles across space and time. Our method addresses key limitations of previous approaches by effectively incorporating relevant geospatial features, capturing local dependencies within each modality, and exploiting interdependencies among them. We evaluate AYCE on two large-scale datasets and demonstrate significant improvements over state-of-the-art baselines in terms of accuracy and efficiency. Further analyses showcase the effectiveness of our design choices and provide insights into the impact of different components on performance. Overall, our work offers new opportunities for effective data retrieval in spatio-temporal domains using natural language queries.",1
"Purpose: Image classification is perhaps the most fundamental task in imaging AI. However, labeling images is time-consuming and tedious. We have recently demonstrated that reinforcement learning (RL) can classify 2D slices of MRI brain images with high accuracy. Here we make two important steps toward speeding image classification: Firstly, we automatically extract class labels from the clinical reports. Secondly, we extend our prior 2D classification work to fully 3D image volumes from our institution. Hence, we proceed as follows: in Part 1, we extract labels from reports automatically using the SBERT natural language processing approach. Then, in Part 2, we use these labels with RL to train a classification Deep-Q Network (DQN) for 3D image volumes.   Methods: For Part 1, we trained SBERT with 90 radiology report impressions. We then used the trained SBERT to predict class labels for use in Part 2. In Part 2, we applied multi-step image classification to allow for combined Deep-Q learning using 3D convolutions and TD(0) Q learning. We trained on a set of 90 images. We tested on a separate set of 61 images, again using the classes predicted from patient reports by the trained SBERT in Part 1. For comparison, we also trained and tested a supervised deep learning classification network on the same set of training and testing images using the same labels.   Results: Part 1: Upon training with the corpus of radiology reports, the SBERT model had 100% accuracy for both normal and metastasis-containing scans. Part 2: Then, using these labels, whereas the supervised approach quickly overfit the training data and as expected performed poorly on the testing set (66% accuracy, just over random guessing), the reinforcement learning approach achieved an accuracy of 92%. The results were found to be statistically significant, with a p-value of 3.1 x 10^-5.",0
"This research presents a novel method for analyzing 3D MRI brain scans using deep reinforcement learning and automated label extraction from clinical reports. The proposed approach achieved highly accurate classification of brain structures on a large dataset, demonstrating the potential for significantly improving medical image analysis. By leveraging both visual features and textual descriptions extracted from clinical reports, our model was able to effectively learn and identify complex anatomy patterns within brain images. Furthermore, we evaluated the generalization ability of our framework across different sites and scanning protocols, showing consistent performance even in new domains. Overall, these results represent a significant step towards fully automatic quantitative radiology, where human experts can focus their time on more challenging tasks while high quality quantification can be done efficiently by machine intelligence algorithms.",1
"This paper is a submission to the Alzheimer's Dementia Recognition through Spontaneous Speech (ADReSS) challenge, which aims to develop methods that can assist in the automated prediction of severity of Alzheimer's Disease from speech data. We focus on acoustic and natural language features for cognitive impairment detection in spontaneous speech in the context of Alzheimer's Disease Diagnosis and the mini-mental state examination (MMSE) score prediction. We proposed a model that obtains unimodal decisions from different LSTMs, one for each modality of text and audio, and then combines them using a gating mechanism for the final prediction. We focused on sequential modelling of text and audio and investigated whether the disfluencies present in individuals' speech relate to the extent of their cognitive impairment. Our results show that the proposed classification and regression schemes obtain very promising results on both development and test sets. This suggests Alzheimer's Disease can be detected successfully with sequence modeling of the speech data of medical sessions.",0
"This study presents a novel approach for recognizing early signs of Alzheimer's Dementia (AD) by analyzing multi-modal features extracted from spontaneous speech samples. To achieve this goal, we propose a feature fusion mechanism based on gate units that integrates three types of informative cues: acoustic features obtained through audio recordings, textual features derived from transcripts, and prosodic features captured via disfluency analysis. By fusing these modalities in a carefully designed manner, our system can better capture important patterns and subtleties that may escape single-modality systems, thus boosting performance significantly. We evaluated our method on two publicly available datasets consisting of individuals diagnosed with mild cognitive impairment or AD, along with healthy controls, achieving promising results. Our findings suggest that our approach offers valuable insights into the development of automated systems capable of identifying early symptoms of AD from natural language data, which could assist in improving patient outcomes.",1
"World models improve a learning agent's ability to efficiently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is filled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent's actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology significantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions.",0
"In recent years, there has been significant interest in developing world models that can represent knowledge graphs of complex environments. These models have many potential applications, including enabling robots to navigate unfamiliar spaces and improving natural language understanding by virtual agents. To build effective world models based on text descriptions, we need algorithms that can efficiently integrate large amounts of disparate data into coherent representations of real and imagined scenarios. This paper presents an approach called ""Learning Knowledge Graph-Based World Models from Text"" (KGWMT) which leverages advances in deep learning, graph representation, and computer vision to create accurate, detailed world models from written descriptions alone. Our method starts by processing input texts using transformers pretrained for question answering and summarization tasks. Next, we use graph convolutional networks to embed text entities as nodes and their relationships as edges within an attributed graph. Finally, our model integrates visual features obtained via object detection systems to improve generalizability. We demonstrate the effectiveness of KGWMT on several benchmark datasets across multiple domains, showing significantly higher accuracy than existing approaches. Overall, our work highlights the feasibility of automatically generating world models from plain text sources and offers new possibilities for building intelligent systems in areas such as robotics and human-computer interaction.",1
"Predicting chemical properties from the structure of a molecule is of great importance in many applications including drug discovery and material design. Machine learning based molecular property prediction holds the promise of enabling accurate predictions at much less complexity, when compared to, for example Density Functional Theory (DFT) calculations. Features extracted from molecular graphs, using graph neural nets in a supervised manner, have emerged as strong baselines for such tasks. However, the vast chemical space together with the limited availability of labels makes supervised learning challenging, calling for learning a general-purpose molecular representation. Recently, pre-trained transformer-based language models (PTLMs) on large unlabeled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, here we present molecular embeddings obtained by training an efficient transformer encoder model, referred to as MoLFormer. This model was employed with a linear attention mechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion unlabeled molecules from the PubChem and ZINC datasets. Experiments show that the learned molecular representation performs competitively, when compared to existing graph-based and fingerprint-based supervised learning baselines, on the challenging tasks of predicting properties of QM8 and QM9 molecules. Further task-specific fine-tuning of the MoLFormerr representation improves performance on several of those property prediction benchmarks. These results provide encouraging evidence that large-scale molecular language models can capture sufficient structural information to be able to accurately predict quantum chemical properties and beyond.",0
"In recent years, large scale molecular language representations have become increasingly popular in natural language processing tasks such as text classification, sentiment analysis, question answering, and machine translation. These methods rely on pretraining massive transformer models on very large corpora of text data, and then fine-tuning them for specific tasks using task-specific objectives. However, there has been limited investigation into whether these models capture important structural information beyond simple co-occurrence patterns that can improve performance on downstream tasks. This study seeks to address this gap by examining how well several state-of-the-art pretrained language models capture three types of linguistic structure: hierarchical dependencies among phrases and sentences, coreference chains linking mentions across texts, and semantic relatedness between concepts. Using standard evaluation metrics from prior work, we find that while these models perform reasonably well overall, they exhibit substantial variation in their ability to capture different types of structure depending on the model architecture and pretraining corpus used. Our results suggest new opportunities for developing more advanced preprocessing techniques based on richer linguistic knowledge and automated annotation tools to enhance the quality of input data for downstream applications. Finally, our study provides insights into potential future research directions aimed at designing more effective models that leverage both deep learning algorithms and formal linguistics theories to achieve better generalization and interpretability for NLP tasks.",1
"The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",0
"In this research paper we explore the effectiveness of transformer models on graph data representation tasks. We begin by reviewing the recent advancements made in the field of natural language processing due to the introduction of transformer architectures. Then, we discuss their application on graph data representation and how their performance compares to traditional approaches like GNNs (Graph Neural Networks). Our experimental results show that while transformer based models have achieved state-of-the art performances on textual data they perform poorly when applied directly to graph data representation tasks. Furthermore, our study reveals insights into why these models struggle to learn meaningful representations from graphs. Finally, we provide suggestions for future research directions aimed at improving transformer model performances on graph data representation. Our findings highlight the need for more careful consideration of model architecture design choices and data preprocessing techniques when working with graph structured data.",1
"Pretrained language models have achieved state-of-the-art performance when adapted to a downstream NLP task. However, theoretical analysis of these models is scarce and challenging since the pretraining and downstream tasks can be very different. We propose an analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text -- the downstream classifier must recover a function of the posterior distribution over the latent variables. We analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. The generative model in our analysis is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. We show that 1) under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy conditions, and 3) our recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory. Experiments on synthetically generated data from HMMs back our theoretical findings.",0
"Abstract: This article analyzes the benefits of pretrained language models (LM) for downstream NLP tasks, specifically focusing on two popular methods; head and prompt tuning. We explore how these techniques impact LM performance across several metrics and domains, comparing them against traditional fine-tuning approaches. Our results show that both types of training yield significant improvements over non-pretrained models but have varying effects depending on factors such as task complexity and data availability. Additionally, we examine potential drawbacks and limitations, highlighting the need for further study and refinement of the LM pipeline. Overall, our work provides valuable insights into the efficacy of pretrained models for natural language processing and their role in future research directions. Keywords: pretrained language model, head tuning, prompt tuning, natural language processing, machine learning",1
"The inductive biases of graph representation learning algorithms are often encoded in the background geometry of their embedding space. In this paper, we show that general directed graphs can be effectively represented by an embedding model that combines three components: a pseudo-Riemannian metric structure, a non-trivial global topology, and a unique likelihood function that explicitly incorporates a preferred direction in embedding space. We demonstrate the representational capabilities of this method by applying it to the task of link prediction on a series of synthetic and real directed graphs from natural language applications and biology. In particular, we show that low-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can produce equal or better graph representations than curved Riemannian manifolds of higher dimensions.",0
"In this paper we consider embeddings of directed graphs into pseudo-Riemannian manifolds, focusing on questions related to existence and uniqueness of such embeddings. We begin by introducing some preliminary definitions and results from the theory of pseudo-Riemannian geometry, as well as discussing motivating examples that arise in physics and engineering. We then turn to our main topic, studying properties of embeddings under different assumptions on the graph and manifold. This involves developing new techniques based on linear algebra and differential equations. Our analysis leads to new results regarding necessary conditions for embedding existence and unique determination of such embeddings up to diffeomorphism. Along the way, we highlight connections to other areas of mathematics and science, including control theory, optimization, and machine learning. Overall, our work contributes towards understanding the mathematical structure underlying systems that can be modeled using graphs embedded in spaces with nonpositive curvature.",1
