"Dense pixel matching is required for many computer vision algorithms such as disparity, optical flow or scene flow estimation. Feature Pyramid Networks (FPN) have proven to be a suitable feature extractor for CNN-based dense matching tasks. FPN generates well localized and semantically strong features at multiple scales. However, the generic FPN is not utilizing its full potential, due to its reasonable but limited localization accuracy. Thus, we present ResFPN -- a multi-resolution feature pyramid network with multiple residual skip connections, where at any scale, we leverage the information from higher resolution maps for stronger and better localized features. In our ablation study, we demonstrate the effectiveness of our novel architecture with clearly higher accuracy than FPN. In addition, we verify the superior accuracy of ResFPN in many different pixel matching applications on established datasets like KITTI, Sintel, and FlyingThings3D.",0
"Many computer vision algorithms require dense pixel matching, including disparity, optical flow, and scene flow estimation. Feature Pyramid Networks (FPN) have been successful in extracting suitable features for CNN-based dense matching tasks. However, FPN has limited localization accuracy, which limits its potential. To address this, we present ResFPN, a multi-resolution feature pyramid network with multiple residual skip connections that leverages information from higher resolution maps for stronger and better localized features. Our ablation study shows that ResFPN has higher accuracy than FPN, and we demonstrate its superior accuracy in many different pixel matching applications on established datasets such as KITTI, Sintel, and FlyingThings3D.",1
"Recently, 3D convolutional networks (3D ConvNets) yield good performance in action recognition. However, optical flow stream is still needed to ensure better performance, the cost of which is very high. In this paper, we propose a fast but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 35.6% and 26.6% points improvements over top-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18 models are trained from scratch. And we achieved the state-of-the-art results in this training mode. Analysis shows that better motion features can be extracted using residual frames compared to RGB counterpart. By combining with a simple appearance path, our proposal can be even better than some methods using optical flow streams.",0
"Action recognition has seen significant advancements with the use of 3D convolutional networks (3D ConvNets). However, to achieve optimal performance, optical flow stream is still required, which is costly. We present a novel approach to extract motion features from videos using residual frames as input data in 3D ConvNets. By replacing stacked RGB frames with residual frames, we observed a noteworthy improvement of 35.6% and 26.6% in top-1 accuracy on UCF101 and HMDB51 datasets, respectively, when ResNet-18 models are trained from scratch. Our approach outperforms existing methods, demonstrating that residual frames are better at extracting motion features than their RGB counterparts. Combining our approach with a simple appearance path can surpass some methods using optical flow streams.",1
"Pseudo-LiDAR point cloud interpolation is a novel and challenging task in the field of autonomous driving, which aims to address the frequency mismatching problem between camera and LiDAR. Previous works represent the 3D spatial motion relationship induced by a coarse 2D optical flow, and the quality of interpolated point clouds only depends on the supervision of depth maps. As a result, the generated point clouds suffer from inferior global distributions and local appearances. To solve the above problems, we propose a Pseudo-LiDAR point cloud interpolation network to generates temporally and spatially high-quality point cloud sequences. By exploiting the scene flow between point clouds, the proposed network is able to learn a more accurate representation of the 3D spatial motion relationship. For the more comprehensive perception of the distribution of point cloud, we design a novel reconstruction loss function that implements the chamfer distance to supervise the generation of Pseudo-LiDAR point clouds in 3D space. In addition, we introduce a multi-modal deep aggregation module to facilitate the efficient fusion of texture and depth features. As the benefits of the improved motion representation, training loss function, and model structure, our approach gains significant improvements on the Pseudo-LiDAR point cloud interpolation task. The experimental results evaluated on KITTI dataset demonstrate the state-of-the-art performance of the proposed network, quantitatively and qualitatively.",0
"The task of Pseudo-LiDAR point cloud interpolation is a new and difficult challenge in the realm of self-driving technology. Its main goal is to overcome the problem of camera and LiDAR frequency mismatch. Previous methods have used a 2D optical flow to represent the 3D spatial motion relationship, resulting in low-quality point clouds with uneven global distribution and local appearance. To address these issues, we present a Pseudo-LiDAR point cloud interpolation network that generates high-quality point cloud sequences with accurate 3D spatial motion representation. To achieve this, we use a scene flow between point clouds and a novel reconstruction loss function that implements the chamfer distance to supervise the generation of Pseudo-LiDAR point clouds in 3D space. Moreover, we introduce a multi-modal deep aggregation module to efficiently combine texture and depth features. Our approach significantly improves the Pseudo-LiDAR point cloud interpolation task, as demonstrated by state-of-the-art results on the KITTI dataset.",1
"Visual attention serves as a means of feature selection mechanism in the perceptual system. Motivated by Broadbent's leaky filter model of selective attention, we evaluate how such mechanism could be implemented and affect the learning process of deep reinforcement learning. We visualize and analyze the feature maps of DQN on a toy problem Catch, and propose an approach to combine visual selective attention with deep reinforcement learning. We experiment with optical flow-based attention and A2C on Atari games. Experiment results show that visual selective attention could lead to improvements in terms of sample efficiency on tested games. An intriguing relation between attention and batch normalization is also discovered.",0
"The perceptual system uses visual attention as a way to select features. Based on Broadbent's leaky filter model of selective attention, we assess how this mechanism can impact the learning process of deep reinforcement learning. Our study involves analyzing the feature maps of DQN on a simple problem called Catch, and proposing a method to integrate visual selective attention with deep reinforcement learning. We conduct experiments using optical flow-based attention and A2C on Atari games, and observe that visual selective attention can enhance sample efficiency on tested games. Moreover, we uncover a fascinating correlation between attention and batch normalization.",1
"Self-supervised learning allows for better utilization of unlabelled data. The feature representation obtained by self-supervision can be used in downstream tasks such as classification, object detection, segmentation, and anomaly detection. While classification, object detection, and segmentation have been investigated with self-supervised learning, anomaly detection needs more attention. We consider the problem of anomaly detection in images and videos, and present a new visual anomaly detection technique for videos. Numerous seminal and state-of-the-art self-supervised methods are evaluated for anomaly detection on a variety of image datasets. The best performing image-based self-supervised representation learning method is then used for video anomaly detection to see the importance of spatial features in visual anomaly detection in videos. We also propose a simple self-supervision approach for learning temporal coherence across video frames without the use of any optical flow information. At its core, our method identifies the frame indices of a jumbled video sequence allowing it to learn the spatiotemporal features of the video. This intuitive approach shows superior performance of visual anomaly detection compared to numerous methods for images and videos on UCF101 and ILSVRC2015 video datasets.",0
"Utilizing unlabelled data is made more effective through self-supervised learning, which can generate feature representations applicable in downstream tasks like classification, anomaly detection, segmentation, and object detection. While self-supervised learning has been examined for classification, segmentation, and object detection, there is a need to focus more on anomaly detection. To address this, we propose a new visual anomaly detection technique for videos by evaluating various self-supervised methods on different image datasets. With the best performing image-based self-supervised representation learning method, we assess the significance of spatial features in visual anomaly detection in videos. We also introduce a simple self-supervision strategy for learning temporal coherence across video frames without relying on optical flow information. Our approach identifies frame indices in a scrambled video sequence, enabling it to learn the spatiotemporal features of the video. Results show that our method outperforms other image and video methods on UCF101 and ILSVRC2015 video datasets in terms of visual anomaly detection.",1
"Recently, several studies proposed methods to utilize some classes of optimization problems in designing deep neural networks to encode constraints that conventional layers cannot capture. However, these methods are still in their infancy and require special treatments, such as analyzing the KKT condition, for deriving the backpropagation formula. In this paper, we propose a new layer formulation called the fixed-point iteration (FPI) layer that facilitates the use of more complicated operations in deep networks. The backward FPI layer is also proposed for backpropagation, which is motivated by the recurrent back-propagation (RBP) algorithm. But in contrast to RBP, the backward FPI layer yields the gradient by a small network module without an explicit calculation of the Jacobian. In actual applications, both the forward and backward FPI layers can be treated as nodes in the computational graphs. All components in the proposed method are implemented at a high level of abstraction, which allows efficient higher-order differentiations on the nodes. In addition, we present two practical methods of the FPI layer, FPI_NN and FPI_GD, where the update operations of FPI are a small neural network module and a single gradient descent step based on a learnable cost function, respectively. FPI\_NN is intuitive, simple, and fast to train, while FPI_GD can be used for efficient training of energy networks that have been recently studied. While RBP and its related studies have not been applied to practical examples, our experiments show the FPI layer can be successfully applied to real-world problems such as image denoising, optical flow, and multi-label classification.",0
"Recently, several studies have proposed using optimization problems to design deep neural networks that can encode constraints that traditional layers are unable to capture. Although these methods are still in their early stages and require special treatment, such as analyzing the KKT condition, for deriving the backpropagation formula. This paper introduces a new layer formulation called the fixed-point iteration (FPI) layer, which allows for the use of more complex operations in deep networks. The backward FPI layer is also proposed for backpropagation, which is motivated by the recurrent back-propagation (RBP) algorithm. However, unlike RBP, the backward FPI layer produces the gradient by a small network module without explicitly calculating the Jacobian. Both the forward and backward FPI layers can be treated as nodes in computational graphs in practical applications. The proposed method's components are implemented at a high level of abstraction, allowing for efficient higher-order differentiations on the nodes. Additionally, the paper presents two practical methods of the FPI layer, FPI_NN and FPI_GD, which use a small neural network module and a single gradient descent step based on a learnable cost function, respectively, for update operations of FPI. FPI_NN is intuitive, simple, and quick to train, while FPI_GD can be used to efficiently train energy networks that have recently been studied. While RBP and related studies have not yet been applied to practical examples, the experiments show that the FPI layer can be successfully applied to real-world problems such as image denoising, optical flow, and multi-label classification.",1
"Humans are very good at directing their visual attention toward relevant areas when they search for different types of objects. For instance, when we search for cars, we will look at the streets, not at the top of buildings. The motivation of this paper is to train a network to do the same via a multi-task learning approach. To train visual attention, we produce foreground/background segmentation labels in a semi-supervised way, using background subtraction or optical flow. Using these labels, we train an object detection model to produce foreground/background segmentation maps as well as bounding boxes while sharing most model parameters. We use those segmentation maps inside the network as a self-attention mechanism to weight the feature map used to produce the bounding boxes, decreasing the signal of non-relevant areas. We show that by using this method, we obtain a significant mAP improvement on two traffic surveillance datasets, with state-of-the-art results on both UA-DETRAC and UAVDT.",0
"The ability of humans to focus their visual attention on relevant areas when searching for objects is noteworthy. For instance, when searching for cars, people tend to look at the streets rather than the tops of buildings. The purpose of this research is to train a network to do the same using a multi-task learning approach. Our method involves producing foreground/background segmentation labels through a semi-supervised process using background subtraction or optical flow. We then train an object detection model to generate both foreground/background segmentation maps and bounding boxes, sharing most model parameters. We utilize these segmentation maps as a self-attention mechanism within the network, weighting the feature map used to generate bounding boxes and reducing the impact of non-relevant areas. Our results demonstrate a significant improvement in mAP on two traffic surveillance datasets, achieving state-of-the-art performance on both UA-DETRAC and UAVDT.",1
"Fall detection in specialized homes for the elderly is challenging. Vision-based fall detection solutions have a significant advantage over sensor-based ones as they do not instrument the resident who can suffer from mental diseases. This work is part of a project intended to deploy fall detection solutions in nursing homes. The proposed solution, based on Deep Learning, is built on a Convolutional Neural Network (CNN) trained to maximize a sensitivity-based metric. This work presents the requirements from the medical side and how it impacts the tuning of a CNN. Results highlight the importance of the temporal aspect of a fall. Therefore, a custom metric adapted to this use case and an implementation of a decision-making process are proposed in order to best meet the medical teams requirements. Clinical relevance This work presents a fall detection solution enabled to detect 86.2% of falls while producing only 11.6% of false alarms in average on the considered databases.",0
"Detecting falls in specialized homes for the elderly poses a challenge, particularly with regards to sensor-based solutions that require residents to be fitted with instruments, which may prove difficult for those with mental health issues. As part of a project aimed at deploying fall detection solutions in nursing homes, this study proposes a Deep Learning-based approach utilizing a Convolutional Neural Network (CNN) trained to optimize a sensitivity-based metric. The study outlines medical requirements and their impact on CNN tuning, highlighting the temporal aspect of falls as a crucial factor. To meet the needs of medical teams, the study presents a custom metric and decision-making process, resulting in a fall detection solution that detected 86.2% of falls while producing only 11.6% of false alarms on average across the databases considered. The clinical relevance of this approach is significant.",1
"Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera-based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.",0
"In any Advanced driver-assistance system (ADAS), two fundamental features are inter-vehicle distance and relative velocity estimations. This article introduces a novel approach to estimate these features using a monocular camera-based method. The approach employs an end-to-end training of a deep neural network, integrating multiple visual clues from two consecutive frames. These clues include deep feature, scene geometry, and temporal optical flow. The method also proposes a vehicle-centric sampling mechanism to mitigate perspective distortion in the motion field. A lightweight deep neural network is used to implement the method, and extensive experiments confirm its superior performance over other state-of-the-art methods. The performance metrics include estimation accuracy, computational speed, and memory footprint.",1
"In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to aid receptive field selection in pose estimation models based on optical flow inference. We contrast the performance of a benchmark state-of-the-art model running on fixed receptive fields with their adaptive field counterparts. By using a reduced receptive field, our model can process slow-motion sequences (10x longer) 23% faster than the benchmark model running at regular speed. The reduction in computational cost is achieved while producing a pose prediction accuracy to within 0.36% of the benchmark model.",0
"This study showcases the successful use of optical flow for determining receptive fields in 3D pose estimation. The introduction of adaptive receptive fields serves as a helpful and uncomplicated approach in selecting receptive fields for pose estimation models that utilize optical flow inference. To compare the performance of fixed receptive fields and adaptive fields, we utilized a state-of-the-art benchmark model. Our results show that our model, which uses reduced receptive fields, is capable of processing slow-motion sequences that are 10 times longer and is 23% faster than the benchmark model running at a regular speed. This reduction in computational costs does not compromise the accuracy of our pose estimation as our model produced pose predictions within 0.36% of the benchmark model's predictions.",1
"Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.",0
"Representing videos is a complex task that poses challenges in both algorithmic and computational aspects. The traditional approach of extending image-based architectures to include time dimension via 3D convolutions or two-stream design has limitations. We propose a new method called AssembleNet that automatically discovers neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is achieved by evolving a population of overly-connected architectures guided by connection weight learning. AssembleNet combines representations that abstract multiple input types at various temporal resolutions, enabling different sources of information to interact with each other. Our approach outperforms prior methods on public video datasets, with 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time, demonstrating its effectiveness.",1
"Motion is a salient cue to recognize actions in video. Modern action recognition models leverage motion information either explicitly by using optical flow as input or implicitly by means of 3D convolutional filters that simultaneously capture appearance and motion information. This paper proposes an alternative approach based on a learnable correlation operator that can be used to establish frame-toframe matches over convolutional feature maps in the different layers of the network. The proposed architecture enables the fusion of this explicit temporal matching information with traditional appearance cues captured by 2D convolution. Our correlation network compares favorably with widely-used 3D CNNs for video modeling, and achieves competitive results over the prominent two-stream network while being much faster to train. We empirically demonstrate that correlation networks produce strong results on a variety of video datasets, and outperform the state of the art on four popular benchmarks for action recognition: Kinetics, Something-Something, Diving48 and Sports1M.",0
"To identify actions in videos, motion is a significant indicator. Contemporary models for action recognition utilize motion information either in an explicit manner by utilizing optical flow as input or implicitly through 3D convolutional filters that capture both appearance and motion information. This article introduces a different approach that relies on a learnable correlation operator to establish matches between frames over convolutional feature maps in different layers of the network. This architecture allows for the combination of explicit temporal matching data with traditional 2D convolutional appearance cues. Our correlation network is comparable to widely-used 3D CNNs for video modeling and produces competitive results compared to the prominent two-stream network while being much quicker to train. We have demonstrated through empirical evidence that correlation networks generate strong results across various video datasets and surpass the state of the art across four popular benchmarks for action recognition: Kinetics, Something-Something, Diving48, and Sports1M.",1
"Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a CNN based patch matching approach for optical flow estimation. An important contribution of our approach is a novel thresholded loss for Siamese networks. We demonstrate that our loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods. We also discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low-pass filtering of feature maps can increase the robustness of features created by CNNs. We proved the competitive performance of our approach by submitting it to the KITTI 2012, KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art results on all three datasets.",0
"Despite efforts, learning based methods have not reached their maximum potential for optical flow estimation, as heuristic approaches still outperform them. This paper introduces a CNN patch matching approach for optical flow estimation, with a key contribution being a new thresholded loss for Siamese networks that outperforms existing losses and speeds up training by 2x. Additionally, the paper presents an innovative method for calculating CNN based features for different image scales that surpasses current methods, as well as new techniques for evaluating the robustness of trained features for patch matching in optical flow. Interestingly, the authors discovered that low-pass filtering of feature maps can increase the robustness of CNN-created features. The approach was evaluated on KITTI 2012, KITTI 2015, and MPI-Sintel evaluation portals, where it achieved state-of-the-art results on all three datasets.",1
"Face spoofing causes severe security threats in face recognition systems. Previous anti-spoofing works focused on supervised techniques, typically with either binary or auxiliary supervision. Most of them suffer from limited robustness and generalization, especially in the cross-dataset setting. In this paper, we propose a semi-supervised adversarial learning framework for spoof face detection, which largely relaxes the supervision condition. To capture the underlying structure of live faces data in latent representation space, we propose to train the live face data only, with a convolutional Encoder-Decoder network acting as a Generator. Meanwhile, we add a second convolutional network serving as a Discriminator. The generator and discriminator are trained by competing with each other while collaborating to understand the underlying concept in the normal class(live faces). Since the spoof face detection is video based (i.e., temporal information), we intuitively take the optical flow maps converted from consecutive video frames as input. Our approach is free of the spoof faces, thus being robust and general to different types of spoof, even unknown spoof. Extensive experiments on intra- and cross-dataset tests show that our semi-supervised method achieves better or comparable results to state-of-the-art supervised techniques.",0
"The use of fake faces in face recognition systems poses a significant security risk. Previous attempts to prevent this have focused on supervised techniques, which have limitations in their robustness and generalization, particularly when used across different datasets. To address these issues, we propose a semi-supervised adversarial learning framework for detecting spoof faces. This approach requires less supervision and trains a convolutional Encoder-Decoder network to recognize live faces. A second convolutional network serves as a Discriminator, and the generator and discriminator compete and collaborate to better understand the underlying concept of live faces. To account for the temporal aspect of spoof face detection, we use optical flow maps converted from successive video frames. Our method is robust and can detect different types of spoof, including unknown ones. Experimental results demonstrate that our semi-supervised approach is comparable to or better than current supervised methods in both intra- and cross-dataset settings.",1
"Instance segmentation of unknown objects from images is regarded as relevant for several robot skills including grasping, tracking and object sorting. Recent results in computer vision have shown that large hand-labeled datasets enable high segmentation performance. To overcome the time-consuming process of manually labeling data for new environments, we present a transfer learning approach for robots that learn to segment objects by interacting with their environment in a self-supervised manner. Our robot pushes unknown objects on a table and uses information from optical flow to create training labels in the form of object masks. To achieve this, we fine-tune an existing DeepMask network for instance segmentation on the self-labeled training data acquired by the robot. We evaluate our trained network (SelfDeepMask) on a set of real images showing challenging and cluttered scenes with novel objects. Here, SelfDeepMask outperforms the DeepMask network trained on the COCO dataset by 9.5% in average precision. Furthermore, we combine our approach with recent approaches for training with noisy labels in order to better cope with induced label noise.",0
"The relevance of instance segmentation for various robot skills, such as grasping, tracking, and object sorting, is widely acknowledged, and recent advances in computer vision have demonstrated that large, hand-labeled datasets improve segmentation performance. However, the process of manually labeling data for new environments is time-consuming. To address this issue, we propose a transfer learning method for robots to learn to segment objects by interacting with their environment in a self-supervised manner. Our approach involves pushing unknown objects on a table and using optical flow information to generate object masks for training. We fine-tune an existing DeepMask network for instance segmentation using the self-labeled training data. The resulting network, SelfDeepMask, is evaluated on real images of challenging and cluttered scenes with novel objects, and it outperforms the DeepMask network trained on the COCO dataset by 9.5% in average precision. We also incorporate recent methods for training with noisy labels to improve our approach's ability to handle induced label noise.",1
"We reveal that the Analytic Signal phase, and its gradient have a hitherto unstudied discontinuity in $2-D $ and higher dimensions. The shortcoming can result in severe artifacts whereas the problem does not exist in $1-D $ signals. Direct use of Gabor phase, or its gradient, in computer vision and biometric recognition e.g., as done in influential studies \cite{fleet90,wiskott1997face}, may produce undesired results that will go unnoticed unless special images similar to ours reveal them. Instead of the Analytic Signal phase, we suggest the use of Linear Symmetry phase, relying on more than one set of Gabor filters, but with a negligible computational add-on, as a remedy. Gradient magnitudes of this phase are continuous in contrast to that of the analytic signal whereas continuity of the gradient direction of the phase is guaranteed if Linear Symmetry Tensor replaces gradient vector. The suggested phase has also a built-in automatic scale estimator, useful for robust detection of patterns by multi-scale processing. We show crucial concepts on synthesized fingerprint images, where ground truth regarding instantaneous frequency, (scale \& direction), and phase are known with favorable results. A comparison to a baseline alternative is also reported. To that end, a novel multi-scale minutia model where location, direction, and scale of minutia parameters are steerable, without the creation of uncontrollable minutia is also presented. This is a useful tool, to reduce development times of minutia detection methods with explainable behavior. A revealed consequence is that minutia directions are not determined by the linear phase alone, but also by each other and the influence must be corrected to obtain steerability and accurate ground truths. Essential conclusions are readily transferable to $N-D $, and unrelated applications, e.g. optical flow or disparity estimation in stereo.",0
"In this study, we have identified a previously unexamined discontinuity in the Analytic Signal phase and its gradient in 2-D and higher dimensions, which can lead to significant artifacts in computer vision and biometric recognition. Although this issue does not exist in 1-D signals, it can cause problems when using Gabor phase or its gradient. To avoid these issues, we propose using Linear Symmetry phase, which relies on multiple sets of Gabor filters and has a negligible computational cost. The gradient magnitudes of this phase are continuous, and the gradient direction is guaranteed to be continuous if the Linear Symmetry Tensor replaces the gradient vector. Additionally, this phase includes an automatic scale estimator, making it useful for robust pattern detection through multi-scale processing. We demonstrate the efficacy of this approach on synthesized fingerprint images, where ground truth regarding instantaneous frequency, scale, direction, and phase is known. Our approach also includes a multi-scale minutia model that enables steerable minutia detection methods with explainable behavior. We highlight that minutia directions are influenced by each other and must be corrected to obtain accurate ground truths. These findings are applicable beyond biometric recognition and computer vision, including optical flow or disparity estimation in stereo, and can be extended to N-D analysis.",1
"In this paper, we address the open research problem of surgical gesture recognition using motion cues from video data only. We adapt Optical flow ConvNets initially proposed by Simonyan et al.. While Simonyan uses both RGB frames and dense optical flow, we use only dense optical flow representations as input to emphasize the role of motion in surgical gesture recognition, and present it as a robust alternative to kinematic data. We also overcome one of the limitations of Optical flow ConvNets by initializing our model with cross modality pre-training. A large number of promising studies that address surgical gesture recognition highly rely on kinematic data which requires additional recording devices. To our knowledge, this is the first paper that addresses surgical gesture recognition using dense optical flow information only. We achieve competitive results on JIGSAWS dataset, moreover, our model achieves more robust results with less standard deviation, which suggests optical flow information can be used as an alternative to kinematic data for the recognition of surgical gestures.",0
"The objective of this research paper is to tackle the unresolved issue of recognizing surgical gestures using solely motion cues from video data. Our approach involves modifying Optical flow ConvNets proposed by Simonyan et al. by using only dense optical flow representations as input to highlight the importance of motion in surgical gesture recognition. We present this as a reliable alternative to kinematic data and address a limitation of Optical flow ConvNets by initializing our model with cross modality pre-training. Many studies on surgical gesture recognition rely heavily on kinematic data which requires additional recording equipment. To our knowledge, this is the first paper to focus on surgical gesture recognition using dense optical flow information exclusively. We obtain competitive results on JIGSAWS dataset, and our model is more dependable with less standard deviation, indicating that optical flow information can be used as a substitute for kinematic data in recognizing surgical gestures.",1
"We present a self-supervised learning framework to estimate the individual object motion and monocular depth from video. We model the object motion as a 6 degree-of-freedom rigid-body transformation. The instance segmentation mask is leveraged to introduce the information of object. Compared with methods which predict dense optical flow map to model the motion, our approach significantly reduces the number of values to be estimated. Our system eliminates the scale ambiguity of motion prediction through imposing a novel geometric constraint loss term. Experiments on KITTI driving dataset demonstrate our system is capable to capture the object motion without external annotation. Our system outperforms previous self-supervised approaches in terms of 3D scene flow prediction, and contribute to the disparity prediction in dynamic area.",0
"A framework for self-supervised learning is proposed in this study for estimating individual object motion and monocular depth from video. The object motion is modeled as a 6 degree-of-freedom rigid-body transformation and the instance segmentation mask is utilized to incorporate object information. Compared to methods that use dense optical flow maps to model motion, our approach significantly reduces the number of values to be estimated. Our system solves the scale ambiguity issue of motion prediction by enforcing a novel geometric constraint loss term. The KITTI driving dataset is used for experiments and our system is found to be capable of capturing object motion without external annotation. Our system is superior to previous self-supervised approaches in terms of 3D scene flow prediction and contributes to disparity prediction in dynamic areas.",1
"Micro-expressions are brief and subtle facial expressions that go on and off the face in a fraction of a second. This kind of facial expressions usually occurs in high stake situations and is considered to reflect a human's real intent. There has been some interest in micro-expression analysis, however, a great majority of the methods are based on classically established computer vision methods such as local binary patterns, histogram of gradients and optical flow. A novel methodology for micro-expression recognition using the Riesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact, an image sequence is transformed with this tool, then the image phase variations are extracted and filtered as proxies for motion. Furthermore, the dominant orientation constancy from the Riesz transform is exploited to average the micro-expression sequence into an image pair. Based on that, the Mean Oriented Riesz Feature description is introduced. Finally the performance of our methods are tested in two spontaneous micro-expressions databases and compared to state-of-the-art methods.",0
"Micro-expressions are subtle facial expressions that occur briefly and quickly, typically in high-pressure situations, and are believed to reveal a person's true intentions. While there is interest in analyzing micro-expressions, most methods rely on traditional computer vision techniques such as local binary patterns, histogram of gradients, and optical flow. However, a new approach to micro-expression recognition is presented using the Riesz pyramid, a multi-scale steerable Hilbert transform. This method transforms an image sequence and extracts the phase variations to filter as a proxy for motion. Additionally, the Riesz transform's dominant orientation constancy is used to average the micro-expression sequence into an image pair, resulting in the introduction of the Mean Oriented Riesz Feature description. The performance of this method is tested on two spontaneous micro-expression databases and compared to state-of-the-art techniques.",1
"Understanding on-road vehicle behaviour from a temporal sequence of sensor data is gaining in popularity. In this paper, we propose a pipeline for understanding vehicle behaviour from a monocular image sequence or video. A monocular sequence along with scene semantics, optical flow and object labels are used to get spatial information about the object (vehicle) of interest and other objects (semantically contiguous set of locations) in the scene. This spatial information is encoded by a Multi-Relational Graph Convolutional Network (MR-GCN), and a temporal sequence of such encodings is fed to a recurrent network to label vehicle behaviours. The proposed framework can classify a variety of vehicle behaviours to high fidelity on datasets that are diverse and include European, Chinese and Indian on-road scenes. The framework also provides for seamless transfer of models across datasets without entailing re-annotation, retraining and even fine-tuning. We show comparative performance gain over baseline Spatio-temporal classifiers and detail a variety of ablations to showcase the efficacy of the framework.",0
"The popularity of understanding on-road vehicle behavior through a sequence of sensor data is increasing. This paper presents a method for comprehending vehicle behavior using a series of monocular images or videos. The technique involves using a monocular sequence, scene semantics, optical flow, and object labels to obtain spatial information for the object of interest and other objects in the scene. This spatial information is then encoded using a Multi-Relational Graph Convolutional Network (MR-GCN), and the resulting temporal sequence of encodings is input into a recurrent network for labeling vehicle behaviors. The proposed framework is effective in classifying various vehicle behaviors accurately using diverse datasets that include European, Chinese, and Indian on-road scenes. Moreover, the framework allows for seamless model transfer across datasets without requiring re-annotation, retraining, or even fine-tuning. Our experiments indicate improved performance over baseline Spatio-temporal classifiers and highlight the effectiveness of the proposed framework through various ablations.",1
"In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm.",0
"This paper addresses the challenge of predicting the actions and object interactions of camera wearers in egocentric videos, a task known as egocentric action anticipation. To achieve this, we propose a new learning architecture called Rolling-Unrolling LSTM. This method consists of three components: 1) a two-LSTM architecture to model past events and future predictions, 2) a Sequence Completion Pre-Training technique to encourage the LSTMs to focus on different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to fuse multi-modal predictions from RGB frames, optical flow fields, and object-based features. Our approach is validated on three datasets and achieves top performance in the 2019 EPIC-Kitchens egocentric action anticipation challenge. Additionally, it performs well on ActivityNet and generalizes to early action recognition and action recognition tasks. To encourage further research, our code, trained models, and pre-extracted features are available on our website: http://iplab.dmi.unict.it/rulstm.",1
"Temporal feature extraction is an important issue in video-based action recognition. Optical flow is a popular method to extract temporal feature, which produces excellent performance thanks to its capacity of capturing pixel-level correlation information between consecutive frames. However, such a pixel-level correlation is extracted at the cost of high computational complexity and large storage resource. In this paper, we propose a novel temporal feature extraction method, named Attentive Correlated Temporal Feature (ACTF), by exploring inter-frame correlation within a certain region. The proposed ACTF exploits both bilinear and linear correlation between successive frames on the regional level. Our method has the advantage of achieving performance comparable to or better than optical flow-based methods while avoiding the introduction of optical flow. Experimental results demonstrate our proposed method achieves the state-of-the-art performances of 96.3% on UCF101 and 76.3% on HMDB51 benchmark datasets.",0
"The extraction of temporal features is crucial for recognizing actions in video-based content. Optical flow is a commonly used approach for temporal feature extraction, as it effectively captures correlations between adjacent frames at the pixel level. However, this method is computationally complex and requires significant storage resources. To address this issue, we introduce a new technique called Attentive Correlated Temporal Feature (ACTF), which leverages inter-frame correlations within specific regions. Our method combines both bilinear and linear correlations between frames at the regional level, achieving comparable or superior performance to optical flow-based methods without the need for optical flow. Our experiments demonstrate that ACTF achieves state-of-the-art results of 96.3% on UCF101 and 76.3% on HMDB51 benchmark datasets.",1
"Understanding ego-motion and surrounding vehicle state is essential to enable automated driving and advanced driving assistance technologies. Typical approaches to solve this problem use fusion of multiple sensors such as LiDAR, camera, and radar to recognize surrounding vehicle state, including position, velocity, and orientation. Such sensing modalities are overly complex and costly for production of personal use vehicles. In this paper, we propose a novel machine learning method to estimate ego-motion and surrounding vehicle state using a single monocular camera. Our approach is based on a combination of three deep neural networks to estimate the 3D vehicle bounding box, depth, and optical flow from a sequence of images. The main contribution of this paper is a new framework and algorithm that integrates these three networks in order to estimate the ego-motion and surrounding vehicle state. To realize more accurate 3D position estimation, we address ground plane correction in real-time. The efficacy of the proposed method is demonstrated through experimental evaluations that compare our results to ground truth data available from other sensors including Can-Bus and LiDAR.",0
"To enable automated driving and advanced driving assistance technologies, it is crucial to comprehend ego-motion and surrounding vehicle state. Typically, fusion of multiple sensors like LiDAR, camera, and radar is employed to identify surrounding vehicle state, encompassing position, velocity, and orientation. However, this approach is excessively complicated and expensive for personal use vehicle production. Hence, in this study, we introduce a new machine learning technique that uses a single monocular camera to estimate ego-motion and surrounding vehicle state. Our method involves three deep neural networks that estimate 3D vehicle bounding box, depth, and optical flow from a sequence of images. We integrate these networks to estimate the ego-motion and surrounding vehicle state and address ground plane correction in real-time to achieve more precise 3D position estimation. We demonstrate the effectiveness of our approach through experimental evaluations that compare our results with ground truth data from other sensors like Can-Bus and LiDAR.",1
"We propose to modify the common training protocols of optical flow, leading to sizable accuracy improvements without adding to the computational complexity of the training process. The improvement is based on observing the bias in sampling challenging data that exists in the current training protocol, and improving the sampling process. In addition, we find that both regularization and augmentation should decrease during the training protocol.   Using an existing low parameters architecture, the method is ranked first on the MPI Sintel benchmark among all other methods, improving the best two frames method accuracy by more than 10%. The method also surpasses all similar architecture variants by more than 12% and 19.7% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods, without using extra datasets.",0
"Our proposal aims to enhance the accuracy of optical flow training protocols by modifying them without increasing computational complexity. We have identified a bias in the sampling of challenging data in the current training protocol, which we address by improving the sampling process. Furthermore, we have found that regularization and augmentation should decrease during the training protocol. By leveraging an existing low parameters architecture, our method has outperformed all other methods on the MPI Sintel benchmark, delivering an accuracy improvement of over 10% for the best two frames method. Additionally, our approach has surpassed all similar architecture variants by more than 12% and 19.7% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods without utilizing additional datasets.",1
"Egocentric activity recognition in first-person videos has an increasing importance with a variety of applications such as lifelogging, summarization, assisted-living and activity tracking. Existing methods for this task are based on interpretation of various sensor information using pre-determined weights for each feature. In this work, we propose a new framework for egocentric activity recognition problem based on combining audio-visual features with multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). For that purpose, firstly grid optical-flow, virtual-inertia feature, log-covariance, cuboid are extracted from the video. The audio signal is characterized using a ""supervector"", obtained based on Gaussian mixture modelling of frame-level features, followed by a maximum a-posteriori adaptation. Then, the extracted multi-modal features are adaptively fused by MKL classifiers in which both the feature and kernel selection/weighing and recognition tasks are performed together. The proposed framework was evaluated on a number of egocentric datasets. The results showed that using multi-modal features with MKL outperforms the existing methods.",0
"The recognition of egocentric activity in first-person videos is becoming increasingly important for various applications, such as activity tracking, assisted-living, lifelogging, and summarization. Current methods for this task utilize pre-determined weights for interpreting sensor information from various sources. This study introduces a novel framework for egocentric activity recognition that combines audio-visual features with multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). To achieve this, the video's grid optical-flow, virtual-inertia feature, log-covariance, and cuboid are extracted, while the audio signal is characterized using a supervector obtained through Gaussian mixture modelling of frame-level features followed by maximum a-posteriori adaptation. The extracted multi-modal features are then fused adaptively by MKL classifiers, which perform both the feature and kernel selection/weighing and recognition tasks simultaneously. The proposed framework was evaluated on various egocentric datasets, and the results indicate that using multi-modal features with MKL produces superior results compared to existing methods.",1
"Current benchmarks for optical flow algorithms evaluate the estimation either directly by comparing the predicted flow fields with the ground truth or indirectly by using the predicted flow fields for frame interpolation and then comparing the interpolated frames with the actual frames. In the latter case, objective quality measures such as the mean squared error are typically employed. However, it is well known that for image quality assessment, the actual quality experienced by the user cannot be fully deduced from such simple measures. Hence, we conducted a subjective quality assessment crowdscouring study for the interpolated frames provided by one of the optical flow benchmarks, the Middlebury benchmark. We collected forced-choice paired comparisons between interpolated images and corresponding ground truth. To increase the sensitivity of observers when judging minute difference in paired comparisons we introduced a new method to the field of full-reference quality assessment, called artefact amplification. From the crowdsourcing data, we reconstructed absolute quality scale values according to Thurstone's model. As a result, we obtained a re-ranking of the 155 participating algorithms w.r.t. the visual quality of the interpolated frames. This re-ranking not only shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks, the results also provide the ground truth for designing novel image quality assessment (IQA) methods dedicated to perceptual quality of interpolated images. As a first step, we proposed such a new full-reference method, called WAE-IQA. By weighing the local differences between an interpolated image and its ground truth WAE-IQA performed slightly better than the currently best FR-IQA approach from the literature.",0
"Optical flow algorithms are currently evaluated using two methods: direct comparison between predicted and actual flow fields, or indirect comparison by interpolating frames using predicted flow fields and comparing them with the actual frames using objective measures like mean squared error. However, such measures do not fully capture the user's actual experience of image quality. Therefore, we conducted a subjective quality assessment study using crowd-sourcing for the Middlebury benchmark's interpolated frames. We introduced a new method called artefact amplification to increase sensitivity in observers, and reconstructed absolute quality scale values according to Thurstone's model. We obtained a re-ranking of the algorithms based on visual quality of the interpolated frames, which highlights the need for visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks. We proposed a new full-reference method called WAE-IQA, which outperformed the current best FR-IQA approach from the literature. This study provides ground truth for designing novel IQA methods dedicated to the perceptual quality of interpolated images.",1
"Modeling hand-object manipulations is essential for understanding how humans interact with their environment. While of practical importance, estimating the pose of hands and objects during interactions is challenging due to the large mutual occlusions that occur during manipulation. Recent efforts have been directed towards fully-supervised methods that require large amounts of labeled training samples. Collecting 3D ground-truth data for hand-object interactions, however, is costly, tedious, and error-prone. To overcome this challenge we present a method to leverage photometric consistency across time when annotations are only available for a sparse subset of frames in a video. Our model is trained end-to-end on color images to jointly reconstruct hands and objects in 3D by inferring their poses. Given our estimated reconstructions, we differentiably render the optical flow between pairs of adjacent images and use it within the network to warp one frame to another. We then apply a self-supervised photometric loss that relies on the visual consistency between nearby images. We achieve state-of-the-art results on 3D hand-object reconstruction benchmarks and demonstrate that our approach allows us to improve the pose estimation accuracy by leveraging information from neighboring frames in low-data regimes.",0
"Understanding how humans interact with their environment requires modeling hand-object manipulations. However, estimating the pose of hands and objects during interactions is challenging due to mutual occlusions. Fully-supervised methods have been used to address this issue, but collecting 3D ground-truth data is costly, tedious, and error-prone. To overcome this challenge, we propose leveraging photometric consistency across time when annotations are only available for a sparse subset of frames in a video. Our end-to-end model jointly reconstructs hands and objects in 3D by inferring their poses from color images. We use differentiable rendering of optical flow between adjacent images to warp one frame to another and apply a self-supervised photometric loss that relies on visual consistency between nearby images. Our approach achieves state-of-the-art results on 3D hand-object reconstruction benchmarks and improves pose estimation accuracy by leveraging information from neighboring frames in low-data regimes.",1
"Recent deep learning approaches have achieved impressive performance on visual sound separation tasks. However, these approaches are mostly built on appearance and optical flow like motion feature representations, which exhibit limited abilities to find the correlations between audio signals and visual points, especially when separating multiple instruments of the same types, such as multiple violins in a scene. To address this, we propose ""Music Gesture,"" a keypoint-based structured representation to explicitly model the body and finger movements of musicians when they perform music. We first adopt a context-aware graph network to integrate visual semantic context with body dynamics, and then apply an audio-visual fusion model to associate body movements with the corresponding audio signals. Experimental results on three music performance datasets show: 1) strong improvements upon benchmark metrics for hetero-musical separation tasks (i.e. different instruments); 2) new ability for effective homo-musical separation for piano, flute, and trumpet duets, which to our best knowledge has never been achieved with alternative methods. Project page: http://music-gesture.csail.mit.edu.",0
"Impressive results have been achieved by recent deep learning approaches in visual sound separation tasks. However, their reliance on appearance and optical flow like motion feature representations has limited their ability to identify the correlations between audio signals and visual points, particularly when separating multiple instruments of the same type. In response, we propose ""Music Gesture,"" which employs a structured representation based on keypoints to model the body and finger movements of musicians when performing music. We use a context-aware graph network to incorporate visual semantic context with body dynamics, and an audio-visual fusion model to connect body movements with corresponding audio signals. Experiments conducted on three music performance datasets demonstrate that our approach leads to significant improvements in hetero-musical separation tasks and enables effective homo-musical separation for piano, flute, and trumpet duets, which has not been achieved by other methods. Additional information can be found on our project page at http://music-gesture.csail.mit.edu.",1
"We study the energy minimization problem in low-level vision tasks from a novel perspective. We replace the heuristic regularization term with a learnable subspace constraint, and preserve the data term to exploit domain knowledge derived from the first principle of a task. This learning subspace minimization (LSM) framework unifies the network structures and the parameters for many low-level vision tasks, which allows us to train a single network for multiple tasks simultaneously with completely shared parameters, and even generalizes the trained network to an unseen task as long as its data term can be formulated. We demonstrate our LSM framework on four low-level tasks including interactive image segmentation, video segmentation, stereo matching, and optical flow, and validate the network on various datasets. The experiments show that the proposed LSM generates state-of-the-art results with smaller model size, faster training convergence, and real-time inference.",0
"A new perspective is taken on the energy minimization problem in low-level vision tasks. Instead of using a heuristic regularization term, a learnable subspace constraint is utilized while maintaining the data term to benefit from domain knowledge. This approach, called the learning subspace minimization (LSM) framework, unites network structures and parameters for various low-level vision tasks. This enables a single network to be trained for multiple tasks with shared parameters, and even generalizes to new tasks as long as the data term can be defined. The LSM framework is demonstrated on four low-level tasks and validated on different datasets. Results show that LSM generates superior outcomes with a smaller model size, faster training convergence, and real-time inference.",1
"Object tracking and 3D reconstruction are often performed together, with tracking used as input for reconstruction. However, the obtained reconstructions also provide useful information for improving tracking. We propose a novel method that closes this loop, first tracking to reconstruct, and then reconstructing to track. Our approach, MOTSFusion (Multi-Object Tracking, Segmentation and dynamic object Fusion), exploits the 3D motion extracted from dynamic object reconstructions to track objects through long periods of complete occlusion and to recover missing detections. Our approach first builds up short tracklets using 2D optical flow, and then fuses these into dynamic 3D object reconstructions. The precise 3D object motion of these reconstructions is used to merge tracklets through occlusion into long-term tracks, and to locate objects when detections are missing. On KITTI, our reconstruction-based tracking reduces the number of ID switches of the initial tracklets by more than 50%, and outperforms all previous approaches for both bounding box and segmentation tracking.",0
"Typically, object tracking and 3D reconstruction are done simultaneously, where tracking serves as input for reconstruction. However, the reconstructions obtained from this process also offer valuable insights for improving tracking. Our new method, MOTSFusion, closes this feedback loop by first using tracking to reconstruct, and then reconstructing to track. By leveraging the 3D motion extracted from dynamic object reconstructions, our approach can track objects through extended periods of total occlusion and recover missing detections. We begin by creating short tracklets using 2D optical flow and then combining them into dynamic 3D object reconstructions. We use the accurate 3D object motion from these reconstructions to merge tracklets through occlusion into long-term tracks and locate objects when detections are missing. Our reconstruction-based tracking on KITTI has reduced the number of ID switches of the initial tracklets by over 50% and outperforms all previous methods for both bounding box and segmentation tracking.",1
"Semi-supervised video object segmentation aims to separate a target object from a video sequence, given the mask in the first frame. Most of current prevailing methods utilize information from additional modules trained in other domains like optical flow and instance segmentation, and as a result they do not compete with other methods on common ground. To address this issue, we propose a simple yet strong transductive method, in which additional modules, datasets, and dedicated architectural designs are not needed. Our method takes a label propagation approach where pixel labels are passed forward based on feature similarity in an embedding space. Different from other propagation methods, ours diffuses temporal information in a holistic manner which take accounts of long-term object appearance. In addition, our method requires few additional computational overhead, and runs at a fast $\sim$37 fps speed. Our single model with a vanilla ResNet50 backbone achieves an overall score of 72.3 on the DAVIS 2017 validation set and 63.1 on the test set. This simple yet high performing and efficient method can serve as a solid baseline that facilitates future research. Code and models are available at \url{https://github.com/microsoft/transductive-vos.pytorch}.",0
"The objective of semi-supervised video object segmentation is to separate a target object from a video sequence by using the mask in the first frame. Presently, most of the methods use information from other domains such as optical flow and instance segmentation, which makes it hard to compare their performance with other techniques. To tackle this problem, we suggest a transductive method that is simple but robust and does not require additional modules, datasets, or special architectural designs. Our approach uses label propagation, where pixel labels move forward based on feature similarity in an embedding space. Unlike other propagation techniques, our method diffuses temporal information holistically, taking into account long-term object appearance. Furthermore, our method has low computational overhead, running at a fast speed of approximately 37 fps. With a vanilla ResNet50 backbone, our single model achieves an overall score of 72.3 on the DAVIS 2017 validation set and 63.1 on the test set. This efficient and high-performing method can serve as a solid baseline for future research. Our code and models are available at \url{https://github.com/microsoft/transductive-vos.pytorch}.",1
"Scene flow estimation has been receiving increasing attention for 3D environment perception. Monocular scene flow estimation -- obtaining 3D structure and 3D motion from two temporally consecutive images -- is a highly ill-posed problem, and practical solutions are lacking to date. We propose a novel monocular scene flow method that yields competitive accuracy and real-time performance. By taking an inverse problem view, we design a single convolutional neural network (CNN) that successfully estimates depth and 3D motion simultaneously from a classical optical flow cost volume. We adopt self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We validate our design choices, including the proxy loss and augmentation setup. Our model achieves state-of-the-art accuracy among unsupervised/self-supervised learning approaches to monocular scene flow, and yields competitive results for the optical flow and monocular depth estimation sub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields promising results in real-time.",0
"The estimation of scene flow has become increasingly important for understanding 3D environments. However, obtaining 3D structure and motion from two consecutive images using monocular scene flow estimation is a difficult problem that currently lacks practical solutions. In this study, we propose a novel method that achieves high accuracy and real-time performance. By approaching the problem from an inverse perspective, we developed a single convolutional neural network that can estimate depth and 3D motion simultaneously. We utilized self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We also validated our choices, including the proxy loss and augmentation setup. Our method outperformed other unsupervised/self-supervised learning approaches for monocular scene flow and achieved competitive results for optical flow and monocular depth estimation. We further improved accuracy through semi-supervised fine-tuning and achieved promising real-time results.",1
"Correspondence estimation is one of the most widely researched and yet only partially solved area of computer vision with many applications in tracking, mapping, recognition of objects and environment. In this paper, we propose a novel way to estimate dense correspondence on an RGB image where visual descriptors are learned from video examples by training a fully convolutional network. Most deep learning methods solve this by training the network with a large set of expensive labeled data or perform labeling through strong 3D generative models using RGB-D videos. Our method learns from RGB videos using contrastive loss, where relative labeling is estimated from optical flow. We demonstrate the functionality in a quantitative analysis on rendered videos, where ground truth information is available. Not only does the method perform well on test data with the same background, it also generalizes to situations with a new background. The descriptors learned are unique and the representations determined by the network are global. We further show the applicability of the method to real-world videos.",0
"The field of computer vision has extensively researched correspondence estimation, which has various practical applications such as object recognition, mapping, and tracking. In this article, we propose a new method to estimate dense correspondence in an RGB image using a fully convolutional network to learn visual descriptors from video examples. Unlike other deep learning approaches that rely on expensive labeled data or RGB-D videos, our method uses contrastive loss and optical flow to estimate relative labeling from RGB videos. We demonstrate the effectiveness of our method through quantitative analysis on rendered videos with known ground truth, showing that it performs well in both familiar and unfamiliar backgrounds. The learned descriptors are unique and have global representations. Additionally, we validate the method's usefulness in real-world videos.",1
"In classic video action recognition, labels may not contain enough information about the diverse video appearance and dynamics, thus, existing models that are trained under the standard supervised learning paradigm may extract less generalizable features. We evaluate these models under a cross-dataset experiment setting, as the above label bias problem in video analysis is even more prominent across different data sources. We find that using the optical flows as model inputs harms the generalization ability of most video recognition models.   Based on these findings, we present a multi-task learning paradigm for video classification. Our key idea is to avoid label bias and improve the generalization ability by taking data as its own supervision or supervising constraints on the data. First, we take the optical flows and the RGB frames by taking them as auxiliary supervisions, and thus naming our model as Reversed Two-Stream Networks (Rev2Net). Further, we collaborate the auxiliary flow prediction task and the frame reconstruction task by introducing a new training objective to Rev2Net, named Decoding Discrepancy Penalty (DDP), which constraints the discrepancy of the multi-task features in a self-supervised manner. Rev2Net is shown to be effective on the classic action recognition task. It specifically shows a strong generalization ability in the cross-dataset experiments.",0
"The conventional approach to recognizing action in videos may be insufficient in providing the necessary information about the varied appearance and movements of videos. This leads to models trained through supervised learning being less effective at extracting features that are generalizable. To test these models, we conducted experiments across different datasets. We discovered that utilizing optical flows as model inputs can hinder the models' ability to generalize. To address this, we propose a multi-task learning technique for video classification that avoids label bias and improves generalization by supervising the data itself. We use the optical flows and RGB frames as auxiliary supervisions and name our model Rev2Net. We also introduce a new training objective, DDP, which collaborates the auxiliary flow prediction task and the frame reconstruction task, and constrains the discrepancy of multi-task features in a self-supervised manner. Rev2Net proves effective in action recognition and shows strong generalization ability in cross-dataset experiments.",1
"Feature warping is a core technique in optical flow estimation; however, the ambiguity caused by occluded areas during warping is a major problem that remains unsolved. In this paper, we propose an asymmetric occlusion-aware feature matching module, which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The proposed module can be easily integrated into end-to-end network architectures and enjoys performance gains while introducing negligible computational cost. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids with which we achieve state-of-the-art performance. At the time of submission, our method, called MaskFlownet, surpasses all published optical flow methods on the MPI Sintel, KITTI 2012 and 2015 benchmarks. Code is available at https://github.com/microsoft/MaskFlownet.",0
"Optical flow estimation relies heavily on feature warping, but occluded areas create ambiguity that poses a significant challenge. This paper introduces an innovative solution - an occlusion-aware feature matching module that can identify and filter out useless (occluded) areas. The module can be easily integrated into end-to-end network architectures and offers substantial performance gains with minimal computational cost. The learned occlusion mask can be further used in a subsequent network cascade with dual feature pyramids, resulting in state-of-the-art performance. This approach, named MaskFlownet, currently outperforms all published optical flow methods on the MPI Sintel, KITTI 2012 and 2015 benchmarks. The code is available at https://github.com/microsoft/MaskFlownet.",1
"Akin to many subareas of computer vision, the recent advances in deep learning have also significantly influenced the literature on optical flow. Previously, the literature had been dominated by classical energy-based models, which formulate optical flow estimation as an energy minimization problem. However, as the practical benefits of Convolutional Neural Networks (CNNs) over conventional methods have become apparent in numerous areas of computer vision and beyond, they have also seen increased adoption in the context of motion estimation to the point where the current state of the art in terms of accuracy is set by CNN approaches. We first review this transition as well as the developments from early work to the current state of CNNs for optical flow estimation. Alongside, we discuss some of their technical details and compare them to recapitulate which technical contribution led to the most significant accuracy improvements. Then we provide an overview of the various optical flow approaches introduced in the deep learning age, including those based on alternative learning paradigms (e.g., unsupervised and semi-supervised methods) as well as the extension to the multi-frame case, which is able to yield further accuracy improvements.",0
"The recent advancements in deep learning have greatly impacted the literature on optical flow, much like other subareas of computer vision. Previously, classical energy-based models dominated the literature on optical flow estimation, which posed the problem as an energy minimization task. However, with the practical benefits of Convolutional Neural Networks (CNNs) becoming evident in various areas of computer vision, they have also garnered increased adoption in the field of motion estimation, surpassing conventional methods and setting the current standard for accuracy. In this article, we examine the evolution of optical flow estimation from early work to CNN approaches, highlighting their technical details and comparing them to identify the most significant accuracy improvements. We also provide an overview of the deep learning-based optical flow approaches, including those based on alternative learning paradigms (such as unsupervised and semi-supervised methods) and the extension to the multi-frame case, which further enhances accuracy.",1
"We present a simple and effective deep convolutional neural network (CNN) model for video deblurring. The proposed algorithm mainly consists of optical flow estimation from intermediate latent frames and latent frame restoration steps. It first develops a deep CNN model to estimate optical flow from intermediate latent frames and then restores the latent frames based on the estimated optical flow. To better explore the temporal information from videos, we develop a temporal sharpness prior to constrain the deep CNN model to help the latent frame restoration. We develop an effective cascaded training approach and jointly train the proposed CNN model in an end-to-end manner. We show that exploring the domain knowledge of video deblurring is able to make the deep CNN model more compact and efficient. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods on the benchmark datasets as well as real-world videos.",0
"Our study introduces a deep convolutional neural network (CNN) model for video deblurring that is both simple and effective. The algorithm primarily involves optical flow estimation from intermediate latent frames and restoration of the latent frames. To achieve this, we first establish a deep CNN model that estimates the optical flow from intermediate latent frames, followed by restoring the latent frames based on the estimated optical flow. To enhance the exploration of temporal information from videos, we create a temporal sharpness prior that constrains the deep CNN model to assist in the restoration of the latent frame. We also develop a cascaded training approach to jointly train the CNN model in an end-to-end manner. Our results demonstrate that incorporating domain knowledge of video deblurring enhances the efficiency and compactness of the deep CNN model. Extensive experiments reveal that our proposed algorithm outperforms state-of-the-art methods on both benchmark datasets and real-world videos.",1
"In this paper, we propose a unified method to jointly learn optical flow and stereo matching. Our first intuition is stereo matching can be modeled as a special case of optical flow, and we can leverage 3D geometry behind stereoscopic videos to guide the learning of these two forms of correspondences. We then enroll this knowledge into the state-of-the-art self-supervised learning framework, and train one single network to estimate both flow and stereo. Second, we unveil the bottlenecks in prior self-supervised learning approaches, and propose to create a new set of challenging proxy tasks to boost performance. These two insights yield a single model that achieves the highest accuracy among all existing unsupervised flow and stereo methods on KITTI 2012 and 2015 benchmarks. More remarkably, our self-supervised method even outperforms several state-of-the-art fully supervised methods, including PWC-Net and FlowNet2 on KITTI 2012.",0
"The aim of this paper is to present a method that can simultaneously learn optical flow and stereo matching. Our initial idea is that stereo matching can be seen as a specific example of optical flow. By utilizing the 3D geometry present in stereoscopic videos, we can guide the learning process for both forms of correspondences. We integrate this knowledge into a self-supervised learning framework, training a single network to estimate both flow and stereo. Additionally, we identify shortcomings in previous self-supervised methods and propose a new set of challenging tasks to improve performance. These two insights combine to create a single model that achieves the highest accuracy among all existing unsupervised flow and stereo methods on KITTI 2012 and 2015 benchmarks. Notably, our self-supervised approach even surpasses several state-of-the-art fully supervised methods, such as PWC-Net and FlowNet2 on KITTI 2012.",1
"In dense foggy scenes, existing optical flow methods are erroneous. This is due to the degradation caused by dense fog particles that break the optical flow basic assumptions such as brightness and gradient constancy. To address the problem, we introduce a semi-supervised deep learning technique that employs real fog images without optical flow ground-truths in the training process. Our network integrates the domain transformation and optical flow networks in one framework. Initially, given a pair of synthetic fog images, its corresponding clean images and optical flow ground-truths, in one training batch we train our network in a supervised manner. Subsequently, given a pair of real fog images and a pair of clean images that are not corresponding to each other (unpaired), in the next training batch, we train our network in an unsupervised manner. We then alternate the training of synthetic and real data iteratively. We use real data without ground-truths, since to have ground-truths in such conditions is intractable, and also to avoid the overfitting problem of synthetic data training, where the knowledge learned on synthetic data cannot be generalized to real data testing. Together with the network architecture design, we propose a new training strategy that combines supervised synthetic-data training and unsupervised real-data training. Experimental results show that our method is effective and outperforms the state-of-the-art methods in estimating optical flow in dense foggy scenes.",0
"Optical flow methods are inaccurate in dense fog due to the degradation caused by fog particles breaking basic assumptions such as brightness and gradient constancy. To overcome this issue, we introduce a semi-supervised deep learning technique that utilizes real fog images without optical flow ground-truths for training. Our network integrates domain transformation and optical flow networks in one framework. We begin training in a supervised manner with a batch of synthetic fog images and their corresponding clean images and optical flow ground-truths. In the next training batch, we train our network in an unsupervised manner with unpaired real fog images and clean images. We alternate training with synthetic and real data iteratively, using real data without ground-truths to avoid overfitting and intractable ground-truth conditions. Our proposed training strategy combines supervised synthetic-data training and unsupervised real-data training. Experimental results demonstrate the effectiveness of our method, which outperforms state-of-the-art methods in estimating optical flow in dense foggy scenes.",1
"The widespread adoption of deep learning models places demands on their robustness. In this paper, we consider the robustness of deep neural networks on videos, which comprise both the spatial features of individual frames extracted by a convolutional neural network and the temporal dynamics between adjacent frames captured by a recurrent neural network. To measure robustness, we study the maximum safe radius problem, which computes the minimum distance from the optical flow sequence obtained from a given input to that of an adversarial example in the neighbourhood of the input. We demonstrate that, under the assumption of Lipschitz continuity, the problem can be approximated using finite optimisation via discretising the optical flow space, and the approximation has provable guarantees. We then show that the finite optimisation problem can be solved by utilising a two-player turn-based game in a cooperative setting, where the first player selects the optical flows and the second player determines the dimensions to be manipulated in the chosen flow. We employ an anytime approach to solve the game, in the sense of approximating the value of the game by monotonically improving its upper and lower bounds. We exploit a gradient-based search algorithm to compute the upper bounds, and the admissible A* algorithm to update the lower bounds. Finally, we evaluate our framework on the UCF101 video dataset.",0
"The robustness of deep learning models is essential for their widespread adoption. This paper examines the robustness of deep neural networks on videos, which involves both the spatial features of individual frames and the temporal dynamics between adjacent frames. The maximum safe radius problem is studied to measure the robustness, which calculates the minimum distance between the optical flow sequence of a given input and that of an adversarial example in the input's neighbourhood. The paper demonstrates that the problem can be approximated using finite optimization by discretizing the optical flow space under the assumption of Lipschitz continuity, and the approximation has provable guarantees. A cooperative two-player turn-based game is used to solve the finite optimization problem, where the first player selects the optical flows and the second player determines the dimensions to be manipulated in the chosen flow. An anytime approach is employed to solve the game, approximating the value of the game by monotonically improving its upper and lower bounds. Gradient-based search algorithms and the admissible A* algorithm are used to compute the upper and lower bounds, respectively. Finally, the framework is evaluated on the UCF101 video dataset.",1
"In this paper, we propose Two-Stream AMTnet, which leverages recent advances in video-based action representation[1] and incremental action tube generation[2]. Majority of the present action detectors follow a frame-based representation, a late-fusion followed by an offline action tube building steps. These are sub-optimal as: frame-based features barely encode the temporal relations; late-fusion restricts the network to learn robust spatiotemporal features; and finally, an offline action tube generation is not suitable for many real-world problems such as autonomous driving, human-robot interaction to name a few. The key contributions of this work are: (1) combining AMTnet's 3D proposal architecture with an online action tube generation technique which allows the model to learn stronger temporal features needed for accurate action detection and facilitates running inference online; (2) an efficient fusion technique allowing the deep network to learn strong spatiotemporal action representations. This is achieved by augmenting the previous Action Micro-Tube (AMTnet) action detection framework in three distinct ways: by adding a parallel motion stIn this paper, we propose a new deep neural network architecture for online action detection, termed ream to the original appearance one in AMTnet; (2) in opposition to state-of-the-art action detectors which train appearance and motion streams separately, and use a test time late fusion scheme to fuse RGB and flow cues, by jointly training both streams in an end-to-end fashion and merging RGB and optical flow features at training time; (3) by introducing an online action tube generation algorithm which works at video-level, and in real-time (when exploiting only appearance features). Two-Stream AMTnet exhibits superior action detection performance over state-of-the-art approaches on the standard action detection benchmarks.",0
"The article introduces Two-Stream AMTnet as a new deep neural network architecture for online action detection. This model incorporates recent advancements in video-based action representation and incremental action tube generation. Existing action detectors typically use a frame-based representation, which limits their ability to encode temporal relations. They also rely on offline action tube building, which is not practical in real-world scenarios such as autonomous driving or human-robot interaction. Two-Stream AMTnet addresses these limitations by combining 3D proposal architecture with online action tube generation and an efficient fusion technique. The network learns strong spatiotemporal action representations by joint training and merging RGB and optical flow features. Additionally, the model uses an online action tube generation algorithm that works at the video-level and in real-time. The results demonstrate that Two-Stream AMTnet outperforms state-of-the-art approaches on standard action detection benchmarks.",1
"We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera. Our method leverages the motion differences between the background and the obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. The learning-based layer reconstruction allows us to accommodate potential errors in the flow estimation and brittle assumptions such as brightness consistency. We show that training on synthetically generated data transfers well to real images. Our results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.",0
"A technique that employs learning has been introduced to eliminate undesired obstructions, like reflections from windows, raindrops, or occlusions by fences, in a brief series of images taken by a moving camera. Our approach utilizes the differences in movement between the background and obstructing components to restore both layers. We achieve this by alternately estimating dense optical flow fields for the two layers and reconstructing each layer from the flow-warped images using a deep convolutional neural network. The learning-based layer reconstruction enables us to account for errors in the flow estimation and fragile assumptions, such as brightness consistency. We have shown that training on artificially produced data can be applied to genuine images effectively. Our method's effectiveness is demonstrated by the results obtained from numerous challenging scenarios of reflection and fence removal.",1
"Event cameras are bio-inspired sensors that asynchronously report intensity changes in microsecond resolution. DAVIS can capture high dynamics of a scene and simultaneously output high temporal resolution events and low frame-rate intensity images. In this paper, we propose a single image (potentially blurred) and events based optical flow estimation approach. First, we demonstrate how events can be used to improve flow estimates. To this end, we encode the relation between flow and events effectively by presenting an event-based photometric consistency formulation. Then, we consider the special case of image blur caused by high dynamics in the visual environments and show that including the blur formation in our model further constrains flow estimation. This is in sharp contrast to existing works that ignore the blurred images while our formulation can naturally handle either blurred or sharp images to achieve accurate flow estimation. Finally, we reduce flow estimation, as well as image deblurring, to an alternative optimization problem of an objective function using the primal-dual algorithm. Experimental results on both synthetic and real data (with blurred and non-blurred images) show the superiority of our model in comparison to state-of-the-art approaches.",0
"The sensors inspired by biology, known as event cameras, report changes in intensity in microseconds. DAVIS, one such sensor, can capture a scene's high dynamics and output high temporal resolution events and low frame-rate intensity images. This paper presents an approach for optical flow estimation based on a single image and events, with the potential for the image to be blurred. The paper first demonstrates how events can enhance flow estimates by presenting an event-based photometric consistency formulation. The paper then considers the case of blurred images caused by high dynamics in the visual environments and shows that including the blur formation in the model further improves flow estimation. The proposed formulation can handle both blurred and sharp images, unlike existing works that only handle sharp images. Finally, the paper reduces flow estimation and image deblurring to an alternative optimization problem of an objective function using the primal-dual algorithm. Experimental results on both synthetic and real data, including blurred and non-blurred images, demonstrate the superiority of the proposed model compared to state-of-the-art approaches.",1
"Detecting and segmenting individual objects, regardless of their category, is crucial for many applications such as action detection or robotic interaction. While this problem has been well-studied under the classic formulation of spatio-temporal grouping, state-of-the-art approaches do not make use of learning-based methods. To bridge this gap, we propose a simple learning-based approach for spatio-temporal grouping. Our approach leverages motion cues from optical flow as a bottom-up signal for separating objects from each other. Motion cues are then combined with appearance cues that provide a generic objectness prior for capturing the full extent of objects. We show that our approach outperforms all prior work on the benchmark FBMS dataset. One potential worry with learning-based methods is that they might overfit to the particular type of objects that they have been trained on. To address this concern, we propose two new benchmarks for generic, moving object detection, and show that our model matches top-down methods on common categories, while significantly out-performing both top-down and bottom-up methods on never-before-seen categories.",0
"The ability to identify and isolate individual objects, regardless of their category, is crucial in various fields, including action detection and robotic interaction. While the issue has been thoroughly investigated using the traditional spatio-temporal grouping approach, current state-of-the-art methods do not utilize learning-based techniques. To address this gap, we suggest a straightforward learning-based approach for spatio-temporal grouping. Our approach employs motion cues from optical flow as a bottom-up signal to separate objects from each other. We then combine motion cues with appearance cues to capture the entire scope of objects, providing a generic objectness prior. Our approach performs better than all prior work on the FBMS dataset. One possible concern with learning-based techniques is that they may overfit to specific object types. To address this issue, we introduce two new benchmarks for detecting generic, moving objects. Our model matches top-down methods on common categories and outperforms both top-down and bottom-up methods on never-before-seen categories.",1
"Encoder-decoder networks have found widespread use in various dense prediction tasks. However, the strong reduction of spatial resolution in the encoder leads to a loss of location information as well as boundary artifacts. To address this, image-adaptive post-processing methods have shown beneficial by leveraging the high-resolution input image(s) as guidance data. We extend such approaches by considering an important orthogonal source of information: the network's confidence in its own predictions. We introduce probabilistic pixel-adaptive convolutions (PPACs), which not only depend on image guidance data for filtering, but also respect the reliability of per-pixel predictions. As such, PPACs allow for image-adaptive smoothing and simultaneously propagating pixels of high confidence into less reliable regions, while respecting object boundaries. We demonstrate their utility in refinement networks for optical flow and semantic segmentation, where PPACs lead to a clear reduction in boundary artifacts. Moreover, our proposed refinement step is able to substantially improve the accuracy on various widely used benchmarks.",0
"Various dense prediction tasks have widely adopted encoder-decoder networks. However, the encoder's strong reduction of spatial resolution results in a loss of location information and boundary artifacts. To combat this, image-adaptive post-processing methods have been successful in utilizing high-resolution input images as guidance data. Our approach extends this by taking into account the network's confidence in its predictions. We introduce probabilistic pixel-adaptive convolutions (PPACs), which utilize both image guidance data and per-pixel prediction reliability to enable image-adaptive smoothing while preserving object boundaries. PPACs are effective in refinement networks for optical flow and semantic segmentation, reducing boundary artifacts and improving accuracy on commonly used benchmarks.",1
"Whole understanding of the surroundings is paramount to autonomous systems. Recent works have shown that deep neural networks can learn geometry (depth) and motion (optical flow) from a monocular video without any explicit supervision from ground truth annotations, particularly hard to source for these two tasks. In this paper, we take an additional step toward holistic scene understanding with monocular cameras by learning depth and motion alongside with semantics, with supervision for the latter provided by a pre-trained network distilling proxy ground truth images. We address the three tasks jointly by a) a novel training protocol based on knowledge distillation and self-supervision and b) a compact network architecture which enables efficient scene understanding on both power hungry GPUs and low-power embedded platforms. We thoroughly assess the performance of our framework and show that it yields state-of-the-art results for monocular depth estimation, optical flow and motion segmentation.",0
"The comprehensive comprehension of the environment is crucial for independent systems. Recent research has revealed that deep neural networks can acquire knowledge of geometric properties (depth) and motion (optical flow) from a single video without any explicit instruction from ground truth annotations, which are particularly difficult to obtain for these two tasks. In this article, we have taken a further step towards a holistic understanding of the scene captured by monocular cameras by teaching the network depth, motion, and semantics all at once, with supervision for the latter provided by a pre-trained network distilling proxy ground truth images. We have accomplished this through a) a new training method based on knowledge distillation and self-supervision, and b) a compact network architecture that allows for efficient scene comprehension on both power-hungry GPUs and low-power embedded platforms. We have thoroughly evaluated the performance of our framework and demonstrated that it produces the best outcomes for monocular depth estimation, optical flow, and motion segmentation.",1
"In this work we contribute a novel pipeline to automatically generate training data, and to improve over state-of-the-art multi-object tracking and segmentation (MOTS) methods. Our proposed track mining algorithm turns raw street-level videos into high-fidelity MOTS training data, is scalable and overcomes the need of expensive and time-consuming manual annotation approaches. We leverage state-of-the-art instance segmentation results in combination with optical flow predictions, also trained on automatically harvested training data. Our second major contribution is MOTSNet - a deep learning, tracking-by-detection architecture for MOTS - deploying a novel mask-pooling layer for improved object association over time. Training MOTSNet with our automatically extracted data leads to significantly improved sMOTSA scores on the novel KITTI MOTS dataset (+1.9%/+7.5% on cars/pedestrians), and MOTSNet improves by +4.1% over previously best methods on the MOTSChallenge dataset. Our most impressive finding is that we can improve over previous best-performing works, even in complete absence of manually annotated MOTS training data.",0
"Our work presents a new pipeline that can generate training data automatically and enhance state-of-the-art multi-object tracking and segmentation (MOTS) approaches. Our innovative track mining algorithm transforms raw street-level videos into high-quality MOTS training data that is scalable and eliminates the need for costly and time-consuming manual annotation techniques. We utilize cutting-edge instance segmentation outcomes coupled with optical flow predictions, which are also acquired from automatically obtained training data. Our second significant contribution is MOTSNet - a deep learning, tracking-by-detection framework for MOTS that employs an original mask-pooling layer to enhance object association over time. Our use of our automatically extracted data to train MOTSNet leads to a substantial increase in sMOTSA scores on the new KITTI MOTS dataset. (+1.9%/+7.5% on cars/pedestrians). Moreover, MOTSNet surpasses previous best methods on the MOTSChallenge dataset by +4.1%. The most impressive outcome of our research is that we can improve upon previous best-performing works, even in the absence of manually annotated MOTS training data.",1
"High-quality 3D reconstructions from endoscopy video play an important role in many clinical applications, including surgical navigation where they enable direct video-CT registration. While many methods exist for general multi-view 3D reconstruction, these methods often fail to deliver satisfactory performance on endoscopic video. Part of the reason is that local descriptors that establish pair-wise point correspondences, and thus drive reconstruction, struggle when confronted with the texture-scarce surface of anatomy. Learning-based dense descriptors usually have larger receptive fields enabling the encoding of global information, which can be used to disambiguate matches. In this work, we present an effective self-supervised training scheme and novel loss design for dense descriptor learning. In direct comparison to recent local and dense descriptors on an in-house sinus endoscopy dataset, we demonstrate that our proposed dense descriptor can generalize to unseen patients and scopes, thereby largely improving the performance of Structure from Motion (SfM) in terms of model density and completeness. We also evaluate our method on a public dense optical flow dataset and a small-scale SfM public dataset to further demonstrate the effectiveness and generality of our method. The source code is available at https://github.com/lppllppl920/DenseDescriptorLearning-Pytorch.",0
"The creation of high-quality 3D reconstructions through endoscopy video is crucial in various clinical applications, such as surgical navigation. However, existing methods for general multi-view 3D reconstruction are insufficient in producing satisfactory results for endoscopic video. This is mainly due to the difficulty local descriptors face in establishing point correspondences because of the anatomy's texture-scarce surface. Therefore, our work proposes a self-supervised training scheme and loss design for dense descriptor learning, which can encode global information to disambiguate matches. Our proposed dense descriptor outperforms recent local and dense descriptors in terms of Structure from Motion (SfM) model density and completeness on an in-house sinus endoscopy dataset. Moreover, our method can generalize to unseen patients and scopes. We also evaluate the effectiveness and generality of our method on a public dense optical flow dataset and a small-scale SfM public dataset. Our source code is available at https://github.com/lppllppl920/DenseDescriptorLearning-Pytorch.",1
"Particle Imaging Velocimetry (PIV) estimates the flow of fluid by analyzing the motion of injected particles. The problem is challenging as the particles lie at different depths but have similar appearance and tracking a large number of particles is particularly difficult. In this paper, we present a PIV solution that uses densely sampled light field to reconstruct and track 3D particles. We exploit the refocusing capability and focal symmetry constraint of the light field for reliable particle depth estimation. We further propose a new motion-constrained optical flow estimation scheme by enforcing local motion rigidity and the Navier-Stoke constraint. Comprehensive experiments on synthetic and real experiments show that using a single light field camera, our technique can recover dense and accurate 3D fluid flows in small to medium volumes.",0
"The estimation of fluid flow through Particle Imaging Velocimetry (PIV) is accomplished by analyzing the movement of injected particles. However, this is a difficult task as the particles are located at various depths and have similar appearances, making tracking a large number of particles particularly challenging. This paper introduces a PIV solution that employs a densely sampled light field to reconstruct and track 3D particles. By leveraging the refocusing ability and focal symmetry limitation of the light field, we ensure reliable estimation of particle depth. We also propose a new motion-constrained optical flow estimation method by enforcing both local motion rigidity and the Navier-Stoke constraint. Comprehensive testing on both synthetic and actual data demonstrates that our technique, utilizing a single light field camera, can accurately recover dense 3D fluid flows in small and medium volumes.",1
"Deep neural networks have been successfully applied to solving the video-based person re-identification problem with impressive results reported. The existing networks for person re-id are designed to extract discriminative features that preserve the identity information. Usually, whole video frames are fed into the neural networks and all the regions in a frame are equally treated. This may be a suboptimal choice because many regions, e.g., background regions in the video, are not related to the person. Furthermore, the person of interest may be occluded by another person or something else. These unrelated regions may hinder person re-identification. In this paper, we introduce a novel gating mechanism to deep neural networks. Our gating mechanism will learn which regions are helpful for person re-identification and let these regions pass the gate. The unrelated background regions or occluding regions are filtered out by the gate. In each frame, the color channels and optical flow channels provide quite different information. To better leverage such information, we generate one gate using the color channels and another gate using the optical flow channels. These two gates are combined to provide a more reliable gate with a novel fusion method. Experimental results on two major datasets demonstrate the performance improvements due to the proposed gating mechanism.",0
"Impressive results have been achieved through the successful application of deep neural networks to solve the video-based person re-identification problem. Currently, person re-identification networks are designed to extract identity-preserving features from whole video frames, treating all regions equally. However, this approach may not be optimal as many regions, such as background regions or occluded areas, are irrelevant to the person of interest and may impede successful re-identification. To solve this issue, our paper proposes a new gating mechanism for deep neural networks. This mechanism learns which regions are useful for person re-identification and allows only those regions to pass through the gate. We create two separate gates, one for color channels and one for optical flow channels, to better leverage the unique information provided by each channel. Our experimental results demonstrate the effectiveness of our proposed gating mechanism on two major datasets.",1
"This paper presents baseline results for the Third Facial Micro-Expression Grand Challenge (MEGC 2020). Both macro- and micro-expression intervals in CAS(ME)$^2$ and SAMM Long Videos are spotted by employing the method of Main Directional Maximal Difference Analysis (MDMD). The MDMD method uses the magnitude maximal difference in the main direction of optical flow features to spot facial movements. The single-frame prediction results of the original MDMD method are post-processed into reasonable video intervals. The metric F1-scores of baseline results are evaluated: for CAS(ME)$^2$, the F1-scores are 0.1196 and 0.0082 for macro- and micro-expressions respectively, and the overall F1-score is 0.0376; for SAMM Long Videos, the F1-scores are 0.0629 and 0.0364 for macro- and micro-expressions respectively, and the overall F1-score is 0.0445. The baseline project codes are publicly available at https://github.com/HeyingGithub/Baseline-project-for-MEGC2020_spotting.",0
"In this article, the baseline outcomes for the Third Facial Micro-Expression Grand Challenge (MEGC 2020) are presented. The Main Directional Maximal Difference Analysis (MDMD) approach is employed to detect both macro- and micro-expression intervals in CAS(ME)$^2$ and SAMM Long Videos. This method identifies facial movements by utilizing the maximal difference in the magnitude of optical flow features in the main direction. The video intervals are generated by post-processing the single-frame prediction results of the original MDMD method. The baseline results are evaluated using F1-scores, which are 0.1196 and 0.0082 for macro- and micro-expressions, respectively, with an overall F1-score of 0.0376 for CAS(ME)$^2$. For SAMM Long Videos, the F1-scores are 0.0629 and 0.0364 for macro- and micro-expressions, respectively, with an overall F1-score of 0.0445. The baseline project codes are available to the public at https://github.com/HeyingGithub/Baseline-project-for-MEGC2020_spotting.",1
"Hand hygiene is one of the most significant factors in preventing hospital acquired infections (HAI) which often be transmitted by medical staffs in contact with patients in the operating room (OR). Hand hygiene monitoring could be important to investigate and reduce the outbreak of infections within the OR. However, an effective monitoring tool for hand hygiene compliance is difficult to develop due to the visual complexity of the OR scene. Recent progress in video understanding with convolutional neural net (CNN) has increased the application of recognition and detection of human actions. Leveraging this progress, we proposed a fully automated hand hygiene monitoring tool of the alcohol-based hand rubbing action of anesthesiologists on OR video using spatio-temporal features with 3D CNN. First, the region of interest (ROI) of anesthesiologists' upper body were detected and cropped. A temporal smoothing filter was applied to the ROIs. Then, the ROIs were given to a 3D CNN and classified into two classes: rubbing hands or other actions. We observed that a transfer learning from Kinetics-400 is beneficial and the optical flow stream was not helpful in our dataset. The final accuracy, precision, recall and F1 score in testing is 0.76, 0.85, 0.65 and 0.74, respectively.",0
"Preventing hospital acquired infections (HAI) is crucial, and one of the most effective ways to achieve this is through hand hygiene. Unfortunately, medical staff in the operating room (OR) may unintentionally transmit infections to patients, making it even more important to monitor hand hygiene in this setting. However, developing a monitoring tool that can effectively capture compliance with hand hygiene protocols is challenging, primarily due to the visual complexity of the OR. Fortunately, recent advancements in video understanding using convolutional neural nets (CNN) have increased the potential for recognizing and detecting human actions. Building on this progress, we have proposed a fully automated tool for monitoring the alcohol-based hand rubbing action of anesthesiologists in OR videos. We achieved this by utilizing spatio-temporal features with 3D CNN, which enabled us to detect and crop the region of interest (ROI) of the anesthesiologists' upper body. We then applied a temporal smoothing filter to the ROIs, before classifying them into two categories: rubbing hands or other actions. Our findings suggest that transfer learning from Kinetics-400 was helpful, while the optical flow stream was not beneficial for our dataset. Finally, our testing results show that our tool achieved an accuracy, precision, recall and F1 score of 0.76, 0.85, 0.65 and 0.74, respectively.",1
"Fine-grained action recognition datasets exhibit environmental bias, where multiple video sequences are captured from a limited number of environments. Training a model in one environment and deploying in another results in a drop in performance due to an unavoidable domain shift. Unsupervised Domain Adaptation (UDA) approaches have frequently utilised adversarial training between the source and target domains. However, these approaches have not explored the multi-modal nature of video within each domain. In this work we exploit the correspondence of modalities as a self-supervised alignment approach for UDA in addition to adversarial alignment.   We test our approach on three kitchens from our large-scale dataset, EPIC-Kitchens, using two modalities commonly employed for action recognition: RGB and Optical Flow. We show that multi-modal self-supervision alone improves the performance over source-only training by 2.4% on average. We then combine adversarial training with multi-modal self-supervision, showing that our approach outperforms other UDA methods by 3%.",0
"Action recognition datasets with fine-grained details tend to have environmental bias, as they are limited to a few environments where multiple video sequences are captured. This results in reduced performance when a model is trained in one environment and deployed in another due to domain shift. Adversarial training has been commonly used in Unsupervised Domain Adaptation (UDA) approaches, but they do not consider the multi-modal nature of video within each domain. This study proposes a self-supervised alignment approach for UDA that exploits the correspondence of modalities, in addition to adversarial alignment. The approach was tested on three kitchens from the EPIC-Kitchens dataset using RGB and Optical Flow modalities for action recognition. Results showed that multi-modal self-supervision alone improved performance over source-only training by an average of 2.4%. Combining adversarial training with multi-modal self-supervision resulted in an approach that outperformed other UDA methods by 3%.",1
"Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2, the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the running speed. LiteFlowNet2 is built on the foundation laid by conventional methods and resembles the corresponding roles as data fidelity and regularization in variational methods. We compute optical flow in a spatial-pyramid formulation as SPyNet but through a novel lightweight cascaded flow inference. It provides high flow estimation accuracy through early correction with seamless incorporation of descriptor matching. Flow regularization is used to ameliorate the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2 and SPyNet. Comparing to LiteFlowNet, LiteFlowNet2 improves the optical flow accuracy on Sintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI 2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained models are made publicly available on https://github.com/twhui/LiteFlowNet2.",0
"For more than forty years, the majority of studies have focused on using variational methods to address the issue of optical flow estimation. Recently, some researchers have attempted to tackle this problem using convolutional neural network (CNN) and have demonstrated encouraging results. FlowNet2 is currently the most advanced CNN and is composed of over 160M parameters to achieve precise flow estimation. Our LiteFlowNet2 surpasses FlowNet2 in terms of performance on Sintel and KITTI benchmarks, while also being 25.3 times smaller in model size and 3.1 times faster in running speed. Our approach builds on conventional methods and plays a similar role to data fidelity and regularization in variational methods. We compute optical flow using a spatial-pyramid formulation similar to SPyNet, but with a novel lightweight cascaded flow inference approach. Our network provides high flow estimation accuracy through early correction and seamless incorporation of descriptor matching. Flow regularization is used to address the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also possesses an effective structure for pyramidal feature extraction and embraces feature warping instead of image warping as utilized in FlowNet2 and SPyNet. Compared to LiteFlowNet, LiteFlowNet2 improves optical flow accuracy on Sintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI 2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained models are publicly available on https://github.com/twhui/LiteFlowNet2.",1
"Pose tracking is an important problem that requires identifying unique human pose-instances and matching them temporally across different frames of a video. However, existing pose tracking methods are unable to accurately model temporal relationships and require significant computation, often computing the tracks offline. We present an efficient Multi-person Pose Tracking method, KeyTrack, that only relies on keypoint information without using any RGB or optical flow information to track human keypoints in real-time. Keypoints are tracked using our Pose Entailment method, in which, first, a pair of pose estimates is sampled from different frames in a video and tokenized. Then, a Transformer-based network makes a binary classification as to whether one pose temporally follows another. Furthermore, we improve our top-down pose estimation method with a novel, parameter-free, keypoint refinement technique that improves the keypoint estimates used during the Pose Entailment step. We achieve state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks while using only a fraction of the computation required by most other methods for computing the tracking information.",0
"Identifying unique human poses and matching them across different frames of a video is a crucial task known as pose tracking. Although existing methods for pose tracking are available, they have limited accuracy in modeling temporal relationships and require substantial computation, often computed offline. In this study, we introduce KeyTrack, an efficient Multi-person Pose Tracking method that only uses keypoint information without relying on RGB or optical flow information to track human keypoints in real-time. Our approach employs the Pose Entailment method to track keypoints, where a pair of pose estimates is tokenized from different frames in a video and subjected to binary classification by a Transformer-based network to determine if one pose follows another. We also enhance our top-down pose estimation method with a novel keypoint refinement technique that improves keypoint estimates used in the Pose Entailment step. Our approach outperforms most other methods in computing tracking information while using only a fraction of the required computation, as demonstrated by state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks.",1
"It has been proposed by many researchers that combining deep neural networks with graphical models can create more efficient and better regularized composite models. The main difficulties in implementing this in practice are associated with a discrepancy in suitable learning objectives as well as with the necessity of approximations for the inference. In this work we take one of the simplest inference methods, a truncated max-product Belief Propagation, and add what is necessary to make it a proper component of a deep learning model: We connect it to learning formulations with losses on marginals and compute the backprop operation. This BP-Layer can be used as the final or an intermediate block in convolutional neural networks (CNNs), allowing us to design a hierarchical model composing BP inference and CNNs at different scale levels. The model is applicable to a range of dense prediction problems, is well-trainable and provides parameter-efficient and robust solutions in stereo, optical flow and semantic segmentation.",0
"Many researchers suggest that deep neural networks combined with graphical models can create composite models that are more efficient and better regulated. However, implementing this idea poses challenges, such as a discrepancy in suitable learning objectives and the need for approximations in inference. In this study, we address these challenges by using a truncated max-product Belief Propagation as a learning component. By connecting it to learning formulations with losses on marginals and computing the backprop operation, we create a BP-Layer that can serve as the final or intermediate block in convolutional neural networks (CNNs). This hierarchical model is ideal for dense prediction problems and can provide parameter-efficient and robust solutions in stereo, optical flow, and semantic segmentation.",1
"Differentiable image sampling in the form of backward warping has seen broad adoption in tasks like depth estimation and optical flow prediction. In contrast, how to perform forward warping has seen less attention, partly due to additional challenges such as resolving the conflict of mapping multiple pixels to the same target location in a differentiable way. We propose softmax splatting to address this paradigm shift and show its effectiveness on the application of frame interpolation. Specifically, given two input frames, we forward-warp the frames and their feature pyramid representations based on an optical flow estimate using softmax splatting. In doing so, the softmax splatting seamlessly handles cases where multiple source pixels map to the same target location. We then use a synthesis network to predict the interpolation result from the warped representations. Our softmax splatting allows us to not only interpolate frames at an arbitrary time but also to fine tune the feature pyramid and the optical flow. We show that our synthesis approach, empowered by softmax splatting, achieves new state-of-the-art results for video frame interpolation.",0
"Backward warping, which involves differentiable image sampling, has been widely used in tasks like depth estimation and optical flow prediction. However, forward warping has received less attention, partly due to challenges in mapping multiple pixels to the same target location in a differentiable manner. To address this issue, we propose using softmax splatting and demonstrate its effectiveness in the context of frame interpolation. In our approach, we use optical flow to forward-warp two input frames and their feature pyramid representations, while ensuring that multiple source pixels are mapped to the same target location in a seamless manner. We then use a synthesis network to predict the interpolated result. Our approach is not only capable of interpolating frames at any time, but also fine-tuning the feature pyramid and optical flow. We show that our synthesis approach, powered by softmax splatting, achieves state-of-the-art results for video frame interpolation.",1
"In this paper, we proposed an unsupervised learning method for estimating the optical flow between video frames, especially to solve the occlusion problem. Occlusion is caused by the movement of an object or the movement of the camera, defined as when certain pixels are visible in one video frame but not in adjacent frames. Due to the lack of pixel correspondence between frames in the occluded area, incorrect photometric loss calculation can mislead the optical flow training process. In the video sequence, we found that the occlusion in the forward ($t\rightarrow t+1$) and backward ($t\rightarrow t-1$) frame pairs are usually complementary. That is, pixels that are occluded in subsequent frames are often not occluded in the previous frame and vice versa. Therefore, by using this complementarity, a new weighted loss is proposed to solve the occlusion problem. In addition, we calculate gradients in multiple directions to provide richer supervision information. Our method achieves competitive optical flow accuracy compared to the baseline and some supervised methods on KITTI 2012 and 2015 benchmarks. This source code has been released at https://github.com/jianfenglihg/UnOpticalFlow.git.",0
"A method for unsupervised learning was proposed in this paper to estimate optical flow between video frames with a focus on addressing the occlusion issue. This problem arises when certain pixels are not visible in adjacent frames due to object or camera movement, which leads to incorrect photometric loss calculation and misleading optical flow training process. The study discovered that occlusion in forward and backward frame pairs tends to be complementary, meaning pixels that are occluded in subsequent frames are often not occluded in the previous frame and vice versa. To tackle this, a new weighted loss was suggested, utilizing this complementarity. Moreover, gradients were calculated in multiple directions to provide more comprehensive supervision information. The method showed competitive optical flow accuracy compared to the baseline and some supervised methods on KITTI 2012 and 2015 benchmarks. The source code is available at https://github.com/jianfenglihg/UnOpticalFlow.git.",1
"Depth from a monocular video can enable billions of devices and robots with a single camera to see the world in 3D. In this paper, we present an approach with a differentiable flow-to-depth layer for video depth estimation. The model consists of a flow-to-depth layer, a camera pose refinement module, and a depth fusion network. Given optical flow and camera pose, our flow-to-depth layer generates depth proposals and the corresponding confidence maps by explicitly solving an epipolar geometry optimization problem. Our flow-to-depth layer is differentiable, and thus we can refine camera poses by maximizing the aggregated confidence in the camera pose refinement module. Our depth fusion network can utilize depth proposals and their confidence maps inferred from different adjacent frames to produce the final depth map. Furthermore, the depth fusion network can additionally take the depth proposals generated by other methods to improve the results further. The experiments on three public datasets show that our approach outperforms state-of-the-art depth estimation methods, and has reasonable cross dataset generalization capability: our model trained on KITTI still performs well on the unseen Waymo dataset.",0
"This paper proposes a method for estimating depth from monocular videos, which has the potential to allow devices and robots with only one camera to perceive the world in 3D. The proposed model consists of a flow-to-depth layer, a camera pose refinement module, and a depth fusion network. The flow-to-depth layer generates depth proposals and confidence maps by solving an epipolar geometry optimization problem. The camera pose refinement module refines the camera poses by maximizing the confidence scores, and the depth fusion network produces the final depth map by utilizing the depth proposals and their confidence maps from adjacent frames. The results of the experiments on three public datasets demonstrate that our approach outperforms the state-of-the-art depth estimation methods and has reasonable cross-dataset generalization capability. The model trained on KITTI also performs well on the unseen Waymo dataset.",1
"Applications of satellite data in areas such as weather tracking and modeling, ecosystem monitoring, wildfire detection, and land-cover change are heavily dependent on the trade-offs to spatial, spectral and temporal resolutions of observations. In weather tracking, high-frequency temporal observations are critical and used to improve forecasts, study severe events, and extract atmospheric motion, among others. However, while the current generation of geostationary satellites have hemispheric coverage at 10-15 minute intervals, higher temporal frequency observations are ideal for studying mesoscale severe weather events. In this work, we apply a task specific optical flow approach to temporal up-sampling using deep convolutional neural networks. We apply this technique to 16-bands of GOES-R/Advanced Baseline Imager mesoscale dataset to temporally enhance full disk hemispheric snapshots of different spatial resolutions from 15 minutes to 1 minute. Experiments show the effectiveness of task specific optical flow and multi-scale blocks for interpolating high-frequency severe weather events relative to bilinear and global optical flow baselines. Lastly, we demonstrate strong performance in capturing variability during a convective precipitation events.",0
"The utilization of satellite data in various fields, including weather tracking, ecosystem monitoring, wildfire detection, and land-cover change, relies heavily on the compromises made in terms of spatial, spectral, and temporal resolutions of observations. High-frequency temporal observations are crucial in weather tracking to enhance forecasting accuracy, study severe events, and extract atmospheric motion. However, geostationary satellites with hemispheric coverage at 10-15 minute intervals cannot provide higher temporal frequency observations, which are more suitable for studying mesoscale severe weather events. In this study, we applied a task-specific optical flow technique to temporally upscale 16-bands of GOES-R/Advanced Baseline Imager mesoscale dataset using deep convolutional neural networks. With this approach, we were able to enhance full disk hemispheric snapshots of different spatial resolutions from 15 minutes to 1 minute. Our experiments revealed the effectiveness of task-specific optical flow and multi-scale blocks in interpolating high-frequency severe weather events compared to bilinear and global optical flow baselines. Lastly, we demonstrated the strong performance of our approach in capturing variability during convective precipitation events.",1
"We address the problem of joint optical flow and camera motion estimation in rigid scenes by incorporating geometric constraints into an unsupervised deep learning framework. Unlike existing approaches which rely on brightness constancy and local smoothness for optical flow estimation, we exploit the global relationship between optical flow and camera motion using epipolar geometry. In particular, we formulate the prediction of optical flow and camera motion as a bi-level optimization problem, consisting of an upper-level problem to estimate the flow that conforms to the predicted camera motion, and a lower-level problem to estimate the camera motion given the predicted optical flow. We use implicit differentiation to enable back-propagation through the lower-level geometric optimization layer independent of its implementation, allowing end-to-end training of the network. With globally-enforced geometric constraints, we are able to improve the quality of the estimated optical flow in challenging scenarios and obtain better camera motion estimates compared to other unsupervised learning methods.",0
"Our objective is to tackle the issue of determining joint optical flow and camera motion in rigid scenes by integrating geometric constraints into an unsupervised deep learning framework. Unlike existing techniques that depend on brightness constancy and local smoothness for optical flow estimation, we utilize epipolar geometry to explore the global connection between optical flow and camera motion. Specifically, we formulate predicting optical flow and camera motion as a bi-level optimization problem, with an upper-level problem that estimates flow that conforms to predicted camera motion and a lower-level problem that estimates camera motion with predicted optical flow. We use implicit differentiation to allow back-propagation through the lower-level geometric optimization layer, regardless of its implementation, enabling end-to-end network training. By enforcing global geometric constraints, we enhance the quality of estimated optical flow in challenging situations and obtain superior camera motion estimates compared to other unsupervised learning methods.",1
"People identification in video based on the way they walk (i.e. gait) is a relevant task in computer vision using a non-invasive approach. Standard and current approaches typically derive gait signatures from sequences of binary energy maps of subjects extracted from images, but this process introduces a large amount of non-stationary noise, thus, conditioning their efficacy. In contrast, in this paper we focus on the raw pixels, or simple functions derived from them, letting advanced learning techniques to extract relevant features. Therefore, we present a comparative study of different Convolutional Neural Network (CNN) architectures by using three different modalities (i.e. gray pixels, optical flow channels and depth maps) on two widely-adopted and challenging datasets: TUM-GAID and CASIA-B. In addition, we perform a comparative study between different early and late fusion methods used to combine the information obtained from each kind of modalities. Our experimental results suggest that (i) the raw pixel values represent a competitive input modality, compared to the traditional state-of-the-art silhouette-based features (e.g. GEI), since equivalent or better results are obtained; (ii) the fusion of the raw pixel information with information from optical flow and depth maps allows to obtain state-of-the-art results on the gait recognition task with an image resolution several times smaller than the previously reported results; and, (iii) the selection and the design of the CNN architecture are critical points that can make a difference between state-of-the-art results or poor ones.",0
"In computer vision, identifying individuals through their gait (walking pattern) is a significant task that can be accomplished non-invasively. However, the usual approach of deriving gait signatures from binary energy maps of subjects extracted from images introduces non-stationary noise, reducing effectiveness. This paper seeks to improve the process by using raw pixels or simple functions derived from them and letting advanced learning techniques extract relevant features. The study involves a comparison of various Convolutional Neural Network (CNN) architectures using three modalities (gray pixels, optical flow channels, and depth maps) on two datasets (TUM-GAID and CASIA-B), as well as the comparison of early and late fusion methods for combining modalities. The results indicate that using raw pixel values as input produces comparable or better results than traditional silhouette-based features, and fusing raw pixel information with optical flow and depth maps leads to state-of-the-art results. The study also highlights the importance of CNN architecture design in achieving optimal results.",1
"This paper proposes a simple yet effective method for human action recognition in video. The proposed method separately extracts local appearance and motion features using state-of-the-art three-dimensional convolutional neural networks from sampled snippets of a video. These local features are then concatenated to form global representations which are then used to train a linear SVM to perform the action classification using full context of the video, as partial context as used in previous works. The videos undergo two simple proposed preprocessing techniques, optical flow scaling and crop filling. We perform an extensive evaluation on three common benchmark dataset to empirically show the benefit of the SVM, and the two preprocessing steps.",0
"In this paper, a straightforward and efficient approach is presented for recognizing human actions in videos. The approach involves extracting local appearance and motion features from selected snippets of a video using advanced three-dimensional convolutional neural networks. Subsequently, these local features are combined to create global representations that are utilized to train a linear SVM for action classification, taking into account the entire context of the video rather than just a portion as previous methods have done. Additionally, two uncomplicated preprocessing techniques, optical flow scaling and crop filling, are proposed and applied to the videos. We conduct a comprehensive evaluation on three standard benchmark datasets to demonstrate the benefits of the SVM and the two preprocessing steps.",1
