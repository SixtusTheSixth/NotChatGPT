"The problem of retrosynthetic planning can be framed as one player game, in which the chemist (or a computer program) works backwards from a molecular target to simpler starting materials though a series of choices regarding which reactions to perform. This game is challenging as the combinatorial space of possible choices is astronomical, and the value of each choice remains uncertain until the synthesis plan is completed and its cost evaluated. Here, we address this problem using deep reinforcement learning to identify policies that make (near) optimal reaction choices during each step of retrosynthetic planning. Using simulated experience or self-play, we train neural networks to estimate the expected synthesis cost or value of any given molecule based on a representation of its molecular structure. We show that learned policies based on this value network outperform heuristic approaches in synthesizing unfamiliar molecules from available starting materials using the fewest number of reactions. We discuss how the learned policies described here can be incorporated into existing synthesis planning tools and how they can be adapted to changes in the synthesis cost objective or material availability.",0
"To plan retrosynthesis, the chemist or a computer program plays a one-player game, tracing back from a complex molecular target to simpler starting materials by choosing reactions. However, determining the optimal choice is challenging due to the vast number of possibilities, and the actual cost is unknown until the synthesis plan is completed. To address this issue, we employ deep reinforcement learning to discover policies that make optimal reaction choices during each retrosynthetic planning step. By training neural networks to estimate the synthesis cost or value of a molecule based on its molecular structure, we establish that policies learned from this value network outperform heuristic methods in synthesizing unfamiliar molecules from available starting materials using the minimum number of reactions. We also consider how these learned policies can be integrated into existing synthesis planning tools and adapted to changes in the synthesis cost objective or material availability.",1
"We give a simple optimistic algorithm for which it is easy to derive regret bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ after $T$ steps in uniformly ergodic Markov decision processes with $S$ states, $A$ actions, and mixing time parameter $t_{\rm mix}$. These bounds are the first regret bounds in the general, non-episodic setting with an optimal dependence on all given parameters. They could only be improved by using an alternative mixing time parameter.",0
"In Markov decision processes that are uniformly ergodic and have $S$ states, $A$ actions, and a mixing time parameter of $t_{\rm mix}$, we offer a straightforward optimistic algorithm. After $T$ steps, regret bounds of $\tilde{O}(\sqrt{t_{\rm mix} SAT})$ can be derived with ease. These are the initial regret bounds in the non-episodic context with an optimal reliance on all the given parameters. The only way to enhance these bounds is by utilizing a different mixing time parameter.",1
"We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM",0
"The aim of our study is to teach the Shadow Dexterous Hand to perform vision-based object reorientation through the use of reinforcement learning (RL). We conduct the training in a simulated environment where we randomize physical properties such as friction coefficients and object appearance. Interestingly, our policies successfully transfer to the physical robot despite being solely trained in simulation, and without any human demonstrations. Our approach enables the emergence of human-like manipulation behaviors, including finger gaiting, multi-finger coordination, and controlled use of gravity. Our results are achieved using the same distributed RL system used for training OpenAI Five, and a video showcasing our findings can be found at https://youtu.be/jwSbzNHGflM.",1
"Machine learning models have become more and more complex in order to better approximate complex functions. Although fruitful in many domains, the added complexity has come at the cost of model interpretability. The once popular k-nearest neighbors (kNN) approach, which finds and uses the most similar data for reasoning, has received much less attention in recent decades due to numerous problems when compared to other techniques. We show that many of these historical problems with kNN can be overcome, and our contribution has applications not only in machine learning but also in online learning, data synthesis, anomaly detection, model compression, and reinforcement learning, without sacrificing interpretability. We introduce a synthesis between kNN and information theory that we hope will provide a clear path towards models that are innately interpretable and auditable. Through this work we hope to gather interest in combining kNN with information theory as a promising path to fully auditable machine learning and artificial intelligence.",0
"To better approximate complex functions, machine learning models have become increasingly complex, but this has resulted in reduced model interpretability. The k-nearest neighbors (kNN) approach, which uses similar data for reasoning, has been less popular due to its numerous problems compared to other techniques. However, we demonstrate that many of these historical issues with kNN can be resolved, and our contribution has applications in various areas of machine learning while maintaining interpretability. We propose a synthesis of kNN and information theory, which we believe is a promising path towards fully auditable machine learning and artificial intelligence, without sacrificing interpretability. Our aim is to generate interest in combining kNN with information theory to achieve this goal.",1
"Building upon the recent success of deep reinforcement learning methods, we investigate the possibility of on-policy reinforcement learning improvement by reusing the data from several consecutive policies. On-policy methods bring many benefits, such as ability to evaluate each resulting policy. However, they usually discard all the information about the policies which existed before. In this work, we propose adaptation of the replay buffer concept, borrowed from the off-policy learning setting, to create the method, combining advantages of on- and off-policy learning. To achieve this, the proposed algorithm generalises the $Q$-, value and advantage functions for data from multiple policies. The method uses trust region optimisation, while avoiding some of the common problems of the algorithms such as TRPO or ACKTR: it uses hyperparameters to replace the trust region selection heuristics, as well as the trainable covariance matrix instead of the fixed one. In many cases, the method not only improves the results comparing to the state-of-the-art trust region on-policy learning algorithms such as PPO, ACKTR and TRPO, but also with respect to their off-policy counterpart DDPG.",0
"We aim to improve on-policy reinforcement learning by reusing data from multiple consecutive policies, building on the success of recent deep reinforcement learning methods. While on-policy methods offer benefits such as the ability to evaluate each policy, they often discard information from previous policies. To address this, we propose a method that combines the advantages of on- and off-policy learning by adapting the replay buffer concept from off-policy learning. Our algorithm generalises the $Q$-, value and advantage functions for data from multiple policies and uses trust region optimisation with hyperparameters replacing trust region selection heuristics and a trainable covariance matrix replacing a fixed one. This method outperforms state-of-the-art trust region on-policy learning algorithms such as PPO, ACKTR, and TRPO, as well as their off-policy counterpart DDPG in many cases.",1
"We propose a hierarchically structured reinforcement learning approach to address the challenges of planning for generating coherent multi-sentence stories for the visual storytelling task. Within our framework, the task of generating a story given a sequence of images is divided across a two-level hierarchical decoder. The high-level decoder constructs a plan by generating a semantic concept (i.e., topic) for each image in sequence. The low-level decoder generates a sentence for each image using a semantic compositional network, which effectively grounds the sentence generation conditioned on the topic. The two decoders are jointly trained end-to-end using reinforcement learning. We evaluate our model on the visual storytelling (VIST) dataset. Empirical results from both automatic and human evaluations demonstrate that the proposed hierarchically structured reinforced training achieves significantly better performance compared to a strong flat deep reinforcement learning baseline.",0
"To tackle the difficulties of generating coherent multi-sentence stories for visual storytelling, we present a reinforcement learning method with hierarchical structure. Our approach involves a two-level decoder system that divides the task of generating a story from a sequence of images. The high-level decoder develops a plan by generating a semantic concept or topic for each image in the sequence, while the low-level decoder generates a sentence for each image using a semantic compositional network that grounds the sentence generation on the topic. Both decoders are jointly trained using reinforcement learning. We evaluate our model using the VIST dataset and show that our hierarchically structured reinforcement learning approach outperforms a strong flat deep reinforcement learning baseline, as demonstrated by empirical results from both automatic and human evaluations.",1
"Central Pattern Generators (CPGs) are biological neural circuits capable of producing coordinated rhythmic outputs in the absence of rhythmic input. As a result, they are responsible for most rhythmic motion in living organisms. This rhythmic control is broadly applicable to fields such as locomotive robotics and medical devices. In this paper, we explore the possibility of creating a self-sustaining CPG network for reinforcement learning that learns rhythmic motion more efficiently and across more general environments than the current multilayer perceptron (MLP) baseline models. Recent work introduces the Structured Control Net (SCN), which maintains linear and nonlinear modules for local and global control, respectively. Here, we show that time-sequence architectures such as Recurrent Neural Networks (RNNs) model CPGs effectively. Combining previous work with RNNs and SCNs, we introduce the Recurrent Control Net (RCN), which adds a linear component to the, RCNs match and exceed the performance of baseline MLPs and SCNs across all environment tasks. Our findings confirm existing intuitions for RNNs on reinforcement learning tasks, and demonstrate promise of SCN-like structures in reinforcement learning.",0
"Biological neural circuits known as Central Pattern Generators (CPGs) can generate coordinated rhythmic outputs without rhythmic input and are responsible for most rhythmic motion in living organisms. This rhythmic control has broad applications in fields such as locomotive robotics and medical devices. The purpose of this paper is to investigate the creation of a self-sustaining CPG network for reinforcement learning that can learn rhythmic motion more efficiently and across more diverse environments than current multilayer perceptron (MLP) baseline models. The Structured Control Net (SCN) has been introduced as a linear and nonlinear module for local and global control, respectively. This paper demonstrates that Recurrent Neural Networks (RNNs) can effectively model CPGs. By combining previous work with RNNs and SCNs, the Recurrent Control Net (RCN) is introduced, which adds a linear component. The RCNs outperform baseline MLPs and SCNs in all environmental tasks, confirming existing intuitions for RNNs in reinforcement learning tasks and demonstrating the potential of SCN-like structures in reinforcement learning.",1
"This paper proposes a new reinforcement learning (RL) algorithm that enhances exploration by amplifying the imitation effect (AIE). This algorithm consists of self-imitation learning and random network distillation algorithms. We argue that these two algorithms complement each other and that combining these two algorithms can amplify the imitation effect for exploration. In addition, by adding an intrinsic penalty reward to the state that the RL agent frequently visits and using replay memory for learning the feature state when using an exploration bonus, the proposed approach leads to deep exploration and deviates from the current converged policy. We verified the exploration performance of the algorithm through experiments in a two-dimensional grid environment. In addition, we applied the algorithm to a simulated environment of unmanned combat aerial vehicle (UCAV) mission execution, and the empirical results show that AIE is very effective for finding the UCAV's shortest flight path to avoid an enemy's missiles.",0
"The paper presents a novel reinforcement learning (RL) technique that boosts exploration by intensifying the imitation effect (AIE). The technique integrates self-imitation learning and random network distillation algorithms to complement each other and enhance the imitation effect for exploration. Furthermore, the proposed approach includes an intrinsic penalty reward for frequently visited states of the RL agent and the use of replay memory for learning feature states, which facilitates deep exploration and deviation from the current policy. The algorithm's exploration performance was evaluated through experiments in a two-dimensional grid environment, and it was also applied to a simulated unmanned combat aerial vehicle (UCAV) mission execution environment, where the empirical results reveal that AIE is highly efficient in discovering the shortest flight path to evade enemy missiles.",1
"In this work, we study value function approximation in reinforcement learning (RL) problems with high dimensional state or action spaces via a generalized version of representation policy iteration (RPI). We consider the limitations of proto-value functions (PVFs) at accurately approximating the value function in low dimensions and we highlight the importance of features learning for an improved low-dimensional value function approximation. Then, we adopt different representation learning algorithm on graphs to learn the basis functions that best represent the value function. We empirically show that node2vec, an algorithm for scalable feature learning in networks, and the Variational Graph Auto-Encoder constantly outperform the commonly used smooth proto-value functions in low-dimensional feature space.",0
"The focus of our research is to explore the use of a generalized version of representation policy iteration (RPI) to approximate value functions in reinforcement learning (RL) problems that involve high dimensional state or action spaces. Our investigation highlights the limitations of proto-value functions (PVFs) in accurately approximating the value function in situations where dimensions are low, and underscores the importance of feature learning for improved low-dimensional value function approximation. To achieve this, we explore different representation learning algorithms on graphs to identify the basis functions that best represent the value function. Our empirical analysis shows that when compared to the commonly used smooth proto-value functions, node2vec, an algorithm for scalable feature learning in networks, and the Variational Graph Auto-Encoder consistently outperform in low-dimensional feature space.",1
"This paper proposes CodeX, an end-to-end framework that facilitates encoding, bitwidth customization, fine-tuning, and implementation of neural networks on FPGA platforms. CodeX incorporates nonlinear encoding to the computation flow of neural networks to save memory. The encoded features demand significantly lower storage compared to the raw full-precision activation values; therefore, the execution flow of CodeX hardware engine is completely performed within the FPGA using on-chip streaming buffers with no access to the off-chip DRAM. We further propose a fully-automated algorithm inspired by reinforcement learning which determines the customized encoding bitwidth across network layers. CodeX full-stack framework comprises of a compiler which takes a high-level Python description of an arbitrary neural network architecture. The compiler then instantiates the corresponding elements from CodeX Hardware library for FPGA implementation. Proof-of-concept evaluations on MNIST, SVHN, and CIFAR-10 datasets demonstrate an average of 4.65x throughput improvement compared to stand-alone weight encoding. We further compare CodeX with six existing full-precision DNN accelerators on ImageNet, showing an average of 3.6x and 2.54x improvement in throughput and performance-per-watt, respectively.",0
"The article introduces CodeX as a comprehensive system for optimizing neural network implementation on FPGA platforms. It employs nonlinear encoding to reduce memory usage and improve efficiency, while a custom algorithm determines the optimal encoding bitwidth for each network layer. The system includes a compiler that uses Python to describe network architecture and instantiate CodeX Hardware library elements for FPGA implementation. CodeX outperforms other DNN accelerators, with a 4.65x increase in throughput for MNIST, SVHN, and CIFAR-10 datasets and 3.6x and 2.54x improvement in throughput and performance-per-watt, respectively, compared to six existing full-precision DNN accelerators on ImageNet. The FPGA hardware engine executes entirely within the FPGA, using on-chip streaming buffers without accessing off-chip DRAM.",1
"We study the computational tractability of PAC reinforcement learning with rich observations. We present new provably sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation -- accessing policy and value function classes exclusively through standard optimization primitives -- and therefore represent computationally efficient alternatives to prior algorithms that require enumeration. With stochastic hidden state dynamics, we prove that the only known sample-efficient algorithm, OLIVE, cannot be implemented in the oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.",0
"Our focus is on the computational feasibility of PAC reinforcement learning when dealing with rich observations. We introduce novel algorithms that are guaranteed to be sample-efficient when handling environments with deterministic hidden state dynamics and stochastic rich observations. These algorithms operate within an oracle model of computation, accessing policy and value function classes solely through standard optimization techniques. This makes them computationally efficient alternatives to previous algorithms that necessitate enumeration. When it comes to stochastic hidden state dynamics, we demonstrate that OLIVE, the only sample-efficient algorithm known, cannot be implemented within the oracle model. Additionally, we provide several examples that highlight the fundamental difficulties of achieving tractable PAC reinforcement learning in such general scenarios.",1
"Deep Reinforcement Learning (DRL) has become increasingly powerful in recent years, with notable achievements such as Deepmind's AlphaGo. It has been successfully deployed in commercial vehicles like Mobileye's path planning system. However, a vast majority of work on DRL is focused on toy examples in controlled synthetic car simulator environments such as TORCS and CARLA. In general, DRL is still at its infancy in terms of usability in real-world applications. Our goal in this paper is to encourage real-world deployment of DRL in various autonomous driving (AD) applications. We first provide an overview of the tasks in autonomous driving systems, reinforcement learning algorithms and applications of DRL to AD systems. We then discuss the challenges which must be addressed to enable further progress towards real-world deployment.",0
"In recent years, Deep Reinforcement Learning (DRL) has gained immense power and has been responsible for notable achievements such as Deepmind's AlphaGo. DRL has even been successfully implemented in commercial vehicles like Mobileye's path planning system. However, the majority of DRL research has been focused on small-scale examples in controlled synthetic car simulator environments like TORCS and CARLA. As a result, DRL is still in its early stages of development when it comes to practical applications in the real world. Our objective in this paper is to promote the integration of DRL in various autonomous driving (AD) applications. We begin by providing a comprehensive overview of the tasks involved in autonomous driving systems, reinforcement learning algorithms, and how DRL can be applied to AD systems. We then address the obstacles that need to be resolved to facilitate further progress towards real-world deployment.",1
"Model compression is a critical technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted heuristics and rule-based policies that require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverage reinforcement learning to provide the model compression policy. This learning-based compression policy outperforms conventional rule-based compression policy by having higher compression ratio, better preserving the accuracy and freeing human labor. Under 4x FLOPs reduction, we achieved 2.7% better accuracy than the handcrafted model compression policy for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet and achieved 1.81x speedup of measured inference latency on an Android phone and 1.43x speedup on the Titan XP GPU, with only 0.1% loss of ImageNet Top-1 accuracy.",0
"Efficient deployment of neural network models on mobile devices with limited computation resources and tight power budgets is critical, and model compression is a key technique to achieve this. However, traditional model compression techniques rely on hand-crafted heuristics and rule-based policies that are sub-optimal and time-consuming. In this paper, we introduce AutoML for Model Compression (AMC), which employs reinforcement learning to provide an automated model compression policy that outperforms traditional rule-based compression policies by achieving higher compression ratios, better accuracy preservation, and freeing up human labor. Our push-the-button compression pipeline achieved 2.7% better accuracy than handcrafted compression policies for VGG-16 on ImageNet, with a 4x reduction in FLOPs. We also applied this automated pipeline to MobileNet and achieved a 1.81x speedup of measured inference latency on an Android phone and a 1.43x speedup on the Titan XP GPU, with only a 0.1% loss of ImageNet Top-1 accuracy.",1
"Representation learning becomes especially important for complex systems with multimodal data sources such as cameras or sensors. Recent advances in reinforcement learning and optimal control make it possible to design control algorithms on these latent representations, but the field still lacks a large-scale standard dataset for unified comparison. In this work, we present a large-scale dataset and evaluation framework for representation learning for the complex task of landing an airplane. We implement and compare several approaches to representation learning on this dataset in terms of the quality of simple supervised learning tasks and disentanglement scores. The resulting representations can be used for further tasks such as anomaly detection, optimal control, model-based reinforcement learning, and other applications.",0
"Representation learning plays a critical role in dealing with intricate systems that involve multiple data sources such as cameras or sensors. Though there have been recent advancements in reinforcement learning and optimal control, which facilitate the development of control algorithms based on these concealed representations, the field still lacks a standardized and vast dataset for comparative assessment. Accordingly, we introduce a comprehensive dataset and assessment framework in this study that focuses on representation learning for the intricate task of landing an airplane. We compare several approaches to representation learning on this dataset in terms of their efficiency in simple supervised learning tasks and disentanglement scores. The resulting representations can be utilized for various other applications, including anomaly detection, optimal control, model-based reinforcement learning, and more.",1
"Sepsis is the leading cause of mortality in the ICU. It is challenging to manage because individual patients respond differently to treatment. Thus, tailoring treatment to the individual patient is essential for the best outcomes. In this paper, we take steps toward this goal by applying a mixture-of-experts framework to personalize sepsis treatment. The mixture model selectively alternates between neighbor-based (kernel) and deep reinforcement learning (DRL) experts depending on patient's current history. On a large retrospective cohort, this mixture-based approach outperforms physician, kernel only, and DRL-only experts.",0
"The ICU's primary cause of death is sepsis, which poses a management challenge due to varying patient responses to treatment. Therefore, customizing treatment to each patient is imperative for optimal outcomes. In this study, we move closer to this objective by utilizing a blend-of-experts framework to personalize sepsis treatment. The model selectively switches between neighbor-based (kernel) and deep reinforcement learning (DRL) experts, depending on the patient's current history. This approach's superiority over physician-only, kernel-only, and DRL-only experts was demonstrated in a large retrospective cohort.",1
"We study the global convergence of generative adversarial imitation learning for linear quadratic regulators, which is posed as minimax optimization. To address the challenges arising from non-convex-concave geometry, we analyze the alternating gradient algorithm and establish its Q-linear rate of convergence to a unique saddle point, which simultaneously recovers the globally optimal policy and reward function. We hope our results may serve as a small step towards understanding and taming the instability in imitation learning as well as in more general non-convex-concave alternating minimax optimization that arises from reinforcement learning and generative adversarial learning.",0
"Our research focuses on examining the worldwide convergence of generative adversarial imitation learning for linear quadratic regulators. This process involves minimizing and maximizing certain aspects. To overcome the challenges that arise from non-convex-concave geometry, we analyze the effectiveness of the alternating gradient algorithm and establish its Q-linear rate of convergence to a unique saddle point. This point allows for the recovery of both the globally optimal policy and reward function. We believe that our findings can contribute to a better understanding of and solution to the instability that occurs in imitation learning and other non-convex-concave alternating minimax optimization processes in reinforcement and generative adversarial learning.",1
"Automatically generating the descriptions of an image, i.e., image captioning, is an important and fundamental topic in artificial intelligence, which bridges the gap between computer vision and natural language processing. Based on the successful deep learning models, especially the CNN model and Long Short-Term Memories (LSTMs) with attention mechanism, we propose a hierarchical attention model by utilizing both of the global CNN features and the local object features for more effective feature representation and reasoning in image captioning. The generative adversarial network (GAN), together with a reinforcement learning (RL) algorithm, is applied to solve the exposure bias problem in RNN-based supervised training for language problems. In addition, through the automatic measurement of the consistency between the generated caption and the image content by the discriminator in the GAN framework and RL optimization, we make the finally generated sentences more accurate and natural. Comprehensive experiments show the improved performance of the hierarchical attention mechanism and the effectiveness of our RL-based optimization method. Our model achieves state-of-the-art results on several important metrics in the MSCOCO dataset, using only greedy inference.",0
"The task of generating descriptions for images, or image captioning, is a crucial area of study in artificial intelligence, as it connects computer vision and natural language processing. Our proposed hierarchical attention model, which incorporates both global CNN features and local object features, leverages successful deep learning models such as CNN and Long Short-Term Memories (LSTMs) with attention mechanisms to improve feature representation and reasoning in image captioning. We also use a generative adversarial network (GAN) and a reinforcement learning (RL) algorithm to address the exposure bias problem in RNN-based supervised training for language tasks. By measuring the consistency between generated captions and image content automatically through the GAN framework and RL optimization, we achieve more accurate and natural sentences. Comprehensive experiments demonstrate the effectiveness of our RL-based optimization method and the improved performance of the hierarchical attention mechanism. Our model achieves state-of-the-art results on multiple important metrics in the MSCOCO dataset, using only greedy inference.",1
"Deep reinforcement learning (RL) has achieved many recent successes, yet experiment turn-around time remains a key bottleneck in research and in practice. We investigate how to optimize existing deep RL algorithms for modern computers, specifically for a combination of CPUs and GPUs. We confirm that both policy gradient and Q-value learning algorithms can be adapted to learn using many parallel simulator instances. We further find it possible to train using batch sizes considerably larger than are standard, without negatively affecting sample complexity or final performance. We leverage these facts to build a unified framework for parallelization that dramatically hastens experiments in both classes of algorithm. All neural network computations use GPUs, accelerating both data collection and training. Our results include using an entire DGX-1 to learn successful strategies in Atari games in mere minutes, using both synchronous and asynchronous algorithms.",0
"Despite its recent successes, deep reinforcement learning (RL) still faces a major obstacle in terms of experiment turn-around time, both in research and practice. Our study explores ways to enhance existing deep RL algorithms for modern computer systems, particularly for a combination of CPUs and GPUs. We have verified that policy gradient and Q-value learning algorithms can be adjusted to learn from numerous parallel simulator instances. Additionally, we have discovered that it is possible to train using batch sizes that are notably larger than the standard, without having an adverse effect on sample complexity or final performance. By incorporating these findings, we have developed a comprehensive framework for parallelization that significantly accelerates experiments in both algorithm categories. All neural network computations leverage GPUs, enabling faster data collection and training. Our findings demonstrate that we can use an entire DGX-1 to learn successful strategies in Atari games in mere minutes, using both synchronous and asynchronous algorithms.",1
"Algorithmic assurances from advanced autonomous systems assist human users in understanding, trusting, and using such systems appropriately. Designing these systems with the capacity of assessing their own capabilities is one approach to creating an algorithmic assurance. The idea of `machine self-confidence' is introduced for autonomous systems. Using a factorization based framework for self-confidence assessment, one component of self-confidence, called `solver-quality', is discussed in the context of Markov decision processes for autonomous systems. Markov decision processes underlie much of the theory of reinforcement learning, and are commonly used for planning and decision making under uncertainty in robotics and autonomous systems. A `solver quality' metric is formally defined in the context of decision making algorithms based on Markov decision processes. A method for assessing solver quality is then derived, drawing inspiration from empirical hardness models. Finally, numerical experiments for an unmanned autonomous vehicle navigation problem under different solver, parameter, and environment conditions indicate that the self-confidence metric exhibits the desired properties. Discussion of results, and avenues for future investigation are included.",0
"Advanced autonomous systems provide algorithmic assurances that aid human users in comprehending, relying on, and utilizing these systems suitably. One approach to creating such assurances is by designing these systems to be capable of evaluating their own abilities. The concept of ""machine self-confidence"" has been introduced for autonomous systems. In this respect, a factorization-based framework has been proposed for assessing self-confidence, with one of its components called ""solver-quality"" being discussed in the context of Markov decision processes, which are fundamental to reinforcement learning theory and are often employed in decision-making and planning under ambiguity in robotics and autonomous systems. The ""solver quality"" metric is formally defined in terms of decision-making algorithms based on Markov decision processes, and a method for evaluating solver quality is derived based on empirical hardness models. Finally, numerical experiments on an unmanned autonomous vehicle navigation problem under varying solver, parameter, and environment conditions demonstrate that the self-confidence metric displays the desired characteristics. The findings are discussed, and potential areas for future research are identified.",1
"Despite the success of single-agent reinforcement learning, multi-agent reinforcement learning (MARL) remains challenging due to complex interactions between agents. Motivated by decentralized applications such as sensor networks, swarm robotics, and power grids, we study policy evaluation in MARL, where agents with jointly observed state-action pairs and private local rewards collaborate to learn the value of a given policy. In this paper, we propose a double averaging scheme, where each agent iteratively performs averaging over both space and time to incorporate neighboring gradient information and local reward information, respectively. We prove that the proposed algorithm converges to the optimal solution at a global geometric rate. In particular, such an algorithm is built upon a primal-dual reformulation of the mean squared projected Bellman error minimization problem, which gives rise to a decentralized convex-concave saddle-point problem. To the best of our knowledge, the proposed double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems.",0
"Although single-agent reinforcement learning has been successful, multi-agent reinforcement learning (MARL) is still difficult due to complex interactions between agents. Our focus is on policy evaluation in MARL, where agents work together to learn the value of a given policy using jointly observed state-action pairs and private local rewards. We propose a double averaging scheme, where each agent incorporates neighboring gradient information and local reward information through iterative averaging over space and time, respectively. Our algorithm is proven to converge to the optimal solution at a global geometric rate. This algorithm is based on a primal-dual reformulation of the mean squared projected Bellman error minimization problem, resulting in a decentralized convex-concave saddle-point problem. Our double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems, as far as we know.",1
"We consider the problem of detecting out-of-distribution (OOD) samples in deep reinforcement learning. In a value based reinforcement learning setting, we propose to use uncertainty estimation techniques directly on the agent's value estimating neural network to detect OOD samples. The focus of our work lies in analyzing the suitability of approximate Bayesian inference methods and related ensembling techniques that generate uncertainty estimates. Although prior work has shown that dropout-based variational inference techniques and bootstrap-based approaches can be used to model epistemic uncertainty, the suitability for detecting OOD samples in deep reinforcement learning remains an open question. Our results show that uncertainty estimation can be used to differentiate in- from out-of-distribution samples. Over the complete training process of the reinforcement learning agents, bootstrap-based approaches tend to produce more reliable epistemic uncertainty estimates, when compared to dropout-based approaches.",0
"Our research examines the issue of identifying out-of-distribution (OOD) samples in deep reinforcement learning. In a value-based reinforcement learning context, we suggest utilizing uncertainty estimation techniques on the neural network responsible for estimating the agent's value to detect OOD samples. Our study centers on assessing the appropriateness of approximate Bayesian inference methods and related ensembling techniques that generate uncertainty estimates. While previous research has indicated that dropout-based variational inference techniques and bootstrap-based approaches can model epistemic uncertainty, their suitability for detecting OOD samples in deep reinforcement learning remains uncertain. Our findings demonstrate that uncertainty estimation can differentiate between in-distribution and out-of-distribution samples. As the reinforcement learning agents progress through their training, bootstrap-based approaches generally provide more dependable epistemic uncertainty estimates than dropout-based approaches.",1
"In real-world scenarios, it is appealing to learn a model carrying out stochastic operations internally, known as stochastic computation graphs (SCGs), rather than learning a deterministic mapping. However, standard backpropagation is not applicable to SCGs. We attempt to address this issue from the angle of cost propagation, with local surrogate costs, called Q-functions, constructed and learned for each stochastic node in an SCG. Then, the SCG can be trained based on these surrogate costs using standard backpropagation. We propose the entire framework as a solution to generalize backpropagation for SCGs, which resembles an actor-critic architecture but based on a graph. For broad applicability, we study a variety of SCG structures from one cost to multiple costs. We utilize recent advances in reinforcement learning (RL) and variational Bayes (VB), such as off-policy critic learning and unbiased-and-low-variance gradient estimation, and review them in the context of SCGs. The generalized backpropagation extends transported learning signals beyond gradients between stochastic nodes while preserving the benefit of backpropagating gradients through deterministic nodes. Experimental suggestions and concerns are listed to help design and test any specific model using this framework.",0
"In practical situations, it is desirable to acquire a model that performs stochastic operations internally known as stochastic computation graphs (SCGs) instead of a deterministic mapping. However, standard backpropagation is not suitable for SCGs. To tackle this problem, we approach it from the cost propagation perspective by constructing and learning local surrogate costs, called Q-functions, for each stochastic node in an SCG. Using these surrogate costs, the SCG can be trained using standard backpropagation. We present this entire framework as a solution to generalize backpropagation for SCGs, which is similar to an actor-critic architecture but based on a graph. We investigate a variety of SCG structures from one cost to multiple costs to ensure broad applicability. We utilize recent advancements in reinforcement learning (RL) and variational Bayes (VB), such as off-policy critic learning and unbiased-and-low-variance gradient estimation, and review them in the context of SCGs. The generalized backpropagation extends transported learning signals beyond gradients between stochastic nodes while maintaining the advantages of backpropagating gradients through deterministic nodes. Experimental recommendations and concerns are provided to assist in designing and testing any specific model using this framework.",1
"Stochastic computation graphs (SCGs) provide a formalism to represent structured optimization problems arising in artificial intelligence, including supervised, unsupervised, and reinforcement learning. Previous work has shown that an unbiased estimator of the gradient of the expected loss of SCGs can be derived from a single principle. However, this estimator often has high variance and requires a full model evaluation per data point, making this algorithm costly in large graphs. In this work, we address these problems by generalizing concepts from the reinforcement learning literature. We introduce the concepts of value functions, baselines and critics for arbitrary SCGs, and show how to use them to derive lower-variance gradient estimates from partial model evaluations, paving the way towards general and efficient credit assignment for gradient-based optimization. In doing so, we demonstrate how our results unify recent advances in the probabilistic inference and reinforcement learning literature.",0
"Stochastic computation graphs (SCGs) offer a formal framework for representing structured optimization problems that arise in artificial intelligence, encompassing supervised, unsupervised, and reinforcement learning. Previous research has indicated that a single principle can generate an unbiased gradient estimator for the expected loss of SCGs. However, this estimator often exhibits high variance and necessitates a full model evaluation for each data point, rendering the algorithm expensive for large graphs. In this study, we tackle these issues by extending concepts from the reinforcement learning literature. We introduce the notions of value functions, critics, and baselines for arbitrary SCGs and illustrate how they can be employed to derive lower-variance gradient estimates from partial model evaluations. This breakthrough paves the way for general and efficient credit assignment for gradient-based optimization, and unifies recent developments in the probabilistic inference and reinforcement learning literature.",1
"Long-term planning poses a major difficulty to many reinforcement learning algorithms. This problem becomes even more pronounced in dynamic visual environments. In this work we propose Hierarchical Planning and Reinforcement Learning (HIP-RL), a method for merging the benefits and capabilities of Symbolic Planning with the learning abilities of Deep Reinforcement Learning. We apply HIPRL to the complex visual tasks of interactive question answering and visual semantic planning and achieve state-of-the-art results on three challenging datasets all while taking fewer steps at test time and training in fewer iterations. Sample results can be found at youtu.be/0TtWJ_0mPfI",0
"Numerous reinforcement learning algorithms face a major obstacle in executing long-term planning, particularly in dynamic visual environments. To address this issue, we propose a solution called Hierarchical Planning and Reinforcement Learning (HIP-RL) that combines the strengths of Symbolic Planning with the learning capabilities of Deep Reinforcement Learning. We demonstrate the effectiveness of HIP-RL in complex visual tasks such as interactive question answering and visual semantic planning, achieving superior performance on three challenging datasets with fewer steps during testing and training. See youtu.be/0TtWJ_0mPfI for sample results.",1
"Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.",0
"The study of inferring intent from observed behavior has been extensively researched in the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods involve inferring a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator, which can then be used by another agent to predict, imitate, or assist the human user. However, inverse reinforcement learning assumes that the demonstrator is close to optimal, and suboptimal behavior is typically attributed to random noise or known cognitive biases. In this paper, the authors propose an alternative approach, modeling suboptimal behavior as a result of internal model misspecification. This means that the user's actions may deviate from near-optimal actions because they have an incorrect set of beliefs about the rules governing how actions affect the environment. By estimating these internal beliefs from observed behavior, the authors arrive at a new method for inferring intent, which has been demonstrated to be more accurate in simulation and in a user study with 12 participants. This approach has potential applications in offering assistance in a shared autonomy framework and inferring human preferences.",1
"Data in real-world application often exhibit skewed class distribution which poses an intense challenge for machine learning. Conventional classification algorithms are not effective in the case of imbalanced data distribution, and may fail when the data distribution is highly imbalanced. To address this issue, we propose a general imbalanced classification model based on deep reinforcement learning. We formulate the classification problem as a sequential decision-making process and solve it by deep Q-learning network. The agent performs a classification action on one sample at each time step, and the environment evaluates the classification action and returns a reward to the agent. The reward from minority class sample is larger so the agent is more sensitive to the minority class. The agent finally finds an optimal classification policy in imbalanced data under the guidance of specific reward function and beneficial learning environment. Experiments show that our proposed model outperforms the other imbalanced classification algorithms, and it can identify more minority samples and has great classification performance.",0
"The skewed distribution of data in real-world scenarios can pose a significant obstacle for machine learning. Traditional classification algorithms may not be effective when dealing with imbalanced data distribution, especially when it is highly skewed. To tackle this challenge, we present a novel imbalanced classification model that utilizes deep reinforcement learning. Our approach frames the classification problem as a sequential decision-making process, which is solved using a deep Q-learning network. At each time step, the agent performs a classification action on a single sample, and the environment evaluates the action and returns a reward to the agent. Notably, rewards for minority class samples are larger, making the agent more sensitive to these samples. By utilizing a specific reward function and a favorable learning environment, our approach enables the agent to identify an optimal classification policy for imbalanced data. Experimental results demonstrate that our proposed model outperforms other imbalanced classification algorithms, with better performance in identifying minority samples and achieving higher classification accuracy.",1
"Optimal decision making with limited or no information in stochastic environments where multiple agents interact is a challenging topic in the realm of artificial intelligence. Reinforcement learning (RL) is a popular approach for arriving at optimal strategies by predicating stimuli, such as the reward for following a strategy, on experience. RL is heavily explored in the single-agent context, but is a nascent concept in multiagent problems. To this end, I propose several principled model-free and partially model-based reinforcement learning approaches for several multiagent settings. In the realm of normative reinforcement learning, I introduce scalable extensions to Monte Carlo exploring starts for partially observable Markov Decision Processes (POMDP), dubbed MCES-P, where I expand the theory and algorithm to the multiagent setting. I first examine MCES-P with probably approximately correct (PAC) bounds in the context of multiagent setting, showing MCESP+PAC holds in the presence of other agents. I then propose a more sample-efficient methodology for antagonistic settings, MCESIP+PAC. For cooperative settings, I extend MCES-P to the Multiagent POMDP, dubbed MCESMP+PAC. I then explore the use of reinforcement learning as a methodology in searching for optima in realistic and latent model environments. First, I explore a parameterized Q-learning approach in modeling humans learning to reason in an uncertain, multiagent environment. Next, I propose an implementation of MCES-P, along with image segmentation, to create an adaptive team-based reinforcement learning technique to positively identify the presence of phenotypically-expressed water and pathogen stress in crop fields.",0
"Artificial intelligence faces a difficult challenge when it comes to making optimal decisions in stochastic environments where multiple agents interact, especially when there is limited or no information available. Reinforcement learning is a widely used method for determining optimal strategies by relying on experience to predict stimuli, such as the reward for following a strategy. However, while reinforcement learning has been extensively researched in the single-agent context, it is still a relatively new concept in multiagent problems. Therefore, I propose several principled approaches to reinforcement learning that are model-free or partially model-based for various multiagent settings. In normative reinforcement learning, I introduce scalable extensions to Monte Carlo exploring starts for partially observable Markov decision processes, or POMDP, that I call MCES-P, and expand the theory and algorithm to the multiagent setting. I first examine MCES-P with probably approximately correct bounds in the multiagent context, demonstrating that MCESP+PAC holds even in the presence of other agents. I then propose a more sample-efficient methodology for antagonistic settings, MCESIP+PAC, and for cooperative settings, I extend MCES-P to the Multiagent POMDP, or MCESMP+PAC. I also explore the use of reinforcement learning as a methodology for searching for optima in both realistic and latent model environments. Specifically, I investigate a parameterized Q-learning approach for modeling human reasoning in an uncertain multiagent environment, as well as an implementation of MCES-P and image segmentation to create an adaptive team-based reinforcement learning technique for identifying the presence of water and pathogen stress in crop fields.",1
"Consider mutli-goal tasks that involve static environments and dynamic goals. Examples of such tasks, such as goal-directed navigation and pick-and-place in robotics, abound. Two types of Reinforcement Learning (RL) algorithms are used for such tasks: model-free or model-based. Each of these approaches has limitations. Model-free RL struggles to transfer learned information when the goal location changes, but achieves high asymptotic accuracy in single goal tasks. Model-based RL can transfer learned information to new goal locations by retaining the explicitly learned state-dynamics, but is limited by the fact that small errors in modelling these dynamics accumulate over long-term planning. In this work, we improve upon the limitations of model-free RL in multi-goal domains. We do this by adapting the Floyd-Warshall algorithm for RL and call the adaptation Floyd-Warshall RL (FWRL). The proposed algorithm learns a goal-conditioned action-value function by constraining the value of the optimal path between any two states to be greater than or equal to the value of paths via intermediary states. Experimentally, we show that FWRL is more sample-efficient and learns higher reward strategies in multi-goal tasks as compared to Q-learning, model-based RL and other relevant baselines in a tabular domain.",0
"Tasks that have multiple goals and involve both static environments and dynamic goals, such as goal-directed navigation and pick-and-place in robotics, are common. Two types of Reinforcement Learning (RL) algorithms, namely model-free and model-based, are used for such tasks, but each approach has its own limitations. Model-free RL is unable to transfer learned information when the goal location changes, but performs well in single goal tasks. In contrast, model-based RL can transfer learned information to new goal locations by retaining the explicitly learned state-dynamics, but is limited by the accumulation of small errors in modelling these dynamics over long-term planning. This study proposes an algorithm called Floyd-Warshall RL (FWRL), which adapts the Floyd-Warshall algorithm for RL to overcome the limitations of model-free RL in multi-goal domains. The proposed algorithm learns a goal-conditioned action-value function by constraining the value of the optimal path between any two states to be greater than or equal to the value of paths via intermediary states. Experimental results show that FWRL is more efficient in terms of sample usage and learns higher reward strategies in multi-goal tasks compared to Q-learning, model-based RL, and other relevant baselines in a tabular domain.",1
"We propose a hybrid approach aimed at improving the sample efficiency in goal-directed reinforcement learning. We do this via a two-step mechanism where firstly, we approximate a model from Model-Free reinforcement learning. Then, we leverage this approximate model along with a notion of reachability using Mean First Passage Times to perform Model-Based reinforcement learning. Built on such a novel observation, we design two new algorithms - Mean First Passage Time based Q-Learning (MFPT-Q) and Mean First Passage Time based DYNA (MFPT-DYNA), that have been fundamentally modified from the state-of-the-art reinforcement learning techniques. Preliminary results have shown that our hybrid approaches converge with much fewer iterations than their corresponding state-of-the-art counterparts and therefore requiring much fewer samples and much fewer training trials to converge.",0
"Our proposed method seeks to enhance the efficiency of goal-directed reinforcement learning through a hybrid approach. This entails utilizing a two-step process, wherein we first approximate a model using Model-Free reinforcement learning. Subsequently, we utilize this approximation in conjunction with Mean First Passage Times to enable Model-Based reinforcement learning. Based on this unique approach, we have developed two new algorithms - Mean First Passage Time based Q-Learning (MFPT-Q) and Mean First Passage Time based DYNA (MFPT-DYNA). These algorithms have been significantly modified from existing reinforcement learning techniques. Our initial findings indicate that our hybrid approach requires fewer iterations to converge compared to traditional methods, thereby necessitating fewer samples and training trials to achieve the same results.",1
"Operating directly from raw high dimensional sensory inputs like images is still a challenge for robotic control. Recently, Reinforcement Learning methods have been proposed to solve specific tasks end-to-end, from pixels to torques. However, these approaches assume the access to a specified reward which may require specialized instrumentation of the environment. Furthermore, the obtained policy and representations tend to be task specific and may not transfer well. In this work we investigate completely self-supervised learning of a general image embedding and control primitives, based on finding the shortest time to reach any state. We also introduce a new structure for the state-action value function that builds a connection between model-free and model-based methods, and improves the performance of the learning algorithm. We experimentally demonstrate these findings in three simulated robotic tasks.",0
"A difficulty faced by robotic control is operating directly from raw high dimensional sensory inputs such as images. Despite recent proposals using Reinforcement Learning methods to solve specific end-to-end tasks from pixels to torques, these approaches require access to a specified reward and may not transfer well between tasks. Moreover, the policy and representations obtained tend to be task-specific. This study explores the possibility of completely self-supervised learning of a general image embedding and control primitives by finding the shortest time to reach any state. In addition, a new structure for the state-action value function is introduced to connect model-free and model-based methods and enhance the learning algorithm's performance. The findings are experimentally demonstrated in three simulated robotic tasks.",1
"This work examines the role of reinforcement learning in reducing the severity of on-road collisions by controlling velocity and steering in situations in which contact is imminent. We construct a model, given camera images as input, that is capable of learning and predicting the dynamics of obstacles, cars and pedestrians, and train our policy using this model. Two policies that control both braking and steering are compared against a baseline where the only action taken is (conventional) braking in a straight line. The two policies are trained using two distinct reward structures, one where any and all collisions incur a fixed penalty, and a second one where the penalty is calculated based on already established delta-v models of injury severity. The results show that both policies exceed the performance of the baseline, with the policy trained using injury models having the highest performance.",0
"This study investigates how reinforcement learning can be used to minimize the impact of on-road collisions by regulating velocity and steering in situations where a collision is unavoidable. A model is developed to learn and anticipate the behavior of obstacles, cars, and pedestrians based on camera images. We then train our policy using this model and compare two policies that control both braking and steering against a baseline that only uses conventional braking in a straight line. Our policies are trained using two different reward structures: one where any collision incurs a fixed penalty, and another where the penalty is calculated based on pre-established delta-v models of injury severity. The results demonstrate that both policies outperform the baseline, with the policy trained using injury models achieving the highest performance.",1
"This study proposes a framework for human-like autonomous car-following planning based on deep reinforcement learning (deep RL). Historical driving data are fed into a simulation environment where an RL agent learns from trial and error interactions based on a reward function that signals how much the agent deviates from the empirical data. Through these interactions, an optimal policy, or car-following model that maps in a human-like way from speed, relative speed between a lead and following vehicle, and inter-vehicle spacing to acceleration of a following vehicle is finally obtained. The model can be continuously updated when more data are fed in. Two thousand car-following periods extracted from the 2015 Shanghai Naturalistic Driving Study were used to train the model and compare its performance with that of traditional and recent data-driven car-following models. As shown by this study results, a deep deterministic policy gradient car-following model that uses disparity between simulated and observed speed as the reward function and considers a reaction delay of 1s, denoted as DDPGvRT, can reproduce human-like car-following behavior with higher accuracy than traditional and recent data-driven car-following models. Specifically, the DDPGvRT model has a spacing validation error of 18% and speed validation error of 5%, which are less than those of other models, including the intelligent driver model, models based on locally weighted regression, and conventional neural network-based models. Moreover, the DDPGvRT demonstrates good capability of generalization to various driving situations and can adapt to different drivers by continuously learning. This study demonstrates that reinforcement learning methodology can offer insight into driver behavior and can contribute to the development of human-like autonomous driving algorithms and traffic-flow models.",0
"The aim of this study is to propose a framework for autonomous car-following planning that mimics human behavior using deep reinforcement learning (deep RL). The approach involves feeding historical driving data into a simulation environment where an RL agent learns from trial and error interactions based on a reward function that measures how much the agent deviates from the empirical data. The resulting car-following model maps from speed, relative speed, and inter-vehicle spacing to acceleration in a human-like way. The model is continuously updated with more data and was trained using 2000 car-following periods from the 2015 Shanghai Naturalistic Driving Study. Results show that the proposed DDPGvRT model outperforms traditional and recent data-driven car-following models, with a spacing validation error of 18% and speed validation error of 5%. The model also demonstrates good generalization to various driving situations and can adapt to different drivers by continuously learning. The study concludes that reinforcement learning methodology can improve the development of human-like autonomous driving algorithms and traffic-flow models.",1
"To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.",0
"In order for agents to effectively navigate complex environments, we suggest that they possess a mental simulator of the world that possesses three key traits. Firstly, it must construct an abstract state that represents the current condition of the world. Secondly, it must form a belief that acknowledges uncertainty about the world. Lastly, it must incorporate temporal abstraction, going beyond a simple step-by-step simulation. However, since there is currently no existing model that fulfills all these requirements, we introduce TD-VAE - a generative sequence model that can learn representations with explicit beliefs about states multiple steps ahead, and can be directly implemented without single-step transitions. TD-VAE is trained on pairs of time points that are separated in time, utilizing a form of temporal difference learning that is commonly used in reinforcement learning.",1
"Although end-to-end (E2E) learning has led to impressive progress on a variety of visual understanding tasks, it is often impeded by hardware constraints (e.g., GPU memory) and is prone to overfitting. When it comes to video captioning, one of the most challenging benchmark tasks in computer vision, those limitations of E2E learning are especially amplified by the fact that both the input videos and output captions are lengthy sequences. Indeed, state-of-the-art methods for video captioning process video frames by convolutional neural networks and generate captions by unrolling recurrent neural networks. If we connect them in an E2E manner, the resulting model is both memory-consuming and data-hungry, making it extremely hard to train. In this paper, we propose a multitask reinforcement learning approach to training an E2E video captioning model. The main idea is to mine and construct as many effective tasks (e.g., attributes, rewards, and the captions) as possible from the human captioned videos such that they can jointly regulate the search space of the E2E neural network, from which an E2E video captioning model can be found and generalized to the testing phase. To the best of our knowledge, this is the first video captioning model that is trained end-to-end from the raw video input to the caption output. Experimental results show that such a model outperforms existing ones to a large margin on two benchmark video captioning datasets.",0
"Despite the impressive progress that end-to-end (E2E) learning has achieved in various visual understanding tasks, it frequently faces limitations due to hardware constraints, such as GPU memory, and is susceptible to overfitting. For video captioning, one of the most challenging tasks in computer vision, these limitations of E2E learning are amplified as both input videos and output captions are lengthy sequences. The current state-of-the-art video captioning methods process video frames using convolutional neural networks and generate captions using unrolled recurrent neural networks. However, connecting these networks in an E2E manner results in a memory-consuming and data-hungry model that is tough to train. In this study, we propose a multitask reinforcement learning approach to train an E2E video captioning model, where we mine and construct multiple effective tasks, such as attributes, rewards, and captions, from human-captioned videos to jointly regulate the E2E neural network's search space. This approach enables us to find an E2E video captioning model that can be generalized to testing phases. To the best of our knowledge, this is the first video captioning model that is trained end-to-end from raw video input to caption output. Experimental results indicate that this model outperforms existing ones by a considerable margin on two benchmark video captioning datasets.",1
"Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation.",0
"To learn a good policy through model-free reinforcement learning (RL), particularly in environments with sparse rewards, a large number of trials are required. However, we have discovered a method that enhances sampling efficiency when there is access to demonstrations. Our technique, called Backplay, utilizes a solitary demonstration to create a curriculum for a given task. Instead of commencing every training episode at the fixed initial state of the environment, we initiate the agent near the end of the demonstration and gradually move the starting point backward during training until we reach the initial state. We have evaluated the effectiveness of Backplay in large grid worlds and a complex four-player zero-sum game (Pommerman) and have demonstrated that this approach is superior to other competitive methods that have been known to improve sample efficiency, such as reward shaping, behavioral cloning, and reverse curriculum generation. Furthermore, we have analytically characterized the types of environments where Backplay can improve training speed.",1
"Deploying the idea of long-term cumulative return, reinforcement learning has shown remarkable performance in various fields. We propose a formulation of the landmark localization in 3D medical images as a reinforcement learning problem. Whereas value-based methods have been widely used to solve similar problems, we adopt an actor-critic based direct policy search method framed in a temporal difference learning approach. Successful behavior learning is challenging in large state and/or action spaces, requiring many trials. We introduce a partial policy-based reinforcement learning to enable solving the large problem of localization by learning the optimal policy on smaller partial domains. Independent actors efficiently learn the corresponding partial policies, each utilizing their own independent critic. The proposed policy reconstruction from the partial policies ensures a robust and efficient localization utilizing the sub-agents solving simple binary decision problems in their corresponding partial action spaces. The proposed reinforcement learning requires a small number of trials to learn the optimal behavior compared with the original behavior learning scheme.",0
"Reinforcement learning has demonstrated impressive results in diverse fields by leveraging the concept of long-term cumulative return. In this study, we present a novel approach to address the problem of landmark localization in 3D medical images using reinforcement learning. Although value-based methods have been commonly utilized for similar tasks, we opted for an actor-critic based direct policy search technique that employs a temporal difference learning framework. Learning optimal behavior in large state and/or action spaces can be challenging and requires numerous trials. To overcome this hurdle, we introduce a partial policy-based reinforcement learning strategy that enables us to solve the localization problem by training optimal policies on smaller partial domains. Independent actors are used to learn the corresponding partial policies, each with its own independent critic. Combining the partial policies ensures robust and efficient localization by leveraging sub-agents that solve simple binary decision problems in their respective partial action spaces. Our proposed reinforcement learning method requires fewer trials to learn optimal behavior compared to the original learning scheme.",1
"This paper deals with the geometric multi-model fitting from noisy, unstructured point set data (e.g., laser scanned point clouds). We formulate multi-model fitting problem as a sequential decision making process. We then use a deep reinforcement learning algorithm to learn the optimal decisions towards the best fitting result. In this paper, we have compared our method against the state-of-the-art on simulated data. The results demonstrated that our approach significantly reduced the number of fitting iterations.",0
"The focus of this paper is on fitting geometric multi-models using imprecise, disorganized point sets like those derived from laser scanning. To accomplish this, we frame the multi-model fitting issue as a step-by-step decision-making process and utilize a deep reinforcement learning algorithm to determine the ideal decisions for the most favorable fitting outcome. Our methodology was pitted against current techniques using simulated data, and the findings demonstrated that our approach considerably decreased the number of fitting iterations required.",1
"We address the problem of person re-identification from commodity depth sensors. One challenge for depth-based recognition is data scarcity. Our first contribution addresses this problem by introducing split-rate RGB-to-Depth transfer, which leverages large RGB datasets more effectively than popular fine-tuning approaches. Our transfer scheme is based on the observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining layers need to be fine-tuned rapidly. Our second contribution enhances re-identification for video by implementing temporal attention as a Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is stochastic, the temporal attention parameters are trained using reinforcement learning. Extensive experiments validate the accuracy of our method in person re-identification from depth sequences. Finally, in a scenario where subjects wear unseen clothes, we show large performance gains compared to a state-of-the-art model which relies on RGB data.",0
"Our focus is on solving the issue of person re-identification using standard depth sensors. The primary challenge with depth-based recognition is the lack of data available. To address this, we introduce a split-rate RGB-to-Depth transfer method that utilizes large RGB datasets more effectively than traditional fine-tuning methods. Our approach involves sharing model parameters at the bottom layers of a deep convolutional neural network between RGB and depth data, while fine-tuning the remaining layers rapidly. Additionally, we enhance re-identification for video by implementing temporal attention using a Bernoulli-Sigmoid unit that acts upon frame-level features. To train the stochastic temporal attention parameters, we employ reinforcement learning. We conduct extensive experiments to validate the accuracy of our method in person re-identification from depth sequences. Finally, we demonstrate significant performance improvements for our approach compared to a state-of-the-art model that relies on RGB data, particularly in scenarios where subjects are wearing new clothing.",1
"We propose a general formulation for addressing reinforcement learning (RL) problems in settings with observational data. That is, we consider the problem of learning good policies solely from historical data in which unobserved factors (confounders) affect both observed actions and rewards. Our formulation allows us to extend a representative RL algorithm, the Actor-Critic method, to its deconfounding variant, with the methodology for this extension being easily applied to other RL algorithms. In addition to this, we develop a new benchmark for evaluating deconfounding RL algorithms by modifying the OpenAI Gym environments and the MNIST dataset. Using this benchmark, we demonstrate that the proposed algorithms are superior to traditional RL methods in confounded environments with observational data. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full RL problems with observational data. Code is available at https://github.com/CausalRL/DRL.",0
"Our proposal puts forward a general approach for tackling reinforcement learning (RL) problems in situations where only observational data is available. Specifically, we focus on the challenge of constructing effective policies based on historical data that is influenced by unobserved factors (known as confounders), which impact both the actions and rewards observed. Our formulation enables us to adapt the Actor-Critic method, a commonly used RL algorithm, to its deconfounding version. This method can also be easily applied to other RL algorithms. Furthermore, we introduce a new benchmark for evaluating deconfounding RL algorithms by modifying the OpenAI Gym environments and the MNIST dataset. Our results demonstrate that our proposed algorithms outperform traditional RL methods in confounded environments with observational data. This marks the first time that confounders have been considered in addressing complete RL problems with observational data. The code can be accessed at https://github.com/CausalRL/DRL.",1
"Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work we present a novel multi-timescale approach for constrained policy optimization, called `Reward Constrained Policy Optimization' (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies.",0
"It is difficult to solve tasks in Reinforcement Learning, as the agent's objective is to maximize the reward, which can lead to undesired behavior due to loopholes and errors in the reward signal. Although constraints can address this problem, a general solution for constraints does not exist. Our study proposes a new approach for constrained policy optimization called 'Reward Constrained Policy Optimization' (RCPO), which employs an alternative penalty signal to guide the policy towards satisfying the constraint. We demonstrate the effectiveness of our approach through empirical evidence and prove its convergence.",1
"In this paper, a new deep reinforcement learning based augmented general sequence tagging system is proposed. The new system contains two parts: a deep neural network (DNN) based sequence tagging model and a deep reinforcement learning (DRL) based augmented tagger. The augmented tagger helps improve system performance by modeling the data with minority tags. The new system is evaluated on SLU and NLU sequence tagging tasks using ATIS and CoNLL-2003 benchmark datasets, to demonstrate the new system's outstanding performance on general tagging tasks. Evaluated by F1 scores, it shows that the new system outperforms the current state-of-the-art model on ATIS dataset by 1.9% and that on CoNLL-2003 dataset by 1.4%.",0
"This paper presents a novel system for general sequence tagging that utilizes deep reinforcement learning. The system comprises of two components: a deep neural network-based sequence tagging model and a deep reinforcement learning-based augmented tagger. The augmented tagger enhances system performance by taking into account data with minor tags. Performance evaluation of the new system is conducted on SLU and NLU sequence tagging tasks using benchmark datasets such as ATIS and CoNLL-2003, which demonstrates its exceptional performance in general tagging tasks. The evaluation, measured by F1 scores, reveals that the new system surpasses the current state-of-the-art model on the ATIS dataset by 1.9% and the CoNLL-2003 dataset by 1.4%.",1
"We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data. We show that the improved performance is due to the increased diversity of reformulation strategies.",0
"Our proposed approach aims to efficiently teach varied strategies in reinforcement learning for query reformulation. This approach is applicable to document retrieval and question answering tasks. The agent comprises of multiple specialized sub-agents along with a meta-agent that learns to combine answers from sub-agents to generate the final answer. The sub-agents are trained on different partitions of the training data, whereas the meta-agent is trained on the entire training set. Our method is highly parallelizable, making the learning process quicker, and it offers better generalization performance compared to strong baselines like a full-data trained agent ensemble. Our approach's enhanced performance is attributed to the increased diversity of reformulation strategies.",1
"Recent breakthroughs in Go play and strategic games have witnessed the great potential of reinforcement learning in intelligently scheduling in uncertain environment, but some bottlenecks are also encountered when we generalize this paradigm to universal complex tasks. Among them, the low efficiency of data utilization in model-free reinforcement algorithms is of great concern. In contrast, the model-based reinforcement learning algorithms can reveal underlying dynamics in learning environments and seldom suffer the data utilization problem. To address the problem, a model-based reinforcement learning algorithm with attention mechanism embedded is proposed as an extension of World Models in this paper. We learn the environment model through Mixture Density Network Recurrent Network(MDN-RNN) for agents to interact, with combinations of variational auto-encoder(VAE) and attention incorporated in state value estimates during the process of learning policy. In this way, agent can learn optimal policies through less interactions with actual environment, and final experiments demonstrate the effectiveness of our model in control problem.",0
"Reinforcement learning has shown great potential in scheduling tasks in uncertain environments, as seen in recent advancements in Go play and strategic games. However, there are challenges in applying this paradigm to complex tasks. The data utilization problem, which affects the efficiency of model-free reinforcement algorithms, is a significant concern. Model-based reinforcement learning algorithms, on the other hand, can identify underlying dynamics in learning environments and are not affected by this problem. To address this issue, this paper proposes an extension of World Models, a model-based reinforcement learning algorithm with an embedded attention mechanism. The environment model is learned using a Mixture Density Network Recurrent Network (MDN-RNN), with state value estimates incorporating a combination of variational auto-encoder (VAE) and attention during policy learning. With this approach, the agent can learn optimal policies with fewer interactions with the actual environment. The final experiments demonstrate the effectiveness of this model in control problems.",1
"Reinforcement learning agents need exploratory behaviors to escape from local optima. These behaviors may include both immediate dithering perturbation and temporally consistent exploration. To achieve these, a stochastic policy model that is inherently consistent through a period of time is in desire, especially for tasks with either sparse rewards or long term information. In this work, we introduce a novel on-policy temporally consistent exploration strategy - Neural Adaptive Dropout Policy Exploration (NADPEx) - for deep reinforcement learning agents. Modeled as a global random variable for conditional distribution, dropout is incorporated to reinforcement learning policies, equipping them with inherent temporal consistency, even when the reward signals are sparse. Two factors, gradients' alignment with the objective and KL constraint in policy space, are discussed to guarantee NADPEx policy's stable improvement. Our experiments demonstrate that NADPEx solves tasks with sparse reward while naive exploration and parameter noise fail. It yields as well or even faster convergence in the standard mujoco benchmark for continuous control.",0
"To avoid getting stuck in local optima, reinforcement learning agents require exploratory behaviors. These behaviors can involve immediate perturbation and consistent exploration over time. A stochastic policy model that maintains consistency over time is particularly useful for tasks with sparse rewards or long-term information. This paper introduces a new exploration strategy for deep reinforcement learning agents called Neural Adaptive Dropout Policy Exploration (NADPEx). NADPEx uses dropout as a global random variable in the conditional distribution to provide temporal consistency to reinforcement learning policies, even when rewards are sparse. The paper discusses two factors, namely gradients' alignment with the objective and KL constraint in policy space, to ensure stable improvement of the NADPEx policy. The experiments conducted in this research show that NADPEx is effective in solving tasks with sparse rewards, outperforming naive exploration and parameter noise methods. Additionally, NADPEx achieves comparable or faster convergence in the standard mujoco benchmark for continuous control.",1
"We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.",0
"A new methodology called Supervised Policy Update (SPU) is proposed for deep reinforcement learning, which is efficient in terms of sample usage. The SPU methodology generates data from the current policy and then solves a constrained optimization problem in the non-parameterized proximal policy space. The optimal non-parameterized policy is then converted into a parameterized policy using supervised regression, from which new samples are drawn. This methodology is versatile, as it can be applied to both discrete and continuous action spaces and can handle various proximity constraints for the non-parameterized optimization problem. The SPU methodology can address problems such as the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems and the Proximal Policy Optimization (PPO) problem. Compared to TRPO, SPU implementation is much simpler and more sample-efficient. Extensive experiments have shown that SPU outperforms TRPO in Mujoco simulated robotic tasks and PPO in Atari video game tasks.",1
"Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by learning directly from image input. A deep neural network is used as a function approximator and requires no specific state information. However, one drawback of using only images as input is that this approach requires a prohibitively large amount of training time and data for the model to learn the state feature representation and approach reasonable performance. This is not feasible in real-world applications, especially when the data are expansive and training phase could introduce disasters that affect human safety. In this work, we use a human demonstration approach to speed up training for learning features and use the resulting pre-trained model to replace the neural network in the deep RL Deep Q-Network (DQN), followed by human interaction to further refine the model. We empirically evaluate our approach by using only a human demonstration model and modified DQN with human demonstration model included in the Microsoft AirSim car simulator. Our results show that (1) pre-training with human demonstration in a supervised learning approach is better and much faster at discovering features than DQN alone, (2) initializing the DQN with a pre-trained model provides a significant improvement in training time and performance even with limited human demonstration, and (3) providing the ability for humans to supply suggestions during DQN training can speed up the network's convergence on an optimal policy, as well as allow it to learn more complex policies that are harder to discover by random exploration.",0
"By learning directly from image input, deep reinforcement learning (deep RL) has shown remarkable success in complex sequential tasks. A deep neural network is employed as a function approximator, which eliminates the need for specific state information. However, relying solely on images as input poses a significant challenge as it requires a substantial amount of training time and data for the model to learn the state feature representation and achieve reasonable performance. This is not practical in real-world applications, especially when the data is extensive, and the training phase could result in disasters that endanger human safety. To tackle this issue, we propose a human demonstration approach to accelerate the training process for learning features. We use the resulting pre-trained model to replace the neural network in the deep RL Deep Q-Network (DQN) and further refine the model through human interaction. Our empirical evaluation, which uses only a human demonstration model and a modified DQN with the human demonstration model included in the Microsoft AirSim car simulator, demonstrates that pre-training with human demonstration in a supervised learning approach is better and much faster at discovering features than DQN alone. In addition, initializing the DQN with a pre-trained model significantly enhances training time and performance, even with limited human demonstration. Finally, we show that enabling humans to provide suggestions during DQN training can accelerate the network's convergence on an optimal policy and enable it to learn more complex policies that are challenging to discover by random exploration.",1
"We consider a framework involving behavioral economics and machine learning. Rationally inattentive Bayesian agents make decisions based on their posterior distribution, utility function and information acquisition cost Renyi divergence which generalizes Shannon mutual information). By observing these decisions, how can an observer estimate the utility function and information acquisition cost? Using deep learning, we estimate framing information (essential extrinsic features) that determines the agent's attention strategy. Then we present a preference based inverse reinforcement learning algorithm to test for rational inattention: is the agent an utility maximizer, attention maximizer, and does an information cost function exist that rationalizes the data? The test imposes a Renyi mutual information constraint which impacts how the agent can select attention strategies to maximize their expected utility. The test provides constructive estimates of the utility function and information acquisition cost of the agent. We illustrate these methods on a massive YouTube dataset for characterizing the commenting behavior of users.",0
"Our approach combines behavioral economics and machine learning to form a framework. Our focus is on rationally inattentive Bayesian agents who base their decisions on their posterior distribution, utility function, and information acquisition cost. To estimate the utility function and information acquisition cost, we examine the agent's decisions and use deep learning to estimate framing information. This framing information determines the agent's attention strategy. We then use a preference-based inverse reinforcement learning algorithm to test for rational inattention and determine if the agent is a utility maximizer, attention maximizer, and if there exists an information cost function that rationalizes the data. The test takes into account a Renyi mutual information constraint that impacts the agent's selection of attention strategies to maximize their expected utility. Our method provides constructive estimates of the utility function and information acquisition cost of the agent. We demonstrate the effectiveness of our approach using a vast YouTube dataset to analyze user commenting behavior.",1
"Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions on World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.",0
"Reinforcement Learning (RL) agents may encounter difficulties in learning when presented with environments that have large state and action spaces and sparse rewards. For example, natural language instructions on the Web, like booking a flight ticket, can lead to RL settings with a vast input vocabulary and a high number of actionable elements on a page. Although recent approaches have improved the success rate on simpler environments with human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. Our proposed guided RL approaches aim to overcome these challenges by generating an unbounded amount of experience for an agent to learn from. Instead of learning from a complex instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum that gradually increases the subset of these relatively easier sub-instructions. Additionally, when expert demonstrations are unavailable, our novel meta-learning framework generates new instruction-following tasks and trains the agent more effectively. We train a DQN deep reinforcement learning agent using a QWeb neural network architecture to approximate the Q-value function on these smaller, synthetic instructions. Our agent's ability to generalize to new instructions is evaluated on the World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without any human demonstration, achieving a 100% success rate on several challenging environments.",1
"Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly understood. Existing methods either neglect credit assignment to pre-adaptation behavior or implement it naively. This leads to poor sample-efficiency during meta-training as well as ineffective task identification strategies. This paper provides a theoretical analysis of credit assignment in gradient-based Meta-RL. Building on the gained insights we develop a novel meta-learning algorithm that overcomes both the issue of poor credit assignment and previous difficulties in estimating meta-policy gradients. By controlling the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm endows efficient and stable meta-learning. Our approach leads to superior pre-adaptation policy behavior and consistently outperforms previous Meta-RL algorithms in sample-efficiency, wall-clock time, and asymptotic performance.",0
"The understanding of credit assignment in Meta-reinforcement learning (Meta-RL) is inadequate, with existing methods overlooking credit assignment for pre-adaptation behavior or implementing it in a simplistic manner. This results in low sample-efficiency during meta-training and ineffective task identification strategies. This article presents a theoretical examination of credit assignment in gradient-based Meta-RL. Based on the acquired insights, a novel meta-learning algorithm is developed that addresses the issues of poor credit assignment and previous difficulties in estimating meta-policy gradients. By regulating the statistical distance of both pre-adaptation and adapted policies during meta-policy search, the proposed algorithm enables efficient and stable meta-learning. Our approach results in superior pre-adaptation policy behavior, outperforming previous Meta-RL algorithms in terms of sample-efficiency, wall-clock time, and asymptotic performance on a consistent basis.",1
"This paper proposes an efficient neural network (NN) architecture design methodology called Chameleon that honors given resource constraints. Instead of developing new building blocks or using computationally-intensive reinforcement learning algorithms, our approach leverages existing efficient network building blocks and focuses on exploiting hardware traits and adapting computation resources to fit target latency and/or energy constraints. We formulate platform-aware NN architecture search in an optimization framework and propose a novel algorithm to search for optimal architectures aided by efficient accuracy and resource (latency and/or energy) predictors. At the core of our algorithm lies an accuracy predictor built atop Gaussian Process with Bayesian optimization for iterative sampling. With a one-time building cost for the predictors, our algorithm produces state-of-the-art model architectures on different platforms under given constraints in just minutes. Our results show that adapting computation resources to building blocks is critical to model performance. Without the addition of any bells and whistles, our models achieve significant accuracy improvements against state-of-the-art hand-crafted and automatically designed architectures. We achieve 73.8% and 75.3% top-1 accuracy on ImageNet at 20ms latency on a mobile CPU and DSP. At reduced latency, our models achieve up to 8.5% (4.8%) and 6.6% (9.3%) absolute top-1 accuracy improvements compared to MobileNetV2 and MnasNet, respectively, on a mobile CPU (DSP), and 2.7% (4.6%) and 5.6% (2.6%) accuracy gains over ResNet-101 and ResNet-152, respectively, on an Nvidia GPU (Intel CPU).",0
"The Chameleon methodology proposed in this paper offers an efficient approach for designing neural network (NN) architectures that meet specific resource constraints. Unlike other methods that require the development of new building blocks or rely on computationally-intensive reinforcement learning algorithms, Chameleon leverages existing network building blocks and focuses on adapting computation resources to fit target latency and/or energy constraints. The proposed algorithm employs an optimization framework to search for optimal architectures while also utilizing efficient accuracy and resource predictors. The algorithm includes an accuracy predictor that uses Gaussian Process with Bayesian optimization for iterative sampling. The algorithm is able to produce state-of-the-art model architectures on different platforms under given constraints in just minutes, with a one-time building cost for the predictors. Results indicate that adapting computation resources to building blocks is crucial for model performance, as the models produced by Chameleon achieve significant accuracy improvements against both hand-crafted and automatically designed architectures. For example, on ImageNet at 20ms latency on a mobile CPU and DSP, the models achieve top-1 accuracy of 73.8% and 75.3%, respectively. At reduced latency, the models achieve up to 8.5% (4.8%) and 6.6% (9.3%) absolute top-1 accuracy improvements compared to MobileNetV2 and MnasNet, respectively, on a mobile CPU (DSP), and 2.7% (4.6%) and 5.6% (2.6%) accuracy gains over ResNet-101 and ResNet-152, respectively, on an Nvidia GPU (Intel CPU).",1
"Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by using deep neural networks as function approximators to learn directly from raw input images. However, learning directly from raw images is data inefficient. The agent must learn feature representation of complex states in addition to learning a policy. As a result, deep RL typically suffers from slow learning speeds and often requires a prohibitively large amount of training time and data to reach reasonable performance, making it inapplicable to real-world settings where data is expensive. In this work, we improve data efficiency in deep RL by addressing one of the two learning goals, feature learning. We leverage supervised learning to pre-train on a small set of non-expert human demonstrations and empirically evaluate our approach using the asynchronous advantage actor-critic algorithms (A3C) in the Atari domain. Our results show significant improvements in learning speed, even when the provided demonstration is noisy and of low quality.",0
"The use of deep neural networks as function approximators in deep reinforcement learning (deep RL) has led to exceptional performance in complex sequential tasks by learning directly from raw input images. However, this approach is not efficient in terms of data utilization as the agent must learn complex state feature representation in addition to policy learning. Consequently, deep RL is hindered by slow learning and requires a large amount of training time and data to achieve satisfactory performance, making it impractical in real-world scenarios where data is expensive. This study aims to enhance data efficiency in deep RL by focusing on feature learning, one of the two learning goals. To achieve this, we utilize supervised learning to pre-train on a small set of non-expert human demonstrations and apply the asynchronous advantage actor-critic algorithms (A3C) in the Atari domain to evaluate our approach empirically. Our findings show significant improvements in learning speed, even when the demonstrations provided are noisy and of low quality.",1
"Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.",0
"Current interactive image retrieval methods have shown that incorporating user feedback can enhance retrieval outcomes. However, most existing systems rely on limited forms of feedback, such as binary relevance responses or fixed sets of relative attributes, which restricts their effectiveness. To address this issue, we introduce a new approach to interactive image search that allows users to provide feedback through natural language, enabling more natural and efficient interaction. We frame the task of dialog-based interactive image retrieval as a reinforcement learning problem, where the dialog system is rewarded for improving the rank of the target image in each turn. To overcome the challenges and costs of collecting human-machine conversations during the system's learning, we train our approach with a user simulator that can differentiate between target and candidate images. We demonstrate the effectiveness of our approach in a footwear retrieval application. Both simulated and real-world experiments show that our learning framework outperforms other supervised and reinforcement learning baselines, and that user feedback based on natural language produces more effective retrieval results and a more natural and expressive communication interface.",1
"The ability of a reinforcement learning (RL) agent to learn about many reward functions at the same time has many potential benefits, such as the decomposition of complex tasks into simpler ones, the exchange of information between tasks, and the reuse of skills. We focus on one aspect in particular, namely the ability to generalise to unseen tasks. Parametric generalisation relies on the interpolation power of a function approximator that is given the task description as input; one of its most common form are universal value function approximators (UVFAs). Another way to generalise to new tasks is to exploit structure in the RL problem itself. Generalised policy improvement (GPI) combines solutions of previous tasks into a policy for the unseen task; this relies on instantaneous policy evaluation of old policies under the new reward function, which is made possible through successor features (SFs). Our proposed universal successor features approximators (USFAs) combine the advantages of all of these, namely the scalability of UVFAs, the instant inference of SFs, and the strong generalisation of GPI. We discuss the challenges involved in training a USFA, its generalisation properties and demonstrate its practical benefits and transfer abilities on a large-scale domain in which the agent has to navigate in a first-person perspective three-dimensional environment.",0
"Learning multiple reward functions simultaneously through reinforcement learning (RL) offers several advantages, such as breaking down complex tasks into simpler ones, sharing information between tasks, and reusing skills. Our focus is on the ability of RL agents to generalize to new tasks. One method of parametric generalization involves using a function approximator, such as universal value function approximators (UVFAs), which can interpolate based on the task description. Another approach is to leverage the structure of the RL problem itself through generalised policy improvement (GPI), which combines solutions from previous tasks to create a policy for the new task. This is made possible by using successor features (SFs) for instantaneous policy evaluation. Our proposed universal successor features approximators (USFAs) combine the benefits of both UVFAs and SFs, offering scalability, instant inference, and strong generalization. We discuss the challenges associated with training a USFA, its generalization properties, and demonstrate its practical benefits and transfer abilities in a large-scale 3D navigation task.",1
"Deep reinforcement learning agents have recently been successful across a variety of discrete and continuous control tasks; however, they can be slow to train and require a large number of interactions with the environment to learn a suitable policy. This is borne out by the fact that a reinforcement learning agent has no prior knowledge of the world, no pre-existing data to depend on and so must devote considerable time to exploration. Transfer learning can alleviate some of the problems by leveraging learning done on some source task to help learning on some target task. Our work presents an algorithm for initialising the hidden feature representation of the target task. We propose a domain adaptation method to transfer state representations and demonstrate transfer across domains, tasks and action spaces. We utilise adversarial domain adaptation ideas combined with an adversarial autoencoder architecture. We align our new policies' representation space with a pre-trained source policy, taking target task data generated from a random policy. We demonstrate that this initialisation step provides significant improvement when learning a new reinforcement learning task, which highlights the wide applicability of adversarial adaptation methods; even as the task and label/action space also changes.",0
"Although deep reinforcement learning agents have shown success in both discrete and continuous control tasks, they can be slow to train and require extensive interactions with the environment to learn a suitable policy. This is due to the fact that they have no prior knowledge or pre-existing data, and therefore require a considerable amount of exploration time. To mitigate these issues, transfer learning can be used to leverage learning from a source task to aid in learning a target task. Our research introduces an algorithm for initializing the hidden feature representation of the target task and proposes a domain adaptation method to transfer state representations. We demonstrate transfer across domains, tasks, and action spaces by utilizing adversarial domain adaptation ideas with an adversarial autoencoder architecture. Our approach aligns the representation space of the new policies with a pre-trained source policy, using target task data generated from a random policy. We show that this initialization step significantly improves the learning of new reinforcement learning tasks, highlighting the broad applicability of adversarial adaptation methods even as the task and label/action space changes.",1
"Gradient-based meta-learners such as MAML are able to learn a meta-prior from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. One important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML algorithm that is able to modulate its meta-learned prior according to the identified task, allowing faster adaptation. We evaluate the proposed model on a diverse set of problems including regression, few-shot image classification, and reinforcement learning. The results demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks sampled from a multimodal distribution.",0
"Meta-learners like MAML use gradients to learn a meta-prior from similar tasks, enabling them to adapt to new tasks from the same distribution with minimal gradient updates. However, these frameworks are limited in their ability to learn from diverse task distributions because they rely on a common initialization shared across the entire distribution. In this study, we introduce a multimodal MAML algorithm that can identify tasks from a multimodal distribution and quickly adapt using gradient updates. Our algorithm modulates the meta-learned prior based on the identified task, resulting in faster adaptation. We evaluate our model on various problems, including regression, few-shot image classification, and reinforcement learning, and show that it effectively modulates the meta-learned prior in response to the characteristics of tasks from a multimodal distribution.",1
"Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are generally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. Designing such architectures requires significant human expertise, substantial computation time and doesn't always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search.",0
"Efficient and flexible, Deep Neural Networks excel in various tasks, including image and speech recognition, as well as natural language understanding. Researchers, in particular, show great interest in Convolutional Neural Networks (CNN) for computer vision and classification tasks. The CNN architecture and its hyperparameters depend on the task at hand, with the network extracting complex and relevant features for optimal convergence. However, designing such architectures requires significant expertise and computation time, and may not always result in an optimal network. Despite extensive research in machine learning, there is no standardized automatic method for model configuration. This survey reviews and discusses the ongoing progress in automating CNN architecture search.",1
"Recognition of human environment with computer systems always was a big deal in artificial intelligence. In this area handwriting recognition and conceptualization of it to computer is an important area in it. In the past years with growth of machine learning in artificial intelligence, efforts to using this technique increased. In this paper is tried to using fuzzy controller, to optimizing amount of reward of reinforcement learning for recognition of handwritten digits. For this aim first a sample of every digit with 10 standard computer fonts, given to actor and then actor is trained. In the next level is tried to test the actor with dataset and then results show improvement of recognition when using fuzzy controller of reinforcement learning.",0
"Artificial intelligence has always faced challenges in recognizing the human environment using computer systems. One key area of focus has been handwriting recognition and its conceptualization in computers. With the rise of machine learning in recent years, there has been an increased effort to leverage this technique. This paper explores the use of fuzzy controllers to optimize the reward of reinforcement learning for handwritten digit recognition. The process involved providing the actor with samples of each digit using 10 standard computer fonts, followed by training. The actor was then tested with a dataset, and the results showed improved recognition when using the fuzzy controller for reinforcement learning.",1
"Mobile edge computing (MEC) emerges recently as a promising solution to relieve resource-limited mobile devices from computation-intensive tasks, which enables devices to offload workloads to nearby MEC servers and improve the quality of computation experience. Nevertheless, by considering a MEC system consisting of multiple mobile users with stochastic task arrivals and wireless channels in this paper, the design of computation offloading policies is challenging to minimize the long-term average computation cost in terms of power consumption and buffering delay. A deep reinforcement learning (DRL) based decentralized dynamic computation offloading strategy is investigated to build a scalable MEC system with limited feedback. Specifically, a continuous action space-based DRL approach named deep deterministic policy gradient (DDPG) is adopted to learn efficient computation offloading policies independently at each mobile user. Thus, powers of both local execution and task offloading can be adaptively allocated by the learned policies from each user's local observation of the MEC system. Numerical results are illustrated to demonstrate that efficient policies can be learned at each user, and performance of the proposed DDPG based decentralized strategy outperforms the conventional deep Q-network (DQN) based discrete power control strategy and some other greedy strategies with reduced computation cost. Besides, the power-delay tradeoff is also analyzed for both the DDPG based and DQN based strategies.",0
"Recently, Mobile Edge Computing (MEC) has emerged as a potential solution for relieving resource-limited mobile devices from computation-heavy tasks. This allows devices to offload their workloads to nearby MEC servers, improving the quality of the computation experience. However, this paper considers a MEC system with multiple mobile users, who experience stochastic task arrivals and wireless channels. The challenge is to design computation offloading policies to minimize the long-term average computation cost based on power consumption and buffering delay. To build a scalable MEC system with limited feedback, this paper investigates a Deep Reinforcement Learning (DRL) based decentralized dynamic computation offloading strategy. Specifically, a continuous action space-based DRL approach called Deep Deterministic Policy Gradient (DDPG) is adopted to learn efficient computation offloading policies independently at each mobile user. This approach allows powers of both local execution and task offloading to be adaptively allocated by the learned policies from each user's local observation of the MEC system. Numerical results demonstrate that efficient policies can be learned at each user, and the proposed DDPG-based decentralized strategy outperforms the conventional Deep Q-Network (DQN) based discrete power control strategy and some other greedy strategies with reduced computation cost. Additionally, the power-delay tradeoff is analyzed for both DDPG-based and DQN-based strategies.",1
"Proximal policy optimization(PPO) has been proposed as a first-order optimization method for reinforcement learning. We should notice that an exterior penalty method is used in it. Often, the minimizers of the exterior penalty functions approach feasibility only in the limits as the penalty parameter grows increasingly large. Therefore, it may result in the low level of sampling efficiency. This method, which we call proximal policy optimization with barrier method (PPO-B), keeps almost all advantageous spheres of PPO such as easy implementation and good generalization. Specifically, a new surrogate objective with interior penalty method is proposed to avoid the defect arose from exterior penalty method. Conclusions can be draw that PPO-B is able to outperform PPO in terms of sampling efficiency since PPO-B achieved clearly better performance on Atari and Mujoco environment than PPO.",0
"Proximal policy optimization (PPO) has been suggested as a primary optimization approach for reinforcement learning. It should be noted that PPO uses an external penalty method, which may result in low sampling efficiency as the minimizers of external penalty functions only approach feasibility when the penalty parameter grows increasingly large. However, a new method called PPO with barrier method (PPO-B) preserves the advantageous aspects of PPO, such as easy implementation and good generalization. PPO-B introduces a new surrogate objective with an internal penalty method to avoid the limitations of the external penalty method. Results show that PPO-B outperforms PPO in terms of sampling efficiency, as evidenced by its superior performance in Atari and Mujoco environments.",1
"We consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that is suitable for problems of practical scale. Our approach builds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016). We demonstrate that, for simple tabular contexts, the approach is competitive with previously proposed tabular model learning methods (Dimakopoulou and Van Roy, 2018). With a higher-dimensional problem and a neural network value function representation, the approach learns quickly with far fewer agents than alternative exploration schemes.",0
"We have created a method for coordinated exploration that is practical for a team of reinforcement learning agents operating in a shared environment. Our method uses seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value function learning (Osband et al., 2016), and is suitable for larger problems. In our tests, the approach performed similarly to other tabular model learning methods for simple contexts. However, when applied to more complex problems with a neural network value function representation, our approach was able to learn quickly with fewer agents than other exploration schemes.",1
"Deep Reinforcement Learning has shown tremendous success in solving several games and tasks in robotics. However, unlike humans, it generally requires a lot of training instances. Trajectories imitating to solve the task at hand can help to increase sample-efficiency of deep RL methods. In this paper, we present a simple approach to use such trajectories, applied to the challenging Ball-in-Maze Games, recently introduced in the literature. We show that in spite of not using human-generated trajectories and just using the simulator as a model to generate a limited number of trajectories, we can get a speed-up of about 2-3x in the learning process. We also discuss some challenges we observed while using trajectory-based learning for very sparse reward functions.",0
"Although Deep Reinforcement Learning has proven to be extremely effective in tackling various games and robotic tasks, it typically requires an extensive amount of training compared to humans. The utilization of trajectories that mimic the resolution of a given task can enhance the sample efficiency of deep RL techniques. This article presents a straightforward approach for implementing such trajectories in the Ball-in-Maze Games, which are notoriously difficult and were recently introduced in literature. Despite not relying on human-generated trajectories and instead utilizing a simulator to generate only a limited number of trajectories, we were able to expedite the learning process by approximately 2-3x. Additionally, we address some of the challenges encountered when using trajectory-based learning for highly sparse reward functions.",1
"In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.",0
"It has been proven that deep learning models can be trained with large batch sizes without losing data efficiency in many fields. However, the limits of this data parallelism vary from domain to domain, with ImageNet using batches of tens of thousands and Dota 2 RL agents using batches of millions. It is unclear why these limits vary and how to choose the correct batch size for a new domain. This paper introduces the gradient noise scale, a simple measure that predicts the largest useful batch size across various domains (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word, Atari, Dota) and generative model training. The noise scale increases as the loss decreases and depends on the model size. The theory also explains the tradeoff between compute-efficiency and time-efficiency and provides a rough model of adaptive batch-size training benefits.",1
"Deep reinforcement learning (deep RL) research has grown significantly in recent years. A number of software offerings now exist that provide stable, comprehensive implementations for benchmarking. At the same time, recent deep RL research has become more diverse in its goals. In this paper we introduce Dopamine, a new research framework for deep RL that aims to support some of that diversity. Dopamine is open-source, TensorFlow-based, and provides compact and reliable implementations of some state-of-the-art deep RL agents. We complement this offering with a taxonomy of the different research objectives in deep RL research. While by no means exhaustive, our analysis highlights the heterogeneity of research in the field, and the value of frameworks such as ours.",0
"In recent years, there has been significant growth in the field of deep reinforcement learning (deep RL) research. There are now several software options available that offer reliable, comprehensive implementations for benchmarking purposes. Additionally, deep RL research has become more varied in its objectives. This paper introduces Dopamine, a new research framework for deep RL that seeks to support this diversity. It is open-source, based on TensorFlow, and includes dependable implementations of some of the latest deep RL agents. We also provide a classification of the various research goals in deep RL research, which demonstrates the wide range of research in the field and the usefulness of frameworks like Dopamine. While not exhaustive, our analysis highlights the heterogeneity of the research and the importance of such frameworks.",1
"Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model's original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent's trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location. Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.",0
"Homomorphic encryption and secure multiparty computation are currently the only methods available for sharing governance of a deep learning model. However, their practicality is limited due to their significant computational and communication overheads, rendering them unsuitable for the training of large neural networks. To address this issue, we propose a scalable technique for shared model governance by dividing the deep learning model among multiple parties. This paper explores the security guarantee of this approach by introducing the model completion problem, which determines how much training is required to restore a trained deep learning model's original performance using a subset of its parameters and the entire training data set or an environment simulator. We establish a metric for evaluating the difficulty of the model completion problem and conduct experiments on supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our findings indicate that the model completion problem is more challenging in reinforcement learning than in supervised learning due to the absence of the trained agent's trajectories. Furthermore, the degree of difficulty is predominantly determined by the type and location of the missing parameters rather than the number of parameters. Our results suggest that model splitting may be a viable technique for shared model governance in settings where training is prohibitively expensive.",1
"Deep reinforcement-learning methods have achieved remarkable performance on challenging control tasks. Observations of the resulting behavior give the impression that the agent has constructed a generalized representation that supports insightful action decisions. We re-examine what is meant by generalization in RL, and propose several definitions based on an agent's performance in on-policy, off-policy, and unreachable states. We propose a set of practical methods for evaluating agents with these definitions of generalization. We demonstrate these techniques on a common benchmark task for deep RL, and we show that the learned networks make poor decisions for states that differ only slightly from on-policy states, even though those states are not selected adversarially. Taken together, these results call into question the extent to which deep Q-networks learn generalized representations, and suggest that more experimentation and analysis is necessary before claims of representation learning can be supported.",0
"Impressive results have been achieved in challenging control tasks through deep reinforcement-learning methods. The behavior observed indicates that the agent has developed a comprehensive representation that enables it to make insightful decisions. In this study, we revisit the concept of generalization in RL and put forward various definitions based on the agent's performance in on-policy, off-policy, and unreachable states. We offer practical techniques for assessing the agents using these definitions of generalization. Using a standard deep RL benchmark task, we show that the learned networks make poor decisions for states that are only slightly different from on-policy states, even when those states are not deliberately chosen to be adversarial. These outcomes highlight the need for further experimentation and analysis to support claims of representation learning and raise concerns regarding the extent to which deep Q-networks learn generalized representations.",1
"We consider the problem of building a state representation model in a continual fashion. As the environment changes, the aim is to efficiently compress the sensory state's information without losing past knowledge. The learned features are then fed to a Reinforcement Learning algorithm to learn a policy. We propose to use Variational Auto-Encoders for state representation, and Generative Replay, i.e. the use of generated samples, to maintain past knowledge. We also provide a general and statistically sound method for automatic environment change detection. Our method provides efficient state representation as well as forward transfer, and avoids catastrophic forgetting. The resulting model is capable of incrementally learning information without using past data and with a bounded system size.",0
"Our focus is on developing a continuous state representation model that can adapt to changes in the environment. Our goal is to compress the sensory state's information in a way that preserves previous knowledge while accommodating new information efficiently. To achieve this, we propose a method that utilizes Variational Auto-Encoders for state representation and Generative Replay to retain past knowledge through generated samples. Additionally, we introduce an effective and reliable approach to detecting changes in the environment automatically. Our model enables efficient state representation, forward transfer, and prevents catastrophic forgetting, allowing for incremental learning without relying on prior data and with a bounded system size.",1
"Efficient Reinforcement Learning usually takes advantage of demonstration or good exploration strategy. By applying posterior sampling in model-free RL under the hypothesis of GP, we propose Gaussian Process Posterior Sampling Reinforcement Learning(GPPSTD) algorithm in continuous state space, giving theoretical justifications and empirical results. We also provide theoretical and empirical results that various demonstration could lower expected uncertainty and benefit posterior sampling exploration. In this way, we combined the demonstration and exploration process together to achieve a more efficient reinforcement learning.",0
"A more efficient way of implementing Reinforcement Learning is achieved through the utilization of demonstration or a well-planned exploration strategy. Our proposed algorithm, Gaussian Process Posterior Sampling Reinforcement Learning (GPPSTD), utilizes posterior sampling in model-free RL, based on the GP hypothesis, for continuous state space. We have provided both theoretical and empirical evidence that different types of demonstration can decrease expected uncertainty and improve posterior sampling exploration. By combining these processes, we have achieved a more efficient Reinforcement Learning approach.",1
"A key challenge for gradient based optimization methods in model-free reinforcement learning is to develop an approach that is sample efficient and has low variance. In this work, we apply Kronecker-factored curvature estimation technique (KFAC) to a recently proposed gradient estimator for control variate optimization, RELAX, to increase the sample efficiency of using this gradient estimation method in reinforcement learning. The performance of the proposed method is demonstrated on a synthetic problem and a set of three discrete control task Atari games.",0
"Developing a sample-efficient and low variance approach is a significant hurdle for gradient-based optimization methods in model-free reinforcement learning. This study employs the Kronecker-factored curvature estimation technique (KFAC) to enhance the sample efficiency of the gradient estimator for control variate optimization, RELAX, in reinforcement learning. The performance of this method is evaluated on a synthetic problem and a series of three discrete control task Atari games.",1
"We propose a method for modeling and learning turn-taking behaviors for accessing a shared resource. We model the individual behavior for each agent in an interaction and then use a multi-agent fusion model to generate a summary over the expected actions of the group to render the model independent of the number of agents. The individual behavior models are weighted finite state transducers (WFSTs) with weights dynamically updated during interactions, and the multi-agent fusion model is a logistic regression classifier.   We test our models in a multi-agent tower-building environment, where a Q-learning agent learns to interact with rule-based agents. Our approach accurately models the underlying behavior patterns of the rule-based agents with accuracy ranging between 0.63 and 1.0 depending on the stochasticity of the other agent behaviors. In addition we show using KL-divergence that the model accurately captures the distribution of next actions when interacting with both a single agent (KL-divergence < 0.1) and with multiple agents (KL-divergence < 0.37). Finally, we demonstrate that our behavior model can be used by a Q-learning agent to take turns in an interactive turn-taking environment.",0
"Our proposed method involves modeling and learning turn-taking behaviors for accessing a shared resource. We create individual behavior models for each agent involved, and then use a multi-agent fusion model to generate a summary of expected actions from the group. This makes the model independent of the number of agents involved. The individual behavior models are WFSTs with weights that update dynamically during interactions, while the multi-agent fusion model is a logistic regression classifier. To test our models, we use a multi-agent tower-building environment where a Q-learning agent learns to interact with rule-based agents. Our approach accurately models the behavior patterns of the rule-based agents, with accuracy ranging from 0.63 to 1.0 depending on the stochasticity of the other agent behaviors. We also demonstrate that our model captures the distribution of next actions when interacting with both a single agent (KL-divergence < 0.1) and multiple agents (KL-divergence < 0.37). Finally, we show that our behavior model can be used by a Q-learning agent to take turns in an interactive turn-taking environment.",1
"We propose a new method for learning from a single demonstration to solve hard exploration tasks like the Atari game Montezuma's Revenge. Instead of imitating human demonstrations, as proposed in other recent works, our approach is to maximize rewards directly. Our agent is trained using off-the-shelf reinforcement learning, but starts every episode by resetting to a state from a demonstration. By starting from such demonstration states, the agent requires much less exploration to learn a game compared to when it starts from the beginning of the game at every episode. We analyze reinforcement learning for tasks with sparse rewards in a simple toy environment, where we show that the run-time of standard RL methods scales exponentially in the number of states between rewards. Our method reduces this to quadratic scaling, opening up many tasks that were previously infeasible. We then apply our method to Montezuma's Revenge, for which we present a trained agent achieving a high-score of 74,500, better than any previously published result.",0
"Our proposed approach offers a novel solution to tackling challenging exploration tasks like Montezuma's Revenge in Atari games. Unlike other recent works that rely on imitating human demonstrations, our method directly maximizes rewards. We leverage off-the-shelf reinforcement learning to train our agent, which begins each episode by resetting to a state from a demonstration. This reduces the amount of exploration required to learn the game compared to starting from scratch every time. We demonstrate the effectiveness of our method in a simple toy environment, where we highlight the exponential scaling of standard RL methods for sparse reward tasks. Our approach reduces this scaling to quadratic, making previously infeasible tasks more accessible. We then apply our method to Montezuma's Revenge, achieving a high-score of 74,500, surpassing all previously published results.",1
"Although deep reinforcement learning (deep RL) methods have lots of strengths that are favorable if applied to autonomous driving, real deep RL applications in autonomous driving have been slowed down by the modeling gap between the source (training) domain and the target (deployment) domain. Unlike current policy transfer approaches, which generally limit to the usage of uninterpretable neural network representations as the transferred features, we propose to transfer concrete kinematic quantities in autonomous driving. The proposed robust-control-based (RC) generic transfer architecture, which we call RL-RC, incorporates a transferable hierarchical RL trajectory planner and a robust tracking controller based on disturbance observer (DOB). The deep RL policies trained with known nominal dynamics model are transfered directly to the target domain, DOB-based robust tracking control is applied to tackle the modeling gap including the vehicle dynamics errors and the external disturbances such as side forces. We provide simulations validating the capability of the proposed method to achieve zero-shot transfer across multiple driving scenarios such as lane keeping, lane changing and obstacle avoidance.",0
"The implementation of deep reinforcement learning (deep RL) methods for autonomous driving has been hindered by the disparity between the training and deployment domains. While existing policy transfer methods rely on impenetrable neural network representations for feature transfer, we suggest transferring tangible kinematic quantities in autonomous driving. Our proposed transfer framework, RL-RC, utilizes a transferable hierarchical RL trajectory planner and robust tracking controller based on disturbance observer (DOB). The deep RL policies trained using nominal dynamics model are transferred directly to the target domain, and DOB-based robust tracking control is used to overcome the modeling gap, including vehicle dynamics errors and external disturbances such as side forces. Our simulations demonstrate the effectiveness of the proposed method in achieving zero-shot transfer across multiple driving scenarios, including lane keeping, lane changing, and obstacle avoidance.",1
"Adversarial self-play in two-player games has delivered impressive results when used with reinforcement learning algorithms that combine deep neural networks and tree search. Algorithms like AlphaZero and Expert Iteration learn tabula-rasa, producing highly informative training data on the fly. However, the self-play training strategy is not directly applicable to single-player games. Recently, several practically important combinatorial optimisation problems, such as the travelling salesman problem and the bin packing problem, have been reformulated as reinforcement learning problems, increasing the importance of enabling the benefits of self-play beyond two-player games. We present the Ranked Reward (R2) algorithm which accomplishes this by ranking the rewards obtained by a single agent over multiple games to create a relative performance metric. Results from applying the R2 algorithm to instances of a two-dimensional and three-dimensional bin packing problems show that it outperforms generic Monte Carlo tree search, heuristic algorithms and integer programming solvers. We also present an analysis of the ranked reward mechanism, in particular, the effects of problem instances with varying difficulty and different ranking thresholds.",0
"The use of adversarial self-play in two-player games has yielded impressive outcomes when paired with reinforcement learning methods that incorporate deep neural networks and tree search. These algorithms, such as AlphaZero and Expert Iteration, learn from scratch, generating highly informative training data on the spot. However, this self-play training strategy cannot be directly applied to single-player games. Recently, various combinatorial optimization problems, like the bin packing problem and the traveling salesman problem, have been redefined as reinforcement learning problems, which emphasizes the need to extend the advantages of self-play beyond two-player games. Our solution to this is the Ranked Reward (R2) algorithm, which creates a relative performance metric by ranking the rewards acquired by a single agent across multiple games. Our results indicate that the R2 algorithm outperforms generic Monte Carlo tree search, heuristic algorithms, and integer programming solvers when applied to two-dimensional and three-dimensional bin packing problem instances. Furthermore, we present an analysis of the ranked reward mechanism, particularly its impact on problem instances with varying levels of difficulty and different ranking thresholds.",1
"Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such ""victim"" models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we present an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a ""knockoff"" with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efficiency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as on a popular image analysis API where we create a reasonable knockoff for as little as $30.",0
"The use of Machine Learning (ML) models has become more common for various tasks, and our research examines the ability of an adversary to steal the functionality of these ""victim"" models through blackbox interactions: input of an image and output of a prediction. Our approach differs from previous studies as we assume the adversary lacks knowledge of the model's training and testing data, its internal workings, and semantics over output values. We propose a two-step process for stealing the model's functionality: (i) sending a set of input images to the blackbox model and obtaining predictions; (ii) training a ""knockoff"" model using the image-prediction pairs obtained from the queries. Our research reveals several interesting findings: (a) even when using a different architecture, the knockoff performs well when queried with random images from a different distribution than the blackbox training data; (b) and (c) our reinforcement learning method improves query sample efficiency and performance gains in certain settings. Our validation of model functionality stealing includes various datasets and tasks, as well as an image analysis API where a reasonable knockoff can be created for as little as $30.",1
"Recent research has shown that although Reinforcement Learning (RL) can benefit from expert demonstration, it usually takes considerable efforts to obtain enough demonstration. The efforts prevent training decent RL agents with expert demonstration in practice. In this work, we propose Active Reinforcement Learning with Demonstration (ARLD), a new framework to streamline RL in terms of demonstration efforts by allowing the RL agent to query for demonstration actively during training. Under the framework, we propose Active Deep Q-Network, a novel query strategy which adapts to the dynamically-changing distributions during the RL training process by estimating the uncertainty of recent states. The expert demonstration data within Active DQN are then utilized by optimizing supervised max-margin loss in addition to temporal difference loss within usual DQN training. We propose two methods of estimating the uncertainty based on two state-of-the-art DQN models, namely the divergence of bootstrapped DQN and the variance of noisy DQN. The empirical results validate that both methods not only learn faster than other passive expert demonstration methods with the same amount of demonstration and but also reach super-expert level of performance across four different tasks.",0
"Recent studies indicate that while Reinforcement Learning (RL) can benefit from expert demonstration, acquiring sufficient demonstration requires significant effort, hampering the training of effective RL agents with expert demonstration. To address this issue, we introduce Active Reinforcement Learning with Demonstration (ARLD), a novel framework that simplifies RL by enabling the RL agent to actively request for demonstration during training. Within this framework, we introduce Active Deep Q-Network, an innovative query strategy that adjusts to the prevalent dynamic distributions during the RL training process by assessing the uncertainty of recent states. The expert demonstration data within Active DQN are then employed by optimizing the supervised max-margin loss, in addition to the temporal difference loss in traditional DQN training. We propose two methods for estimating uncertainty based on two state-of-the-art DQN models, that is, the divergence of bootstrapped DQN and the variance of noisy DQN. Empirical results indicate that both methods not only facilitate quicker learning than other passive expert demonstration methods with the same level of demonstration, but also achieve super-expert performance across four distinct tasks.",1
"We present an off-policy actor-critic algorithm for Reinforcement Learning (RL) that combines ideas from gradient-free optimization via stochastic search with learned action-value function. The result is a simple procedure consisting of three steps: i) policy evaluation by estimating a parametric action-value function; ii) policy improvement via the estimation of a local non-parametric policy; and iii) generalization by fitting a parametric policy. Each step can be implemented in different ways, giving rise to several algorithm variants. Our algorithm draws on connections to existing literature on black-box optimization and 'RL as an inference' and it can be seen either as an extension of the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et al., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997] to a policy iteration scheme. Our comparison on 31 continuous control tasks from parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al., 2018] and OpenAI Gym [Brockman et al., 2016] with diverse properties, limited amount of compute and a single set of hyperparameters, demonstrate the effectiveness of our method and the state of art results. Videos, summarizing results, can be found at goo.gl/HtvJKR .",0
"We introduce an off-policy actor-critic algorithm for Reinforcement Learning (RL) that blends gradient-free optimization through stochastic search with a learned action-value function. This results in a straightforward three-step process: i) policy evaluation by approximating a parametric action-value function; ii) policy improvement through estimating a local non-parametric policy; and iii) generalization by fitting a parametric policy. Each step can be executed in various ways, leading to multiple algorithmic variations. Our algorithm is grounded in previous research on black-box optimization and 'RL as an inference.' It can be viewed as an extension of the Maximum a Posteriori Policy Optimisation algorithm (MPO) [Abdolmaleki et al., 2018a], or as an extension of Trust Region Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [Abdolmaleki et al., 2017b; Hansen et al., 1997] to a policy iteration scheme. Our approach is compared to 31 continuous control tasks from parkour suite [Heess et al., 2017], DeepMind control suite [Tassa et al., 2018] and OpenAI Gym [Brockman et al., 2016] with a limited amount of computing power and a single set of hyperparameters, demonstrating its effectiveness and producing state-of-the-art results. Videos summarizing the findings can be found at goo.gl/HtvJKR.",1
"The capacity of meta-learning algorithms to quickly adapt to a variety of tasks, including ones they did not experience during meta-training, has been a key factor in the recent success of these methods on few-shot learning problems. This particular advantage of using meta-learning over standard supervised or reinforcement learning is only well founded under the assumption that the adaptation phase does improve the performance of our model on the task of interest. However, in the classical framework of meta-learning, this constraint is only mildly enforced, if not at all, and we only see an improvement on average over a distribution of tasks. In this paper, we show that the adaptation in an algorithm like MAML can significantly decrease the performance of an agent in a meta-reinforcement learning setting, even on a range of meta-training tasks.",0
"Meta-learning algorithms have the ability to quickly adapt to various tasks, even ones not encountered during meta-training, which has contributed to their success in few-shot learning problems. This advantage over standard supervised or reinforcement learning is based on the assumption that the adaptation phase improves the model's performance on the target task. However, the traditional meta-learning framework only mildly enforces this constraint, resulting in an average improvement over a task distribution. In this paper, we demonstrate that in a meta-reinforcement learning scenario, adaptation in an algorithm such as MAML can significantly reduce an agent's performance, even across a range of meta-training tasks.",1
"We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q&A. Our visual context tree model, dubbed VCTree, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efficient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, e.g., ""clothes"" and ""pants"" are usually co-occur and belong to ""person""; 2) the dynamic structure varies from image to image and task to task, allowing more content-/task-specific message passing among objects. To construct a VCTree, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-specific models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former's evaluation result serves as a self-critic for the latter's structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2.0 for visual Q&A, show that VCTree outperforms state-of-the-art results while discovering interpretable visual context structures.",0
"Our proposal is to create dynamic tree structures that provide a visual context for objects within an image, aiding visual reasoning tasks such as scene graph generation and visual Q&A. Our model, called VCTree, offers two distinct advantages over existing object representations including chains and fully-connected graphs. Firstly, the binary tree structure efficiently encodes the parallel/hierarchical relationships between objects, such as how ""clothes"" and ""pants"" are typically associated with a ""person"". Secondly, the dynamic structure can vary depending on the image and task, enabling more targeted message passing among objects. To construct a VCTree, we use a score function to determine the validity of object pairs, and then generate a binary tree using the maximum spanning tree from the score matrix. The visual contexts are encoded and decoded using bidirectional TreeLSTM and task-specific models. We employ a hybrid learning approach that combines end-task supervised learning with tree structure reinforcement learning, where the former serves as a self-critic for the latter's structure exploration. Our experimental results on two benchmarks, Visual Genome and VQA2.0, demonstrate that VCTree outperforms existing methods while producing interpretable visual context structures.",1
"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.",0
"This paper deals with the issue of assessing learning systems in safety-critical fields such as autonomous driving, where errors can lead to disastrous outcomes. Our focus is on two problems: identifying scenarios where trained agents fail and determining their likelihood of failure. The conventional reinforcement learning method for assessing agents, Vanilla Monte Carlo, may not detect failures, resulting in the deployment of dangerous agents. We demonstrate that this is a problem even for current agents, where matching the compute used for training is sometimes not enough for evaluation. To address this limitation, we draw on the literature on rare event probability estimation and propose an adversarial evaluation strategy that centers on adversarial situations while still offering unbiased failure probability estimates. The main challenge is identifying these adversarial situations, as failure is infrequent, and there is little signal to guide optimization. To overcome this, we propose a continuation approach that learns failure modes in related but less robust agents. Additionally, our approach allows for the reuse of data already gathered for agent training. We demonstrate the effectiveness of adversarial evaluation in two standard domains: humanoid control and simulated driving. The experimental outcomes show that our methods can detect catastrophic failures and estimate agent failure rates several orders of magnitude faster than traditional evaluation techniques, taking minutes to hours instead of days.",1
"The recently proposed option-critic architecture Bacon et al. provide a stochastic policy gradient approach to hierarchical reinforcement learning. Specifically, they provide a way to estimate the gradient of the expected discounted return with respect to parameters that define a finite number of temporally extended actions, called \textit{options}. In this paper we show how the option-critic architecture can be extended to estimate the natural gradient of the expected discounted return. To this end, the central questions that we consider in this paper are: 1) what is the definition of the natural gradient in this context, 2) what is the Fisher information matrix associated with an option's parameterized policy, 3) what is the Fisher information matrix associated with an option's parameterized termination function, and 4) how can a compatible function approximation approach be leveraged to obtain natural gradient estimates for both the parameterized policy and parameterized termination functions of an option with per-time-step time and space complexity linear in the total number of parameters. Based on answers to these questions we introduce the natural option critic algorithm. Experimental results showcase improvement over the vanilla gradient approach.",0
"Bacon et al. proposed the option-critic architecture for hierarchical reinforcement learning, which employs a stochastic policy gradient approach. This method estimates the gradient of the expected discounted return by defining a finite number of temporally extended actions, or ""options,"" and calculating their associated parameters. In this study, we expand on this approach by introducing the natural gradient estimation for the expected discounted return. We address four key questions: the definition of the natural gradient, the Fisher information matrix for policy and termination functions, and a compatible function approximation method. Using these answers, we introduce the natural option critic algorithm, which demonstrates improvement over the standard gradient approach in our experiments. The time and space complexity of this algorithm is proportional to the total number of parameters.",1
"For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised ""practice"" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.",0
"To perform a variety of user-defined tasks during testing, an independent entity must possess a collection of skills that are widely applicable and adaptable. These skills should also be able to handle raw sensory data, such as images, to ensure they are general enough. In this article, we suggest a method that combines unsupervised representation learning and reinforcement learning of goal-oriented policies to acquire such general-purpose skills. Since the entity is unaware of the specific goals it may need to achieve during testing, it goes through a self-supervised ""practice"" phase where it imagines and attempts to achieve goals. We also teach the entity a visual representation with three objectives: generating goals for self-supervised practice, transforming raw sensory inputs in a structured manner, and calculating a reward signal for goal completion. We propose a retroactive goal relabeling approach to enhance the efficiency of our method. Our off-policy method is powerful enough to learn policies that use raw image data and objectives for a real-world robotic system and outperforms prior methods significantly.",1
"A vine copula model is a flexible high-dimensional dependence model which uses only bivariate building blocks. However, the number of possible configurations of a vine copula grows exponentially as the number of variables increases, making model selection a major challenge in development. In this work, we formulate a vine structure learning problem with both vector and reinforcement learning representation. We use neural network to find the embeddings for the best possible vine model and generate a structure. Throughout experiments on synthetic and real-world datasets, we show that our proposed approach fits the data better in terms of log-likelihood. Moreover, we demonstrate that the model is able to generate high-quality samples in a variety of applications, making it a good candidate for synthetic data generation.",0
"The vine copula model is a highly flexible dependence model that relies solely on bivariate building blocks. However, as the number of variables increases, the possible configurations of a vine copula grow exponentially, posing a significant challenge for model selection. To address this issue, we present a vine structure learning problem that utilizes both vector and reinforcement learning representations. Our approach employs neural networks to identify the best possible vine model and generate a structure. Our experiments on synthetic and real-world datasets demonstrate that our proposed method provides a better fit to the data in terms of log-likelihood. Additionally, our model produces high-quality samples across various applications, making it a promising candidate for synthetic data generation.",1
"Intelligent Transportation Systems (ITSs) are envisioned to play a critical role in improving traffic flow and reducing congestion, which is a pervasive issue impacting urban areas around the globe. Rapidly advancing vehicular communication and edge cloud computation technologies provide key enablers for smart traffic management. However, operating viable real-time actuation mechanisms on a practically relevant scale involves formidable challenges, e.g., policy iteration and conventional Reinforcement Learning (RL) techniques suffer from poor scalability due to state space explosion. Motivated by these issues, we explore the potential for Deep Q-Networks (DQN) to optimize traffic light control policies. As an initial benchmark, we establish that the DQN algorithms yield the ""thresholding"" policy in a single-intersection. Next, we examine the scalability properties of DQN algorithms and their performance in a linear network topology with several intersections along a main artery. We demonstrate that DQN algorithms produce intelligent behavior, such as the emergence of ""greenwave"" patterns, reflecting their ability to learn favorable traffic light actuations.",0
"The use of Intelligent Transportation Systems (ITSs) is crucial in reducing traffic congestion, a common issue in cities worldwide. The development of vehicular communication and edge cloud computation technologies has enabled smart traffic management. However, implementing real-time actuation mechanisms on a large scale is challenging due to issues such as policy iteration and conventional Reinforcement Learning techniques being unable to handle the state space explosion. To address these problems, we investigate the potential of using Deep Q-Networks (DQN) to optimize traffic light control policies. We test the DQN algorithms in a single-intersection scenario and find that they produce the ""thresholding"" policy. Furthermore, we evaluate their scalability and performance in a linear network topology with multiple intersections along a main road. Results show that the DQN algorithms exhibit intelligent behavior, such as the emergence of ""greenwave"" patterns, indicating their ability to learn favorable traffic light actuations.",1
"De novo protein structure prediction from amino acid sequence is one of the most challenging problems in computational biology. As one of the extensively explored mathematical models for protein folding, Hydrophobic-Polar (HP) model enables thorough investigation of protein structure formation and evolution. Although HP model discretizes the conformational space and simplifies the folding energy function, it has been proven to be an NP-complete problem. In this paper, we propose a novel protein folding framework FoldingZero, self-folding a de novo protein 2D HP structure from scratch based on deep reinforcement learning. FoldingZero features the coupled approach of a two-head (policy and value heads) deep convolutional neural network (HPNet) and a regularized Upper Confidence Bounds for Trees (R-UCT). It is trained solely by a reinforcement learning algorithm, which improves HPNet and R-UCT iteratively through iterative policy optimization. Without any supervision and domain knowledge, FoldingZero not only achieves comparable results, but also learns the latent folding knowledge to stabilize the structure. Without exponential computation, FoldingZero shows promising potential to be adopted for real-world protein properties prediction.",0
"The task of predicting protein structure from amino acid sequence is extremely difficult in computational biology. The Hydrophobic-Polar (HP) model is a widely used mathematical model for protein folding which allows for in-depth exploration of protein structure development and evolution. Despite its simplification of the folding energy function and discretization of the conformational space, the HP model has been proven to be an NP-complete problem. In this paper, we introduce FoldingZero, a novel protein folding framework that uses deep reinforcement learning to self-fold a de novo protein 2D HP structure from scratch. FoldingZero combines a two-head deep convolutional neural network (HPNet) and a regularized Upper Confidence Bounds for Trees (R-UCT) to train solely with a reinforcement learning algorithm which iteratively optimizes HPNet and R-UCT. Without any supervision or domain knowledge, FoldingZero produces comparable results and learns latent folding knowledge to stabilize the structure. It shows great potential for real-world protein properties prediction without the need for exponential computation.",1
"This paper explores a simple regularizer for reinforcement learning by proposing Generative Adversarial Self-Imitation Learning (GASIL), which encourages the agent to imitate past good trajectories via generative adversarial imitation learning framework. Instead of directly maximizing rewards, GASIL focuses on reproducing past good trajectories, which can potentially make long-term credit assignment easier when rewards are sparse and delayed. GASIL can be easily combined with any policy gradient objective by using GASIL as a learned shaped reward function. Our experimental results show that GASIL improves the performance of proximal policy optimization on 2D Point Mass and MuJoCo environments with delayed reward and stochastic dynamics.",0
"The objective of this paper is to propose a straightforward regularizer for reinforcement learning referred to as Generative Adversarial Self-Imitation Learning (GASIL). This approach encourages the agent to imitate prior successful trajectories through the use of a generative adversarial imitation learning framework. Instead of solely maximizing rewards, GASIL emphasizes replicating past good trajectories, which may simplify long-term credit assignment when rewards are few and delayed. GASIL can be readily incorporated with any policy gradient objective by using it as a learned shaped reward function. Empirical results demonstrate that GASIL enhances the performance of proximal policy optimization in both 2D Point Mass and MuJoCo environments with stochastic dynamics and delayed reward.",1
"Multi-agent reinforcement learning systems aim to provide interacting agents with the ability to collaboratively learn and adapt to the behaviour of other agents. In many real-world applications, the agents can only acquire a partial view of the world. Here we consider a setting whereby most agents' observations are also extremely noisy, hence only weakly correlated to the true state of the environment. Under these circumstances, learning an optimal policy becomes particularly challenging, even in the unrealistic case that an agent's policy can be made conditional upon all other agents' observations. To overcome these difficulties, we propose a multi-agent deep deterministic policy gradient algorithm enhanced by a communication medium (MADDPG-M), which implements a two-level, concurrent learning mechanism. An agent's policy depends on its own private observations as well as those explicitly shared by others through a communication medium. At any given point in time, an agent must decide whether its private observations are sufficiently informative to be shared with others. However, our environments provide no explicit feedback informing an agent whether a communication action is beneficial, rather the communication policies must also be learned through experience concurrently to the main policies. Our experimental results demonstrate that the algorithm performs well in six highly non-stationary environments of progressively higher complexity, and offers substantial performance gains compared to the baselines.",0
"The objective of multi-agent reinforcement learning systems is to enable agents to learn and adapt to the behavior of their peers. In practical applications, agents often have a limited view of the environment and face the challenge of dealing with noisy observations that are only weakly correlated with the true state of the environment. Even in cases where an agent's policy can be made conditional upon all other agents' observations, learning an optimal policy is difficult. To address this challenge, we propose a multi-agent deep deterministic policy gradient algorithm enhanced by a communication medium (MADDPG-M). Our algorithm implements a two-level, concurrent learning mechanism where an agent's policy depends on its own private observations and those explicitly shared by others. However, communication policies must also be learned through experience concurrently with the main policies, as there is no explicit feedback to inform an agent whether a communication action is beneficial. Our experimental results demonstrate that the algorithm performs well in six highly non-stationary environments of progressively higher complexity and provides substantial performance gains compared to the baselines.",1
"Advances in Deep Reinforcement Learning have led to agents that perform well across a variety of sensory-motor domains. In this work, we study the setting in which an agent must learn to generate programs for diverse scenes conditioned on a given symbolic instruction. Final goals are specified to our agent via images of the scenes. A symbolic instruction consistent with the goal images is used as the conditioning input for our policies. Since a single instruction corresponds to a diverse set of different but still consistent end-goal images, the agent needs to learn to generate a distribution over programs given an instruction. We demonstrate that with simple changes to the reinforced adversarial learning objective, we can learn instruction conditioned policies to achieve the corresponding diverse set of goals. Most importantly, our agent's stochastic policy is shown to more accurately capture the diversity in the goal distribution than a fixed pixel-based reward function baseline. We demonstrate the efficacy of our approach on two domains: (1) drawing MNIST digits with a paint software conditioned on instructions and (2) constructing scenes in a 3D editor that satisfies a certain instruction.",0
"The progress made in Deep Reinforcement Learning has resulted in efficient agents that excel in several sensory-motor areas. In this study, we examine a scenario where an agent must acquire the skill to create programs for a range of scenes, with a specific symbolic command as the precondition. The final objectives are conveyed to the agent through images of the scenes. Our policies rely on a symbolic instruction that corresponds to the goal images. As a single instruction can lead to multiple but compatible end-result images, the agent must learn to generate a program distribution based on the instruction. By making simple modifications to the reinforced adversarial learning objective, we can teach instruction-dependent policies to meet the diverse range of goals. Our crucial finding is that our agent's stochastic policy more effectively captures the variation in the goal distribution than the fixed pixel-based reward function baseline. We exhibit the effectiveness of our method in two domains: (1) using a painting software to draw MNIST digits conditioned on instructions, and (2) creating scenes in a 3D editor that fulfill specific commands.",1
"To solve a text-based game, an agent needs to formulate valid text commands for a given context and find the ones that lead to success. Recent attempts at solving text-based games with deep reinforcement learning have focused on the latter, i.e., learning to act optimally when valid actions are known in advance. In this work, we propose to tackle the first task and train a model that generates the set of all valid commands for a given context. We try three generative models on a dataset generated with Textworld. The best model can generate valid commands which were unseen at training and achieve high $F_1$ score on the test set.",0
"In order to succeed in a text-based game, an agent must create appropriate text commands within a given context and determine which ones will lead to success. Previous attempts to solve these games using deep reinforcement learning have primarily focused on the latter task, which involves learning to act optimally when valid actions have already been determined. This study, however, aims to address the former task by training a model that can generate a complete set of valid commands for any given context. The study tests three different generative models using a dataset created with Textworld. The best performing model was able to generate previously unseen valid commands and achieve a high F1 score on the test set.",1
"Target tracking in a camera network is an important task for surveillance and scene understanding. The task is challenging due to disjoint views and illumination variation in different cameras. In this direction, many graph-based methods were proposed using appearance-based features. However, the appearance information fades with high illumination variation in the different camera FOVs. We, in this paper, use spatial and temporal information as the state of the target to learn a policy that predicts the next camera given the current state. The policy is trained using Q-learning and it does not assume any information about the topology of the camera network. We will show that the policy learns the camera network topology. We demonstrate the performance of the proposed method on the NLPR MCT dataset.",0
"Surveillance and scene comprehension require effective target tracking in camera networks, which is a challenging task due to the varying illumination and disjoint views across multiple cameras. Previous graph-based methods utilized appearance features, but these features degrade in the presence of high illumination variation. This paper proposes a policy that predicts the next camera based on the current target state, using spatial and temporal information. The policy is trained using Q-learning, without assuming any knowledge of the camera network topology. The study shows that the policy can learn the camera network topology, and the proposed method's effectiveness is demonstrated on the NLPR MCT dataset.",1
"Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.",0
"The amalgamation of reinforcement learning (RL) and deep learning has given rise to a new field of study known as deep reinforcement learning. This area of research has been able to tackle a vast array of intricate decision-making tasks that were previously beyond the capabilities of a machine. As a result, deep RL has opened up numerous potential applications in various fields, including healthcare, robotics, smart grids, and finance, among others. This paper aims to provide an overview of deep reinforcement learning models, algorithms, and techniques, with a particular emphasis on generalization and practical applications. It is assumed that the reader has a basic understanding of machine learning concepts.",1
"In urban environments, supply resources have to be constantly matched to the ""right"" locations (where customer demand is present) so as to improve quality of life. For instance, ambulances have to be matched to base stations regularly so as to reduce response time for emergency incidents in EMS (Emergency Management Systems); vehicles (cars, bikes, scooters etc.) have to be matched to docking stations so as to reduce lost demand in shared mobility systems. Such problem domains are challenging owing to the demand uncertainty, combinatorial action spaces (due to allocation) and constraints on allocation of resources (e.g., total resources, minimum and maximum number of resources at locations and regions).   Existing systems typically employ myopic and greedy optimization approaches to optimize allocation of supply resources to locations. Such approaches typically are unable to handle surges or variances in demand patterns well. Recent research has demonstrated the ability of Deep RL methods in adapting well to highly uncertain environments. However, existing Deep RL methods are unable to handle combinatorial action spaces and constraints on allocation of resources. To that end, we have developed three approaches on top of the well known actor critic approach, DDPG (Deep Deterministic Policy Gradient) that are able to handle constraints on resource allocation. More importantly, we demonstrate that they are able to outperform leading approaches on simulators validated on semi-real and real data sets.",0
"Improving quality of life in urban environments requires constantly matching supply resources to areas with high customer demand. This includes allocating ambulances to base stations to reduce emergency response times and assigning vehicles to docking stations in shared mobility systems to minimize lost demand. However, these allocation problems are challenging due to demand uncertainty, constraints on resource allocation, and combinatorial action spaces. Existing optimization methods are myopic and unable to handle demand surges. Deep RL methods have shown promise in uncertain environments, but struggle with combinatorial action spaces and constraints. To address this, we developed three approaches on top of DDPG that can handle resource allocation constraints and outperform leading methods on semi-real and real data sets.",1
"Recent studies on neural architecture search have shown that automatically designed neural networks perform as good as expert-crafted architectures. While most existing works aim at finding architectures that optimize the prediction accuracy, these architectures may have complexity and is therefore not suitable being deployed on certain computing environment (e.g., with limited power budgets). We propose MONAS, a framework for Multi-Objective Neural Architectural Search that employs reward functions considering both prediction accuracy and other important objectives (e.g., power consumption) when searching for neural network architectures. Experimental results showed that, compared to the state-ofthe-arts, models found by MONAS achieve comparable or better classification accuracy on computer vision applications, while satisfying the additional objectives such as peak power.",0
"Recent research on neural architecture search has revealed that automatically designed neural networks perform equally well as architectures crafted by experts. However, most of these studies focus solely on optimizing prediction accuracy, which can lead to complex architectures that are unsuitable for deployment on limited computing environments. To address this issue, we present MONAS, a framework for Multi-Objective Neural Architectural Search that incorporates reward functions for other important objectives, such as power consumption, alongside prediction accuracy. Our experimental results demonstrate that models discovered by MONAS are comparable, if not superior, to state-of-the-art models for computer vision tasks, while also satisfying additional objectives like peak power.",1
"Semi-supervised learning is attracting increasing attention due to the fact that datasets of many domains lack enough labeled data. Variational Auto-Encoder (VAE), in particular, has demonstrated the benefits of semi-supervised learning. The majority of existing semi-supervised VAEs utilize a classifier to exploit label information, where the parameters of the classifier are introduced to the VAE. Given the limited labeled data, learning the parameters for the classifiers may not be an optimal solution for exploiting label information. Therefore, in this paper, we develop a novel approach for semi-supervised VAE without classifier. Specifically, we propose a new model called Semi-supervised Disentangled VAE (SDVAE), which encodes the input data into disentangled representation and non-interpretable representation, then the category information is directly utilized to regularize the disentangled representation via the equality constraint. To further enhance the feature learning ability of the proposed VAE, we incorporate reinforcement learning to relieve the lack of data. The dynamic framework is capable of dealing with both image and text data with its corresponding encoder and decoder networks. Extensive experiments on image and text datasets demonstrate the effectiveness of the proposed framework.",0
"The lack of labeled data in many domains has led to an increase in interest in semi-supervised learning. Variational Auto-Encoder (VAE) has shown promising results in this area. However, most existing semi-supervised VAEs rely on a classifier to exploit label information, which may not be optimal given the limited labeled data. In this paper, we present a new approach for semi-supervised VAE called Semi-supervised Disentangled VAE (SDVAE), which does not use a classifier. SDVAE encodes input data into disentangled and non-interpretable representations and directly utilizes category information to regulate the disentangled representation through the equality constraint. To improve the feature learning ability of the proposed VAE, we incorporate reinforcement learning to overcome data scarcity. The dynamic framework can handle both image and text data using corresponding encoder and decoder networks. Extensive experiments on image and text datasets prove the effectiveness of our proposed framework.",1
"Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.",0
"The investment companies heavily rely on their stock trading strategy, but crafting an optimal plan in the ever-changing and complex stock market is a daunting task. To tackle this issue, we investigate the potential of deep reinforcement learning to maximize investment returns by optimizing the stock trading strategy. Our study involves selecting thirty trading stocks and utilizing their daily prices as the market environment for both training and trading. Through this, we develop an adaptive trading strategy by training a deep reinforcement learning agent. We evaluate the agent's performance by comparing it to the traditional min-variance portfolio allocation strategy and the Dow Jones Industrial Average. The results reveal that our proposed approach of deep reinforcement learning outperforms the two baselines in terms of both the Sharpe ratio and cumulative returns.",1
"Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.",0
"Reinforcement learning employs curriculum learning as a training approach to expedite the learning of a complex target task. This is achieved by first training on a series of simpler tasks and then transferring the acquired knowledge to the target task. However, automatically selecting a sequence of such tasks remains a challenge, and recent studies have focused on resolving this issue. In this research, we enhance a recent curriculum design technique that transforms the curriculum sequencing problem into a Markov Decision Process. We extend this model to manage multiple transfer learning techniques and prove that a curriculum policy over this MDP can be learned through experience. We assess our approach by designing curriculum policies for multiple agents in two distinct domains, using various representations that enable this. Our method yields curricula that can train agents to perform as fast or faster on a target task than existing methods.",1
"Current clinical practice to monitor patients' health follows either regular or heuristic-based lab test (e.g. blood test) scheduling. Such practice not only gives rise to redundant measurements accruing cost, but may even lead to unnecessary patient discomfort. From the computational perspective, heuristic-based test scheduling might lead to reduced accuracy of clinical forecasting models. Computationally learning an optimal clinical test scheduling and measurement collection, is likely to lead to both, better predictive models and patient outcome improvement. We address the scheduling problem using deep reinforcement learning (RL) to achieve high predictive gain and low measurement cost, by scheduling fewer, but strategically timed tests. We first show that in the simulation our policy outperforms heuristic-based measurement scheduling with higher predictive gain or lower cost measured by accumulated reward. We then learn a scheduling policy for mortality forecasting in the real-world clinical dataset (MIMIC3), our learned policy is able to provide useful clinical insights. To our knowledge, this is the first RL application on multi-measurement scheduling problem in the clinical setting.",0
"The current practice of monitoring patients' health involves regular or heuristic-based lab tests, such as blood tests. However, this approach can result in unnecessary costs and patient discomfort due to redundant measurements. Additionally, heuristic-based scheduling may not accurately predict clinical outcomes. Therefore, it is important to computationally learn an optimal test scheduling approach to improve predictive models and patient outcomes. We propose using deep reinforcement learning to schedule strategically timed tests that achieve high predictive gain and low measurement cost. Our simulation results show that our policy outperforms heuristic-based scheduling. Furthermore, we apply our approach to a real-world clinical dataset (MIMIC3) for mortality forecasting and find that our learned policy provides useful clinical insights. This is the first application of reinforcement learning to multi-measurement scheduling in a clinical setting.",1
"Deep reinforcement learning (DRL) has achieved great successes in recent years with the help of novel methods and higher compute power. However, there are still several challenges to be addressed such as convergence to locally optimal policies and long training times. In this paper, firstly, we augment Asynchronous Advantage Actor-Critic (A3C) method with a novel self-supervised auxiliary task, i.e. \emph{Terminal Prediction}, measuring temporal closeness to terminal states, namely A3C-TP. Secondly, we propose a new framework where planning algorithms such as Monte Carlo tree search or other sources of (simulated) demonstrators can be integrated to asynchronous distributed DRL methods. Compared to vanilla A3C, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.",0
"Recent years have seen significant accomplishments in Deep Reinforcement Learning (DRL) due to advanced methods and enhanced computing capabilities. However, there are still obstacles to overcome, such as the attainment of locally optimal policies and extended training periods. In this study, we introduce two novel approaches. Firstly, we enhance the Asynchronous Advantage Actor-Critic (A3C) method with a self-supervised auxiliary task, referred to as Terminal Prediction, which measures proximity to terminal states. Our new method, A3C-TP, is designed to overcome the limitations of vanilla A3C. Secondly, we suggest a new framework that integrates planning algorithms, such as Monte Carlo tree search, or other sources of simulated demonstrators, into asynchronous distributed DRL approaches. Our proposed methods outperform vanilla A3C in terms of faster learning and better convergence to optimal policies, as demonstrated in the two-player mini version of the Pommerman game.",1
"Solving tasks with sparse rewards is a main challenge in reinforcement learning. While hierarchical controllers are an intuitive approach to this problem, current methods often require manual reward shaping, alternating training phases, or manually defined sub tasks. We introduce modulated policy hierarchies (MPH), that can learn end-to-end to solve tasks from sparse rewards. To achieve this, we study different modulation signals and exploration for hierarchical controllers. Specifically, we find that communicating via bit-vectors is more efficient than selecting one out of multiple skills, as it enables mixing between them. To facilitate exploration, MPH uses its different time scales for temporally extended intrinsic motivation at each level of the hierarchy. We evaluate MPH on the robotics tasks of pushing and sparse block stacking, where it outperforms recent baselines.",0
"One of the main challenges in reinforcement learning is addressing tasks with sparse rewards. Although hierarchical controllers are commonly used to solve this problem, current methods often require manual reward shaping, alternating training phases, or manually defined sub tasks. Our solution to this problem is the introduction of modulated policy hierarchies (MPH), which can learn end-to-end and solve tasks with sparse rewards. We conducted a study on different modulation signals and exploration techniques for hierarchical controllers, and discovered that communicating via bit-vectors is more efficient than selecting one out of multiple skills because it allows for mixing between them. MPH uses its different time scales to facilitate exploration through temporally extended intrinsic motivation at each level of the hierarchy. We evaluated MPH on the robotics tasks of pushing and sparse block stacking, and it outperformed recent baselines.",1
"Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.",0
"Tasks with sparse environment rewards have traditionally posed a challenge for deep reinforcement learning methods. One effective solution is to imitate trajectories demonstrated by a human, but these demonstrations are usually collected under artificial conditions with precise access to the agent's environment setup, as well as the demonstrator's action and reward trajectories. To address these limitations, we propose a two-stage approach that utilizes noisy, unaligned footage without such data access. First, we employ self-supervised objectives that encompass time and modality (i.e. vision and sound) to map unaligned videos from various sources to a shared representation. Second, we use a single YouTube video to create a reward function that encourages an agent to mimic human gameplay. This one-shot imitation technique enables our agent to outperform human-level performance on notoriously difficult exploration games such as Montezuma's Revenge, Pitfall!, and Private Eye, even without any environment rewards.",1
"Advances in the field of inverse reinforcement learning (IRL) have led to sophisticated inference frameworks that relax the original modeling assumption of observing an agent behavior that reflects only a single intention. Instead of learning a global behavioral model, recent IRL methods divide the demonstration data into parts, to account for the fact that different trajectories may correspond to different intentions, e.g., because they were generated by different domain experts. In this work, we go one step further: using the intuitive concept of subgoals, we build upon the premise that even a single trajectory can be explained more efficiently locally within a certain context than globally, enabling a more compact representation of the observed behavior. Based on this assumption, we build an implicit intentional model of the agent's goals to forecast its behavior in unobserved situations. The result is an integrated Bayesian prediction framework that significantly outperforms existing IRL solutions and provides smooth policy estimates consistent with the expert's plan. Most notably, our framework naturally handles situations where the intentions of the agent change over time and classical IRL algorithms fail. In addition, due to its probabilistic nature, the model can be straightforwardly applied in active learning scenarios to guide the demonstration process of the expert.",0
"The field of inverse reinforcement learning (IRL) has seen progress in the development of advanced inference frameworks that challenge the original assumption of modeling an agent's behavior according to a single intention. Recent IRL methods have improved on this assumption by dividing demonstration data into parts that account for different intentions. This work takes it further by introducing the concept of subgoals, which allows for a more efficient explanation of a single trajectory within a specific context. With this idea in mind, an implicit intentional model is created to predict an agent's behavior in new situations. The result is a Bayesian prediction framework superior to existing IRL solutions, providing consistent policy estimates. This approach is well-suited for situations where an agent's intentions change over time, which classical IRL algorithms struggle with. Additionally, the probabilistic nature of this model makes it useful in active learning scenarios to guide experts in the demonstration process.",1
"Microfluidic devices are utilized to control and direct flow behavior in a wide variety of applications, particularly in medical diagnostics. A particularly popular form of microfluidics -- called inertial microfluidic flow sculpting -- involves placing a sequence of pillars to controllably deform an initial flow field into a desired one. Inertial flow sculpting can be formally defined as an inverse problem, where one identifies a sequence of pillars (chosen, with replacement, from a finite set of pillars, each of which produce a specific transformation) whose composite transformation results in a user-defined desired transformation. Endemic to most such problems in engineering, inverse problems are usually quite computationally intractable, with most traditional approaches based on search and optimization strategies. In this paper, we pose this inverse problem as a Reinforcement Learning (RL) problem. We train a DoubleDQN agent to learn from this environment. The results suggest that learning is possible using a DoubleDQN model with the success frequency reaching 90% in 200,000 episodes and the rewards converging. While most of the results are obtained by fixing a particular target flow shape to simplify the learning problem, we later demonstrate how to transfer the learning of an agent based on one target shape to another, i.e. from one design to another and thus be useful for a generic design of a flow shape.",0
"Microfluidic devices are commonly utilized in a variety of fields, especially in medical diagnostics, for controlling and directing flow behavior. One popular type of microfluidics is inertial microfluidic flow sculpting, which involves using a sequence of pillars to manipulate an initial flow field into a desired one. This process is considered an inverse problem, as it requires identifying a sequence of pillars that will produce a specific transformation resulting in the desired flow. Due to the complexity of inverse problems, traditional approaches rely on search and optimization strategies. However, this paper suggests using a Reinforcement Learning (RL) approach to train a DoubleDQN agent to learn from the environment. The results show that the DoubleDQN model is successful, with a 90% success rate in 200,000 episodes and converging rewards. Although most of the results were obtained by fixing a specific target flow shape, the paper also demonstrates how the agent's learning can be transferred from one design to another, making it useful for a generic flow shape design.",1
"Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators' distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.",0
"Deep reinforcement learning has proven to be successful in various areas, with the capability to learn directly from high-dimensional sensory streams. However, neural networks that are trained in a fixed environment, such as a single level in a video game, tend to overfit and fail to generalize to new levels. Overfitting in RL models can lead to poor agent performance when even the slightest modifications are made to the environment. To address this issue, this study investigates the potential of procedurally generated levels during training to improve generality. The findings suggest that procedural level generation can enhance generalization to new levels within the same distribution and achieve better performance with less data. The study also evaluates the generality of learned behaviors on a set of human-designed levels, where the ability to generalize is largely dependent on the design of the level generators. To better understand the generators' distributions of levels, the paper applies dimensionality reduction and clustering techniques. The analysis reveals the extent to which the generators can produce levels similar to those designed by humans.",1
"We address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration rates of suboptimal (state, action) pairs. For any arbitrary structure, we derive problem-specific regret lower bounds satisfied by any learning algorithm. These lower bounds are made explicit for unstructured MDPs and for those whose transition probabilities and average reward functions are Lipschitz continuous w.r.t. the state and action. For Lipschitz MDPs, the bounds are shown not to scale with the sizes $S$ and $A$ of the state and action spaces, i.e., they are smaller than $c\log T$ where $T$ is the time horizon and the constant $c$ only depends on the Lipschitz structure, the span of the bias function, and the minimal action sub-optimality gap. This contrasts with unstructured MDPs where the regret lower bound typically scales as $SA\log T$. We devise DEL (Directed Exploration Learning), an algorithm that matches our regret lower bounds. We further simplify the algorithm for Lipschitz MDPs, and show that the simplified version is still able to efficiently exploit the structure.",0
"Our focus is on reinforcement learning problems with finite state and action spaces. In cases where the underlying MDP has a known structure, we aim to minimize exploration rates of suboptimal (state, action) pairs by exploiting this structure. We develop problem-specific regret lower bounds for any arbitrary structure, which apply to all learning algorithms. These lower bounds are explicitly stated for unstructured MDPs, as well as those with Lipschitz continuous transition probabilities and average reward functions relative to state and action. For Lipschitz MDPs, the bounds are not dependent on the sizes of the state and action spaces, but rather on the Lipschitz structure, the bias function span, and the minimal action sub-optimality gap. In contrast, unstructured MDPs typically have a regret lower bound that scales as SA log T. To this end, we introduce DEL (Directed Exploration Learning), an algorithm that matches our regret lower bounds. We also simplify the algorithm for Lipschitz MDPs and demonstrate its efficiency in exploiting the structure.",1
"Initial DR studies mainly adopt model predictive control and thus require accurate models of the control problem (e.g., a customer behavior model), which are to a large extent uncertain for the EV scenario. Hence, model-free approaches, especially based on reinforcement learning (RL) are an attractive alternative. In this paper, we propose a new Markov decision process (MDP) formulation in the RL framework, to jointly coordinate a set of EV charging stations. State-of-the-art algorithms either focus on a single EV, or perform the control of an aggregate of EVs in multiple steps (e.g., aggregate load decisions in one step, then a step translating the aggregate decision to individual connected EVs). On the contrary, we propose an RL approach to jointly control the whole set of EVs at once. We contribute a new MDP formulation, with a scalable state representation that is independent of the number of EV charging stations. Further, we use a batch reinforcement learning algorithm, i.e., an instance of fitted Q-iteration, to learn the optimal charging policy. We analyze its performance using simulation experiments based on a real-world EV charging data. More specifically, we (i) explore the various settings in training the RL policy (e.g., duration of the period with training data), (ii) compare its performance to an oracle all-knowing benchmark (which provides an upper bound for performance, relying on information that is not available or at least imperfect in practice), (iii) analyze performance over time, over the course of a full year to evaluate possible performance fluctuations (e.g, across different seasons), and (iv) demonstrate the generalization capacity of a learned control policy to larger sets of charging stations.",0
"The use of model predictive control in initial DR studies requires precise models of the control problem, which can be uncertain in the EV scenario. Consequently, model-free approaches, particularly those based on reinforcement learning, are gaining popularity. In this study, we propose a new Markov decision process formulation in the RL framework to jointly coordinate a group of EV charging stations. Unlike existing algorithms that either focus on a single EV or control an aggregate of EVs in multiple steps, we suggest an RL approach to control the entire set of EVs simultaneously. Our contribution includes a new MDP formulation with a scalable state representation that is independent of the number of EV charging stations. We use a batch reinforcement learning algorithm to learn the optimal charging policy and analyze its performance through simulation. Specifically, we examine various settings in training the RL policy, compare its performance to an oracle all-knowing benchmark, analyze performance over time, and demonstrate the generalization capacity of learned control policy to larger sets of charging stations.",1
"To overcome the curse of dimensionality and curse of modeling in Dynamic Programming (DP) methods for solving classical Markov Decision Process (MDP) problems, Reinforcement Learning (RL) algorithms are popular. In this paper, we consider an infinite-horizon average reward MDP problem and prove the optimality of the threshold policy under certain conditions. Traditional RL techniques do not exploit the threshold nature of optimal policy while learning. In this paper, we propose a new RL algorithm which utilizes the known threshold structure of the optimal policy while learning by reducing the feasible policy space. We establish that the proposed algorithm converges to the optimal policy. It provides a significant improvement in convergence speed and computational and storage complexity over traditional RL algorithms. The proposed technique can be applied to a wide variety of optimization problems that include energy efficient data transmission and management of queues. We exhibit the improvement in convergence speed of the proposed algorithm over other RL algorithms through simulations.",0
"Reinforcement Learning (RL) algorithms are commonly used to overcome the challenges of the curse of dimensionality and curse of modeling that Dynamic Programming (DP) methods face when solving classical Markov Decision Process (MDP) problems. This paper focuses on an infinite-horizon average reward MDP problem and proves the optimality of the threshold policy under certain conditions. However, traditional RL techniques do not take advantage of the threshold nature of the optimal policy during learning. To address this, a new RL algorithm is proposed that reduces the feasible policy space by utilizing the known threshold structure of the optimal policy. The proposed algorithm is proven to converge to the optimal policy and offers significant improvements in convergence speed, computational complexity, and storage complexity over traditional RL algorithms. This technique can be applied to various optimization problems, such as energy-efficient data transmission and queue management. Simulations demonstrate the superior convergence speed of the proposed algorithm compared to other RL algorithms.",1
"Goal-oriented dialog has been given attention due to its numerous applications in artificial intelligence. Goal-oriented dialogue tasks occur when a questioner asks an action-oriented question and an answerer responds with the intent of letting the questioner know a correct action to take. To ask the adequate question, deep learning and reinforcement learning have been recently applied. However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose ""Answerer in Questioner's Mind"" (AQM), a novel information theoretic algorithm for goal-oriented dialog. With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer's intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. We test our framework on two goal-oriented visual dialog tasks: ""MNIST Counting Dialog"" and ""GuessWhat?!"". In our experiments, AQM outperforms comparative algorithms by a large margin.",0
"The significance of goal-oriented dialog has led to its widespread use in artificial intelligence. This type of dialogue occurs when a questioner poses a question related to an action, and the answerer provides a response that helps the questioner determine the appropriate course of action. In recent years, deep learning and reinforcement learning have been employed to formulate relevant questions. However, these methods face difficulties in developing a proficient recurrent neural questioner due to the complexity of learning a sequence of sentences. Inspired by the theory of mind, we introduce a new information theoretic algorithm for goal-oriented dialog called ""Answerer in Questioner's Mind"" (AQM). With AQM, the questioner makes inquiries and infers using an estimated probabilistic model of the answerer. By explicitly calculating the information gain of candidate intentions and possible answers for each question, the questioner determines the answerer's intention. We applied AQM to two goal-oriented visual dialog tasks: ""MNIST Counting Dialog"" and ""GuessWhat?!"". Our experiments showed that AQM outperforms other algorithms by a significant margin.",1
"In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.",0
"This paper explores the challenge of acquiring vision-based dynamic manipulation skills through a scalable reinforcement learning technique. The study focuses on the problem of grasping, which has long been a challenge in robotic manipulation. Unlike traditional static learning behaviors that select a grasp point and execute the desired grasp, our approach enables closed-loop vision-based control. This means that the robot continuously updates its grasp strategy based on the latest observations to optimize long-term grasp success. We introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that leverages over 580k real-world grasp attempts. The deep neural network Q-function with over 1.2M parameters can perform closed-loop, real-world grasping with a 96% success rate on unseen objects. Our method not only achieves a high success rate but also exhibits unique behaviors compared to standard grasping systems. Using only RGB vision-based perception from an over-the-shoulder camera, our approach automatically learns regrasping strategies, probes objects to determine the most effective grasps, learns to reposition objects, and performs other non-prehensile pre-grasp manipulations while dynamically responding to disturbances and perturbations.",1
"Learning to control an environment without hand-crafted rewards or expert data remains challenging and is at the frontier of reinforcement learning research. We present an unsupervised learning algorithm to train agents to achieve perceptually-specified goals using only a stream of observations and actions. Our agent simultaneously learns a goal-conditioned policy and a goal achievement reward function that measures how similar a state is to the goal state. This dual optimization leads to a co-operative game, giving rise to a learned reward function that reflects similarity in controllable aspects of the environment instead of distance in the space of observations. We demonstrate the efficacy of our agent to learn, in an unsupervised manner, to reach a diverse set of goals on three domains -- Atari, the DeepMind Control Suite and DeepMind Lab.",0
"It is difficult to learn how to control an environment without pre-designed rewards or expert data, and this is a topic of research within reinforcement learning. Our research introduces an algorithm for training agents to achieve specified goals using only observations and actions, without supervision. The agent learns both a policy for achieving goals and a reward function that measures similarity between the current state and the goal state. By optimizing both of these simultaneously, the agent learns to cooperate with itself and generate a reward function that reflects controllable aspects of the environment instead of observations. We demonstrate the effectiveness of our algorithm by showing that it can learn to achieve a diverse set of goals in three domains: Atari, the DeepMind Control Suite, and DeepMind Lab, all without supervision.",1
"With the advent of the Internet of Things (IoT), an increasing number of energy harvesting methods are being used to supplement or supplant battery based sensors. Energy harvesting sensors need to be configured according to the application, hardware, and environmental conditions to maximize their usefulness. As of today, the configuration of sensors is either manual or heuristics based, requiring valuable domain expertise. Reinforcement learning (RL) is a promising approach to automate configuration and efficiently scale IoT deployments, but it is not yet adopted in practice. We propose solutions to bridge this gap: reduce the training phase of RL so that nodes are operational within a short time after deployment and reduce the computational requirements to scale to large deployments. We focus on configuration of the sampling rate of indoor solar panel based energy harvesting sensors. We created a simulator based on 3 months of data collected from 5 sensor nodes subject to different lighting conditions. Our simulation results show that RL can effectively learn energy availability patterns and configure the sampling rate of the sensor nodes to maximize the sensing data while ensuring that energy storage is not depleted. The nodes can be operational within the first day by using our methods. We show that it is possible to reduce the number of RL policies by using a single policy for nodes that share similar lighting conditions.",0
"Due to the Internet of Things (IoT), more energy harvesting methods are being utilized to replace battery-based sensors. These sensors require configuration based on the application, hardware, and environmental conditions to maximize their use, which currently relies on manual or heuristic-based methods. Reinforcement learning (RL) is a promising approach to automate configuration and enhance IoT scalability, but it has not yet been widely adopted. To address this, we propose solutions to shorten the RL training phase and reduce computational requirements to enable scaling. Our focus is on configuring the sampling rate of indoor solar panel-based energy harvesting sensors. We developed a simulator based on 3 months of data from 5 sensor nodes with varying lighting conditions. We demonstrate that RL can learn energy availability patterns and optimize the sampling rate to maximize data while preserving energy storage. Our methods can make nodes operational within a day, and we show that it is possible to reduce the number of RL policies by using a single policy for nodes with similar lighting conditions.",1
"We seek to automate the design of molecules based on specific chemical properties. Our primary contributions are a simpler method for generating SMILES strings guaranteed to be chemically valid, using a combination of a new context-free grammar for SMILES and additional masking logic; and casting the molecular property optimization as a reinforcement learning problem, specifically best-of-batch policy gradient applied to a Transformer model architecture. This approach uses substantially fewer model steps per atom than earlier approaches, thus enabling generation of larger molecules, and beats previous state-of-the art baselines by a significant margin. Applying reinforcement learning to a combination of a custom context-free grammar with additional masking to enforce non-local constraints is applicable to any optimization of a graph structure under a mixture of local and nonlocal constraints.",0
"Our objective is to automate the process of designing molecules based on specific chemical properties. Our primary contributions include a simpler method of generating SMILES strings that are chemically valid by implementing a new context-free grammar for SMILES and using additional masking logic. We also use reinforcement learning to optimize molecular properties, specifically through a best-of-batch policy gradient applied to a Transformer model architecture. This approach requires fewer model steps per atom, allowing for the generation of larger molecules, and outperforms previous state-of-the-art baselines by a significant margin. Additionally, this approach can be applied to optimizing any graph structure under a combination of local and non-local constraints by combining a custom context-free grammar with additional masking to enforce these constraints.",1
"Incorporating various modes of information into the machine learning procedure is becoming a new trend. And data from various source can provide more information than single one no matter they are heterogeneous or homogeneous. Existing deep learning based algorithms usually directly concatenate features from each domain to represent the input data. Seldom of them take the quality of data into consideration which is a key issue in related multimodal problems. In this paper, we propose an efficient quality-aware deep neural network to model the weight of data from each domain using deep reinforcement learning (DRL). Specifically, we take the weighting of each domain as a decision-making problem and teach an agent learn to interact with the environment. The agent can tune the weight of each domain through discrete action selection and obtain a positive reward if the saliency results are improved. The target of the agent is to achieve maximum rewards after finished its sequential action selection. We validate the proposed algorithms on multimodal saliency detection in a coarse-to-fine way. The coarse saliency maps are generated from an encoder-decoder framework which is trained with content loss and adversarial loss. The final results can be obtained via adaptive weighting of maps from each domain. Experiments conducted on two kinds of salient object detection benchmarks validated the effectiveness of our proposed quality-aware deep neural network.",0
"The integration of multiple forms of information in the machine learning process has become a recent trend. Utilizing data from various sources, regardless of whether they are diverse or similar, can provide a more comprehensive understanding. Typically, existing deep learning algorithms concatenate features from each domain to represent the input data, without considering the quality of the data, which is crucial in multimodal problems. In this study, we propose an efficient deep neural network that uses deep reinforcement learning to determine the weight of data from each domain. The approach treats the weighting of each domain as a decision-making problem, and the agent learns to interact with the environment to optimize the weighting of each domain. By selecting discrete actions and obtaining positive rewards for improved saliency results, the agent aims to achieve maximum rewards after sequential action selection. Our proposed algorithm is validated through experiments on multimodal saliency detection, which involves generating coarse saliency maps from an encoder-decoder framework, followed by adaptive weighting of maps from each domain to obtain the final results. Results from experiments on two salient object detection benchmarks illustrate the effectiveness of our proposed quality-aware deep neural network.",1
"We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.   Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.",0
"In this article, we provide an overview of the recent accomplishments of deep reinforcement learning (RL). We cover six essential components, six significant mechanisms, and twelve applications. Our discussion begins with the foundation of machine learning, deep learning, and reinforcement learning. The core elements of RL are then examined, including value function, specifically Deep Q-Network (DQN), policy, reward, model, planning, and exploration. We then delve into crucial mechanisms for RL, such as attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Furthermore, we describe diverse RL applications, such as games, AlphaGo, robotics, natural language processing, dialogue systems, machine translation, text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. Lastly, we mention unexplored topics and provide a list of RL resources. Our article concludes with a brief summary and discussions. For a significant update, please refer to Deep Reinforcement Learning, arXiv:1810.06339.",1
"In this work, we present our various contributions to the objective of building a decision support tool for the diagnosis of rare diseases. Our goal is to achieve a state of knowledge where the uncertainty about the patient's disease is below a predetermined threshold. We aim to reach such states while minimizing the average number of medical tests to perform. In doing so, we take into account the need, in many medical applications, to avoid, as much as possible, any misdiagnosis. To solve this optimization task, we investigate several reinforcement learning algorithm and make them operable in our high-dimensional and sparse-reward setting. We also present a way to combine expert knowledge, expressed as conditional probabilities, with real clinical data. This is crucial because the scarcity of data in the field of rare diseases prevents any approach based solely on clinical data. Finally we show that it is possible to integrate the ontological information about symptoms while remaining in our probabilistic reasoning. It enables our decision support tool to process information given at different level of precision by the user.",0
"Our work focuses on creating a decision support tool for diagnosing rare diseases with minimal medical testing while ensuring a predetermined level of certainty in the diagnosis. We prioritize avoiding misdiagnosis and address the challenge of operating in a high-dimensional, sparse-reward environment by exploring various reinforcement learning algorithms. To overcome data scarcity in rare diseases, we combine expert knowledge expressed as conditional probabilities with clinical data. Additionally, we demonstrate the integration of ontological symptom information for flexible processing of user-provided data.",1
"We demonstrate the use of conditional autoregressive generative models (van den Oord et al., 2016a) over a discrete latent space (van den Oord et al., 2017b) for forward planning with MCTS. In order to test this method, we introduce a new environment featuring varying difficulty levels, along with moving goals and obstacles. The combination of high-quality frame generation and classical planning approaches nearly matches true environment performance for our task, demonstrating the usefulness of this method for model-based planning in dynamic environments.",0
"Our study showcases the employment of generative models with a conditional autoregressive nature (van den Oord et al., 2016a) on a latent space that is discrete (van den Oord et al., 2017b) to enable forward planning with MCTS. To assess the efficacy of this approach, we created a fresh environment that has obstacles and goals in motion, and difficulty levels that vary. Our method, which utilizes both classical planning methods and superior quality frame generation, nearly matches authentic environmental performance for our task, thus proving the potential of this technique for model-based planning in dynamic environments.",1
"Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. HRGR-Agent employs a hierarchical decision-making procedure. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets, generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition, our model achieves the highest detection accuracy of medical terminologies, and improved human evaluation performance.",0
"The task of creating comprehensive reports for medical images is difficult because it requires bridging visual patterns with descriptive language. To address this challenge, we propose a new approach called the Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent). This method combines traditional retrieval-based techniques with modern learning-based methods, allowing for structured, robust, and diverse report generation. The HRGR-Agent uses a hierarchical decision-making process to determine whether to retrieve a sentence template from a pre-existing database or to generate a new sentence. The model is updated through reinforcement learning, guided by sentence-level and word-level rewards. Our experiments demonstrate that the HRGR-Agent achieves state-of-the-art results on two medical report datasets, producing well-balanced sentences and accurately identifying medical terminologies. Additionally, our model improves human evaluation performance.",1
"Recent machine learning models have shown that including attention as a component results in improved model accuracy and interpretability, despite the concept of attention in these approaches only loosely approximating the brain's attention mechanism. Here we extend this work by building a more brain-inspired deep network model of the primate ATTention Network (ATTNet) that learns to shift its attention so as to maximize the reward. Using deep reinforcement learning, ATTNet learned to shift its attention to the visual features of a target category in the context of a search task. ATTNet's dorsal layers also learned to prioritize these shifts of attention so as to maximize success of the ventral pathway classification and receive greater reward. Model behavior was tested against the fixations made by subjects searching images for the same cued category. Both subjects and ATTNet showed evidence for attention being preferentially directed to target goals, behaviorally measured as oculomotor guidance to targets. More fundamentally, ATTNet learned to shift its attention to target like objects and spatially route its visual inputs to accomplish the task. This work makes a step toward a better understanding of the role of attention in the brain and other computational systems.",0
"Recent advancements in machine learning have demonstrated that incorporating attention as a component in models leads to enhanced accuracy and interpretability. However, the attention mechanism used in these approaches only approximates the brain's attention mechanism. This study expands on this knowledge by constructing a deep network model called the primate ATTention Network (ATTNet) that is more brain-inspired. ATTNet uses deep reinforcement learning to learn how to shift attention to maximize rewards in a search task. The dorsal layers of ATTNet prioritize attention shifts to increase the success of the ventral pathway classification and receive more significant rewards. This study compared the behavior of ATTNet to human subjects searching for the same cued category and found that both preferentially directed attention to target goals. ATTNet also learned to shift attention to target-like objects and spatially route its visual inputs to execute the task successfully. These findings contribute to a better understanding of the role of attention in the brain and other computational systems.",1
"Predicting the structure of a protein from its sequence is a cornerstone task of molecular biology. Established methods in the field, such as homology modeling and fragment assembly, appeared to have reached their limit. However, this year saw the emergence of promising new approaches: end-to-end protein structure and dynamics models, as well as reinforcement learning applied to protein folding. For these approaches to be investigated on a larger scale, an efficient implementation of their key computational primitives is required. In this paper we present a library of differentiable mappings from two standard dihedral-angle representations of protein structure (full-atom representation ""$\phi,\psi,\omega,\chi$"" and backbone-only representation ""$\phi,\psi,\omega$"") to atomic Cartesian coordinates. The source code and documentation can be found at https://github.com/lupoglaz/TorchProteinLibrary.",0
"The prediction of a protein's structure from its sequence is a fundamental task in molecular biology, and established methods like homology modeling and fragment assembly have hit a plateau. However, promising new approaches have surfaced this year, including end-to-end protein structure and dynamics models and reinforcement learning techniques applied to protein folding. To explore these approaches further, efficient implementation of their key computational functions is necessary. This paper introduces a library of differentiable mappings that can convert two standard dihedral-angle representations of protein structure (full-atom representation ""$\phi,\psi,\omega,\chi$"" and backbone-only representation ""$\phi,\psi,\omega$"") into atomic Cartesian coordinates. The source code and documentation can be accessed at https://github.com/lupoglaz/TorchProteinLibrary.",1
"Sepsis is a dangerous condition that is a leading cause of patient mortality. Treating sepsis is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. In this work, we explore the use of continuous state-space model-based reinforcement learning (RL) to discover high-quality treatment policies for sepsis patients. Our quantitative evaluation reveals that by blending the treatment strategy discovered with RL with what clinicians follow, we can obtain improved policies, potentially allowing for better medical treatment for sepsis.",0
"Sepsis, which is a major cause of patient death, poses a significant challenge in terms of treatment due to the varying responses of patients to medical interventions and the lack of a widely accepted treatment. This study investigates the potential of using continuous state-space model-based reinforcement learning (RL) to identify effective treatment policies for sepsis patients. Our evaluation demonstrates that by combining the RL-discovered treatment approach with the conventional clinical approach, we can develop superior policies that may enhance the management of sepsis.",1
"In hierarchical reinforcement learning a major challenge is determining appropriate low-level policies. We propose an unsupervised learning scheme, based on asymmetric self-play from Sukhbaatar et al. (2018), that automatically learns a good representation of sub-goals in the environment and a low-level policy that can execute them. A high-level policy can then direct the lower one by generating a sequence of continuous sub-goal vectors. We evaluate our model using Mazebase and Mujoco environments, including the challenging AntGather task. Visualizations of the sub-goal embeddings reveal a logical decomposition of tasks within the environment. Quantitatively, our approach obtains compelling performance gains over non-hierarchical approaches.",0
"Determining appropriate low-level policies is a major challenge in hierarchical reinforcement learning. We propose an unsupervised learning method, which is based on Sukhbaatar et al.'s (2018) asymmetric self-play, to automatically learn a good representation of sub-goals in the environment and a low-level policy that can execute them. By generating a sequence of continuous sub-goal vectors, a high-level policy can direct the lower one. We evaluate our model in Mazebase and Mujoco environments, including the difficult AntGather task. Visualizations of the sub-goal embeddings show a logical decomposition of tasks in the environment. Our approach obtains significant performance gains over non-hierarchical methods according to quantitative measures.",1
"Machine learning applications in medical imaging are frequently limited by the lack of quality labeled data. In this paper, we explore the self training method, a form of semi-supervised learning, to address the labeling burden. By integrating reinforcement learning, we were able to expand the application of self training to complex segmentation networks without any further human annotation. The proposed approach, reinforced self training (ReST), fine tunes a semantic segmentation networks by introducing a policy network that learns to generate pseudolabels. We incorporate an expert demonstration network, based on inverse reinforcement learning, to enhance clinical validity and convergence of the policy network. The model was tested on a pulmonary nodule segmentation task in chest X-rays and achieved the performance of a standard U-Net while using only 50% of the labeled data, by exploiting unlabeled data. When the same number of labeled data was used, a moderate to significant cross validation accuracy improvement was achieved depending on the absolute number of labels used.",0
"The lack of quality labeled data often limits the effectiveness of machine learning applications in medical imaging. In this study, we examine the self training method as a solution to the labeling burden. Through the integration of reinforcement learning, we have extended the use of self training to complex segmentation networks without the need for human annotation. Our proposed method, reinforced self training (ReST), involves fine tuning a semantic segmentation network by introducing a policy network that generates pseudolabels. To enhance the clinical validity and convergence of the policy network, we have incorporated an expert demonstration network based on inverse reinforcement learning. We tested the model on a pulmonary nodule segmentation task in chest X-rays and achieved similar performance to a standard U-Net using only 50% of the labeled data. A moderate to significant improvement in cross-validation accuracy was also achieved when the same number of labeled data was used, depending on the absolute number of labels.",1
"We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.",0
"Our proposed framework addresses the challenge of ensuring the safe behavior of a reinforcement learning agent in cases where the reward function is difficult to define. To achieve this, we leverage expert policy demonstrations and establish a theoretical framework for the agent to optimize rewards that align with its current knowledge. We present two solutions to tackle the resulting optimization problem: an ellipsoid-based approach and a variation on the ""follow-the-perturbed-leader"" algorithm. Our experiments showcase the effectiveness of our algorithm in both continuous and discrete problems, with the trained agent avoiding hazardous states while emulating the expert's actions in other states.",1
"We consider the problem of high-level strategy selection in the adversarial setting of real-time strategy games from a reinforcement learning perspective, where taking an action corresponds to switching to the respective strategy. Here, a good strategy successfully counters the opponent's current and possible future strategies which can only be estimated using partial observations. We investigate whether we can utilize the full game state information during training time (in the form of an auxiliary prediction task) to increase performance. Experiments carried out within a StarCraft: Brood War bot against strong community bots show substantial win rate improvements over a fixed-strategy baseline and encouraging results when learning with the auxiliary task.",0
"From a reinforcement learning viewpoint, we explore the challenge of selecting high-level strategies in real-time strategy games in an adversarial environment, where each action taken corresponds to a strategy switch. In this context, an effective strategy must be able to counter both the opponent's current and potential future strategies, which can only be approximated based on limited observations. Our objective is to investigate whether the performance of the strategy selection process can be improved by leveraging complete game state information available during training, which can be accomplished through an auxiliary prediction task. Our experiments, which were carried out using a StarCraft: Brood War bot against well-established community bots, demonstrate significant enhancements in win rates over a fixed-strategy baseline and promising outcomes when the auxiliary task is employed for learning.",1
"We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time. Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger.",0
"We have developed Intelligent Annotation Dialogs that utilize a trained agent to guide a human annotator in producing bounding boxes quickly and accurately. Two actions are considered: box verification and manual box drawing. Our agents are of two types, one that predicts the probability of positive verification and the other based on reinforcement learning. Our experiments show that our agents can learn efficient annotation strategies in different scenarios, adapting to the image difficulty, quality of boxes, and detector strength. The annotation dialogs speed up annotation and outperform any fixed combination of verification and drawing. Finally, in a realistic scenario, our agents evolve strategies that reflect the changing trade-off between verification and drawing as the detector becomes stronger.",1
"This paper proposes a new optimization objective for value-based deep reinforcement learning. We extend conventional Deep Q-Networks (DQNs) by adding a model-learning component yielding a transcoder network. The prediction errors for the model are included in the basic DQN loss as additional regularizers. This augmented objective leads to a richer training signal that provides feedback at every time step. Moreover, because learning an environment model shares a common structure with the RL problem, we hypothesize that the resulting objective improves both sample efficiency and performance. We empirically confirm our hypothesis on a range of 20 games from the Atari benchmark attaining superior results over vanilla DQN without model-based regularization.",0
"In this paper, a new optimization objective is suggested for value-based deep reinforcement learning. The conventional Deep Q-Networks (DQNs) are expanded by the addition of a model-learning component, creating a transcoder network. The prediction errors for the model are used as additional regularizers in the basic DQN loss. This updated objective provides a more comprehensive training signal, giving feedback at every time step. As learning an environment model shares a common structure with the RL problem, the resulting objective is expected to improve both sample efficiency and performance. Our hypothesis is supported by empirical confirmation on a range of 20 games from the Atari benchmark, with superior results obtained over vanilla DQN without model-based regularization.",1
"Representation learning of pedestrian trajectories transforms variable-length timestamp-coordinate tuples of a trajectory into a fixed-length vector representation that summarizes spatiotemporal characteristics. It is a crucial technique to connect feature-based data mining with trajectory data. Trajectory representation is a challenging problem, because both environmental constraints (e.g., wall partitions) and temporal user dynamics should be meticulously considered and accounted for. Furthermore, traditional sequence-to-sequence autoencoders using maximum log-likelihood often require dataset covering all the possible spatiotemporal characteristics to perform well. This is infeasible or impractical in reality. We propose TREP, a practical pedestrian trajectory representation learning algorithm which captures the environmental constraints and the pedestrian dynamics without the need of any training dataset. By formulating a sequence-to-sequence autoencoder with a spatial-aware objective function under the paradigm of actor-critic reinforcement learning, TREP intelligently encodes spatiotemporal characteristics of trajectories with the capability of handling diverse trajectory patterns. Extensive experiments on both synthetic and real datasets validate the high fidelity of TREP to represent trajectories.",0
"The process of representation learning for pedestrian trajectories involves converting trajectory data, which includes varying lengths of timestamp-coordinate pairs, into a condensed, fixed-length vector representation that summarizes the spatiotemporal features. This technique is crucial to connect feature-based data mining with trajectory data. The problem of trajectory representation is challenging due to the need to account for environmental constraints, such as wall partitions, as well as temporal user dynamics. Traditional sequence-to-sequence autoencoders that rely on maximum log-likelihood often require datasets covering all possible spatiotemporal characteristics, which is impractical. To address this issue, we propose TREP, a pedestrian trajectory representation learning algorithm that captures both environmental constraints and pedestrian dynamics without the need for training datasets. TREP uses a sequence-to-sequence autoencoder with a spatial-aware objective function under the actor-critic reinforcement learning paradigm to intelligently encode spatiotemporal characteristics of trajectories, making it capable of handling diverse trajectory patterns. Extensive experiments on synthetic and real datasets demonstrate TREP's ability to accurately represent trajectories.",1
"The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.",0
"Significant progress has been made with the broad range of deep generative models (DGMs); however, integrating complex structured domain knowledge into end-to-end DGMs can often be challenging. Posterior regularization (PR) is a systematic approach that enables the imposition of structured constraints on probabilistic models but has limited applicability in the case of diverse DGMs that may lack a Bayesian formulation or explicit density evaluation. Furthermore, PR necessitates the complete specification of constraints a priori, which may be impractical or suboptimal for complex knowledge with uncertain parts that can be learned. In this paper, we establish a mathematical correspondence between PR and reinforcement learning (RL), and using this connection, we expand PR to learn constraints similar to extrinsic rewards in RL. This algorithm is model-agnostic, making it usable across any DGMs, and it is flexible enough to allow the adaptation of arbitrary constraints with the model jointly. Our experiments, which involve generating human images and templated sentences, demonstrate that models with learned knowledge constraints using our algorithm significantly improve upon base generative models.",1
"Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains. Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance.",0
"Policy gradient methods are commonly used reinforcement learning algorithms that are model-free. These methods utilize a state-dependent baseline to reduce the variance of the gradient estimator. Recently, some papers have suggested that introducing an action-dependent baseline alongside the state-dependent baseline can further reduce variance and improve sample efficiency without introducing bias into the gradient estimates. In this study, we sought to examine this development by breaking down the variance of the policy gradient estimator. We conducted numerical analyses and found that learned state-action-dependent baselines do not decrease variance compared to a state-dependent baseline in commonly tested benchmark domains. We validated this unexpected result by reviewing the open-source code accompanying the prior papers and discovered that subtle implementation decisions deviated from the methods presented in the papers, explaining the previously observed empirical gains. Moreover, our variance decomposition revealed areas for improvement, which we demonstrated by illustrating a simple change to the typical value function parameterization that can significantly enhance performance.",1
"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",0
"The challenge of utilizing reinforcement learning algorithms in real-world scenarios stems from the absence of appropriate reward functions. The task of constructing these functions is complicated because users possess only an implicit grasp of the objective. This results in the agent alignment issue, which concerns creating agents that operate in accordance with the user's intentions. We propose a research direction to tackle this problem by concentrating on reward modeling, which involves learning a reward function through user interaction and optimizing it with reinforcement learning. We also address the primary hurdles that we may encounter while scaling reward modeling to complex and universal domains, suggest practical methods to mitigate these challenges, and explore ways to establish confidence in the agents generated.",1
"A* is a popular path-finding algorithm, but it can only be applied to those domains where a good heuristic function is known. Inspired by recent methods combining Deep Neural Networks (DNNs) and trees, this study demonstrates how to train a heuristic represented by a DNN and combine it with A*. This new algorithm which we call aleph-star can be used efficiently in domains where the input to the heuristic could be processed by a neural network. We compare aleph-star to N-Step Deep Q-Learning (DQN Mnih et al. 2013) in a driving simulation with pixel-based input, and demonstrate significantly better performance in this scenario.",0
"The A* algorithm is widely used for path-finding, but its application is limited to domains where a reliable heuristic function exists. This study takes inspiration from recent techniques that merge Deep Neural Networks (DNNs) with trees. It shows how to teach a DNN-based heuristic and integrate it with A*, resulting in a new algorithm called aleph-star. This approach is effective in domains where the neural network can process the heuristic input. We compare aleph-star with N-Step Deep Q-Learning (DQN Mnih et al. 2013) in a driving simulation that uses pixel-based input. Our results indicate that aleph-star performs significantly better in this scenario.",1
"Learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning due to the model uncertainty inherent in the problem. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, the method is capable of learning complex uncertainty structure beyond a point estimate or a simple Gaussian approximation. In addition, a robust Bayesian meta-update mechanism with a new meta-loss prevents overfitting during meta-update. Remaining an efficient gradient-based meta-learner, the method is also model-agnostic and simple to implement. Experiment results show the accuracy and robustness of the proposed method in various tasks: sinusoidal regression, image classification, active learning, and reinforcement learning.",0
"An important aspect of achieving strong meta-learning is the ability to infer Bayesian posterior from a few-shot dataset, as model uncertainty is inherent in the problem. This paper introduces a novel Bayesian model-agnostic meta-learning approach that combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, this method can learn complex uncertainty structures beyond a point estimate or a simple Gaussian approximation. Additionally, a new meta-loss is introduced to prevent overfitting during meta-update, resulting in a robust Bayesian meta-update mechanism. As an efficient gradient-based meta-learner, the approach is also model-agnostic and straightforward to implement. Experiment results demonstrate the proposed method's accuracy and robustness across various tasks, such as sinusoidal regression, image classification, active learning, and reinforcement learning.",1
"In the quest for efficient and robust reinforcement learning methods, both model-free and model-based approaches offer advantages. In this paper we propose a new way of explicitly bridging both approaches via a shared low-dimensional learned encoding of the environment, meant to capture summarizing abstractions. We show that the modularity brought by this approach leads to good generalization while being computationally efficient, with planning happening in a smaller latent state space. In addition, this approach recovers a sufficient low-dimensional representation of the environment, which opens up new strategies for interpretable AI, exploration and transfer learning.",0
"Efficient and strong reinforcement learning methods can be achieved through both model-free and model-based approaches. This paper presents a novel method of combining both approaches by utilizing a shared low-dimensional encoding of the environment to capture abstract summaries. The modular nature of this approach results in excellent generalization and computational efficiency, with planning taking place in a smaller latent state space. Furthermore, this method produces a satisfactory low-dimensional representation of the environment, providing opportunities for interpretable AI, exploration, and transfer learning.",1
"Model-free reinforcement learning methods such as the Proximal Policy Optimization algorithm (PPO) have successfully applied in complex decision-making problems such as Atari games. However, these methods suffer from high variances and high sample complexity. On the other hand, model-based reinforcement learning methods that learn the transition dynamics are more sample efficient, but they often suffer from the bias of the transition estimation. How to make use of both model-based and model-free learning is a central problem in reinforcement learning. In this paper, we present a new technique to address the trade-off between exploration and exploitation, which regards the difference between model-free and model-based estimations as a measure of exploration value. We apply this new technique to the PPO algorithm and arrive at a new policy optimization method, named Policy Optimization with Model-based Explorations (POME). POME uses two components to predict the actions' target values: a model-free one estimated by Monte-Carlo sampling and a model-based one which learns a transition model and predicts the value of the next state. POME adds the error of these two target estimations as the additional exploration value for each state-action pair, i.e, encourages the algorithm to explore the states with larger target errors which are hard to estimate. We compare POME with PPO on Atari 2600 games, and it shows that POME outperforms PPO on 33 games out of 49 games.",0
"Successful application of model-free reinforcement learning methods like the Proximal Policy Optimization algorithm (PPO) in complex decision-making problems like Atari games is known. However, these methods are prone to high variances and sample complexity. Conversely, model-based reinforcement learning methods that learn transition dynamics are more sample efficient, but transition estimation bias is a common issue. Hence, combining model-based and model-free learning is a central challenge in reinforcement learning. This paper presents a new exploration-exploitation trade-off technique that measures exploration value by the difference between model-free and model-based estimations. The technique is applied to PPO to create a new policy optimization method called Policy Optimization with Model-based Explorations (POME). POME predicts action target values using two components: a model-free component estimated by Monte-Carlo sampling and a model-based component that learns a transition model and predicts the value of the next state. POME adds the error of these two target estimations as additional exploration value for each state-action pair, encouraging exploration of states with larger target errors that are difficult to estimate. POME outperforms PPO on 33 out of 49 Atari 2600 games, as per the comparison.",1
"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries. In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",0
"Industry is increasingly using Deep Learning for computer vision on embedded devices, but Convolutional Neural Networks' accuracy has plateaued, and inference latency and throughput are major concerns, especially for low-cost and low-power devices. This bottleneck may hinder Deep Learning's adoption, and CNN deployment across different platforms faces issues due to vendor-specific technology and acceleration libraries. In this study, we introduce QS-DNN, an automatic search using Reinforcement Learning that, with an inference engine optimizer, explores the design space and finds the best combinations of libraries and primitives to speed up CNN inference on heterogeneous embedded devices. We demonstrate that an optimized combination achieves a 45x speedup in inference latency on CPU and a 2x speedup on average on GPGPU compared to the best vendor library. Moreover, our method yields better results and shorter ""to-solution"" time than Random Search, with up to 15x improvement in a short-time search.",1
"Multi-agent learning provides a potential framework for learning and simulating traffic behaviors. This paper proposes a novel architecture to learn multiple driving behaviors in a traffic scenario. The proposed architecture can learn multiple behaviors independently as well as simultaneously. We take advantage of the homogeneity of agents and learn in a parameter sharing paradigm. To further speed up the training process asynchronous updates are employed into the architecture. While learning different behaviors simultaneously, the given framework was also able to learn cooperation between the agents, without any explicit communication. We applied this framework to learn two important behaviors in driving: 1) Lane-Keeping and 2) Over-Taking. Results indicate faster convergence and learning of a more generic behavior, that is scalable to any number of agents. When compared the results with existing approaches, our results indicate equal and even better performance in some cases.",0
"The possibility of learning and simulating traffic behaviors using multi-agent learning is explored in this paper. A new architecture is proposed to learn various driving behaviors in a traffic scenario, which can learn multiple behaviors both independently and simultaneously. The homogeneity of agents is taken advantage of, and a parameter sharing paradigm is used to learn. To speed up the training process, asynchronous updates are employed. Cooperation between agents is learned implicitly without explicit communication. Two significant driving behaviors, Lane-Keeping and Over-Taking, are learned using this framework. The results show that the proposed approach has faster convergence and can learn more generic behavior that is scalable to any number of agents. Compared to existing approaches, our results demonstrate similar or better performance.",1
"The class of Gaussian Process (GP) methods for Temporal Difference learning has shown promise for data-efficient model-free Reinforcement Learning. In this paper, we consider a recent variant of the GP-SARSA algorithm, called Sparse Pseudo-input Gaussian Process SARSA (SPGP-SARSA), and derive recursive formulas for its predictive moments. This extension promotes greater memory efficiency, since previous computations can be reused and, interestingly, it provides a technique for updating value estimates on a multiple timescales",0
"Data-efficient model-free Reinforcement Learning has shown potential through the Gaussian Process (GP) methods class, particularly for Temporal Difference learning. The paper examines the Sparse Pseudo-input Gaussian Process SARSA (SPGP-SARSA), a recent variant of the GP-SARSA algorithm, and develops recursive formulas for its predictive moments. This approach enhances memory efficiency by reusing past computations and, notably, offers a method for updating value estimates at various timescales.",1
"We investigate sparse representations for control in reinforcement learning. While these representations are widely used in computer vision, their prevalence in reinforcement learning is limited to sparse coding where extracting representations for new data can be computationally intensive. Here, we begin by demonstrating that learning a control policy incrementally with a representation from a standard neural network fails in classic control domains, whereas learning with a representation obtained from a neural network that has sparsity properties enforced is effective. We provide evidence that the reason for this is that the sparse representation provides locality, and so avoids catastrophic interference, and particularly keeps consistent, stable values for bootstrapping. We then discuss how to learn such sparse representations. We explore the idea of Distributional Regularizers, where the activation of hidden nodes is encouraged to match a particular distribution that results in sparse activation across time. We identify a simple but effective way to obtain sparse representations, not afforded by previously proposed strategies, making it more practical for further investigation into sparse representations for reinforcement learning.",0
"Our research delves into the use of sparse representations in reinforcement learning, a technique commonly employed in computer vision but not as much in reinforcement learning due to its computational complexity in extracting representations for new data. Our study shows that learning a control policy incrementally with a representation from a standard neural network is ineffective in classic control domains, while using a representation obtained from a neural network with sparsity properties enforced yields better results. This is because the sparse representation provides locality, preventing catastrophic interference and ensuring consistent, stable values for bootstrapping. We also explore methods for learning such sparse representations, including Distributional Regularizers that encourage activation of hidden nodes to match a specific distribution resulting in sparse activation across time. Our approach offers a novel and practical strategy for further investigation into sparse representations for reinforcement learning.",1
"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",0
"In order to tackle complicated real-life issues through reinforcement learning, it is not feasible to depend on manually defined reward functions. Instead, the objective can be directly communicated to the agent by humans. This study combines two methods of learning from human feedback, namely expert demonstrations and trajectory preferences. A deep neural network is trained to create a model of the reward function, which is then used to train a deep reinforcement learning agent based on DQN for 9 Atari games. Our technique outperforms the imitation learning baseline in 7 games and accomplishes strictly superior performance in 2 games, without relying on game rewards. Moreover, we examine the suitability of the reward model, present some reward hacking issues, and explore the effects of noise in human labels.",1
"Robustness is important for sequential decision making in a stochastic dynamic environment with uncertain probabilistic parameters. We address the problem of using robust MDPs (RMDPs) to compute policies with provable worst-case guarantees in reinforcement learning. The quality and robustness of an RMDP solution is determined by its ambiguity set. Existing methods construct ambiguity sets that lead to impractically conservative solutions. In this paper, we propose RSVF, which achieves less conservative solutions with the same worst-case guarantees by 1) leveraging a Bayesian prior, 2) optimizing the size and location of the ambiguity set, and, most importantly, 3) relaxing the requirement that the set is a confidence interval. Our theoretical analysis shows the safety of RSVF, and the empirical results demonstrate its practical promise.",0
"In a dynamic environment with uncertain probabilities, making sequential decisions requires robustness. To ensure that reinforcement learning policies have provable worst-case guarantees, we explore the use of robust MDPs (RMDPs). However, the quality and robustness of RMDP solutions depend on their ambiguity sets, which can be constructed by existing methods in a manner that is overly conservative. To address this issue, we present RSVF, which achieves less conservative solutions while maintaining the same worst-case guarantees. By leveraging a Bayesian prior, optimizing ambiguity set size and location, and relaxing the requirement that the set be a confidence interval, RSVF offers practical promise. Our theoretical analysis confirms the safety of RSVF, and empirical results support its effectiveness.",1
"Dealing with uncertainty is essential for efficient reinforcement learning. There is a growing literature on uncertainty estimation for deep learning from fixed datasets, but many of the most popular approaches are poorly-suited to sequential decision problems. Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data. We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable `prior' network to each ensemble member. We prove that this approach is efficient with linear representations, provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.",0
"Efficient reinforcement learning requires the ability to deal with uncertainty. While there is a growing body of literature on uncertainty estimation for deep learning from fixed datasets, many of the most popular approaches are not suitable for sequential decision problems. Bootstrap sampling, for example, offers no means of accounting for uncertainty beyond the observed data. This can be a significant limitation, which we address by proposing the addition of a randomized untrainable 'prior' network to each ensemble member. Our approach is effective with linear representations, and we provide examples of its efficacy with nonlinear representations. Moreover, our approach scales to large-scale problems much better than previous attempts.",1
"Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for model-based policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.",0
"Reinforcement learning algorithms require large amounts of real experience, which can be costly to acquire. Learning policies on data synthesized by models can theoretically solve this problem, but simulating plausible experience can be difficult in complex environments and can lead to biases for model-based policy evaluation and search. Instead, this paper proposes the Counterfactually-Guided Policy Search (CF-GPS) algorithm, which assumes logged, real experience and models alternative outcomes of this experience under counterfactual actions. CF-GPS leverages structural causal models to evaluate arbitrary policies on individual off-policy episodes and improve on vanilla model-based RL algorithms by debiasing model predictions. Unlike off-policy algorithms based on Importance Sampling, which re-weight data, CF-GPS explicitly considers alternative outcomes and makes better use of experience data. Empirical results show that CF-GPS improves policy evaluation and search results on a non-trivial grid-world task, and it generalizes the previously proposed Guided Policy Search. The paper also suggests that reparameterization-based algorithms like Stochastic Value Gradient can be interpreted as counterfactual methods.",1
"Policy gradient methods are very attractive in reinforcement learning due to their model-free nature and convergence guarantees. These methods, however, suffer from high variance in gradient estimation, resulting in poor sample efficiency. To mitigate this issue, a number of variance-reduction approaches have been proposed. Unfortunately, in the challenging problems with delayed rewards, these approaches either bring a relatively modest improvement or do reduce variance at expense of introducing a bias and undermining convergence. The unbiased methods of gradient estimation, in general, only partially reduce variance, without eliminating it completely even in the limit of exact knowledge of the value functions and problem dynamics, as one might have wished. In this work we propose an unbiased method that does completely eliminate variance under some, commonly encountered, conditions. Of practical interest is the limit of deterministic dynamics and small policy stochasticity. In the case of a quadratic value function, as in linear quadratic Gaussian models, the policy randomness need not be small. We use such a model to analyze performance of the proposed variance-elimination approach and compare it with standard variance-reduction methods. The core idea behind the approach is to use control variates at all future times down the trajectory. We present both a model-based and model-free formulations.",0
"Reinforcement learning favors policy gradient methods for their model-free nature and convergence guarantees. However, these methods suffer from high variance in gradient estimation, leading to poor sample efficiency. Although variance-reduction approaches have been proposed, they often introduce bias and undermine convergence, providing only a modest improvement in challenging problems with delayed rewards. Even unbiased methods of gradient estimation only partially reduce variance, failing to eliminate it entirely even with exact knowledge of value functions and problem dynamics. Our work proposes an unbiased method that completely eliminates variance under commonly encountered conditions, particularly in the limit of small policy stochasticity and deterministic dynamics. We use a quadratic value function model to analyze the proposed variance-elimination approach and compare it with standard variance-reduction methods. The approach uses control variates throughout the trajectory and is presented in both model-based and model-free formulations.",1
"Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.",0
"A wide range of environments have shown that deep reinforcement learning techniques perform better. However, research on understanding the networks' learning has not kept up with the fast-paced advancements in training algorithms. This paper suggests an interpretable neural network architecture for Q-learning that explains the model's behavior using key-value memories, attention, and reconstructible embeddings. The model has a directed exploration strategy and can reach training rewards equivalent to state-of-the-art deep Q-learning models. Nevertheless, the features extracted by the neural network are shallow, and it is simple for the agent to overfit to training trajectories, as evidenced by out-of-sample testing.",1
"Deep neural networks have shown superior performance in many regimes to remember familiar patterns with large amounts of data. However, the standard supervised deep learning paradigm is still limited when facing the need to learn new concepts efficiently from scarce data. In this paper, we present a memory-augmented neural network which is motivated by the process of human concept learning. The training procedure, imitating the concept formation course of human, learns how to distinguish samples from different classes and aggregate samples of the same kind. In order to better utilize the advantages originated from the human behavior, we propose a sequential process, during which the network should decide how to remember each sample at every step. In this sequential process, a stable and interactive memory serves as an important module. We validate our model in some typical one-shot learning tasks and also an exploratory outlier detection problem. In all the experiments, our model gets highly competitive to reach or outperform those strong baselines.",0
"Although deep neural networks have been proven to excel at recognizing common patterns using vast amounts of data, the conventional supervised deep learning approach still has limitations when it comes to efficiently learning new concepts with limited data. This paper introduces a neural network with memory augmentation, inspired by the human concept learning process. By simulating human concept formation, the training process learns to differentiate between samples from different classes and combine samples of the same type. To optimize the advantages of human behavior, a sequential process is proposed where the network decides how to remember each sample at each step, utilizing a stable and interactive memory as a crucial module. The model is validated through various one-shot learning tasks and an exploratory outlier detection problem, where it performs competitively or outperforms strong baselines.",1
"While current benchmark reinforcement learning (RL) tasks have been useful to drive progress in the field, they are in many ways poor substitutes for learning with real-world data. By testing increasingly complex RL algorithms on low-complexity simulation environments, we often end up with brittle RL policies that generalize poorly beyond the very specific domain. To combat this, we propose three new families of benchmark RL domains that contain some of the complexity of the natural world, while still supporting fast and extensive data acquisition. The proposed domains also permit a characterization of generalization through fair train/test separation, and easy comparison and replication of results. Through this work, we challenge the RL research community to develop more robust algorithms that meet high standards of evaluation.",0
"Although current benchmark reinforcement learning (RL) tasks have driven progress in the field, they cannot replace learning from real-world data. The low-complexity simulation environments used to test complex RL algorithms often lead to policies that do not generalize well beyond specific domains. To address this issue, we introduce three new benchmark RL domains that mimic the complexity of the natural world but still enable quick and extensive data acquisition. These domains allow for fair train/test separation, easy comparison, and replication of results, and facilitate the evaluation of generalization. Our aim is to encourage the RL research community to develop more robust algorithms that meet high evaluation standards.",1
"Reinforcement learning (RL) has recently been introduced to interactive recommender systems (IRS) because of its nature of learning from dynamic interactions and planning for long-run performance. As IRS is always with thousands of items to recommend (i.e., thousands of actions), most existing RL-based methods, however, fail to handle such a large discrete action space problem and thus become inefficient. The existing work that tries to deal with the large discrete action space problem by utilizing the deep deterministic policy gradient framework suffers from the inconsistency between the continuous action representation (the output of the actor network) and the real discrete action. To avoid such inconsistency and achieve high efficiency and recommendation effectiveness, in this paper, we propose a Tree-structured Policy Gradient Recommendation (TPGR) framework, where a balanced hierarchical clustering tree is built over the items and picking an item is formulated as seeking a path from the root to a certain leaf of the tree. Extensive experiments on carefully-designed environments based on two real-world datasets demonstrate that our model provides superior recommendation performance and significant efficiency improvement over state-of-the-art methods.",0
"Interactive recommender systems (IRS) have started incorporating reinforcement learning (RL) due to its ability to learn from dynamic interactions and plan for long-term performance. However, existing RL-based methods struggle to handle the large discrete action space problem that arises when recommending from thousands of items. This inefficiency is compounded by the inconsistency between the continuous action representation and the real discrete action in the deep deterministic policy gradient framework. To address these issues, we introduce the Tree-structured Policy Gradient Recommendation (TPGR) framework, which utilizes a hierarchical clustering tree to select items. Our experiments on two real-world datasets show that TPGR outperforms state-of-the-art methods in both recommendation effectiveness and efficiency.",1
"This paper presents a novel approach to the technical analysis of wireheading in intelligent agents. Inspired by the natural analogues of wireheading and their prevalent manifestations, we propose the modeling of such phenomenon in Reinforcement Learning (RL) agents as psychological disorders. In a preliminary step towards evaluating this proposal, we study the feasibility and dynamics of emergent addictive policies in Q-learning agents in the tractable environment of the game of Snake. We consider a slightly modified settings for this game, in which the environment provides a ""drug"" seed alongside the original ""healthy"" seed for the consumption of the snake. We adopt and extend an RL-based model of natural addiction to Q-learning agents in this settings, and derive sufficient parametric conditions for the emergence of addictive behaviors in such agents. Furthermore, we evaluate our theoretical analysis with three sets of simulation-based experiments. The results demonstrate the feasibility of addictive wireheading in RL agents, and provide promising venues of further research on the psychopathological modeling of complex AI safety problems.",0
"This paper introduces a fresh approach to analyzing wireheading in intelligent agents. Drawing inspiration from natural examples of wireheading and their widespread occurrences, we suggest treating this phenomenon in Reinforcement Learning (RL) agents as a type of psychological disorder. To test this idea, we investigate the possibility of emergent addictive policies in Q-learning agents in the game of Snake. We adjust the game by adding a ""drug"" seed, and use an RL-based model of natural addiction to derive conditions for the emergence of addictive behavior in these agents. We then conduct three simulation-based experiments to validate our theoretical analysis, which shows the potential for further research on the modeling of complex AI safety issues related to psychopathology.",1
"Deep learning models are vulnerable to external attacks. In this paper, we propose a Reinforcement Learning (RL) based approach to generate adversarial examples for the pre-trained (target) models. We assume a semi black-box setting where the only access an adversary has to the target model is the class probabilities obtained for the input queries. We train a Deep Q Network (DQN) agent which, with experience, learns to attack only a small portion of image pixels to generate non-targeted adversarial images. Initially, an agent explores an environment by sequentially modifying random sets of image pixels and observes its effect on the class probabilities. At the end of an episode, it receives a positive (negative) reward if it succeeds (fails) to alter the label of the image. Experimental results with MNIST, CIFAR-10 and Imagenet datasets demonstrate that our RL framework is able to learn an effective attack policy.",0
"The vulnerability of deep learning models to outside attacks is a concern. This paper proposes an approach based on Reinforcement Learning (RL) to create adversarial examples for pre-trained (target) models. The setting assumed is semi-black-box, with adversaries only having access to class probabilities for input queries. A Deep Q Network (DQN) agent is trained to attack a small portion of image pixels, generating non-targeted adversarial images through experience. The agent explores the environment by sequentially modifying random sets of image pixels and receives positive (negative) rewards at the end of an episode if it succeeds (fails) to alter the image label. Experimental results using MNIST, CIFAR-10, and Imagenet datasets show that our RL framework is effective in learning an attack policy.",1
"We present a novel algorithm to train a deep Q-learning agent using natural-gradient techniques. We compare the original deep Q-network (DQN) algorithm to its natural-gradient counterpart, which we refer to as NGDQN, on a collection of classic control domains. Without employing target networks, NGDQN significantly outperforms DQN without target networks, and performs no worse than DQN with target networks, suggesting that NGDQN stabilizes training and can help reduce the need for additional hyperparameter tuning. We also find that NGDQN is less sensitive to hyperparameter optimization relative to DQN. Together these results suggest that natural-gradient techniques can improve value-function optimization in deep reinforcement learning.",0
"A new method for training a deep Q-learning agent using natural-gradient techniques is presented in this study. The original deep Q-network (DQN) algorithm is compared to its natural-gradient counterpart, called NGDQN, on a variety of classic control domains. NGDQN outperforms DQN without requiring target networks, and it performs just as well as DQN with target networks. This indicates that NGDQN can stabilize training and minimize the need for additional hyperparameter tuning. Moreover, the study found that NGDQN is less sensitive to hyperparameter optimization compared to DQN. Overall, these findings demonstrate that natural-gradient techniques can enhance value-function optimization in deep reinforcement learning.",1
"This paper tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep RL for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. We apply the proposed method to three image processing tasks: image denoising, image restoration, and local color enhancement. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning.",0
"This article deals with a novel challenge: using reinforcement learning with pixel-wise rewards (pixelRL) for image processing. Although deep RL has been highly successful since the introduction of the deep Q-network, its applications in image processing are still limited. In order to overcome this, we propose extending deep RL to pixelRL for various image processing purposes. In pixelRL, each pixel has its own agent that alters its value by taking an action. We have also devised an effective learning approach for pixelRL that greatly enhances performance by considering the future states of both the pixel and its neighboring pixels. This method can be used for image processing tasks that require pixel-wise manipulation, where deep RL has never been utilized. We have applied this method to three image processing tasks: image denoising, image restoration, and local color enhancement. Our experiments have demonstrated that our proposed method achieves comparable or superior performance when compared to state-of-the-art supervised learning methods.",1
"Modelling and exploiting teammates' policies in cooperative multi-agent systems have long been an interest and also a big challenge for the reinforcement learning (RL) community. The interest lies in the fact that if the agent knows the teammates' policies, it can adjust its own policy accordingly to arrive at proper cooperations; while the challenge is that the agents' policies are changing continuously due to they are learning concurrently, which imposes difficulty to model the dynamic policies of teammates accurately. In this paper, we present \emph{ATTention Multi-Agent Deep Deterministic Policy Gradient} (ATT-MADDPG) to address this challenge. ATT-MADDPG extends DDPG, a single-agent actor-critic RL method, with two special designs. First, in order to model the teammates' policies, the agent should get access to the observations and actions of teammates. ATT-MADDPG adopts a centralized critic to collect such information. Second, to model the teammates' policies using the collected information in an effective way, ATT-MADDPG enhances the centralized critic with an attention mechanism. This attention mechanism introduces a special structure to explicitly model the dynamic joint policy of teammates, making sure that the collected information can be processed efficiently. We evaluate ATT-MADDPG on both benchmark tasks and the real-world packet routing tasks. Experimental results show that it not only outperforms the state-of-the-art RL-based methods and rule-based methods by a large margin, but also achieves better performance in terms of scalability and robustness.",0
"For the reinforcement learning (RL) community, modelling and utilizing the policies of teammates in cooperative multi-agent systems has been a long-standing area of interest and a significant challenge. This is because knowing teammates' policies enables an agent to adjust its policy to achieve optimal cooperation, but the continuous learning of agents results in dynamic policy changes, making it difficult to accurately model teammates' policies. To overcome this challenge, we propose ATTention Multi-Agent Deep Deterministic Policy Gradient (ATT-MADDPG), an extension of DDPG, a single-agent actor-critic RL method. ATT-MADDPG features two special designs: a centralized critic that provides the agent access to teammates' observations and actions, and an attention mechanism that enhances the centralized critic by explicitly modelling the dynamic joint policy of teammates to ensure efficient processing of collected information. We evaluate ATT-MADDPG on benchmark tasks and real-world packet routing tasks, and our experimental results demonstrate that it outperforms the state-of-the-art RL-based and rule-based methods in terms of scalability, robustness, and overall performance.",1
"Hierarchical reinforcement learning (HRL) has recently shown promising advances on speeding up learning, improving the exploration, and discovering intertask transferable skills. Most recent works focus on HRL with two levels, i.e., a master policy manipulates subpolicies, which in turn manipulate primitive actions. However, HRL with multiple levels is usually needed in many real-world scenarios, whose ultimate goals are highly abstract, while their actions are very primitive. Therefore, in this paper, we propose a diversity-driven extensible HRL (DEHRL), where an extensible and scalable framework is built and learned levelwise to realize HRL with multiple levels. DEHRL follows a popular assumption: diverse subpolicies are useful, i.e., subpolicies are believed to be more useful if they are more diverse. However, existing implementations of this diversity assumption usually have their own drawbacks, which makes them inapplicable to HRL with multiple levels. Consequently, we further propose a novel diversity-driven solution to achieve this assumption in DEHRL. Experimental studies evaluate DEHRL with five baselines from four perspectives in two domains; the results show that DEHRL outperforms the state-of-the-art baselines in all four aspects.",0
"Recently, there have been promising advancements in Hierarchical Reinforcement Learning (HRL) that have led to faster learning, improved exploration, and the discovery of transferable skills between tasks. Most studies have focused on HRL with two levels, where a master policy controls subpolicies that manipulate primitive actions. However, in many real-world scenarios, multiple levels of HRL are necessary, as the ultimate goals are abstract while the actions are primitive. To address this, we propose Diversity-Driven Extensible HRL (DEHRL), which uses an extensible and scalable framework to learn HRL at multiple levels. DEHRL assumes that diverse subpolicies are useful, but previous implementations have limitations that make them unsuitable for multiple levels of HRL. Therefore, we propose a novel diversity-driven solution to address this issue. Experimental studies show that DEHRL outperforms state-of-the-art baselines in all four aspects evaluated across two domains.",1
"A crucial and time-sensitive task when any disaster occurs is to rescue victims and distribute resources to the right groups and locations. This task is challenging in populated urban areas, due to the huge burst of help requests generated in a very short period. To improve the efficiency of the emergency response in the immediate aftermath of a disaster, we propose a heuristic multi-agent reinforcement learning scheduling algorithm, named as ResQ, which can effectively schedule the rapid deployment of volunteers to rescue victims in dynamic settings. The core concept is to quickly identify victims and volunteers from social network data and then schedule rescue parties with an adaptive learning algorithm. This framework performs two key functions: 1) identify trapped victims and rescue volunteers, and 2) optimize the volunteers' rescue strategy in a complex time-sensitive environment. The proposed ResQ algorithm can speed up the training processes through a heuristic function which reduces the state-action space by identifying the set of particular actions over others. Experimental results showed that the proposed heuristic multi-agent reinforcement learning based scheduling outperforms several state-of-art methods, in terms of both reward rate and response times.",0
"When a disaster occurs, rescuing victims and distributing resources quickly and efficiently is crucial. This can be particularly challenging in densely populated urban areas where there is a high volume of help requests in a short period of time. To address this issue, we propose a heuristic multi-agent reinforcement learning scheduling algorithm called ResQ. ResQ uses social network data to identify victims and volunteers and then schedules rescue parties with an adaptive learning algorithm. This algorithm has two main functions: identifying trapped victims and rescue volunteers, and optimizing rescue strategies for volunteers in a complex, time-sensitive environment. ResQ speeds up the training process through a heuristic function that reduces the state-action space by identifying specific actions over others. Our experimental results demonstrate that ResQ outperforms several state-of-the-art methods in terms of reward rate and response times.",1
"The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose variational inverse control with events (VICE), which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.",0
"Real-world applications of reinforcement learning often face a practical challenge in designing a reward function. While approaches like inverse reinforcement learning aim to solve this problem, obtaining expert demonstrations can be difficult and expensive. To address this issue, we introduce variational inverse control with events (VICE) which extends inverse reinforcement learning to scenarios where full demonstrations are not required, such as when only goal state samples are available. Our approach is grounded in a different perspective on control and reinforcement learning, where the agent's objective is to maximize the likelihood of one or more future events occurring, rather than cumulative rewards. We demonstrate the efficacy of our method on continuous control tasks, particularly those with high-dimensional observations like images where defining rewards can be challenging or even impossible.",1
"Previous attempts for data augmentation are designed manually, and the augmentation policies are dataset-specific. Recently, an automatic data augmentation approach, named AutoAugment, is proposed using reinforcement learning. AutoAugment searches for the augmentation polices in the discrete search space, which may lead to a sub-optimal solution. In this paper, we employ the Augmented Random Search method (ARS) to improve the performance of AutoAugment. Our key contribution is to change the discrete search space to continuous space, which will improve the searching performance and maintain the diversities between sub-policies. With the proposed method, state-of-the-art accuracies are achieved on CIFAR-10, CIFAR-100, and ImageNet (without additional data). Our code is available at https://github.com/gmy2013/ARS-Aug.",0
"Manual design of data augmentation techniques has been the norm in the past, with such policies being specific to the dataset. However, a new approach called AutoAugment, which uses reinforcement learning, has been introduced to automate this process. The search for augmentation policies in the discrete search space used by AutoAugment may result in sub-optimal solutions. In this paper, we present the Augmented Random Search method (ARS) to enhance the performance of AutoAugment. Our approach involves changing the search space from discrete to continuous, which improves search performance and maintains diversity between sub-policies. Our method outperforms existing techniques on CIFAR-10, CIFAR-100, and ImageNet (without additional data). Our code is available at https://github.com/gmy2013/ARS-Aug.",1
"Evolution Strategies (ES) emerged as a scalable alternative to popular Reinforcement Learning (RL) techniques, providing an almost perfect speedup when distributed across hundreds of CPU cores thanks to a reduced communication overhead. Despite providing large improvements in wall-clock time, ES is data inefficient when compared to competing RL methods. One of the main causes of such inefficiency is the collection of large batches of experience, which are discarded after each policy update. In this work, we study how to perform more than one update per batch of experience by means of Importance Sampling while preserving the scalability of the original method. The proposed method, Importance Weighted Evolution Strategies (IW-ES), shows promising results and is a first step towards designing efficient ES algorithms.",0
"Evolution Strategies (ES) have emerged as a scalable alternative to Reinforcement Learning (RL) techniques, offering almost perfect speedup when distributed across numerous CPU cores due to minimal communication overhead. Despite significant improvements in wall-clock time, ES falls short in terms of data efficiency when compared to competing RL methods. This inefficiency stems mainly from the collection of large batches of experience, which are discarded after each policy update. In this study, we explore the possibility of performing more than one update per batch of experience through Importance Sampling, without compromising the scalability of the original technique. Our proposed method, Importance Weighted Evolution Strategies (IW-ES), displays promising results and represents a crucial first step towards developing efficient ES algorithms.",1
"I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.",0
"In this paragraph, I present an ideal perspective on adversarial machine learning that involves viewing the machine learner as a dynamic system, the input as adversarial actions, and the control costs as the adversary's objectives to cause damage and avoid detection. This approach encompasses various forms of adversarial machine learning, such as test-item assaults, training-data contamination, and adversarial reward shaping. By adopting this viewpoint, researchers in adversarial machine learning are encouraged to take advantage of developments in control theory and reinforcement learning.",1
