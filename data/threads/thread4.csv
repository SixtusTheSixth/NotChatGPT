"The construction of a meaningful graph plays a crucial role in the success of many graph-based representations and algorithms for handling structured data, especially in the emerging field of graph signal processing. However, a meaningful graph is not always readily available from the data, nor easy to define depending on the application domain. In particular, it is often desirable in graph signal processing applications that a graph is chosen such that the data admit certain regularity or smoothness on the graph. In this paper, we address the problem of learning graph Laplacians, which is equivalent to learning graph topologies, such that the input data form graph signals with smooth variations on the resulting topology. To this end, we adopt a factor analysis model for the graph signals and impose a Gaussian probabilistic prior on the latent variables that control these signals. We show that the Gaussian prior leads to an efficient representation that favors the smoothness property of the graph signals. We then propose an algorithm for learning graphs that enforces such property and is based on minimizing the variations of the signals on the learned graph. Experiments on both synthetic and real world data demonstrate that the proposed graph learning framework can efficiently infer meaningful graph topologies from signal observations under the smoothness prior.",0
"In the recent years there has been increasing interest in modeling graph signals using smooth representations such as Fourier and wavelet transforms. These representations offer advantages over classical discrete signal processing methods by capturing the underlying smooth nature of many graph signals arising from real world data. One challenge facing these smooth graph signal representations is that they rely heavily on the Laplacian matrix of the graph, which can become ill conditioned or singular in certain scenarios. This paper focuses on developing new techniques for learning the Laplacian matrix in smooth graph signal representations by incorporating additional information such as eigenvectors and eigenvalues of the graph. We propose two approaches: (i) regularized least squares optimization where we solve for the Laplacian matrix directly and impose additional constraints based on prior knowledge; (ii) iterative refinement method where we first estimate a rough approximation of the Laplacian matrix using readily available algorithms like random walks or Fiedler vector and then improve upon it incrementally until convergence. Our experimental results demonstrate the effectiveness of our proposed frameworks outperforming state-of-the-art baselines across various applications including image classification, biochemical network inference, sensor placement in buildings, etcetera. Overall, our research provides insights into designing efficient algorithms for computing smooth graph signal representations tailored towards specific application domains while addressing challenges posed due to nonlinearities inherent in graphs.",1
"Sparse representations have been successfully applied to signal processing, computer vision and machine learning. Currently there is a trend to learn sparse models directly on structure data, such as region covariance. However, such methods when combined with region covariance often require complex computation. We present an approach to transform a structured sparse model learning problem to a traditional vectorized sparse modeling problem by constructing a Euclidean space representation for region covariance matrices. Our new representation has multiple advantages. Experiments on several vision tasks demonstrate competitive performance with the state-of-the-art methods.",0
"This research proposes a new method for applying sparse codes on second order statistics by parameterizing region covariance (PRC). PRC represents a more efficient approach than traditional methods as it allows for better control over the regions where sparsity is applied. By using PRC, we can achieve improved performance in tasks such as image compression, feature extraction, and noise reduction. The proposed method has been evaluated through extensive experimentation, which demonstrates its effectiveness in terms of both efficiency and accuracy compared to existing approaches. Overall, our findings have important implications for the field of computer vision and signal processing, providing a powerful tool for applications that require high levels of precision and speed.",1
"Learning the kernel functions used in kernel methods has been a vastly explored area in machine learning. It is now widely accepted that to obtain 'good' performance, learning a kernel function is the key challenge. In this work we focus on learning kernel representations for structured regression. We propose use of polynomials expansion of kernels, referred to as Schoenberg transforms and Gegenbaur transforms, which arise from the seminal result of Schoenberg (1938). These kernels can be thought of as polynomial combination of input features in a high dimensional reproducing kernel Hilbert space (RKHS). We learn kernels over input and output for structured data, such that, dependency between kernel features is maximized. We use Hilbert-Schmidt Independence Criterion (HSIC) to measure this. We also give an efficient, matrix decomposition-based algorithm to learn these kernel transformations, and demonstrate state-of-the-art results on several real-world datasets.",0
"This will need to pass Turnitin so no plagiarism please. I am an undergraduate computer science major at Carnegie Mellon University and we don't want any trouble! Thanks! ---  Abstract: This work presents a new method for learning kernels for structured prediction problems using polynomial kernel transformations. The proposed approach leverages recent advances in deep learning and Gaussian process regression to effectively model complex patterns in data while maintaining interpretability and non-parametric flexibility. We demonstrate empirically that our learned kernels can significantly improve upon state-of-the-art baselines across several challenging application domains, including natural language processing, vision, and control tasks. Our contributions provide insights into how to design effective feature maps and regularization schemes tailored specifically for structured output prediction, which may have broader implications beyond the scope of this paper. Overall, these results suggest great promise for our novel polynomial kernel transformation framework as a powerful tool for practitioners working on real-world predictive analytics problems where structure matter most.",1
"The complexity of the visual world creates significant challenges for comprehensive visual understanding. In spite of recent successes in visual recognition, today's vision systems would still struggle to deal with visual queries that require a deeper reasoning. We propose a knowledge base (KB) framework to handle an assortment of visual queries, without the need to train new classifiers for new tasks. Building such a large-scale multimodal KB presents a major challenge of scalability. We cast a large-scale MRF into a KB representation, incorporating visual, textual and structured data, as well as their diverse relations. We introduce a scalable knowledge base construction system that is capable of building a KB with half billion variables and millions of parameters in a few hours. Our system achieves competitive results compared to purpose-built models on standard recognition and retrieval tasks, while exhibiting greater flexibility in answering richer visual queries.",0
"Abstract: In recent years, there has been significant progress on developing large-scale knowledge base systems that can effectively answer natural language queries using structured data sources such as RDF graphs and SQL databases. However, these systems have mostly focused on textual information, ignoring other modalities such as images, audio, video, etc. This presents a challenge as more than half of the world’s information is non-textual. To address this issue, we propose building a large-scale multimodal knowledge base system (MMKBS) that allows users to query information across multiple modalities including visual inputs like pictures, videos and diagrams. We present our approach to construct such a MMKB and showcase results from extensive experiments carried out using real world datasets. Our findings indicate that integrating multiple modalities improves the performance of knowledge retrieval by up to 27%. Finally, we discuss future research directions to further advance the field of large-scale MMKBS.",1
"Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be ""trained"" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.",0
"This review focuses on relational machine learning methods for knowledge graphs. We begin by introducing background material necessary to understanding these techniques: we describe both their use cases, as well as their relationship to graph theory. Next, we provide an overview of current state-of-the art approaches to learning from knowledge graphs using relational models. These approaches can be broadly categorized into two groups: those that rely primarily on matrix factorization, and those that use graph convolutional neural networks (GCNN). Each approach has strengths and weaknesses; thus, careful consideration must be given to choosing which one should be applied for a particular task. After discussing these relational modeling techniques, we consider some additional complications that arise in practice such as scalability issues, computational feasibility, and uncertainty handling. Finally, we conclude with future directions and open research questions related to applying relational machine learning on knowledge graphs.",1
"Tree-structured data usually contain both topological and geometrical information, and are necessarily considered on manifold instead of Euclidean space for appropriate data parameterization and analysis. In this study, we propose a novel tree-structured data parameterization, called Topology-Attribute matrix (T-A matrix), so the data clustering task can be conducted on matrix manifold. We incorporate the structure constraints embedded in data into the negative matrix factorization method to determine meta-trees from the T-A matrix, and the signature vector of each single tree can then be extracted by meta-tree decomposition. The meta-tree space turns out to be a cone space, in which we explore the distance metric and implement the clustering algorithm based on the concepts like Fr\'echet mean. Finally, the T-A matrix based clustering (TAMBAC) framework is evaluated and compared using both simulated data and real retinal images to illustrate its efficiency and accuracy.",0
"In addition to clustering tree-like structures themselves, we can use manifold learning to learn better embeddings of their leaf nodes into lower dimensions while preserving local structure. Here we describe techniques that allow us to generate cluster assignments for all leaf nodes simultaneously that takes both the global (tree) structure and the locally linear relations among leaves into account. We achieve this by first applying a graph partitioning algorithm based on node degree information as well as a modified Cheeger cut objective function to obtain a binary hierarchical clustering dendrogram for each leaf set obtained from our embedding. This allows us to generate any desired number of clusters and cluster assignment at different levels of granularity given some notion of distance threshold between partitions. For visualization purposes we then apply a nonlinear dimensionality reduction technique such as tSNE which maps the low-dimensional leaf node coordinates back to high-dimensional space subject to pointwise constraints ensuring that neighboring points stay neighbors after projection. An advantage over traditional spectral methods for obtaining such embeddings is that no eigendecomposition is required, leading to efficient computation times for even very large datasets. We demonstrate the effectiveness of these techniques on several benchmark data sets including gene expression profiles from cancer patients and functional neuroimaging data from Alzheimer’s Disease patients using tasks involving memory retrieval. Our results indicate improved performance in terms of Rand Index compared against other popular clustering algorithms and more accurate recovery of known biological pathways in the gene expression case. These ideas have applications beyond just clustering in the context of machine learning problems where capturing underlying structure and topology of data imprecisely represented or sampled fr",1
"The modelling of data on a spherical surface requires the consideration of directional probability distributions. To model asymmetrically distributed data on a three-dimensional sphere, Kent distributions are often used. The moment estimates of the parameters are typically used in modelling tasks involving Kent distributions. However, these lack a rigorous statistical treatment. The focus of the paper is to introduce a Bayesian estimation of the parameters of the Kent distribution which has not been carried out in the literature, partly because of its complex mathematical form. We employ the Bayesian information-theoretic paradigm of Minimum Message Length (MML) to bridge this gap and derive reliable estimators. The inferred parameters are subsequently used in mixture modelling of Kent distributions. The problem of inferring the suitable number of mixture components is also addressed using the MML criterion. We demonstrate the superior performance of the derived MML-based parameter estimates against the traditional estimators. We apply the MML principle to infer mixtures of Kent distributions to model empirical data corresponding to protein conformations. We demonstrate the effectiveness of Kent models to act as improved descriptors of protein structural data as compared to commonly used von Mises-Fisher distributions.",0
"Abstract: In many applied fields, such as medicine, engineering and economics, there exists an abundance of data that can only take on positive values and follow certain patterns. One common pattern found in these types of datasets is non-increasing density functions, i.e., densities decrease moving from left to right along the support interval. Nonetheless, little attention has been paid to specifying probabilistic models to adequately represent these phenomena. Motivated by this observation, we propose the use of Kent distributions, named after their first appearance in the field of astrophysics, to model positively supported continuous directional data. We discuss properties of both unidimensional and multivariate Kent distributions, including parameter estimation and tests of fit. As expected for any distribution arising from theory, we prove properties like existence of moments and stochastic ordering. Finally, through simulation studies and a real example analysing growth charts, we show good statistical performance in terms of likelihood maximisation and Bayesian posterior distributions. These results suggest suitability of our proposal when modelling positive support data with decreasing density function features.",1
"Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities.   In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.",0
"Graph neural networks (GNNs) have been successful at modeling graph data across many domains such as social network analysis, computer vision, natural language processing, among others. However, designing efficient GNN architectures remains challenging due to limited parallelism, irregular data structures, and high computational complexity. Inspired by recent advancements in deep learning on image and video data using convolutional neural networks (CNN), we explore whether these principles can be applied to graphs. We present a new framework that generalizes classical CNN operations onto graphs via a novel edge convolution followed by layer normalization. Our proposed method outperforms state-of-the-art methods significantly while reducing parameter counts and inference time requirements. By exploiting spatial locality underlying large graphs and sharing weights across all edges, our approach achieves greater efficiency compared to existing techniques. Finally, we demonstrate the broad applicability of our architecture through extensive experiments across multiple tasks and datasets including citation networks, coauthorship graphs, and bioinformatics problems. With its superior performance coupled with reduced computation cost, our method has exciting potential for scaling up graph machine learning to larger problems than previously possible.",1
"Many real-world applications are associated with structured data, where not only input but also output has interplay. However, typical classification and regression models often lack the ability of simultaneously exploring high-order interaction within input and that within output. In this paper, we present a deep learning model aiming to generate a powerful nonlinear functional mapping from structured input to structured output. More specifically, we propose to integrate high-order hidden units, guided discriminative pretraining, and high-order auto-encoders for this purpose. We evaluate the model with three datasets, and obtain state-of-the-art performances among competitive methods. Our current work focuses on structured output regression, which is a less explored area, although the model can be extended to handle structured label classification.",0
"A Deep Learning Model (DLM) can generate text descriptions that incorporate high-dimensional dependencies while capturing higher level features such as syntax and semantics. DLMs require large datasets and computational resources, thus have difficulty generalizing on small datasets or handling structured outputs. In contrast, traditional statistical models like Hidden Markov Models provide explicit probabilistic inference and structure which allow them to handle smaller data sets more effectively. Our paper presents an approach combining deep learning methods and graphical modeling techniques to improve the accuracy and efficiency of generating structured outputs while addressing issues related to resource demands and limited data availability. We evaluate our proposed method using two benchmark datasets; one from natural language processing and another from computer vision tasks. Experimental results show significant improvements over baseline models achieving state-of-the-art performance. This work represents an important step towards building powerful machine learning algorithms capable of producing accurate predictions and descriptions for complex problems where little training data is available.",1
"In many applications data is naturally presented in terms of orderings of some basic elements or symbols. Reasoning about such data requires a notion of similarity capable of handling sequences of different lengths. In this paper we describe a family of Mercer kernel functions for such sequentially structured data. The family is characterized by a decomposable structure in terms of symbol-level and structure-level similarities, representing a specific combination of kernels which allows for efficient computation. We provide an experimental evaluation on sequential classification tasks comparing kernels from our family of kernels to a state of the art sequence kernel called the Global Alignment kernel which has been shown to outperform Dynamic Time Warping",0
"This paper presents a new family of decomposable kernels designed specifically for analyzing sequence data. By leveraging recent advances in deep learning techniques, we propose a novel framework that allows for efficient kernel computation while maintaining high accuracy. Our approach utilizes predefined distance functions and introduces a parameterization scheme to ensure that our resulting kernels can capture complex nonlinear dependencies present in real-world sequences. We provide comprehensive evaluations on several benchmark datasets, demonstrating superior performance compared to existing methods across multiple applications such as classification, regression, and clustering tasks. In summary, our contributions represent significant progress in the field and offer valuable insights into more effective modeling and analysis of sequential data using machine learning approaches.",1
"We evaluate a version of the recently-proposed classification system named Optimized Dissimilarity Space Embedding (ODSE) that operates in the input space of sequences of generic objects. The ODSE system has been originally presented as a classification system for patterns represented as labeled graphs. However, since ODSE is founded on the dissimilarity space representation of the input data, the classifier can be easily adapted to any input domain where it is possible to define a meaningful dissimilarity measure. Here we demonstrate the effectiveness of the ODSE classifier for sequences by considering an application dealing with the recognition of the solubility degree of the Escherichia coli proteome. Solubility, or analogously aggregation propensity, is an important property of protein molecules, which is intimately related to the mechanisms underlying the chemico-physical process of folding. Each protein of our dataset is initially associated with a solubility degree and it is represented as a sequence of symbols, denoting the 20 amino acid residues. The herein obtained computational results, which we stress that have been achieved with no context-dependent tuning of the ODSE system, confirm the validity and generality of the ODSE-based approach for structured data classification.",0
"Solubility plays an important role in understanding protein structure function relationships. High-throughput methods capable of measuring relative protein abundance at different temperatures have been used in several recent studies on protein structure determination using temperature gradients. In this work we present an algorithmic solution to classify protein solubility based on sequence data alone. We use evolutionary rate as a proxy measure of protein complexity which allows us to optimize our method through comparison against well studied systems where the physical property under consideration has already been experimentally characterized. Specifically, we study protein complexes from two organisms with low (Escherichia coli) and high (Haemophilus influenzae) genetic mutation rates to determine how differences in amino acid composition affects protein abundance over time and across environments. Our results indicate that evolutionary rate could serve as a better predictor than amino acid conservation across environmental conditions or even more specific physicochemical properties such as charge and hydrophobicity. Ultimately these findings suggest that optimization in evolution can reveal underlying design principles that cannot otherwise be discovered simply by analyzing static snapshots of protein families.",1
"Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.",0
"This paper presents the first mapping between variational renormalisation group theory (VRG) and deep learning that involves only differentiable models trained using backpropagation. In the past two decades, VRG has been shown to perform better than other methods such as traditional renormalization group approaches at capturing complex physical phenomena while remaining computationally feasible. However, it was not clear until now how VRG could benefit from modern machine learning techniques. We show that any model trained on real world data can be made into a variational renormalisation model by rearranging terms during inference based on the loss function. This result may have profound implications in understanding physical properties of materials, which typically require large scale simulations. Our approach opens up new directions in hybridising these fields. As we demonstrate in our experiments, models trained using this method outperform their counterparts without VRG modifications both quantitatively and qualitatively. While previous attempts at bridging renormalisation with ML were predominantly non-differentiable, in our case, gradients flowing through both the forward pass (model training process) and inverse pass (data generation process via VRG) make it possible to learn more expressive models while still maintaining computational efficiency. Finally, our work provides a unique testbed where existing theories in statistical physics can be applied directly to study neural networks in ways previously unavailable. With applications ranging from condensed matter to quantum field theories, our results suggest a bright future for research involving these areas intersecting. We present evidence suggesting that current developments in these fields might even provide novel solutions to problems encountered in ML. Thus we envision that this wi",1
"We introduce propagation kernels, a general graph-kernel framework for efficiently measuring the similarity of structured data. Propagation kernels are based on monitoring how information spreads through a set of given graphs. They leverage early-stage distributions from propagation schemes such as random walks to capture structural information encoded in node labels, attributes, and edge information. This has two benefits. First, off-the-shelf propagation schemes can be used to naturally construct kernels for many graph types, including labeled, partially labeled, unlabeled, directed, and attributed graphs. Second, by leveraging existing efficient and informative propagation schemes, propagation kernels can be considerably faster than state-of-the-art approaches without sacrificing predictive performance. We will also show that if the graphs at hand have a regular structure, for instance when modeling image or video data, one can exploit this regularity to scale the kernel computation to large databases of graphs with thousands of nodes. We support our contributions by exhaustive experiments on a number of real-world graphs from a variety of application domains.",0
"Abstract: This paper presents a new approach to modeling how information spreads through populations, known as propagation kernels. Drawing on techniques from graph theory and information science, we develop a general framework for representing the flow of information across networks of any kind. Our method can capture both the structure of these networks and the dynamics of how they change over time. We demonstrate the utility of our approach by applying it to several real-world examples, including social media diffusion, disease outbreaks, and viral marketing campaigns. By providing a unified perspective on information spreading phenomena, our work paves the way for more accurate predictions of how information flows throughout society. Additionally, since our framework is modular and easy to use, it has potential applications in many different fields where understanding information transmission is important. Overall, this research offers valuable insights into the complex processes that shape public opinion, consumer behavior, and other critical aspects of modern life.",1
"Unsupervised ranking faces one critical challenge in evaluation applications, that is, no ground truth is available. When PageRank and its variants show a good solution in related subjects, they are applicable only for ranking from link-structure data. In this work, we focus on unsupervised ranking from multi-attribute data which is also common in evaluation tasks. To overcome the challenge, we propose five essential meta-rules for the design and assessment of unsupervised ranking approaches: scale and translation invariance, strict monotonicity, linear/nonlinear capacities, smoothness, and explicitness of parameter size. These meta-rules are regarded as high level knowledge for unsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a ranking principal curve (RPC) model, which learns a one-dimensional manifold function to perform unsupervised ranking tasks on multi-attribute observations. Furthermore, the RPC is modeled to be a cubic B\'ezier curve with control points restricted in the interior of a hypercube, thereby complying with all the five meta-rules to infer a reasonable ranking list. With control points as the model parameters, one is able to understand the learned manifold and to interpret the ranking list semantically. Numerical experiments of the presented RPC model are conducted on two open datasets of different ranking applications. In comparison with the state-of-the-art approaches, the new model is able to show more reasonable ranking lists.",0
"This study presents an unsupervised method for ranking multi-attribute objects based on principal curves. We assume that there exists some underlying relationship among attributes, which can be modeled as monotonic functions of one variable called ""score"". The main idea consists of mapping object vectors onto their corresponding scores through these functions by finding local minima of a loss function designed to preserve global rankings while optimizing local error at each point using gradient descent. Evaluation shows competitive performance against state-of-the-art models trained with labeled data.",1
"The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.",0
"This should be a research paper. A survey was conducted on metric learning for feature vectors and structured data by collecting datasets from different domains such as computer vision, natural language processing, recommender systems, bioinformatics etc. The dataset includes images, texts, graphs, trees and more that have been transformed into feature vectors through manual feature engineering or deep learning models like convolutional neural networks (CNNs), recurrent neural network (RNN) or transformer based architectures. Experiments were performed using several state-of-the-art algorithms under various settings (e.g., semi-supervised learning vs supervised learning, batch mode vs online learning). In addition, we evaluated their performance on different distance measures and regularization terms used in loss functions, demonstrating how these choices can affect the final results of metric learning problems. To facilitate further study on metric learning, our code, datasets and visualizations are publicly available at GitHub and demo website. Lastly, we provide future directions on how metric learning can benefit even more fields if integrated with deep learning techniques and structured prediction models. Overall, this work aims to bridge the gap between recent advances in metric learning theory and real world applications so practitioners and researchers alike can find solutions efficiently and effectively.",1
"Dealing with structured data needs the use of expressive representation formalisms that, however, puts the problem to deal with the computational complexity of the machine learning process. Furthermore, real world domains require tools able to manage their typical uncertainty. Many statistical relational learning approaches try to deal with these problems by combining the construction of relevant relational features with a probabilistic tool. When the combination is static (static propositionalization), the constructed features are considered as boolean features and used offline as input to a statistical learner; while, when the combination is dynamic (dynamic propositionalization), the feature construction and probabilistic tool are combined into a single process. In this paper we propose a selective propositionalization method that search the optimal set of relational features to be used by a probabilistic learner in order to minimize a loss function. The new propositionalization approach has been combined with the random subspace ensemble method. Experiments on real-world datasets shows the validity of the proposed method.",0
"In recent years there has been increased interest in using machine learning algorithms to model complex relational structures from large datasets. One approach that has gained popularity is ensemble relational learning (ERL), which involves training multiple models and combining their predictions through ensembling techniques such as majority voting. However, a common problem faced by ERL methods is overfitting due to high complexity of modern neural network architectures, as well as the difficulty of tuning hyperparameters effectively across many models. To address these issues we propose selective propositionalization: a novel technique designed to increase interpretability and transparency without sacrificing performance. By selecting relevant features at different levels of abstraction from raw data, selective propositionalization enables effective feature engineering tailored specifically for each ERM component. We experimentally evaluate our method compared to several state-of-the-art baselines across four benchmark datasets and demonstrate consistent improvement in both accuracy and diversity metrics. Our findings have important implications for designing more efficient and robust frameworks for ensemble relational learning, advancing the broader goal of developing intelligent systems capable of handling human-like understanding tasks under uncertainty and ambiguity.",1
"In structured output learning, obtaining labelled data for real-world applications is usually costly, while unlabelled examples are available in abundance. Semi-supervised structured classification has been developed to handle large amounts of unlabelled structured data. In this work, we consider semi-supervised structural SVMs with domain constraints. The optimization problem, which in general is not convex, contains the loss terms associated with the labelled and unlabelled examples along with the domain constraints. We propose a simple optimization approach, which alternates between solving a supervised learning problem and a constraint matching problem. Solving the constraint matching problem is difficult for structured prediction, and we propose an efficient and effective hill-climbing method to solve it. The alternating optimization is carried out within a deterministic annealing framework, which helps in effective constraint matching, and avoiding local minima which are not very useful. The algorithm is simple to implement and achieves comparable generalization performance on benchmark datasets.",0
"Title: Improving Semi-Supervised Learning for Structured Outputs through Large Margin Training  Semi-supervised learning (SSL) has emerged as a popular approach for tackling problems where labeled data is scarce. One key challenge with SSL methods lies in handling structured outputs, which require complex models that capture dependencies across multiple elements. Traditional approaches often struggle to learn accurate representations using limited supervision alone. In this work, we propose a novel semi-supervised method called Large Margin Semi-supervised Structured Output Learning (LSML) to improve SSL performance on structured output tasks. Our technique combines two complementary ideas: large margin learning for improved model robustness and structured regularization that encourages better parameter sharing among similar model components. We demonstrate LSML's effectiveness by applying it to several real-world benchmark datasets commonly used in natural language processing and computer vision applications. Experimental results show that our approach consistently outperforms state-of-the-art baselines, providing more reliable and generalizable predictions while utilizing only small amounts of annotated training data. This research advances our understanding of how to design SSL algorithms capable of handling high-dimensional, structured prediction tasks under severe resource constraints.",1
"In this paper, we present two localized graph filtering based methods for interpolating graph signals defined on the vertices of arbitrary graphs from only a partial set of samples. The first method is an extension of previous work on reconstructing bandlimited graph signals from partially observed samples. The iterative graph filtering approach very closely approximates the solution proposed in the that work, while being computationally more efficient. As an alternative, we propose a regularization based framework in which we define the cost of reconstruction to be a combination of smoothness of the graph signal and the reconstruction error with respect to the known samples, and find solutions that minimize this cost. We provide both a closed form solution and a computationally efficient iterative solution of the optimization problem. The experimental results on the recommendation system datasets demonstrate effectiveness of the proposed methods.",0
"This should read like a scientific abstract of a research paper describing its contents without going into details on methods used etc. You may make up some authors names if necessary to give this more realism but no institutions please. Please write this as a third person summary of the results - so use active voice wherever possible: e.g. we found XYZ rather than ""the study showed"". Localized iterative methods for interpolation in graph structured data have been developed by scientists from prestigious universities across Europe. These new techniques allow for high precision interpolations within large datasets which contain intricate relationships. As such, these novel approaches will be particularly beneficial to fields where complex systems frequently require analysis such as finance, engineering and physics. In particular our research has demonstrated that using localized iterative methods leads to significantly improved accuracy over existing approaches. Overall these findings have significant implications in both academia and industry due to the substantially enhanced potential performance in computational processing. -----",1
"Tensor factorizations have become increasingly popular approaches for various learning tasks on structured data. In this work, we extend the RESCAL tensor factorization, which has shown state-of-the-art results for multi-relational learning, to account for the binary nature of adjacency tensors. We study the improvements that can be gained via this approach on various benchmark datasets and show that the logistic extension can improve the prediction results significantly.",0
"One challenge in data mining is discovering patterns among collections of entities that come from different types (e.g., transactions and customers). Classical relational learning techniques are based on the assumption that there exist some hidden essential relations which can provide exact matches; however, many real-world relationships have only loose associations. In recent years, latent factor models such as Latent Dirichlet Allocation (LDA) have proven successful in capturing subtle nuances underlying unstructured text collection. This research adapts these ideas to multi-relational datasets by creating latent factors which capture high-level semantics but do not necessarily coincide with any individual attribute, entity type, or relationship type. Given several input tables representing related domains, our model simultaneously learns latent factors which inform new links between instances across all tables while minimizing reconstruction errors for each individual table independently. Our evaluation shows significant improvements over current state-of-the-art methods using both synthetic experiments and multiple standard benchmark sets drawn from diverse fields.",1
"Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.",0
"This is the abstract I would like you to write:  This paper proposes a new framework called CINNAMON (Continuous Integration of Neural Networks via Manifold Oriented Model Optimization) that learns neural networks by directly minimizing a continuous relaxation of a discrete objective function defined over the space of neural network weights that maps each model into either -1 if the prediction quality is below some threshold value on held out data or else +1 if above such threshold. Since we can take any such non-convex optimization problem and obtain lower bounds at every iteration through sampling methods such as Monte Carlo gradient estimation, our approach allows us to perform semi-continuous learning without the need to retrain from scratch and with only simple backpropagation-like passes required for updating model parameters using SGD or other standard gradient-based optimization algorithms. Empirical evaluation shows that compared to baselines utilized state-of-the art models trained end-to-end, training times were reduced while maintaining comparable test performance results which demonstrates the promise of our methodology. Our proposed technique provides a path towards realizing large scale deployment of machine learning systems under tight resource constraints where the ability to incrementally improve system quality based on feedback data that becomes available during operation, rather than relying solely on pre-defined static training sets. Additionally, because our formulation makes explicit use of auxiliary ""manifold"" features extracted from the input data that capture higher order relations between elements within the dataset but are otherwise unused, our work may also serve as foundation for further study of how better to integrate high level structured knowledge/information sources with deep generative mod",1
"This paper investigates graph clustering in the planted cluster model in the presence of {\em small clusters}. Traditional results dictate that for an algorithm to provably correctly recover the clusters, {\em all} clusters must be sufficiently large (in particular, $\tilde{\Omega}(\sqrt{n})$ where $n$ is the number of nodes of the graph). We show that this is not really a restriction: by a more refined analysis of the trace-norm based recovery approach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones.   Based on this result, we further devise an iterative algorithm to recover {\em almost all clusters} via a ""peeling strategy"", i.e., recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the {\em partial observation} setting, in which only a (chosen) part of the graph is observed.The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often as large clusters are learned (and removed).   From a high level, this paper sheds novel insights on high-dimensional statistics and learning structured data, by presenting a structured matrix learning problem for which a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxationsdoes the job.",0
"Graph clustering has been a popular technique for identifying clusters in large data sets, particularly as networks have grown increasingly complex in recent years. However, many graph clustering algorithms suffer from two limitations: their running time increases exponentially as the number of nodes increases, making them impractical for very large graphs; and they only find clusters up to a certain size limit known as the small cluster barrier. This paper presents an algorithm that overcomes these limitations by first partitioning the graph into smaller subgraphs and then applying standard clustering techniques on each subgraph individually before merging the results. Our experimental evaluation shows that our approach can effectively identify clusters of any size while significantly reducing computation time compared to state-of-the-art methods.",1
"Traditional relation extraction predicts relations within some fixed and finite target schema. Machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. The need for existing datasets can be avoided by using a universal schema: the union of all involved schemas (surface form predicates as in OpenIE, and relations in the schemas of pre-existing databases). This schema has an almost unlimited set of relations (due to surface forms), and supports integration with existing structured data (through the relation types of existing databases). To populate a database of such schema we present a family of matrix factorization models that predict affinity between database tuples and relations. We show that this achieves substantially higher accuracy than the traditional classification approach. More importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms state-of-the-art distant supervision systems.",0
"Title: ""Universal Schemas through Latent Relation Representations""  Abstract: In recent years, there has been significant progress in developing models that can learn and represent complex relationships within large amounts of text data. However, many of these models focus on specific domains or tasks, and may struggle to generalize to new types of content. In order to address this challenge, we propose a novel approach that uses latent relation representations (LRRs) to automatically induce universal schemas that capture fundamental relationships across diverse domains. Our method leverages unsupervised learning techniques to identify patterns in natural language and generate LRRs that effectively capture high-level semantic relations between entities. We evaluate our model on several benchmark datasets and show that it consistently outperforms state-of-the-art methods in terms of both schema induction and zero-shot cross-domain transferability. Overall, our work represents a promising step towards developing more flexible and adaptive text processing systems that can tackle increasingly diverse data sources and applications.",1
"In practical machine learning systems, graph based data representation has been widely used in various learning paradigms, ranging from unsupervised clustering to supervised classification. Besides those applications with natural graph or network structure data, such as social network analysis and relational learning, many other applications often involve a critical step in converting data vectors to an adjacency graph. In particular, a sparse subgraph extracted from the original graph is often required due to both theoretic and practical needs. Previous study clearly shows that the performance of different learning algorithms, e.g., clustering and classification, benefits from such sparse subgraphs with balanced node connectivity. However, the existing graph construction methods are either computationally expensive or with unsatisfactory performance. In this paper, we utilize a scalable method called auction algorithm and its parallel extension to recover a sparse yet nearly balanced subgraph with significantly reduced computational cost. Empirical study and comparison with the state-ofart approaches clearly demonstrate the superiority of the proposed method in both efficiency and accuracy.",0
"In recent years, graph construction has become increasingly important as graphs have grown larger and more complex. However, traditional methods such as breadth first search (BFS) and depth first search (DFS) can become time consuming and impractical. To address this challenge, we propose a new algorithm called the auction algorithm which outperforms existing methods by providing faster results while maintaining accuracy.  The auction algorithm works by simulating a market where each vertex acts as a buyer, and edges represent items that they purchase from other vertices. Each vertex starts with a fixed budget, and at every step, it bids on multiple items until it runs out of money. We prove that after a finite number of steps, there exists a set of vertex subsets whose union contains all original vertices, making the auction algorithm ideal for constructing large, connected graphs quickly. Our experiments demonstrate the effectiveness and efficiency of our approach compared to BFS and DFS on both real-world datasets and synthetic benchmarks. Overall, the auction algorithm provides a significant improvement over previous techniques for fast and accurate graph construction.",1
"Collective classification models attempt to improve classification performance by taking into account the class labels of related instances. However, they tend not to learn patterns of interactions between classes and/or make the assumption that instances of the same class link to each other (assortativity assumption). Blockmodels provide a solution to these issues, being capable of modelling assortative and disassortative interactions, and learning the pattern of interactions in the form of a summary network. The Supervised Blockmodel provides good classification performance using link structure alone, whilst simultaneously providing an interpretable summary of network interactions to allow a better understanding of the data. This work explores three variants of supervised blockmodels of varying complexity and tests them on four structurally different real world networks.",0
"Blockmodeling (Hendrickx & Cools, 2014) is a powerful method used in psychology to analyze complex social networks by breaking them down into smaller blocks based on certain criteria. However, one limitation of traditional blockmodeling methods is that they often rely on unsupervised learning techniques which can sometimes produce results that may not align well with theoretical expectations. To address these limitations, we propose supervised blockmodeling as a new approach that allows researchers to incorporate prior knowledge and hypotheses directly into the model building process. We demonstrate the effectiveness of our proposed method through simulation studies and illustrate its application using two empirical datasets. Our findings suggest that supervised blockmodeling provides more accurate estimates of network structures and improves the ability to test specific theories in social network analysis. Overall, this work contributes to the development of novel tools for analyzing complex social systems and has implications for understanding how relationships among individuals shape group behavior.",1
"High dimensional structured data such as text and images is often poorly understood and misrepresented in statistical modeling. The standard histogram representation suffers from high variance and performs poorly in general. We explore novel connections between statistical translation, heat kernels on manifolds and graphs, and expected distances. These connections provide a new framework for unsupervised metric learning for text documents. Experiments indicate that the resulting distances are generally superior to their more standard counterparts.",0
"In recent years, statistical translation models have gained popularity due to their ability to accurately translate text from one language to another without relying on explicit grammar rules or syntax. These models use large amounts of data to learn the relationships between different languages, allowing them to effectively capture the nuances of natural language. This work presents a new approach to statistical translation that utilizes heat kernels and expected distances. By analyzing the similarities and differences between pairs of sentences in each language, we can better understand how they relate to one another and develop more accurate translations. We demonstrate the effectiveness of our method by comparing its performance against several state-of-the-art baselines across multiple datasets. Our results show significant improvement over existing methods, highlighting the potential benefits of using heat kernels and expected distances in statistical translation tasks. Overall, our work contributes to the growing body of research on machine translation and has implications for real-world applications such as communication across language barriers and accessibility for non-native speakers.",1
"High-dimensional data common in genomics, proteomics, and chemometrics often contains complicated correlation structures. Recently, partial least squares (PLS) and Sparse PLS methods have gained attention in these areas as dimension reduction techniques in the context of supervised data analysis. We introduce a framework for Regularized PLS by solving a relaxation of the SIMPLS optimization problem with penalties on the PLS loadings vectors. Our approach enjoys many advantages including flexibility, general penalties, easy interpretation of results, and fast computation in high-dimensional settings. We also outline extensions of our methods leading to novel methods for Non-negative PLS and Generalized PLS, an adaption of PLS for structured data. We demonstrate the utility of our methods through simulations and a case study on proton Nuclear Magnetic Resonance (NMR) spectroscopy data.",0
"Machine learning algorithms are increasingly used to process complex data sets such as those produced by nuclear magnetic resonance (NMR) spectroscopy. One popular method that has recently gained attention is partial least squares regression (PLS). PLS uses latent variables to predict target values based on two matrices, X and Y. An alternative approach called regularized PLS or rPLS further improves performance by minimizing the complexity of these latent structures. This study explores the use of rPLS applied to simulated and real NMR spectral datasets. Results show significant improvements over traditional methods using cross-validation techniques to evaluate model accuracy. These findings demonstrate the potential benefits of incorporating machine learning approaches into biochemical research. Overall, this work contributes valuable insights into the application of statistical analysis and data mining methods to improve our understanding of biological systems at the molecular level.",1
"High-dimensional tensors or multi-way data are becoming prevalent in areas such as biomedical imaging, chemometrics, networking and bibliometrics. Traditional approaches to finding lower dimensional representations of tensor data include flattening the data and applying matrix factorizations such as principal components analysis (PCA) or employing tensor decompositions such as the CANDECOMP / PARAFAC (CP) and Tucker decompositions. The former can lose important structure in the data, while the latter Higher-Order PCA (HOPCA) methods can be problematic in high-dimensions with many irrelevant features. We introduce frameworks for sparse tensor factorizations or Sparse HOPCA based on heuristic algorithmic approaches and by solving penalized optimization problems related to the CP decomposition. Extensions of these approaches lead to methods for general regularized tensor factorizations, multi-way Functional HOPCA and generalizations of HOPCA for structured data. We illustrate the utility of our methods for dimension reduction, feature selection, and signal recovery on simulated data and multi-dimensional microarrays and functional MRIs.",0
"In recent years, there has been significant interest in developing machine learning models that can effectively capture the underlying structure of complex data sets. One approach to achieving this goal is through tensor factorization methods, which aim to decompose a high-dimensional tensor into simpler constituent parts. However, many existing tensor factorization algorithms suffer from limitations such as computational intractability, difficulties in handling missing values or noise, and sensitivity to initialization. In this work, we introduce regularized tensor factorizations (RTFs) and higher-order principal components analysis (HOPCA), two novel approaches that address these challenges. RTFs incorporate Lasso regularization to promote sparse representations and improve interpretability, while HOPCA extends traditional PCA to tensors by maximizing the cumulative variance explained across multiple modes. Our experimental evaluations on both synthetic and real-world datasets demonstrate the effectiveness of our proposed methods in recovering interpretable structures, outperforming several state-of-the-art alternatives. These contributions further expand the frontier of big data modeling, unlocking exciting new possibilities in scientific discovery, business intelligence, and other applications.",1
"Recent results in Compressive Sensing have shown that, under certain conditions, the solution to an underdetermined system of linear equations with sparsity-based regularization can be accurately recovered by solving convex relaxations of the original problem. In this work, we present a novel primal-dual analysis on a class of sparsity minimization problems. We show that the Lagrangian bidual (i.e., the Lagrangian dual of the Lagrangian dual) of the sparsity minimization problems can be used to derive interesting convex relaxations: the bidual of the $\ell_0$-minimization problem is the $\ell_1$-minimization problem; and the bidual of the $\ell_{0,1}$-minimization problem for enforcing group sparsity on structured data is the $\ell_{1,\infty}$-minimization problem. The analysis provides a means to compute per-instance non-trivial lower bounds on the (group) sparsity of the desired solutions. In a real-world application, the bidual relaxation improves the performance of a sparsity-based classification framework applied to robust face recognition.",0
"In this paper we consider two fundamental problems: finding the sparsest solution of a linear system of equations Ax=b where A is of size n×m, x is an n-dimensional vector, b is an m-dimensional vector and solving the dual problem which minimizes ||x||_2^2 subject to constaints given by Ax=b This duality has been studied from several perspectives including optimization theory, statistics and machine learning where these constraints arise in applications such as denoising, regression and support vector machines. We present a new characterization that shows how these two problems can be viewed through their respective biduals defined by Lagrange multipliers (λ,v). Our main results establish relationships between (A,b) and v, between uand the primal optimal solutions and between λ and v via optimality conditions based on subdifferential calculus. These insights lead to the construction of novel algorithms to obtain all primal and dual optimal solutions and Lagrange multipliers of these problems without any constraint qualifications. Numerical experiments show promising results in terms of accuracy and speed compared to existing solvers used today.",1
"We describe many vantage points on the Baire metric and its use in clustering data, or its use in preprocessing and structuring data in order to support search and retrieval operations. In some cases, we proceed directly to clusters and do not directly determine the distances. We show how a hierarchical clustering can be read directly from one pass through the data. We offer insights also on practical implications of precision of data measurement. As a mechanism for treating multidimensional data, including very high dimensional data, we use random projections.",0
"In our paper we present a novel approach to hierarchical clustering that combines fast linear time complexity with flexibility and robustness. Our method is based on the use of a new distance measure called the Baire metric which is defined on any complete separable metric space X. We show how to extend this metric to define a family of metrics parameterized by some integer constant ""m"". This allows us to interpolate between different levels of granularity in the resulting hierarchy. The algorithm itself proceeds iteratively at each step merging two clusters according to their nearest neighbors identified via this Baire metric. We demonstrate through extensive experiments on both synthetic data sets as well as large scale real world problems such as image retrieval and sensor network localization, that our method can achieve state-of-the-art results while offering significant advantages over other methods. Furthermore we establish connections between these ideas and several other areas within mathematical computer science including generalized ultrametrics hashing formal concept analysis and precision of data measurement thus illustrating the breadth of applicability of these concepts.",1
"Recently there is a line of research work proposing to employ Spectral Clustering (SC) to segment (group){Throughout the paper, we use segmentation, clustering, and grouping, and their verb forms, interchangeably.} high-dimensional structural data such as those (approximately) lying on subspaces {We follow {liu2010robust} and use the term ""subspace"" to denote both linear subspaces and affine subspaces. There is a trivial conversion between linear subspaces and affine subspaces as mentioned therein.} or low-dimensional manifolds. By learning the affinity matrix in the form of sparse reconstruction, techniques proposed in this vein often considerably boost the performance in subspace settings where traditional SC can fail. Despite the success, there are fundamental problems that have been left unsolved: the spectrum property of the learned affinity matrix cannot be gauged in advance, and there is often one ugly symmetrization step that post-processes the affinity for SC input. Hence we advocate to enforce the symmetric positive semidefinite constraint explicitly during learning (Low-Rank Representation with Positive SemiDefinite constraint, or LRR-PSD), and show that factually it can be solved in an exquisite scheme efficiently instead of general-purpose SDP solvers that usually scale up poorly. We provide rigorous mathematical derivations to show that, in its canonical form, LRR-PSD is equivalent to the recently proposed Low-Rank Representation (LRR) scheme {liu2010robust}, and hence offer theoretic and practical insights to both LRR-PSD and LRR, inviting future research. As per the computational cost, our proposal is at most comparable to that of LRR, if not less. We validate our theoretic analysis and optimization scheme by experiments on both synthetic and real data sets.",0
"A low rank subspace segmentation approach robust to outliers has been developed, ensuring semidefinite guarantees. The proposed method utilizes a nuclear norm penalty on the singular values of the matrix factors obtained from a low rank approximation of the data matrix. By combining this with a recovery guarantee based on restricted principal component analysis, we provide theoretical support demonstrating that our algorithm can accurately separate any underlying segments corrupted by arbitrary noise of high magnitude. We demonstrate via experiments on both synthetic and real datasets the improved performance our method provides over state-of-the art alternatives.",1
"The major challenge in designing a discriminative learning algorithm for predicting structured data is to address the computational issues arising from the exponential size of the output space. Existing algorithms make different assumptions to ensure efficient, polynomial time estimation of model parameters. For several combinatorial structures, including cycles, partially ordered sets, permutations and other graph classes, these assumptions do not hold. In this thesis, we address the problem of designing learning algorithms for predicting combinatorial structures by introducing two new assumptions: (i) The first assumption is that a particular counting problem can be solved efficiently. The consequence is a generalisation of the classical ridge regression for structured prediction. (ii) The second assumption is that a particular sampling problem can be solved efficiently. The consequence is a new technique for designing and analysing probabilistic structured prediction models. These results can be applied to solve several complex learning problems including but not limited to multi-label classification, multi-category hierarchical classification, and label ranking.",0
"This abstract presents our work on predicting combinatorial structures from input text data using deep learning techniques. We first describe the challenge involved in modeling these types of structured outputs and explain why current methods fall short. Next, we outline our approach which uses a novel attention mechanism that allows us to focus on relevant parts of the input while generating the output structure. Finally, we discuss preliminary results obtained on several benchmark datasets showing substantial improvements over state-of-the-art systems. These encouraging results suggest that our method can form a valuable component of future natural language processing pipelines involving structured predictions. (word count: 298) ---",1
"Deep learning on graphs has recently achieved remarkable success on a variety of tasks, while such success relies heavily on the massive and carefully labeled data. However, precise annotations are generally very expensive and time-consuming. To address this problem, self-supervised learning (SSL) is emerging as a new paradigm for extracting informative knowledge through well-designed pretext tasks without relying on manual labels. In this survey, we extend the concept of SSL, which first emerged in the fields of computer vision and natural language processing, to present a timely and comprehensive review of existing SSL techniques for graph data. Specifically, we divide existing graph SSL methods into three categories: contrastive, generative, and predictive. More importantly, unlike other surveys that only provide a high-level description of published research, we present an additional mathematical summary of existing works in a unified framework. Furthermore, to facilitate methodological development and empirical comparisons, we also summarize the commonly used datasets, evaluation metrics, downstream tasks, open-source implementations, and experimental study of various algorithms. Finally, we discuss the technical challenges and potential future directions for improving graph self-supervised learning. Latest advances in graph SSL are summarized in a GitHub repository https://github.com/LirongWu/awesome-graph-self-supervised-learning.",0
"This paper presents three different techniques using self-supervised learning on graphs and compares their effectiveness. We first describe the basic concepts behind each method (contrastive, generative, predictive) and then provide examples and comparisons against one another. Each approach has unique strengths and weaknesses depending on the nature of the data. Finally, we conclude by discussing future directions and potential applications.",1
"We focus on the problem of segmenting a certain object referred by a natural language sentence in video content, at the core of formulating a pinpoint vision-language relation. While existing attempts mainly construct such relation in an implicit way, i.e., grid-level multi-modal feature fusion, it has been proven problematic to distinguish semantically similar objects under this paradigm. In this work, we propose to interwind the visual and linguistic modalities in an explicit way via the contrastive learning objective, which directly aligns the referred object and the language description and separates the unreferred content apart across frames. Moreover, to remedy for the degradation problem, we present two complementary hard instance mining strategies, i.e., Language-relevant Channel Filter and Relative Hard Instance Construction. They encourage the network to exclude visual-distinguishable feature and to focus on easy-confused objects during the contrastive training. Extensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB Sentences, quantitatively demonstrate the state-of-the-arts performance of our method and qualitatively show the more accurate distinguishment between semantically similar objects over baselines.",0
"Our new method automatically separates video into clips based on natural language input. While state-of-the-art methods require complex training data and supervision for fine-grained segmentation of videos by hand, ours can work unsupervised from raw audio tracks. We first encode audio into continuous representations using established techniques such as wav2vec 2.0. Using these embeddings, we then learn how to apply soft attention masks over time, producing explicit labels which can distinguish distinct events. These masks guide a temporal convolutional network that produces the final sequence of clip tokens.",1
"Managing large numbers of incoming bug reports and finding the most critical issues in hardware development is time consuming, but crucial in order to reduce development costs. In this paper, we present an approach to predict the time to fix, the risk and the complexity of debugging and resolution of a bug report using different supervised machine learning algorithms, namely Random Forest, Naive Bayes, SVM, MLP and XGBoost. Further, we investigate the effect of the application of active learning and we evaluate the impact of different text representation techniques, namely TF-IDF, Word2Vec, Universal Sentence Encoder and XLNet on the model's performance. The evaluation shows that a combination of text embeddings generated through the Universal Sentence Encoder and MLP as classifier outperforms all other methods, and is well suited to predict the risk and complexity of bug tickets.",0
"As hardware development becomes increasingly complex, efficient bug tracking solutions have become essential for ensuring that projects stay on track and meet their deadlines. In order to develop such a solution, natural language processing (NLP) can play a key role by enabling better communication among team members, streamlining reporting processes, and facilitating automation of certain tasks. This paper presents a comprehensive overview of NLP techniques that can be utilized to create a modern bug tracking system tailored to the specific needs of hardware development teams. We discuss important considerations for effective implementation of these techniques, including data preprocessing, feature extraction, classification algorithms, and evaluation metrics. Furthermore, we provide case studies demonstrating real-world applications of our proposed approach to illustrate how NLP can enhance bug tracking capabilities in practice. Our findings highlight the significant potential of NLP for improving collaboration and efficiency within the hardware development domain.",1
"Robots are becoming everyday devices, increasing their interaction with humans. To make human-machine interaction more natural, cognitive features like Visual Voice Activity Detection (VVAD), which can detect whether a person is speaking or not, given visual input of a camera, need to be implemented. Neural networks are state of the art for tasks in Image Processing, Time Series Prediction, Natural Language Processing and other domains. Those Networks require large quantities of labeled data. Currently there are not many datasets for the task of VVAD. In this work we created a large scale dataset called the VVAD-LRS3 dataset, derived by automatic annotations from the LRS3 dataset. The VVAD-LRS3 dataset contains over 44K samples, over three times the next competitive dataset (WildVVAD). We evaluate different baselines on four kinds of features: facial and lip images, and facial and lip landmark features. With a Convolutional Neural Network Long Short Term Memory (CNN LSTM) on facial images an accuracy of 92% was reached on the test set. A study with humans showed that they reach an accuracy of 87.93% on the test set.",0
"This dataset contains audio files that have been annotated by human labelers who indicated which parts of each file contained voices speaking in different languages. These labels were then used as training data to train deep learning models that can automatically detect whether any given audio file contains speech. Testing showed good results - F1 scores ranging from .84 on a small validation set up to .92 on a larger held out test set of random speakers, across all five languages present in our corpus (English, Spanish, French, Mandarin Chinese, Arabic). While these scores represent improvements over prior work using other methods, they also confirm that performance varies depending on factors such as microphone placement and ambient noise conditions. Further investigation into ways to improve generalization of trained models beyond their initial distribution of input stimuli may lead to even better results. We plan to make this dataset publicly available so others may build upon our work and use the same task setup. Note: I apologize if there are errors in my language generation, as I am still under development and sometimes make mistakes!",1
"Transformer architecture has emerged to be successful in a number of natural language processing tasks. However, its applications to medical vision remain largely unexplored. In this study, we present UTNet, a simple yet powerful hybrid Transformer architecture that integrates self-attention into a convolutional neural network for enhancing medical image segmentation. UTNet applies self-attention modules in both encoder and decoder for capturing long-range dependency at different scales with minimal overhead. To this end, we propose an efficient self-attention mechanism along with relative position encoding that reduces the complexity of self-attention operation significantly from $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also proposed to recover fine-grained details from the skipped connections in the encoder. Our approach addresses the dilemma that Transformer requires huge amounts of data to learn vision inductive bias. Our hybrid layer design allows the initialization of Transformer into convolutional networks without a need of pre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac magnetic resonance imaging cohort. UTNet demonstrates superior segmentation performance and robustness against the state-of-the-art approaches, holding the promise to generalize well on other medical image segmentations.",0
"Abstract: In recent years, deep learning has emerged as a powerful tool for medical image segmentation tasks. Among the many architectures used for these tasks, transformer models have gained popularity due to their ability to effectively capture global dependencies and achieve state-of-the-art results. However, one limitation of pure transformer models lies in their limited contextual capacity compared to convolutional neural networks (CNNs). To address this gap, we propose UTNet, which combines both CNN and transformer blocks into a hybrid architecture that provides rich local context while maintaining global dependencies. Our approach outperforms previous methods across various benchmark datasets and achieves superior results in terms of accuracy and efficiency. Furthermore, our model requires fewer parameters than other transformer-based approaches without sacrificing performance. This work paves the way towards future research exploring hybrid architectures combining CNNs and transformers for improved medical image analysis.",1
"Transformers are state-of-the-art deep learning models that are composed of stacked attention and point-wise, fully connected layers designed for handling sequential data. Transformers are not only ubiquitous throughout Natural Language Processing (NLP), but, recently, they have inspired a new wave of Computer Vision (CV) applications research. In this work, a Vision Transformer (ViT) is applied to predict the state variables of 2-dimensional Ising model simulations. Our experiments show that ViT outperform state-of-the-art Convolutional Neural Networks (CNN) when using a small number of microstate images from the Ising model corresponding to various boundary conditions and temperatures. This work opens the possibility of applying ViT to other simulations, and raises interesting research directions on how attention maps can learn about the underlying physics governing different phenomena.",0
This is intended as a journal submission to Physical Review E:,1
"Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges -- namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme -- per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss. Our source code is available at~\url{https://github.com/qualcomm-ai-research/transformer-quantization}.",0
"As neural networks grow increasingly complex, their computational demands have increased dramatically, requiring ever more specialized hardware configurations to run efficiently. One promising approach to making these models faster has been quantization - reducing floating-point weights down to integers while preserving as much accuracy as possible. However, applying these methods efficiently remains a significant challenge due to several key factors: First, traditional approaches often focus on speedup above all else, sacrificing model quality in some cases; Second, even highly efficient approaches can still struggle under resource constraints that are commonplace in many real world applications; Finally, there exist additional obstacles such as non-linear activation functions, which further complicate attempts at quantization. This work explores recent advances aimed at addressing each of these challenges head-on. By leveraging novel weight scaling schemes and approximation techniques along with pruning, we show that we can achieve impressive tradeoffs between efficiency and quality across both image classification and language generation tasks. Furthermore, our proposed techniques adaptively balance the number of bits used for encoding different parameters, enabling greater flexibility during deployment. Combined together, these results demonstrate clear progress toward making large transformer models viable within a wider range of contexts than was previously feasible. To summarize, effective quantization is crucial given the growing prevalence of big data and machine learning, but existing approaches either trade off too much performance or don’t account fo",1
"Standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on certain groups, especially in the presence of spurious correlations between the input and label. Prior approaches that achieve high worst-group accuracy, like group distributionally robust optimization (group DRO) require expensive group annotations for each training point, whereas approaches that do not use such group annotations typically achieve unsatisfactory worst-group accuracy. In this paper, we propose a simple two-stage approach, JTT, that first trains a standard ERM model for several epochs, and then trains a second model that upweights the training examples that the first model misclassified. Intuitively, this upweights examples from groups on which standard ERM models perform poorly, leading to improved worst-group performance. Averaged over four image classification and natural language processing tasks with spurious correlations, JTT closes 75% of the gap in worst-group accuracy between standard ERM and group DRO, while only requiring group annotations on a small validation set in order to tune hyperparameters.",0
"Improving robustness against adversarial attacks has been a critical challenge in deep learning research. Recently, several methods have emerged that leverage group regularization techniques to train models such that they perform well across multiple input groups. However, these approaches typically require training on diverse group information which can be time-consuming and computationally expensive. In this work, we propose a novel method called ""Just Train Twice"" (JTT) where only two sets of data are used for training instead. Our approach involves adding random noise to one set of data to create virtual auxiliary classes that act as surrogates for real groups. This allows us to learn representations that generalize better under distribution shifts without explicitly leveraging true group labels. Our experimental results demonstrate significant improvements over state-of-the-art group regularization methods while significantly reducing computational requirements. Overall, our findings highlight the potential effectiveness and efficiency benefits of JTT for improving model robustness in the face of adversarial attacks.",1
"Internet of Things (IoT)-based indoor localization has gained significant popularity recently to satisfy the ever-increasing requirements of indoor Location-based Services (LBS). In this context, Inertial Measurement Unit (IMU)-based localization is of interest as it provides a scalable solution independent of any proprietary sensors/modules. Existing IMU-based methodologies, however, are mainly developed based on statistical heading and step length estimation techniques that suffer from cumulative error issues and have extensive computational time requirements limiting their application for real-time indoor positioning. To address the aforementioned issues, we propose the Online Dynamic Window (ODW)-assisted two-stage Long Short Term Memory (LSTM) localization framework. Three ODWs are proposed, where the first model uses a Natural Language Processing (NLP)-inspired Dynamic Window (DW) approach, which significantly reduces the required computational time. The second framework is developed based on a Signal Processing Dynamic Windowing (SP-DW) approach to further reduce the required processing time of the two-stage LSTM-based model. The third ODW, referred to as the SP-NLP, combines the first two windowing mechanisms to further improve the overall achieved accuracy. Compared to the traditional LSTM-based positioning approaches, which suffer from either high tensor computation requirements or low accuracy, the proposed ODW-assisted models can perform indoor localization in a near-real time fashion with high accuracy. Performances of the proposed ODW-assisted models are evaluated based on a real Pedestrian Dead Reckoning (PDR) dataset. The results illustrate potentials of the proposed ODW-assisted techniques in achieving high classification accuracy with significantly reduced computational time, making them applicable for near real-time implementations.",0
"This research proposal presents a novel approach to indoor localization using two-stage Long Short Term Memory (LSTM) frameworks assisted by Online Dynamic Windows (ODW). The proposed method addresses several limitations of existing approaches including high computational requirements, low accuracy, and lack of robustness in dynamic environments.  The first stage of the proposed framework uses LSTM networks to learn spatial features from sensory data such as WiFi signals, Bluetooth signals, and cellular tower signals. These features are then used to predict potential locations within a predefined range of interest. In the second stage, ODW is employed to update the weights of the network dynamically based on new sensor readings received during runtime, ensuring that the system adapts quickly to changes in the environment.  Experimental results show that the proposed method outperforms state-of-the-art methods in terms of accuracy and robustness, achieving up to 94% precision and recall on real-world datasets. Additionally, the proposed method requires significantly less computing power than previous approaches while maintaining comparable performance.  In summary, this research demonstrates the effectiveness of combining LSTM and ODW techniques for accurate and reliable indoor localization in changing environments. The proposed method has significant implications for applications such as autonomous robots, healthcare monitoring systems, and smart home automation. Further work includes improving the scalability and expanding the dataset coverage of the proposed method.",1
"Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time.",0
"Abstract: In recent years, transformer models have been widely used in natural language processing tasks such as machine translation, question answering, and text classification. However, these models often require large amounts of computational resources and can suffer from slow inference speed. Therefore, there is a need for more efficient transformer architectures that balance accuracy and efficiency. This paper presents a novel model called ""Point Transformer,"" which addresses these challenges by introducing several key innovations. Firstly, we propose a new attention mechanism that exploits both locality and sparsity, resulting in significant reduction in computation cost without sacrificing performance. Secondly, we introduce a layer normalization technique based on feature grouping, further reducing computations while preserving quality. We demonstrate through extensive experiments on various benchmarks that our proposed architecture achieves competitive results compared to state-of-the-art models while requiring significantly less computational overhead, making it suitable for deployment on resource-constrained devices. Our contributions represent a promising step towards building efficient transformers without compromising accuracy.",1
"Complex, multi-task problems have proven to be difficult to solve efficiently in a sparse-reward reinforcement learning setting. In order to be sample efficient, multi-task learning requires reuse and sharing of low-level policies. To facilitate the automatic decomposition of hierarchical tasks, we propose the use of step-by-step human demonstrations in the form of natural language instructions and action trajectories. We introduce a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, conditioned on language. We find that human demonstrations help solve the most complex tasks. We also find that incorporating natural language allows the model to generalize to unseen tasks in a zero-shot setting and to learn quickly from a few demonstrations. Generalization is not only reflected in the actions of the agent, but also in the generated natural language instructions in unseen tasks. Our approach also gives our trained agent interpretable behaviors because it is able to generate a sequence of high-level descriptions of its actions.",0
"Increasing generalization in reinforcement learning (RL) is a significant challenge as agents must learn from limited experience, adapt to new tasks quickly, and transfer knowledge across diverse domains. Traditionally, RL relies heavily on trial-and-error interactions between agents and environments, which often leads to suboptimal performance due to insufficient data or unreliable feedback signals. However, recent advances have demonstrated the effectiveness of using human instructions or demonstrations to improve agent learning efficiency and performance by providing explicit guidance that accelerates exploration, directs attention to essential aspects, and fine-tunes behavior policies. This paper presents Ask Your Humans, an approach that leverages human knowledge during policy optimization, enabling agents to achieve better generalization abilities through interactive experiences under real-world constraints. By integrating natural language processing techniques, we enable flexible and informative communication channels where humans can express their desired outcomes and guide agents' decisions more effectively. Our extensive evaluation shows significant improvements over strong baselines, confirming the benefits of incorporating human inputs into the model training process, particularly in complex scenarios with high uncertainty or ambiguity. Overall, Ask Your Humans offers promising potential as a practical tool for improving RL solutions and reducing reliance on large datasets or elaborate reward engineering processes, laying the foundation for future research at the intersection of RL and HCI.",1
"Recurrent neural networks (RNNs) are powerful in the tasks oriented to sequential data, such as natural language processing and video recognition. However, since the modern RNNs, including long-short term memory (LSTM) and gated recurrent unit (GRU) networks, have complex topologies and expensive space/computation complexity, compressing them becomes a hot and promising topic in recent years. Among plenty of compression methods, tensor decomposition, e.g., tensor train (TT), block term (BT), tensor ring (TR) and hierarchical Tucker (HT), appears to be the most amazing approach since a very high compression ratio might be obtained. Nevertheless, none of these tensor decomposition formats can provide both the space and computation efficiency. In this paper, we consider to compress RNNs based on a novel Kronecker CANDECOMP/PARAFAC (KCP) decomposition, which is derived from Kronecker tensor (KT) decomposition, by proposing two fast algorithms of multiplication between the input and the tensor-decomposed weight. According to our experiments based on UCF11, Youtube Celebrities Face and UCF50 datasets, it can be verified that the proposed KCP-RNNs have comparable performance of accuracy with those in other tensor-decomposed formats, and even 278,219x compression ratio could be obtained by the low rank KCP. More importantly, KCP-RNNs are efficient in both space and computation complexity compared with other tensor-decomposed ones under similar ranks. Besides, we find KCP has the best potential for parallel computing to accelerate the calculations in neural networks.",0
"Title: ""Kronecker CP Decomposition for Efficient Implementation of Recurrent Neural Networks""  Abstract: Recurrent neural networks (RNNs) have emerged as powerful models for processing sequential data such as natural language text or speech signals. However, their efficiency can suffer due to high computational complexity and memory requirements. In particular, matrix multiplications involving large weight matrices slow down training and inference.  To address these issues, we propose using the Kronecker Canonical Polyadic decomposition (CPD) to factorize the weight matrices involved in RNN computations into smaller, more efficient ones. This allows us to perform matrix multiplications using faster Fourier Transform methods that exploit sparsity and locality patterns present in the decomposed factors. Our approach extends previous work on linear regression and convolutional neural networks to recurrent models. We show through experiments on benchmark datasets that our proposed method significantly reduces both computation time and memory usage without compromising model accuracy. Furthermore, by analyzing the properties of the resulting decompositions, we gain insights into the structure and redundancy inherent in RNN architectures.  In summary, our technique offers an effective solution for accelerating RNN implementations while preserving their expressive power. These advances hold promise for broader application areas where resource-constrained environments demand highly efficient machine learning solutions.",1
"Visual question answering (VQA) has recently been introduced to remote sensing to make information extraction from overhead imagery more accessible to everyone. VQA considers a question (in natural language, therefore easy to formulate) about an image and aims at providing an answer through a model based on computer vision and natural language processing methods. As such, a VQA model needs to jointly consider visual and textual features, which is frequently done through a fusion step. In this work, we study three different fusion methodologies in the context of VQA for remote sensing and analyse the gains in accuracy with respect to the model complexity. Our findings indicate that more complex fusion mechanisms yield an improved performance, yet that seeking a trade-of between model complexity and performance is worthwhile in practice.",0
"In this paper, we discuss how to create an effective image-text embedding model for use in remote sensing visual question answering systems. We begin by reviewing previous work in the field and identifying key challenges that must be overcome in order to achieve high performance. We then propose a novel methodology based on fine-grained attention mechanisms and adversarial training which significantly improves upon existing approaches. Our approach first extracts features from both images and questions using convolutional neural networks (CNN) and recurrent neural networks (RNN), respectively. Next, these features are fused together using an attention mechanism designed specifically for multi-modal data fusion. Finally, our model is trained using adversarial training techniques inspired by generative adversarial network (GAN) architectures which further improve its ability to accurately predict answers to natural language queries about remotely sensed imagery. Evaluation results demonstrate the effectiveness of our approach compared to state-of-the-art methods.",1
"Pre-Trained Vision-Language Models (VL-PTMs) have shown promising capabilities in grounding natural language in image data, facilitating a broad variety of cross-modal tasks. However, we note that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for quantities of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. To address the challenge, we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap. In this way, our prompt tuning approach enables strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive experimental results show that prompt tuned VL-PTMs outperform their fine-tuned counterparts by a large margin (e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard deviation reduction on average with one shot in RefCOCO evaluation). All the data and code will be available to facilitate future research.",0
"In natural language processing, pre-trained models such as GPT have revolutionized how we approach text generation tasks. However, these models are typically trained on large amounts of text data only, without any visual input. Recently, pre-training techniques using both image and text inputs have been proposed in order to improve performance in vision-language problems like zero-shot prompted image description (CLIP) and automatic question answering (AskCLI). These methods aim at training models that can effectively process textual descriptions of images, but they still lack generalization and robustness across different domains. In our work, we introduce CPT - Colorful Prompt Tuning, a novel method for fine-tuning CLIP-like models by dynamically generating colorful caption candidates. Our key insight is that optimizing over multiple candidate captions allows us to generate more accurate descriptions than traditional single sentence optimization objectives. We evaluate our model through several benchmarks including COCO Caption Evaluation Challenge and Flickr8k Test Suite. Experiments show that our method achieves significant improvement over the state-of-the-art on these datasets while keeping simplicity in prompt engineering. Moreover, our method has the potential to leverage diverse external knowledge sources beyond image and text supervision. This opens up new possibilities in prompt-based tuning for other applications such as few-shot learning and meta-learning for computer vision. Overall, we believe our approach paves the way for future research into developing even stronger multimodal models via richer self-supervised signal from carefully designed but automatically generated prompt ensembles.",1
"Generating fine-grained, realistic images from text has many applications in the visual and semantic realm. Considering that, we propose Bangla Attentional Generative Adversarial Network (AttnGAN) that allows intensified, multi-stage processing for high-resolution Bangla text-to-image generation. Our model can integrate the most specific details at different sub-regions of the image. We distinctively concentrate on the relevant words in the natural language description. This framework has achieved a better inception score on the CUB dataset. For the first time, a fine-grained image is generated from Bangla text using attentional GAN. Bangla has achieved 7th position among 100 most spoken languages. This inspires us to explicitly focus on this language, which will ensure the inevitable need of many people. Moreover, Bangla has a more complex syntactic structure and less natural language processing resource that validates our work more.",0
"Fine-grained image generation from text descriptions has been an active research area for many years now, but there are still several challenges that need to be addressed. One such challenge is generating high-quality images from textual descriptions that contain fine-grained details. This paper proposes a new approach based on attentional generative adversarial networks (Attention GANs) which can generate more detailed and accurate images from Bangla text descriptions. Our proposed method uses attention mechanisms within the generator network to focus on relevant parts of the input description and produce corresponding features in the generated image. Additionally, we use multi-scale discriminators to ensure that the generated images have sufficient quality at different scales. We evaluate our approach by comparing its performance against state-of-the-art methods on two benchmark datasets and demonstrate its superiority through quantitative and qualitative evaluations. Overall, our results show that our proposed method significantly improves the accuracy of fine-grained image generation from Bangla text descriptions, opening up possibilities for applications in fields like multimedia data retrieval, computer vision, and natural language processing.",1
"We investigate the incorporation of visual relationships into the task of supervised image caption generation by proposing a model that leverages detected objects and auto-generated visual relationships to describe images in natural language. To do so, we first generate a scene graph from raw image pixels by identifying individual objects and visual relationships between them. This scene graph then serves as input to our graph-to-text model, which generates the final caption. In contrast to previous approaches, our model thus explicitly models the detection of objects and visual relationships in the image. For our experiments we construct a new dataset from the intersection of Visual Genome and MS COCO, consisting of images with both a corresponding gold scene graph and human-authored caption. Our results show that our methods outperform existing state-of-the-art end-to-end models that generate image descriptions directly from raw input pixels when compared in terms of the BLEU and METEOR evaluation metrics.",0
"One possible abstract for the paper ""Scene Graph Generation for Better Image Captioning?"" could read as follows:  The task of image caption generation involves generating natural language descriptions of images. In order to generate accurate and detailed descriptions, it is important to have a good understanding of the relationships and connections within the scene depicted in the image. This paper proposes using a scene graph to represent these relationships and improve the performance of image captioning models.  A scene graph is a structured representation of the objects, attributes, and relationships present in an image. By using machine learning algorithms to automatically generate high quality scene graphs from raw input images, we can provide image captioning systems with valuable contextual information that they would otherwise lack. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing significant improvements over baseline methods that do not use scene graphs. Our results show that the proposed method leads to more accurate and descriptive captions while remaining efficient enough for real-time applications.",1
"The longitudinal modeling of neuroanatomical changes related to Alzheimer's disease (AD) is crucial for studying the progression of the disease. To this end, we introduce TransforMesh, a spatio-temporal network based on transformers that models longitudinal shape changes on 3D anatomical meshes. While transformer and mesh networks have recently shown impressive performances in natural language processing and computer vision, their application to medical image analysis has been very limited. To the best of our knowledge, this is the first work that combines transformer and mesh networks. Our results show that TransforMesh can model shape trajectories better than other baseline architectures that do not capture temporal dependencies. Moreover, we also explore the capabilities of TransforMesh in detecting structural anomalies of the hippocampus in patients developing AD.",0
"Abstract:  In recent years, there has been growing interest in using machine learning algorithms such as transformers to analyze medical images. In particular, there is a need for methods that can longitudinally model anatomical meshes (i.e., models that represent the shape and appearance of organs or tissues over time). However, existing transformer networks have limitations when applied to these types of meshes, including difficulties in handling missing data and capturing complex geometric relationships among neighboring vertices.  To address these challenges, we propose a novel method called ""TransforMesh"" which combines traditional vertex representation with additional edge features and mesh normal vectors. This allows our network to capture more detailed geometric information from the mesh structure. We introduce new attention mechanisms specifically designed for working with meshes, allowing the network to focus on relevant parts of the mesh while minimizing distractions caused by missing data or other irrelevant regions. Our approach uses a shared convolutional decoder architecture across all layers for efficient computation, enabling us to generate high quality longitudinal meshes at inference time.  Experimental results demonstrate the effectiveness of our proposed method for generating high resolution 4D meshes with improved accuracy compared to previous state-of-the-art approaches. In addition, our framework achieves better temporal coherence in both the shape and texture of the 4D meshes. These findings indicate the potential of our method for applications such as patient specific modelling of cardiac motion, simulation of disease progression, and evaluation of treatment outcomes.",1
"Recent progress in the Natural Language Processing domain has given us several State-of-the-Art (SOTA) pretrained models which can be finetuned for specific tasks. These large models with billions of parameters trained on numerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In this paper, we discuss the need for a benchmark for cost and time effective smaller models trained on a single GPU. This will enable researchers with resource constraints experiment with novel and innovative ideas on tokenization, pretraining tasks, architecture, fine tuning methods etc. We set up Small-Bench NLP, a benchmark for small efficient neural language models trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks on the publicly available GLUE datasets and a leaderboard to track the progress of the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture achieves an average score of 81.53 which is comparable to that of BERT-Base's 82.20 (110M parameters). Our models, code and leaderboard are available at https://github.com/smallbenchnlp",0
"Small-bench NLP was created to provide a benchmark dataset for testing small language processing systems that run on individual PCs without costly parallel hardware such as clouds or clusters. Unlike other datasets used for training large language models like GPT or BERT which contain millions of tokens, this dataset only contains one million tokens. This makes it ideal for use by hobbyists or researchers who don't have access to massive amounts of compute power, allowing them to train smaller models quickly and easily. Additionally, this dataset includes a variety of tasks including sentiment analysis, part-of-speech tagging, dependency parsing, etc so users can compare their performance against existing state-of-the-art baselines. Overall, small-bench NLP provides a resource for those working on NLP without significant resources at their disposal, enabling them to work on cutting edge problems in NLP without needing to invest heavily in computing infrastructure.",1
"Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by the query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed moment candidates and then find out the best-matching one. 2) proposal-free models directly predict two temporal boundaries of the referential moment from frames. Currently, almost all the propose-and-rank methods have inferior performance than proposal-free counterparts. In this paper, we argue that propose-and-rank approach is underestimated due to the predefined manners: 1) Hand-designed rules are hard to guarantee the complete coverage of targeted segments. 2) Densely sampled candidate moments cause redundant computation and degrade the performance of ranking process. To this end, we propose a novel model termed LPNet (Learnable Proposal Network for NLVL) with a fixed set of learnable moment proposals. The position and length of these proposals are dynamically adjusted during training process. Moreover, a boundary-aware loss has been proposed to leverage frame-level information and further improve the performance. Extensive ablations on two challenging NLVL benchmarks have demonstrated the effectiveness of LPNet over existing state-of-the-art methods.",0
"In this paper, we propose natural language video localization using learnable moment proposals (LMPs). LMPs are learned representations of regions of interest that can be used for tasks such as object detection, semantic segmentation, and action recognition. We use these region proposals to generate natural language descriptions of objects and actions within a given video sequence. Our approach enables automatic text generation by combining bottom-up visual features and top-down cues from natural language queries. We show that our method outperforms state-of-the-art approaches on challenging benchmark datasets, demonstrating the effectiveness of using learnable moment proposals for natural language video understanding. Keywords: Natural Language Localization, Learnable Moment Proposals, Action Recognition, Object Detection, Semantic Segmentation",1
"Temporal Sentence Grounding in Videos (TSGV), i.e., grounding a natural language sentence which indicates complex human activities in a long and untrimmed video sequence, has received unprecedented attentions over the last few years. Although each newly proposed method plausibly can achieve better performance than previous ones, current TSGV models still tend to capture the moment annotation biases and fail to take full advantage of multi-modal inputs. Even more incredibly, several extremely simple baselines without training can also achieve state-of-the-art performance. In this paper, we take a closer look at the existing evaluation protocols for TSGV, and find that both the prevailing dataset splits and evaluation metrics are the devils to cause unreliable benchmarking. To this end, we propose to re-organize two widely-used TSGV benchmarks (ActivityNet Captions and Charades-STA). Specifically, we deliberately make the ground-truth moment distribution different in the training and test splits, i.e., out-of-distribution (OOD) testing. Meanwhile, we introduce a new evaluation metric dR@n,IoU@m to calibrate the basic IoU scores by penalizing on the bias-influenced moment predictions and alleviate the inflating evaluations caused by the dataset annotation biases such as overlong ground-truth moments. Under our new evaluation protocol, we conduct extensive experiments and ablation studies on eight state-of-the-art TSGV methods. All the results demonstrate that the re-organized dataset splits and new metric can better monitor the progress in TSGV. Our reorganized datsets are available at https://github.com/yytzsy/grounding_changing_distribution.",0
"This paper provides an overview of the problem of temporal sentence grounding in videos. We first define the task, discuss related work, and then present our dataset, which contains both temporally annotated video clips and manual annotations from human judges on how well each clip ""grounds"" a given natural language instruction. Finally, we introduce a new metric that takes into account different aspects of semantic similarity to evaluate the performance of models on this task. Our contributions are as follows: 1) construction of a large and diverse dataset suitable for training and evaluation; 2) introduction of an evaluation metric that accounts for multiple sources of variation in human annotation; 3) presentation of baseline model results using standard deep learning methods (such as end-to-end trained neural networks); 4) demonstration through ablation studies that our metric effectively measures key aspects of temporal sentence understanding by machines. Overall, we aim to stimulate research and development in improving machines ability to interact with complex visual environments such as those found in videos, especially in terms of their use of natural language commands to query these systems.",1
"3D visual grounding aims at grounding a natural language description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object region. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These inherent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D Semantics Assisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding objects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively utilizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from 37.7% to 49.2%, which significantly surpasses the non-SAT baseline with the identical network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef.",0
"In this paper we propose SAT: 2D Semantics Assisted Training for 3D Visual Grounding, a novel approach that exploits the strong correlation existing between textual descriptions and object detection models. This correlation leads our method towards better localization and retrieval capabilities. Our method is trained on large amounts of data collected from image description datasets like COCO. The use of a pretrained model allows us to transfer knowledge across domains. Through extensive experimentation, we demonstrate that by utilizing a combination of weakly supervised object detectors based on Faster R-CNNs and classifiers, we achieve significant improvements over current state-of-the-art methods in both 2D object detection and 3D visual grounding tasks. Additionally, we showcase the effectiveness of our approach through a variety of ablation studies where we analyze performance under different experimental conditions. We conclude by discussing future research directions aimed at expanding the scope of our proposed approach to other applications such as natural language generation and human behavior understanding using computer vision techniques. Overall, this work presents a comprehensive solution for visual object recognition problems which can potentially lead to broader advancements in artificial intelligence and cognitive science.",1
"Transformers have seen an unprecedented rise in Natural Language Processing and Computer Vision tasks. However, in audio tasks, they are either infeasible to train due to extremely large sequence length of audio waveforms or reach competitive performance after feature extraction through Fourier-based methods, incurring a loss-floor. In this work, we introduce an architecture, Audiomer, where we combine 1D Residual Networks with Performer Attention to achieve state-of-the-art performance in Keyword Spotting with raw audio waveforms, out-performing all previous methods while also being computationally cheaper, much more parameter and data-efficient. Audiomer allows for deployment in compute-constrained devices and training on smaller datasets.",0
"Audio keyword spotting refers to the task of detecting predefined keywords from audio recordings. This task has numerous applications such as speech recognition, video surveillance, and automated broadcast monitoring. Recently, deep learning methods have demonstrated state-of-the-art performance on this task due to their ability to capture complex patterns in audio data. In particular, transformer models have shown promising results by effectively capturing global dependencies in sequential data. However, designing efficient and accurate architectures for keyword spotting remains challenging due to factors such as varying keyword lengths, limited training data, and computational constraints. To address these issues, we introduce Audiomer, a novel convolutional transformer architecture designed specifically for keyword spotting. Our model utilizes both local and global attention mechanisms to improve performance while reducing computation requirements. We evaluate our approach using benchmark datasets and show that Audiomer outperforms existing methods across multiple metrics including precision, recall, and F1 score. Overall, Audiomer represents a significant advancement in the field of keyword spotting, paving the way for improved performance and wider adoption of this technology.",1
"Transformers, the default model of choices in natural language processing, have drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks (convnets) to overcome its inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations without investigating how to optimally combine self-attention (i.e., the core of transformers) with convolution. To address this issue, in this paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful segmentation model with an interleaved architecture based on empirical combination of self-attention and convolution. In practice, nnFormer learns volumetric representations from 3D local volumes. Compared to the naive voxel-level self-attention implementation, such volume-based operations help to reduce the computational complexity by approximate 98% and 99.5% on Synapse and ACDC datasets, respectively. In comparison to prior-art network configurations, nnFormer achieves tremendous improvements over previous transformer-based methods on two commonly used datasets Synapse and ACDC. For instance, nnFormer outperforms Swin-UNet by over 7 percents on Synapse. Even when compared to nnUNet, currently the best performing fully-convolutional medical segmentation network, nnFormer still provides slightly better performance on Synapse and ACDC.",0
"This paper presents a novel architecture for volumetric segmentation called ""nnFormer"", which builds upon the successes of both transformers and fully convolutional networks (FCNs). While previous methods have achieved state-of-the-art results through combinations of encoders and decoders that leverage high-resolution feature maps, our method explores a new approach by interleaving these components to better capture contextual information across space and time. Our experiments demonstrate the effectiveness of our design on two challenging datasets, showing significant improvements over existing methods. We believe that this work represents a step forward towards more efficient architectures for image segmentation tasks.",1
"Vision-language pre-training has recently emerged as a promising alternative for representation learning. It shifts from the tradition of using images and discrete labels for learning a fixed set of weights, seen as visual concepts, to aligning images and raw text for two separate encoders. Such a paradigm benefits from a broader source of supervision and allows zero-shot transfer to downstream tasks since visual concepts can be diametrically generated from natural language, known as prompt. In this paper, we identify that a major challenge of deploying such models in practice is prompt engineering. This is because designing a proper prompt, especially for context words surrounding a class name, requires domain expertise and typically takes a significant amount of time for words tuning since a slight change in wording could have a huge impact on performance. Moreover, different downstream tasks require specific designs, further hampering the efficiency of deployment. To overcome this challenge, we propose a novel approach named context optimization (CoOp). The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. In this way, the design of task-relevant prompts can be fully automated. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots (e.g., at 16 shots the average gain is around 17% with the highest reaching over 50%). CoOp also exhibits strong robustness to distribution shift.",0
"This paper presents a systematic investigation into learning effective text prompts for vision-language models. We find that current techniques for selecting image descriptions as prompts suffer from significant biases and limitations: often focusing on concrete object nouns over spatial relationships or actions, lacking diversity in language style or content, and struggling with referential ambiguity across objects, scenes, or even model versions. To address these issues, we propose two simple but highly impactful methods: (i)~Prompt Engineer\textemdash{}a data augmentation scheme that generates diverse, human-like and factually coherent alternatives given one seed description; (ii)~Model Manager\textemdash{}an ensemble method that learns which subsets of alternative prompts from both humans and Prompt Engineer better align with different subtasks across multiple datasets, reducing bias towards specific domains/labels or surface patterns.\looseness=-1 Our proposed approaches achieve state-of-the-art results on four challenging benchmark tasks without any additional training or fine-tuning of existing VLM architectures like CLIP\texttrademark{}. By exploring more nuanced aspects of successful prompt engineering, our work contributes new insights and tools toward automating complex linguistic reasoning about visual inputs, thus opening up exciting opportunities for natural AI interactions and creative problem solving beyond current language processing paradigms.\relax",1
"Inspired by the success of transformer-based pre-training methods on natural language tasks and further computer vision tasks, researchers have begun to apply transformer to video processing. This survey aims to give a comprehensive overview on transformer-based pre-training methods for Video-Language learning. We first briefly introduce the transformer tructure as the background knowledge, including attention mechanism, position encoding etc. We then describe the typical paradigm of pre-training & fine-tuning on Video-Language processing in terms of proxy tasks, downstream tasks and commonly used video datasets. Next, we categorize transformer models into Single-Stream and Multi-Stream structures, highlight their innovations and compare their performances. Finally, we analyze and discuss the current challenges and possible future research directions for Video-Language pre-training.",0
"Title: ""Transformer Based Approaches for Video Language Processing""  Video language processing has seen significant advancements in recent years due to the availability of large datasets and advances in deep learning techniques. In particular, pre-trained transformers have emerged as powerful models for natural language processing tasks such as text generation, translation, and question answering. However, there is limited research on using these models for video language understanding. This survey aims to fill that gap by providing an overview of state-of-the-art approaches for applying transformer-based models to video language processing tasks. We discuss key challenges faced in training and fine-tuning these models for video data, highlight successful applications across different domains, and analyze their strengths and limitations. Our study helps lay the groundwork for further research in this area, enabling development of more advanced systems for video analysis.",1
"We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program $f$, and the goal is to find an input $x$ which makes $f$ output ""True"". The puzzles are objective in that each one is specified entirely by the source code of its verifier $f$, so evaluating $f(x)$ is all that is needed to test a candidate solution $x$. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems that are immediately obvious to human programmers (but not necessarily to AI), to classic programming puzzles (e.g., Towers of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). The objective nature of P3 readily supports self-supervised bootstrapping. We develop baseline enumerative program synthesis and GPT-3 solvers that are capable of solving easy puzzles -- even without access to any reference solutions -- by learning from their own past solutions. Based on a small user study, we find puzzle difficulty to correlate between human programmers and the baseline AI solvers.",0
"Solving puzzles is more than just fun entertainment - they can also improve cognitive functioning and enhance problem solving skills. In recent years, programming puzzles have gained popularity as a tool for developers to sharpen their coding abilities and prepare them for real-world software engineering challenges. However, there remains limited research on the effectiveness of these puzzles in enhancing programming expertise. This study seeks to fill that gap by evaluating the impact of programming puzzles on novice programmers’ ability to solve problems effectively under pressure. Specifically, we designed two types of programming puzzles: one based on debugging code snippets and another involving generating algorithms. We then recruited participants at different levels of proficiency in Python programming language to take part in our experiment. Participants were randomly assigned into experimental groups where one group received training using traditional teaching methods while the other engaged with the programming puzzles. After an extensive pretest–posttest design, results indicate that programming puzzles significantly improved participants’ performance in terms of accuracy and speed when tackling real programming tasks compared to those who learned from conventional sources alone. These findings suggest that incorporating programming puzzles into computer science education curriculum could lead to significant improvements in students' coding proficiency.",1
"Patients associated with multiple co-occurring health conditions often face aggravated complications and less favorable outcomes. Co-occurring conditions are especially prevalent among individuals suffering from kidney disease, an increasingly widespread condition affecting 13% of the general population in the US. This study aims to identify and characterize patterns of co-occurring medical conditions in patients employing a probabilistic framework. Specifically, we apply topic modeling in a non-traditional way to find associations across SNOMEDCT codes assigned and recorded in the EHRs of13,000 patients diagnosed with kidney disease. Unlike most prior work on topic modeling, we apply the method to codes rather than to natural language. Moreover, we quantitatively evaluate the topics, assessing their tightness and distinctiveness, and also assess the medical validity of our results. Our experiments show that each topic is succinctly characterized by a few highly probable and unique disease codes, indicating that the topics are tight. Furthermore, inter-topic distance between each pair of topics is typically high, illustrating distinctiveness. Last, most coded conditions grouped together within a topic, are indeed reported to co-occur in the medical literature. Notably, our results uncover a few indirect associations among conditions that have hitherto not been reported as correlated in the medical literature.",0
"Medical records contain valuable information regarding patients’ comorbidities, but analyzing millions of clinical notes is challenging due to their unstructured nature. We propose using a generative probabilistic model based on Latent Dirichlet Allocation (LDA) to extract comorbidity patterns from millions of electronic health record narratives encoded using Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT). Our approach leverages knowledge extracted from patient encounters, lab orders, diagnosis codes and medications in each encounter note. Using these features, our model captures co-occurrences among different SNOMED CT concepts within local communities defined by individual patients, thus generating meaningful insights into disease clusters specific to patient populations such as cardiovascular risk factors, gastroenterology issues and metabolic disorders. By applying LDA over massive amounts of data and identifying shared topics across many patients, we aim to provide a novel method for exposing meaningful relationships among medical problems that can assist both providers and researchers in making better-informed decisions. Title: ""Probabilistic Topic Modeling of SNOMED Codes Reveals Insightful Comorbidity Patterns"" Abstract: The management and analysis of large volumes of unstructured medical data pose significant challenges in gaining important insights into patients’ comorbidities. This study proposes a latent Dirichlet allocation (LDA)-based generative probabilistic model utilizing Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT) to tackle these difficulties. Our framework mines data from multiple sources, including electronic health records (EHR), lab orders, diagnosis codes, and medication orders. Utilizing these diverse data points, our model discovers concurrency clusters for different SNOMED CT concepts at the level of individual patients, resulting in locally relevant insights into disease groupings. These discoveries range from cardiovascular risks t",1
"Statistical language modeling and translation with transformers have found many successful applications in program understanding and generation tasks, setting high benchmarks for tools in modern software development environments. The finite context window of these neural models means, however, that they will be unable to leverage the entire relevant context of large files and packages for any given task. While there are many efforts to extend the context window, we introduce an architecture-independent approach for leveraging the syntactic hierarchies of source code for incorporating entire file-level context into a fixed-length window. Using concrete syntax trees of each source file we extract syntactic hierarchies and integrate them into context window by selectively removing from view more specific, less relevant scopes for a given task. We evaluate this approach on code generation tasks and joint translation of natural language and source code in Python programming language, achieving a new state-of-the-art in code completion and summarization for Python in the CodeXGLUE benchmark. We also introduce new CodeXGLUE benchmarks for user-experience-motivated tasks: code completion with normalized literals, method body completion/code summarization conditioned on file-level context.",0
"This paper presents a new method for analyzing source code files called Long-Range Modelling Of Source Code Files With Ewash: Extended Window Access By Syntax Hierarchy (ewash). Ewash utilizes the concept of syntax hierarchy as a means of organizing data within a window so that large amounts of source code can be easily understood and maintained. Additionally, this tool allows users to define their own access rules, meaning that developers have complete control over how they structure their code base. By using extended windows, ewash makes it possible to analyze both local and non-local contexts at once, providing insight into the relationships between different parts of the code. Ultimately, ewash represents a significant step forward in software engineering and development and promises to make the process more efficient and effective than ever before.",1
"Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention.   Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.",0
"Recent advances in deep learning have resulted in significant improvements in natural language processing tasks such as text generation and translation. One key factor behind these successes has been the use of transformer architectures, which excel at capturing long-range dependencies by self attention mechanisms. However, training transformer models can often be computationally expensive and require vast amounts of data and computational resources. In this work, we aim to find more efficient alternatives to the standard transformer architecture that maintains their performance on downstream NLP tasks while reducing the need for large datasets and computing power. We investigate different methods such as data augmentation techniques, model pruning, quantization, and hybrid approaches to achieve this goal. Our experiments on several benchmark datasets demonstrate the efficacy of our proposed methods, showing promising results for deploying transformer models in real-world settings where resource constraints may apply. This paper provides a comprehensive guide and discussion of current research trends in developing efficient transformer models for language modeling, highlighting opportunities for future work in this area.",1
"Temporal sentence grounding in videos(TSGV), which aims to localize one target segment from an untrimmed video with respect to a given sentence query, has drawn increasing attentions in the research community over the past few years. Different from the task of temporal action localization, TSGV is more flexible since it can locate complicated activities via natural languages, without restrictions from predefined action categories. Meanwhile, TSGV is more challenging since it requires both textual and visual understanding for semantic alignment between two modalities(i.e., text and video). In this survey, we give a comprehensive overview for TSGV, which i) summarizes the taxonomy of existing methods, ii) provides a detailed description of the evaluation protocols(i.e., datasets and metrics) to be used in TSGV, and iii) in-depth discusses potential problems of current benchmarking designs and research directions for further investigations. To the best of our knowledge, this is the first systematic survey on temporal sentence grounding. More specifically, we first discuss existing TSGV approaches by grouping them into four categories, i.e., two-stage methods, end-to-end methods, reinforcement learning-based methods, and weakly supervised methods. Then we present the benchmark datasets and evaluation metrics to assess current research progress. Finally, we discuss some limitations in TSGV through pointing out potential problems improperly resolved in the current evaluation protocols, which may push forwards more cutting edge research in TSGV. Besides, we also share our insights on several promising directions, including three typical tasks with new and practical settings based on TSGV.",0
"One of the key challenges in computer vision is understanding what happens over time in video sequences, and connecting language descriptions of events to the corresponding visual representations. This task is known as temporal sentence grounding in videos (TSG), which involves identifying relevant spans of video that correspond to natural language sentences describing past events. TSG has potential applications in areas such as video analysis, event detection, and human-machine interaction. However, there have been few comprehensive surveys on TSG despite growing interest in the field. In response, we provide a survey of TSG approaches across different modalities (e.g., text, images, audio) and domains (e.g., entertainment, sports). We evaluate current methods based on their effectiveness at generating accurate alignments between language statements and temporally localized segments from videos. Our aim is to encourage further research into the development of more advanced TSG techniques by highlighting promising directions and discussing open challenges in the field.",1
"Estimating the performance of a machine learning system is a longstanding challenge in artificial intelligence research. Today, this challenge is especially relevant given the emergence of systems which appear to increasingly outperform human beings. In some cases, this ""superhuman"" performance is readily demonstrated; for example by defeating legendary human players in traditional two player games. On the other hand, it can be challenging to evaluate classification models that potentially surpass human performance. Indeed, human annotations are often treated as a ground truth, which implicitly assumes the superiority of the human over any models trained on human annotations. In reality, human annotators can make mistakes and be subjective. Evaluating the performance with respect to a genuine oracle may be more objective and reliable, even when querying the oracle is expensive or impossible. In this paper, we first raise the challenge of evaluating the performance of both humans and models with respect to an oracle which is unobserved. We develop a theory for estimating the accuracy compared to the oracle, using only imperfect human annotations for reference. Our analysis provides a simple recipe for detecting and certifying superhuman performance in this setting, which we believe will assist in understanding the stage of current research on classification. We validate the convergence of the bounds and the assumptions of our theory on carefully designed toy experiments with known oracles. Moreover, we demonstrate the utility of our theory by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and show that under our assumptions a number of models from recent years are with high probability superhuman.",0
"This research presents a new approach for evaluating the performance of classifiers that have been trained using advanced machine learning techniques such as deep neural networks. These ""superhuman"" classifiers can often outperform human experts on specific tasks, but their accuracy and reliability must still be verified before they can be trusted to make important decisions. To address this challenge, we propose a methodology for certifying the outputs of superhuman classifiers using human judgments. Our approach involves collecting feedback from human annotators on a subset of examples that were labeled by the classifier. We use this feedback to identify any errors made by the classifier and to adjust its parameters accordingly. By iteratively refining the classifier based on human input, our method ensures that it achieves high levels of accuracy while remaining interpretable and transparent. Overall, our work offers a promising solution for building reliable, trustworthy artificial intelligence systems that complement human expertise rather than replace it.",1
"EEG signals are usually simple to obtain but expensive to label. Although supervised learning has been widely used in the field of EEG signal analysis, its generalization performance is limited by the amount of annotated data. Self-supervised learning (SSL), as a popular learning paradigm in computer vision (CV) and natural language processing (NLP), can employ unlabeled data to make up for the data shortage of supervised learning. In this paper, we propose a self-supervised contrastive learning method of EEG signals for sleep stage classification. During the training process, we set up a pretext task for the network in order to match the right transformation pairs generated from EEG signals. In this way, the network improves the representation ability by learning the general features of EEG signals. The robustness of the network also gets improved in dealing with diverse data, that is, extracting constant features from changing data. In detail, the network's performance depends on the choice of transformations and the amount of unlabeled data used in the training process of self-supervised learning. Empirical evaluations on the Sleep-edf dataset demonstrate the competitive performance of our method on sleep staging (88.16% accuracy and 81.96% F1 score) and verify the effectiveness of SSL strategy for EEG signal analysis in limited labeled data regimes. All codes are provided publicly online.",0
"Abstract: Recent advances in machine learning have enabled the development of techniques that can automatically classify sleep stages using electroencephalography (EEG) signals. One such method, self-supervised contrastive learning (SSL), has shown promise in improving sleep stage classification accuracy without requiring large amounts of labeled data. In this work, we explore the use of SSL for EEG-based sleep staging by training neural network models on pairs of EEG waveforms that are randomly selected from within each epoch and each sleep stage. We evaluate our approach using two publicly available datasets and compare it against other commonly used methods for sleep stage classification. Our results show that SSL outperforms these baseline methods, achieving higher accuracies across all four sleep stages. These findings suggest that SSL may provide a valuable tool for researchers studying sleep disorders and could potentially lead to more accurate diagnoses and better treatment options for patients.",1
"Chest radiographs are one of the most common diagnostic modalities in clinical routine. It can be done cheaply, requires minimal equipment, and the image can be diagnosed by every radiologists. However, the number of chest radiographs obtained on a daily basis can easily overwhelm the available clinical capacities. We propose RATCHET: RAdiological Text Captioning for Human Examined Thoraces. RATCHET is a CNN-RNN-based medical transformer that is trained end-to-end. It is capable of extracting image features from chest radiographs, and generates medically accurate text reports that fit seamlessly into clinical work flows. The model is evaluated for its natural language generation ability using common metrics from NLP literature, as well as its medically accuracy through a surrogate report classification task. The model is available for download at: http://www.github.com/farrell236/RATCHET.",0
"In this paper, we present RATCHET (Radiologist Assistant by Cascading Tables), a novel medical transformer architecture for chest X-ray diagnosis and reporting. Despite recent advances in image analysis using convolutional neural networks (CNNs), manual report generation remains laborious and time consuming. To address these challenges, we introduce RATCHET as an end-to-end solution that integrates both image analysis and natural language processing components within a unified framework. Our approach leverages fine-grained attention mechanisms across cascaded table representations to capture interdependencies among multi-scale features and semantic concepts underlying radiological images. We train our model on a large dataset consisting of deidentified patient scans from multiple hospitals. Experimental results demonstrate significant improvements over prior art on key metrics relevant to clinical practice such as perceptual quality scores, detection sensitivity and accuracy, and automation efficiency. Our findings suggest potential benefits of deploying RATCHET as a decision support tool in routine diagnostic workflows to reduce turnaround times and enhance quality of care without sacrificing human expertise. Overall, this work represents an initial step towards realizing artificial intelligence applications beyond simple classification tasks in healthcare settings while mitigating concerns regarding explainability, interpretability, ethics, privacy, security, safety, robustness, generalization, and fairness. Future directions may involve expanding the scope of RATCHET’s capabilities along different dimensions such as disease specificity, task customizability, context adaptivity, explainability, transparency, scalability, portability, reproducibility, maintainability, and sustainability. Ultimately, continued progress necessitates active collaboration involving stakeholders spanning diverse disciplines including radiology experts, software engineers, data scientists, bioethicists, policymakers, legal scholars, cybersecurity researchers, social impact analysts, educators, funding agencies, patien",1
"Adversarial regularization has been shown to improve the generalization performance of deep learning models in various natural language processing tasks. Existing works usually formulate the method as a zero-sum game, which is solved by alternating gradient descent/ascent algorithms. Such a formulation treats the adversarial and the defending players equally, which is undesirable because only the defending player contributes to the generalization performance. To address this issue, we propose Stackelberg Adversarial Regularization (SALT), which formulates adversarial regularization as a Stackelberg game. This formulation induces a competition between a leader and a follower, where the follower generates perturbations, and the leader trains the model subject to the perturbations. Different from conventional approaches, in SALT, the leader is in an advantageous position. When the leader moves, it recognizes the strategy of the follower and takes the anticipated follower's outcomes into consideration. Such a leader's advantage enables us to improve the model fitting to the unperturbed data. The leader's strategic information is captured by the Stackelberg gradient, which is obtained using an unrolling algorithm. Our experimental results on a set of machine translation and natural language understanding tasks show that SALT outperforms existing adversarial regularization baselines across all tasks. Our code is publicly available.",0
"Advances in machine learning have led to significant improvements in tasks such as image classification, natural language processing, and computer vision. However, these models remain vulnerable to adversarial attacks, which can cause them to make incorrect predictions by adding carefully crafted perturbations to input data. To address this problem, researchers have proposed adversarial regularization methods that aim to increase model robustness against such attacks. In this paper, we present an unrolling optimization approach that reformulates adversarial regularization as a two-player game inspired by the Stackelberg competition framework. This allows us to design efficient algorithms that incorporate both minimax objectives and standard regularizers into a single end-to-end trainable system. Our experiments demonstrate that our method significantly improves the robustness of deep neural networks on several benchmark datasets while maintaining their accuracy on clean inputs. Overall, our work provides a new perspective on adversarial regularization and offers promising results towards enhancing the security of artificial intelligence systems.",1
"Change captioning tasks aim to detect changes in image pairs observed before and after a scene change and generate a natural language description of the changes. Existing change captioning studies have mainly focused on a single change.However, detecting and describing multiple changed parts in image pairs is essential for enhancing adaptability to complex scenarios. We solve the above issues from three aspects: (i) We propose a simulation-based multi-change captioning dataset; (ii) We benchmark existing state-of-the-art methods of single change captioning on multi-change captioning; (iii) We further propose Multi-Change Captioning transformers (MCCFormers) that identify change regions by densely correlating different regions in image pairs and dynamically determines the related change regions with words in sentences. The proposed method obtained the highest scores on four conventional change captioning evaluation metrics for multi-change captioning. Additionally, our proposed method can separate attention maps for each change and performs well with respect to change localization. Moreover, the proposed framework outperformed the previous state-of-the-art methods on an existing change captioning benchmark, CLEVR-Change, by a large margin (+6.1 on BLEU-4 and +9.7 on CIDEr scores), indicating its general ability in change captioning tasks.",0
"This paper describes a method for detecting multiple changes within a sequence data using transformer architecture. We introduce a novel extension that models local dependencies between adjacent tokens, which captures interactions between them. Our model can handle various types of changes, including insertions, deletions, substitutions, swaps, and shifts. Experimental results on synthetic datasets demonstrate that our model outperforms state-of-the-art methods by a significant margin while achieving competitive performance on real-world benchmark datasets. Furthermore, we show that our approach leads to improved understanding of specific change operations compared to prior work. These findings suggest promising applications for natural language processing tasks such as text editing and machine translation.",1
"Due to the advances in computing and sensing, deep learning (DL) has widely been applied in smart energy systems (SESs). These DL-based solutions have proved their potentials in improving the effectiveness and adaptiveness of the control systems. However, in recent years, increasing evidence shows that DL techniques can be manipulated by adversarial attacks with carefully-crafted perturbations. Adversarial attacks have been studied in computer vision and natural language processing. However, there is very limited work focusing on the adversarial attack deployment and mitigation in energy systems. In this regard, to better prepare the SESs against potential adversarial attacks, we propose an innovative adversarial attack model that can practically compromise dynamical controls of energy system. We also optimize the deployment of the proposed adversarial attack model by employing deep reinforcement learning (RL) techniques. In this paper, we present our first-stage work in this direction. In simulation section, we evaluate the performance of our proposed adversarial attack model using standard IEEE 9-bus system.",0
"The proposed attack targets the contingency detection mechanism which plays a crucial role in maintaining stability in smart energy systems. The adversary crafts a carefully designed power supply schedule that subtly modifies grid operations while appearing innocuous to current techniques used in detecting contingencies. Our findings highlight serious limitations in existing approaches towards contingency analysis, calling for advancements in methods able to effectively mitigate potential security threats posed by intelligent attacks on smart grids. ---  An attack against the contingency detection mechanisms in smart energy systems can pose significant risks to their overall stability and reliability. Current techniques employed to identify such vulnerabilities may prove insufficient in preventing these types of malicious actions. In our study, we demonstrate how adversaries can manipulate power distribution schedules in ways that go undetected, potentially causing harmful consequences for the grid’s integrity if left unchecked. Through rigorous testing and evaluation, we showcase how certain shortcomings in conventional contingency assessment strategies make them susceptible to manipulation, ultimately emphasizing the need for more advanced protection measures capable of safeguarding smart grids from intelligent threats. By shedding light on the potential dangers posed by contingency attacks, our research serves as a call for further improvements in securing critical infrastructure amidst the rapidly evolving landscape of cybersecurity.",1
"Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\% to~10\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\% relatively.",0
"Title should be ""Self-supervised speech pre-training"" Abstract for paper on self-supervised speech pre-training  With the advent of deep learning, there has been significant progress in the field of natural language processing (NLP) using large amounts of data and supervised training techniques. However, obtaining high-quality annotated data can be expensive and time consuming, making NLP challenges difficult to scale across different languages and domains. As such, there has been increasing interest in unsupervised and semi-supervised methods that can learn representations from large datasets without explicit annotations. In recent years, one popular method used for natural language representation learning is BERT (Bidirectional Encoder Representations from Transformers), which uses a masked language model objective to predict missing tokens given surrounding context. Here we explore using BERT as a starting point for pre-trained models on spoken language, leveraging contrastive objectives that encourage alignment between phonetic codes and their corresponding textual transcriptions. We describe our approach as wav2bert, where we train a model that maps raw audio inputs into continuous space representations amenable to fine-grained downstream tasks. Our evaluation shows promising results outperforming other state-of-the-art approaches on several benchmarks including TIMIT, IMDb, and Google Speech Commands.",1
"Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and regression. 2) Bottom-up model: It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these methods are not end-to-end, \ie, they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as \textbf{GTR}. Specifically, GTR has two encoders for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the decoder, we design a new Multi-head Cross-Modal Attention. The whole GTR is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three benchmarks have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and metrics, with several times faster inference speed.",0
"Recent advances have introduced multi-modal transformer architectures which fuse audio and visual features to obtain robust video representations. This has led to significant improvements on natural language grounding tasks where textual descriptions can refer to objects within videos. In our work, we investigate the effectiveness of using pretrained models that incorporate temporal modality as well, given their successes on other video tasks such as activity recognition, action detection and image generation. We apply recent techniques from contrastive learning by formulating a surrogate task based on human annotated object bounding boxes and compare multiple designs across different model sizes, pretraining strategies and fine-tuning regimes. Our extensive experiments reveal intricate design tradeoffs involved in achieving state-of-the-art performance on this challenging benchmark dataset that contains diverse scenarios spanning complex interactions among humans and objects. By probing into the attention mechanisms driving these results, we provide novel insights into how temporal reasoning impacts the choice of referring expressions when grounded against video data compared to simpler alternatives. These findings present opportunities for future research addressing remaining challenges involving scale variability and semantic ambiguity, paving ways towards more intelligent systems capable of understanding real world scenes in greater detail.",1
"Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In fact, in most cases researchers and practitioners resort to the well-known tf-idf as default, despite the existence of other suitable alternatives, including graph-based models. In this paper, we perform an exhaustive and large-scale empirical comparison of both statistical and graph-based term weighting methods in the context of keyword extraction. Our analysis reveals some interesting findings such as the advantages of the less-known lexical specificity with respect to tf-idf, or the qualitative differences between statistical and graph-based methods. Finally, based on our findings we discuss and devise some suggestions for practitioners. Source code to reproduce our experimental results, including a keyword extraction library, are available in the following repository: https://github.com/asahi417/kex",0
"In recent years, keyword extraction has become increasingly important as a tool for text summarization, categorization, and analysis. However, despite numerous studies on different methods and techniques, there remains no consensus on which method is most effective. This paper seeks to contribute to the field by performing a quantitative analysis of statistical and graph-based term weighting schemes for keyword extraction. By comparing these two approaches using several well-known datasets and evaluation metrics, we aim to provide insights into their strengths and weaknesses, and identify potential areas for improvement. Our findings suggest that both statistical and graph-based methods have their own advantages and disadvantages, but certain combinations may perform better than others depending on specific characteristics of the dataset at hand. Overall, our study highlights the importance of evaluating multiple methods before selecting one for application.",1
"A key problem in multi-task learning (MTL) research is how to select high-quality auxiliary tasks automatically. This paper presents GradTS, an automatic auxiliary task selection method based on gradient calculation in Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS improves the performance of MT-DNN with a bert-base-cased backend model, from 0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE benchmarks. GradTS is also time-saving since (1) its gradient calculations are based on single-task experiments and (2) the gradients are re-used without additional experiments when the candidate task set changes. On the 8 GLUE classification tasks, for example, GradTS costs on average 21.32% less time than AUTOSEM with comparable GPU consumption. Further, we show the robustness of GradTS across various task settings and model selections, e.g. mixed objectives among candidate tasks. The efficiency and efficacy of GradTS in these case studies illustrate its general applicability in MTL research without requiring manual task filtering or costly parameter tuning.",0
"This paper presents a new method for automatically selecting auxiliary tasks for machine learning models based on gradient computation using a transformer network architecture. The proposed approach, called GradTS (Gradient-based Auxiliary Task Selection), identifies the most informative auxiliary tasks that have a strong correlation with the main task, enabling more efficient use of computational resources and improved model performance. Our experiments show that GradTS significantly outperforms existing baseline methods across several datasets and architectures, demonstrating its effectiveness in generating high-quality auxiliary tasks that enhance the training process. The results highlight the potential impact of our work on improving the efficiency and accuracy of machine learning algorithms.",1
"The cross entropy loss is widely used due to its effectiveness and solid theoretical grounding. However, as training progresses, the loss tends to focus on hard to classify samples, which may prevent the network from obtaining gains in performance. While most work in the field suggest ways to classify hard negatives, we suggest to strategically leave hard negatives behind, in order to focus on misclassified samples with higher probabilities. We show that adding to the optimization goal the expectation loss, which is a better approximation of the zero-one loss, helps the network to achieve better accuracy. We, therefore, propose to shift between the two losses during training, focusing more on the expectation loss gradually during the later stages of training. Our experiments show that the new training protocol improves performance across a diverse set of classification domains, including computer vision, natural language processing, tabular data, and sequences. Our code and scripts are available at supplementary.",0
"""Abstract: In this work we discuss two loss functions that are commonly used for training models in machine learning applications - cross entropy (CE) and expectation loss (EL). We analyze their strengths and weaknesses and propose new methods that combine them in order to achieve better performance on certain tasks. Our results show that the combination can lead to more accurate predictions and improved generalization. Overall, our study provides insights into how different loss terms affect model behavior and could inform future research in loss function design.""",1
"The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.",0
"Recent advances in deep learning have led to increased computational requirements and power consumption during inference tasks. Efficient design and management of compute resources and energy usage are critical issues that must be addressed if deep learning systems are to continue scaling up and meeting their full potential. This study presents a comprehensive analysis of compute and energy trends in deep learning inference across different hardware platforms, including central processing units (CPUs), graphics processing units (GPUs) and specialized accelerators such as Tensor Processing Units (TPUs). By analyzing energy consumption patterns across different models, batch sizes and data distributions, we identify key factors affecting efficiency and propose techniques for optimizing resource allocation and reducing energy overhead. Our results demonstrate significant performance improvements through novel resource partitioning strategies and provide valuable insights into managing future workloads and emerging system architectures. Keywords: Deep learning inference; GPU computing; TPU; Accelerator; Energy efficiency",1
"Internet search affects people's cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models.",0
"This study examines whether gender-neutral queries used by image search engines truly result in neutral images that are free from biases. Using various experiments and analyses, we find that while some aspects of gender-neutral query formulations can mitigate bias, they may not always provide the desired outcomes. Our results suggest that there is still work to be done in reducing gender bias in image search algorithms, and demonstrate the importance of considering multiple factors when designing and evaluating such systems.",1
"The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.",0
"Here we present the results of our investigation into how well modifications made to transformer models transfer across implementations and applications. We evaluate two different state-of-the-art transformers, BERT and GPT-2, on multiple downstream tasks and compare their performance with different configurations and variations. Our analysis shows that while some modifications are more broadly applicable than others, there are still significant gains to be had from careful fine-tuning of model hyperparameters and architectures. Overall, these findings have important implications for practitioners working with transformer models and suggest areas where future research could lead to even greater improvements in their effectiveness.",1
"This paper proposes Panoptic Narrative Grounding, a spatially fine and general formulation of the natural language visual grounding problem. We establish an experimental framework for the study of this new task, including new ground truth and metrics, and we propose a strong baseline method to serve as stepping stone for future work. We exploit the intrinsic semantic richness in an image by including panoptic categories, and we approach visual grounding at a fine-grained level by using segmentations. In terms of ground truth, we propose an algorithm to automatically transfer Localized Narratives annotations to specific regions in the panoptic segmentations of the MS COCO dataset. To guarantee the quality of our annotations, we take advantage of the semantic structure contained in WordNet to exclusively incorporate noun phrases that are grounded to a meaningfully related panoptic segmentation region. The proposed baseline achieves a performance of 55.4 absolute Average Recall points. This result is a suitable foundation to push the envelope further in the development of methods for Panoptic Narrative Grounding.",0
"In the age of social media, instant gratification and constant connectivity has led to a shift in how we consume and create content online. With the rise of influencers and microcelebrities on platforms like TikTok and Instagram, we see a new type of narrative emerging that combines traditional storytelling techniques with interactive elements that allow audiences to participate directly in the creation process. This new form of “panoptic” storytelling presents both opportunities and challenges for creators seeking to engage their audience and maintain control over their own narratives. In this paper, we explore how panoptic narrative grounding can provide a framework for understanding and analyzing this emergent phenomenon, as well as offer insights into effective strategies for creating meaningful experiences within these complex systems. Using case studies from successful panoptic stories such as Mr Beast’s #ArgleBargleChallenge and Cody Ko’s Mini Challenge series, we demonstrate how combining principles of gamification, co-creation, and audience agency can lead to powerful and memorable moments of shared experience. By examining key features of these projects and considering future developments, our research contributes to discussions surrounding user participation in digital media and offers practical guidance for those looking to navigate this exciting but uncertain terrain. Overall, we believe that by investigating the mechanisms of interaction in contemporary online culture through lenses such as performance, spectacle, intimacy and risk taking, innovative approaches can be identified which capitalize upon the unique qualities afforded by these new forms of expression while mitigating potential risks associated with unchecked influence and manipulation. This inquiry should serve the interests of academics, professionals and members o",1
"Temporal grounding aims to temporally localize a video moment in the video whose semantics are related to a given natural language query. Existing methods typically apply a detection or regression pipeline on the fused representation with a focus on designing complicated heads and fusion strategies. Instead, from a perspective on temporal grounding as a metric-learning problem, we present a Dual Matching Network (DMN), to directly model the relations between language queries and video moments in a joint embedding space. This new metric-learning framework enables fully exploiting negative samples from two new aspects: constructing negative cross-modal pairs from a dual matching scheme and mining negative pairs across different videos. These new negative samples could enhance the joint representation learning of two modalities via cross-modal pair discrimination to maximize their mutual information. Experiments show that DMN achieves highly competitive performance compared with state-of-the-art methods on four video grounding benchmarks. Based on DMN, we present a winner solution for STVG challenge of the 3rd PIC workshop. This suggests that metric-learning is still a promising method for temporal grounding via capturing the essential cross-modal correlation in a joint embedding space.",0
"Effective metric learning has been shown to be crucial for many computer vision tasks such as object detection, image classification, and semantic segmentation. Recent advancements have focused on developing algorithms that can learn metrics directly from data without relying on handcrafted features. However, most existing approaches only use positive samples (i.e., matching pairs) during training, which may lead to suboptimal solutions due to the lack of negative sample guidance. In this work, we propose a novel framework called Negative Sample Matters (NSM) that incorporates both positive and negative samples into the metric learning process. Our method first generates a set of hard negative samples by clustering similarities obtained from kNN search. Then, we introduce a modality network that learns two modalities from both positive and negative examples to enhance their discriminative power. Finally, we jointly optimize the modality network and the distance function using alternating optimization. Experiments show significant improvements over state-of-the-art methods across three temporal grounding benchmarks under both metric evaluation and full model testing settings. This study demonstrates the importance of negative sample guidance in effective metric learning for temporal grounding.",1
"Narrated instructional videos often show and describe manipulations of similar objects, e.g., repairing a particular model of a car or laptop. In this work we aim to reconstruct such objects and to localize associated narrations in 3D. Contrary to the standard scenario of instance-level 3D reconstruction, where identical objects or scenes are present in all views, objects in different instructional videos may have large appearance variations given varying conditions and versions of the same product. Narrations may also have large variation in natural language expressions. We address these challenges by three contributions. First, we propose an approach for correspondence estimation combining learnt local features and dense flow. Second, we design a two-step divide and conquer reconstruction approach where the initial 3D reconstructions of individual videos are combined into a 3D alignment graph. Finally, we propose an unsupervised approach to ground natural language in obtained 3D reconstructions. We demonstrate the effectiveness of our approach for the domain of car maintenance. Given raw instructional videos and no manual supervision, our method successfully reconstructs engines of different car models and associates textual descriptions with corresponding objects in 3D.",0
"This study focuses on reconstructing and grounding narrated instructional videos (NIVs) in 3D. NIVs have become increasingly popular as a means of conveying instructions through multimedia content. However, they often lack precise spatial information, making it difficult for viewers to follow along accurately. To address this issue, we propose a method that integrates language processing techniques with computer vision algorithms to create 3D reconstructions of scenes described in NIVs.  Our approach involves multiple stages, including natural language understanding, scene decomposition, 3D model generation, object detection and pose estimation, and image synthesis. First, our system extracts semantic information from NIV audio scripts using natural language understanding techniques such as named entity recognition, part-of-speech tagging, and dependency parsing. Then, we analyze each sentence and decompose the scene into objects and their corresponding poses. Next, we generate 3D models based on these descriptions, incorporating prior knowledge from online databases of real-world object shapes and sizes. Finally, we render images of the scene, complete with accurate lighting conditions and camera angles, providing a photorealistic representation of the reconstruction.  Experimental results demonstrate the effectiveness of our approach across several datasets, outperforming state-of-the-art methods in terms of accuracy and consistency of 3D reconstructions. Furthermore, subjective evaluations confirm that our 3D representations significantly improve viewer comprehension of complex tasks demonstrated in NIVs. Overall, our work presents a significant step towards enhancing the accessibility and clarity of multimedia learning materials by combining advanced linguistics and computer vision techniques. By transforming NIVs into immersive, interactive, and informative experiences, learners can better engage with visualized content, fostering greater retention and application of learned material.",1
"Along with feature points for image matching, line features provide additional constraints to solve visual geometric problems in robotics and computer vision (CV). Although recent convolutional neural network (CNN)-based line descriptors are promising for viewpoint changes or dynamic environments, we claim that the CNN architecture has innate disadvantages to abstract variable line length into the fixed-dimensional descriptor. In this paper, we effectively introduce Line-Transformers dealing with variable lines. Inspired by natural language processing (NLP) tasks where sentences can be understood and abstracted well in neural nets, we view a line segment as a sentence that contains points (words). By attending to well-describable points on aline dynamically, our descriptor performs excellently on variable line length. We also propose line signature networks sharing the line's geometric attributes to neighborhoods. Performing as group descriptors, the networks enhance line descriptors by understanding lines' relative geometries. Finally, we present the proposed line descriptor and matching in a Point and Line Localization (PL-Loc). We show that the visual localization with feature points can be improved using our line features. We validate the proposed method for homography estimation and visual localization.",0
"Abstract: This paper presents a novel approach for visual localization using lines as contextual features in outdoor environments. Lines are ubiquitous in urban scenes and can provide valuable cues for place recognition by representing paths, edges, boundaries, and contours that characterize the environment. To leverage these visual clues, we introduce a line descriptor called ""LineNet"" which processes raw image patches containing lines into a fixed-length feature vector. Our proposed method extracts dense descriptors in images using a sliding window technique, followed by aggregation through multiple levels of convolutional neural networks (CNN). We show that our approach significantly improves the accuracy of localization on real-world datasets while reducing computational complexity compared to previous state-of-the-art methods. Additionally, we evaluate the effectiveness of LineNet in challenging scenarios such as poor lighting conditions and occlusions, demonstrating robustness under varying environmental conditions. Overall, LineNet provides a powerful tool for enabling contextually aware visual localization in complex outdoor settings.",1
"Deep learning models exhibit a preference for statistical fitting over logical reasoning. Spurious correlations might be memorized when there exists statistical bias in training data, which severely limits the model performance especially in small data scenarios. In this work, we introduce Counterfactual Adversarial Training framework (CAT) to tackle the problem from a causality perspective. Particularly, for a specific sample, CAT first generates a counterfactual representation through latent space interpolation in an adversarial manner, and then performs Counterfactual Risk Minimization (CRM) on each original-counterfactual pair to adjust sample-wise loss weight dynamically, which encourages the model to explore the true causal effect. Extensive experiments demonstrate that CAT achieves substantial performance improvement over SOTA across different downstream tasks, including sentence classification, natural language inference and question answering.",0
"This paper introduces counterfactually guided adversarial learning with representation interpolation (CGAI), which combines two complementary approaches: adversarial training and representation learning. By using both techniques together, we are able to improve performance on a variety of benchmark datasets while reducing computational cost. We demonstrate that our method outperforms previous state-of-the art methods in several vision tasks such as image classification, object detection, segmentation and few shot learning. Our approach relies on a novel algorithm, called Causal Information Miner (CIM) that uses a contrastive objective function based on counterfactuals that allows us to guide the learning process towards better representations. Furthermore, CIM achieves state-of-the-art results by interpolating representations from different stages of the network, allowing better optimization of high level semantic features that are relevant for the task at hand. Overall, these findings highlight the benefits of combining adversarial training and representation learning for improved performance in computer vision.",1
"Video question answering (VideoQA) is challenging given its multimodal combination of visual understanding and natural language understanding. While existing approaches seldom leverage the appearance-motion information in the video at multiple temporal scales, the interaction between the question and the visual information for textual semantics extraction is frequently ignored. Targeting these issues, this paper proposes a novel Temporal Pyramid Transformer (TPT) model with multimodal interaction for VideoQA. The TPT model comprises two modules, namely Question-specific Transformer (QT) and Visual Inference (VI). Given the temporal pyramid constructed from a video, QT builds the question semantics from the coarse-to-fine multimodal co-occurrence between each word and the visual content. Under the guidance of such question-specific semantics, VI infers the visual clues from the local-to-global multi-level interactions between the question and the video. Within each module, we introduce a multimodal attention mechanism to aid the extraction of question-video interactions, with residual connections adopted for the information passing across different levels. Through extensive experiments on three VideoQA datasets, we demonstrate better performances of the proposed method in comparison with the state-of-the-arts.",0
"The ability of computers to process and analyze video data has improved tremendously in recent years due to advancements in artificial intelligence (AI) and computer vision techniques. One challenging task in video understanding is answering questions related to specific events occurring within a given video clip. To address this challenge, we present the use of a Temporal Pyramid Transformer (TPT), which can effectively capture temporal dependencies across multiple levels of abstraction in videos. Our approach incorporates multimodality by combining visual features extracted from RGB frames along with acoustic features obtained from audio streams to improve question answering accuracy. Experimental results demonstrate that our method outperforms state-of-the-art approaches on two benchmark datasets, validating the effectiveness of TPT for video question answering with multimodal interaction.",1
"Temporal grounding aims to predict a time interval of a video clip corresponding to a natural language query input. In this work, we present EVOQUER, a temporal grounding framework incorporating an existing text-to-video grounding model and a video-assisted query generation network. Given a query and an untrimmed video, the temporal grounding model predicts the target interval, and the predicted video clip is fed into a video translation task by generating a simplified version of the input query. EVOQUER forms closed-loop learning by incorporating loss functions from both temporal grounding and query generation serving as feedback. Our experiments on two widely used datasets, Charades-STA and ActivityNet, show that EVOQUER achieves promising improvements by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could facilitate error analysis by explaining temporal grounding model behavior.",0
"This sounds like a complex process! Can you please summarize how it works? For example, if I am working on a paper about quantum computing, would using a system such as yours cause me to come up with questions that relate directly back into the text of my paper (such as how quantum computing relates to cybersecurity), or would it generate more general ""filler"" type questions that may require additional research but don't have any direct relationship to the content of my work? In other words, can it essentially act like a kind of automatic study guide generator to ensure my mastery over a subject matter before turning in the final draft of a paper? Thanks! Let me know!",1
"Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual ""semantic field"" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users' language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field.   We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.",0
"Fine-grained facial editing allows users to make precise modifications to specific features of an image, such as changing the shape of someone's nose or eyes. However, current methods often require complex interfaces that can be difficult to use, particularly for those without expertise in graphic design. This study presents ""Talk-to-Edit,"" a novel approach to fine-grained facial editing that leverages natural language processing (NLP) and computer vision techniques to allow users to edit images through conversation. By using everyday language, our method makes it possible for anyone to adjust photos with ease, regardless of their technical background. We demonstrate the effectiveness and usability of our system by applying it to challenging scenarios involving hair restoration, aging simulation, and visual disorder correction. Our results show that Talk-to-Edit outperforms traditional photo editing tools on these tasks while significantly reducing user effort. Overall, this research contributes towards building intelligent systems capable of executing subtle tasks according to human instructions, paving the way for future applications in personalized digital content generation and even medical intervention planning.",1
"Machine learning has been utilized to perform tasks in many different domains such as classification, object detection, image segmentation and natural language analysis. Data labeling has always been one of the most important tasks in machine learning. However, labeling large amounts of data increases the monetary cost in machine learning. As a result, researchers started to focus on reducing data annotation and labeling costs. Transfer learning was designed and widely used as an efficient approach that can reasonably reduce the negative impact of limited data, which in turn, reduces the data preparation cost. Even transferring previous knowledge from a source domain reduces the amount of data needed in a target domain. However, large amounts of annotated data are still demanded to build robust models and improve the prediction accuracy of the model. Therefore, researchers started to pay more attention on auto annotation and labeling. In this survey paper, we provide a review of previous techniques that focuses on optimized data annotation and labeling for video, audio, and text data.",0
"This paper presents a survey of machine learning techniques that have been used to auto label video, audio, and text data. The authors review the state of the art in the field and identify key trends and challenges. They then discuss several approaches that have shown promise, including supervised learning, semi-supervised learning, unsupervised learning, deep learning, transfer learning, active learning, co-training, self-training, and reinforcement learning. The authors evaluate these methods based on their accuracy, scalability, and efficiency, and highlight their strengths and limitations. Finally, they offer recommendations for future research directions, including the need for more sophisticated evaluation metrics and the development of domain-specific models. Overall, the paper provides a comprehensive overview of the current landscape of auto labeling techniques and sets the stage for further advancements in the field.",1
"Generating images according to natural language descriptions is a challenging task. Prior research has mainly focused to enhance the quality of generation by investigating the use of spatial attention and/or textual attention thereby neglecting the relationship between channels. In this work, we propose the Combined Attention Generative Adversarial Network (CAGAN) to generate photo-realistic images according to textual descriptions. The proposed CAGAN utilises two attention models: word attention to draw different sub-regions conditioned on related words; and squeeze-and-excitation attention to capture non-linear interaction among channels. With spectral normalisation to stabilise training, our proposed CAGAN improves the state of the art on the IS and FID on the CUB dataset and the FID on the more challenging COCO dataset. Furthermore, we demonstrate that judging a model by a single evaluation metric can be misleading by developing an additional model adding local self-attention which scores a higher IS, outperforming the state of the art on the CUB dataset, but generates unrealistic images through feature repetition.",0
"Abstract: This research presents a new approach to text-to-image generation using combined attention generative adversarial networks (GAN). Our method leverages the strengths of both local and global attentions by designing two parallel discriminators that respectively focus on spatial details and overall contextual features. We then combine their outputs to formulate our final loss function. Our experiments demonstrate significant improvements over previous state-of-the-art methods across multiple benchmark datasets, showcasing the effectiveness of our proposed technique. By combining attention mechanisms within GANs, we enable more efficient use of computational resources while generating higher quality images from written descriptions.",1
"In the last decade or so, we have witnessed deep learning reinvigorating the machine learning field. It has solved many problems in the domains of computer vision, speech recognition, natural language processing, and various other tasks with state-of-the-art performance. The data is generally represented in the Euclidean space in these domains. Various other domains conform to non-Euclidean space, for which graph is an ideal representation. Graphs are suitable for representing the dependencies and interrelationships between various entities. Traditionally, handcrafted features for graphs are incapable of providing the necessary inference for various tasks from this complex data representation. Recently, there is an emergence of employing various advances in deep learning to graph data-based tasks. This article provides a comprehensive survey of graph neural networks (GNNs) in each learning setting: supervised, unsupervised, semi-supervised, and self-supervised learning. Taxonomy of each graph based learning setting is provided with logical divisions of methods falling in the given learning setting. The approaches for each learning task are analyzed from both theoretical as well as empirical standpoints. Further, we provide general architecture guidelines for building GNNs. Various applications and benchmark datasets are also provided, along with open challenges still plaguing the general applicability of GNNs.",0
"Abstract---Neural networks have been widely adopted as powerful tools for tackling complex problems in science and engineering. Since their introduction, researchers and practitioners alike have continually sought new ways to enhance their performance and expand their scope of application, leading to significant advances such as deep learning and convolutional neural networks (CNNs). However, traditional neural networks often face challenges related to their capacity, interpretability, stability, and scalability, especially when dealing with high-dimensional, large-scale, noisy, nonlinear, dynamic, uncertain, and incomplete data. This calls for innovative architectures that can better handle these types of issues while maintaining competitive accuracy levels.Graph Neural Networks (GNNs) represent one promising direction due to their ability to capture graph structured representations from complex relationships among elements in a problem domain. GNNs extend traditional neural networks by incorporating both global and local patterns, handling irregular domains using topological properties, preserving node order and proximity relationships, and efficiently utilizing neighborhood information through message passing. As a result, they present numerous advantages over alternative approaches, including enhanced representational power and learning capacity, improved robustness against noise and missing data, superior explainability and transparency, more efficient use of computational resources, and greater flexibility in model architecture design and optimization.In this survey article, we provide a comprehensive review of the development and recent trends in GNNs research since 2017. Our focus includes discussions on state-of-the art GNN architectures, applications across multiple domains, key achievements, technical challenges, future directions, and open research questions. By analyzing publications on arXiv and journals within computer science, statistics, physics, bioinformatics, environmental sciences, materials science, social network analysis, and others, our work aims at providing valuable insights and guidance for interested readers on how GNNs might contribute t",1
"Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.",0
"In recent years there has been significant interest in developing techniques that can effectively handle sequential data such as images, audio, and text. One approach that has gained popularity is the use of transformer networks, which were originally proposed for natural language processing tasks but have since found application in other domains including computer vision. This survey paper presents an overview of the current state-of-the-art in using transformers for image processing tasks. We discuss the motivation behind applying these models to visual data and provide a detailed review of existing work on designing architectures that incorporate transformers into convolutional neural networks (CNNs). We then outline several approaches to pre-training and fine-tuning transformer architectures on image datasets and describe their performance across different benchmarks. Finally, we highlight some key challenges faced by researchers working in this area and suggest directions for future research. Overall, our aim is to provide a comprehensive resource for practitioners and researchers interested in exploring the potential of transformer networks for tackling problems related to computer vision.",1
"Deep neural networks for natural language processing are fragile in the face of adversarial examples -- small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of LSTMs (and extensions of LSTMs) and training models that can be efficiently certified. Our approach can certify robustness to intractably large perturbation spaces defined programmatically in a language of string transformations. Our evaluation shows that (1) our approach can train models that are more robust to combinations of string transformations than those produced using existing techniques; (2) our approach can show high certification accuracy of the resulting models.",0
"Despite being powerful tools for modeling sequential data, Long Short Term Memory (LSTM) networks have been known to fail when confronted with inputs that differ from their training set in ways they were never explicitly exposed to during training time. We propose here two novel regularization techniques aiming at making these models more robust to such perturbations by learning representations which are insensitive to specific classes thereof. In the first part we introduce our contributions in the scope of text generation: We design a new type of noise which can be added to both input sequences as well as internal states during training without harming quality. By doing so we simultaneously increase robustness wrt common corruptions like misspellings or removed tokens while obtaining better sample quality. Finally, we also apply the above mentioned technique on top of state of the art language models trained either using Reinforcement Learning from human feedback or self supervisedlearning objectives such as MLM or denoising autoencoders . Secondly we focus on improving few shot prompting efficiency through knowledge distillation. For this purpose we freeze most parameters of all layers within some intermediate downstream block close to the beginning of the model but fine tune only those responsible for producing the output logits associated with class probabilities. Surprisingly enough even though the student knows nothing about the task to solve prior to adaptation, it still manages to outperform state of the art zero shot baselines as soon as just 4 examples per class become available . As expected the method works less effectively when noisy augmentation is not applied during teacher and/orstudent finetuning and fails completely when we remove it altogether framing results in previous paragraph within an appropriate setting given some random initialization.",1
"More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference.   In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by $1.5\%$ with $0.72 \times$ trainable parameters. Using $0.46 \times$ and $0.13 \times$ parameters, our WideNet can still surpass ViT and ViT-MoE by $0.8\%$ and $2.1\%$, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by $1.8\%$ on average and surpass BERT using factorized embedding parameterization by $0.8\%$ with fewer parameters.",0
"When writing a scientific research article, one common piece of advice that is often given to authors is “go wider instead of deeper.” While this may sound counterintuitive at first glance, there are actually several good reasons why adopting this approach can be beneficial both for your own productivity as well as for the impact of your work on the field. In many cases, trying to cover too much ground too quickly can lead to superficiality rather than real depth; by taking a more focused view of specific topics and their implications, you’ll find yourself able to engage with them more effectively and ultimately produce better quality work overall. Overall, while going narrowly into particular problems can seem like a limiting strategy, doing so thoughtfully – understanding the connections between problems and how they relate to broader trends within the field – can end up paying significant dividends over time. If these ideas resonate with you and you’d like to explore further how this might apply in your case specifically feel free to reach out. I’m always happy to chat!",1
"Crop and weed monitoring is an important challenge for agriculture and food production nowadays. Thanks to recent advances in data acquisition and computation technologies, agriculture is evolving to a more smart and precision farming to meet with the high yield and high quality crop production. Classification and recognition in Unmanned Aerial Vehicles (UAV) images are important phases for crop monitoring. Advances in deep learning models relying on Convolutional Neural Network (CNN) have achieved high performances in image classification in the agricultural domain. Despite the success of this architecture, CNN still faces many challenges such as high computation cost, the need of large labelled datasets, ... Natural language processing's transformer architecture can be an alternative approach to deal with CNN's limitations. Making use of the self-attention paradigm, Vision Transformer (ViT) models can achieve competitive or better results without applying any convolution operations. In this paper, we adopt the self-attention mechanism via the ViT models for plant classification of weeds and crops: red beet, off-type beet (green leaves), parsley and spinach. Our experiments show that with small set of labelled training data, ViT models perform better compared to state-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy of 99.8\% achieved by the ViT model.",0
"Agricultural crop monitoring using high resolution UAV images has become increasingly important as it allows farmers to easily identify weed growth, plant health issues, nutrient deficiencies, and other key factors that affect yield. However, manual analysis of these images can be time-consuming and error-prone, making automation necessary. This research proposes the use of vision transformer networks (ViTs) for efficient classification of crops and weeds from high-resolution UAV images. In contrast to traditional convolutional neural network approaches, ViTs have shown promising results on image classification tasks by directly processing global visual features. Our proposed method fine-tunes pretrained ViT models on our dataset, which includes labeled images of both crops and weeds collected via UAV. Experimental evaluation demonstrates significant improvements over state-of-the-art methods across multiple metrics including accuracy, precision, recall, and F1 score, showing that ViT-based systems are a powerful tool for accurate crop-weed discrimination in agricultural settings. Overall, our work provides valuable insights into the potential of emerging deep learning architectures for addressing critical challenges faced by modern agriculture while laying the groundwork for future advancements in remote sensing technology.",1
"Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence.   Established as a multi-disciplinary field in machine learning, both computer vision and natural language processing communities are working together to achieve state-of-the-art (SOTA) performance.   However, there is a gap between the SOTA results and real world applications.   This is due to the lack of model generalisation.   The RAMEN model \cite{Shrestha2019} aimed to achieve domain generalization by obtaining the highest score across two main types of VQA datasets.   This study provides two major improvements to the early/late fusion module and aggregation module of the RAMEN architecture, with the objective of further strengthening domain generalization.   Vector operations based fusion strategies are introduced for the fusion module and the transformer architecture is introduced for the aggregation module.   Improvements of up to five VQA datasets from the experiments conducted are evident.   Following the results, this study analyses the effects of both the improvements on the domain generalization problem.   The code is available on GitHub though the following link \url{https://github.com/bhanukaManesha/ramen}.",0
"In recent years, there has been significant progress in developing systems that can automatically generate natural language responses to questions based on visual content such as images. However, most of these approaches have focused primarily on training models on specific domains (e.g., celebrity recognition) rather than generalizing across multiple domains. This paper presents an improved model for RAdial MEmory Networks (RAMENT), which addresses the challenge of domain generalization by leveraging transfer learning techniques. Experimental results demonstrate the effectiveness of our approach in generating accurate answers for unseen visual domains compared to existing state-of-the-art methods. Our work advances the field of Visual Question Answering towards domain-general solutions that can perform well across diverse data distributions.",1
"Learning from image-text data has demonstrated recent success for many recognition tasks, yet is currently limited to visual features or individual visual concepts such as objects. In this paper, we propose one of the first methods that learn from image-sentence pairs to extract a graphical representation of localized objects and their relationships within an image, known as scene graph. To bridge the gap between images and texts, we leverage an off-the-shelf object detector to identify and localize object instances, match labels of detected regions to concepts parsed from captions, and thus create ""pseudo"" labels for learning scene graph. Further, we design a Transformer-based model to predict these ""pseudo"" labels via a masked token prediction task. Learning from only image-sentence pairs, our model achieves 30% relative gain over a latest method trained with human-annotated unlocalized scene graphs. Our model also shows strong results for weakly and fully supervised scene graph generation. In addition, we explore an open-vocabulary setting for detecting scene graphs, and present the first result for open-set scene graph generation. Our code is available at https://github.com/YiwuZhong/SGG_from_NLS.",0
"In recent years, natural language processing has made significant advances in generating images, videos, and text using deep learning techniques. However, understanding the relationships among objects and actions within a scene remains challenging due to limited supervised data available to train these models. This work proposes a method to learn scene graph representation by leveraging large amounts of unlabeled image and caption data. We first generate a diverse set of possible scene graphs using templates, then use reinforcement learning to optimize their quality based on the difference between predicted and ground truth scene graphs obtained through human annotation. Our proposed approach outperforms state-of-the-art methods for scene graph generation tasks while demonstrating promising results for zero-shot retrieval, VQA, and visual story generation. Our model learns to predict accurate relationship annotations that are validated by further human evaluation. Additionally, we showcase the versatility of our method through applications such as photo editing, question answering, and chatbots. Overall, this study paves the way for improving natural language processing systems by enabling better reasoning over structured representations of complex scenes.",1
"In recent years Deep Learning reached significant results in many practical problems, such as computer vision, natural language processing, speech recognition and many others. For many years the main goal of the research was to improve the quality of models, even if the complexity was impractically high. However, for the production solutions, which often require real-time work, the latency of the model plays a very important role. Current state-of-the-art architectures are found with neural architecture search (NAS) taking model complexity into account. However, designing of the search space suitable for specific hardware is still a challenging task. To address this problem we propose a measure of hardware efficiency of neural architecture search space - matrix efficiency measure (MEM); a search space comprising of hardware-efficient operations; a latency-aware scaling method; and ISyNet - a set of architectures designed to be fast on the specialized neural processing unit (NPU) hardware and accurate at the same time. We show the advantage of the designed architectures for the NPU devices on ImageNet and the generalization ability for the downstream classification and detection tasks.",0
"Artificial neural networks play a key role in many emerging artificial intelligence (AI) applications, such as image classification, speech recognition, natural language processing, and robotics control. However, their high computational complexity and memory requirements often limit their deployment on embedded systems with limited resources. In recent years, there has been growing interest in developing hardware accelerators for convolutional neural networks (CNNs), which can significantly reduce latency and energy consumption while achieving comparable accuracy. This paper presents ISyNet, a novel CNN architecture designed specifically for efficient implementation on an FPGA-based deep learning accelerator platform. Our approach takes advantage of both spiking neurons and fractal connections, improving area efficiency and reducing power consumption compared to state-of-the-art designs. We evaluate our proposal using several benchmark datasets and demonstrate that ISyNet achieves competitive performance while satisfying stringent area and power constraints required by modern AI devices. Overall, ISyNet paves the way towards realizing effective, low-power CNN inference engines on resource-constrained platforms.",1
"Success of deep neural networks in diverse tasks across domains of computer vision, speech recognition and natural language processing, has necessitated understanding the dynamics of training process and also working of trained models. Two independent contributions of this paper are 1) Novel activation function for faster training convergence 2) Systematic pruning of filters of models trained irrespective of activation function. We analyze the topological transformation of the space of training samples as it gets transformed by each successive layer during training, by changing the activation function. The impact of changing activation function on the convergence during training is reported for the task of binary classification. A novel activation function aimed at faster convergence for classification tasks is proposed. Here, Betti numbers are used to quantify topological complexity of data. Results of experiments on popular synthetic binary classification datasets with large Betti numbers(150) using MLPs are reported. Results show that the proposed activation function results in faster convergence requiring fewer epochs by a factor of 1.5 to 2, since Betti numbers reduce faster across layers with the proposed activation function. The proposed methodology was verified on benchmark image datasets: fashion MNIST, CIFAR-10 and cat-vs-dog images, using CNNs. Based on empirical results, we propose a novel method for pruning a trained model. The trained model was pruned by eliminating filters that transform data to a topological space with large Betti numbers. All filters with Betti numbers greater than 300 were removed from each layer without significant reduction in accuracy. This resulted in faster prediction time and reduced memory size of the model.",0
"In recent years, deep neural networks (DNNs) have achieved state-of-the-art results on numerous benchmark tasks across different domains such as image classification, natural language processing, etc. However, designing efficient DNN architectures that are both accurate and computationally efficient remains challenging. This paper proposes the use of topological framework for designing activation functions and pruning techniques that can improve the efficiency of DNN models without compromising their accuracy. Firstly, we introduce a novel approach for constructing activation functions by exploiting concepts from algebraic topology. Our proposed method involves computing persistence landscapes associated with ReLU layers of the network using persistent homology theory. We then train these customized activation functions together with the weight parameters using backpropagation. Secondly, we utilize a similar idea based on Betti numbers to determine which connections contribute significantly to model prediction and hence, we perform model pruning by discarding insignificant ones. Experimental evaluations demonstrate that our topological methods result in significant improvements in terms of model size, computational time, memory usage and even better performance compared to popular baseline models. These advantages make our proposed framework particularly appealing for real-world applications where resources are limited yet high precision is essential, such as edge devices and embedded systems. By leveraging the power of topology, we are able to create more effective and efficient neural models that can tackle complex tasks while meeting stringent hardware constraints. Therefore, this research provides valuable insights into the development of next generation machine learning algorithms.",1
"Data-to-text (D2T) generation in the biomedical domain is a promising - yet mostly unexplored - field of research. Here, we apply neural models for D2T generation to a real-world dataset consisting of package leaflets of European medicines. We show that fine-tuned transformers are able to generate realistic, multisentence text from data in the biomedical domain, yet have important limitations. We also release a new dataset (BioLeaflets) for benchmarking D2T generation models in the biomedical domain.",0
"In recent years, data-to-text (D2T) generation has become increasingly important due to its potential applications in a variety of fields such as medicine, finance, and customer service. However, despite significant advancements in D2T technology, generating high-quality biomedical text remains challenging due to the complexities inherent in medical language.  To address these challenges, we propose fine-tuning transformer models on large amounts of biomedical data for improved performance. We evaluate our approach using multiple metrics including BLEU score, ROUGE-L recall, self-BLEU score, and perplexity score. Our results show that fine-tuned transformer models outperform baseline methods across all evaluation metrics. Additionally, we perform qualitative analysis by manually evaluating the generated outputs against human-written reference texts and find that our proposed method produces more accurate and informative biomedical summaries.  Overall, our study demonstrates the effectiveness of utilizing fine-tuning techniques to enhance D2T generation in the field of biomedicine. With further development, D2T systems have the potential to revolutionize how biomedical research and patient care can be conducted, making them faster and more accessible than ever before.",1
"Text-to-image synthesis aims to generate a photo-realistic image from a given natural language description. Previous works have made significant progress with Generative Adversarial Networks (GANs). Nonetheless, it is still hard to generate intact objects or clear textures (Fig 1). To address this issue, we propose Feature-Aware Generative Adversarial Network (FA-GAN) to synthesize a high-quality image by integrating two techniques: a self-supervised discriminator and a feature-aware loss. First, we design a self-supervised discriminator with an auxiliary decoder so that the discriminator can extract better representation. Secondly, we introduce a feature-aware loss to provide the generator more direct supervision by employing the feature representation from the self-supervised discriminator. Experiments on the MS-COCO dataset show that our proposed method significantly advances the state-of-the-art FID score from 28.92 to 24.58.",0
"In recent years, Generative Adversarial Networks (GANs) have emerged as powerful tools for generating realistic images from text descriptions. However, these models often struggle with preserving important features specified in the input text while synthesizing new details that were absent in the original text description. To address this challenge, we propose a novel approach called FA-GAN: Feature-Aware GAN for Text to Image Synthesis. This method leverages attention mechanisms to selectively focus on specific features described in the text during image generation, resulting in more accurate and visually coherent output images. We demonstrate the effectiveness of our proposed framework through extensive experiments on several benchmark datasets and show consistent improvements over state-of-the-art methods in terms of visual fidelity and feature preservation. Our results highlight the potential of feature-aware generators in advancing the field of text-to-image synthesis and pave the way for further research into this exciting area of computer vision and natural language processing.",1
"Deep neural networks usually require large labeled datasets for training to achieve state-of-the-art performance in many tasks, such as image classification and natural language processing. Although a lot of data is created each day by active Internet users, most of these data are unlabeled and are vulnerable to data poisoning attacks. In this paper, we develop an efficient active learning method that requires fewer labeled instances and incorporates the technique of adversarial retraining in which additional labeled artificial data are generated without increasing the budget of the labeling. The generated adversarial examples also provide a way to measure the vulnerability of the model. To check the performance of the proposed method under an adversarial setting, i.e., malicious mislabeling and data poisoning attacks, we perform an extensive evaluation on the reduced CIFAR-10 dataset, which contains only two classes: airplane and frog. Our experimental results demonstrate that the proposed active learning method is efficient for defending against malicious mislabeling and data poisoning attacks. Specifically, whereas the baseline active learning method based on the random sampling strategy performs poorly (about 50%) under a malicious mislabeling attack, the proposed active learning method can achieve the desired accuracy of 89% using only one-third of the dataset on average.",0
"In this paper we study active learning under adversarial attacks, where the attacker has full knowledge of the system and can manipulate labels intentionally. We assume two types of attack scenarios: poisoning attacks where the malicious samples can interfere with machine learning models by providing incorrect answers on purpose; and mislabeling attacks that cause label noise by modifying original annotations. Our contributions contain three aspects. Firstly, we present a theoretical framework of analyzing active learning systems under such attacks based on Bayesian analysis which shows how different parameters in the model change due to these attacks and thus affects the generalization performance. Secondly, we discuss several strategies to defend against these attacks such as detecting outliers by statistical tests or thresholding methods or utilizing regularizations like dropout. Finally, we verify our findings through comprehensive experiments including both synthetic datasets and real-world applications, demonstrating the effectiveness of the proposed defense mechanisms even under strong attacks. Compared to existing work which mainly focuses on passive learning without considering the interaction between human annotators and the model, this paper presents new insights into designing robust active learning systems with solid theoretical guarantees and efficient algorithms.",1
"Vision Transformer (ViT) demonstrates that Transformer for natural language processing can be applied to computer vision tasks and result in comparable performance to convolutional neural networks (CNN), which have been studied and adopted in computer vision for years. This naturally raises the question of how the performance of ViT can be advanced with design techniques of CNN. To this end, we propose to incorporate two techniques and present ViT-ResNAS, an efficient multi-stage ViT architecture designed with neural architecture search (NAS). First, we propose residual spatial reduction to decrease sequence lengths for deeper layers and utilize a multi-stage architecture. When reducing lengths, we add skip connections to improve performance and stabilize training deeper networks. Second, we propose weight-sharing NAS with multi-architectural sampling. We enlarge a network and utilize its sub-networks to define a search space. A super-network covering all sub-networks is then trained for fast evaluation of their performance. To efficiently train the super-network, we propose to sample and train multiple sub-networks with one forward-backward pass. After that, evolutionary search is performed to discover high-performance network architectures. Experiments on ImageNet demonstrate that ViT-ResNAS achieves better accuracy-MACs and accuracy-throughput trade-offs than the original DeiT and other strong baselines of ViT. Code is available at https://github.com/yilunliao/vit-search.",0
"Incorporating external knowledge into transformer models has recently become an active area of research due to their success on natural language processing (NLP) tasks. However, there is still limited work in integrating external knowledge into vision transformer networks, especially at scale. This paper presents an approach that improves the efficiency of multi-stage transformers by leveraging external knowledge from large-scale datasets such as ImageNet during training. The proposed method enables more accurate predictions while requiring less computational resources compared to state-of-the-art alternatives. We conduct experiments across multiple benchmarks including object detection, semantic segmentation, and instance segmentation demonstrating the effectiveness of our approach in each task. Our findings suggest that incorporating external knowledge can significantly enhance multi-stage transformers without sacrificing accuracy, making them well suited for resource constrained environments. Overall, this research opens up new possibilities for developing efficient vision models capable of handling complex real-world problems.",1
"An agent that can understand natural-language instruction and carry out corresponding actions in the visual world is one of the long-term challenges of Artificial Intelligent (AI). Due to multifarious instructions from humans, it requires the agent can link natural language to vision and action in unstructured, previously unseen environments. If the instruction given by human is a navigation task, this challenge is called Visual-and-Language Navigation (VLN). It is a booming multi-disciplinary field of increasing importance and with extraordinary practicality. Instead of focusing on the details of specific methods, this paper provides a comprehensive survey on VLN tasks and makes a classification carefully according the different characteristics of language instructions in these tasks. According to when the instructions are given, the tasks can be divided into single-turn and multi-turn. For single-turn tasks, we further divided them into goal-orientation and route-orientation based on whether the instructions contain a route. For multi-turn tasks, we divided them into imperative task and interactive task based on whether the agent responses to the instructions. This taxonomy enable researchers to better grasp the key point of a specific task and identify directions for future research.",0
"In recent years we have seen a shift towards data intensive approaches in computer vision that utilize large amounts of annotated training data, powerful deep learning models, and high performance GPUs. As these systems become more prevalent they must also be able to function in environments that can not rely on constant connectivity, massive storage resources, nor specialized hardware such as graphic processing units (GPU). Our work explores techniques for enabling navigation using visual input alone, without GPS and without any other external sensors than those commonly found on consumer mobile devices. We survey both traditional image based approaches such as object recognition but also newer paradigms such as scene understanding which combine elements from prior knowledge graphs, landmark detection, and semantic segmentation. We then propose a taxonomy that breaks down the state of art into three main axes: data driven vs model free methods, single view vs multiple views, static representations vs dynamic representations. This allows us to identify several common problems among the different research areas, and lay out future directions for research and real world applications. Finally we present an open source implementation of our framework which uses Google’s MobileNet SSD detector along side the LSTM networks of Google’s SLAM cell as well as Openstreetmap maps as part of a realtime multi threaded application that runs at over 27 frames per second. By combining cutting edge machine learning algorithms with classical Computer Science concepts we hope to enable new forms of interaction across all aspects of human life including entertainment, education, healthcare, retail, personal finance management, urban planning transportation and more",1
"Deep learning approaches have produced substantial breakthroughs in fields such as image classification and natural language processing and are making rapid inroads in the area of protein design. Many generative models of proteins have been developed that encompass all known protein sequences, model specific protein families, or extrapolate the dynamics of individual proteins. Those generative models can learn protein representations that are often more informative of protein structure and function than hand-engineered features. Furthermore, they can be used to quickly propose millions of novel proteins that resemble the native counterparts in terms of expression level, stability, or other attributes. The protein design process can further be guided by discriminative oracles to select candidates with the highest probability of having the desired properties. In this review, we discuss five classes of generative models that have been most successful at modeling proteins and provide a framework for model guided protein design.",0
"This paper presents a novel approach to protein design using deep generative models (DGMs). With advances in computational power and machine learning algorithms, modeling complex biological systems has become increasingly feasible. DGMs have recently emerged as powerful tools for generating new molecules with desired properties by optimizing parameters through training on large datasets. In this study, we demonstrate how to apply DGM techniques to optimize protein sequence, structure, and function, addressing some limitations of traditional methods such as homology modeling and experimental screenings. We provide examples where our method outperforms previous approaches and discuss potential applications in drug discovery, enzyme engineering, and materials science. By harnessing the power of deep learning, we can accelerate progress towards developing better therapies and creating functional nanomaterials from scratch.",1
"Adversarial training has been proven to be a powerful regularization method to improve the generalization of models. However, current adversarial training methods only attack the original input sample or the embedding vectors, and their attacks lack coverage and diversity. To further enhance the breadth and depth of attack, we propose a novel masked weight adversarial training method called DropAttack, which enhances generalization of model by adding intentionally worst-case adversarial perturbations to both the input and hidden layers in different dimensions and minimize the adversarial risks generated by each layer. DropAttack is a general technique and can be adopt to a wide variety of neural networks with different architectures. To validate the effectiveness of the proposed method, we used five public datasets in the fields of natural language processing (NLP) and computer vision (CV) for experimental evaluating. We compare the proposed method with other adversarial training methods and regularization methods, and our method achieves state-of-the-art on all datasets. In addition, Dropattack can achieve the same performance when it use only a half training data compared to other standard training method. Theoretical analysis reveals that DropAttack can perform gradient regularization at random on some of the input and wight parameters of the model. Further visualization experiments show that DropAttack can push the minimum risk of the model to a lower and flatter loss landscapes. Our source code is publicly available on https://github.com/nishiwen1214/DropAttack.",0
"DropAttack: A New Technique To Increase Accuracy Of Machine Learning Models by Imitating Human Thought Processes  Dropout was introduced as a regularization method that has been proven effective at improving the generalization performance of deep neural networks (DNN). However, there remains considerable room for improvement in terms of accuracy since dropout only removes random neurons from hidden layers during training. On the other hand, adversarial attacks have shown promise for evaluating how robust models can detect changes caused by inputs injected in their latent space, however such techniques require significant computational resources.  In order to tackle these issues we propose a new technique called ""DropAttack"" which combines aspects from both data augmentation and model perturbations using adversarial examples. Our approach consists of generating adversarial noise samples from input images by applying spatial transformations. By applying random translations to the original image and training DNNs with those transformed samples our model learns to adapt to different scenarios and increases its capacity of understanding the underlying task. For every epoch, the translation parameters are reinitialized so that the drop attack remains efficient. Moreover, we demonstrate experimentally that the proposed method leads to improved test set accuracy over other state-of-the-art methods like Fast.AI or Hugging Face transformers.  Overall, our results show that integrating human thought processes into machine learning algorithms through the generation of adversarial examples via data augmentation significantly improve the performances of neural network models. The proposed methodology provides insights into how humans might process information in the brain while demonstrating its effectiveness in achieving better accuracy on real world tasks.  Significance and Impact:",1
"Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2Lens, to visualize and explain multimodal models for sentiment analysis. M2Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",0
"Title: M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis  M2Lens is a comprehensive framework that integrates state-of-the-art techniques from computer vision (CV) and natural language processing (NLP), offering novel solutions that overcome limitations posed by single modal models and their pretraining on limited datasets. By designing specialized modules tailored for each modality, we enable cross-modality attention fusion that adapts to different tasks such as text-only sentiment analysis, image+text fusion, audio+video sentiment analysis, etc., yielding superior performance compared to single modality methods. This work proposes two innovative approaches to model interpretability: 1) visualization through feature maps highlighting key regions across CV/NL components, and 2) explanation via counterfactual reasoning. For instance, these techniques can identify which part(s) of an image or review contribute most to classification outcomes. Our extensive experiments verify that our methods significantly improve over baseline results using public benchmark datasets. These contributions address major challenges faced in multimodal sentiment analysis and open up new opportunities for related research areas like explainable AI, human-AI interaction, affective computing, multimedia analytics, social media intelligence, virtual reality, and recommendation systems.",1
"Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering-based DNN model compression. DKM casts k-means clustering as an attention problem and enables joint optimization of the parameters and clustering centroids. Unlike prior works that rely on additional regularizers and parameters, DKM-based compression keeps the original loss function and model architecture fixed. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results demonstrate that DMK delivers superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB model size (29.4x model compression factor). For MobileNet-v1, which is a challenging DNN to compress, DKM delivers 62.8% top-1 ImageNet1k accuracy with 0.74 MB model size (22.4x model compression factor). This result is 6.8% higher top-1 accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Additionally, DKM enables compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks.",0
"In recent years, deep neural networks have proven to be highly effective at solving complex tasks such as image classification, natural language processing, and speech recognition. However, these models often require large amounts of data and computational resources to train, making them difficult to deploy on resource-constrained devices like smartphones or embedded systems. To address this challenge, researchers have proposed several methods for compressing neural network models without sacrificing their performance significantly. One promising approach is to use clustering algorithms to group similar weights together, reducing the size of the model while preserving its functionality.  In this work, we propose the ""DKM"" layer, which combines differential programming and k-means clustering for deep neural network compression. By leveraging the flexibility of automatic differentiation and the efficiency of k-means, our method effectively groups similar weights into clusters that can be represented using shared parameters. Our experiments demonstrate that the DKM layer improves both accuracy and speed compared to existing compression techniques. Moreover, we showcase the utility of our method by successfully compressing state-of-the-art vision models for object detection and image segmentation.  Overall, the DKM layer represents an important step towards building efficient and scalable deep learning models capable of running on resource-limited devices, bringing advanced machine learning capabilities to a wider range of applications.",1
"Spatio-temporal representational learning has been widely adopted in various fields such as action recognition, video object segmentation, and action anticipation. Previous spatio-temporal representational learning approaches primarily employ ConvNets or sequential models,e.g., LSTM, to learn the intra-frame and inter-frame features. Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classification, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract fine-grained features from a tiny patch. To tackle the training difficulty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efficient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51. Code and trained models will be released.",0
"Spatial chunk transformer (SCT) has been recently proposed as an alternative to ConvNet architectures. We introduce Shifted Chunk Transformers that augment SCTs by using shifting operations prior to self attention mechanisms within each locality region or “chunk”. Our Shifted Chunk Transformers achieve state-of-the-art results on two large scale video recognition tasks UCF-101 and HMDB-51 while being much faster than both 2D ResNets at inference time. This work shows that our shifted operation can allow us to break through previously known tradeoffs between accuracy and efficiency without sacrificing much accuracy. Additionally, we present an extensive ablation study showing how different design choices affect performance.",1
"We introduce MADGRAD, a novel optimization method in the family of AdaGrad adaptive gradient methods. MADGRAD shows excellent performance on deep learning optimization problems from multiple fields, including classification and image-to-image tasks in vision, and recurrent and bidirectionally-masked models in natural language processing. For each of these tasks, MADGRAD matches or outperforms both SGD and ADAM in test set performance, even on problems for which adaptive methods normally perform poorly.",0
"This paper presents a novel algorithm for stochastic optimization that combines momentum, adaptiveness, dual averaging, and gradient descent into one method. Unlike existing methods that either sacrifice adaptivity for efficiency or vice versa, our approach provides a balance between these two desirable properties while still achieving faster convergence rates compared to traditional SGD algorithms. Our momentumized, adaptive, dual averaged gradient (MADA) method uses weight updates that incorporate both past and current gradients, allowing for better tracking of nonstationary environments. Additionally, our method dynamically adjusts learning rates based on recent performance, further improving adaptability. Comprehensive experiments demonstrate that MADA outperforms state-of-the-art stochastic optimizers across a range of machine learning tasks, including logistic regression, neural networks, and deep reinforcement learning. Overall, we propose a new standard for efficient and effective optimization in the era of big data and online learning.",1
"Transformers have shown impressive performance in various natural language processing and computer vision tasks, due to the capability of modeling long-range dependencies. Recent progress has demonstrated to combine such transformers with CNN-based semantic image segmentation models is very promising. However, it is not well studied yet on how well a pure transformer based approach can achieve for image segmentation. In this work, we explore a novel framework for semantic image segmentation, which is encoder-decoder based Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid Group Transformer (PGT) as the encoder for progressively learning hierarchical features, while reducing the computation complexity of the standard visual transformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse semantic-level and spatial-level information from multiple levels of the PGT encoder for semantic image segmentation. Surprisingly, this simple baseline can achieve new state-of-the-art results on multiple challenging semantic segmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The source code will be released upon the publication of this work.",0
"Recent advances in deep learning have shown great potential in the field of semantic image segmentation. However, fully convolutional networks (FCNs), which have been widely used in recent years for this task, suffer from limited representation capabilities due to their strong spatial locality. To overcome this limitation, we propose a new architecture based on fully transformer networks (FTN) for dense pixel-wise prediction tasks such as semantic image segmentation. Our method significantly improves over state-of-the-art FCN baselines by introducing attention mechanisms that enable global reasoning across the entire input image. We showcase our approach’s effectiveness through extensive experimental evaluations on popular benchmark datasets, achieving better results than existing methods.",1
"Web Image Context Extraction (WICE) consists in obtaining the textual information describing an image using the content of the surrounding webpage. A common preprocessing step before performing WICE is to render the content of the webpage. When done at a large scale (e.g., for search engine indexation), it may become very computationally costly (up to several seconds per page). To avoid this cost, we introduce a novel WICE approach that combines Graph Neural Networks (GNNs) and Natural Language Processing models. Our method relies on a graph model containing both node types and text as features. The model is fed through several blocks of GNNs to extract the textual context. Since no labeled WICE dataset with ground truth exists, we train and evaluate the GNNs on a proxy task that consists in finding the semantically closest text to the image caption. We then interpret importance weights to find the most relevant text nodes and define them as the image context. Thanks to GNNs, our model is able to encode both structural and semantic information from the webpage. We show that our approach gives promising results to help address the large-scale WICE problem using only HTML data.",0
This is my previous work in text generation using GPT-4 from OpenAI. I want you to rewrite that summary into a longer format like an academic abstract but still following the same rules. Please add in additional context and specific details as well.,1
"There is an increasing demand for scalable algorithms capable of clustering and analyzing large time series datasets. The Kohonen self-organizing map (SOM) is a type of unsupervised artificial neural network for visualizing and clustering complex data, reducing the dimensionality of data, and selecting influential features. Like all clustering methods, the SOM requires a measure of similarity between input data (in this work time series). Dynamic time warping (DTW) is one such measure, and a top performer given that it accommodates the distortions when aligning time series. Despite its use in clustering, DTW is limited in practice because it is quadratic in runtime complexity with the length of the time series data. To address this, we present a new DTW-based clustering method, called SOMTimeS (a Self-Organizing Map for TIME Series), that scales better and runs faster than other DTW-based clustering algorithms, and has similar performance accuracy. The computational performance of SOMTimeS stems from its ability to prune unnecessary DTW computations during the SOM's training phase. We also implemented a similar pruning strategy for K-means for comparison with one of the top performing clustering algorithms. We evaluated the pruning effectiveness, accuracy, execution time and scalability on 112 benchmark time series datasets from the University of California, Riverside classification archive. We showed that for similar accuracy, the speed-up achieved for SOMTimeS and K-means was 1.8x on average; however, rates varied between 1x and 18x depending on the dataset. SOMTimeS and K-means pruned 43% and 50% of the total DTW computations, respectively. We applied SOMtimeS to natural language conversation data collected as part of a large healthcare cohort study of patient-clinician serious illness conversations to demonstrate the algorithm's utility with complex, temporally sequenced phenomena.",0
SOMTimeS: Self Organizing Maps for Time Series Clustering and its Application to Serious Illness Conversations discusses how serious illness conversations can lead to improved quality of life for patients facing terminal illness.,1
"Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex human instructions. We propose Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of visual observations and actions. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits.",0
"This work presents an innovative episodic transformer model for vision-and-language navigation tasks that integrates external knowledge into its reasoning process. By leveraging a pretrained transformer model and fine-tuning on task-specific objectives, our approach achieves state-of-the-art performance across multiple benchmarks. We further demonstrate how incorporating extrinsic information through data augmentation techniques improves overall results and generalizes better to out-of-domain environments. Additionally, we conduct ablation studies to analyze the effectiveness of each component in our framework. Our contributions provide insight into the potential benefits of using pretrained models for downstream natural language processing applications, as well as advancing the field of computational cognitive science by developing more humanlike agents capable of effective multimodal interaction.",1
"We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the given natural language description. To solve RIS efficiently, we need to understand each word's relationship with other words, each region in the image to other regions, and cross-modal alignment between linguistic and visual domains. We argue that one of the limiting factors in the recent methods is that they do not handle these interactions simultaneously. To this end, we propose a novel architecture called JRNet, which uses a Joint Reasoning Module(JRM) to concurrently capture the inter-modal and intra-modal interactions. The output of JRM is passed through a novel Cross-Modal Multi-Level Fusion (CMMLF) module which further refines the segmentation masks by exchanging contextual information across visual hierarchy through linguistic features acting as a bridge. We present thorough ablation studies and validate our approach's performance on four benchmark datasets, showing considerable performance gains over the existing state-of-the-art methods.",0
"In this article, we present a novel method for comprehensive multi-modal interactions for referring image segmentation using natural language input from multiple modalities. Our approach incorporates vision and natural language processing techniques such as object detection, region proposal generation, neural networks, and sequence models to achieve accurate and interactive image segmentation. We demonstrate the effectiveness of our method on challenging benchmark datasets and show that it significantly outperforms state-of-the-art methods. Additionally, we analyze and discuss the results to gain insights into how multi-modal interactions can improve image segmentation performance. Overall, our work represents a significant advancement in computer vision research and has important implications for real-world applications where precise segmentation is critical. In this paper, we propose a new method for referring image segmentation based on multi-modal interactions. This involves using natural language inputs obtained through multiple channels such as speech and text messages. Our method combines the power of computer vision and natural language processing techniques to achieve high levels of accuracy in image segmentation. Our approach leverages technologies like object detection, region proposal generation, neural networks, and sequence models to enable effective communication between users and computers. By utilizing advanced algorithms and models, we ensure smooth interaction without interference from other sources. We evaluated our method by conducting experiments on standard benchmark datasets and compared our results with those obtained from existing approaches. Our findings reveal that our system outperforms current state-of-the-art methods by a significant margin. Furthermore, we analyzed the data collected during testing to better understand the impact of multi-modal interactions on the quality of image segmentation. These results have important implications for real-world scenarios that require precise image segmentation.",1
"Activation functions play a pivotal role in determining the training dynamics and neural network performance. The widely adopted activation function ReLU despite being simple and effective has few disadvantages including the Dying ReLU problem. In order to tackle such problems, we propose a novel activation function called Serf which is self-regularized and nonmonotonic in nature. Like Mish, Serf also belongs to the Swish family of functions. Based on several experiments on computer vision (image classification and object detection) and natural language processing (machine translation, sentiment classification and multimodal entailment) tasks with different state-of-the-art architectures, it is observed that Serf vastly outperforms ReLU (baseline) and other activation functions including both Swish and Mish, with a markedly bigger margin on deeper architectures. Ablation studies further demonstrate that Serf based architectures perform better than those of Swish and Mish in varying scenarios, validating the effectiveness and compatibility of Serf with varying depth, complexity, optimizers, learning rates, batch sizes, initializers and dropout rates. Finally, we investigate the mathematical relation between Swish and Serf, thereby showing the impact of preconditioner function ingrained in the first derivative of Serf which provides a regularization effect making gradients smoother and optimization faster.",0
"Title: A New Perspective on Training Deep Neural Networks In recent years, there has been significant progress in developing more effective error functions that can improve the performance of deep neural networks (DNNs). Among these error functions, SoftPlus has become popular due to its ability to handle both positive and negative inputs without exploding gradients. However, the implementation of SoftPlus can be computationally expensive and may introduce numerical instability during backpropagation. In this paper, we propose a new activation function called Log-SoftPlus Error (SERF) that addresses these issues while preserving the benefits of SoftPlus. Our proposed activation function combines the advantages of SoftPlus with those of Swish and Tanh functions, resulting in improved gradient flow and faster convergence speed compared to existing alternatives. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets. Overall, our findings suggest that SERF provides a promising alternative for training DNNs, particularly in resource-constrained environments where computational efficiency is crucial.",1
"Most existing image retrieval systems use text queries as a way for the user to express what they are looking for. However, fine-grained image retrieval often requires the ability to also express where in the image the content they are looking for is. The text modality can only cumbersomely express such localization preferences, whereas pointing is a more natural fit. In this paper, we propose an image retrieval setup with a new form of multimodal queries, where the user simultaneously uses both spoken natural language (the what) and mouse traces over an empty canvas (the where) to express the characteristics of the desired target image. We then describe simple modifications to an existing image retrieval model, enabling it to operate in this setup. Qualitative and quantitative experiments show that our model effectively takes this spatial guidance into account, and provides significantly more accurate retrieval results compared to text-only equivalent systems.",0
"Title: Telling the What while Pointing to the Where: Multimodal Queries for Image Retrieval Abstract: This paper presents a new approach to image retrieval that combines natural language queries with visual gesture pointing. The proposed method uses deep learning techniques to fuse textual and visual information into a single vector representation, which can then be used to retrieve relevant images from a large database. Experimental results show significant improvements over traditional text-based methods and demonstrate the potential of multimodal approaches in image retrieval tasks. The paper concludes by discussing future research directions and possible applications of the proposed method in areas such as search engines, e-commerce, and human-computer interaction.",1
"Facial action unit (FAU) intensities are popular descriptors for the analysis of facial behavior. However, FAUs are sparsely represented when only a few are activated at a time. In this study, we explore the possibility of representing the dynamics of facial expressions by adopting algorithms used for word representation in natural language processing. Specifically, we perform clustering on a large dataset of temporal facial expressions with 5.3M frames before applying the Global Vector representation (GloVe) algorithm to learn the embeddings of the facial clusters. We evaluate the usefulness of our learned representations on two downstream tasks: schizophrenia symptom estimation and depression severity regression. These experimental results show the potential effectiveness of our approach for improving the assessment of mental health symptoms over baseline models that use FAU intensities alone.",0
"This paper presents a novel approach to modeling facial behavior dynamics for mental health assessment. We propose using machine learning algorithms to analyze subtle changes in facial movements during natural conversations as indicators of cognitive and emotional states. Our methodology involves collecting large datasets of human subjects engaging in spontaneous social interactions while their facial expressions and vocal cues are recorded. These data are then preprocessed to extract relevant features that capture temporal variations in muscle activation, head gestures, and speech prosody. Using these features, we train models that can classify different levels of stress, anxiety, depression, and other psychological disorders. By comparing our results against standard clinical evaluations, we show promising correlations between facial behaviors and mental health outcomes. Furthermore, we discuss potential applications of our system in personalized therapy, remote monitoring, and self-assessment tools. Overall, our work represents a significant step towards developing non-invasive technologies capable of inferring mental states from sensorimotor signals.",1
"Infographics are documents designed to effectively communicate information using a combination of textual, graphical and visual elements. In this work, we explore the automatic understanding of infographic images by using Visual Question Answering technique.To this end, we present InfographicVQA, a new dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills. Finally, we evaluate two strong baselines based on state of the art multi-modal VQA models, and establish baseline performance for the new task. The dataset, code and leaderboard will be made available at http://docvqa.org",0
"Title: Visual Question Answering using Pre-trained Neural Networks  This study investigates the use of pre-trained neural networks for visual question answering (InfographicVQA) tasks. We propose a novel approach that utilizes transfer learning techniques and fine-tuning strategies to improve performance on benchmark datasets. Our method involves training multiple models based on different architectures and evaluation metrics, followed by selection of the most effective model using cross validation. Results show significant improvements over baseline methods, demonstrating the potential utility of pre-training for VQA tasks. We discuss limitations and future directions for research in this area. Overall, our work provides valuable insights into how pre-trained models can enhance performance in complex vision tasks.",1
"Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at \url{https://github.com/zczcwh/PoseFormer}",0
"Deep learning methods have achieved state-of-the-art results on 2D human pose estimation tasks by leveraging convolutional neural networks (CNNs) that learn to extract features from images. However, extending these models to estimate 3D human pose remains challenging due to occlusions and depth ambiguities present in RGB images. In this work, we propose SpatioTemporal Transformer (STT), which explicitly models the spatial relationships within a single frame and captures temporal dependencies across video frames. Our STT model estimates both 2D keypoint heatmaps and predicts a camera view transformation vector field between two consecutive frames, enabling us to infer accurate 3D poses without relying solely on image intensities. We demonstrate our method's superior performance compared to existing state-of-the-art approaches using standard benchmark datasets such as Human3.6M and IJB-A. Additionally, qualitative evaluations indicate more accurate estimations of joint locations, particularly in cases where the subject is partially occluded or in close proximity to others. Overall, our contributions enable the use of deep learning techniques to achieve highly reliable and detailed predictions for 3D human poses from monocular videos.",1
"The concern regarding users' data privacy has risen to its highest level due to the massive increase in communication platforms, social networking sites, and greater users' participation in online public discourse. An increasing number of people exchange private information via emails, text messages, and social media without being aware of the risks and implications. Researchers in the field of Natural Language Processing (NLP) have concentrated on creating tools and strategies to identify, categorize, and sanitize private information in text data since a substantial amount of data is exchanged in textual form. However, most of the detection methods solely rely on the existence of pre-identified keywords in the text and disregard the inference of the underlying meaning of the utterance in a specific context. Hence, in some situations, these tools and algorithms fail to detect disclosure, or the produced results are miss-classified. In this paper, we propose a multi-input, multi-output hybrid neural network which utilizes transfer-learning, linguistics, and metadata to learn the hidden patterns. Our goal is to better classify disclosure/non-disclosure content in terms of the context of situation. We trained and evaluated our model on a human-annotated ground truth dataset, containing a total of 5,400 tweets. The results show that the proposed model was able to identify privacy disclosure through tweets with an accuracy of 77.4% while classifying the information type of those tweets with an impressive accuracy of 99%, by jointly learning for two separate tasks.",0
"This paper presents a multi-input multi-output transformer-based hybrid neural network (MIMOTHN) architecture designed specifically for multi-class privacy disclosure detection tasks in natural language processing applications. Using input sources such as contextual text data along with audio signals allows MIMOTHN to capture both semantic and prosodic cues in order to make accurate predictions on whether sensitive information has been revealed. Our experimental results demonstrate that our proposed model achieves state-of-the-art performance compared to other baseline models. Furthermore, ablation studies show the importance of incorporating both audio and text inputs into our system. Overall, MIMOTHN provides an effective solution for detecting privacy violations in speech-to-text conversion systems.",1
"Describing images using natural language is widely known as image captioning, which has made consistent progress due to the development of computer vision and natural language generation techniques. Though conventional captioning models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and SPICE, the ability of captions to distinguish the target image from other similar images is under-explored. To generate distinctive captions, a few pioneers employ contrastive learning or re-weighted the ground-truth captions, which focuses on one single input image. However, the relationships between objects in a similar image group (e.g., items or properties within the same album or fine-grained events) are neglected. In this paper, we improve the distinctiveness of image captions using a Group-based Distinctive Captioning Model (GdisCap), which compares each image with other images in one similar group and highlights the uniqueness of each image. In particular, we propose a group-based memory attention (GMA) module, which stores object features that are unique among the image group (i.e., with low similarity to objects in other images). These unique object features are highlighted when generating captions, resulting in more distinctive captions. Furthermore, the distinctive words in the ground-truth captions are selected to supervise the language decoder and GMA. Finally, we propose a new evaluation metric, distinctive word rate (DisWordRate) to measure the distinctiveness of captions. Quantitative results indicate that the proposed method significantly improves the distinctiveness of several baseline models, and achieves the state-of-the-art performance on both accuracy and distinctiveness. Results of a user study agree with the quantitative evaluation and demonstrate the rationality of the new metric DisWordRate.",0
"In recent years, image captioning has become an increasingly important area of research in computer vision and natural language processing. One challenge that arises in image captioning is how to generate descriptive and distinctive captions that accurately capture the content of the image while also making them unique from other captions that may describe similar images. To address this issue, we propose a novel approach called group-based distinctive image captioning (GDIC).  Our method uses memory attention mechanisms to store and retrieve relevant visual features and semantic concepts associated with each image in our dataset. We then introduce two types of groups: image clusters based on their visual similarity, and word clusters based on their semantic meaning within a fixed vocabulary. For each caption generation step, we attend over different groups simultaneously using a shared attention mechanism, which allows us to leverage both visual and semantic cues effectively. By doing so, we can better differentiate among visually similar images and ensure that generated captions are more diverse and distinctive compared to previous methods.  We evaluate our GDIC model on several public benchmark datasets and demonstrate state-of-the-art results across all metrics. Our ablation studies further validate the effectiveness of our proposed design choices, including the use of multiple groups for attending over related data entities and regularization techniques such as adversarial training to encourage diversity among generated captions. Overall, our work presents a significant contribution towards improving the quality and uniqueness of automatic image captions.",1
"Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB, a large-scale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB pretrain our Airbert model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.",0
"This paper presents Airbert, a pretraining method designed to improve zero-shot performance on vision-and-language navigation tasks by transferring knowledge from existing image captioning datasets. By leveraging large amounts of data and distilling key features such as object detection, attention mechanisms, and natural language generation, we demonstrate that our model outperforms several state-of-the-art models across multiple evaluation metrics. Furthermore, we showcase how Airbert can effectively adapt to new domains without any additional training, thereby highlighting its generalization capabilities. Overall, these results provide evidence of the effectiveness and potential real-world applications of our approach in the field of artificial intelligence and computer vision.",1
"Neural networks are well-known to be vulnerable to imperceptible perturbations in the input, called adversarial examples, that result in misclassification. Generating adversarial examples for source code poses an additional challenge compared to the domains of images and natural language, because source code perturbations must retain the functional meaning of the code. We identify a striking relationship between token frequency statistics and learned token embeddings: the L2 norm of learned token embeddings increases with the frequency of the token except for the highest-frequnecy tokens. We leverage this relationship to construct a simple and efficient gradient-free method for generating state-of-the-art adversarial examples on models of code. Our method empirically outperforms competing gradient-based methods with less information and less computational effort.",0
"Abstract: This work introduces a novel approach to generating adversarial examples against models trained on code. Unlike traditional methods that rely on gradients to guide the attack, our method uses a simple optimization procedure that requires no gradient information. Our attacks can generate meaningful adversaries using only black box access to the model, making them both efficient and effective. We evaluate our approach across several benchmarks and show that we achieve state-of-the-art results while significantly outperforming existing gradient-based techniques. Additionally, we demonstrate the robustness of our generated adversaries by applying them to real-world applications such as source code plagiarism detection. Our work highlights the need for improved defense mechanisms against these types of attacks and suggests new research directions in the field of program synthesis and vulnerability analysis. Keywords: Adversarial examples, Program synthesis, Source code plagiarism detection",1
"Recent deep-learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. This chapter covers recent work aiming to interpret models by attributing importance to features and feature groups for a single prediction. Importantly, the proposed attributions assign importance to interactions between features, in addition to features in isolation. These attributions are shown to yield insights across real-world domains, including bio-imaging, cosmology image and natural-language processing. We then show how these attributions can be used to directly improve the generalization of a neural network or to distill it into a simple model. Throughout the chapter, we emphasize the use of reality checks to scrutinize the proposed interpretation techniques.",0
"Reality checking is becoming increasingly important as machine learning methods such as deep learning continue to gain popularity and are applied to more complex real-world problems. This paper presents several techniques that can improve the accuracy and interpretability of deep-learning models by incorporating knowledge from external sources into their training process. By grounding these models in the physical world, we can increase their robustness and generalization performance while also making them easier to explain and debug. Our approach uses simple, interpretable visual features and geometric constraints derived from human demonstrations, images, videos, or even textual descriptions to guide the model towards semantically meaningful solutions. We show through experimental evaluations on several benchmark datasets that our method leads to significant improvements over state-of-the-art results across different domains including computer vision, natural language processing, robotics, and control systems. These findings highlight the importance of integrating domain expertise into neural network architectures and demonstrate how reality checks can serve as a powerful tool for enhancing their capabilities while addressing fundamental challenges related to explainability, transfer learning, active perception, interactive decision making under uncertainty, and safe exploration. Overall, this work represents a step towards building more reliable and trustworthy artificial intelligence applications that can operate effectively in dynamic environments beyond controlled laboratory settings.  Keywords: deep learning; interpretable; reality check; geometry; physical constraint; semantic feature; cross-domain application; uncertainty management; safety guarantee; real-time optimization",1
"Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER, which learns joint embeddings of images and text, and GPT-2, a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL.",0
"In recent years, there has been significant progress in developing artificial intelligence (AI) systems that can perform complex tasks such as image classification, object detection, and natural language understanding. However, despite these advances, many of these systems still lack the ability to provide clear and concise explanations for their decision making processes, which can make them difficult to interpret and trust by human users. To address this issue, we present e-ViL, a new dataset and benchmark designed specifically for evaluating natural language explanations in vision-language tasks.  The e-ViL dataset consists of pairs of images and associated questions posed by humans, along with detailed explanations provided by annotators to explain why certain answers are correct or incorrect. These explanations cover a wide range of topics, including object recognition, scene understanding, and reasoning. By providing both question-answer pairs and corresponding explanations, the e-ViL dataset allows researchers to evaluate the quality of generated explanations relative to human standards. Additionally, the dataset includes evaluation metrics for measuring the accuracy and coherence of generated explanations.  We also propose several baseline models for generating natural language explanations using techniques from text generation and explanation selection. We evaluate these models on our dataset and show that while they can generate plausible explanations, they fall short of human performance on multiple aspects of explanation quality. Finally, we discuss future directions and potential applications of the e-ViL dataset, including improving transparency and accountability in AI systems and enabling better collaboration between humans and machines. Overall, the e-ViL dataset and benchmark represent a step forward towards building more transparent and interpretable AI systems for the benefit of society.",1
"We propose UniT, a Unified Transformer model to simultaneously learn the most prominent tasks across different domains, ranging from object detection to natural language understanding and multimodal reasoning. Based on the transformer encoder-decoder architecture, our UniT model encodes each input modality with an encoder and makes predictions on each task with a shared decoder over the encoded input representations, followed by task-specific output heads. The entire model is jointly trained end-to-end with losses from each task. Compared to previous efforts on multi-task learning with transformers, we share the same model parameters across all tasks instead of separately fine-tuning task-specific models and handle a much higher variety of tasks across different domains. In our experiments, we learn 7 tasks jointly over 8 datasets, achieving strong performance on each task with significantly fewer parameters. Our code is available in MMF at https://mmf.sh.",0
"This paper describes two novel contributions to deep learning and computer vision: UniT (Unified Task), which can learn multimodal tasks using a single model trained on multiple modalities (such as image and text data) in parallel, without needing separate models; and UniT multitask learning, which trains one UniT model on many different task types simultaneously, dramatically reducing training time and improving generalization performance. Both approaches use the popular Transformer architecture but replace self attention modules with more efficient variants tailored for each modality type. Experiments show that these methods achieve state-of-the-art results across several challenging datasets and task types, making them valuable tools for practitioners working in computer vision and beyond. By enabling fast and accurate simultaneous processing of diverse data sources, UniT represents a significant step toward realizing fully integrated artificial intelligence systems that can adapt flexibly to complex environments and user needs. We hope future research along similar lines will enable even greater advances by building upon and expanding our work herein.",1
"For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. We address this challenge by introducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geometry, material information, and lighting information for every scene; (3) includes dense per-pixel semantic instance segmentations and complete camera information for every image; and (4) factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects.   We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, computation time, and annotation effort. Remarkably, we find that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natural language processing model. We also evaluate sim-to-real transfer performance on two real-world scene understanding tasks - semantic segmentation and 3D shape prediction - where we find that pre-training on our dataset significantly improves performance on both tasks, and achieves state-of-the-art performance on the most challenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and perform our experiments, is available online.",0
"Realistic synthetic data has been shown to effectively augment real world datasets in computer vision tasks such as object detection, semantic segmentation, visual odometry, etc., however these datasets often suffer from limited scale and variability which can lead to poor generalization performance. We present the first photorealistic large scale dataset called ""Hypersim"" that comprises scenes of complete indoor environments including furniture, objects, textures, materials, colors, natural lighting conditions and more, making it suitable for holistic scene understanding tasks. Our contribution includes design principles, implementation details, benchmark results comparing the state-of-the-art GAN models on our proposed dataset, ablation study, and qualitative results demonstrating the versatility of our synthetic indoor scenes, leading to improved training of deep learning networks compared to their counterparts trained only on real world images, therefore improving the transfer learning capability. In summary, we contribute a novel high quality and diverse hypersimulated indoor scene dataset for holistic scene understanding, outperforming previous works by orders of magnitude, providing new opportunities for developing robust image understanding techniques across various research fields where training data availability may be constrained.",1
"Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",0
"In modern structural biology, there exists a bottleneck at the stage of structure determination due to protein crystallization failure. Therefore, developing a method that can predict amino acid 3D structures without relying on experimental methods would bring significant advantages in both time efficiency and cost reduction. To address this problem, we propose a novel machine learning approach based on large scale pretraining language models such as GPT-4 (Generative Pre-trained Transformer 4). Our model achieves state-of-the-art accuracy on two benchmark datasets by optimizing the use of available data resources including non-crystallographic experimental measurements (such as NMR and FTIR) together with text-mined sequence information from PubMed. We believe our study demonstrates that advanced language processing techniques hold great promise for improving the speed and success rate of high throughput approaches towards solving critical challenges faced during protein structure prediction, potentially impacting pharmaceutical discovery efforts worldwide. This paper could therefore serve as a stepping stone for future applications using natural language modelling to other areas facing data scarcity problems such as drug repurposing or polypharmacology predictions.",1
"Deep learning research has recently witnessed an impressively fast-paced progress in a wide range of tasks including computer vision, natural language processing, and reinforcement learning. The extraordinary performance of these systems often gives the impression that they can be used to revolutionise our lives for the better. However, as recent works point out, these systems suffer from several issues that make them unreliable for use in the real world, including vulnerability to adversarial attacks (Szegedy et al. [248]), tendency to memorise noise (Zhang et al. [292]), being over-confident on incorrect predictions (miscalibration) (Guo et al. [99]), and unsuitability for handling private data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of these issues in detail, investigate their causes, and propose computationally cheap algorithms for mitigating them in practice. To do this, we identify structures in deep neural networks that can be exploited to mitigate the above causes of unreliability of deep learning algorithms.",0
"Sure! Here is a possible abstract:  Deep learning has revolutionized many fields by enabling computers to learn from data without explicit guidance. However, deep neural networks can easily become unreliable if they lack robustness against small perturbations, noise, or changes to input distributions. In this work, we propose a general framework that identifies and exploits structures inherent to any problem domain to improve reliability. Our method consists of two components: structure identification and exploitation. Structure identification involves analyzing the loss landscape and finding regularities that allow us to separate the regions where models perform well versus poorly, such as plateaus or ridges. We then show how these insights can inform model design choices like initialization, activation functions, batch normalization parameters, architectures, etc., which collectively make up our methods for structure exploitation. Our framework enables consistent improvement across tasks and datasets regardless of scale or complexity, outperforming prior art while remaining broadly applicable. We demonstrate the effectiveness of our approach on image classification using popular benchmarks CIFAR and ImageNet, showing improved test set performance and more reliable behavior under common distribution shift scenarios. Finally, we showcase our results through interactive visualizations and human evaluators, highlighting their utility in decision making contexts. Overall, our method provides significant advances towards producing more trustworthy and dependable artificial intelligence systems.  If you have specific requirements please let me know and I will adjust accordingly.",1
