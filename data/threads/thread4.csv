"Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10.",0
"Computer vision applications rely on fundamental tasks such as matching and partitioning, which are essential in multilabel segmentation, stereo estimation, and optical-flow computation. These tasks can be formulated as non-convex energy minimization problems and solved using convex lifting approaches, which provide near-globally optimal solutions. However, the computational effort required to apply these techniques is significant, making them impractical for many applications. To address this issue, we propose a spatial discretization method that converts continuous partitioning problems into a graph structure, allowing us to work on super-pixel graphs constructed by SLIC or Cut-Pursuit. This graph discretization significantly reduces the computational effort required for lifted partitioning problems compared to a Cartesian grid, while still providing near-globally optimal solutions. We provide detailed explanations of this methodology and present examples in multi-label segmentation and stereo estimation. Our results demonstrate that the proposed graph discretization can reduce the runtime and memory consumption of convex relaxations of matching problems by up to a factor of 10.",1
"Real-time tool segmentation is an essential component in computer-assisted surgical systems. We propose a novel real-time automatic method based on Fully Convolutional Networks (FCN) and optical flow tracking. Our method exploits the ability of deep neural networks to produce accurate segmentations of highly deformable parts along with the high speed of optical flow. Furthermore, the pre-trained FCN can be fine-tuned on a small amount of medical images without the need to hand-craft features. We validated our method using existing and new benchmark datasets, covering both ex vivo and in vivo real clinical cases where different surgical instruments are employed. Two versions of the method are presented, non-real-time and real-time. The former, using only deep learning, achieves a balanced accuracy of 89.6% on a real clinical dataset, outperforming the (non-real-time) state of the art by 3.8% points. The latter, a combination of deep learning with optical flow tracking, yields an average balanced accuracy of 78.2% across all the validated datasets.",0
"In computer-assisted surgical systems, real-time tool segmentation is crucial. Our proposed method uses Fully Convolutional Networks (FCN) and optical flow tracking to achieve accurate segmentations of deformable parts at high speed. The FCN can be fine-tuned with a small amount of medical images, eliminating the need for hand-crafted features. We tested our method on benchmark datasets covering ex vivo and in vivo clinical cases with various surgical instruments. We present two versions of the method: non-real-time and real-time. The former, using only deep learning, outperforms the state of the art by 3.8% points with a balanced accuracy of 89.6% on a clinical dataset. The latter, combining deep learning with optical flow tracking, achieves an average balanced accuracy of 78.2% across all validated datasets.",1
"Visual odometry (VO) is a prevalent way to deal with the relative localization problem, which is becoming increasingly mature and accurate, but it tends to be fragile under challenging environments. Comparing with classical geometry-based methods, deep learning-based methods can automatically learn effective and robust representations, such as depth, optical flow, feature, ego-motion, etc., from data without explicit computation. Nevertheless, there still lacks a thorough review of the recent advances of deep learning-based VO (Deep VO). Therefore, this paper aims to gain a deep insight on how deep learning can profit and optimize the VO systems. We first screen out a number of qualifications including accuracy, efficiency, scalability, dynamicity, practicability, and extensibility, and employ them as the criteria. Then, using the offered criteria as the uniform measurements, we detailedly evaluate and discuss how deep learning improves the performance of VO from the aspects of depth estimation, feature extraction and matching, pose estimation. We also summarize the complicated and emerging areas of Deep VO, such as mobile robots, medical robots, augmented reality and virtual reality, etc. Through the literature decomposition, analysis, and comparison, we finally put forward a number of open issues and raise some future research directions in this field.",0
"Although visual odometry (VO) is a widely used approach to solve the problem of relative localization, it is often unreliable in challenging environments. In contrast, deep learning-based methods can automatically learn robust representations, such as depth, optical flow, and feature, without explicit computation. However, there is still a lack of comprehensive reviews on recent advances in deep learning-based VO (Deep VO). Therefore, this paper aims to explore how deep learning can optimize VO systems by evaluating qualifications such as accuracy, efficiency, scalability, dynamicity, practicability, and extensibility. Using these criteria, we analyze how deep learning improves the performance of VO in terms of depth estimation, feature extraction and matching, and pose estimation. We also discuss emerging areas of Deep VO, including mobile robots, medical robots, augmented reality, and virtual reality. Finally, we highlight open issues and future research directions in this field.",1
"Objective Semi-supervised video object segmentation refers to segmenting the object in subsequent frames given the object label in the first frame. Existing algorithms are mostly based on the objectives of matching and propagation strategies, which often make use of the previous frame with masking or optical flow. This paper explores a new propagation method, uses short-term matching modules to extract the information of the previous frame and apply it in propagation, and proposes the network of Long-Short-Term similarity matching for video object segmentation (LSMOVS) Method: By conducting pixel-level matching and correlation between long-term matching module and short-term matching module with the first frame and previous frame, global similarity map and local similarity map are obtained, as well as feature pattern of current frame and masking of previous frame. After two refine networks, final results are obtained through segmentation network. Results: According to the experiments on the two data sets DAVIS 2016 and 2017, the method of this paper achieves favorable average of region similarity and contour accuracy without online fine tuning, which achieves 86.5% and 77.4% in terms of single target and multiple targets. Besides, the count of segmented frames per second reached 21. Conclusion: The short-term matching module proposed in this paper is more conducive to extracting the information of the previous frame than only the mask. By combining the long-term matching module with the short-term matching module, the whole network can achieve efficient video object segmentation without online fine tuning",0
"The goal of semi-supervised video object segmentation is to identify the object in subsequent frames based on its label in the first frame. Most current algorithms rely on matching and propagation strategies that utilize previous frames with masking or optical flow. This study introduces a new propagation method called the Long-Short-Term similarity matching for video object segmentation (LSMOVS) method, which uses short-term matching modules to extract information from the previous frame and apply it to propagation. By conducting pixel-level matching and correlation between long-term and short-term matching modules with the first frame and previous frame, global and local similarity maps are obtained, as well as the feature pattern of the current frame and masking of the previous frame. The final results are obtained through a segmentation network after two refine networks. Experimental results on two data sets show that the LSMOVS method achieves favorable region similarity and contour accuracy without online fine-tuning, reaching 86.5% and 77.4% in terms of single and multiple targets. Additionally, the method segments 21 frames per second. This study demonstrates that the short-term matching module is more effective in extracting information from previous frames than masking alone, and that combining long-term and short-term matching modules can result in efficient video object segmentation without online fine-tuning.",1
"In this paper, we propose a panorama stitching algorithm based on asymmetric bidirectional optical flow. This algorithm expects multiple photos captured by fisheye lens cameras as input, and then, through the proposed algorithm, these photos can be merged into a high-quality 360-degree spherical panoramic image. For photos taken from a distant perspective, the parallax among them is relatively small, and the obtained panoramic image can be nearly seamless and undistorted. For photos taken from a close perspective or with a relatively large parallax, a seamless though partially distorted panoramic image can also be obtained. Besides, with the help of Graphics Processing Unit (GPU), this algorithm can complete the whole stitching process at a very fast speed: typically, it only takes less than 30s to obtain a panoramic image of 9000-by-4000 pixels, which means our panorama stitching algorithm is of high value in many real-time applications. Our code is available at https://github.com/MungoMeng/Panorama-OpticalFlow.",0
"The proposed algorithm presented in this paper utilizes asymmetric bidirectional optical flow to stitch together multiple photos taken by fisheye lens cameras. This process results in a high-quality 360-degree spherical panoramic image. When photos are captured from a distant perspective, the parallax is minimal, leading to a nearly seamless and undistorted panoramic image. However, if photos are taken from a close perspective or have a large parallax, the resulting panoramic image may be partially distorted but still seamless. With the assistance of Graphics Processing Unit (GPU), this algorithm can complete the stitching process quickly, taking less than 30 seconds to obtain a panoramic image of 9000-by-4000 pixels. Therefore, this algorithm is highly beneficial for real-time applications. Interested parties can access our code at https://github.com/MungoMeng/Panorama-OpticalFlow.",1
"As a vital topic in media content interpretation, video anomaly detection (VAD) has made fruitful progress via deep neural network (DNN). However, existing methods usually follow a reconstruction or frame prediction routine. They suffer from two gaps: (1) They cannot localize video activities in a both precise and comprehensive manner. (2) They lack sufficient abilities to utilize high-level semantics and temporal context information. Inspired by frequently-used cloze test in language study, we propose a brand-new VAD solution named Video Event Completion (VEC) to bridge gaps above: First, we propose a novel pipeline to achieve both precise and comprehensive enclosure of video activities. Appearance and motion are exploited as mutually complimentary cues to localize regions of interest (RoIs). A normalized spatio-temporal cube (STC) is built from each RoI as a video event, which lays the foundation of VEC and serves as a basic processing unit. Second, we encourage DNN to capture high-level semantics by solving a visual cloze test. To build such a visual cloze test, a certain patch of STC is erased to yield an incomplete event (IE). The DNN learns to restore the original video event from the IE by inferring the missing patch. Third, to incorporate richer motion dynamics, another DNN is trained to infer erased patches' optical flow. Finally, two ensemble strategies using different types of IE and modalities are proposed to boost VAD performance, so as to fully exploit the temporal context and modality information for VAD. VEC can consistently outperform state-of-the-art methods by a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks. Our codes and results can be verified at github.com/yuguangnudt/VEC_VAD.",0
"Video anomaly detection (VAD) has made significant progress through the use of deep neural network (DNN) and is an essential topic in media content interpretation. However, current methods have limitations. They typically follow a reconstruction or frame prediction routine, which results in two issues: (1) they cannot accurately and comprehensively localize video activities; (2) they lack the ability to utilize high-level semantics and temporal context information effectively. To address these gaps, we propose a new VAD solution called Video Event Completion (VEC). To achieve precise and comprehensive enclosure of video activities, we propose a novel pipeline that exploits both appearance and motion as mutually complementary cues to localize regions of interest (RoIs). We build a normalized spatio-temporal cube (STC) from each RoI as a video event, which serves as a basic processing unit and the foundation of VEC. To capture high-level semantics, we propose a visual cloze test, where a certain patch of STC is erased to yield an incomplete event (IE), and the DNN learns to restore the original video event from the IE by inferring the missing patch. To incorporate richer motion dynamics, we train another DNN to infer erased patches' optical flow. We propose two ensemble strategies using different types of IE and modalities to boost VAD performance and fully exploit the temporal context and modality information for VAD. VEC outperforms state-of-the-art methods by a significant margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks, and our codes and results can be verified at github.com/yuguangnudt/VEC_VAD.",1
"Our previous work classified a taxonomy of suturing gestures during a vesicourethral anastomosis of robotic radical prostatectomy in association with tissue tears and patient outcomes. Herein, we train deep-learning based computer vision (CV) to automate the identification and classification of suturing gestures for needle driving attempts. Using two independent raters, we manually annotated live suturing video clips to label timepoints and gestures. Identification (2395 videos) and classification (511 videos) datasets were compiled to train CV models to produce two- and five-class label predictions, respectively. Networks were trained on inputs of raw RGB pixels as well as optical flow for each frame. Each model was trained on 80/20 train/test splits. In this study, all models were able to reliably predict either the presence of a gesture (identification, AUC: 0.88) as well as the type of gesture (classification, AUC: 0.87) at significantly above chance levels. For both gesture identification and classification datasets, we observed no effect of recurrent classification model choice (LSTM vs. convLSTM) on performance. Our results demonstrate CV's ability to recognize features that not only can identify the action of suturing but also distinguish between different classifications of suturing gestures. This demonstrates the potential to utilize deep learning CV towards future automation of surgical skill assessment.",0
"In our previous work, we created a taxonomy of suturing movements during a specific surgical procedure and linked them to patient outcomes. In this study, we used deep-learning based computer vision to automate the identification and classification of these suturing movements. We manually annotated video clips to create datasets for training the computer vision models, which were able to predict the presence and type of suturing movements with high accuracy. We found that the choice of recurrent classification model did not affect performance. Our results suggest that deep learning computer vision has the potential to automate surgical skill assessment by recognizing and distinguishing between different suturing gestures.",1
"We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10%, a 16% error reduction from the best published result (6.10%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.",0
"The introduction of Recurrent All-Pairs Field Transforms (RAFT) presents a novel deep network architecture for optical flow. RAFT extracts per-pixel features and constructs multi-scale 4D correlation volumes for every pair of pixels. It then updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves outstanding performance, as evidenced by its F1-all error of 5.10% on KITTI, which is a 16% error reduction from the best published result (6.10%). Additionally, RAFT achieves an end-point-error of 2.855 pixels on Sintel (final pass), which is a 30% error reduction from the best published result (4.098 pixels). RAFT has strong cross-dataset generalization and is highly efficient in terms of inference time, training speed, and parameter count. The code is available at https://github.com/princeton-vl/RAFT.",1
"Event cameras are paradigm-shifting novel sensors that report asynchronous, per-pixel brightness changes called 'events' with unparalleled low latency. This makes them ideal for high speed, high dynamic range scenes where conventional cameras would fail. Recent work has demonstrated impressive results using Convolutional Neural Networks (CNNs) for video reconstruction and optic flow with events. We present strategies for improving training data for event based CNNs that result in 20-40% boost in performance of existing state-of-the-art (SOTA) video reconstruction networks retrained with our method, and up to 15% for optic flow networks. A challenge in evaluating event based video reconstruction is lack of quality ground truth images in existing datasets. To address this, we present a new High Quality Frames (HQF) dataset, containing events and ground truth frames from a DAVIS240C that are well-exposed and minimally motion-blurred. We evaluate our method on HQF + several existing major event camera datasets.",0
"Event cameras are innovative sensors that capture per-pixel brightness changes called 'events' with unparalleled low latency, making them ideal for high speed and high dynamic range scenes where traditional cameras are not effective. Recent studies have shown that Convolutional Neural Networks (CNNs) can improve video reconstruction and optic flow with events. We propose techniques to improve the training data for CNNs that result in a 20-40% performance increase for state-of-the-art video reconstruction networks and up to 15% for optic flow networks. However, evaluating event-based video reconstruction is challenging due to a lack of quality ground truth images in existing datasets. To overcome this, we introduce the High Quality Frames (HQF) dataset, which includes events and ground truth frames from a DAVIS240C that are well-exposed and minimally motion-blurred. Our approach is evaluated on HQF and several other major event camera datasets.",1
"Voice Activity Detection (VAD) refers to the task of identification of regions of human speech in digital signals such as audio and video. While VAD is a necessary first step in many speech processing systems, it poses challenges when there are high levels of ambient noise during the audio recording. To improve the performance of VAD in such conditions, several methods utilizing the visual information extracted from the region surrounding the mouth/lip region of the speakers' video recording have been proposed. Even though these provide advantages over audio-only methods, they depend on faithful extraction of lip/mouth regions. Motivated by these, a new paradigm for VAD based on the fact that respiration forms the primary source of energy for speech production is proposed. Specifically, an audio-independent VAD technique using the respiration pattern extracted from the speakers' video is developed. The Respiration Pattern is first extracted from the video focusing on the abdominal-thoracic region of a speaker using an optical flow based method. Subsequently, voice activity is detected from the respiration pattern signal using neural sequence-to-sequence prediction models. The efficacy of the proposed method is demonstrated through experiments on a challenging dataset recorded in real acoustic environments and compared with four previous methods based on audio and visual cues.",0
"Voice Activity Detection (VAD) is the process of identifying human speech regions in digital signals, including audio and video. When there is a high level of ambient noise during audio recording, VAD poses challenges. To improve VAD performance in such conditions, various methods have been proposed that use visual information extracted from the area surrounding the speaker's mouth/lip region. However, these methods rely heavily on accurate extraction of lip/mouth regions. Therefore, a new VAD paradigm based on the fact that respiration is the primary energy source for speech production is suggested. An audio-independent VAD technique is developed that uses respiration patterns extracted from the speaker's video. The optical flow-based method focuses on the abdominal-thoracic region of the speaker to extract the Respiration Pattern. Neural sequence-to-sequence prediction models are then used to detect voice activity from the respiration pattern signal. The proposed method's efficacy is demonstrated through experiments on a challenging dataset recorded in real acoustic environments and compared with four previous methods based on audio and visual cues.",1
"In this paper, we introduce a novel suspect-and-investigate framework, which can be easily embedded in a drone for automated parking violation detection (PVD). Our proposed framework consists of: 1) SwiftFlow, an efficient and accurate convolutional neural network (CNN) for unsupervised optical flow estimation; 2) Flow-RCNN, a flow-guided CNN for car detection and classification; and 3) an illegally parked car (IPC) candidate investigation module developed based on visual SLAM. The proposed framework was successfully embedded in a drone from ATG Robotics. The experimental results demonstrate that, firstly, our proposed SwiftFlow outperforms all other state-of-the-art unsupervised optical flow estimation approaches in terms of both speed and accuracy; secondly, IPC candidates can be effectively and efficiently detected by our proposed Flow-RCNN, with a better performance than our baseline network, Faster-RCNN; finally, the actual IPCs can be successfully verified by our investigation module after drone re-localization.",0
"This paper presents a new framework called suspect-and-investigate, designed for automated parking violation detection (PVD) using a drone. Our framework includes three components: SwiftFlow, a precise and effective convolutional neural network (CNN) for unsupervised optical flow estimation; Flow-RCNN, a CNN guided by flow for car detection and classification; and a module for investigating illegally parked car (IPC) candidates that is developed based on visual SLAM. We integrated this framework into a drone from ATG Robotics and conducted experiments to evaluate its performance. Our results show that SwiftFlow outperforms all other unsupervised optical flow estimation methods in terms of speed and accuracy. Additionally, our proposed Flow-RCNN effectively and efficiently detects IPC candidates, outperforming the Faster-RCNN baseline network. Finally, our investigation module successfully verifies actual IPCs after drone re-localization.",1
"In autonomous driving, monocular sequences contain lots of information. Monocular depth estimation, camera ego-motion estimation and optical flow estimation in consecutive frames are high-profile concerns recently. By analyzing tasks above, pixels in the middle frame are modeled into three parts: the rigid region, the non-rigid region, and the occluded region. In joint unsupervised training of depth and pose, we can segment the occluded region explicitly. The occlusion information is used in unsupervised learning of depth, pose and optical flow, as the image reconstructed by depth-pose and optical flow will be invalid in occluded regions. A less-than-mean mask is designed to further exclude the mismatched pixels interfered with by motion or illumination change in the training of depth and pose networks. This method is also used to exclude some trivial mismatched pixels in the training of the optical flow network. Maximum normalization is proposed for depth smoothness term to restrain depth degradation in textureless regions. In the occluded region, as depth and camera motion can provide more reliable motion estimation, they can be used to instruct unsupervised learning of optical flow. Our experiments in KITTI dataset demonstrate that the model based on three regions, full and explicit segmentation of the occlusion region, the rigid region, and the non-rigid region with corresponding unsupervised losses can improve performance on three tasks significantly. The source code is available at: https://github.com/guangmingw/DOPlearning.",0
"The use of monocular sequences in autonomous driving provides a wealth of information, with recent attention focused on monocular depth estimation, camera ego-motion estimation, and optical flow estimation in consecutive frames. To better understand these tasks, the pixels in the middle frame can be divided into three distinct regions: the rigid, non-rigid, and occluded regions. By conducting joint unsupervised training of depth and pose, the occluded region can be explicitly segmented, allowing for improved understanding of the scene. Additionally, a less-than-mean mask is utilized to exclude mismatched pixels caused by motion or illumination changes during the training of the depth and pose networks. This method is also used to exclude trivial mismatched pixels when training the optical flow network. To improve depth smoothness in textureless regions, maximum normalization is suggested for the depth smoothness term. In the occluded region, depth and camera motion can provide more reliable motion estimation, which can then be used to inform unsupervised learning of optical flow. Experiments conducted on the KITTI dataset demonstrate that this approach, which segments the occlusion region and includes corresponding unsupervised losses, can significantly improve performance across all three tasks. The source code for this approach is available at https://github.com/guangmingw/DOPlearning.",1
"We propose a simply method to generate high quality synthetic dataset based on open-source game Minecraft includes rendered image, Depth map, surface normal map, and 6-dof camera trajectory. This dataset has a perfect ground-truth generated by plug-in program, and thanks for the large game's community, there is an extremely large number of 3D open-world environment, users can find suitable scenes for shooting and build data sets through it and they can also build scenes in-game. as such, We don't need to worry about manual over fitting caused by too small datasets. what's more, there is also a shader community which We can use to minimize data bias between rendered images and real-images as little as possible. Last but not least, we now provide three tools to generate the data for depth prediction ,surface normal prediction and visual odometry, user can also develop the plug-in module for other vision task like segmentation or optical flow prediction.",0
"Our proposal offers a straightforward approach to creating a top-notch artificial dataset using the open-source game Minecraft. This dataset comprises a rendered image, depth map, surface normal map, and a 6-dof camera trajectory, all with accurate ground-truth generated by a plug-in program. Thanks to the vast community of this game, there is an abundant supply of 3D open-world environments, which users can utilize to shoot suitable scenes and create datasets. Additionally, users can build their scenes within the game, eliminating any concerns of manual overfitting due to small datasets. Furthermore, we can leverage the shader community to minimize data bias between rendered and real images. Lastly, we offer three tools for generating data for depth prediction, surface normal prediction, and visual odometry, while users can develop plug-in modules for other vision tasks like segmentation or optical flow prediction.",1
"Activity detection from first-person videos (FPV) captured using a wearable camera is an active research field with potential applications in many sectors, including healthcare, law enforcement, and rehabilitation. State-of-the-art methods use optical flow-based hybrid techniques that rely on features derived from the motion of objects from consecutive frames. In this work, we developed a two-stream network, the \emph{SegCodeNet}, that uses a network branch containing video-streams with color-coded semantic segmentation masks of relevant objects in addition to the original RGB video-stream. We also include a stream-wise attention gating that prioritizes between the two streams and a frame-wise attention module that prioritizes the video frames that contain relevant features. Experiments are conducted on an FPV dataset containing $18$ activity classes in office environments. In comparison to a single-stream network, the proposed two-stream method achieves an absolute improvement of $14.366\%$ and $10.324\%$ for averaged F1 score and accuracy, respectively, when average results are compared for three different frame sizes $224\times224$, $112\times112$, and $64\times64$. The proposed method provides significant performance gains for lower-resolution images with absolute improvements of $17\%$ and $26\%$ in F1 score for input dimensions of $112\times112$ and $64\times64$, respectively. The best performance is achieved for a frame size of $224\times224$ yielding an F1 score and accuracy of $90.176\%$ and $90.799\%$ which outperforms the state-of-the-art Inflated 3D ConvNet (I3D) \cite{carreira2017quo} method by an absolute margin of $4.529\%$ and $2.419\%$, respectively.",0
"The field of activity detection from first-person videos (FPV) captured by wearable cameras has promising applications in various sectors, such as healthcare, law enforcement, and rehabilitation. Current methods rely on optical flow-based hybrid techniques that use features derived from object motion in consecutive frames. In this study, we propose a two-stream network called the \emph{SegCodeNet}, which includes a network branch containing video-streams with color-coded semantic segmentation masks of relevant objects and an original RGB video-stream. The model also comprises a stream-wise attention gating that prioritizes between the two streams and a frame-wise attention module that prioritizes frames with relevant features. We evaluate the proposed method on an FPV dataset with 18 activity classes in office environments. Compared to a single-stream network, the two-stream method shows significant improvement, with an absolute improvement of 14.366% and 10.324% for averaged F1 score and accuracy, respectively. Particularly, the proposed method provides substantial performance gains for lower-resolution images. The best performance was achieved for a frame size of 224x224, with an F1 score and accuracy of 90.176% and 90.799%, respectively, outperforming the state-of-the-art Inflated 3D ConvNet (I3D) method by 4.529% and 2.419%, respectively.",1
"Crowd flow describes the elementary group behavior of crowds. Understanding the dynamics behind these movements can help to identify various abnormalities in crowds. However, developing a crowd model describing these flows is a challenging task. In this paper, a physics-based model is proposed to describe the movements in dense crowds. The crowd model is based on active Langevin equation where the motion points are assumed to be similar to active colloidal particles in fluids. The model is further augmented with computer-vision techniques to segment both linear and non-linear motion flows in a dense crowd. The evaluation of the active Langevin equation-based crowd segmentation has been done on publicly available crowd videos and on our own videos. The proposed method is able to segment the flow with lesser optical flow error and better accuracy in comparison to existing state-of-the-art methods.",0
"The behavior of crowds is referred to as crowd flow and understanding the dynamics of these movements can help detect abnormalities. Creating a model to describe these flows is a challenging task, but this paper proposes a physics-based model that utilizes an active Langevin equation. The motion points in this model are assumed to be similar to active colloidal particles in fluids, and computer-vision techniques are used to segment both linear and non-linear motion flows in dense crowds. The evaluation of this model has been conducted on publicly available crowd videos and the results show that it is able to segment the flow with greater accuracy and lesser optical flow error than existing state-of-the-art methods.",1
"Personal robots and driverless cars need to be able to operate in novel environments and thus quickly and efficiently learn to recognise new object classes. We address this problem by considering the task of video object segmentation. Previous accurate methods for this task finetune a model using the first annotated frame, and/or use additional inputs such as optical flow and complex post-processing. In contrast, we develop a fast, causal algorithm that requires no finetuning, auxiliary inputs or post-processing, and segments a variable number of objects in a single forward-pass. We represent an object with clusters, or ""visual words"", in the embedding space, which correspond to object parts in the image space. This allows us to robustly match to the reference objects throughout the video, because although the global appearance of an object changes as it undergoes occlusions and deformations, the appearance of more local parts may stay consistent. We learn these visual words in an unsupervised manner, using meta-learning to ensure that our training objective matches our inference procedure. We achieve comparable accuracy to finetuning based methods (whilst being 1 to 2 orders of magnitude faster), and state-of-the-art in terms of speed/accuracy trade-offs on four video segmentation datasets. Code is available at https://github.com/harkiratbehl/MetaVOS.",0
"The fast learning of new object classes is crucial for personal robots and driverless cars to operate effectively in novel environments. To address this challenge, we focus on video object segmentation and aim to develop an algorithm that is quick and efficient. Existing methods for this task involve finetuning a model using the first annotated frame and/or utilizing additional inputs such as optical flow and complex post-processing. However, we propose a novel approach that requires no finetuning, auxiliary inputs, or post-processing, and can segment multiple objects in a single forward-pass. We represent an object with clusters or ""visual words"" in the embedding space, which correspond to object parts in the image space. This enables us to match reference objects consistently throughout the video, even as they undergo changes in appearance due to occlusions and deformations. We use unsupervised learning to acquire these visual words, ensuring that our training objective aligns with our inference procedure. Our algorithm achieves accuracy comparable to finetuning-based methods but is significantly faster, and we demonstrate state-of-the-art speed/accuracy trade-offs on four video segmentation datasets. Our code is available at https://github.com/harkiratbehl/MetaVOS.",1
"Nowadays, digital facial content manipulation has become ubiquitous and realistic with the success of generative adversarial networks (GANs), making face recognition (FR) systems suffer from unprecedented security concerns. In this paper, we investigate and introduce a new type of adversarial attack to evade FR systems by manipulating facial content, called \textbf{\underline{a}dversarial \underline{mor}phing \underline{a}ttack} (a.k.a. Amora). In contrast to adversarial noise attack that perturbs pixel intensity values by adding human-imperceptible noise, our proposed adversarial morphing attack works at the semantic level that perturbs pixels spatially in a coherent manner. To tackle the black-box attack problem, we devise a simple yet effective joint dictionary learning pipeline to obtain a proprietary optical flow field for each attack. Our extensive evaluation on two popular FR systems demonstrates the effectiveness of our adversarial morphing attack at various levels of morphing intensity with smiling facial expression manipulations. Both open-set and closed-set experimental results indicate that a novel black-box adversarial attack based on local deformation is possible, and is vastly different from additive noise attacks. The findings of this work potentially pave a new research direction towards a more thorough understanding and investigation of image-based adversarial attacks and defenses.",0
"Digital manipulation of facial content has become highly advanced and widespread due to the success of generative adversarial networks (GANs), which has led to significant security concerns for face recognition (FR) systems. This study presents a new kind of adversarial attack named Adversarial Morphing Attack (Amora) that manipulates facial content to evade FR systems. Unlike adversarial noise attacks that distort pixel intensity values by adding imperceptible noise, our proposed attack works at the semantic level and perturbs pixels spatially in a coherent manner. To address the black-box attack problem, we introduce a simple yet effective joint dictionary learning pipeline to obtain an exclusive optical flow field for each attack. Our extensive evaluation on two popular FR systems shows that our adversarial morphing attack is effective at different levels of morphing intensity, particularly with smiling facial expression manipulations. Our results suggest that a novel black-box adversarial attack based on local deformation is possible, and it differs significantly from additive noise attacks. Overall, our findings open up new avenues for research into image-based adversarial attacks and defenses.",1
"We propose a light-weight variational framework for online tracking of object segmentations in videos based on optical flow and image boundaries. While high-end computer vision methods on this task rely on sequence specific training of dedicated CNN architectures, we show the potential of a variational model, based on generic video information from motion and color. Such cues are usually required for tasks such as robot navigation or grasp estimation. We leverage them directly for video object segmentation and thus provide accurate segmentations at potentially very low extra cost. Our simple method can provide competitive results compared to the costly CNN-based methods with parameter tuning. Furthermore, we show that our approach can be combined with state-of-the-art CNN-based segmentations in order to improve over their respective results. We evaluate our method on the datasets DAVIS 16,17 and SegTrack v2.",0
"Our proposal introduces a simple variational framework for tracking object segmentations in videos using optical flow and image boundaries. Unlike advanced computer vision techniques that require sequence-specific training of dedicated CNN architectures, we rely on generic video information from motion and color, which is commonly used for tasks like robot navigation or grasp estimation. This approach enables us to achieve accurate segmentations at a lower cost. Our method can also produce competitive results compared to expensive CNN-based methods with parameter tuning. Additionally, we demonstrate that our approach can be combined with state-of-the-art CNN-based segmentations to improve results. We evaluate our method on DAVIS 16,17 and SegTrack v2 datasets.",1
"We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.",0
"We conduct a thorough comparison and analysis of key components in unsupervised optical flow in order to determine the most effective photometric loss, occlusion handling, and smoothness regularization methods. In addition, we propose several innovative enhancements to existing unsupervised flow models, including cost volume normalization, stopping gradient at the occlusion mask, promoting smoothness prior to upsampling the flow field, and continual self-supervision through image resizing. Our new unsupervised flow technique, which combines the results of our investigation with our improved model components, surpasses the previous unsupervised state-of-the-art and performs similarly to supervised FlowNet2 on the KITTI 2015 dataset, while also being simpler than related approaches.",1
"This paper presents a novel end-to-end dynamic time-lapse video generation framework, named DTVNet, to generate diversified time-lapse videos from a single landscape image, which are conditioned on normalized motion vectors. The proposed DTVNet consists of two submodules: \emph{Optical Flow Encoder} (OFE) and \emph{Dynamic Video Generator} (DVG). The OFE maps a sequence of optical flow maps to a \emph{normalized motion vector} that encodes the motion information inside the generated video. The DVG contains motion and content streams that learn from the motion vector and the single image respectively, as well as an encoder and a decoder to learn shared content features and construct video frames with corresponding motion respectively. Specifically, the \emph{motion stream} introduces multiple \emph{adaptive instance normalization} (AdaIN) layers to integrate multi-level motion information that are processed by linear layers. In the testing stage, videos with the same content but various motion information can be generated by different \emph{normalized motion vectors} based on only one input image. We further conduct experiments on Sky Time-lapse dataset, and the results demonstrate the superiority of our approach over the state-of-the-art methods for generating high-quality and dynamic videos, as well as the variety for generating videos with various motion information.",0
"The aim of this paper is to introduce a new method for generating time-lapse videos from a single landscape image. The proposed method, called DTVNet, uses normalized motion vectors to produce diverse videos. DTVNet consists of two parts: the Optical Flow Encoder (OFE) and the Dynamic Video Generator (DVG). The OFE converts a sequence of optical flow maps into a normalized motion vector that encodes motion information. The DVG has two streams, one for motion and one for content, which learn from the motion vector and the image respectively. The encoder and decoder work together to construct video frames with corresponding motion. The motion stream employs adaptive instance normalization (AdaIN) layers to integrate multi-level motion information processed by linear layers. By using different normalized motion vectors, videos with the same content but varying motion can be generated from a single image. The proposed method outperforms existing approaches for generating high-quality and dynamic videos with varied motion information, as demonstrated through experiments on the Sky Time-lapse dataset.",1
"Depth map estimation is a crucial task in computer vision, and new approaches have recently emerged taking advantage of light fields, as this new imaging modality captures much more information about the angular direction of light rays compared to common approaches based on stereoscopic images or multi-view. In this paper, we propose a novel depth estimation method from light fields based on existing optical flow estimation methods. The optical flow estimator is applied on a sequence of images taken along an angular dimension of the light field, which produces several disparity map estimates. Considering both accuracy and efficiency, we choose the feature flow method as our optical flow estimator. Thanks to its spatio-temporal edge-aware filtering properties, the different disparity map estimates that we obtain are very consistent, which allows a fast and simple aggregation step to create a single disparity map, which can then converted into a depth map. Since the disparity map estimates are consistent, we can also create a depth map from each disparity estimate, and then aggregate the different depth maps in the 3D space to create a single dense depth map.",0
"Recently, new approaches have emerged in computer vision for estimating depth maps using light fields. Unlike stereoscopic images or multi-view techniques, light fields capture a greater amount of information about the direction of light rays. In this study, we propose a unique method for estimating depth from light fields by utilizing existing optical flow estimation methods. Specifically, we apply an optical flow estimator on a sequence of images taken along the angular dimension of the light field to produce multiple disparity map estimates. To ensure both accuracy and efficiency, we use the feature flow method as our optical flow estimator, which includes spatio-temporal edge-aware filtering properties. The resulting disparity map estimates are highly consistent, allowing for a fast and straightforward aggregation process to create a single disparity map that can be converted into a depth map. Additionally, we can create a depth map from each disparity estimate and aggregate them in 3D space to produce a dense depth map.",1
"Our objective is to transform a video into a set of discrete audio-visual objects using self-supervised learning. To this end, we introduce a model that uses attention to localize and group sound sources, and optical flow to aggregate information over time. We demonstrate the effectiveness of the audio-visual object embeddings that our model learns by using them for four downstream speech-oriented tasks: (a) multi-speaker sound source separation, (b) localizing and tracking speakers, (c) correcting misaligned audio-visual data, and (d) active speaker detection. Using our representation, these tasks can be solved entirely by training on unlabeled video, without the aid of object detectors. We also demonstrate the generality of our method by applying it to non-human speakers, including cartoons and puppets.Our model significantly outperforms other self-supervised approaches, and obtains performance competitive with methods that use supervised face detection.",0
"Our aim is to utilize self-supervised learning to convert a video into a collection of distinct audio-visual objects. To achieve this, we have developed a model that employs attention to locate and group sound sources, as well as optical flow to consolidate information across time. By utilizing the audio-visual object embeddings that our model learns, we have successfully accomplished four speech-oriented tasks: (a) separating multiple sound sources, (b) tracking and localizing speakers, (c) rectifying misaligned audio-visual data, and (d) detecting active speakers. Our approach allows for these tasks to be accomplished solely by training on unlabeled video, without the assistance of object detectors. Moreover, we have demonstrated that our method can be applied to non-human speakers, including cartoons and puppets. Our model surpasses other self-supervised techniques and achieves performance that is comparable to methods that utilize supervised face detection.",1
"To address the problem of training on small datasets for action recognition tasks, most prior works are either based on a large number of training samples or require pre-trained models transferred from other large datasets to tackle overfitting problems. However, it limits the research within organizations that have strong computational abilities. In this work, we try to propose a data-efficient framework that can train the model from scratch on small datasets while achieving promising results. Specifically, by introducing a 3D central difference convolution operation, we proposed a novel C3D neural network-based two-stream (Rank Pooling RGB and Optical Flow) framework for the task. The method is validated on the action recognition track of the ECCV 2020 VIPriors challenges and got the 2nd place (88.31%). It is proved that our method can achieve a promising result even without a pre-trained model on large scale datasets. The code will be released soon.",0
"Most previous works dealing with the problem of training on small datasets for action recognition tasks either rely on a large number of training samples or require pre-trained models from other large datasets to avoid overfitting. However, this approach is limited to organizations with strong computational capabilities. In this study, we aim to propose a data-efficient framework that can train the model from scratch on small datasets while achieving promising results. Our approach involves introducing a 3D central difference convolution operation and creating a novel C3D neural network-based two-stream framework that utilizes Rank Pooling RGB and Optical Flow. We validated our method on the action recognition track of the ECCV 2020 VIPriors challenges and achieved the 2nd place with 88.31% accuracy. Our findings demonstrate that our method can yield promising results without a pre-trained model on large scale datasets. We plan to release our code soon.",1
"Currently, the safety of people has become a very important problem in different places including subway station, universities, colleges, airport, shopping mall and square, city squares. Therefore, considering intelligence event detection systems is more and urgently required. The event detection method is developed to identify abnormal behavior intelligently, so public can take action as soon as possible to prevent unwanted activities. The problem is very challenging due to high crowd density in different areas. One of these issues is occlusion due to which individual tracking and analysis becomes impossible as shown in Fig. 1. Secondly, more challenging is the proper representation of individual behavior in the crowd. We consider a novel method to deal with these challenges. Considering the challenge of tracking, we partition complete frame into smaller patches, and extract motion pattern to demonstrate the motion in each individual patch. For this purpose, our work takes into account KLT corners as consolidated features to describe moving regions and track these features by considering optical flow method. To embed motion patterns, we develop and consider the distribution of all motion information in a patch as Gaussian distribution, and formulate parameters of Gaussian model as our motion pattern descriptor.",0
"The issue of people's safety has become a pressing concern in various public areas such as subway stations, airports, universities, colleges, shopping malls, and city squares. Therefore, it is essential to implement intelligent event detection systems to detect abnormal behavior and take immediate action to prevent unwanted activities. However, this task is challenging due to the high density of crowds in different areas. Two significant difficulties are occlusion, which makes it impossible to track individuals, and representing individual behavior in the crowd. To address these challenges, we propose a novel approach. We partition the complete frame into smaller patches and extract motion patterns in each patch using KLT corners as consolidated features and the optical flow method to track them. We then use the distribution of all motion information in a patch to formulate parameters of a Gaussian model as our motion pattern descriptor.",1
"Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",0
"Event cameras are sensors that are inspired by biology and are different from traditional frame cameras. They do not capture images at a fixed rate but rather measure changes in brightness asynchronously per pixel. This results in a stream of events that encode information about the time, location, and sign of the brightness changes. Event cameras have several advantages over traditional cameras, including high temporal resolution, very high dynamic range, low power consumption, and reduced motion blur. They are ideal for challenging scenarios such as low-latency, high speed, and high dynamic range, which traditional cameras struggle with. However, processing the output of event cameras requires novel methods to unlock their potential. This paper provides an overview of event-based vision, including the applications and algorithms used to unlock the properties of event cameras. It covers the working principle of event cameras, the available sensors, and the tasks they have been used for. The paper also discusses the techniques developed to process events, including learning-based techniques and specialized processors such as spiking neural networks. Finally, the paper highlights the challenges that remain and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",1
"Efficiently modeling dynamic motion information in videos is crucial for action recognition task. Most state-of-the-art methods heavily rely on dense optical flow as motion representation. Although combining optical flow with RGB frames as input can achieve excellent recognition performance, the optical flow extraction is very time-consuming. This undoubtably will count against real-time action recognition. In this paper, we shed light on fast action recognition by lifting the reliance on optical flow. Our motivation lies in the observation that small displacements of motion boundaries are the most critical ingredients for distinguishing actions, so we design a novel motion cue called Persistence of Appearance (PA). In contrast to optical flow, our PA focuses more on distilling the motion information at boundaries. Also, it is more efficient by only accumulating pixel-wise differences in feature space, instead of using exhaustive patch-wise search of all the possible motion vectors. Our PA is over 1000x faster (8196fps vs. 8fps) than conventional optical flow in terms of motion modeling speed. To further aggregate the short-term dynamics in PA to long-term dynamics, we also devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP) that can adaptively model long-range temporal relationships across various timescales. We finally incorporate the proposed PA and VAP to form a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Extensive experiments on six challenging action recognition benchmarks verify that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch.",0
"It is crucial to efficiently model dynamic motion information in videos for action recognition. However, most current methods heavily rely on dense optical flow, which can be time-consuming. While combining optical flow with RGB frames can achieve excellent recognition performance, this approach is not suitable for real-time action recognition. In this study, we propose a novel motion cue called Persistence of Appearance (PA), which focuses on distilling motion information at boundaries and is more efficient than optical flow. Our PA is over 1000x faster than conventional optical flow in terms of motion modeling speed. To aggregate short-term dynamics in PA to long-term dynamics, we devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP), which can adaptively model long-range temporal relationships across various timescales. We incorporate PA and VAP into a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Experiments on six challenging action recognition benchmarks demonstrate that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch.",1
"Situational awareness and Indoor location tracking for firefighters is one of the tasks with paramount importance in search and rescue operations. For Indoor Positioning systems (IPS), GPS is not the best possible solution. There are few other techniques like dead reckoning, Wifi and bluetooth based triangulation, Structure from Motion (SFM) based scene reconstruction for Indoor positioning system. However due to high temperatures, the rapidly changing environment of fires, and low parallax in the thermal images, these techniques are not suitable for relaying the necessary information in a fire fighting environment needed to increase situational awareness in real time. In fire fighting environments, thermal imaging cameras are used due to smoke and low visibility hence obtaining relative orientation from the vanishing point estimation is very difficult. The following technique that is the content of this research implements a novel optical flow based video compass for orientation estimation and fused IMU data based activity recognition for IPS. This technique helps first responders to go into unprepared, unknown environments and still maintain situational awareness like the orientation and, position of the victim fire fighters.",0
"In search and rescue operations, situational awareness and indoor location tracking are of utmost importance for firefighters. Global Positioning System (GPS) is not the most effective solution for Indoor Positioning Systems (IPS). Alternative techniques such as dead reckoning, Wifi and Bluetooth-based triangulation, and Structure from Motion (SFM) based scene reconstruction are available for IPS. However, these techniques are unsuitable for fire-fighting environments due to the high temperatures, rapidly changing conditions, and low parallax in thermal images. Thermal imaging cameras are used in such environments, but obtaining relative orientation from vanishing point estimation is challenging because of smoke and low visibility. This research proposes a novel technique that uses optical flow-based video compass for orientation estimation and fused Inertial Measurement Unit (IMU) data-based activity recognition for IPS. This technique enables first responders to navigate unprepared environments and maintain situational awareness, including the orientation and position of victim firefighters.",1
"The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",0
"The aim of this article is to explore self-supervised learning from video, with a focus on creating representations for action recognition. Our contributions are as follows: Firstly, we introduce a novel architecture and learning framework called Memory-augmented Dense Predictive Coding (MemDPC) for this task. This approach uses a predictive attention mechanism over compressed memories, enabling the efficient construction of multiple hypotheses for future states through a convex combination of condensed representations. Secondly, we examine visual-only self-supervised video representation learning from RGB frames, unsupervised optical flow, or both. Lastly, we conduct a comprehensive evaluation of the quality of the learned representations across four different downstream tasks, including action recognition, video retrieval, learning with limited annotations, and unintentional action classification. Our results demonstrate state-of-the-art or comparable performance compared to other approaches, with significantly less training data required.",1
"Human action recognition is regarded as a key cornerstone in domains such as surveillance or video understanding. Despite recent progress in the development of end-to-end solutions for video-based action recognition, achieving state-of-the-art performance still requires using auxiliary hand-crafted motion representations, e.g., optical flow, which are usually computationally demanding. In this work, we propose to use residual frames (i.e., differences between adjacent RGB frames) as an alternative ""lightweight"" motion representation, which carries salient motion information and is computationally efficient. In addition, we develop a new pseudo-3D convolution module which decouples 3D convolution into 2D and 1D convolution. The proposed module exploits residual information in the feature space to better structure motions, and is equipped with a self-attention mechanism that assists to recalibrate the appearance and motion features. Empirical results confirm the efficiency and effectiveness of residual frames as well as the proposed pseudo-3D convolution module.",0
"The recognition of human actions is considered crucial in fields such as video comprehension and surveillance. Although recent advancements have been made in creating end-to-end solutions for identifying actions in videos, achieving optimal performance often requires the use of manually crafted motion representations, like optical flow, which can be computationally intensive. This study proposes the use of residual frames (the contrast between adjacent RGB frames) as a more lightweight alternative for motion representation. These frames contain significant motion information and are less computationally demanding. Additionally, a new pseudo-3D convolution module is developed, which splits 3D convolution into 2D and 1D convolution. This module utilizes residual information in the feature space to better organize motions and is equipped with a self-attention mechanism to recalibrate appearance and motion features. Experimental results demonstrate the efficiency and effectiveness of residual frames and the proposed pseudo-3D convolution module.",1
"Dynamic Vision Sensors (DVSs) asynchronously stream events in correspondence of pixels subject to brightness changes. Differently from classic vision devices, they produce a sparse representation of the scene. Therefore, to apply standard computer vision algorithms, events need to be integrated into a frame or event-surface. This is usually attained through hand-crafted grids that reconstruct the frame using ad-hoc heuristics. In this paper, we propose Matrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that efficiently process events and learn end-to-end task-dependent event-surfaces. Compared to existing reconstruction approaches, our learned event-surface shows good flexibility and expressiveness on optical flow estimation on the MVSEC benchmark and it improves the state-of-the-art of event-based object classification on the N-Cars dataset.",0
"Dynamic Vision Sensors (DVSs) detect changes in pixel brightness and transmit events asynchronously. Unlike traditional vision devices, DVSs produce a sparse representation of the scene. To utilize standard computer vision algorithms, events must be integrated into a frame or event-surface. Typically, this involves using hand-crafted grids and ad-hoc heuristics to reconstruct the frame. In this paper, we propose Matrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that efficiently process events and learn task-dependent event-surfaces in an end-to-end manner. Our approach outperforms existing reconstruction methods on optical flow estimation in the MVSEC benchmark and improves the state-of-the-art of event-based object classification on the N-Cars dataset, while also providing greater flexibility and expressiveness.",1
"Particle Image Velocimetry (PIV) is a classical flow estimation problem which is widely considered and utilised, especially as a diagnostic tool in experimental fluid dynamics and the remote sensing of environmental flows. Recently, the development of deep learning based methods has inspired new approaches to tackle the PIV problem. These supervised learning based methods are driven by large volumes of data with ground truth training information. However, it is difficult to collect reliable ground truth data in large-scale, real-world scenarios. Although synthetic datasets can be used as alternatives, the gap between the training set-ups and real-world scenarios limits applicability. We present here what we believe to be the first work which takes an unsupervised learning based approach to tackle PIV problems. The proposed approach is inspired by classic optical flow methods. Instead of using ground truth data, we make use of photometric loss between two consecutive image frames, consistency loss in bidirectional flow estimates and spatial smoothness loss to construct the total unsupervised loss function. The approach shows significant potential and advantages for fluid flow estimation. Results presented here demonstrate that our method outputs competitive results compared with classical PIV methods as well as supervised learning based methods for a broad PIV dataset, and even outperforms these existing approaches in some difficult flow cases. Codes and trained models are available at https://github.com/erizmr/UnLiteFlowNet-PIV.",0
"PIV, a well-known flow estimation problem, is commonly used in experimental fluid dynamics and environmental flow remote sensing as a diagnostic tool. Recently, deep learning has led to new methods for tackling this problem. However, these supervised learning methods rely on large data volumes with ground truth training information, which is difficult to collect in real-world scenarios. Synthetic datasets can be used, but their limited applicability due to differences with real-world set-ups restricts their use. We present an unsupervised learning based approach, inspired by classic optical flow methods, which does not rely on ground truth data. Our approach uses photometric loss, consistency loss in bidirectional flow estimates, and spatial smoothness loss to construct the total unsupervised loss function. This approach shows significant potential and advantages for fluid flow estimation, as demonstrated by our competitive results with classical PIV methods and supervised learning based methods for a broad PIV dataset. In some difficult flow cases, our method even outperforms existing approaches. We have made our codes and trained models available at https://github.com/erizmr/UnLiteFlowNet-PIV.",1
"Many researches have been carried out for change detection using temporal SAR images. In this paper an algorithm for change detection using SAR videos has been proposed. There are various challenges related to SAR videos such as high level of speckle noise, rotation of SAR image frames of the video around a particular axis due to the circular movement of airborne vehicle, non-uniform back scattering of SAR pulses. Hence conventional change detection algorithms used for optical videos and SAR temporal images cannot be directly utilized for SAR videos. We propose an algorithm which is a combination of optical flow calculation using Lucas Kanade (LK) method and blob detection. The developed method follows a four steps approach: image filtering and enhancement, applying LK method, blob analysis and combining LK method with blob analysis. The performance of the developed approach was tested on SAR videos available on Sandia National Laboratories website and SAR videos generated by a SAR simulator.",0
"Numerous studies have been conducted to detect changes through the use of temporal SAR images. This paper introduces a new algorithm for change detection using SAR videos. However, SAR videos present several challenges, including a high level of speckle noise, rotation of SAR image frames due to the circular movement of the airborne vehicle, and non-uniform backscattering of SAR pulses. As a result, conventional change detection algorithms for optical videos and SAR temporal images cannot be directly applied to SAR videos. To address this, we propose a four-step approach combining optical flow calculation using the Lucas Kanade (LK) method with blob detection. The approach involves image filtering and enhancement, application of the LK method, blob analysis, and combining the LK method with blob analysis. We tested the performance of this approach using SAR videos from the Sandia National Laboratories website and SAR videos generated by a SAR simulator.",1
"Video super-resolution (VSR) aims to utilize multiple low-resolution frames to generate a high-resolution prediction for each frame. In this process, inter- and intra-frames are the key sources for exploiting temporal and spatial information. However, there are a couple of limitations for existing VSR methods. First, optical flow is often used to establish temporal correspondence. But flow estimation itself is error-prone and affects recovery results. Second, similar patterns existing in natural images are rarely exploited for the VSR task. Motivated by these findings, we propose a temporal multi-correspondence aggregation strategy to leverage similar patches across frames, and a cross-scale nonlocal-correspondence aggregation scheme to explore self-similarity of images across scales. Based on these two new modules, we build an effective multi-correspondence aggregation network (MuCAN) for VSR. Our method achieves state-of-the-art results on multiple benchmark datasets. Extensive experiments justify the effectiveness of our method.",0
"The objective of Video super-resolution (VSR) is to create high-resolution predictions for each frame by utilizing multiple low-resolution frames. To achieve this, temporal and spatial information from inter- and intra-frames is vital. However, current VSR techniques have some limitations. Firstly, optical flow is used to establish temporal correspondence, but its estimation is unreliable and can negatively impact the recovery outcome. Secondly, natural image patterns that could be beneficial for VSR are not often exploited. In light of these challenges, we propose a new method called the temporal multi-correspondence aggregation strategy, which employs similar patches across frames. Additionally, we use a cross-scale nonlocal-correspondence aggregation scheme to explore self-similarity of images across scales. Our approach, the multi-correspondence aggregation network (MuCAN), is built on these two new modules and has shown to produce superior results compared to existing methods when tested on various benchmark datasets. We have conducted extensive experiments that validate the effectiveness of our approach.",1
"Video style transfer techniques inspire many exciting applications on mobile devices. However, their efficiency and stability are still far from satisfactory. To boost the transfer stability across frames, optical flow is widely adopted, despite its high computational complexity, e.g. occupying over 97% inference time. This paper proposes to learn a lightweight video style transfer network via knowledge distillation paradigm. We adopt two teacher networks, one of which takes optical flow during inference while the other does not. The output difference between these two teacher networks highlights the improvements made by optical flow, which is then adopted to distill the target student network. Furthermore, a low-rank distillation loss is employed to stabilize the output of student network by mimicking the rank of input videos. Extensive experiments demonstrate that our student network without an optical flow module is still able to generate stable video and runs much faster than the teacher network.",0
"The use of video style transfer techniques has sparked interest in many potential mobile device applications. However, their effectiveness and reliability are currently unsatisfactory. To enhance the transfer stability between frames, optical flow is commonly used, even though it requires a high amount of computational complexity, taking up over 97% inference time. In this study, we propose a lightweight video style transfer network that is learned through the knowledge distillation approach. Two teacher networks are adopted, one of which uses optical flow during inference while the other does not. By comparing the output of these two teacher networks, we can identify the improvements made by optical flow, which are then used to teach the target student network. Additionally, we employ a low-rank distillation loss to increase the stability of the student network output by mimicking the rank of input videos. Our extensive experiments demonstrate that, even without the optical flow module, our student network can generate stable video and runs much faster than the teacher network.",1
"Novel view synthesis often needs the paired data from both the source and target views. This paper proposes a view translation model under cVAE-GAN framework without requiring the paired data. We design a conditional deformable module (CDM) which uses the view condition vectors as the filters to convolve the feature maps of the main branch in VAE. It generates several pairs of displacement maps to deform the features, like the 2D optical flows. The results are fed into the deformed feature based normalization module (DFNM), which scales and offsets the main branch feature, given its deformed one as the input from the side branch. Taking the advantage of the CDM and DFNM, the encoder outputs a view-irrelevant posterior, while the decoder takes the code drawn from it to synthesize the reconstructed and the viewtranslated images. To further ensure the disentanglement between the views and other factors, we add adversarial training on the code. The results and ablation studies on MultiPIE and 3D chair datasets validate the effectiveness of the framework in cVAE and the designed module.",0
"This article introduces a new method for view synthesis that does not require paired data from both the source and target views. The proposed approach is based on a view translation model within the cVAE-GAN framework. The model includes a conditional deformable module (CDM) that uses view condition vectors as filters to convolve feature maps, producing displacement maps that deform features like 2D optical flows. These results are then fed into the deformed feature based normalization module (DFNM), which scales and offsets the main branch feature. By taking advantage of CDM and DFNM, the encoder outputs a view-irrelevant posterior, while the decoder uses the code drawn from it to synthesize both the reconstructed and view-translated images. To ensure disentanglement between views and other factors, adversarial training is added to the code. Experiments and ablation studies on MultiPIE and 3D chair datasets confirm the effectiveness of the proposed approach within the cVAE framework and the designed module.",1
"Cost volume is an essential component of recent deep models for optical flow estimation and is usually constructed by calculating the inner product between two feature vectors. However, the standard inner product in the commonly-used cost volume may limit the representation capacity of flow models because it neglects the correlation among different channel dimensions and weighs each dimension equally. To address this issue, we propose a learnable cost volume (LCV) using an elliptical inner product, which generalizes the standard inner product by a positive definite kernel matrix. To guarantee its positive definiteness, we perform spectral decomposition on the kernel matrix and re-parameterize it via the Cayley representation. The proposed LCV is a lightweight module and can be easily plugged into existing models to replace the vanilla cost volume. Experimental results show that the LCV module not only improves the accuracy of state-of-the-art models on standard benchmarks, but also promotes their robustness against illumination change, noises, and adversarial perturbations of the input signals.",0
"In current deep models for optical flow estimation, cost volume is a crucial aspect that is often developed by computing the inner product of two feature vectors. However, this standard inner product in the prevalent cost volume can limit the flow model's representation capacity as it disregards the correlation among different channel dimensions and treats each dimension equally. To tackle this problem, we present a learnable cost volume (LCV) that employs an elliptical inner product, which extends the standard inner product with a positive definite kernel matrix. To ensure its positive definiteness, we perform spectral decomposition on the kernel matrix and re-parameterize it via the Cayley representation. The LCV module is lightweight and can be easily integrated into existing models to replace the vanilla cost volume. Our experimental findings demonstrate that the LCV module enhances the accuracy of cutting-edge models on standard benchmarks and enhances their resilience against changes in illumination, noises, and adversarial perturbations in the input signals.",1
"Recently, researchers in Machine Learning algorithms, Computer Vision scientists, engineers and others, showed a growing interest in 3D simulators as a mean to artificially create experimental settings that are very close to those in the real world. However, most of the existing platforms to interface algorithms with 3D environments are often designed to setup navigation-related experiments, to study physical interactions, or to handle ad-hoc cases that are not thought to be customized, sometimes lacking a strong photorealistic appearance and an easy-to-use software interface. In this paper, we present a novel platform, SAILenv, that is specifically designed to be simple and customizable, and that allows researchers to experiment visual recognition in virtual 3D scenes. A few lines of code are needed to interface every algorithm with the virtual world, and non-3D-graphics experts can easily customize the 3D environment itself, exploiting a collection of photorealistic objects. Our framework yields pixel-level semantic and instance labeling, depth, and, to the best of our knowledge, it is the only one that provides motion-related information directly inherited from the 3D engine. The client-server communication operates at a low level, avoiding the overhead of HTTP-based data exchanges. We perform experiments using a state-of-the-art object detector trained on real-world images, showing that it is able to recognize the photorealistic 3D objects of our environment. The computational burden of the optical flow compares favourably with the estimation performed using modern GPU-based convolutional networks or more classic implementations. We believe that the scientific community will benefit from the easiness and high-quality of our framework to evaluate newly proposed algorithms in their own customized realistic conditions.",0
"In recent times, there has been a growing interest among Machine Learning algorithm researchers, Computer Vision scientists, engineers, and others in the use of 3D simulators to artificially create experimental settings that closely resemble real-world scenarios. However, the majority of existing platforms for interfacing algorithms with 3D environments are typically designed for navigation-related experiments, physical interaction studies, or ad-hoc cases without customization options, and often lack photorealistic appearance and user-friendly software interfaces. This paper presents a new platform, SAILenv, specifically designed to be customizable and easy to use, allowing researchers to experiment with visual recognition in virtual 3D scenes. With just a few lines of code, algorithms can be easily interfaced with the virtual world, and non-experts in 3D graphics can easily customize the 3D environment using a variety of photorealistic objects. Our framework provides pixel-level semantic and instance labeling, depth, and motion-related information inherited directly from the 3D engine. The client-server communication operates at a low level, avoiding the overhead of HTTP-based data exchanges. We conducted experiments using a state-of-the-art object detector trained on real-world images, demonstrating its ability to recognize photorealistic 3D objects in our environment. The computational burden of the optical flow is favorable compared to modern GPU-based convolutional networks or classic implementations. We believe that the scientific community will benefit from the ease and high quality of our framework to evaluate newly proposed algorithms in their own customized realistic conditions.",1
"Motion plays a crucial role in understanding videos and most state-of-the-art neural models for video classification incorporate motion information typically using optical flows extracted by a separate off-the-shelf method. As the frame-by-frame optical flows require heavy computation, incorporating motion information has remained a major computational bottleneck for video understanding. In this work, we replace external and heavy computation of optical flows with internal and light-weight learning of motion features. We propose a trainable neural module, dubbed MotionSqueeze, for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate that the proposed method provides a significant gain on four standard benchmarks for action recognition with only a small amount of additional cost, outperforming the state of the art on Something-Something-V1&V2 datasets.",0
"Understanding videos relies heavily on motion, and modern neural models for video classification incorporate motion information using optical flows extracted through a separate, computationally-intensive method. However, processing optical flows frame-by-frame remains a significant computational bottleneck. This study proposes MotionSqueeze, a trainable neural module that extracts motion features internally and efficiently, without relying on external optical flow computation. MotionSqueeze establishes correspondences across frames and converts them into motion features, which are then fed to the next downstream layer for improved prediction. The proposed method outperforms the state of the art on Something-Something-V1&V2 datasets and provides significant gains on four standard benchmarks for action recognition, with only a small increase in cost.",1
"Supervised learning in large discriminative models is a mainstay for modern computer vision. Such an approach necessitates investing in large-scale human-annotated datasets for achieving state-of-the-art results. In turn, the efficacy of supervised learning may be limited by the size of the human annotated dataset. This limitation is particularly notable for image segmentation tasks, where the expense of human annotation is especially large, yet large amounts of unlabeled data may exist. In this work, we ask if we may leverage semi-supervised learning in unlabeled video sequences and extra images to improve the performance on urban scene segmentation, simultaneously tackling semantic, instance, and panoptic segmentation. The goal of this work is to avoid the construction of sophisticated, learned architectures specific to label propagation (e.g., patch matching and optical flow). Instead, we simply predict pseudo-labels for the unlabeled data and train subsequent models with both human-annotated and pseudo-labeled data. The procedure is iterated for several times. As a result, our Naive-Student model, trained with such simple yet effective iterative semi-supervised learning, attains state-of-the-art results at all three Cityscapes benchmarks, reaching the performance of 67.8% PQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notable step towards building a simple procedure to harness unlabeled video sequences and extra images to surpass state-of-the-art performance on core computer vision tasks.",0
"Modern computer vision heavily relies on supervised learning for large discriminative models, which requires investing in human-annotated datasets to achieve the best possible results. However, the efficacy of this approach is limited by the size of the annotated dataset, especially for image segmentation tasks that are expensive to annotate. To address this, we explore the potential of semi-supervised learning using unlabeled video sequences and extra images to improve urban scene segmentation performance for semantic, instance, and panoptic segmentation. Instead of using complex architectures for label propagation, we predict pseudo-labels for the unlabeled data and train subsequent models with both human-annotated and pseudo-labeled data. This iterative process using simple yet effective semi-supervised learning resulted in our Naive-Student model achieving state-of-the-art results in all three Cityscapes benchmarks, with a performance of 67.8% PQ, 42.6% AP, and 85.2% mIOU on the test set. This work demonstrates the potential of a straightforward approach to leverage unlabeled video sequences and extra images to surpass state-of-the-art performance in core computer vision tasks.",1
"Single encoder-decoder methodologies for semantic segmentation are reaching their peak in terms of segmentation quality and efficiency per number of layers. To address these limitations, we propose a new architecture based on a decoder which uses a set of shallow networks for capturing more information content. The new decoder has a new topology of skip connections, namely backward and stacked residual connections. In order to further improve the architecture we introduce a weight function which aims to re-balance classes to increase the attention of the networks to under-represented objects. We carried out an extensive set of experiments that yielded state-of-the-art results for the CamVid, Gatech and Freiburg Forest datasets. Moreover, to further prove the effectiveness of our decoder, we conducted a set of experiments studying the impact of our decoder to state-of-the-art segmentation techniques. Additionally, we present a set of experiments augmenting semantic segmentation with optical flow information, showing that motion clues can boost pure image based semantic segmentation approaches.",0
"Semantic segmentation using single encoder-decoder methodologies has reached a peak in terms of segmentation quality and efficiency per number of layers. To overcome these limitations, we propose a novel architecture that uses a decoder with a set of shallow networks to capture more information content. Our new decoder has a unique topology of skip connections, including backward and stacked residual connections. To improve the architecture further, we introduce a weight function that rebalances classes to increase the networks' attention to under-represented objects. We conducted extensive experiments that yielded state-of-the-art results for the CamVid, Gatech, and Freiburg Forest datasets. Furthermore, we studied the impact of our decoder on state-of-the-art segmentation techniques and presented experiments that integrate optical flow information to augment semantic segmentation, showing that motion clues can enhance image-based semantic segmentation approaches.",1
"In this work we review the coarse-to-fine spatial feature pyramid concept, which is used in state-of-the-art optical flow estimation networks to make exploration of the pixel flow search space computationally tractable and efficient. Within an individual pyramid level, we improve the cost volume construction process by departing from a warping- to a sampling-based strategy, which avoids ghosting and hence enables us to better preserve fine flow details. We further amplify the positive effects through a level-specific, loss max-pooling strategy that adaptively shifts the focus of the learning process on under-performing predictions. Our second contribution revises the gradient flow across pyramid levels. The typical operations performed at each pyramid level can lead to noisy, or even contradicting gradients across levels. We show and discuss how properly blocking some of these gradient components leads to improved convergence and ultimately better performance. Finally, we introduce a distillation concept to counteract the issue of catastrophic forgetting and thus preserving knowledge over models sequentially trained on multiple datasets. Our findings are conceptually simple and easy to implement, yet result in compelling improvements on relevant error measures that we demonstrate via exhaustive ablations on datasets like Flying Chairs2, Flying Things, Sintel and KITTI. We establish new state-of-the-art results on the challenging Sintel and KITTI 2012 test datasets, and even show the portability of our findings to different optical flow and depth from stereo approaches.",0
"This work examines the use of coarse-to-fine spatial feature pyramids in optical flow estimation networks. We aim to make the pixel flow search space more manageable by implementing a sampling-based strategy for constructing cost volumes, rather than a warping-based approach. Our approach avoids ghosting and preserves fine flow details, which is further enhanced by a level-specific, loss max-pooling strategy that focuses on underperforming predictions. We also address the issue of noisy and contradicting gradients across pyramid levels by blocking some gradient components, resulting in improved convergence and better performance. To counteract the problem of catastrophic forgetting, we introduce a distillation concept that preserves knowledge over models sequentially trained on multiple datasets. Our approach yields significant improvements on relevant error measures, as demonstrated through exhaustive ablations on datasets like Flying Chairs2, Flying Things, Sintel, and KITTI. We establish new state-of-the-art results on the challenging Sintel and KITTI 2012 test datasets, and our findings are applicable to different optical flow and depth from stereo approaches.",1
"Deep learning approaches have achieved great success in addressing the problem of optical flow estimation. The keys to success lie in the use of cost volume and coarse-to-fine flow inference. However, the matching problem becomes ill-posed when partially occluded or homogeneous regions exist in images. This causes a cost volume to contain outliers and affects the flow decoding from it. Besides, the coarse-to-fine flow inference demands an accurate flow initialization. Ambiguous correspondence yields erroneous flow fields and affects the flow inferences in subsequent levels. In this paper, we introduce LiteFlowNet3, a deep network consisting of two specialized modules, to address the above challenges. (1) We ameliorate the issue of outliers in the cost volume by amending each cost vector through an adaptive modulation prior to the flow decoding. (2) We further improve the flow accuracy by exploring local flow consistency. To this end, each inaccurate optical flow is replaced with an accurate one from a nearby position through a novel warping of the flow field. LiteFlowNet3 not only achieves promising results on public benchmarks but also has a small model size and a fast runtime.",0
"The problem of optical flow estimation has been successfully addressed by deep learning methods which employ cost volume and coarse-to-fine flow inference. However, the presence of partially occluded or homogeneous regions in images can cause matching problems leading to outliers in the cost volume and inaccurate flow decoding. Additionally, accurate flow initialization is required for coarse-to-fine flow inference, as ambiguous correspondence can result in erroneous flow fields and negatively impact subsequent flow inferences. To alleviate these challenges, we propose LiteFlowNet3, a deep network with two specialized modules. Firstly, we address the issue of outliers in the cost volume by using adaptive modulation to amend each cost vector prior to flow decoding. Secondly, we enhance flow accuracy by exploring local flow consistency through a novel warping of the flow field. LiteFlowNet3 achieves promising results on public benchmarks, with the added benefits of a small model size and fast runtime.",1
"This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/~shenx/RANSAC-Flow/",0
"This article explores the general issue of dense alignment between two images, regardless of whether they are two frames in a video, two vastly different views of a scene, or two paintings depicting similar content. Although these tasks are usually tackled with specialized solutions, the authors demonstrate that a straightforward unsupervised approach can perform surprisingly well across a range of tasks. The authors' primary insight is that parametric and non-parametric alignment methods have complementary strengths, and they propose a two-stage process that involves feature-based parametric coarse alignment and non-parametric fine pixel-wise alignment. The coarse alignment employs RANSAC on pre-existing deep features, while the fine alignment is learned in an unsupervised manner by a deep network that optimizes a standard structural similarity metric between the two images, as well as cycle-consistency. Despite its simplicity, this approach shows promising results on a variety of tasks and datasets, including unsupervised optical flow, dense correspondences, two-view geometry estimation, localization, and artwork alignment. The authors offer their code and data at http://imagine.enpc.fr/~shenx/RANSAC-Flow/.",1
"For semantic segmentation, most existing real-time deep models trained with each frame independently may produce inconsistent results for a video sequence. Advanced methods take into considerations the correlations in the video sequence, e.g., by propagating the results to the neighboring frames using optical flow, or extracting the frame representations with other frames, which may lead to inaccurate results or unbalanced latency. In this work, we process efficient semantic video segmentation in a per-frame fashion during the inference process. Different from previous per-frame models, we explicitly consider the temporal consistency among frames as extra constraints during the training process and embed the temporal consistency into the segmentation network. Therefore, in the inference process, we can process each frame independently with no latency, and improve the temporal consistency with no extra computational cost and post-processing. We employ compact models for real-time execution. To narrow the performance gap between compact models and large models, new knowledge distillation methods are designed. Our results outperform previous keyframe based methods with a better trade-off between the accuracy and the inference speed on popular benchmarks, including the Cityscapes and Camvid. The temporal consistency is also improved compared with corresponding baselines which are trained with each frame independently. Code is available at: https://tinyurl.com/segment-video",0
"Many real-time deep models trained for semantic segmentation produce inconsistent results when applied to a video sequence due to the lack of consideration for correlations between frames. Methods that do account for these correlations, such as those that use optical flow or extract frame representations with other frames, often result in inaccurate results or unbalanced latency. In this study, we introduce an approach that performs efficient semantic video segmentation on a per-frame basis during the inference process while explicitly considering temporal consistency between frames as an extra constraint during training. This embedding of temporal consistency into the segmentation network enables us to process each frame independently without additional computational cost or post-processing and improve temporal consistency. Additionally, we use compact models for real-time execution and have designed new knowledge distillation methods to narrow the performance gap between compact and large models. Our approach outperforms previous keyframe-based methods in benchmarks, including Cityscapes and Camvid, with a better trade-off between accuracy and inference speed while also improving temporal consistency compared to corresponding baselines that are trained independently. Our code is available at https://tinyurl.com/segment-video.",1
"We present a novel deep learning architecture for probabilistic future prediction from video. We predict the future semantics, geometry and motion of complex real-world urban scenes and use this representation to control an autonomous vehicle. This work is the first to jointly predict ego-motion, static scene, and the motion of dynamic agents in a probabilistic manner, which allows sampling consistent, highly probable futures from a compact latent space. Our model learns a representation from RGB video with a spatio-temporal convolutional module. The learned representation can be explicitly decoded to future semantic segmentation, depth, and optical flow, in addition to being an input to a learnt driving policy. To model the stochasticity of the future, we introduce a conditional variational approach which minimises the divergence between the present distribution (what could happen given what we have seen) and the future distribution (what we observe actually happens). During inference, diverse futures are generated by sampling from the present distribution.",0
"Our innovative deep learning architecture is designed to predict the future of complex real-world urban scenes in a probabilistic manner using video. We can anticipate the future semantics, geometry, and motion, which is valuable for controlling autonomous vehicles. Our approach is unique in that it is the first to predict ego-motion, static scene, and dynamic agents' motion concurrently using a compact latent space to produce consistent, highly likely futures. Our model uses spatio-temporal convolutional modules to learn a representation from RGB video, which can then be explicitly decoded to future semantic segmentation, depth, and optical flow. This representation is also utilized as an input for a learnt driving policy. To account for the stochastic nature of the future, we introduce a conditional variational technique that minimizes the gap between the present and future distributions. During inference, we generate diverse futures by sampling from the present distribution.",1
"The use of hand gestures can be a useful tool for many applications in the human-computer interaction community. In a broad range of areas hand gesture techniques can be applied specifically in sign language recognition, robotic surgery, etc. In the process of hand gesture recognition, proper detection, and tracking of the moving hand become challenging due to the varied shape and size of the hand. Here the objective is to track the movement of the hand irrespective of the shape, size, and color of the hand. And, for this, a motion template guided by optical flow (OFMT) is proposed. OFMT is a compact representation of the motion information of a gesture encoded into a single image. In the experimentation, different datasets using bare hand with an open palm, and folded palm wearing green-glove are used, and in both cases, we could generate the OFMT images with equal precision. Recently, deep network-based techniques have shown impressive improvements as compared to conventional hand-crafted feature-based techniques. Moreover, in the literature, it is seen that the use of different streams with informative input data helps to increase the performance in the recognition accuracy. This work basically proposes a two-stream fusion model for hand gesture recognition and a compact yet efficient motion template based on optical flow. Specifically, the two-stream network consists of two layers: a 3D convolutional neural network (C3D) that takes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D has shown its efficiency in capturing spatio-temporal information of a video. Whereas OFMT helps to eliminate irrelevant gestures providing additional motion information. Though each stream can work independently, they are combined with a fusion scheme to boost the recognition results. We have shown the efficiency of the proposed two-stream network on two databases.",0
"Hand gestures have many applications in human-computer interaction. These techniques can be used in various areas such as sign language recognition and robotic surgery. However, detecting and tracking the moving hand can be challenging due to the hand's varying shape and size. The objective is to track the hand's movement regardless of its shape, size, and color. To achieve this, a motion template guided by optical flow (OFMT) has been proposed. OFMT is a compact representation of the motion information of a gesture encoded into a single image. In experiments, different datasets using bare hands with an open or folded palm and a green-gloved hand were used, and OFMT images were generated with equal precision. Recent studies have shown that deep network-based techniques outperform conventional hand-crafted feature-based techniques. Using different streams with informative input data can improve recognition accuracy. This work proposes a two-stream fusion model for hand gesture recognition, including a compact yet efficient motion template based on optical flow. The two-stream network consists of a 3D convolutional neural network (C3D) that takes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D efficiently captures spatio-temporal information of a video, while OFMT eliminates irrelevant gestures and provides additional motion information. Although each stream can work independently, they are combined with a fusion scheme to boost the recognition results. The efficiency of the proposed two-stream network has been demonstrated on two databases.",1
"Optical flow is a crucial component of the feature space for early visual processing of dynamic scenes especially in new applications such as self-driving vehicles, drones and autonomous robots. The dynamic vision sensors are well suited for such applications because of their asynchronous, sparse and temporally precise representation of the visual dynamics. Many algorithms proposed for computing visual flow for these sensors suffer from the aperture problem as the direction of the estimated flow is governed by the curvature of the object rather than the true motion direction. Some methods that do overcome this problem by temporal windowing under-utilize the true precise temporal nature of the dynamic sensors. In this paper, we propose a novel multi-scale plane fitting based visual flow algorithm that is robust to the aperture problem and also computationally fast and efficient. Our algorithm performs well in many scenarios ranging from fixed camera recording simple geometric shapes to real world scenarios such as camera mounted on a moving car and can successfully perform event-by-event motion estimation of objects in the scene to allow for predictions of upto 500 ms i.e. equivalent to 10 to 25 frames with traditional cameras.",0
"Optical flow is an essential feature space component for early visual processing of dynamic scenes, particularly in innovative applications like autonomous robots, drones, and self-driving vehicles. Dynamic vision sensors are well-suited for these applications due to their asynchronous, sparse, and temporally precise visual dynamics representation. However, many algorithms for computing visual flow for these sensors face the aperture problem, where the estimated flow direction is determined by the object's curvature rather than the true motion direction. Some methods that overcome this problem underutilize the sensor's true temporal nature. This paper proposes a novel multi-scale plane fitting-based visual flow algorithm that is both robust to the aperture problem and computationally efficient. Our algorithm excels in several scenarios, including fixed camera recording of simple geometric shapes and real-world situations such as a camera mounted on a moving vehicle. It can successfully estimate event-by-event motion in the scene, allowing for predictions of up to 500 ms, equivalent to 10 to 25 frames with traditional cameras.",1
"Transferring existing image-based detectors to the video is non-trivial since the quality of frames is always deteriorated by part occlusion, rare pose, and motion blur. Previous approaches exploit to propagate and aggregate features across video frames by using optical flow-warping. However, directly applying image-level optical flow onto the high-level features might not establish accurate spatial correspondences. Therefore, a novel module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to learn semantic-level correspondences among adjacent frame features accurately. The sampled locations are first randomly initialized, then updated iteratively to find better spatial correspondences guided by detection supervision progressively. Besides, Sparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation (DFA) module are also introduced to model temporal relations and enhance per-frame features, respectively. Without bells and whistles, the proposed method achieves state-of-the-art performance on the ImageNet VID dataset with less computational complexity and real-time speed. Code will be made available at https://github.com/jiangzhengkai/LSTS.",0
"It is difficult to transfer image-based detectors to video due to the degradation of frame quality caused by factors such as part occlusion, rare pose, and motion blur. Previous methods have used optical flow-warping to propagate and aggregate features across frames, but applying this directly to high-level features may not establish accurate spatial correspondences. To address this issue, a new module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to accurately learn semantic-level correspondences among adjacent frame features. The LSTS module utilizes detection supervision to iteratively update sampled locations for better spatial correspondences. Additionally, the proposed method includes the Sparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation (DFA) module to model temporal relations and improve per-frame features, respectively. The method achieves state-of-the-art performance on the ImageNet VID dataset with real-time speed and less computational complexity. The code for this method will be available at https://github.com/jiangzhengkai/LSTS.",1
"Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames.   In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it also enables us to exploit the correlation between people flow and optical flow to further improve the results.   We will demonstrate that we consistently outperform state-of-the-art methods on five benchmark datasets.",0
"Crowd counting methods nowadays use deep networks to estimate people densities in individual images, but only a small number of them utilize the temporal consistency in video sequences. The few that do, only impose weak smoothness constraints across consecutive frames. This paper proposes estimating people flows between consecutive images and inferring people densities from these flows instead of direct regression. This approach enables stronger constraints encoding the conservation of the number of people, leading to improved performance without requiring a more complex architecture. Moreover, it leverages the correlation between people flow and optical flow to further enhance the results. The study shows consistent outperformance of this method over state-of-the-art approaches across five benchmark datasets.",1
"In this paper, we focus on a prediction-based novelty estimation strategy upon the deep reinforcement learning (DRL) framework, and present a flow-based intrinsic curiosity module (FICM) to exploit the prediction errors from optical flow estimation as exploration bonuses. We propose the concept of leveraging motion features captured between consecutive observations to evaluate the novelty of observations in an environment. FICM encourages a DRL agent to explore observations with unfamiliar motion features, and requires only two consecutive frames to obtain sufficient information when estimating the novelty. We evaluate our method and compare it with a number of existing methods on multiple benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. We demonstrate that FICM is favorable to tasks or environments featuring moving objects, which allow FICM to utilize the motion features between consecutive observations. We further ablatively analyze the encoding efficiency of FICM, and discuss its applicable domains comprehensively.",0
"The focus of this paper is on a unique method for estimating novelty in deep reinforcement learning (DRL) called prediction-based novelty estimation. To accomplish this, a flow-based intrinsic curiosity module (FICM) is introduced, which uses prediction errors from optical flow estimation as exploration bonuses. The authors propose using motion features between consecutive observations to assess the novelty of observations in an environment. With FICM, a DRL agent is encouraged to explore observations with unfamiliar motion features, and only requires two consecutive frames to estimate the novelty adequately. The authors evaluate their method on various benchmark environments, including Atari games, Super Mario Bros., and ViZDoom, and demonstrate that FICM works best in environments with moving objects. They also analyze the encoding efficiency of FICM and examine its applicable domains in detail.",1
"The objective of this paper is to recover the original component signals from a mixture audio with the aid of visual cues of the sound sources. Such task is usually referred as visually guided sound source separation. The proposed Cascaded Opponent Filter (COF) framework consists of multiple stages, which recursively refine the source separation. A key element in COF is a novel opponent filter module that identifies and relocates residual components between sources. The system is guided by the appearance and motion of the source, and, for this purpose, we study different representations based on video frames, optical flows, dynamic images, and their combinations. Finally, we propose a Sound Source Location Masking (SSLM) technique, which, together with COF, produces a pixel level mask of the source location. The entire system is trained end-to-end using a large set of unlabelled videos. We compare COF with recent baselines and obtain the state-of-the-art performance in three challenging datasets (MUSIC, A-MUSIC, and A-NATURAL). Project page: https://ly-zhu.github.io/cof-net.",0
"The aim of this article is to use visual cues of sound sources to separate the original component signals from a mixed audio. This process is known as visually guided sound source separation. The proposed framework, Cascaded Opponent Filter (COF), is composed of several stages that progressively refine the source separation. The COF system relies on an innovative opponent filter module that identifies and relocates residual components between sources. To guide the system, we explore different representations based on video frames, optical flows, dynamic images, and their combinations. Additionally, we introduce a Sound Source Location Masking (SSLM) technique, which, in conjunction with COF, generates a pixel level mask of the source location. We train the entire system end-to-end using a substantial amount of unlabelled videos. We compare COF with recent baselines and achieve the best performance in three challenging datasets (MUSIC, A-MUSIC, and A-NATURAL). For further information, visit our project page at https://ly-zhu.github.io/cof-net.",1
"Many semantic events in team sport activities e.g. basketball often involve both group activities and the outcome (score or not). Motion patterns can be an effective means to identify different activities. Global and local motions have their respective emphasis on different activities, which are difficult to capture from the optical flow due to the mixture of global and local motions. Hence it calls for a more effective way to separate the global and local motions. When it comes to the specific case for basketball game analysis, the successful score for each round can be reliably detected by the appearance variation around the basket. Based on the observations, we propose a scheme to fuse global and local motion patterns (MPs) and key visual information (KVI) for semantic event recognition in basketball videos. Firstly, an algorithm is proposed to estimate the global motions from the mixed motions based on the intrinsic property of camera adjustments. And the local motions could be obtained from the mixed and global motions. Secondly, a two-stream 3D CNN framework is utilized for group activity recognition over the separated global and local motion patterns. Thirdly, the basket is detected and its appearance features are extracted through a CNN structure. The features are utilized to predict the success or failure. Finally, the group activity recognition and success/failure prediction results are integrated using the kronecker product for event recognition. Experiments on NCAA dataset demonstrate that the proposed method obtains state-of-the-art performance.",0
"Team sports like basketball involve various semantic events that combine group activities and outcome, often measured by the score. Motion patterns are useful for identifying these events, but it's challenging to distinguish between global and local motions using optical flow. To address this issue, we propose a scheme that combines global and local motion patterns with key visual information for semantic event recognition in basketball videos. First, we use an algorithm to estimate global motions from mixed motions, followed by extracting local motions. We then employ a two-stream 3D CNN framework for group activity recognition and detect the basket to predict success or failure using CNN features. Finally, we integrate group activity recognition and success/failure prediction results using the kronecker product for event recognition. Our experiments on the NCAA dataset show that our method outperforms other approaches.",1
"We present a new lightweight CNN-based algorithm for multi-frame optical flow estimation. Our solution introduces a double recurrence over spatial scale and time through repeated use of a generic ""STaR"" (SpatioTemporal Recurrent) cell. It includes (i) a temporal recurrence based on conveying learned features rather than optical flow estimates; (ii) an occlusion detection process which is coupled with optical flow estimation and therefore uses a very limited number of extra parameters. The resulting STaRFlow algorithm gives state-of-the-art performances on MPI Sintel and Kitti2015 and involves significantly less parameters than all other methods with comparable results.",0
"A novel CNN-based algorithm for estimating multi-frame optical flow is presented, which is lightweight and efficient. Our approach adopts a double recurrence strategy that operates over both spatial and temporal scales. This is achieved by utilizing a SpatioTemporal Recurrent (STaR) cell in a repetitive manner. The STaRFlow algorithm incorporates a temporal recurrence mechanism that leverages learned features instead of optical flow estimates. Furthermore, an occlusion detection process is integrated with optical flow estimation, requiring only a small number of additional parameters. The resulting approach achieves state-of-the-art performance on the MPI Sintel and Kitti2015 datasets, and requires significantly fewer parameters than other comparable methods.",1
"Shopping behaviour analysis through counting and tracking of people in shop-like environments offers valuable information for store operators and provides key insights in the stores layout (e.g. frequently visited spots). Instead of using extra staff for this, automated on-premise solutions are preferred. These automated systems should be cost-effective, preferably on lightweight embedded hardware, work in very challenging situations (e.g. handling occlusions) and preferably work real-time. We solve this challenge by implementing a real-time TensorRT optimized YOLOv3-based pedestrian detector, on a Jetson TX2 hardware platform. By combining the detector with a sparse optical flow tracker we assign a unique ID to each customer and tackle the problem of loosing partially occluded customers. Our detector-tracker based solution achieves an average precision of 81.59% at a processing speed of 10 FPS. Besides valuable statistics, heat maps of frequently visited spots are extracted and used as an overlay on the video stream.",0
"By utilizing people counting and tracking in shop-like environments, store operators can obtain valuable insights into their customers' shopping behavior and the store's layout, such as frequently visited areas. To avoid hiring additional staff, automated on-premise systems are preferred. These systems should be cost-effective, lightweight, work in challenging situations (such as dealing with occlusions), and operate in real-time. Our approach to this challenge involves implementing a real-time TensorRT-optimized YOLOv3-based pedestrian detector on a Jetson TX2 hardware platform. By combining the detector with a sparse optical flow tracker, we can assign a unique ID to each customer and address the issue of partially occluded customers. Our detector-tracker solution achieves an average precision of 81.59% at a processing speed of 10 FPS. In addition to providing valuable statistics, we also extract heat maps of frequently visited spots, which are overlaid on the video stream.",1
"Fast motion feedback is crucial in computer-aided surgery (CAS) on moving tissue. Image-assistance in safety-critical vision applications requires a dense tracking of tissue motion. This can be done using optical flow (OF). Accurate motion predictions at high processing rates lead to higher patient safety. Current deep learning OF models show the common speed vs. accuracy trade-off. To achieve high accuracy at high processing rates, we propose patient-specific fine-tuning of a fast model. This minimizes the domain gap between training and application data, while reducing the target domain to the capability of the lower complex, fast model. We propose to obtain training sequences pre-operatively in the operation room. We handle missing ground truth, by employing teacher-student learning. Using flow estimations from teacher model FlowNet2 we specialize a fast student model FlowNet2S on the patient-specific domain. Evaluation is performed on sequences from the Hamlyn dataset. Our student model shows very good performance after fine-tuning. Tracking accuracy is comparable to the teacher model at a speed up of factor six. Fine-tuning can be performed within minutes, making it feasible for the operation room. Our method allows to use a real-time capable model that was previously not suited for this task. This method is laying the path for improved patient-specific motion estimation in CAS.",0
"In computer-aided surgery, quick feedback on motion is essential, especially when dealing with moving tissue. To ensure safety in critical vision applications, precise tracking of tissue motion is necessary, and optical flow (OF) is commonly used. However, there is often a trade-off between speed and accuracy in current deep learning OF models. To address this issue, we propose fine-tuning a fast model to achieve high accuracy at high processing rates, using patient-specific training data obtained pre-operatively in the operation room. To handle missing ground truth data, we employ teacher-student learning by using flow estimations from a teacher model (FlowNet2) to specialize a fast student model (FlowNet2S) on the patient-specific domain. Our approach yields excellent tracking accuracy comparable to the teacher model but at a six-times faster speed. Fine-tuning can be done quickly, making it feasible for the operation room. Our method paves the way for improved patient-specific motion estimation in computer-aided surgery.",1
"We design and implement an end-to-end system for real-time crime detection in low-light environments. Unlike Closed-Circuit Television, which performs reactively, the Low-Light Environment Neural Surveillance provides real time crime alerts. The system uses a low-light video feed processed in real-time by an optical-flow network, spatial and temporal networks, and a Support Vector Machine to identify shootings, assaults, and thefts. We create a low-light action-recognition dataset, LENS-4, which will be publicly available. An IoT infrastructure set up via Amazon Web Services interprets messages from the local board hosting the camera for action recognition and parses the results in the cloud to relay messages. The system achieves 71.5% accuracy at 20 FPS. The user interface is a mobile app which allows local authorities to receive notifications and to view a video of the crime scene. Citizens have a public app which enables law enforcement to push crime alerts based on user proximity.",0
"Our team has developed and implemented a comprehensive system to detect crimes in low-light environments in real-time. Unlike Closed-Circuit Television, which responds to incidents after they occur, our Low-Light Environment Neural Surveillance system provides immediate crime alerts. By processing a low-light video feed in real-time using an optical-flow network, spatial and temporal networks, and a Support Vector Machine, our system can identify incidents such as shootings, assaults, and thefts. We have also created a publicly available low-light action-recognition dataset called LENS-4. An IoT infrastructure is set up via Amazon Web Services to interpret messages from the local camera board and relay the results to the cloud for further analysis. The system boasts an accuracy rate of 71.5% at 20 FPS. The user interface is a mobile app that provides local authorities with notifications and video footage of the crime scene. Additionally, a public app is available to citizens, allowing law enforcement to send crime alerts based on user proximity.",1
"An object's geocentric pose, defined as the height above ground and orientation with respect to gravity, is a powerful representation of real-world structure for object detection, segmentation, and localization tasks using RGBD images. For close-range vision tasks, height and orientation have been derived directly from stereo-computed depth and more recently from monocular depth predicted by deep networks. For long-range vision tasks such as Earth observation, depth cannot be reliably estimated with monocular images. Inspired by recent work in monocular height above ground prediction and optical flow prediction from static images, we develop an encoding of geocentric pose to address this challenge and train a deep network to compute the representation densely, supervised by publicly available airborne lidar. We exploit these attributes to rectify oblique images and remove observed object parallax to dramatically improve the accuracy of localization and to enable accurate alignment of multiple images taken from very different oblique viewpoints. We demonstrate the value of our approach by extending two large-scale public datasets for semantic segmentation in oblique satellite images. All of our data and code are publicly available.",0
"The geocentric pose of an object, which refers to its height above the ground and its orientation in relation to gravity, is a useful tool for detecting, segmenting, and locating objects in RGBD images. While stereo-computed depth and monocular depth predicted by deep networks can determine height and orientation for close-range vision tasks, they are unreliable for long-range vision tasks such as Earth observation. To address this issue, we have developed a geocentric pose encoding and trained a deep network to compute it using publicly available airborne lidar data. This approach can rectify oblique images, remove object parallax, and improve localization accuracy, enabling precise alignment of multiple images from different viewpoints. Our method has been applied to extend two large-scale public datasets for semantic segmentation in oblique satellite images, and all data and code are available to the public.",1
"Occlusion is an inevitable and critical problem in unsupervised optical flow learning. Existing methods either treat occlusions equally as non-occluded regions or simply remove them to avoid incorrectness. However, the occlusion regions can provide effective information for optical flow learning. In this paper, we present OccInpFlow, an occlusion-inpainting framework to make full use of occlusion regions. Specifically, a new appearance-flow network is proposed to inpaint occluded flows based on the image content. Moreover, a boundary warp is proposed to deal with occlusions caused by displacement beyond image border. We conduct experiments on multiple leading flow benchmark data sets such as Flying Chairs, KITTI and MPI-Sintel, which demonstrate that the performance is significantly improved by our proposed occlusion handling framework.",0
"Unsupervised optical flow learning is plagued by the inevitable and vital problem of occlusion. Current methods either treat occlusions the same as non-occluded regions or eliminate them to avoid errors. Nonetheless, occlusion regions contain valuable information for optical flow learning. This article introduces OccInpFlow, an occlusion-inpainting framework that maximizes the use of occlusion regions. The framework proposes a new appearance-flow network to complete occluded flows based on image content and a boundary warp to address occlusions caused by displacement beyond the image border. Experiments on several renowned flow benchmark data sets including Flying Chairs, KITTI, and MPI-Sintel demonstrate significant improvement in performance due to our proposed occlusion handling framework.",1
"Special cameras that provide useful features for face anti-spoofing are desirable, but not always an option. In this work we propose a method to utilize the difference in dynamic appearance between bona fide and spoof samples by creating artificial modalities from RGB videos. We introduce two types of artificial transforms: rank pooling and optical flow, combined in end-to-end pipeline for spoof detection. We demonstrate that using intermediate representations that contain less identity and fine-grained features increase model robustness to unseen attacks as well as to unseen ethnicities. The proposed method achieves state-of-the-art on the largest cross-ethnicity face anti-spoofing dataset CASIA-SURF CeFA (RGB).",0
"While special cameras that have useful features for face anti-spoofing may be desirable, they are not always available. This study presents an alternative method that utilizes the differences in the dynamic appearance of bona fide and spoof samples. The proposed method involves creating artificial modalities from RGB videos using two types of artificial transforms: rank pooling and optical flow. These transforms are combined in an end-to-end pipeline for more effective spoof detection. The study shows that using intermediate representations that contain fewer identity and fine-grained features can increase the model's robustness to unseen attacks and ethnicities. The proposed method achieves state-of-the-art results on the largest cross-ethnicity face anti-spoofing dataset CASIA-SURF CeFA (RGB).",1
"One of the most relevant tasks in an intelligent vehicle navigation system is the detection of obstacles. It is important that a visual perception system for navigation purposes identifies obstacles, and it is also important that this system can extract essential information that may influence the vehicle's behavior, whether it will be generating an alert for a human driver or guide an autonomous vehicle in order to be able to make its driving decisions. In this paper we present an approach for the identification of obstacles and extraction of class, position, depth and motion information from these objects that employs data gained exclusively from passive vision. We performed our experiments on two different data-sets and the results obtained shown a good efficacy from the use of depth and motion patterns to assess the obstacles' potential threat status.",0
"Detecting obstacles is a crucial task for an intelligent vehicle navigation system. A visual perception system must identify obstacles and extract critical information that can affect the vehicle's behavior, whether it is alerting a human driver or guiding an autonomous vehicle to make driving decisions. This paper proposes an approach that leverages passive vision data to identify obstacles and extract information on their class, position, depth, and motion. We conducted experiments on two data-sets, and our results demonstrate that using depth and motion patterns effectively assesses the potential threat status of obstacles.",1
"Robust and accurate six degree-of-freedom tracking on portable devices remains a challenging problem, especially on small hand-held devices such as smartphones. For improved robustness and accuracy, complementary movement information from an IMU and a camera is often fused. Conventional visual-inertial methods fuse information from IMUs with a sparse cloud of feature points tracked by the device camera. We consider a visually dense approach, where the IMU data is fused with the dense optical flow field estimated from the camera data. Learning-based methods applied to the full image frames can leverage visual cues and global consistency of the flow field to improve the flow estimates. We show how a learning-based optical flow model can be combined with conventional inertial navigation, and how ideas from probabilistic deep learning can aid the robustness of the measurement updates. The practical applicability is demonstrated on real-world data acquired by an iPad in a challenging low-texture environment.",0
"The problem of achieving accurate six degree-of-freedom tracking on portable devices, particularly small devices like smartphones, remains challenging. To enhance accuracy and robustness, movement information from both an IMU and camera is often combined. Traditional visual-inertial approaches fuse data from IMUs with a sparse set of feature points tracked by the camera. In contrast, we propose a visually dense method that fuses IMU data with the dense optical flow field obtained from the camera. By leveraging learning-based techniques applied to the entire image frames, visual cues and global flow field consistency can improve the flow estimates. We demonstrate how a learning-based optical flow model can be combined with conventional inertial navigation, and how probabilistic deep learning can enhance measurement update robustness. Our approach is tested in a low-texture environment using real-world data acquired by an iPad, showing promising practical applicability.",1
"Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited. We propose ViPR, a novel modular architecture for long-term 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state of the art in long-term navigation tasks.",0
"Long-term robot navigation tasks are impacted by positional drift in Visual Odometry (VO), despite the enhancements made by Convolutional Neural Networks (CNNs). However, VO still struggles with challenges such as obstacles in motion, inconsistent observation of features, and limited visual information. Although some recent techniques estimate a 6DoF pose using images or by merging depth maps with optical flow (OF), there is limited research that combines absolute pose regression with OF. Our proposed solution, ViPR, is a novel modular architecture that incorporates both absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) through recurrent layers, taking advantage of temporal information and synergies between the two. Results from experiments on well-known datasets and our own Industry dataset demonstrate that our modular design is superior to state-of-the-art methods in long-term navigation tasks.",1
"Significant progress has been made for estimating optical flow using deep neural networks. Advanced deep models achieve accurate flow estimation often with a considerable computation complexity and time-consuming training processes. In this work, we present a lightweight yet effective model for real-time optical flow estimation, termed FDFlowNet (fast deep flownet). We achieve better or similar accuracy on the challenging KITTI and Sintel benchmarks while being about 2 times faster than PWC-Net. This is achieved by a carefully-designed structure and newly proposed components. We first introduce an U-shape network for constructing multi-scale feature which benefits upper levels with global receptive field compared with pyramid network. In each scale, a partial fully connected structure with dilated convolution is proposed for flow estimation that obtains a good balance among speed, accuracy and number of parameters compared with sequential connected and dense connected structures. Experiments demonstrate that our model achieves state-of-the-art performance while being fast and lightweight.",0
"Deep neural networks have made significant strides in estimating optical flow, although the advanced models can be computationally complex and require time-consuming training processes. Our work introduces a lightweight and effective model for real-time optical flow estimation called FDFlowNet (fast deep flownet). We achieve comparable or better accuracy than PWC-Net on challenging benchmarks such as KITTI and Sintel, while being twice as fast. This is accomplished through a well-designed structure and newly proposed components. We use a U-shape network for constructing multi-scale features, which benefits upper levels with a global receptive field over a pyramid network. For flow estimation, we propose a partial fully connected structure with dilated convolution in each scale, which balances speed, accuracy, and number of parameters compared to sequential and dense connected structures. Our model achieves state-of-the-art performance while remaining fast and lightweight, as demonstrated through experiments.",1
