"Today's image prediction methods struggle to change the locations of objects in a scene, producing blurry images that average over the many positions they might occupy. In this paper, we propose a simple change to existing image similarity metrics that makes them more robust to positional errors: we match the images using optical flow, then measure the visual similarity of corresponding pixels. This change leads to crisper and more perceptually accurate predictions, and can be used with any image prediction network. We apply our method to predicting future frames of a video, where it obtains strong performance with simple, off-the-shelf architectures.",0
"Current techniques for predicting images encounter difficulties in altering the positions of objects within a given scene. This results in blurry images that do not accurately represent the possible locations of the objects. To address this issue, our paper suggests a minor adjustment to the current image similarity metrics. By utilizing optical flow to match images and assessing the visual similarity of corresponding pixels, our proposed modification improves the precision and perceptual accuracy of predictions. Moreover, this modification can be applied to any image prediction network. We demonstrate the effectiveness of our method in predicting future video frames, where it delivers excellent results with basic, readily available architectures.",1
"3D scene flow estimation is a vital tool in perceiving our environment given depth or range sensors. Unlike optical flow, the data is usually sparse and in most cases partially occluded in between two temporal samplings. Here we propose a new scene flow architecture called OGSF-Net which tightly couples the learning for both flow and occlusions between frames. Their coupled symbiosis results in a more accurate prediction of flow in space. Unlike a traditional multi-action network, our unified approach is fused throughout the network, boosting performances for both occlusion detection and flow estimation. Our architecture is the first to gauge the occlusion in 3D scene flow estimation on point clouds. In key datasets such as Flyingthings3D and KITTI, we achieve the state-of-the-art results.",0
"The estimation of 3D scene flow is a crucial element in our ability to perceive our surroundings when using range or depth sensors. In contrast to optical flow, this data is often incomplete and obscured between two temporal samples. Our proposed solution, known as OGSF-Net, presents a new approach to scene flow architecture that combines the learning of flow and occlusions between frames. By doing so, we achieve a more precise prediction of spatial flow. Our method differs from traditional multi-action networks in that we adopt a unified approach that enhances both occlusion detection and flow estimation throughout the network. We are the first to utilize this approach for 3D scene flow estimation on point clouds, and our results on key datasets such as Flyingthings3D and KITTI are state-of-the-art.",1
"Recently, deep-learning based approaches have achieved impressive performance for autonomous driving. However, end-to-end vision-based methods typically have limited interpretability, making the behaviors of the deep networks difficult to explain. Hence, their potential applications could be limited in practice. To address this problem, we propose an interpretable end-to-end vision-based motion planning approach for autonomous driving, referred to as IVMP. Given a set of past surrounding-view images, our IVMP first predicts future egocentric semantic maps in bird's-eye-view space, which are then employed to plan trajectories for self-driving vehicles. The predicted future semantic maps not only provide useful interpretable information, but also allow our motion planning module to handle objects with low probability, thus improving the safety of autonomous driving. Moreover, we also develop an optical flow distillation paradigm, which can effectively enhance the network while still maintaining its real-time performance. Extensive experiments on the nuScenes dataset and closed-loop simulation show that our IVMP significantly outperforms the state-of-the-art approaches in imitating human drivers with a much higher success rate. Our project page is available at https://sites.google.com/view/ivmp.",0
"Autonomous driving has seen impressive advancements through deep-learning based approaches. However, end-to-end vision-based methods have limited interpretability, hindering the explanation of deep network behaviors and potentially limiting practical applications. To address this, we propose a new approach called IVMP, which is an interpretable end-to-end vision-based motion planning solution for autonomous driving. Our IVMP employs a set of past surrounding-view images to predict future egocentric semantic maps for planning self-driving vehicle trajectories. The predicted future semantic maps provide interpretable information and enable our motion planning module to handle low probability objects, improving autonomous driving safety. Additionally, our optical flow distillation paradigm enhances the network while maintaining real-time performance. Our extensive experiments on the nuScenes dataset and closed-loop simulation demonstrate that IVMP significantly outperforms existing approaches and successfully imitates human drivers. More information can be found on our project page at https://sites.google.com/view/ivmp.",1
"The objective of this paper is to perform audio-visual sound source separation, i.e.~to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators. Project page: https://ly-zhu.github.io/self-supervised-motion-representations",0
"The purpose of this paper is to achieve audio-visual sound source separation by separating component audios from a mixture based on sound source videos and locating the source in the input video sequence. Some recent studies have shown impressive audio-visual separation results by using prior knowledge of source type and pre-trained motion detectors, but these models have limitations in certain application domains. In this paper, we propose a two-stage architecture called AMnet that specializes in appearance and motion cues, respectively. The entire system is trained in a self-supervised manner. Additionally, we introduce an AME framework to represent the motions related to sound and an audio-motion transformer architecture for audio and motion feature fusion. Despite not using pre-trained keypoint detectors or optical flow estimators, we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE). The project page can be found at https://ly-zhu.github.io/self-supervised-motion-representations.",1
"Visual sound source separation aims at identifying sound components from a given sound mixture with the presence of visual cues. Prior works have demonstrated impressive results, but with the expense of large multi-stage architectures and complex data representations (e.g. optical flow trajectories). In contrast, we study simple yet efficient models for visual sound separation using only a single video frame. Furthermore, our models are able to exploit the information of the sound source category in the separation process. To this end, we propose two models where we assume that i) the category labels are available at the training time, or ii) we know if the training sample pairs are from the same or different category. The experiments with the MUSIC dataset show that our model obtains comparable or better performance compared to several recent baseline methods. The code is available at https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation",0
"The goal of visual sound source separation is to identify individual sound elements in a given mixture by using visual cues. Previous research has shown impressive results, but often relies on complex and multi-stage architectures, as well as intricate data representations such as optical flow trajectories. Conversely, our study focuses on developing simple and efficient models for visual sound separation using only one video frame. Moreover, our models can take advantage of information regarding the sound source category during the separation process. We propose two models, one that assumes category labels are available during training, and another that assumes training sample pairs are either from the same or different categories. We conducted experiments on the MUSIC dataset and found that our models perform comparably or better than several recent baseline methods. Our code is available at https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation.",1
"Optical flow is the motion of a pixel between at least two consecutive video frames and can be estimated through an end-to-end trainable convolutional neural network. To this end, large training datasets are required to improve the accuracy of optical flow estimation. Our paper presents OmniFlow: a new synthetic omnidirectional human optical flow dataset. Based on a rendering engine we create a naturalistic 3D indoor environment with textured rooms, characters, actions, objects, illumination and motion blur where all components of the environment are shuffled during the data capturing process. The simulation has as output rendered images of household activities and the corresponding forward and backward optical flow. To verify the data for training volumetric correspondence networks for optical flow estimation we train different subsets of the data and test on OmniFlow with and without Test-Time-Augmentation. As a result we have generated 23,653 image pairs and corresponding forward and backward optical flow. Our dataset can be downloaded from: https://mytuc.org/byfs",0
"The estimation of optical flow involves analyzing the motion of a pixel in consecutive video frames and can be achieved using a trainable convolutional neural network. To enhance the accuracy of this process, it is necessary to have access to large training datasets. In our study, we introduce OmniFlow, a novel synthetic omnidirectional human optical flow dataset. By using a rendering engine, we created a 3D indoor environment that mimics naturalistic settings, complete with textured rooms, characters, objects, actions, illumination, and motion blur. During the data capturing process, all components of the environment were shuffled to generate a realistic simulation of household activities, with corresponding forward and backward optical flow. We trained different subsets of the data and tested them on OmniFlow, with and without Test-Time-Augmentation, to verify their accuracy for training volumetric correspondence networks for optical flow estimation. Our dataset includes 23,653 image pairs and their respective forward and backward optical flow, which can be downloaded from the following link: https://mytuc.org/byfs.",1
"A majority of methods for video frame interpolation compute bidirectional optical flow between adjacent frames of a video, followed by a suitable warping algorithm to generate the output frames. However, approaches relying on optical flow often fail to model occlusions and complex non-linear motions directly from the video and introduce additional bottlenecks unsuitable for widespread deployment. We address these limitations with FLAVR, a flexible and efficient architecture that uses 3D space-time convolutions to enable end-to-end learning and inference for video frame interpolation. Our method efficiently learns to reason about non-linear motions, complex occlusions and temporal abstractions, resulting in improved performance on video interpolation, while requiring no additional inputs in the form of optical flow or depth maps. Due to its simplicity, FLAVR can deliver 3x faster inference speed compared to the current most accurate method on multi-frame interpolation without losing interpolation accuracy. In addition, we evaluate FLAVR on a wide range of challenging settings and consistently demonstrate superior qualitative and quantitative results compared with prior methods on various popular benchmarks including Vimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR for video frame interpolation can serve as a useful self-supervised pretext task for action recognition, optical flow estimation, and motion magnification.",0
"Several methods for video frame interpolation rely on bidirectional optical flow between adjacent frames, with a warping algorithm to generate output frames. However, this approach can struggle with occlusions and complex, non-linear motions, leading to bottlenecks and limited deployment. Our solution, FLAVR, utilizes 3D space-time convolutions for end-to-end learning and inference, addressing these challenges without requiring additional inputs such as optical flow or depth maps. FLAVR achieves improved performance in video interpolation, with 3x faster inference speed than the current most accurate method, and consistently superior results in various benchmark tests. Additionally, FLAVR can serve as a self-supervised pretext task for other video-related tasks, including action recognition, optical flow estimation, and motion magnification.",1
"A common strategy to video understanding is to incorporate spatial and motion information by fusing features derived from RGB frames and optical flow. In this work, we introduce a new way to leverage semantic segmentation as an intermediate representation for video understanding and use it in a way that requires no additional labeling.   Second, we propose a general framework which learns the intermediate representations (optical flow and semantic segmentation) jointly with the final video understanding task and allows the adaptation of the representations to the end goal. Despite the use of intermediate representations within the network, during inference, no additional data beyond RGB sequences is needed, enabling efficient recognition with a single network.   Finally, we present a way to find the optimal learning configuration by searching the best loss weighting via evolution. We obtain more powerful visual representations for videos which lead to performance gains over the state-of-the-art.",0
"One commonly used approach in video understanding involves merging spatial and motion information obtained from RGB frames and optical flow. However, we propose a novel method that utilizes semantic segmentation as an intermediate representation to improve video understanding without requiring additional labeling. Our framework allows for joint learning of intermediate representations (such as optical flow and semantic segmentation) and the final video understanding task, with the ability to adapt to specific end goals. Moreover, our approach requires no extra data beyond the RGB sequences during inference, which makes recognition more efficient using a single network. Additionally, we introduce a method to discover the optimal learning configuration using evolutionary search to determine the best loss weighting. Our approach results in more powerful visual representations for videos, surpassing state-of-the-art performance.",1
"We present a dense-indirect SLAM system using external dense optical flows as input. We extend the recent probabilistic visual odometry model VOLDOR [Min et al. CVPR'20], by incorporating the use of geometric priors to 1) robustly bootstrap estimation from monocular capture, while 2) seamlessly supporting stereo and/or RGB-D input imagery. Our customized back-end tightly couples our intermediate geometric estimates with an adaptive priority scheme managing the connectivity of an incremental pose graph. We leverage recent advances in dense optical flow methods to achieve accurate and robust camera pose estimates, while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [https://github.com/htkseason/VOLDOR] operates online at around 15 FPS on a single GTX1080Ti GPU.",0
"An indirect SLAM system that utilizes external dense optical flows as input is introduced in this study. By integrating geometric priors, we enhance the probabilistic visual odometry model VOLDOR [Min et al. CVPR'20] to allow for robust estimation from monocular capture and support stereo and/or RGB-D input imagery. Our customized back-end tightly couples intermediate geometric estimates with an adaptive priority scheme that manages the connectivity of an incremental pose graph. We utilize advanced dense optical flow methods to achieve accurate and robust camera pose estimates while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [https://github.com/htkseason/VOLDOR] can operate online at approximately 15 FPS on a single GTX1080Ti GPU.",1
"We propose a dense indirect visual odometry method taking as input externally estimated optical flow fields instead of hand-crafted feature correspondences. We define our problem as a probabilistic model and develop a generalized-EM formulation for the joint inference of camera motion, pixel depth, and motion-track confidence. Contrary to traditional methods assuming Gaussian-distributed observation errors, we supervise our inference framework under an (empirically validated) adaptive log-logistic distribution model. Moreover, the log-logistic residual model generalizes well to different state-of-the-art optical flow methods, making our approach modular and agnostic to the choice of optical flow estimators. Our method achieved top-ranking results on both TUM RGB-D and KITTI odometry benchmarks. Our open-sourced implementation is inherently GPU-friendly with only linear computational and storage growth.",0
"We suggest a novel approach for visual odometry that employs dense indirect methods and utilizes externally estimated optical flow fields instead of manually crafted feature correspondences. Our approach utilizes a probabilistic model and a generalized-EM formulation to jointly infer camera motion, pixel depth, and motion-track confidence. Unlike traditional methods that assume Gaussian-distributed observation errors, we supervise our inference framework using an empirically validated adaptive log-logistic distribution model. Our approach is modular and agnostic to the choice of optical flow estimators, as the log-logistic residual model generalizes well to various state-of-the-art optical flow methods. Our method outperformed other techniques on the TUM RGB-D and KITTI odometry benchmarks, and our open-sourced implementation is inherently GPU-friendly, with only linear computational and storage growth.",1
"The optical flow estimation has been assessed in various applications. In this paper, we propose a novel method named motion edge structure difference(MESD) to assess estimation errors of optical flow fields on edge of motion objects. We implement comparison experiments for MESD by evaluating five representative optical flow algorithms on four popular benchmarks: MPI Sintel, Middlebury, KITTI 2012 and KITTI 2015. Our experimental results demonstrate that MESD can reasonably and discriminatively assess estimation errors of optical flow fields on motion edge. The results indicate that MESD could be a supplementary metric to existing general assessment metrics for evaluating optical flow algorithms in related computer vision applications.",0
"Various applications have utilized optical flow estimation. This study introduces a new method called motion edge structure difference (MESD) to determine the errors in the estimation of optical flow fields on the edges of motion objects. To test MESD, we conducted comparative experiments by evaluating five commonly used optical flow algorithms on four popular benchmarks: MPI Sintel, Middlebury, KITTI 2012, and KITTI 2015. The experimental findings suggest that MESD can reliably and effectively determine the estimation errors of optical flow fields on motion edges. This indicates that MESD could serve as a useful supplementary metric to existing general assessment metrics for evaluating optical flow algorithms in computer vision applications.",1
"Event cameras are novel vision sensors that sample, in an asynchronous fashion, brightness increments with low latency and high temporal resolution. The resulting streams of events are of high value by themselves, especially for high speed motion estimation. However, a growing body of work has also focused on the reconstruction of intensity frames from the events, as this allows bridging the gap with the existing literature on appearance- and frame-based computer vision. Recent work has mostly approached this problem using neural networks trained with synthetic, ground-truth data. In this work we approach, for the first time, the intensity reconstruction problem from a self-supervised learning perspective. Our method, which leverages the knowledge of the inner workings of event cameras, combines estimated optical flow and the event-based photometric constancy to train neural networks without the need for any ground-truth or synthetic data. Results across multiple datasets show that the performance of the proposed self-supervised approach is in line with the state-of-the-art. Additionally, we propose a novel, lightweight neural network for optical flow estimation that achieves high speed inference with only a minor drop in performance.",0
"Event cameras are a type of vision sensor that captures brightness changes with high temporal resolution and low latency in an asynchronous manner. The resulting stream of events is highly valuable for motion estimation, but researchers have also focused on reconstructing intensity frames from these events to connect with existing computer vision literature. Recent studies have used neural networks trained on synthetic data to solve this problem. However, this work takes a self-supervised learning approach, leveraging knowledge of event camera operations to train neural networks without relying on ground-truth or synthetic data. The proposed method combines optical flow estimation and event-based photometric constancy to achieve state-of-the-art performance across multiple datasets. A novel, lightweight neural network for optical flow estimation is also proposed, which maintains high speed inference with minimal loss in performance.",1
"This paper deals with the scarcity of data for training optical flow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Specifically, we introduce a framework to generate accurate ground-truth optical flow annotations quickly and in large amounts from any readily available single real picture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vectors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical flow field connecting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art optical flow networks achieve superior generalization to unseen real data compared to the same models trained either on annotated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images.",0
"The main focus of this paper is the lack of available data for optical flow network training. It outlines the limitations of current sources, such as labeled synthetic datasets or unlabeled real videos. The paper presents a framework that can generate accurate optical flow annotations in large amounts quickly from a single real picture. The process involves using a monocular depth estimation network to create a point cloud of the scene and simulating camera movement with known motion vectors and rotation angles. This results in a novel view and corresponding optical flow field connecting each pixel in the input image to the new frame. By training optical flow networks with this data, they demonstrate improved generalization to unseen real data compared to models trained on annotated synthetic datasets or unlabeled videos. Combining synthetic images with this data also leads to better specialization.",1
"We present an unsupervised optical flow estimation method by proposing an adaptive pyramid sampling in the deep pyramid network. Specifically, in the pyramid downsampling, we propose an Content Aware Pooling (CAP) module, which promotes local feature gathering by avoiding cross region pooling, so that the learned features become more representative. In the pyramid upsampling, we propose an Adaptive Flow Upsampling (AFU) module, where cross edge interpolation can be avoided, producing sharp motion boundaries. Equipped with these two modules, our method achieves the best performance for unsupervised optical flow estimation on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. Particuarlly, we achieve EPE=1.5 on KITTI 2012 and F1=9.67% KITTI 2015, which outperform the previous state-of-the-art methods by 16.7% and 13.1%, respectively.",0
"Our unsupervised optical flow estimation method utilizes an adaptive pyramid sampling approach within the deep pyramid network. To improve local feature gathering during pyramid downsampling, we introduce the Content Aware Pooling (CAP) module, which avoids cross region pooling and enhances feature representativeness. Additionally, to produce sharp motion boundaries during pyramid upsampling, we propose the Adaptive Flow Upsampling (AFU) module, which eliminates cross edge interpolation. Our method outperforms previous state-of-the-art methods on several leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. Notably, we achieve a EPE of 1.5 on KITTI 2012 and F1 of 9.67% on KITTI 2015, surpassing previous methods by 16.7% and 13.1%, respectively.",1
"Video inpainting aims to fill spatio-temporal ""corrupted"" regions with plausible content. To achieve this goal, it is necessary to find correspondences from neighbouring frames to faithfully hallucinate the unknown content. Current methods achieve this goal through attention, flow-based warping, or 3D temporal convolution. However, flow-based warping can create artifacts when optical flow is not accurate, while temporal convolution may suffer from spatial misalignment. We propose 'Progressive Temporal Feature Alignment Network', which progressively enriches features extracted from the current frame with the feature warped from neighbouring frames using optical flow. Our approach corrects the spatial misalignment in the temporal feature propagation stage, greatly improving visual quality and temporal consistency of the inpainted videos. Using the proposed architecture, we achieve state-of-the-art performance on the DAVIS and FVI datasets compared to existing deep learning approaches. Code is available at https://github.com/MaureenZOU/TSAM.",0
"The objective of video inpainting is to replace corrupt regions in video with believable content. In order to achieve this, it is necessary to locate corresponding regions in adjacent frames to realistically generate the missing content. Different methods such as attention, flow-based warping, and 3D temporal convolution have been used to achieve this goal. However, flow-based warping may produce flaws when the optical flow is inaccurate, while spatial misalignment may be a problem for temporal convolution. To tackle these issues, we have introduced the 'Progressive Temporal Feature Alignment Network'. This method gradually enhances features obtained from the current frame by using features from neighboring frames that are transformed using optical flow. Our approach rectifies the spatial misalignment during the temporal feature propagation stage, leading to a remarkable improvement in the visual quality and temporal consistency of the inpainted videos. The proposed architecture has demonstrated superior performance compared to other deep learning techniques on the DAVIS and FVI datasets. The code is available at https://github.com/MaureenZOU/TSAM.",1
"We propose an unsupervised method for detecting and tracking moving objects in 3D, in unlabelled RGB-D videos. The method begins with classic handcrafted techniques for segmenting objects using motion cues: we estimate optical flow and camera motion, and conservatively segment regions that appear to be moving independently of the background. Treating these initial segments as pseudo-labels, we learn an ensemble of appearance-based 2D and 3D detectors, under heavy data augmentation. We use this ensemble to detect new instances of the ""moving"" type, even if they are not moving, and add these as new pseudo-labels. Our method is an expectation-maximization algorithm, where in the expectation step we fire all modules and look for agreement among them, and in the maximization step we re-train the modules to improve this agreement. The constraint of ensemble agreement helps combat contamination of the generated pseudo-labels (during the E step), and data augmentation helps the modules generalize to yet-unlabelled data (during the M step). We compare against existing unsupervised object discovery and tracking methods, using challenging videos from CATER and KITTI, and show strong improvements over the state-of-the-art.",0
"Our proposed method utilizes unsupervised techniques to detect and track moving objects in unlabelled RGB-D videos. Initially, we apply traditional handcrafted methods to segment objects based on motion cues by estimating optical flow and camera motion and identifying regions that appear to be moving independently from the background. These initial segments serve as pseudo-labels, which are used to train an ensemble of appearance-based 2D and 3D detectors through extensive data augmentation. Our algorithm is an expectation-maximization approach where all modules are activated to identify agreement in the expectation step, and re-training is conducted in the maximization step to improve this agreement. The constraint of ensemble agreement helps counteract contamination of the generated pseudo-labels during the expectation step, while data augmentation assists the modules in generalizing to unlabelled data during the maximization step. We compare our method to existing unsupervised object discovery and tracking techniques using challenging videos from CATER and KITTI and demonstrate significant enhancements over the current state-of-the-art.",1
"We address the problem of scene flow: given a pair of stereo or RGB-D video frames, estimate pixelwise 3D motion. We introduce RAFT-3D, a new deep architecture for scene flow. RAFT-3D is based on the RAFT model developed for optical flow but iteratively updates a dense field of pixelwise SE3 motion instead of 2D motion. A key innovation of RAFT-3D is rigid-motion embeddings, which represent a soft grouping of pixels into rigid objects. Integral to rigid-motion embeddings is Dense-SE3, a differentiable layer that enforces geometric consistency of the embeddings. Experiments show that RAFT-3D achieves state-of-the-art performance. On FlyingThings3D, under the two-view evaluation, we improved the best published accuracy (d < 0.05) from 34.3% to 83.7%. On KITTI, we achieve an error of 5.77, outperforming the best published method (6.31), despite using no object instance supervision. Code is available at https://github.com/princeton-vl/RAFT-3D.",0
"Our focus is on scene flow, specifically, the estimation of 3D motion for each pixel in a pair of stereo or RGB-D video frames. To achieve this, we have developed a new deep architecture called RAFT-3D. While based on the RAFT model for optical flow, RAFT-3D updates a dense field of pixelwise SE3 motion instead of 2D motion. This is made possible through the use of rigid-motion embeddings, which softly group pixels into rigid objects. To ensure geometric consistency of these embeddings, we employ the differentiable Dense-SE3 layer. Our experiments demonstrate that RAFT-3D outperforms existing methods, achieving state-of-the-art accuracy. For instance, on FlyingThings3D, we improved the best published accuracy (d < 0.05) from 34.3% to 83.7%. On KITTI, we achieved an error of 5.77, surpassing the best published method (6.31), despite not using object instance supervision. For those interested, the code is available at https://github.com/princeton-vl/RAFT-3D.",1
"While single-image super-resolution (SISR) has attracted substantial interest in recent years, the proposed approaches are limited to learning image priors in order to add high frequency details. In contrast, multi-frame super-resolution (MFSR) offers the possibility of reconstructing rich details by combining signal information from multiple shifted images. This key advantage, along with the increasing popularity of burst photography, have made MFSR an important problem for real-world applications.   We propose a novel architecture for the burst super-resolution task. Our network takes multiple noisy RAW images as input, and generates a denoised, super-resolved RGB image as output. This is achieved by explicitly aligning deep embeddings of the input frames using pixel-wise optical flow. The information from all frames are then adaptively merged using an attention-based fusion module. In order to enable training and evaluation on real-world data, we additionally introduce the BurstSR dataset, consisting of smartphone bursts and high-resolution DSLR ground-truth. We perform comprehensive experimental analysis, demonstrating the effectiveness of the proposed architecture.",0
"Although single-image super-resolution (SISR) has garnered significant interest, its approaches are limited to learning image priors to add high frequency details. On the other hand, multi-frame super-resolution (MFSR) offers the potential to reconstruct intricate details by combining signal information from multiple shifted images. This advantage, coupled with the growing popularity of burst photography, has made MFSR a crucial issue for practical applications. To address this, we propose a new architecture for burst super-resolution, which takes multiple noisy RAW images as input and generates a denoised, super-resolved RGB image as output. Our approach involves aligning deep embeddings of the input frames using pixel-wise optical flow and merging information from all frames using an attention-based fusion module. To facilitate real-world data training and evaluation, we introduce the BurstSR dataset, which features smartphone bursts and high-resolution DSLR ground-truth. Our comprehensive experimental analysis showcases the effectiveness of our proposed architecture.",1
"Heart beat rhythm and heart rate (HR) are important physiological parameters of the human body. This study presents an efficient multi-hierarchical spatio-temporal convolutional network that can quickly estimate remote physiological (rPPG) signal and HR from face video clips. First, the facial color distribution characteristics are extracted using a low-level face feature Generation (LFFG) module. Then, the three-dimensional (3D) spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) are used to strengthen the spatio-temporal correlation of multi-channel features. In the MHFF, sparse optical flow is used to capture the tiny motion information of faces between frames and generate a self-adaptive region of interest (ROI) skin mask. Finally, the signal prediction module (SP) is used to extract the estimated rPPG signal. The experimental results on the three datasets show that the proposed network outperforms the state-of-the-art methods.",0
"The human body's physiological parameters, including heart beat rhythm and heart rate (HR), are crucial. This research introduces a highly effective multi-hierarchical spatio-temporal convolutional network that rapidly calculates remote physiological (rPPG) signal and HR from facial video clips. Initially, the low-level face feature Generation (LFFG) module extracts facial color distribution characteristics. Then, the spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) enhance the spatio-temporal correlation of multi-channel features. The MHFF employs sparse optical flow to capture slight motion information of faces between frames, creating a self-adaptive region of interest (ROI) skin mask. Lastly, the signal prediction module (SP) extracts the estimated rPPG signal. Experimental results from three datasets demonstrate that the proposed network surpasses state-of-the-art techniques.",1
"State-of-the-art neural network models for optical flow estimation require a dense correlation volume at high resolutions for representing per-pixel displacement. Although the dense correlation volume is informative for accurate estimation, its heavy computation and memory usage hinders the efficient training and deployment of the models. In this paper, we show that the dense correlation volume representation is redundant and accurate flow estimation can be achieved with only a fraction of elements in it. Based on this observation, we propose an alternative displacement representation, named Sparse Correlation Volume, which is constructed directly by computing the k closest matches in one feature map for each feature vector in the other feature map and stored in a sparse data structure. Experiments show that our method can reduce computational cost and memory use significantly, while maintaining high accuracy compared to previous approaches with dense correlation volumes. Code is available at https://github.com/zacjiang/scv .",0
"Advanced neural network models used to estimate optical flow require a dense correlation volume with high resolution in order to accurately represent per-pixel displacement. However, this leads to heavy computation and memory usage, causing the models to be inefficient during training and deployment. This paper demonstrates that accurate flow estimation can be achieved with only a fraction of elements in the dense correlation volume, rendering it redundant. As a result, an alternative displacement representation called Sparse Correlation Volume is proposed, which is created by directly computing the k closest matches in one feature map for each feature vector in the other feature map and stored in a sparse data structure. Experimental results show that this new approach significantly reduces computational cost and memory usage, while maintaining high accuracy compared to previous methods that utilize dense correlation volumes. The source code can be found at https://github.com/zacjiang/scv.",1
"The feature correlation layer serves as a key neural network module in numerous computer vision problems that involve dense correspondences between image pairs. It predicts a correspondence volume by evaluating dense scalar products between feature vectors extracted from pairs of locations in two images. However, this point-to-point feature comparison is insufficient when disambiguating multiple similar regions in an image, severely affecting the performance of the end task. We propose GOCor, a fully differentiable dense matching module, acting as a direct replacement to the feature correlation layer. The correspondence volume generated by our module is the result of an internal optimization procedure that explicitly accounts for similar regions in the scene. Moreover, our approach is capable of effectively learning spatial matching priors to resolve further matching ambiguities. We analyze our GOCor module in extensive ablative experiments. When integrated into state-of-the-art networks, our approach significantly outperforms the feature correlation layer for the tasks of geometric matching, optical flow, and dense semantic matching. The code and trained models will be made available at github.com/PruneTruong/GOCor.",0
"The feature correlation layer is a crucial module in computer vision problems that require dense correspondences between image pairs. It predicts a correspondence volume by evaluating dense scalar products between feature vectors from two images. However, this method fails to differentiate between multiple similar regions in an image, leading to poor performance in the final task. To address this issue, we introduce GOCor, a fully differentiable dense matching module that directly replaces the feature correlation layer. Our module generates a correspondence volume using an internal optimization process that takes similar regions into account. Furthermore, our approach can learn spatial matching priors to resolve additional ambiguities. We extensively analyze our GOCor module through ablative experiments and show that it outperforms the feature correlation layer for tasks such as geometric matching, optical flow, and dense semantic matching when integrated into state-of-the-art networks. The code and trained models are available on github.com/PruneTruong/GOCor.",1
"Establishing dense correspondences between a pair of images is an important and general problem, covering geometric matching, optical flow and semantic correspondences. While these applications share fundamental challenges, such as large displacements, pixel-accuracy, and appearance changes, they are currently addressed with specialized network architectures, designed for only one particular task. This severely limits the generalization capabilities of such networks to new scenarios, where e.g. robustness to larger displacements or higher accuracy is required.   In this work, we propose a universal network architecture that is directly applicable to all the aforementioned dense correspondence problems. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution. The proposed GLU-Net achieves state-of-the-art performance for geometric and semantic matching as well as optical flow, when using the same network and weights. Code and trained models are available at https://github.com/PruneTruong/GLU-Net.",0
"The establishment of dense correspondences between a pair of images is a significant task that encompasses geometric matching, optical flow, and semantic correspondences. Although these applications share common difficulties such as large displacements, pixel accuracy, and appearance changes, they are currently tackled using specialized network architectures that are designed for specific tasks. This limits the networks' ability to generalize to new situations where, for example, robustness to larger displacements or higher accuracy is necessary. This study proposes a universal network architecture that can address all the aforementioned dense correspondence problems. By combining global and local correlation layers, we achieve high accuracy and robustness to large displacements. Additionally, we suggest an adaptive resolution strategy that enables our network to operate on any input image resolution. GLU-Net, the proposed model, achieves state-of-the-art performance in geometric and semantic matching and optical flow, using the same network and weights. We have made the code and trained models available at https://github.com/PruneTruong/GLU-Net.",1
"Semi-supervised video object segmentation (semi-VOS) is widely used in many applications. This task is tracking class-agnostic objects from a given target mask. For doing this, various approaches have been developed based on online-learning, memory networks, and optical flow. These methods show high accuracy but are hard to be utilized in real-world applications due to slow inference time and tremendous complexity. To resolve this problem, template matching methods are devised for fast processing speed but sacrificing lots of performance in previous models. We introduce a novel semi-VOS model based on a template matching method and a temporal consistency loss to reduce the performance gap from heavy models while expediting inference time a lot. Our template matching method consists of short-term and long-term matching. The short-term matching enhances target object localization, while long-term matching improves fine details and handles object shape-changing through the newly proposed adaptive template attention module. However, the long-term matching causes error-propagation due to the inflow of the past estimated results when updating the template. To mitigate this problem, we also propose a temporal consistency loss for better temporal coherence between neighboring frames by adopting the concept of a transition matrix. Our model obtains 79.5% J&F score at the speed of 73.8 FPS on the DAVIS16 benchmark. The code is available in https://github.com/HYOJINPARK/TTVOS.",0
"Semi-VOS is a commonly used technique for video object segmentation. It involves tracking objects that are class-agnostic from a given target mask and has been approached using online-learning, memory networks, and optical flow. However, these methods are complex and slow, making them impractical for real-world applications. To address this issue, template matching methods have been developed. Although they are faster, they lack the performance of previous models. Our team introduces a novel semi-VOS model that combines template matching with a temporal consistency loss to improve performance while maintaining fast inference times. Our template matching method includes short-term and long-term matching, with the latter utilizing an adaptive template attention module to handle object shape-changing. However, long-term matching can cause error-propagation, which we mitigate with a temporal consistency loss based on a transition matrix. Our model achieves a J&F score of 79.5% and runs at 73.8 FPS on the DAVIS16 benchmark. Code for our model is available at https://github.com/HYOJINPARK/TTVOS.",1
"Video interpolation aims to generate a non-existent intermediate frame given the past and future frames. Many state-of-the-art methods achieve promising results by estimating the optical flow between the known frames and then generating the backward flows between the middle frame and the known frames. However, these methods usually suffer from the inaccuracy of estimated optical flows and require additional models or information to compensate for flow estimation errors. Following the recent development in using deformable convolution (DConv) for video interpolation, we propose a light but effective model, called Pyramid Deformable Warping Network (PDWN). PDWN uses a pyramid structure to generate DConv offsets of the unknown middle frame with respect to the known frames through coarse-to-fine successive refinements. Cost volumes between warped features are calculated at every pyramid level to help the offset inference. At the finest scale, the two warped frames are adaptively blended to generate the middle frame. Lastly, a context enhancement network further enhances the contextual detail of the final output. Ablation studies demonstrate the effectiveness of the coarse-to-fine offset refinement, cost volumes, and DConv. Our method achieves better or on-par accuracy compared to state-of-the-art models on multiple datasets while the number of model parameters and the inference time are substantially less than previous models. Moreover, we present an extension of the proposed framework to use four input frames, which can achieve significant improvement over using only two input frames, with only a slight increase in the model size and inference time.",0
"The goal of video interpolation is to create a new frame between two existing frames. While some methods have been successful in estimating the optical flow between frames and generating backward flows, they often suffer from inaccuracies and require additional models to compensate for errors. To address these issues, we propose the Pyramid Deformable Warping Network (PDWN), which uses a pyramid structure to generate Deformable Convolution (DConv) offsets for the middle frame. The PDWN also includes cost volume calculations and adaptive blending to generate the final output. Our approach achieves comparable or better accuracy than previous models while using fewer parameters and less inference time. We also present an extension of our framework using four input frames, which results in significant improvement with only a slight increase in model size and inference time.",1
"We present a deep neural network (DNN) that uses both sensor data (gyroscope) and image content (optical flow) to stabilize videos through unsupervised learning. The network fuses optical flow with real/virtual camera pose histories into a joint motion representation. Next, the LSTM block infers the new virtual camera pose, and this virtual pose is used to generate a warping grid that stabilizes the frame. Novel relative motion representation as well as a multi-stage training process are presented to optimize our model without any supervision. To the best of our knowledge, this is the first DNN solution that adopts both sensor data and image for stabilization. We validate the proposed framework through ablation studies and demonstrated the proposed method outperforms the state-of-art alternative solutions via quantitative evaluations and a user study.",0
"Our study introduces a DNN that applies unsupervised learning to stabilize videos using both gyroscope sensor data and optical flow image content. The DNN combines optical flow with camera pose histories to create a joint motion representation. The LSTM block then predicts a new virtual camera pose, which is used to generate a warping grid that stabilizes the frame. We introduce a novel relative motion representation and a multi-stage training process to optimize the model without supervision. This is the first DNN solution to utilize both sensor data and image for stabilization. Our proposed framework is evaluated through ablation studies, quantitative evaluations, and a user study, all of which demonstrate that our method outperforms existing alternative solutions.",1
"Abnormal event detection is a challenging task that requires effectively handling intricate features of appearance and motion. In this paper, we present an approach of detecting anomalies in videos by learning a novel LSTM based self-contained network on normal dense optical flow. Due to their sigmoid implementations, standard LSTM's forget gate is susceptible to overlooking and dismissing relevant content in long sequence tasks like abnormality detection. The forget gate mitigates participation of previous hidden state for computation of cell state prioritizing current input. In addition, the hyperbolic tangent activation of standard LSTMs sacrifices performance when a network gets deeper. To tackle these two limitations, we introduce a bi-gated, light LSTM cell by discarding the forget gate and introducing sigmoid activation. Specifically, the LSTM architecture we come up with fully sustains content from previous hidden state thereby enabling the trained model to be robust and make context-independent decision during evaluation. Removing the forget gate results in a simplified and undemanding LSTM cell with improved performance effectiveness and computational efficiency. Empirical evaluations show that the proposed bi-gated LSTM based network outperforms various LSTM based models verifying its effectiveness for abnormality detection and generalization tasks on CUHK Avenue and UCSD datasets.",0
"Detecting abnormal events in videos is a complex task that involves managing intricate appearance and motion features. In this article, we introduce a new method for identifying anomalies in videos by teaching a novel LSTM self-contained network to learn from normal dense optical flow. Standard LSTM's forget gate can overlook and disregard relevant content in long sequence tasks like detecting abnormalities because of its sigmoid implementation. The forget gate restricts the previous hidden state's involvement in computing the cell state, prioritizing the current input. Additionally, the hyperbolic tangent activation of standard LSTMs reduces performance as the network gets deeper. To overcome these limitations, we propose a bi-gated, light LSTM cell that eliminates the forget gate and introduces sigmoid activation. The LSTM architecture we develop fully retains content from the previous hidden state, allowing the trained model to be resilient and make context-independent decisions during evaluation. By removing the forget gate, we create a simplified and less demanding LSTM cell with improved performance effectiveness and computational efficiency. Our empirical evaluations show that the proposed bi-gated LSTM-based network surpasses various LSTM-based models, demonstrating its effectiveness for abnormality detection and generalization tasks on CUHK Avenue and UCSD datasets.",1
"The estimation of optical flow is an ambiguous task due to the lack of correspondence at occlusions, shadows, reflections, lack of texture and changes in illumination over time. Thus, unsupervised methods face major challenges as they need to tune complex cost functions with several terms designed to handle each of these sources of ambiguity. In contrast, supervised methods avoid these challenges altogether by relying on explicit ground truth optical flow obtained directly from synthetic or real data. In the case of synthetic data, the ground truth provides an exact and explicit description of what optical flow to assign to a given scene. However, the domain gap between synthetic data and real data often limits the ability of a trained network to generalize. In the case of real data, the ground truth is obtained through multiple sensors and additional data processing, which might introduce persistent errors and contaminate it. As a solution to these issues, we introduce a novel method to build a training set of pseudo-real images that can be used to train optical flow in a supervised manner. Our dataset uses two unpaired frames from real data and creates pairs of frames by simulating random warps, occlusions with super-pixels, shadows and illumination changes, and associates them to their corresponding exact optical flow. We thus obtain the benefit of directly training on real data while having access to an exact ground truth. Training with our datasets on the Sintel and KITTI benchmarks is straightforward and yields models on par or with state of the art performance compared to much more sophisticated training approaches.",0
"The task of estimating optical flow is challenging due to various sources of ambiguity, such as occlusions, shadows, reflections, lack of texture, and changes in illumination over time. Unsupervised methods face significant difficulties in dealing with these complexities since they require tuning complex cost functions with several terms to handle each source of ambiguity individually. On the other hand, supervised methods rely on explicit ground truth optical flow obtained from synthetic or real data to avoid these challenges. Although synthetic data provides an exact description of optical flow, the domain gap between synthetic and real data limits the generalization ability of trained networks. Similarly, obtaining ground truth from real data through multiple sensors and processing can introduce persistent errors. To address these issues, we propose a novel method of constructing a training set of pseudo-real images that can be used to train optical flow in a supervised manner. Our dataset simulates various distortions on unpaired real data frames and associates them with their corresponding ground truth optical flow. This approach allows us to train on real data while having access to exact ground truth. Using our dataset, we achieved state-of-the-art performance on the Sintel and KITTI benchmarks, which is comparable to more complex training approaches.",1
"Establishing dense correspondences between a pair of images is an important and general problem. However, dense flow estimation is often inaccurate in the case of large displacements or homogeneous regions. For most applications and down-stream tasks, such as pose estimation, image manipulation, or 3D reconstruction, it is crucial to know when and where to trust the estimated matches.   In this work, we aim to estimate a dense flow field relating two images, coupled with a robust pixel-wise confidence map indicating the reliability and accuracy of the prediction. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the task of pose estimation. Code and models are available at https://github.com/PruneTruong/PDCNet.",0
"The problem of establishing dense correspondences between two images is crucial, but it is often inaccurate in cases of significant displacements or uniform regions. Therefore, it is important to know when and where to rely on estimated matches for downstream tasks like pose estimation, 3D reconstruction, or image manipulation. Our aim is to estimate a dense flow field between two images while also providing a reliable pixel-wise confidence map showing the accuracy and reliability of the prediction. We have developed a flexible probabilistic approach that learns the flow prediction and its uncertainty together. Our approach includes a constrained mixture model that models accurate flow predictions and outliers, and an architecture and training strategy that are tailored for robust and generalizable uncertainty prediction via self-supervised training. Our approach achieves state-of-the-art results on multiple geometric matching and optical flow datasets, and we demonstrate the usefulness of our probabilistic confidence estimation for the task of pose estimation. Our code and models are available at https://github.com/PruneTruong/PDCNet.",1
"Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction and visual SLAM. Existing deep learning-based approaches formulate the problem by either recovering absolute pose scales from two consecutive frames or predicting a depth map from a single image, both of which are ill-posed problems. In contrast, we propose to revisit the problem of deep two-view SfM by leveraging the well-posedness of the classic pipeline. Our method consists of 1) an optical flow estimation network that predicts dense correspondences between two frames; 2) a normalized pose estimation module that computes relative camera poses from the 2D optical flow correspondences, and 3) a scale-invariant depth estimation network that leverages epipolar geometry to reduce the search space, refine the dense correspondences, and estimate relative depth maps. Extensive experiments show that our method outperforms all state-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth estimation.",0
"The primary method for achieving 3D reconstruction and visual SLAM is through two-view structure-from-motion (SfM). However, current deep learning-based approaches are flawed, as they attempt to solve ill-posed problems by either recovering absolute pose scales or predicting a depth map from a single image. Our proposal is to revisit deep two-view SfM by using the well-posedness of the classic pipeline. Our approach involves an optical flow estimation network to predict dense correspondences, a normalized pose estimation module to compute relative camera poses, and a scale-invariant depth estimation network to refine the dense correspondences, estimate relative depth maps and reduce the search space by leveraging epipolar geometry. Our method has been extensively tested and has outperformed all state-of-the-art two-view SfM methods on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth estimation.",1
"The cost volume, capturing the similarity of possible correspondences across two input images, is a key ingredient in state-of-the-art optical flow approaches. When sampling for correspondences to build the cost volume, a large neighborhood radius is required to deal with large displacements, introducing a significant computational burden. To address this, a sequential strategy is usually adopted, where correspondence sampling in a local neighborhood with a small radius suffices. However, such sequential approaches, instantiated by either a pyramid structure over a deep neural network's feature hierarchy or by a recurrent neural network, are slow due to the inherent need for sequential processing of cost volumes. In this paper, we propose dilated cost volumes to capture small and large displacements simultaneously, allowing optical flow estimation without the need for the sequential estimation strategy. To process the cost volume to get pixel-wise optical flow, existing approaches employ 2D or separable 4D convolutions, which we show either suffer from high GPU memory consumption, inferior accuracy, or large model size. Therefore, we propose using 3D convolutions for cost volume filtering to address these issues. By combining the dilated cost volumes and 3D convolutions, our proposed model DCVNet not only exhibits real-time inference (71 fps on a mid-end 1080ti GPU) but is also compact and obtains comparable accuracy to existing approaches.",0
"Optical flow approaches rely on the cost volume to determine possible correspondences between two input images. However, when sampling for correspondences, a large neighborhood radius is necessary to handle large displacements, which can be computationally burdensome. To address this, sequential strategies are typically used, but they are slow due to the need for sequential processing. In this study, we introduce dilated cost volumes that can capture small and large displacements at the same time, eliminating the need for sequential estimation. Existing approaches use 2D or separable 4D convolutions to process the cost volume to obtain pixel-wise optical flow, but we propose using 3D convolutions to address issues such as high GPU memory consumption, inferior accuracy, or large model size. Our proposed model, DCVNet, combines dilated cost volumes and 3D convolutions, resulting in real-time inference and comparable accuracy to existing approaches.",1
"Denoisers trained with synthetic data often fail to cope with the diversity of unknown noises, giving way to methods that can adapt to existing noise without knowing its ground truth. Previous image-based method leads to noise overfitting if directly applied to video denoisers, and has inadequate temporal information management especially in terms of occlusion and lighting variation, which considerably hinders its denoising performance. In this paper, we propose a general framework for video denoising networks that successfully addresses these challenges. A novel twin sampler assembles training data by decoupling inputs from targets without altering semantics, which not only effectively solves the noise overfitting problem, but also generates better occlusion masks efficiently by checking optical flow consistency. An online denoising scheme and a warping loss regularizer are employed for better temporal alignment. Lighting variation is quantified based on the local similarity of aligned frames. Our method consistently outperforms the prior art by 0.6-3.2dB PSNR on multiple noises, datasets and network architectures. State-of-the-art results on reducing model-blind video noises are achieved. Extensive ablation studies are conducted to demonstrate the significance of each technical components.",0
"Denoisers trained using synthetic data often struggle to deal with unknown noises, which has led to the development of methods that can adapt to existing noise without knowledge of its ground truth. Prior image-based methods have proven ineffective when applied directly to video denoisers due to noise overfitting and inadequate management of temporal information, particularly in terms of occlusion and lighting variation. These challenges are successfully addressed in this paper through a general framework for video denoising networks. A novel twin sampler is utilized to decouple inputs from targets and generate better occlusion masks efficiently by checking optical flow consistency. An online denoising scheme and a warping loss regularizer are employed for better temporal alignment, while lighting variation is quantified based on the local similarity of aligned frames. Our method consistently outperforms prior art by 0.6-3.2dB PSNR on multiple noises, datasets, and network architectures, achieving state-of-the-art results on reducing model-blind video noises. Extensive ablation studies are conducted to demonstrate the significance of each technical component.",1
"Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop. However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learning framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the general content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alternative augmentations or modalities into the broad view such as optical flow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classification benchmarks including UCF101, HMDB51, Kinetics, ESC-50 and AudioSet.",0
"The majority of successful self-supervised learning approaches are trained to align the representations of two independent views from the data. In video, state-of-the-art methods are modeled after image techniques, where the two views are obtained by cropping and augmenting the resulting crop in a similar way. However, these methods neglect a crucial aspect of the video domain, namely time. Our proposed framework, BraVe, addresses this issue by incorporating a narrow temporal window of the video in one view and a broad access to the video content in the other. Our models learn to generalize from the narrow view to the overall content of the video. Additionally, BraVe employs distinct backbones for processing the views, allowing for alternative augmentations or modalities to be introduced into the broad view, such as optical flow, randomly convolved RGB frames, audio, or a combination of these. Our results demonstrate that BraVe outperforms previous state-of-the-art methods in self-supervised representation learning on well-known video and audio classification benchmarks, including UCF101, HMDB51, Kinetics, ESC-50, and AudioSet.",1
"Temporal action localization (TAL) is a fundamental yet challenging task in video understanding. Existing TAL methods rely on pre-training a video encoder through action classification supervision. This results in a task discrepancy problem for the video encoder -- trained for action classification, but used for TAL. Intuitively, end-to-end model optimization is a good solution. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity end-to-end (LoFi) video encoder pre-training method. Instead of always using the full training configurations for TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that end-to-end optimization for the video encoder becomes operable under the memory conditions of a mid-range hardware budget. Crucially, this enables the gradient to flow backward through the video encoder from a TAL loss supervision, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi pre-training approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream ResNet50 based alternatives with expensive optical flow, often by a good margin.",0
"Video understanding involves a challenging task known as temporal action localization (TAL). Existing TAL methods pre-train a video encoder for action classification, leading to a task discrepancy problem for the encoder when used for TAL. While end-to-end model optimization seems intuitive, it is not feasible due to GPU memory constraints when processing long untrimmed videos. To overcome this challenge, we introduce a novel low-fidelity end-to-end (LoFi) video encoder pre-training method. By reducing mini-batch composition in terms of temporal, spatial or spatio-temporal resolution, we enable end-to-end optimization for the video encoder under memory constraints. This allows the gradient to flow backward through the video encoder from a TAL loss supervision, solving the task discrepancy problem and providing more effective feature representations. Our experiments demonstrate that our LoFi pre-training approach can enhance the performance of existing TAL methods, surpassing even two-stream ResNet50 based alternatives with expensive optical flow using a lightweight ResNet18 based video encoder in a single RGB stream.",1
"Recent work demonstrated the lack of robustness of optical flow networks to physical, patch-based adversarial attacks. The possibility to physically attack a basic component of automotive systems is a reason for serious concerns. In this paper, we analyze the cause of the problem and show that the lack of robustness is rooted in the classical aperture problem of optical flow estimation in combination with bad choices in the details of the network architecture. We show how these mistakes can be rectified in order to make optical flow networks robust to physical, patch-based attacks.",0
"A study has revealed that optical flow networks are not strong enough to withstand physical, patch-based adversarial attacks. This is a major concern since it means that an essential component of automotive systems can be attacked. The root cause of this vulnerability is attributed to the classical aperture problem of optical flow estimation and poor choices in the network architecture. This paper discusses how these errors can be corrected to enhance the resilience of optical flow networks against physical, patch-based attacks.",1
"In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used -- atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4\% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at \url{https://github.com/lxtGH/SFSegNets}.",0
"The aim of this research is to develop a fast and precise method for scene parsing. One common technique to enhance performance involves obtaining high resolution feature maps with robust semantic representation. However, the two currently utilized strategies, atrous convolutions and feature pyramid fusion, have notable drawbacks, such as being computationally intensive or ineffective. To address these limitations, we propose a Flow Alignment Module (FAM) inspired by Optical Flow for motion alignment between adjacent video frames. FAM learns Semantic Flow between feature maps of adjacent levels and effectively broadcasts high-level features to high-resolution features in a resource-efficient manner. Integrating our module to a common feature pyramid structure results in superior performance compared to other real-time methods, even on lightweight backbone networks such as ResNet-18. We conducted extensive experiments on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K, and CamVid. Our network achieved 80.4% mIoU on Cityscapes with a frame rate of 26 FPS, making it the first to do so. Our code is available at \url{https://github.com/lxtGH/SFSegNets}.",1
"Despite learning-based visual odometry (VO) has shown impressive results in recent years, the pretrained networks may easily collapse in unseen environments. The large domain gap between training and testing data makes them difficult to generalize to new scenes. In this paper, we propose an online adaptation framework for deep VO with the assistance of scene-agnostic geometric computations and Bayesian inference. In contrast to learning-based pose estimation, our method solves pose from optical flow and depth while the single-view depth estimation is continuously improved with new observations by online learned uncertainties. Meanwhile, an online learned photometric uncertainty is used for further depth and pose optimization by a differentiable Gauss-Newton layer. Our method enables fast adaptation of deep VO networks to unseen environments in a self-supervised manner. Extensive experiments including Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that our method achieves state-of-the-art generalization ability among self-supervised VO methods.",0
"Although learning-based visual odometry (VO) has demonstrated impressive performance in recent years, pretrained networks may easily fail in unfamiliar environments due to the large domain gap between training and testing data. This makes it difficult for them to adapt to new scenes. To address this issue, we propose an online adaptation framework for deep VO that utilizes scene-agnostic geometric computations and Bayesian inference. Our approach differs from learning-based pose estimation as it solves pose from optical flow and depth, and continuously improves single-view depth estimation with new observations using online learned uncertainties. Additionally, a differentiable Gauss-Newton layer optimizes depth and pose through an online learned photometric uncertainty. Our method enables rapid adaptation of deep VO networks to unseen environments in a self-supervised manner. Extensive experiments, including Cityscapes to KITTI and outdoor KITTI to indoor TUM, demonstrate that our method outperforms other self-supervised VO methods and achieves state-of-the-art generalization ability.",1
"We present an approach for high-resolution video frame prediction by conditioning on both past frames and past optical flows. Previous approaches rely on resampling past frames, guided by a learned future optical flow, or on direct generation of pixels. Resampling based on flow is insufficient because it cannot deal with disocclusions. Generative models currently lead to blurry results. Recent approaches synthesis a pixel by convolving input patches with a predicted kernel. However, their memory requirement increases with kernel size. Here, we spatially-displaced convolution (SDC) module for video frame prediction. We learn a motion vector and a kernel for each pixel and synthesize a pixel by applying the kernel at a displaced location in the source image, defined by the predicted motion vector. Our approach inherits the merits of both vector-based and kernel-based approaches, while ameliorating their respective disadvantages. We train our model on 428K unlabelled 1080p video game frames. Our approach produces state-of-the-art results, achieving an SSIM score of 0.904 on high-definition YouTube-8M videos, 0.918 on Caltech Pedestrian videos. Our model handles large motion effectively and synthesizes crisp frames with consistent motion.",0
"We propose a method for predicting high-resolution video frames using a combination of past frames and optical flows. Existing methods either resample past frames based on future optical flows or generate pixels directly, resulting in blurry or insufficient results. Some recent approaches use a predicted kernel to synthesize a pixel by convolving input patches, but this method requires high memory usage. To overcome these limitations, we introduce a spatially-displaced convolution (SDC) module. Our approach learns a motion vector and a kernel for each pixel, which are used to synthesize a pixel by applying the kernel at a displaced location in the source image based on the predicted motion vector. Our method combines the advantages of both vector-based and kernel-based approaches while addressing their drawbacks. We trained our model on 428K unlabelled 1080p video game frames and achieved state-of-the-art results with an SSIM score of 0.904 on high-definition YouTube-8M videos and 0.918 on Caltech Pedestrian videos. Our approach effectively handles large motion and generates crisp frames with consistent motion.",1
"In this paper, we address the problem of estimating dense depth from a sequence of images using deep neural networks. Specifically, we employ a dense-optical-flow network to compute correspondences and then triangulate the point cloud to obtain an initial depth map.Parts of the point cloud, however, may be less accurate than others due to lack of common observations or small parallax. To further increase the triangulation accuracy, we introduce a depth-refinement network (DRN) that optimizes the initial depth map based on the image's contextual cues. In particular, the DRN contains an iterative refinement module (IRM) that improves the depth accuracy over iterations by refining the deep features. Lastly, the DRN also predicts the uncertainty in the refined depths, which is desirable in applications such as measurement selection for scene reconstruction. We show experimentally that our algorithm outperforms state-of-the-art approaches in terms of depth accuracy, and verify that our predicted uncertainty is highly correlated to the actual depth error.",0
"This paper focuses on the challenge of accurately estimating dense depth using deep neural networks from a sequence of images. We utilize a dense-optical-flow network to establish correspondences and generate an initial depth map by triangulating the point cloud. However, some areas of the point cloud may be less precise due to limited observations or minimal parallax. To improve the accuracy of the initial depth map, we introduce a depth-refinement network (DRN) that employs contextual cues from the image. The DRN includes an iterative refinement module (IRM) that enhances depth accuracy by refining deep features over iterations. Additionally, the DRN predicts uncertainty in the refined depths, which is useful in applications like scene reconstruction measurement selection. Our experimental results demonstrate that our algorithm surpasses state-of-the-art methods for depth accuracy, and we confirm that our predicted uncertainty correlates strongly with actual depth error.",1
"Video object detection is a fundamental problem in computer vision and has a wide spectrum of applications. Based on deep networks, video object detection is actively studied for pushing the limits of detection speed and accuracy. To reduce the computation cost, we sparsely sample key frames in video and treat the rest frames are non-key frames; a large and deep network is used to extract features for key frames and a tiny network is used for non-key frames. To enhance the features of non-key frames, we propose a novel short-term feature aggregation method to propagate the rich information in key frame features to non-key frame features in a fast way. The fast feature aggregation is enabled by the freely available motion cues in compressed videos. Further, key frame features are also aggregated based on optical flow. The propagated deep features are then integrated with the directly extracted features for object detection. The feature extraction and feature integration parameters are optimized in an end-to-end manner. The proposed video object detection network is evaluated on the large-scale ImageNet VID benchmark and achieves 77.2\% mAP, which is on-par with state-of-the-art accuracy, at the speed of 30 FPS using a Titan X GPU. The source codes are available at \url{https://github.com/hustvl/LSFA}.",0
"Video object detection is a crucial issue in computer vision with a broad range of practical applications. Researchers are actively exploring deep networks to improve detection speed and precision. To reduce the computation cost, key frames are selected and treated differently from non-key frames, with a large and deep network used to extract features for key frames and a tiny network used for non-key frames. To improve non-key frame features, a new short-term feature aggregation method is proposed to quickly propagate rich information from key frame features to non-key frame features. This is enabled by motion cues in compressed videos, and key frame features are also aggregated based on optical flow. The deep features are then integrated with directly extracted features for object detection, with feature extraction and integration parameters optimized in an end-to-end manner. The proposed network achieves 77.2% mAP on the ImageNet VID benchmark, comparable with state-of-the-art accuracy, at 30 FPS using a Titan X GPU. The source codes are available at \url{https://github.com/hustvl/LSFA}.",1
"Dense optical flow estimation plays a key role in many robotic vision tasks. In the past few years, with the advent of deep learning, we have witnessed great progress in optical flow estimation. However, current networks often consist of a large number of parameters and require heavy computation costs, largely hindering its application on low power-consumption devices such as mobile phones. In this paper, we tackle this challenge and design a lightweight model for fast and accurate optical flow prediction. Our proposed FastFlowNet follows the widely-used coarse-to-fine paradigm with following innovations. First, a new head enhanced pooling pyramid (HEPP) feature extractor is employed to intensify high-resolution pyramid features while reducing parameters. Second, we introduce a new center dense dilated correlation (CDDC) layer for constructing compact cost volume that can keep large search radius with reduced computation burden. Third, an efficient shuffle block decoder (SBD) is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on both synthetic Sintel data and real-world KITTI datasets demonstrate the effectiveness of the proposed approach, which needs only 1/10 computation of comparable networks to achieve on par accuracy. In particular, FastFlowNet only contains 1.37M parameters; and can execute at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images of resolution 1024x436.",0
"Robotic vision tasks heavily rely on dense optical flow estimation, and recent years have seen significant progress in this area due to deep learning. However, current networks are often complex and require high computational costs, making them unsuitable for low power-consumption devices like mobile phones. To address this issue, this paper proposes a lightweight model called FastFlowNet for fast and accurate optical flow prediction. The model uses a new head enhanced pooling pyramid feature extractor to intensify high-resolution pyramid features while reducing parameters and a new center dense dilated correlation layer to construct a compact cost volume that can keep a large search radius with reduced computation burden. Additionally, an efficient shuffle block decoder is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on synthetic Sintel data and real-world KITTI datasets show that FastFlowNet achieves comparable accuracy to other networks but with only 1/10th of the computation needed. FastFlowNet contains 1.37M parameters and can execute at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images of resolution 1024x436.",1
"One-sided facial paralysis causes uneven movements of facial muscles on the sides of the face. Physicians currently assess facial asymmetry in a subjective manner based on their clinical experience. This paper proposes a novel method to provide an objective and quantitative asymmetry score for frontal faces. Our metric has the potential to help physicians for diagnosis as well as monitoring the rehabilitation of patients with one-sided facial paralysis. A deep learning based landmark detection technique is used to estimate style invariant facial landmark points and dense optical flow is used to generate motion maps from a short sequence of frames. Six face regions are considered corresponding to the left and right parts of the forehead, eyes, and mouth. Motion is computed and compared between the left and the right parts of each region of interest to estimate the symmetry score. For testing, asymmetric sequences are synthetically generated from a facial expression dataset. A score equation is developed to quantify symmetry in both symmetric and asymmetric face sequences.",0
"The movement of facial muscles on one side of the face causes an imbalanced appearance, known as one-sided facial paralysis. Currently, physicians evaluate facial asymmetry subjectively relying on clinical experience. However, this paper presents a new approach for measuring asymmetry objectively and quantitatively on frontal faces. This method could aid physicians in diagnosing and monitoring the recovery of individuals with one-sided facial paralysis. The proposed technique utilizes deep learning-based landmark detection to estimate invariant facial landmark points and dense optical flow to create motion maps from a brief sequence of frames. The method focuses on six areas of the face, including the left and right portions of the forehead, eyes, and mouth, and calculates and compares motion between each of these regions to determine the symmetry score. To test this method, asymmetric sequences were synthetically generated from a facial expression dataset, and a score equation was created to measure symmetry in both symmetric and asymmetric face sequences.",1
"Surveillance anomaly detection searches for anomalous events, such as crimes or accidents, among normal scenes. Because it occurs rarely, most training data consists of unlabeled, normal videos, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn reconstructing normal videos and detect anomalies by a failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance or motion, many previous approaches have explicitly separated appearance and motion information--for example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, and a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and find anomalies using out-of-distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling in three benchmarks, especially on the Shanghai Tech Campus (ST) database composed of various anomalies in real-world scenarios.",0
"Surveillance anomaly detection seeks to identify abnormal occurrences, such as accidents or crimes, within normal settings. This task is challenging because the majority of training data is unlabeled, normal video footage. Many existing methods employ autoencoders to learn normal video patterns and detect anomalies by identifying failures to reproduce abnormal scenes. However, previous approaches have separated appearance and motion information, which limits reciprocal representation capabilities. We propose an implicit two-path AE (ITAE) that utilizes two encoders to implicitly model appearance and motion features, and a single decoder to combine them and learn normal video patterns. To learn the tractable likelihoods of ITAE features for complex normal scene distributions, we suggest normal density estimation through normalizing flow (NF)-based generative models for out-of-distribution detection of anomalies. NF models enhance ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling in three benchmarks, particularly on the Shanghai Tech Campus (ST) database, which contains various anomalies in real-world scenarios.",1
"Standard frame-based cameras that sample light intensity frames are heavily impacted by motion blur for high-speed motion and fail to perceive scene accurately when the dynamic range is high. Event-based cameras, on the other hand, overcome these limitations by asynchronously detecting the variation in individual pixel intensities. However, event cameras only provide information about pixels in motion, leading to sparse data. Hence, estimating the overall dense behavior of pixels is difficult. To address such issues associated with the sensors, we present Fusion-FlowNet, a sensor fusion framework for energy-efficient optical flow estimation using both frame- and event-based sensors, leveraging their complementary characteristics. Our proposed network architecture is also a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs) where each network is designed to simultaneously process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations. The method generalizes well across distinct environments (rapid motion and challenging lighting conditions) and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost.",0
"When motion blur is present in high-speed motion, standard frame-based cameras that sample light intensity frames may not accurately perceive the scene, particularly when there is a high dynamic range. Event-based cameras, on the other hand, detect the variation in individual pixel intensities asynchronously and thus overcome these limitations. However, event cameras only provide information about pixels in motion, making it difficult to estimate the overall dense behavior of pixels. To address these issues, we have developed Fusion-FlowNet, a sensor fusion framework that uses both frame- and event-based sensors to estimate optical flow efficiently. Our proposed network architecture is a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs), each designed to process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost while generalizing well across different environments with rapid motion and challenging lighting conditions.",1
"Predicting future frames for robotic surgical video is an interesting, important yet extremely challenging problem, given that the operative tasks may have complex dynamics. Existing approaches on future prediction of natural videos were based on either deterministic models or stochastic models, including deep recurrent neural networks, optical flow, and latent space modeling. However, the potential in predicting meaningful movements of robots with dual arms in surgical scenarios has not been tapped so far, which is typically more challenging than forecasting independent motions of one arm robots in natural scenarios. In this paper, we propose a ternary prior guided variational autoencoder (TPG-VAE) model for future frame prediction in robotic surgical video sequences. Besides content distribution, our model learns motion distribution, which is novel to handle the small movements of surgical tools. Furthermore, we add the invariant prior information from the gesture class into the generation process to constrain the latent space of our model. To our best knowledge, this is the first time that the future frames of dual arm robots are predicted considering their unique characteristics relative to general robotic videos. Experiments demonstrate that our model gains more stable and realistic future frame prediction scenes with the suturing task on the public JIGSAWS dataset.",0
"It is a significant challenge to predict future frames for robotic surgical videos due to the complex dynamics of operative tasks. Previous methods for predicting future natural videos have utilized either deterministic models or stochastic models such as deep recurrent neural networks, optical flow, and latent space modeling. However, predicting meaningful movements of robots with dual arms in surgical scenarios has not been explored, which is typically more difficult than forecasting independent motions of one-arm robots in natural scenarios. In this study, we introduce a ternary prior guided variational autoencoder (TPG-VAE) model to predict future frames in robotic surgical video sequences. Our model learns both content and motion distribution, which is novel for handling small movements of surgical tools. Additionally, we incorporate invariant prior information from the gesture class into the generation process to constrain the latent space of our model. To our knowledge, this is the first time that future frames of dual-arm robots have been predicted considering their unique characteristics relative to general robotic videos. Our experiments demonstrate that our model produces more stable and realistic future frame predictions for the suturing task on the public JIGSAWS dataset.",1
"Accurate fall detection for the assistance of older people is crucial to reduce incidents of deaths or injuries due to falls. Meanwhile, a vision-based fall detection system has shown some significant results to detect falls. Still, numerous challenges need to be resolved. The impact of deep learning has changed the landscape of the vision-based system, such as action recognition. The deep learning technique has not been successfully implemented in vision-based fall detection systems due to the requirement of a large amount of computation power and the requirement of a large amount of sample training data. This research aims to propose a vision-based fall detection system that improves the accuracy of fall detection in some complex environments such as the change of light condition in the room. Also, this research aims to increase the performance of the pre-processing of video images. The proposed system consists of the Enhanced Dynamic Optical Flow technique that encodes the temporal data of optical flow videos by the method of rank pooling, which thereby improves the processing time of fall detection and improves the classification accuracy in dynamic lighting conditions. The experimental results showed that the classification accuracy of the fall detection improved by around 3% and the processing time by 40 to 50ms. The proposed system concentrates on decreasing the processing time of fall detection and improving classification accuracy. Meanwhile, it provides a mechanism for summarizing a video into a single image by using a dynamic optical flow technique, which helps to increase the performance of image pre-processing steps.",0
"Reducing incidents of deaths and injuries due to falls in older people is crucial, and accurate fall detection is necessary for this purpose. Although a vision-based fall detection system has shown significant results, there are several challenges that need to be addressed. The deep learning technique has revolutionized the landscape of vision-based systems, but its successful implementation requires a large amount of computation power and training data. This study proposes a vision-based fall detection system that improves accuracy and processing time in complex environments with changing light conditions. The system uses the Enhanced Dynamic Optical Flow technique that encodes temporal data and rank pooling to improve classification accuracy and processing time. Experimental results showed an increase in classification accuracy by 3% and a decrease in processing time by 40 to 50ms. The proposed system aims to improve fall detection accuracy and reduce processing time while also summarizing a video into a single image using a dynamic optical flow technique to enhance image pre-processing.",1
"Since the wide employment of deep learning frameworks in video salient object detection, the accuracy of the recent approaches has made stunning progress. These approaches mainly adopt the sequential modules, based on optical flow or recurrent neural network (RNN), to learn robust spatiotemporal features. These modules are effective but significantly increase the computational burden of the corresponding deep models. In this paper, to simplify the network and maintain the accuracy, we present a lightweight network tailored for video salient object detection through the spatiotemporal knowledge distillation. Specifically, in the spatial aspect, we combine a saliency guidance feature embedding structure and spatial knowledge distillation to refine the spatial features. In the temporal aspect, we propose a temporal knowledge distillation strategy, which allows the network to learn the robust temporal features through the infer-frame feature encoding and distilling information from adjacent frames. The experiments on widely used video datasets (e.g., DAVIS, DAVSOD, SegTrack-V2) prove that our approach achieves competitive performance. Furthermore, without the employment of the complex sequential modules, the proposed network can obtain high efficiency with 0.01s per frame.",0
"The use of deep learning frameworks for video salient object detection has resulted in remarkable progress in the accuracy of recent approaches. These approaches typically rely on sequential modules based on optical flow or RNN to learn robust spatiotemporal features, which are effective but increase the computational load of deep models significantly. In this study, we introduce a lightweight network designed for video salient object detection through spatiotemporal knowledge distillation. To simplify the network while maintaining accuracy, we combine saliency guidance feature embedding and spatial knowledge distillation to refine spatial features. Additionally, we propose a temporal knowledge distillation strategy to enable the network to learn robust temporal features by encoding infer-frame features and distilling information from adjacent frames. Our approach achieves competitive performance on widely used video datasets (e.g., DAVIS, DAVSOD, SegTrack-V2). Moreover, our proposed network can achieve high efficiency without the need for complex sequential modules, with only 0.01s per frame.",1
"In this study, we propose a self-supervised video denoising method called ""restore-from-restored."" This method fine-tunes a pre-trained network by using a pseudo clean video during the test phase. The pseudo clean video is obtained by applying a noisy video to the baseline network. By adopting a fully convolutional neural network (FCN) as the baseline, we can improve video denoising performance without accurate optical flow estimation and registration steps, in contrast to many conventional video restoration methods, due to the translation equivariant property of the FCN. Specifically, the proposed method can take advantage of plentiful similar patches existing across multiple consecutive frames (i.e., patch-recurrence); these patches can boost the performance of the baseline network by a large margin. We analyze the restoration performance of the fine-tuned video denoising networks with the proposed self-supervision-based learning algorithm, and demonstrate that the FCN can utilize recurring patches without requiring accurate registration among adjacent frames. In our experiments, we apply the proposed method to state-of-the-art denoisers and show that our fine-tuned networks achieve a considerable improvement in denoising performance.",0
"The study introduces a video denoising technique named ""restore-from-restored"" that is self-supervised. During the testing phase, the method fine-tunes a pre-trained network using a pseudo clean video obtained by applying a noisy video to the baseline network. Unlike traditional video restoration methods that require accurate optical flow estimation and registration steps, the method employs a fully convolutional neural network (FCN) as the baseline, which has a translation equivariant property, thus improving video denoising performance. The FCN can take advantage of similar patches across consecutive frames (patch-recurrence) to enhance the performance of the baseline network. The study shows that the proposed self-supervision-based learning algorithm can improve restoration performance without requiring accurate registration among adjacent frames. The proposed method is applied to state-of-the-art denoisers, and the results demonstrate a significant improvement in denoising performance.",1
"Abrupt motion of camera or objects in a scene result in a blurry video, and therefore recovering high quality video requires two types of enhancements: visual enhancement and temporal upsampling. A broad range of research attempted to recover clean frames from blurred image sequences or temporally upsample frames by interpolation, yet there are very limited studies handling both problems jointly. In this work, we present a novel framework for deblurring, interpolating and extrapolating sharp frames from a motion-blurred video in an end-to-end manner. We design our framework by first learning the pixel-level motion that caused the blur from the given inputs via optical flow estimation and then predict multiple clean frames by warping the decoded features with the estimated flows. To ensure temporal coherence across predicted frames and address potential temporal ambiguity, we propose a simple, yet effective flow-based rule. The effectiveness and favorability of our approach are highlighted through extensive qualitative and quantitative evaluations on motion-blurred datasets from high speed videos.",0
"Recovering high quality video requires two types of enhancements, visual enhancement and temporal upsampling, as abrupt camera or object motions in a scene can cause blurriness. While previous research has attempted to recover clear frames from blurred image sequences or temporally upsample frames through interpolation, there are limited studies that address both problems simultaneously. This study presents a new framework for deblurring, interpolating, and extrapolating sharp frames from motion-blurred videos in an end-to-end manner. The framework first learns the pixel-level motion that caused the blur through optical flow estimation, then predicts multiple clear frames by warping the decoded features with the estimated flows. To ensure temporal coherence and address potential temporal ambiguity, the study proposes a flow-based rule. The effectiveness of this approach is demonstrated through qualitative and quantitative evaluations on motion-blurred datasets from high speed videos.",1
"In most of computer vision applications, motion blur is regarded as an undesirable artifact. However, it has been shown that motion blur in an image may have practical interests in fundamental computer vision problems. In this work, we propose a novel framework to estimate optical flow from a single motion-blurred image in an end-to-end manner. We design our network with transformer networks to learn globally and locally varying motions from encoded features of a motion-blurred input, and decode left and right frame features without explicit frame supervision. A flow estimator network is then used to estimate optical flow from the decoded features in a coarse-to-fine manner. We qualitatively and quantitatively evaluate our model through a large set of experiments on synthetic and real motion-blur datasets. We also provide in-depth analysis of our model in connection with related approaches to highlight the effectiveness and favorability of our approach. Furthermore, we showcase the applicability of the flow estimated by our method on deblurring and moving object segmentation tasks.",0
"Motion blur is typically considered an unwanted artifact in computer vision applications. However, recent research has revealed that motion blur in an image can actually be beneficial in certain fundamental computer vision problems. This study introduces a new framework for estimating optical flow from a single motion-blurred image in an end-to-end fashion. The network is designed using transformer networks to learn both globally and locally varying motions from the encoded features of a motion-blurred input. The left and right frame features are decoded without explicit frame supervision. The decoded features are then used to estimate optical flow in a coarse-to-fine manner using a flow estimator network. The proposed approach is evaluated qualitatively and quantitatively through a series of experiments on both synthetic and real motion-blur datasets. A comparative analysis with related approaches highlights the effectiveness and favorability of the proposed approach. Additionally, the study demonstrates the applicability of the flow estimated by the proposed method on deblurring and moving object segmentation tasks.",1
"Video deblurring models exploit consecutive frames to remove blurs from camera shakes and object motions. In order to utilize neighboring sharp patches, typical methods rely mainly on homography or optical flows to spatially align neighboring blurry frames. However, such explicit approaches are less effective in the presence of fast motions with large pixel displacements. In this work, we propose a novel implicit method to learn spatial correspondence among blurry frames in the feature space. To construct distant pixel correspondences, our model builds a correlation volume pyramid among all the pixel-pairs between neighboring frames. To enhance the features of the reference frame, we design a correlative aggregation module that maximizes the pixel-pair correlations with its neighbors based on the volume pyramid. Finally, we feed the aggregated features into a reconstruction module to obtain the restored frame. We design a generative adversarial paradigm to optimize the model progressively. Our proposed method is evaluated on the widely-adopted DVD dataset, along with a newly collected High-Frame-Rate (1000 fps) Dataset for Video Deblurring (HFR-DVD). Quantitative and qualitative experiments show that our model performs favorably on both datasets against previous state-of-the-art methods, confirming the benefit of modeling all-range spatial correspondence for video deblurring.",0
"Models for video deblurring utilize consecutive frames to eliminate blurs caused by camera shakes and object motions. To leverage sharp neighboring patches, existing methods primarily rely on homography or optical flows for spatial alignment of blurry frames. However, these explicit approaches prove less effective when dealing with fast motions involving large pixel displacements. In our work, we propose an innovative implicit method that learns spatial correspondence among blurry frames in the feature space. Our model builds a correlation volume pyramid among all neighboring frames' pixel-pairs to establish distant pixel correspondences. To enhance the reference frame's features, we design a correlative aggregation module that maximizes pixel-pair correlations based on the volume pyramid with its neighbors. Finally, we feed the aggregated features into a reconstruction module to obtain the restored frame. We utilize a generative adversarial paradigm to progressively optimize the model. We test our proposed method on the DVD dataset and a newly collected High-Frame-Rate (1000 fps) Dataset for Video Deblurring (HFR-DVD). The quantitative and qualitative experiments show that our model outperforms previous state-of-the-art methods on both datasets, highlighting the benefits of modeling all-range spatial correspondence for video deblurring.",1
"Learning reliable motion representation between consecutive frames, such as optical flow, has proven to have great promotion to video understanding. However, the TV-L1 method, an effective optical flow solver, is time-consuming and expensive in storage for caching the extracted optical flow. To fill the gap, we propose UF-TSN, a novel end-to-end action recognition approach enhanced with an embedded lightweight unsupervised optical flow estimator. UF-TSN estimates motion cues from adjacent frames in a coarse-to-fine manner and focuses on small displacement for each level by extracting pyramid of feature and warping one to the other according to the estimated flow of the last level. Due to the lack of labeled motion for action datasets, we constrain the flow prediction with multi-scale photometric consistency and edge-aware smoothness. Compared with state-of-the-art unsupervised motion representation learning methods, our model achieves better accuracy while maintaining efficiency, which is competitive with some supervised or more complicated approaches.",0
"The use of optical flow as a reliable motion representation between consecutive frames has been shown to significantly aid in video understanding. However, the current method of using the TV-L1 optical flow solver is both time-consuming and requires a large amount of storage for caching extracted optical flow. To address this issue, we have developed UF-TSN, an innovative end-to-end action recognition technique that includes an embedded, lightweight unsupervised optical flow estimator. UF-TSN uses a coarse-to-fine approach to estimate motion cues from adjacent frames and focuses on small displacements by extracting a pyramid of features and warping them based on the estimated flow from the previous level. Since action datasets lack labeled motion, we have constrained the flow prediction using multi-scale photometric consistency and edge-aware smoothness. Compared to other unsupervised motion representation learning techniques, UF-TSN achieves better accuracy while maintaining efficiency, making it competitive with some supervised or more complex approaches.",1
"Multi-view geometry-based methods dominate the last few decades in monocular Visual Odometry for their superior performance, while they have been vulnerable to dynamic and low-texture scenes. More importantly, monocular methods suffer from scale-drift issue, i.e., errors accumulate over time. Recent studies show that deep neural networks can learn scene depths and relative camera in a self-supervised manner without acquiring ground truth labels. More surprisingly, they show that the well-trained networks enable scale-consistent predictions over long videos, while the accuracy is still inferior to traditional methods because of ignoring geometric information. Building on top of recent progress in computer vision, we design a simple yet robust VO system by integrating multi-view geometry and deep learning on Depth and optical Flow, namely DF-VO. In this work, a) we propose a method to carefully sample high-quality correspondences from deep flows and recover accurate camera poses with a geometric module; b) we address the scale-drift issue by aligning geometrically triangulated depths to the scale-consistent deep depths, where the dynamic scenes are taken into account. Comprehensive ablation studies show the effectiveness of the proposed method, and extensive evaluation results show the state-of-the-art performance of our system, e.g., Ours (1.652%) v.s. ORB-SLAM (3.247%}) in terms of translation error in KITTI Odometry benchmark. Source code is publicly available at: \href{https://github.com/Huangying-Zhan/DF-VO}{DF-VO}.",0
"In recent years, multi-view geometry-based methods have been widely used in monocular Visual Odometry due to their superior performance, but they have proven to be vulnerable to dynamic and low-texture scenes. Additionally, monocular methods suffer from the scale-drift issue, where errors accumulate over time. Recent studies have shown that deep neural networks can learn scene depths and relative camera positions in a self-supervised manner, making them a promising solution. However, these networks still struggle with accuracy due to the lack of geometric information. To tackle this issue, we propose a new system called DF-VO that integrates both multi-view geometry and deep learning on Depth and optical Flow. Our method carefully samples high-quality correspondences from deep flows and recovers accurate camera poses using a geometric module. We also address the scale-drift issue by aligning geometrically triangulated depths with scale-consistent deep depths, taking dynamic scenes into account. Our comprehensive ablation studies show the effectiveness of our method, and extensive evaluation results demonstrate the state-of-the-art performance of our system. For example, our system achieved a translation error of 1.652%, compared to ORB-SLAM's 3.247%, in the KITTI Odometry benchmark. The source code for our system is publicly available on GitHub at \href{https://github.com/Huangying-Zhan/DF-VO}{DF-VO}.",1
"Understanding driver activity is vital for in-vehicle systems that aim to reduce the incidence of car accidents rooted in cognitive distraction. Automating real-time behavior recognition while ensuring actions classification with high accuracy is however challenging, given the multitude of circumstances surrounding drivers, the unique traits of individuals, and the computational constraints imposed by in-vehicle embedded platforms. Prior work fails to jointly meet these runtime/accuracy requirements and mostly rely on a single sensing modality, which in turn can be a single point of failure. In this paper, we harness the exceptional feature extraction abilities of deep learning and propose a dedicated Interwoven Deep Convolutional Neural Network (InterCNN) architecture to tackle the problem of accurate classification of driver behaviors in real-time. The proposed solution exploits information from multi-stream inputs, i.e., in-vehicle cameras with different fields of view and optical flows computed based on recorded images, and merges through multiple fusion layers abstract features that it extracts. This builds a tight ensembling system, which significantly improves the robustness of the model. In addition, we introduce a temporal voting scheme based on historical inference instances, to enhance the classification accuracy. Experiments conducted with a dataset that we collect in a mock-up car environment demonstrate that the proposed InterCNN with MobileNet convolutional blocks can classify 9 different behaviors with 73.97% accuracy, and 5 'aggregated' behaviors with 81.66% accuracy. We further show that our architecture is highly computationally efficient, as it performs inferences within 15ms, which satisfies the real-time constraints of intelligent cars. Nevertheless, our InterCNN is robust to lossy input, as the classification remains accurate when two input streams are occluded.",0
"The accurate classification of driver behaviors in real-time is crucial for in-vehicle systems aiming to reduce car accidents caused by cognitive distraction. However, this is a challenging task due to the numerous circumstances surrounding drivers, the unique traits of individuals, and the computational limitations of in-vehicle embedded platforms. Previous attempts at behavior recognition have failed to meet the necessary runtime/accuracy requirements and typically rely on a single sensing modality, which is a single point of failure. To overcome these challenges, we propose a dedicated Interwoven Deep Convolutional Neural Network (InterCNN) architecture that harnesses the feature extraction abilities of deep learning. Our solution utilizes information from multiple inputs, such as in-vehicle cameras with various fields of view and optical flows computed based on recorded images, and merges them through multiple fusion layers to extract abstract features. This creates a robust ensembling system that significantly improves the accuracy of the model. We also introduce a temporal voting scheme based on historical inference instances to further enhance the classification accuracy. Our experiments with a dataset collected in a mock-up car environment demonstrate that our InterCNN with MobileNet convolutional blocks can classify 9 different behaviors with 73.97% accuracy and 5 'aggregated' behaviors with 81.66% accuracy. Additionally, our architecture is highly computationally efficient, performing inferences within 15ms, which satisfies the real-time constraints of intelligent cars. Furthermore, our InterCNN is robust to lossy inputs, as the classification remains accurate even when two input streams are occluded.",1
"Optical flow is a regression task where convolutional neural networks (CNNs) have led to major breakthroughs. However, this comes at major computational demands due to the use of cost-volumes and pyramidal representations. This was mitigated by producing flow predictions at quarter the resolution, which are upsampled using bilinear interpolation during test time. Consequently, fine details are usually lost and post-processing is needed to restore them. We propose the Normalized Convolution UPsampler (NCUP), an efficient joint upsampling approach to produce the full-resolution flow during the training of optical flow CNNs. Our proposed approach formulates the upsampling task as a sparse problem and employs the normalized convolutional neural networks to solve it. We evaluate our upsampler against existing joint upsampling approaches when trained end-to-end with a a coarse-to-fine optical flow CNN (PWCNet) and we show that it outperforms all other approaches on the FlyingChairs dataset while having at least one order fewer parameters. Moreover, we test our upsampler with a recurrent optical flow CNN (RAFT) and we achieve state-of-the-art results on Sintel benchmark with ~6% error reduction, and on-par on the KITTI dataset, while having 7.5% fewer parameters (see Figure 1). Finally, our upsampler shows better generalization capabilities than RAFT when trained and evaluated on different datasets.",0
"Convolutional neural networks (CNNs) have achieved significant progress in optical flow, a regression task. However, the use of cost-volumes and pyramidal representations results in high computational demands. To overcome this issue, flow predictions are made at a quarter of the resolution and upsampled using bilinear interpolation during test time, but this often leads to the loss of fine details that require post-processing. To address this problem, we propose an efficient joint upsampling approach called Normalized Convolution UPsampler (NCUP) that formulates the upsampling task as a sparse problem and employs normalized convolutional neural networks. We evaluate our approach by training it end-to-end with a coarse-to-fine optical flow CNN (PWCNet) and show that it outperforms existing joint upsampling approaches on the FlyingChairs dataset while having significantly fewer parameters. We also test our upsampler with a recurrent optical flow CNN (RAFT) and achieve state-of-the-art results on the Sintel benchmark with a 6% error reduction and on-par results on the KITTI dataset while using 7.5% fewer parameters. Additionally, our upsampler exhibits better generalization capabilities than RAFT when trained and evaluated on different datasets.",1
"Neural style transfer models have been used to stylize an ordinary video to specific styles. To ensure temporal inconsistency between the frames of the stylized video, a common approach is to estimate the optic flow of the pixels in the original video and make the generated pixels match the estimated optical flow. This is achieved by minimizing an optical flow-based (OFB) loss during model training. However, optical flow estimation is itself a challenging task, particularly in complex scenes. In addition, it incurs a high computational cost. We propose a much simpler temporal loss called the frame difference-based (FDB) loss to solve the temporal inconsistency problem. It is defined as the distance between the difference between the stylized frames and the difference between the original frames. The differences between the two frames are measured in both the pixel space and the feature space specified by the convolutional neural networks. A set of human behavior experiments involving 62 subjects with 25,600 votes showed that the performance of the proposed FDB loss matched that of the OFB loss. The performance was measured by subjective evaluation of stability and stylization quality of the generated videos on two typical video stylization models. The results suggest that the proposed FDB loss is a strong alternative to the commonly used OFB loss for video stylization.",0
"One approach to stylizing videos using neural style transfer models involves estimating the optic flow of the original video's pixels and matching the generated pixels to this flow. However, this can be challenging and computationally expensive. To address this issue, we propose a simpler temporal loss called the frame difference-based (FDB) loss. This loss calculates the distance between the differences in stylized and original frames, measured in both pixel and feature space. We conducted human behavior experiments that showed the FDB loss performed as well as the optical flow-based (OFB) loss for subjective evaluation of stability and stylization quality on two video stylization models. Our results suggest that the FDB loss is a strong alternative to the OFB loss for video stylization.",1
"When the input to a deep neural network (DNN) is a video signal, a sequence of feature tensors is produced at the intermediate layers of the model. If neighboring frames of the input video are related through motion, a natural question is, ""what is the relationship between the corresponding feature tensors?"" By analyzing the effect of common DNN operations on optical flow, we show that the motion present in each channel of a feature tensor is approximately equal to the scaled version of the input motion. The analysis is validated through experiments utilizing common motion models. %These results will be useful in collaborative intelligence applications where sequences of feature tensors need to be compressed or further analyzed.",0
"If a video signal is the input for a deep neural network (DNN), the model produces a series of feature tensors at its intermediate layers. If neighboring frames of the video signal are linked through motion, an inquiry arises concerning the correlation between the related feature tensors. By examining the impact of common DNN operations on optical flow, we demonstrate that the motion in each channel of a feature tensor is roughly equivalent to the scaled form of the input motion. Typical motion models were employed in experiments to confirm the analysis. These findings will prove beneficial in collaborative intelligence applications that require the compression or further analysis of sequences of feature tensors.",1
"Recognizing human actions based on videos has became one of the most popular areas of research in computer vision in recent years. This area has many applications such as surveillance, robotics, health care, video search and human-computer interaction. There are many problems associated with recognizing human actions in videos such as cluttered backgrounds, obstructions, viewpoints variation, execution speed and camera movement. A large number of methods have been proposed to solve the problems. This paper focus on spatial and temporal pattern recognition for the classification of videos using Deep Neural Networks. This model takes RGB images and Optical Flow as input data and outputs an action class number. The final recognition accuracy was about 94%.",0
"In recent years, identifying human actions in videos has become a highly researched area in computer vision. This field has numerous applications, including surveillance, robotics, healthcare, video search, and human-computer interaction. However, recognizing human actions in videos poses several challenges, such as cluttered backgrounds, obstructions, viewpoint variations, execution speed, and camera movements. Many techniques have been proposed to overcome these challenges. This study focuses on spatial and temporal pattern recognition using Deep Neural Networks to classify videos. The model utilizes RGB images and Optical Flow as input data and yields an action class number. The accuracy of recognition achieved was approximately 94%.",1
"We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we design a unified instance-aware photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we introduce a general-purpose auto-annotation scheme using any off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI and Cityscapes dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are available at https://github.com/SeokjuLee/Insta-DM .",0
"We have developed a training framework that models the 6-DoF motion of multiple dynamic objects, ego-motion, and depth in a monocular camera setup without supervision. Our framework has three main technical contributions. Firstly, we differentiate between inverse and forward projection when modeling the individual motion of each rigid object. We propose a neural forward projection module that uses a geometrically correct projection pipeline. Secondly, we design a unified instance-aware photometric and geometric consistency loss that imposes self-supervisory signals for every background and object region. Finally, we introduce a general-purpose auto-annotation scheme using off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps. In a detailed ablation study, we validate these proposed elements. Our experiments on the KITTI and Cityscapes dataset show that our framework outperforms state-of-the-art depth and motion estimation methods. We have made our code, dataset, and models available at https://github.com/SeokjuLee/Insta-DM.",1
"Recent constellations of satellites, including the Skysat constellation, are able to acquire bursts of images. This new acquisition mode allows for modern image restoration techniques, including multi-frame super-resolution. As the satellite moves during the acquisition of the burst, elevation changes in the scene translate into noticeable parallax. This parallax hinders the results of the restoration. To cope with this issue, we propose a novel parallax estimation method. The method is composed of a linear Plane+Parallax decomposition of the apparent motion and a multi-frame optical flow algorithm that exploits all frames simultaneously. Using SkySat L1A images, we show that the estimated per-pixel displacements are important for applying multi-frame super-resolution on scenes containing elevation changes and that can also be used to estimate a coarse 3D surface model.",0
"New satellite constellations, such as Skysat, have the capability to capture batches of images, allowing for advanced image restoration techniques like multi-frame super-resolution. However, due to the parallax caused by elevation changes during the image acquisition process, the restoration results may be hindered. To address this issue, we introduce a new method for estimating parallax. This method involves a linear Plane+Parallax decomposition of the apparent motion, along with a multi-frame optical flow algorithm that utilizes all frames at once. By using SkySat L1A images, we demonstrate that this method can accurately estimate per-pixel displacements, which is crucial for applying multi-frame super-resolution to images with elevation changes. Additionally, this method can be used to generate a rudimentary 3D surface model.",1
"Cycling is a promising sustainable mode for commuting and leisure in cities, however, the fear of getting hit or fall reduces its wide expansion as a commuting mode. In this paper, we introduce a novel method called CyclingNet for detecting cycling near misses from video streams generated by a mounted frontal camera on a bike regardless of the camera position, the conditions of the built, the visual conditions and without any restrictions on the riding behaviour. CyclingNet is a deep computer vision model based on convolutional structure embedded with self-attention bidirectional long-short term memory (LSTM) blocks that aim to understand near misses from both sequential images of scenes and their optical flows. The model is trained on scenes of both safe rides and near misses. After 42 hours of training on a single GPU, the model shows high accuracy on the training, testing and validation sets. The model is intended to be used for generating information that can draw significant conclusions regarding cycling behaviour in cities and elsewhere, which could help planners and policy-makers to better understand the requirement of safety measures when designing infrastructure or drawing policies. As for future work, the model can be pipelined with other state-of-the-art classifiers and object detectors simultaneously to understand the causality of near misses based on factors related to interactions of road-users, the built and the natural environments.",0
"Although cycling is a sustainable method of transportation and leisure in urban areas, the fear of accidents and injuries has hindered its widespread adoption as a commuting mode. To address this issue, we present a new technique called CyclingNet, which uses a frontal camera mounted on a bicycle to identify near misses regardless of the camera's location or the visual conditions. CyclingNet is a deep learning model that incorporates a convolutional structure with self-attention bidirectional long-short term memory (LSTM) blocks to detect near misses from sequential images and their optical flows. The model is trained on both safe and near miss scenarios, achieving high accuracy on training, testing, and validation sets after 42 hours of training on one GPU. The model can provide valuable insights into cycling behavior in urban areas, helping city planners and policymakers better understand the need for safety measures in infrastructure design and policy-making. Future work can involve combining CyclingNet with other cutting-edge classifiers and object detectors to examine the factors influencing near misses, such as interactions between road users and the built or natural environments.",1
The goal of this paper is propose a mathematical framework for optical flow refinement with non-quadratic regularization using variational techniques. We demonstrate how the model can be suitably adapted for both rigid and fluid motion estimation. We study the problem as an abstract IVP using an evolutionary PDE approach. We show that for a particular choice of constraint our model approximates the continuity model with non-quadratic regularization using augmented Lagrangian techniques. We subsequently show the results of our algorithm on different datasets.,0
"In this paper, we aim to present a mathematical framework that utilizes variational techniques and non-quadratic regularization to refine optical flow. We showcase the adaptability of our model for estimating both rigid and fluid motion. Our approach involves studying the problem as an abstract initial value problem using an evolutionary PDE approach. By selecting a specific constraint, we demonstrate that our model approximates the continuity model with non-quadratic regularization using augmented Lagrangian techniques. Finally, we present the results of our algorithm on various datasets.",1
"Optical flow estimation is an essential step for many real-world computer vision tasks. Existing deep networks have achieved satisfactory results by mostly employing a pyramidal coarse-to-fine paradigm, where a key process is to adopt warped target feature based on previous flow prediction to correlate with source feature for building 3D matching cost volume. However, the warping operation can lead to troublesome ghosting problem that results in ambiguity. Moreover, occluded areas are treated equally with non occluded regions in most existing works, which may cause performance degradation. To deal with these challenges, we propose a lightweight yet efficient optical flow network, named OAS-Net (occlusion aware sampling network) for accurate optical flow. First, a new sampling based correlation layer is employed without noisy warping operation. Second, a novel occlusion aware module is presented to make raw cost volume conscious of occluded regions. Third, a shared flow and occlusion awareness decoder is adopted for structure compactness. Experiments on Sintel and KITTI datasets demonstrate the effectiveness of proposed approaches.",0
"Many computer vision tasks depend on accurately estimating optical flow, but current deep network approaches can suffer from ghosting issues caused by warping operations and treat occluded areas the same as non-occluded regions, which can lead to reduced performance. To address these challenges, we propose a lightweight and efficient optical flow network called OAS-Net (occlusion aware sampling network). OAS-Net employs a new sampling-based correlation layer instead of warping and includes an occlusion-aware module to handle occluded areas. We also use a shared flow and occlusion awareness decoder to keep the structure compact. Our experiments on Sintel and KITTI datasets demonstrate the effectiveness of our approach.",1
"Generating non-existing frames from a consecutive video sequence has been an interesting and challenging problem in the video processing field. Typical kernel-based interpolation methods predict pixels with a single convolution process that convolves source frames with spatially adaptive local kernels, which circumvents the time-consuming, explicit motion estimation in the form of optical flow. However, when scene motion is larger than the pre-defined kernel size, these methods are prone to yield less plausible results. In addition, they cannot directly generate a frame at an arbitrary temporal position because the learned kernels are tied to the midpoint in time between the input frames. In this paper, we try to solve these problems and propose a novel non-flow kernel-based approach that we refer to as enhanced deformable separable convolution (EDSC) to estimate not only adaptive kernels, but also offsets, masks and biases to make the network obtain information from non-local neighborhood. During the learning process, different intermediate time step can be involved as a control variable by means of an extension of coord-conv trick, allowing the estimated components to vary with different input temporal information. This makes our method capable to produce multiple in-between frames. Furthermore, we investigate the relationships between our method and other typical kernel- and flow-based methods. Experimental results show that our method performs favorably against the state-of-the-art methods across a broad range of datasets. Code will be publicly available on URL: \url{https://github.com/Xianhang/EDSC-pytorch}.",0
"The challenge of generating non-existent frames from a consecutive video sequence has intrigued and tested the capabilities of the video processing field. Traditional interpolation techniques based on kernels use a single convolution process to predict pixels, employing locally adaptive kernels that alleviate the need for explicit motion estimation in the form of optical flow, which can be time-consuming. However, these methods fail to produce convincing results when scene motion exceeds the kernel's predefined size. Additionally, they cannot generate a frame at any arbitrary temporal position since the learned kernels are linked to the midpoint in time between the input frames. To address these limitations, we present a novel non-flow kernel-based method called enhanced deformable separable convolution (EDSC), which estimates adaptive kernels, masks, biases, and offsets to enable the network to gather information from non-local neighborhoods. Our approach permits the involvement of different intermediate time steps as a control variable through an extension of the coord-conv trick during the learning process, allowing the estimated components to vary with various input temporal information, which enables the generation of multiple in-between frames. Furthermore, we explore the connections between our method and other traditional kernel- and flow-based techniques. Our experimental results show that our method outperforms state-of-the-art methods across a wide range of datasets. The code for our method is publicly available at \url{https://github.com/Xianhang/EDSC-pytorch}.",1
